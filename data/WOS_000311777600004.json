{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "finite_minimax_problems"}, {"score": 0.011328424034649669, "phrase": "j.v._burke"}, {"score": 0.011216278962471704, "phrase": "a._s._lewis"}, {"score": 0.011050116263806755, "phrase": "m._l._overton"}, {"score": 0.00476697427418509, "phrase": "derivative-free_optimization"}, {"score": 0.004602773636967002, "phrase": "optimization_problems"}, {"score": 0.004534136929881113, "phrase": "analytical_knowledge"}, {"score": 0.004205940624913343, "phrase": "derivative-free_methods"}, {"score": 0.003673675864705682, "phrase": "smooth_substructure"}, {"score": 0.003511613254403125, "phrase": "burke_et_al"}, {"score": 0.0032900200522703923, "phrase": "random_sampling"}, {"score": 0.0027327327233891865, "phrase": "nonconvex_optimization"}, {"score": 0.00270550310876174, "phrase": "siam"}, {"score": 0.0026919067120537055, "phrase": "j._optim"}, {"score": 0.0023864566654035924, "phrase": "robust_simplex_gradient_descent_direction"}, {"score": 0.0022133711375320266, "phrase": "resulting_algorithm"}, {"score": 0.0021693692831601745, "phrase": "directional_direct-search_framework"}, {"score": 0.0021049977753042253, "phrase": "algorithm's_effectiveness"}], "paper_keywords": ["optimization", " derivative-free optimization", " active set", " active manifold", " gradient sampling"], "paper_abstract": "Derivative-free optimization focuses on designing methods to solve optimization problems without the analytical knowledge of the function. In this paper, we consider the problem of designing derivative-free methods for finite minimax problems: min(x) max(i)= 1,2,..., N {f(i)(x)}. In order to solve the problem efficiently, we seek to exploit the smooth substructure within the problem. Using ideas developed by Burke et al. [J.V. Burke, A. S. Lewis, and M. L. Overton, Approximating subdifferentials by random sampling of gradients, Math. Oper. Res. 27(3) (2002), pp. 567-584; J.V. Burke, A. S. Lewis, and M. L. Overton, A robust gradient sampling algorithm for nonsmooth, nonconvex optimization, SIAM J. Optim. 15(3) (2005), pp. 751-779 (electronic)], we create the idea of a robust simplex gradient descent direction and use it to accelerate convergence. Convergence is proven by showing that the resulting algorithm fits into the directional direct-search framework. Numerical tests demonstrate the algorithm's effectiveness on finite minimax problems.", "paper_title": "Derivative-free optimization methods for finite minimax problems", "paper_id": "WOS:000311777600004"}