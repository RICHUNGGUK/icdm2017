{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "audio_signals"}, {"score": 0.004743594110762319, "phrase": "causal_system"}, {"score": 0.004558414405811025, "phrase": "musical_events"}, {"score": 0.004272724011826495, "phrase": "auditory_front-end"}, {"score": 0.0037913267098534887, "phrase": "musical_stream_events"}, {"score": 0.0036980501827538455, "phrase": "time_descriptions"}, {"score": 0.003553540530701019, "phrase": "inter-onset_intervals"}, {"score": 0.0033306180744375616, "phrase": "expectation_module"}, {"score": 0.0032975819773787985, "phrase": "predictive_partial_match"}, {"score": 0.002687922713487696, "phrase": "unsupervised_encoding"}, {"score": 0.0026480044868438875, "phrase": "musical_sequence"}, {"score": 0.0025066304722422463, "phrase": "enst-drums_database"}, {"score": 0.002481747081849503, "phrase": "annotated_drum_recordings"}, {"score": 0.0021907299375981356, "phrase": "induced_representation"}, {"score": 0.002136747803494932, "phrase": "expectation_patterns"}, {"score": 0.0021049977753042253, "phrase": "causal_way"}], "paper_keywords": ["unsupervised learning", " music", " audio", " expectation"], "paper_abstract": "A causal system to represent a stream of music into musical events, and to generate further expected events, is presented. Starting from an auditory front-end that extracts low-level (i.e. MFCC) and mid-level features such as onsets and beats, an unsupervised clustering process builds and maintains a set of symbols aimed at representing musical stream events using both timbre and time descriptions. The time events are represented using inter-onset intervals relative to the beats. These symbols are then processed by an expectation module using Predictive Partial Match, a multiscale technique based on N-grams. To characterise the ability of the system to generate an expectation that matches both ground truth and system transcription, we introduce several measures that take into account the uncertainty associated with the unsupervised encoding of the musical sequence. The system is evaluated using a subset of the ENST-drums database of annotated drum recordings. We compare three approaches to combine timing (when) and timbre (what) expectation. In our experiments, we show that the induced representation is useful for generating expectation patterns in a causal way.", "paper_title": "What/when causal expectation modelling applied to audio signals", "paper_id": "WOS:000266280300003"}