{"auto_keywords": [{"score": 0.04455074562181148, "phrase": "training_samples"}, {"score": 0.010929583052602283, "phrase": "unseen_samples"}, {"score": 0.00481495049065317, "phrase": "architecture_selection"}, {"score": 0.00475924837336259, "phrase": "radial_basis_function_neural_network"}, {"score": 0.004676895436217523, "phrase": "generalization_error_bounds"}, {"score": 0.0045959609346817535, "phrase": "current_error_models"}, {"score": 0.004464156619275582, "phrase": "effective_parameters"}, {"score": 0.003973523379376352, "phrase": "entire_input_space"}, {"score": 0.0038820411362656803, "phrase": "support_vector_machine"}, {"score": 0.003641141978059436, "phrase": "rbfnn"}, {"score": 0.003557283483525861, "phrase": "multilayer_perceptron_neural_network"}, {"score": 0.003516079377293321, "phrase": "mlpnn"}, {"score": 0.0034551632687788857, "phrase": "local_learning_machines"}, {"score": 0.003039445153076014, "phrase": "localized_generalization_error_model"}, {"score": 0.0029349892964423197, "phrase": "generalization_error"}, {"score": 0.00280126189041041, "phrase": "stochastic_sensitivity_measure"}, {"score": 0.0026580683046325253, "phrase": "architecture_selection_technique"}, {"score": 0.002566685388128598, "phrase": "maximal_coverage"}, {"score": 0.0024784363584530976, "phrase": "generalization_error_threshold"}, {"score": 0.002324433454350404, "phrase": "cross_validation"}, {"score": 0.0022576249214220187, "phrase": "sequential_learning"}, {"score": 0.0021297010898172534, "phrase": "best_testing_classification_accuracy"}], "paper_keywords": ["localized generalization error", " network architecture selection", " radial basis function neural network (RBFNN)", " sensitivity measure"], "paper_abstract": "The generalization error bounds found by current error models using the number of effective parameters of a classifier and the number of training samples are usually very loose. These bounds are intended for the entire input space. However, support vector machine (SVM), radial basis function neural network (RBFNN), and multilayer perceptron neural network (MLPNN) are local learning machines for solving problems and treat unseen samples near the training samples to be more important. In this paper, we propose a localized generalization error model which bounds from above the generalization error within a neighborhood of the training samples using stochastic sensitivity measure. It is then used to develop an architecture selection technique for a classifier-with maximal coverage of unseen samples by specifying a generalization error threshold. Experiments using 17 University of California at Irvine (UCI) data sets show that, in comparison with cross validation (CV), sequential learning, and two other ad hoc methods, our technique consistently yields the best testing classification accuracy with fewer hidden neurons and less training time.", "paper_title": "Localized generalization error model and its application to architecture selection for radial basis function neural network", "paper_id": "WOS:000249279400003"}