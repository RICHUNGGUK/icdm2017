{"auto_keywords": [{"score": 0.03990657191803381, "phrase": "acoustic_vector"}, {"score": 0.010612387000973441, "phrase": "computational_auditory_scene_analysis"}, {"score": 0.007576024046049014, "phrase": "singing_voice"}, {"score": 0.004780030485538593, "phrase": "missing_feature_methods"}, {"score": 0.004728122501822142, "phrase": "major_challenge"}, {"score": 0.004609175313390942, "phrase": "monaural_popular_music_recording"}, {"score": 0.00430112267051817, "phrase": "first_stage"}, {"score": 0.004208172995899471, "phrase": "casa"}, {"score": 0.004132237700479874, "phrase": "singing_voice_units"}, {"score": 0.0040873354289955605, "phrase": "mixture_signal"}, {"score": 0.0038841330270679097, "phrase": "pitch-based_features"}, {"score": 0.003664196785572724, "phrase": "binary_time-frequency"}, {"score": 0.0034946693480479712, "phrase": "corresponding_t-f_unit"}, {"score": 0.0030537595549694134, "phrase": "second_stage"}, {"score": 0.0028183634079717136, "phrase": "incomplete_acoustic_vectors"}, {"score": 0.002727362288104992, "phrase": "complete_acoustic_vector"}, {"score": 0.0026296828616623994, "phrase": "gammatone_frequency_cepstral_coefficients"}, {"score": 0.002280948283235427, "phrase": "reconstruction_method"}, {"score": 0.002256117165288155, "phrase": "marginalization_method"}, {"score": 0.0022072611570187334, "phrase": "significantly_good_performances"}, {"score": 0.0021752783872387173, "phrase": "signal-to-accompaniment_ratios"}, {"score": 0.0021204172495922513, "phrase": "-aeuro_parts"}], "paper_keywords": ["Computational auditory scene analysis (CASA)", " Reconstruction", " Marginalization", " Singer identification"], "paper_abstract": "A major challenge for the identification of singers from monaural popular music recording is to remove or alleviate the influence of accompaniments. Our system is realized in two stages. In the first stage, we exploit computational auditory scene analysis (CASA) to segregate the singing voice units from a mixture signal. First, the pitch of singing voice is estimated to extract the pitch-based features of each unit in an acoustic vector. These features are then exploited to estimate the binary time-frequency (T-F) masks, where 1 indicates that the corresponding T-F unit is dominated by the singing voice, and 0 indicates otherwise. These regions dominated by the singing voice are considered reliable, and other units are unreliable or missing. Thus the acoustic vector is incomplete. In the second stage, two missing feature methods, the reconstruction of acoustic vector and the marginalization, are used to identify the singer by dealing with the incomplete acoustic vectors. For the reconstruction of acoustic vector, the complete acoustic vector is first reconstructed and then converted to obtain the Gammatone frequency cepstral coefficients (GFCCs), which are further used to identify the singer. For the marginalization, the probabilities that the voice belonging to a certain singer are computed on the basis of only the reliable components. We find that the reconstruction method outperforms the marginalization method, while both methods have significantly good performances, especially at signal-to-accompaniment ratios (SARs) of 0 dB and -aEuro parts per thousand 3 dB, in contrast to another system.", "paper_title": "Singer identification based on computational auditory scene analysis and missing feature methods", "paper_id": "WOS:000336277700001"}