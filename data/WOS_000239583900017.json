{"auto_keywords": [{"score": 0.04326193515885001, "phrase": "hypothesis"}, {"score": 0.005373408751561068, "phrase": "text"}, {"score": 0.00481495049065317, "phrase": "inversion_transduction_grammars"}, {"score": 0.004715702140552199, "phrase": "pascal_challenge's_textual_entailment_recognition_task"}, {"score": 0.0046583658614604145, "phrase": "rte"}, {"score": 0.004523272849033429, "phrase": "intriguing_opportunities"}, {"score": 0.004338661611644755, "phrase": "strong_language_universal_constraint"}, {"score": 0.0042301170957922294, "phrase": "itg"}, {"score": 0.0036468482216440233, "phrase": "accuracy_gains"}, {"score": 0.0035965032173588753, "phrase": "numerous_language_acquisition_tasks"}, {"score": 0.0034978813769575233, "phrase": "rte_challenge"}, {"score": 0.0033317342432676385, "phrase": "meaningful_analysis"}, {"score": 0.0032629585038738856, "phrase": "itg_hypothesis"}, {"score": 0.0031734538599865973, "phrase": "information_retrieval"}, {"score": 0.003129623571496151, "phrase": "comparable_documents"}, {"score": 0.003086396772496527, "phrase": "reading_comprehension"}, {"score": 0.002980916123365584, "phrase": "information_extraction"}, {"score": 0.002939737235379635, "phrase": "machine_translation"}, {"score": 0.002723186210025975, "phrase": "rte_problem"}, {"score": 0.00266693888510356, "phrase": "simple_generic_bracketing_itgs"}, {"score": 0.0023859534478427313, "phrase": "lexical_variation"}, {"score": 0.0022883734727569298, "phrase": "hypothesis_strings"}, {"score": 0.0021795515361987144, "phrase": "task_subsets"}, {"score": 0.0021049977753042253, "phrase": "bracketing_itg's_structure"}], "paper_keywords": [""], "paper_abstract": "The PASCAL Challenge's textual entailment recognition task, or RTE, presents intriguing opportunities to test various implications of the strong language universal constraint posited by Wu's (1995, 1997) Inversion Transduction Grammar (ITG) hypothesis. The ITG Hypothesis provides a strong inductive bias, and has been repeatedly shown empirically to yield both efficiency and accuracy gains for numerous language acquisition tasks. Since the RTE challenge abstracts over many tasks, it invites meaningful analysis of the ITG Hypothesis across tasks including information retrieval, comparable documents, reading comprehension, question answering, information extraction, machine translation, and paraphrase acquisition. We investigate two new models for the RTE problem that employ simple generic Bracketing ITGs. Experimental results show that, even in the absence of any thesaurus to accommodate lexical variation between the Text and the Hypothesis strings, surprisingly strong results for a number of the task subsets are obtainable from the Bracketing ITG's structure matching bias alone.", "paper_title": "Textual entailment recognition using inversion transduction grammars", "paper_id": "WOS:000239583900017"}