{"auto_keywords": [{"score": 0.042150781020745584, "phrase": "bpdc"}, {"score": 0.00481495049065317, "phrase": "reservoir_computing_methods"}, {"score": 0.004712988631183401, "phrase": "recurrent_neural_network"}, {"score": 0.004382127278148236, "phrase": "simple_external_classification_layer"}, {"score": 0.004234532601573088, "phrase": "liquid_state_machines"}, {"score": 0.004144810537437608, "phrase": "echo_state_networks"}, {"score": 0.003937126962709418, "phrase": "individual_descriptions"}, {"score": 0.0036137713873827374, "phrase": "experimental_results"}, {"score": 0.0034033029171804106, "phrase": "broad_range"}, {"score": 0.003374250301861101, "phrase": "reservoir_parameters"}, {"score": 0.0033454448643854525, "phrase": "network_dynamics"}, {"score": 0.003288567185099276, "phrase": "node_complexity"}, {"score": 0.00319134078848996, "phrase": "benchmark_tests"}, {"score": 0.0031640918630415566, "phrase": "different_characteristics"}, {"score": 0.0030705342386533083, "phrase": "new_measure"}, {"score": 0.003031287195944274, "phrase": "reservoir_dynamics"}, {"score": 0.0029925402963843282, "phrase": "lyapunov_exponents"}, {"score": 0.002954287204671029, "phrase": "previous_measures"}, {"score": 0.002586457551025025, "phrase": "optimal_value"}, {"score": 0.002553382046204689, "phrase": "global_scaling"}, {"score": 0.0025207284405252914, "phrase": "weight_matrix"}, {"score": 0.002467228805628948, "phrase": "standard_measures"}, {"score": 0.0022936610196951962, "phrase": "reservoir_computing"}, {"score": 0.002254624822004988, "phrase": "easy_simulation"}, {"score": 0.0022257833099336858, "phrase": "wide_range"}, {"score": 0.0022067604433257814, "phrase": "reservoir_topologies"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["reservoir computing", " memory capability", " chaos", " Lyapunov exponent"], "paper_abstract": "Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "An experimental unification of reservoir computing methods", "paper_id": "WOS:000247684600010"}