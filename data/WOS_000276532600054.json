{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multikernel_semiparametric_support_vector_regression"}, {"score": 0.011789778841063109, "phrase": "computational_complexity"}, {"score": 0.004570491683870396, "phrase": "reducing_samples"}, {"score": 0.00399714680711765, "phrase": "single_kernel_v-svr"}, {"score": 0.0038224221344037236, "phrase": "training_samples"}, {"score": 0.0037658881407605445, "phrase": "admissible_functions"}, {"score": 0.0035216195857272403, "phrase": "proposed_multikernel_learning_algorithm"}, {"score": 0.0030794433253250476, "phrase": "single_kernel_support_vector_regression"}, {"score": 0.002901124599876666, "phrase": "regression_accuracy"}, {"score": 0.002480520548392772, "phrase": "comparable_generalization_performance"}, {"score": 0.0022013916728990564, "phrase": "synthetic_and_real-world_benchmark_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Support vector regression", " Multiple kernel learning", " Semiparmetric technique"], "paper_abstract": "In this paper, the reducing samples strategy instead of classical v-support vector regression (v-SVR), viz. single kernel v-SVR, is utilized to select training samples for admissible functions so as to curtail the computational complexity. The proposed multikernel learning algorithm, namely reducing samples based multikernel semiparametric support vector regression (RS-MSSVR), has an advantage over the single kernel support vector regression (classical epsilon-SVR) in regression accuracy. Meantime, in comparison with multikernel semiparametric support vector regression (MSSVR), the algorithm is also favorable for computational complexity with the comparable generalization performance. Finally, the efficacy and feasibility of RS-MSSVR are corroborated by experiments on the synthetic and real-world benchmark data sets. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Reducing samples for accelerating multikernel semiparametric support vector regression", "paper_id": "WOS:000276532600054"}