{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "performance_faults"}, {"score": 0.004769444094319619, "phrase": "large_scale_mpi_applications"}, {"score": 0.004724365740062236, "phrase": "probabilistic_progress-dependence_inference"}, {"score": 0.004657542074154025, "phrase": "large-scale_parallel_applications"}, {"score": 0.004483909657713567, "phrase": "failure_root_causes"}, {"score": 0.004316722156877335, "phrase": "program_execution"}, {"score": 0.004195418010131832, "phrase": "massively_parallel_applications"}, {"score": 0.004077508669922702, "phrase": "novel_technique"}, {"score": 0.003925414526537783, "phrase": "parallel_program"}, {"score": 0.0035864603175592854, "phrase": "scalable_runtime_analysis"}, {"score": 0.0035525230119401153, "phrase": "static_analysis"}, {"score": 0.0033875770020590796, "phrase": "code_lines"}, {"score": 0.0032302647179383915, "phrase": "novel_algorithm"}, {"score": 0.0031845067058084583, "phrase": "probabilistically_progress_dependence"}, {"score": 0.003154360892955196, "phrase": "mpi_tasks"}, {"score": 0.0031096747349602344, "phrase": "globally_constructed_markov_model"}, {"score": 0.003065619675251781, "phrase": "tasks'_control-flow_behavior"}, {"score": 0.0029935730127943496, "phrase": "previous_work"}, {"score": 0.0027610089418453614, "phrase": "static_backward_slicing_analysis"}, {"score": 0.0026452734009203764, "phrase": "current_state"}, {"score": 0.0026077803487003, "phrase": "blind_study"}, {"score": 0.0025343769150179764, "phrase": "root_cause"}, {"score": 0.0024984517279573906, "phrase": "concurrency_bug"}, {"score": 0.0024630345285372958, "phrase": "molecular_dynamics_simulation"}, {"score": 0.002304263635960015, "phrase": "fault_coverage"}, {"score": 0.0022608057001451414, "phrase": "fault_injections"}, {"score": 0.0021049977753042253, "phrase": "parallel_tasks"}], "paper_keywords": ["Distributed debugging", " MPI", " progress dependence", " parallel applications"], "paper_abstract": "Debugging large-scale parallel applications is challenging. Most existing techniques provide little information about failure root causes. Further, most debuggers significantly slow down program execution, and run sluggishly with massively parallel applications. This paper presents a novel technique that scalably infers the tasks in a parallel program on which a failure occurred, as well as the code in which it originated. Our technique combines scalable runtime analysis with static analysis to determine the least-progressed task(s) and to identify the code lines at which the failure arose. We present a novel algorithm that infers probabilistically progress dependence among MPI tasks using a globally constructed Markov model that represents tasks' control-flow behavior. In comparison to previous work, our algorithm infers more precisely the least-progressed task. We combine this technique with static backward slicing analysis, further isolating the code responsible for the current state. A blind study demonstrates that our technique isolates the root cause of a concurrency bug in a molecular dynamics simulation, which only manifests itself at 7,996 tasks or more. We extensively evaluate fault coverage of our technique via fault injections in 10 HPC benchmarks and show that our analysis takes less than a few seconds on thousands of parallel tasks.", "paper_title": "Diagnosis of Performance Faults in Large Scale MPI Applications via Probabilistic Progress-Dependence Inference", "paper_id": "WOS:000352728200007"}