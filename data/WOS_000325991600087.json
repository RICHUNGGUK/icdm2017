{"auto_keywords": [{"score": 0.04678178460605593, "phrase": "ieee_visualization_conference"}, {"score": 0.035305993723685085, "phrase": "ieee_information_visualization_conference"}, {"score": 0.004646496622257368, "phrase": "historic_development"}, {"score": 0.004613516457919661, "phrase": "evaluation_practices"}, {"score": 0.004250588556735512, "phrase": "systematic_understanding"}, {"score": 0.004145882022189891, "phrase": "presented_evaluations"}, {"score": 0.004029359223675788, "phrase": "systematic_review"}, {"score": 0.0039300808153474205, "phrase": "published_papers"}, {"score": 0.0038606630003037864, "phrase": "coding_scheme"}, {"score": 0.0038060092376105414, "phrase": "lam_et_al"}, {"score": 0.0032073043728420843, "phrase": "resulting_images"}, {"score": 0.0031845067058084583, "phrase": "algorithm_performance"}, {"score": 0.0029336622847544857, "phrase": "steady_increase"}, {"score": 0.002912803994461996, "phrase": "evaluation_methods"}, {"score": 0.0028107098861552124, "phrase": "subjective_feedback"}, {"score": 0.0025073851194697397, "phrase": "increasing_percentage"}, {"score": 0.0024718415971488627, "phrase": "user_performance"}, {"score": 0.002454258761940649, "phrase": "experience_testing"}, {"score": 0.002419466507425567, "phrase": "ieee_information_visualization"}, {"score": 0.0023513512089705325, "phrase": "work_practices"}, {"score": 0.0022851491693940272, "phrase": "visual_tools"}, {"score": 0.0021971473452127126, "phrase": "requirements_analyses"}, {"score": 0.002181514206602054, "phrase": "domain-specific_work_practices"}, {"score": 0.0021049977753042253, "phrase": "external_validity"}], "paper_keywords": ["Evaluation", " validation", " systematic review", " visualization", " scientific visualization", " information visualization"], "paper_abstract": "We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.", "paper_title": "A Systematic Review on the Practice of Evaluating Visualization", "paper_id": "WOS:000325991600087"}