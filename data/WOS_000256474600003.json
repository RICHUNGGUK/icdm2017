{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sigmoid_function"}, {"score": 0.004725385523804256, "phrase": "large_fpga_implementations"}, {"score": 0.004424768776701965, "phrase": "asic_neurocomputers"}, {"score": 0.004066076430818046, "phrase": "fpga_neurocomputers"}, {"score": 0.0026384167183586015, "phrase": "activation_function"}, {"score": 0.0025892330566358503, "phrase": "artificial_neural_networks"}, {"score": 0.0021049977753042253, "phrase": "published_literature"}], "paper_keywords": ["artificial neural network", " FPGA", " sigmoid", " arithmetic"], "paper_abstract": "There has been much study of ASIC neurocomputers but, in comparison, relatively little for FPGA neurocomputers. Nevertheless, with current (and future) dense, high-speed FPGAs, the latter are now viable and will be more successful than the former. In this paper, we discuss a technique for low-error, high-speed implementations of the sigmoid function in such FPGAs. This function is commonly used as an activation function in artificial neural networks, but it also has applications in many other areas. Our results compare very favourably with others that have been reported in the published literature.", "paper_title": "Low-error, high-speed approximation of the sigmoid function for large FPGA implementations", "paper_id": "WOS:000256474600003"}