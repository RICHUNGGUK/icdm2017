{"auto_keywords": [{"score": 0.04720912624670318, "phrase": "associated_human_subjective_scores"}, {"score": 0.04117275374122391, "phrase": "opinion-aware_methods"}, {"score": 0.035063062788852184, "phrase": "opinion-unaware_biqa_method"}, {"score": 0.004613516457919661, "phrase": "regression_models"}, {"score": 0.004569905085027681, "phrase": "training_images"}, {"score": 0.004441517985511521, "phrase": "perceptual_quality"}, {"score": 0.004399525320473839, "phrase": "test_images"}, {"score": 0.0042354707960660706, "phrase": "large_amount"}, {"score": 0.004195418010131832, "phrase": "training_samples"}, {"score": 0.004038943596016986, "phrase": "distortion_types"}, {"score": 0.00398177607914381, "phrase": "biqa_models"}, {"score": 0.0038698476651328898, "phrase": "weak_generalization_capability"}, {"score": 0.0036727235025069828, "phrase": "opinion-unaware_methods"}, {"score": 0.003603549914861701, "phrase": "human_subjective_scores"}, {"score": 0.003436242862104006, "phrase": "good_generalization_capability"}, {"score": 0.0032766780632972363, "phrase": "consistently_better_quality_prediction_accuracy"}, {"score": 0.002937153116494391, "phrase": "existing_opinion-aware_methods"}, {"score": 0.0028409576481842457, "phrase": "natural_image_statistics"}, {"score": 0.0028006988338192375, "phrase": "multiple_cues"}, {"score": 0.0027348611289528583, "phrase": "multivariate_gaussian_model"}, {"score": 0.002708960273193547, "phrase": "image_patches"}, {"score": 0.0026452734009203764, "phrase": "pristine_natural_images"}, {"score": 0.0025954008219792337, "phrase": "learned_multivariate_gaussian_model"}, {"score": 0.0025586128304269616, "phrase": "bhattacharyya-like_distance"}, {"score": 0.0024513404432665153, "phrase": "image_patch"}, {"score": 0.0023936956017015696, "phrase": "overall_quality_score"}, {"score": 0.0023485549678832628, "phrase": "average_pooling"}, {"score": 0.0023152577218214804, "phrase": "proposed_biqa_method"}, {"score": 0.0022608057001451414, "phrase": "distorted_sample_images"}, {"score": 0.0022393843660697484, "phrase": "subjective_quality_scores"}, {"score": 0.002186712881521657, "phrase": "extensive_experiments"}, {"score": 0.0021352776064254195, "phrase": "state-of-the-art_opinion-aware_biqa_methods"}, {"score": 0.002105105290820392, "phrase": "matlab"}], "paper_keywords": ["Blind image quality assessment", " natural image statistics", " multivariate Gaussian"], "paper_abstract": "Existing blind image quality assessment (BIQA) methods are mostly opinion-aware. They learn regression models from training images with associated human subjective scores to predict the perceptual quality of test images. Such opinion-aware methods, however, require a large amount of training samples with associated human subjective scores and of a variety of distortion types. The BIQA models learned by opinion-aware methods often have weak generalization capability, hereby limiting their usability in practice. By comparison, opinion-unaware methods do not need human subjective scores for training, and thus have greater potential for good generalization capability. Unfortunately, thus far no opinion-unaware BIQA method has shown consistently better quality prediction accuracy than the opinion-aware methods. Here, we aim to develop an opinion-unaware BIQA method that can compete with, and perhaps outperform, the existing opinion-aware methods. By integrating the features of natural image statistics derived from multiple cues, we learn a multivariate Gaussian model of image patches from a collection of pristine natural images. Using the learned multivariate Gaussian model, a Bhattacharyya-like distance is used to measure the quality of each image patch, and then an overall quality score is obtained by average pooling. The proposed BIQA method does not need any distorted sample images nor subjective quality scores for training, yet extensive experiments demonstrate its superior quality-prediction performance to the state-of-the-art opinion-aware BIQA methods. The MATLAB source code of our algorithm is publicly available at www.comp.polyu.edu.hk/similar to cslzhang/IQA/ILNIQE/ILNIQE.htm.", "paper_title": "A Feature-Enriched Completely Blind Image Quality Evaluator", "paper_id": "WOS:000354459700005"}