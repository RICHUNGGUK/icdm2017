{"auto_keywords": [{"score": 0.025979599489653175, "phrase": "recognition_system"}, {"score": 0.00481495049065317, "phrase": "simultaneous_speaker_identification"}, {"score": 0.004777569299319633, "phrase": "speech_recognition"}, {"score": 0.004506265705673849, "phrase": "competing_speaker"}, {"score": 0.0044192837023190445, "phrase": "speech_fragment_decoding_technique"}, {"score": 0.004300310127339099, "phrase": "coupled_problems"}, {"score": 0.004266907250334316, "phrase": "data-driven_techniques"}, {"score": 0.004168240841388958, "phrase": "spectro-temporal_representation"}, {"score": 0.003870541730075597, "phrase": "speech_sources"}, {"score": 0.0038255128976509545, "phrase": "speech_fragment_decoder"}, {"score": 0.0037516215763875225, "phrase": "missing_data_techniques"}, {"score": 0.0037224647319912293, "phrase": "clean_speech_models"}, {"score": 0.0035660913513115267, "phrase": "word_sequence"}, {"score": 0.0034971932704828197, "phrase": "target_speaker_model"}, {"score": 0.00337650257503995, "phrase": "system_oil"}, {"score": 0.0033502512757488433, "phrase": "recognition_task"}, {"score": 0.003324203391715804, "phrase": "artificially_mixed_target"}, {"score": 0.003298357357267836, "phrase": "masker_speech_utterances"}, {"score": 0.0032599633778910516, "phrase": "fragment_decoder"}, {"score": 0.003234615195382533, "phrase": "significantly_lower_error_rates"}, {"score": 0.003196960816092101, "phrase": "conventional_recogniser"}, {"score": 0.0031107911390188055, "phrase": "human_performance"}, {"score": 0.003015143226706584, "phrase": "energetic_and_informational_masking"}, {"score": 0.00277778944472107, "phrase": "large_number"}, {"score": 0.0026609940110902666, "phrase": "novel_fragment-based_speaker_identification_approach"}, {"score": 0.00253916011199984, "phrase": "wide_range"}, {"score": 0.0024134449026937586, "phrase": "significant_improvements"}, {"score": 0.002357535807803925, "phrase": "masker_utterance"}, {"score": 0.002171836484200008, "phrase": "error_rate"}, {"score": 0.002154930852763048, "phrase": "roughly_twice_the_human_error_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Speech recognition", " Speech separation", " Speaker identification", " Simultaneous speech", " Auditory scene analysis", " Noise robustness"], "paper_abstract": "This paper addresses the problem of recognising speech in the presence of a competing speaker. We review a speech fragment decoding technique that treats segregation and recognition as coupled problems. Data-driven techniques are used to segment a spectro-temporal representation into a set of fragments, such that each fragment is dominated by one or other of the speech sources. A speech fragment decoder is used which employs missing data techniques and clean speech models to simultaneously search for the set of fragments and the word sequence that best matches the target speaker model. The paper investigates the performance of the system oil a recognition task employing artificially mixed target and masker speech utterances. The fragment decoder produces significantly lower error rates than a conventional recogniser, and mimics the pattern of human performance that is produced by the interplay between energetic and informational masking. However, at around 0 dB the performance is generally quite poor. An analysis of the errors shows that a large number of target/masker confusions are being made. The paper presents a novel fragment-based speaker identification approach that allows the target speaker to be reliably identified across a wide range of SNRs. This component is combined with the recognition system to produce significant improvements. When the target and masker utterance have the same gender, the recognition system has a performance at 0 dB equal to that of humans; in other conditions the error rate is roughly twice the human error rate. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Speech fragment decoding techniques for simultaneous speaker identification and speech recognition", "paper_id": "WOS:000270630700007"}