{"auto_keywords": [{"score": 0.037959032075944976, "phrase": "first_algorithm"}, {"score": 0.015242307979733449, "phrase": "sparse_matrices"}, {"score": 0.00481495049065317, "phrase": "large_sparse_matrices"}, {"score": 0.004488734023485018, "phrase": "parallel_system"}, {"score": 0.0043509032299218955, "phrase": "significant_impact"}, {"score": 0.004184526032558792, "phrase": "sparse-matrix_algorithms"}, {"score": 0.0034971932704828197, "phrase": "particular_matrix-processors_mapping"}, {"score": 0.003389704997952315, "phrase": "second_one"}, {"score": 0.0030625949549620475, "phrase": "contiguous_chunks"}, {"score": 0.002681859084438076, "phrase": "collective_communication"}, {"score": 0.00253916011199984, "phrase": "second_algorithm"}, {"score": 0.0021049977753042253, "phrase": "running_time"}], "paper_keywords": ["Downsampling", " Sparse matrix", " Matrix-processors mapping", " Distributed algorithm", " Parallel computing", " MPI", " Visualization"], "paper_abstract": "Mapping of sparse matrices to processors of a parallel system may have a significant impact on the development of sparse-matrix algorithms and, in effect, to their efficiency. We present and empirically compare two downsampling algorithms for sparse matrices. The first algorithm is independent of particular matrix-processors mapping, while the second one is adapted for cases where matrices are partitioned among processors according to contiguous chunks of rows/columns. We show that the price for the versatility of the first algorithm is the collective communication performed by all processors. The second algorithm uses more efficient communication strategy, which stems from the knowledge of mapping of matrices to processors, and effectively outperforms the first algorithm in terms of running time.", "paper_title": "Downsampling Algorithms for Large Sparse Matrices", "paper_id": "WOS:000358648600001"}