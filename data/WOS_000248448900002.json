{"auto_keywords": [{"score": 0.031085154980921955, "phrase": "ls-svm"}, {"score": 0.022960514128145423, "phrase": "matrix_patterns"}, {"score": 0.02012433650739484, "phrase": "svm"}, {"score": 0.007955022824979766, "phrase": "vector_pattern"}, {"score": 0.007484145929324444, "phrase": "weight_vector"}, {"score": 0.005671966049534853, "phrase": "unclassifiable_regions"}, {"score": 0.00481495049065317, "phrase": "new_least_squares"}, {"score": 0.004792338011545063, "phrase": "vector_machines"}, {"score": 0.0047362681211047, "phrase": "support_vector_machine"}, {"score": 0.004658865501597339, "phrase": "effective_method"}, {"score": 0.004636982629843122, "phrase": "classification_problems"}, {"score": 0.0045719459699563895, "phrase": "optimal_hyperplane"}, {"score": 0.0044237033788643715, "phrase": "constrained_optimization_criterion"}, {"score": 0.004402920186633492, "phrase": "quadratic_programming"}, {"score": 0.004310592339589697, "phrase": "higher_computational_cost"}, {"score": 0.004290338269158808, "phrase": "least_squares"}, {"score": 0.004270178957872099, "phrase": "vector_machine"}, {"score": 0.004064117897000019, "phrase": "analytical_solution"}, {"score": 0.003997655793143628, "phrase": "linear_equations"}, {"score": 0.0037073529462522403, "phrase": "non-vector_pattern"}, {"score": 0.0035200497669064267, "phrase": "implicit_structural_or_local_contextual_information"}, {"score": 0.003318626931059049, "phrase": "linear_kernel"}, {"score": 0.0032184697893776052, "phrase": "original_input_pattern"}, {"score": 0.003165792939515048, "phrase": "higher_the_dimension"}, {"score": 0.002998688867384969, "phrase": "feature_extraction"}, {"score": 0.0029013095080254944, "phrase": "new_classifier_design_method"}, {"score": 0.002827014493358143, "phrase": "new_method"}, {"score": 0.0027807267376041556, "phrase": "original_matrix_patterns"}, {"score": 0.0025907604549358673, "phrase": "matlssvm"}, {"score": 0.0025785663873507747, "phrase": "ls-svm's_existence"}, {"score": 0.002542326597008011, "phrase": "multiclass_problems"}, {"score": 0.0025125151618810523, "phrase": "fuzzy_version"}, {"score": 0.002391067964886602, "phrase": "multi-class_problems"}, {"score": 0.002357457094026928, "phrase": "benchmark_datasets"}, {"score": 0.0023353121226049062, "phrase": "proposed_method"}, {"score": 0.002313374689466966, "phrase": "classification_performance"}, {"score": 0.0021199543519570376, "phrase": "novel_way"}, {"score": 0.0021049977753042253, "phrase": "learning_model"}], "paper_keywords": ["support vector machine (SVM)", " least squares support vector machine (LS-SVM)", " fuzzy least squares support vector machine (FLS-SVM)", " vector pattern", " matrix pattern", " pattern recognition"], "paper_abstract": "Support vector machine (SVM), as an effective method in classification problems, tries to find the optimal hyperplane that maximizes the margin between two classes and can be obtained by solving a constrained optimization criterion using quadratic programming (QP). This QP leads to higher computational cost. Least squares support vector machine (LS-SVM), as a variant of SVM, tries to avoid the above shortcoming and obtain an analytical solution directly from solving a set of linear equations instead of QP. Both SVM and LS-SVM operate directly on patterns represented by vector, i.e., before applying SVM or LS-SVM to a pattern, any non-vector pattern such as an image has to be first vectorized into a vector pattern by some techniques like concatenation. However, some implicit structural or local contextual information may be lost in this transformation. Moreover, as the dimension d of the weight vector in SVM or LS-SVM with the linear kernel is equal to the dimension d(1) x d(2) of the original input pattern, as a result, the higher the dimension of a vector pattern is, the more space is needed for storing it. In this paper, inspired by the method of feature extraction directly based on matrix patterns and the advantages of LS-SVM, we propose a new classifier design method based on matrix patterns, called MatLSSVM, such that the new method can not only directly operate on original matrix patterns, but also efficiently reduce memory for the weight vector (d) from d(1) x d(2) to d(1) + d(2). However like LS-SVM, MatLSSVM inherits LS-SVM's existence of unclassifiable regions when extended to multiclass problems. Thus with the fuzzy version of LS-SVM, a corresponding fuzzy version of MatLSSVM (MatFLSSVM) is further proposed to remove unclassifiable regions effectively for multi-class problems. Experimental results on some benchmark datasets show that the proposed method is competitive in classification performance compared to LS-SVM, fuzzy LS-SVM (FLS-SVM), more-recent MatPCA and MatFLDA. In addition, more importantly, the idea used here has a possibility of providing a novel way of constructing learning model.", "paper_title": "New least squares support vector machines based on matrix patterns", "paper_id": "WOS:000248448900002"}