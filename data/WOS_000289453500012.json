{"auto_keywords": [{"score": 0.03689565391010681, "phrase": "montreal"}, {"score": 0.009785247382812563, "phrase": "helsinki"}, {"score": 0.00481495049065317, "phrase": "weighted_k-nearest_neighbors"}, {"score": 0.004452641437293878, "phrase": "robust_high_performance"}, {"score": 0.004198839649063847, "phrase": "temporal-difference_learning"}, {"score": 0.0041174821422410544, "phrase": "nearest_neighbors"}, {"score": 0.004077394032903726, "phrase": "linear_function_approximation"}, {"score": 0.0032237268289353983, "phrase": "second_place"}, {"score": 0.0029086869572567072, "phrase": "art_general_purpose_reinforcement"}, {"score": 0.002689430273401131, "phrase": "continuous_state_spaces"}, {"score": 0.0025860636370864084, "phrase": "high_degree"}, {"score": 0.0025608471909800076, "phrase": "environmental_noise"}, {"score": 0.002344655708000408, "phrase": "continuous_actions"}, {"score": 0.002321787726770895, "phrase": "clear_advantages"}, {"score": 0.002265586601922693, "phrase": "fine_grained_discrete_actions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Reinforcement learning", " k-Nearest-neighbors", " Function approximation", " Temporal-difference learning"], "paper_abstract": "The aim of this paper is to present (jointly) a series of robust high performance (award winning) implementations of reinforcement learning algorithms based on temporal-difference learning and weighted k- nearest neighbors for linear function approximation. These algorithms, named kNN-TD(lambda) methods, where rigorously tested at the Second and Third Annual Reinforcement Learning Competitions (RLC2008 and RCL2009) held in Helsinki and Montreal respectively, where the kNN-TD(A) method (JAMH team) won in the PolyAthlon 2008 domain, obtained the second place in 2009 and also the second place in the Mountain-Car 2008 domain showing that it is one of the state of the art general purpose reinforcement learning implementations. These algorithms are able to learn quickly, to generalize properly over continuous state spaces and also to be robust to a high degree of environmental noise. Furthermore, we describe a derivation of kNN-TD(A) algorithm for problems where the use of continuous actions have clear advantages over the use of fine grained discrete actions: the Ex < a > reinforcement learning algorithm. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Robust high performance reinforcement learning through weighted k-nearest neighbors", "paper_id": "WOS:000289453500012"}