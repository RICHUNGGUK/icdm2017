{"auto_keywords": [{"score": 0.03275741841320617, "phrase": "pvm"}, {"score": 0.011109833765890332, "phrase": "elm"}, {"score": 0.007740485161750021, "phrase": "random_weight_idea"}, {"score": 0.00647323234334741, "phrase": "input_weights"}, {"score": 0.0062608617899581635, "phrase": "small-sample_dataset"}, {"score": 0.00478579024367007, "phrase": "neural_network"}, {"score": 0.004727995968069753, "phrase": "long_time"}, {"score": 0.004685106586595599, "phrase": "wide_attention"}, {"score": 0.0046004861111734185, "phrase": "tuning-based_methods"}, {"score": 0.004544919390397046, "phrase": "extreme_learning_machine"}, {"score": 0.004449280529563265, "phrase": "single_hidden_layer_neural_network"}, {"score": 0.004161553532799662, "phrase": "traditional_tuning-based_methods"}, {"score": 0.004123781267600542, "phrase": "better_generalization_performance"}, {"score": 0.003857022173615399, "phrase": "dimension_reduction"}, {"score": 0.0037529131348732715, "phrase": "two-stage_method"}, {"score": 0.003685067218455088, "phrase": "structure_complexity"}, {"score": 0.0035530197642369464, "phrase": "novel_single-stage_algorithm"}, {"score": 0.003353521531649663, "phrase": "neural_network_training"}, {"score": 0.0033230588808076267, "phrase": "bpvm"}, {"score": 0.0033029038236817372, "phrase": "kpvm"}, {"score": 0.003194209418929854, "phrase": "joint_name"}, {"score": 0.003051713660488494, "phrase": "slfn"}, {"score": 0.0030147970456037274, "phrase": "singular_decomposition"}, {"score": 0.0029783256772952073, "phrase": "hidden_neurons"}, {"score": 0.002802462902794585, "phrase": "better_generalization_ability"}, {"score": 0.0027685531038981847, "phrase": "elm._similar"}, {"score": 0.0026937446571204653, "phrase": "iterative_steps"}, {"score": 0.002644995764097807, "phrase": "significantly_fast_training"}, {"score": 0.0025892330566358503, "phrase": "two-stage_algorithms"}, {"score": 0.0024437176947749014, "phrase": "running_space"}, {"score": 0.002421499421848063, "phrase": "experimental_results"}, {"score": 0.0022854202181354076, "phrase": "bp"}, {"score": 0.0022303885749306645, "phrase": "best_generalization_ability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural network", " Extreme learning machine", " Projection vector machine"], "paper_abstract": "The random weight idea in neural network has existed for long time and attracted wide attention for its faster learning speed than that of tuning-based methods. Recently, extreme learning machine (ELM), one non-tuning algorithm for single hidden layer neural network (SLFN) was proposed and applied in many applications. ELM can learn hundreds of times faster than that of the traditional tuning-based methods while obtaining better generalization performance. But since the ELM randomly assigns input weights, its stability and accuracy will decline sharply for high-dimension and small-sample dataset. Although dimension reduction can be used to improve it, this two-stage method will need more physical space and increase structure complexity of the model. In this paper, we propose a novel single-stage algorithm, base projection vector machine (BPVM) and its kernel-version kernel projection vector machine (KPVM) by combining dimension reduction and neural network training. The BPVM and KPVM are called projection vector machine (PVM) by a joint name. Different from random weight idea used in ELM, in PVM, the input weights of SLFN are obtained by singular decomposition, and the hidden neurons are ranked by their contribution level and only those that are important are selected. This will make PVM to obtain better generalization ability and more compact structure than ELM. Similar to ELM, PVM does not need any iterative steps, and thus can yield significantly fast training. Additionally, compared with the two-stage algorithms such as BP/SVD (BP co-work with singular value decomposition) and ELM/SVD (ELM co-work with singular value decomposition), PVM need less parameters and memory running space. The experimental results on many datasets show that; although PVM is not faster than ELM, it is much faster than BP, BP/SVD and ELM/SVD which produce the best generalization ability in most cases. Especially, PVM is very suitable for high-dimension and small-sample dataset. (c) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Projection vector machine", "paper_id": "WOS:000324847100052"}