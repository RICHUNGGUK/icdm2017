{"auto_keywords": [{"score": 0.0364826355522259, "phrase": "decision_trees"}, {"score": 0.00481495049065317, "phrase": "data_streams"}, {"score": 0.00476323161560059, "phrase": "classical_classification_methods"}, {"score": 0.004661446705304497, "phrase": "pattern_recognition_models"}, {"score": 0.004161553532799662, "phrase": "new_data"}, {"score": 0.00385832528624328, "phrase": "spam_filtering"}, {"score": 0.0038168424095396205, "phrase": "fraud_detection"}, {"score": 0.0037150687722055727, "phrase": "feature_values"}, {"score": 0.003675120438902734, "phrase": "class_numbers"}, {"score": 0.0031417377608722, "phrase": "so-called_concept_drift"}, {"score": 0.0030086877092638945, "phrase": "new_classification_model"}, {"score": 0.002865710140187219, "phrase": "concept_drift"}, {"score": 0.0027295084882654917, "phrase": "data_mining_methods"}, {"score": 0.002685558057339722, "phrase": "complex_structures"}, {"score": 0.0024231382864800173, "phrase": "-train_decision_trees"}, {"score": 0.0022830706430588482, "phrase": "proposed_algorithm"}, {"score": 0.0021863046959469863, "phrase": "computer_experiments"}, {"score": 0.002127915423138391, "phrase": "benchmark_datasets"}], "paper_keywords": ["Nested generalized exemplar", " Nearest hyperrectangle", " Concept drift", " Decision tree", " Incremental learning", " Pattern recognition"], "paper_abstract": "Classical classification methods usually assume that pattern recognition models do not depend on the timing of the data. However, this assumption is not valid in cases where new data frequently become available. Such situations are common in practice, for example, spam filtering or fraud detection, where dependencies between feature values and class numbers are continually changing. Unfortunately, most classical machine learning methods (such as decision trees) do not take into consideration the possibility of the model changing, as a result of so-called concept drift and they cannot adapt to a new classification model. This paper focuses on the problem of concept drift, which is a very important issue, especially in data mining methods that use complex structures (such as decision trees) for making decisions. We propose an algorithm that is able to co-train decision trees using a modified NGE (Nested Generalized Exemplar) algorithm. The potential for adaptation of the proposed algorithm and the quality thereof are evaluated through computer experiments, carried out on benchmark datasets from the UCI Machine Learning Repository.", "paper_title": "A hybrid decision tree training method using data streams", "paper_id": "WOS:000295786000004"}