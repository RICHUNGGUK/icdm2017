{"auto_keywords": [{"score": 0.044978063067936135, "phrase": "hidden_neurons"}, {"score": 0.03358809053029017, "phrase": "generalization_capability"}, {"score": 0.013513095308092209, "phrase": "elm-like_systems"}, {"score": 0.00964197261904541, "phrase": "elm-like_system"}, {"score": 0.00481495049065317, "phrase": "extreme_learning_machine_feasible"}, {"score": 0.0046526204980467676, "phrase": "extreme_learning_machine"}, {"score": 0.004566384853664753, "phrase": "feedforward_neural_network"}, {"score": 0.00453799737618942, "phrase": "fnn"}, {"score": 0.004440005714775306, "phrase": "output_neurons"}, {"score": 0.004237065611401975, "phrase": "numerous_applications"}, {"score": 0.004158500036893521, "phrase": "high_efficiency"}, {"score": 0.003943688519789829, "phrase": "general_applications"}, {"score": 0.003894772585498127, "phrase": "two-part_paper"}, {"score": 0.003834476721674575, "phrase": "comprehensive_feasibility_analysis"}, {"score": 0.0038106212542376454, "phrase": "elm."}, {"score": 0.003557752688779838, "phrase": "suitable_activation_functions"}, {"score": 0.0034917393298510685, "phrase": "nadaraya-watson"}, {"score": 0.0033528672260542854, "phrase": "theoretical_generalization"}, {"score": 0.0029042284259394044, "phrase": "theoretical_bound"}, {"score": 0.002814913791165247, "phrase": "activation_function"}, {"score": 0.0027540283945376594, "phrase": "deduced_hidden_layer_output_matrix"}, {"score": 0.0027283383611597495, "phrase": "full_column-rank"}, {"score": 0.0026944563649138713, "phrase": "generalized_inverse_technique"}, {"score": 0.00253916011199984, "phrase": "nonpolynomial_case"}, {"score": 0.0025154693498317757, "phrase": "tikhonov_regularization"}, {"score": 0.002461044825190643, "phrase": "weak_regularity"}, {"score": 0.0023410164207985297, "phrase": "different_aspect"}, {"score": 0.00226190243205002, "phrase": "activation_functions"}, {"score": 0.002178635733085195, "phrase": "obtained_results"}], "paper_keywords": ["Extreme learning machine (ELM)", " feasibility", " generalization capability", " neural networks"], "paper_abstract": "An extreme learning machine (ELM) is a feedforward neural network (FNN) like learning system whose connections with output neurons are adjustable, while the connections with and within hidden neurons are randomly fixed. Numerous applications have demonstrated the feasibility and high efficiency of ELM-like systems. It has, however, been open if this is true for any general applications. In this two-part paper, we conduct a comprehensive feasibility analysis of ELM. In Part I, we provide an answer to the question by theoretically justifying the following: 1) for some suitable activation functions, such as polynomials, Nadaraya-Watson and sigmoid functions, the ELM-like systems can attain the theoretical generalization bound of the FNNs with all connections adjusted, i.e., they do not degrade the generalization capability of the FNNs even when the connections with and within hidden neurons are randomly fixed; 2) the number of hidden neurons needed for an ELM-like system to achieve the theoretical bound can be estimated; and 3) whenever the activation function is taken as polynomial, the deduced hidden layer output matrix is of full column-rank, therefore the generalized inverse technique can be efficiently applied to yield the solution of an ELM-like system, and, furthermore, for the nonpolynomial case, the Tikhonov regularization can be applied to guarantee the weak regularity while not sacrificing the generalization capability. In Part II, however, we reveal a different aspect of the feasibility of ELM: there also exists some activation functions, which makes the corresponding ELM degrade the generalization capability. The obtained results underlie the feasibility and efficiency of ELM-like systems, and yield various generalizations and improvements of the systems as well.", "paper_title": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part I)", "paper_id": "WOS:000348854800002"}