{"auto_keywords": [{"score": 0.04467661766774399, "phrase": "test-time_cpu_cost"}, {"score": 0.03698933881555943, "phrase": "test-time_cost"}, {"score": 0.00481495049065317, "phrase": "classifier_cascades"}, {"score": 0.004733625412289433, "phrase": "minimizing_feature_evaluation_cost"}, {"score": 0.004693476992528682, "phrase": "machine_learning_algorithms"}, {"score": 0.0036657110980445416, "phrase": "classifier_accuracy"}, {"score": 0.003557924423642353, "phrase": "feature_extraction"}, {"score": 0.003038532168038562, "phrase": "test_inputs"}, {"score": 0.0029998918671747168, "phrase": "individual_paths"}, {"score": 0.002936577024096783, "phrase": "different_features"}, {"score": 0.0028623555849897632, "phrase": "specific_sub-partition"}, {"score": 0.002825949329865926, "phrase": "input_space"}, {"score": 0.0027662954811372175, "phrase": "natural_reduction"}, {"score": 0.002583713580625699, "phrase": "class-imbalanced_data_sets"}, {"score": 0.002159623311924437, "phrase": "high_accuracies"}, {"score": 0.002132135985573126, "phrase": "small_fraction"}, {"score": 0.0021049977753042253, "phrase": "computational_cost"}], "paper_keywords": ["budgeted learning", " resource efficient machine learning", " feature cost sensitive learning", " web-search ranking", " tree of classifiers"], "paper_abstract": "Machine learning algorithms have successfully entered industry through many real-world applications (e.g., search engines and product recommendations). In these applications, the test-time CPU cost must be budgeted and accounted for. In this paper, we examine two main components of the test-time CPU cost, classifier evaluation cost and feature extraction cost, and show how to balance these costs with the classifier accuracy. Since the computation required for feature extraction dominates the test-time cost of a classifier in these settings, we develop two algorithms to efficiently balance the performance with the test-time cost. Our first contribution describes how to construct and optimize a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. Our second contribution is a natural reduction of the tree of classifiers into a cascade. The cascade is particularly useful for class-imbalanced data sets as the majority of instances can be early-exited out of the cascade when the algorithm is sufficiently confident in its prediction. Because both approaches only compute features for inputs that benefit from them the most, we find our trained classifiers lead to high accuracies at a small fraction of the computational cost.", "paper_title": "Classifier Cascades and Trees for Minimizing Feature Evaluation Cost", "paper_id": "WOS:000344638300008"}