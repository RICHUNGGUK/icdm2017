{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "active_learning"}, {"score": 0.004653120760786245, "phrase": "potential_advantages"}, {"score": 0.004600389726895716, "phrase": "theoretical_challenges"}, {"score": 0.004548253531664032, "phrase": "\"active_learning"}, {"score": 0.0043953478557600565, "phrase": "sequential_sampling_procedures"}, {"score": 0.004247560712327667, "phrase": "previous_samples"}, {"score": 0.00401216440036853, "phrase": "learning_process"}, {"score": 0.0039216854691927865, "phrase": "passive_learning\"_algorithms"}, {"score": 0.003559284776569813, "phrase": "empirical_and_theoretical_results"}, {"score": 0.00332375606483146, "phrase": "passive_learning"}, {"score": 0.003193606324727067, "phrase": "active_learning_algorithms"}, {"score": 0.0031573626285069157, "phrase": "feedback_systems"}, {"score": 0.002931569771634119, "phrase": "achievable_limits"}, {"score": 0.0028490783932465288, "phrase": "minimax_analysis_techniques"}, {"score": 0.0027689017918347755, "phrase": "achievable_rates"}, {"score": 0.0027374647587075935, "phrase": "classification_error_convergence"}, {"score": 0.002706383680762465, "phrase": "broad_classes"}, {"score": 0.0026152363154073707, "phrase": "boundary_regularity"}, {"score": 0.002585539416877115, "phrase": "noise_conditions"}, {"score": 0.0024005409468262203, "phrase": "significant_gains"}, {"score": 0.0022672718785099666, "phrase": "learning_rates"}, {"score": 0.0021908807203548345, "phrase": "boundary_fragment\"_classes"}, {"score": 0.002153653728048424, "phrase": "-dimensional_feature_spaces"}, {"score": 0.0021049977753042253, "phrase": "marginal_density"}], "paper_keywords": ["active learning", " adaptive sampling", " minimax lower bounds", " statistical learning theory"], "paper_abstract": "This paper analyzes the potential advantages and theoretical challenges of \"active learning\" algorithms. Active learning involves sequential sampling procedures that use information gleaned from previous samples in order to focus the sampling and accelerate the learning process relative to \"passive learning\" algorithms, which are based on nonadaptive (usually random) samples. There are a number of empirical and theoretical results suggesting that in certain situations active learning can be significantly more effective than passive learning. However, the fact that active learning algorithms are feedback systems makes their theoretical analysis very challenging. This paper aims to shed light on achievable limits in active learning. Using minimax analysis techniques, we study the achievable rates of classification error convergence for broad classes of distributions characterized by decision boundary regularity and noise conditions. The results clearly indicate the conditions under which one can expect significant gains through active learning. Furthermore, we show that the learning rates derived are tight for \"boundary fragment\" classes in d-dimensional feature spaces when the feature marginal density is bounded from above and below.", "paper_title": "Minimax bounds for active learning", "paper_id": "WOS:000255505700028"}