{"auto_keywords": [{"score": 0.043824399406508514, "phrase": "multiple_videos"}, {"score": 0.00481495049065317, "phrase": "ever-increasing_volumes"}, {"score": 0.0047006879240764935, "phrase": "automatic_extraction"}, {"score": 0.004655742198075387, "phrase": "salient_object_regions"}, {"score": 0.004523455475492876, "phrase": "visual_analytic_solutions"}, {"score": 0.0042495320879288615, "phrase": "collective_cues"}, {"score": 0.0041287394089001405, "phrase": "cooperative_manner"}, {"score": 0.0039729853486821995, "phrase": "major_challenges"}, {"score": 0.0038600230206554792, "phrase": "drastic_appearance"}, {"score": 0.0038230844545071303, "phrase": "motion_pattern"}, {"score": 0.003696550794743764, "phrase": "foreground_objects"}, {"score": 0.0036261282565554507, "phrase": "indiscriminate_backgrounds"}, {"score": 0.0035060903664150115, "phrase": "cosegmentation_framework"}, {"score": 0.0034063576796640603, "phrase": "common_object_regions"}, {"score": 0.0033737457695128233, "phrase": "multiple_frames"}, {"score": 0.003293570937654716, "phrase": "joint_fashion"}, {"score": 0.002948547333717581, "phrase": "energy_optimization_framework"}, {"score": 0.0028784478769295204, "phrase": "restrictive_assumptions"}, {"score": 0.002850875630787708, "phrase": "foreground_appearance"}, {"score": 0.002455712299904807, "phrase": "sift"}, {"score": 0.0024088451312725924, "phrase": "interframe_motion_flow"}, {"score": 0.0023857603808363527, "phrase": "optical_flow"}, {"score": 0.0023515464194592195, "phrase": "novel_spatio-temporal_sift_flow"}, {"score": 0.0022956075241520064, "phrase": "common_foregrounds"}, {"score": 0.0022626835039082746, "phrase": "entire_video_data"}, {"score": 0.0022302306304023602, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "new_extensive_data"}], "paper_keywords": ["Video object co-segmentation", " energy optimization", " object refinement", " spatio-temporal scale-invariant feature transform (SIFT) flow"], "paper_abstract": "With ever-increasing volumes of video data, automatic extraction of salient object regions became even more significant for visual analytic solutions. This surge has also opened up opportunities for taking advantage of collective cues encapsulated in multiple videos in a cooperative manner. However, it also brings up major challenges, such as handling of drastic appearance, motion pattern, and pose variations, of foreground objects as well as indiscriminate backgrounds. Here, we present a cosegmentation framework to discover and segment out common object regions across multiple frames and multiple videos in a joint fashion. We incorporate three types of cues, i.e., intraframe saliency, interframe consistency, and across-video similarity into an energy optimization framework that does not make restrictive assumptions on foreground appearance and motion model, and does not require objects to be visible in all frames. We also introduce a spatio-temporal scale-invariant feature transform (SIFT) flow descriptor to integrate across-video correspondence from the conventional SIFT-flow into interframe motion flow from optical flow. This novel spatio-temporal SIFT flow generates reliable estimations of common foregrounds over the entire video data set. Experimental results show that our method outperforms the state-of-the-art on a new extensive data set (ViCoSeg).", "paper_title": "Robust Video Object Cosegmentation", "paper_id": "WOS:000356539800003"}