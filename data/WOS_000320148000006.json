{"auto_keywords": [{"score": 0.03809452887646078, "phrase": "gpu"}, {"score": 0.00481495049065317, "phrase": "smoothed_particle_hydrodynamics"}, {"score": 0.004761494044422926, "phrase": "heterogeneous_clusters"}, {"score": 0.004682414867878963, "phrase": "massively_parallel_sph_scheme"}, {"score": 0.004553510708976456, "phrase": "central_processing_units"}, {"score": 0.004428139418750855, "phrase": "graphics_processing_units"}, {"score": 0.004211069233133584, "phrase": "new_implementation"}, {"score": 0.004118026552723285, "phrase": "single-gpu_dualsphysics_code"}, {"score": 0.003808209866576388, "phrase": "different_parallel_programming_languages"}, {"score": 0.0034824716857663114, "phrase": "different_machines"}, {"score": 0.003348838095597268, "phrase": "improved_message"}, {"score": 0.0031315358193567708, "phrase": "well-known_drawbacks"}, {"score": 0.0030113280682863234, "phrase": "dynamic_load_balancing"}, {"score": 0.002977835845437357, "phrase": "overlapping_data_communications"}, {"score": 0.0029447150254434842, "phrase": "computation_tasks"}, {"score": 0.002707741309851526, "phrase": "new_dualsphysics_code"}, {"score": 0.0026478260192958924, "phrase": "different_numbers"}, {"score": 0.0025892330566358503, "phrase": "different_number"}, {"score": 0.0022013916728990564, "phrase": "large_cpu_clusters"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["HPC", " GPU", " Multi-GPU", " MPI", " CUDA", " SPH", " Meshfree methods"], "paper_abstract": "A massively parallel SPH scheme using heterogeneous clusters of Central Processing Units (CPUs) and Graphics Processing Units (GPUs) has been developed. The new implementation originates from the single-GPU DualSPHysics code previously demonstrated to be powerful, stable and accurate. A combination of different parallel programming languages is combined to exploit not only one device (CPU or GPU) but also the combination of different machines. Communication among devices uses an improved Message Passing Interface (MPI) implementation which addresses some of the well-known drawbacks of MPI such as including a dynamic load balancing and overlapping data communications and computation tasks. The efficiency and scalability (strong and weak scaling) obtained with the new DualSPHysics code are analysed for different numbers of particles and different number of GPUs. Last, an application with more than 10(9) particles is presented to show the capability of the code to handle simulations that otherwise require large CPU clusters or supercomputers. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "New multi-GPU implementation for smoothed particle hydrodynamics on heterogeneous clusters", "paper_id": "WOS:000320148000006"}