{"auto_keywords": [{"score": 0.03507995906078705, "phrase": "single_device"}, {"score": 0.00481495049065317, "phrase": "multi-gpu_smoothed_particle_hydrodynamics_simulations"}, {"score": 0.0047006879240764935, "phrase": "multi-gpu_version"}, {"score": 0.004655742198075387, "phrase": "gpusph"}, {"score": 0.004523455475492876, "phrase": "fluid-dynamics_models"}, {"score": 0.004437349839203418, "phrase": "smoothed_particle_hydrodynamics"}, {"score": 0.0042703645486912795, "phrase": "sph"}, {"score": 0.004208881856146758, "phrase": "well-known_lagrangian_model"}, {"score": 0.003934970057482768, "phrase": "high_degree"}, {"score": 0.0036788184374592706, "phrase": "gpu-based_simulator"}, {"score": 0.0035914198812657897, "phrase": "multiple_gpus"}, {"score": 0.0033900126423990823, "phrase": "memory_limitations"}, {"score": 0.0032777653153451265, "phrase": "computational_domain"}, {"score": 0.003199864072042816, "phrase": "minimal_overlapping"}, {"score": 0.003169222811082848, "phrase": "shared_volume_slices"}, {"score": 0.0030203487665054806, "phrase": "data_transfers"}, {"score": 0.002837188425640173, "phrase": "slice_exchange"}, {"score": 0.0027965187255008647, "phrase": "simple_yet_effective_load_balancing_policy"}, {"score": 0.002703869368554536, "phrase": "unbalanced_simulations"}, {"score": 0.00266510562382701, "phrase": "asymmetric_fluid_topologies"}, {"score": 0.002467536552997677, "phrase": "expected_one"}, {"score": 0.0022956075241520064, "phrase": "higher_number"}, {"score": 0.0021563019696857768, "phrase": "karp-flatt_metric"}, {"score": 0.0021049977753042253, "phrase": "overall_efficiency"}], "paper_keywords": ["GPU", " multi-GPU", " SPH", " CUDA", " fluid dynamics", " numerical simulations", " load balancing", " parallel computing", " HPC"], "paper_abstract": "We present a multi-GPU version of GPUSPH, a CUDA implementation of fluid-dynamics models based on the smoothed particle hydrodynamics (SPH) numerical method. The SPH is a well-known Lagrangian model for the simulation of free-surface fluid flows; it exposes a high degree of parallelism and has already been successfully ported to GPU. We extend the GPU-based simulator to run simulations on multiple GPUs simultaneously, to obtain a gain in speed and overcome the memory limitations of using a single device. The computational domain is spatially split with minimal overlapping and shared volume slices are updated at every iteration of the simulation. Data transfers are asynchronous with computations, thus completely covering the overhead introduced by slice exchange. A simple yet effective load balancing policy preserves the performance in case of unbalanced simulations due to asymmetric fluid topologies. The obtained speedup factor (up to 4.5x for 6 GPUs) closely follows the expected one (5x for 6 GPUs) and it is possible to run simulations with a higher number of particles than would fit on a single device. We use the Karp-Flatt metric to formally estimate the overall efficiency of the parallelization.", "paper_title": "Advances in Multi-GPU Smoothed Particle Hydrodynamics Simulations", "paper_id": "WOS:000327662100006"}