{"auto_keywords": [{"score": 0.04806131990325831, "phrase": "common_practices"}, {"score": 0.015719716506582538, "phrase": "computer_intrusion_detection_systems"}, {"score": 0.013599931587419834, "phrase": "intrusion_detection_systems"}, {"score": 0.010525083442531393, "phrase": "design_space"}, {"score": 0.0040186038781533946, "phrase": "active_research_area"}, {"score": 0.002544951823639408, "phrase": "evaluation_approaches"}, {"score": 0.002236055625099805, "phrase": "open_issues"}, {"score": 0.0021416404045850224, "phrase": "evaluation_methodologies"}, {"score": 0.0021049977753042253, "phrase": "novel_intrusion_detection_systems"}], "paper_keywords": ["Computer intrusion detection systems", " workload generation", " metrics", " measurement methodology"], "paper_abstract": "The evaluation of computer intrusion detection systems (which we refer to as intrusion detection systems) is an active research area. In this article, we survey and systematize common practices in the area of evaluation of such systems. For this purpose, we define a design space structured into three parts: workload, metrics, and measurement methodology. We then provide an overview of the common practices in evaluation of intrusion detection systems by surveying evaluation approaches and methods related to each part of the design space. Finally, we discuss open issues and challenges focusing on evaluation methodologies for novel intrusion detection systems.", "paper_title": "Evaluating Computer Intrusion Detection Systems: A Survey of Common Practices", "paper_id": "WOS:000363733200012"}