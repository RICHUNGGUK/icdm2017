{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multimedia_indexing"}, {"score": 0.004444317282171449, "phrase": "content-based_multimedia_indexing"}, {"score": 0.004351595642056769, "phrase": "large_variety"}, {"score": 0.003932654396285872, "phrase": "large_scale_systems"}, {"score": 0.0037861265016041813, "phrase": "tens_of_thousands_dimensions"}, {"score": 0.0034504423179340738, "phrase": "proposed_method"}, {"score": 0.003407026489727146, "phrase": "pca-based_dimensionality_reduction"}, {"score": 0.003378385493203297, "phrase": "pre-_and_post-pca_non-linear_transformations"}, {"score": 0.0032387402145603412, "phrase": "produced_descriptors"}, {"score": 0.0030017207229983385, "phrase": "euclidean_distance"}, {"score": 0.0029639342245188203, "phrase": "original_high_dimensionality_descriptors"}, {"score": 0.0028654728453890426, "phrase": "hyper-parameter_optimization_procedure"}, {"score": 0.0027819985260847577, "phrase": "fast_knn_classifier"}, {"score": 0.002735392281320413, "phrase": "polynomial_fit"}, {"score": 0.002689564715486338, "phrase": "map_metric_instability"}, {"score": 0.0024405312482831646, "phrase": "large_scale"}, {"score": 0.0023395610923736595, "phrase": "initial_dimensionalities"}, {"score": 0.002195866991284673, "phrase": "multimedia_retrieval"}, {"score": 0.0021049977753042253, "phrase": "relevance_feedback"}], "paper_keywords": ["Multimedia indexing and retrieval", " Descriptor optimization", " Dimensionality reduction"], "paper_abstract": "In this paper, we propose and evaluate a method for optimizing descriptors used for content-based multimedia indexing and retrieval. A large variety of descriptors are commonly used for this purpose. However, the most efficient ones often have characteristics preventing them to be easily used in large scale systems. They may have very high dimensionality (up to tens of thousands dimensions) and/or be suited for a distance which is costly to compute (e.g. chi (2)). The proposed method combines a PCA-based dimensionality reduction with pre- and post-PCA non-linear transformations. The resulting transformation is globally optimized. The produced descriptors have a much lower dimensionality while performing at least as well, and often significantly better, with the Euclidean distance than the original high dimensionality descriptors with their optimal distance. Our approach also includes a hyper-parameter optimization procedure based on the use of a fast kNN classifier and on a polynomial fit to overcome the MAP metric instability. The method has been validated and evaluated on a variety of descriptors using the TRECVid 2010 semantic indexing task data. It has been applied at large scale for the TRECVid 2012 semantic indexing task on tens of descriptors of various types and with initial dimensionalities ranging from 15 up to 32,768. The same transformation can be used also for multimedia retrieval in the context of query by example and/or of relevance feedback.", "paper_title": "Descriptor optimization for multimedia indexing and retrieval", "paper_id": "WOS:000349356300007"}