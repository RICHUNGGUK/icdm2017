{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "reinforcement_learning"}, {"score": 0.00476697427418509, "phrase": "multisensor_fusion_problems"}, {"score": 0.004719473826242326, "phrase": "conflicting_control_objectives._smart_agents"}, {"score": 0.004291073117901643, "phrase": "multiple_raw_streams"}, {"score": 0.004248294537944227, "phrase": "sensory_data"}, {"score": 0.004184921610672245, "phrase": "appropriate_actions"}, {"score": 0.004101886428528311, "phrase": "easy_problem"}, {"score": 0.004020492138367616, "phrase": "multiple_conflicting_objectives"}, {"score": 0.0038819041309353024, "phrase": "multi-sensor_fusion_problem"}, {"score": 0.003785836059995376, "phrase": "power_management"}, {"score": 0.0037480753312398754, "phrase": "smart_mobile_devices"}, {"score": 0.0035827424231607784, "phrase": "application_domain"}, {"score": 0.003441891354624516, "phrase": "user's_mobile_devices"}, {"score": 0.0033905067962602515, "phrase": "\"on\"_state"}, {"score": 0.00327356326590461, "phrase": "just-in-time_services"}, {"score": 0.0030060256866508606, "phrase": "mobile_device's_inherent_goal"}, {"score": 0.002830410693265704, "phrase": "stochastic_nature"}, {"score": 0.0028021527740710508, "phrase": "human_behavior"}, {"score": 0.0027602925235474317, "phrase": "hand-coded_fixed_strategy"}, {"score": 0.0026919067120537055, "phrase": "best_solution"}, {"score": 0.0026516890043911836, "phrase": "learning_control_approach"}, {"score": 0.002509284173541882, "phrase": "experimental_results"}, {"score": 0.00242266141176955, "phrase": "appropriate_mapping"}, {"score": 0.002398464540102246, "phrase": "multiple_streams"}, {"score": 0.0023745087648471613, "phrase": "raw_sensory_data"}, {"score": 0.002246955931290095, "phrase": "hand-crafted_policies"}, {"score": 0.0022133711375320244, "phrase": "learned_policies"}, {"score": 0.0021049977753042253, "phrase": "unscheduled_events"}], "paper_keywords": ["Multi-sensor fusion", " conflicting objectives", " reinforcement learning", " adaptive power management"], "paper_abstract": "Smart agents are equipped with sensors that enable them to be sensitive to their surrounding environment. However, the mapping of multiple raw streams of sensory data to the appropriate actions is not an easy problem, especially when multiple conflicting objectives are involved. This type of multi-sensor fusion problem through the domain of power management for smart mobile devices was investigated in this study. In this application domain, the objective is to keep the user's mobile devices in an \"on\" state as long as possible such that just-in-time services (such as reminder announcements) can be provided. However, this conflicts with the mobile device's inherent goal - to turn off to conserve power. Due to the stochastic nature of human behavior, a hand-coded fixed strategy may not be the best solution. A learning control approach to the problem is presented in this paper. The experimental results show that the approach learns the appropriate mapping from multiple streams of raw sensory data to power conserving actions that can out-perform the hand-crafted policies. The learned policies are also shown to be more robust in handling unscheduled events.", "paper_title": "APPLICATION OF REINFORCEMENT LEARNING IN MULTISENSOR FUSION PROBLEMS WITH CONFLICTING CONTROL OBJECTIVES", "paper_id": "WOS:000264661000008"}