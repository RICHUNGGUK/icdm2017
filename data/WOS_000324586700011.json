{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mobile_robot"}, {"score": 0.004683678340162048, "phrase": "essential_requirement"}, {"score": 0.004654993966953098, "phrase": "safe_and_efficient_human-robot_interaction"}, {"score": 0.004431725879033857, "phrase": "real_time"}, {"score": 0.004404577716425371, "phrase": "multiple_humans"}, {"score": 0.004324122679561784, "phrase": "significant_challenge"}, {"score": 0.004284445575512898, "phrase": "recent_availability"}, {"score": 0.004258195773502909, "phrase": "commercial_color-depth_cameras"}, {"score": 0.004091432545219401, "phrase": "depth_dimension"}, {"score": 0.003765575242370161, "phrase": "moving_robot"}, {"score": 0.0037195507046380036, "phrase": "color-depth_camera"}, {"score": 0.0036854004736318197, "phrase": "consumer-grade_computer"}, {"score": 0.0036291762280629115, "phrase": "computation_time"}, {"score": 0.0035958527554878655, "phrase": "real-time_performance"}, {"score": 0.003562834168369313, "phrase": "unique_combination"}, {"score": 0.0035409897891004105, "phrase": "new_ideas"}, {"score": 0.0035192788686342668, "phrase": "established_techniques"}, {"score": 0.0034443299902626834, "phrase": "ceiling_planes"}, {"score": 0.0033813554890743665, "phrase": "candidate_point_clusters"}, {"score": 0.0033297542331358057, "phrase": "novel_information_concept"}, {"score": 0.003140727213993457, "phrase": "computationally_expensive_scanning-window_methods"}, {"score": 0.0029442253041602044, "phrase": "intelligent_reuse"}, {"score": 0.0029261625894377286, "phrase": "intermediary_features"}, {"score": 0.002908210365588114, "phrase": "successive_detectors"}, {"score": 0.002837493000816031, "phrase": "high_computational_cost"}, {"score": 0.002743048133491456, "phrase": "directed_acyclic_graph"}, {"score": 0.0026112423766508543, "phrase": "successful_implementation"}, {"score": 0.002516550704970801, "phrase": "real-world_challenges"}, {"score": 0.0024781140074694205, "phrase": "robot_motion"}, {"score": 0.002462903804873404, "phrase": "nonupright_humans"}, {"score": 0.002315831335278694, "phrase": "human-human_interaction"}, {"score": 0.002231828204535298, "phrase": "depth_information"}, {"score": 0.0021842376228111537, "phrase": "modern_techniques"}, {"score": 0.00217082725339744, "phrase": "new_ways"}, {"score": 0.0021180023437575167, "phrase": "accurate_system"}], "paper_keywords": ["3-D vision", " depth of interest", " human detection and tracking", " human perception", " RGB-D camera application"], "paper_abstract": "The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.", "paper_title": "Real-Time Multiple Human Perception with Color-Depth Cameras on a Mobile Robot", "paper_id": "WOS:000324586700011"}