{"auto_keywords": [{"score": 0.04547624598453916, "phrase": "em_algorithm"}, {"score": 0.03241061602837417, "phrase": "levenberg-marquardt_algorithm"}, {"score": 0.015287098790922089, "phrase": "hidden_markov_models"}, {"score": 0.00481495049065317, "phrase": "hidden_markov_model"}, {"score": 0.004643452155467127, "phrase": "baum"}, {"score": 0.004406360418456518, "phrase": "maximum_likelihood"}, {"score": 0.004147879374937003, "phrase": "hessian"}, {"score": 0.0040978781692373005, "phrase": "log_likelihood"}, {"score": 0.004064958518350867, "phrase": "hidden_markov_and_related_models"}, {"score": 0.003935892934907777, "phrase": "filtering_process"}, {"score": 0.003229815147800001, "phrase": "straightforward_implementation"}, {"score": 0.0032038465090068646, "phrase": "newton's_method"}, {"score": 0.00301560261110296, "phrase": "excellent_reliability"}, {"score": 0.0027041233221381756, "phrase": "second_example"}, {"score": 0.0025658094607383646, "phrase": "maximum_likelihood_estimator"}, {"score": 0.002405230258482895, "phrase": "levenberg-marquardt"}, {"score": 0.002376256687615971, "phrase": "em."}, {"score": 0.002282166507956957, "phrase": "first_example"}, {"score": 0.00224558083320154, "phrase": "generic_numerical_maximization_procedure"}, {"score": 0.0021392979397427984, "phrase": "generic_procedure"}, {"score": 0.0021049977753042253, "phrase": "analytic_derivatives"}], "paper_keywords": [""], "paper_abstract": "Ever since the introduction of hidden Markov models by Baum and his co-workers, the method of choice for fitting such models has been maximum likelihood via the EM algorithm. In recent years it has been noticed that the gradient and Hessian of the log likelihood of hidden Markov and related models may be calculated in parallel with a filtering process by which the likelihood may be calculated. Various authors have used, or suggested the use of, this idea in order to maximize the likelihood directly, without using the EM algorithm. In this paper we discuss an implementation of such an approach. We have found that a straightforward implementation of Newton's method sometimes works but is unreliable. A form of the Levenberg-Marquardt algorithm appears to provide excellent reliability. Two rather complex examples are given for applying this algorithm to the fitting of hidden Markov models. In the first a better than 6-fold increase in speed over the EM algorithm was achieved. The second example turned out to be problematic (somewhat interestingly) in that the maximum likelihood estimator appears to be inconsistent. Whatever its merit, this estimator is calculated much faster by Levenberg-Marquardt than by EM. We also compared the Levenberg-Marquardt algorithm, applied to the first example, with a generic numerical maximization procedure. The Levenberg-Marquardt algorithm appeared to perform almost three times better than the generic procedure, even when analytic derivatives were provided, and 19 times better when they were not provided. (c) 2008 Elsevier B.V All rights reserved.", "paper_title": "Direct maximization of the likelihood of a hidden Markov model", "paper_id": "WOS:000257014000001"}