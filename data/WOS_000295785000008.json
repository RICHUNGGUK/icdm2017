{"auto_keywords": [{"score": 0.041584008648741164, "phrase": "covariance_selection_problem"}, {"score": 0.015719716506582538, "phrase": "gradient_descent_method"}, {"score": 0.008467040237549175, "phrase": "bcgd_method"}, {"score": 0.0047707093928203, "phrase": "regularized_convex_separable_optimization"}, {"score": 0.004555499634179394, "phrase": "unconstrained_nonsmooth_convex_optimization_problems"}, {"score": 0.004451546786101008, "phrase": "objective_function"}, {"score": 0.004329916183121203, "phrase": "convex_smooth_function"}, {"score": 0.004270347498153512, "phrase": "open_subset"}, {"score": 0.004172874316207784, "phrase": "separable_convex_function"}, {"score": 0.003582843029166968, "phrase": "bcgd"}, {"score": 0.0034528025626422154, "phrase": "separable_problems"}, {"score": 0.0034052594710793664, "phrase": "coordinate_block"}, {"score": 0.0033428820402140683, "phrase": "gauss-seidel_rule"}, {"score": 0.0031479142819948007, "phrase": "large-scale_problems"}, {"score": 0.00309023627455429, "phrase": "global_convergence"}, {"score": 0.0030196181347744372, "phrase": "local_lipschizian_error"}, {"score": 0.0026775328564629577, "phrase": "epsilon-optimal_solution"}, {"score": 0.0025447065763362984, "phrase": "first-order_methods"}, {"score": 0.002509700949891861, "phrase": "lu"}, {"score": 0.0024865750610840493, "phrase": "siam"}, {"score": 0.0022149235007885826, "phrase": "randomly_generated_instances"}, {"score": 0.0021049977753042253, "phrase": "large-scale_covariance_selection_problems"}], "paper_keywords": ["Block coordinate gradient descent", " Complexity", " Convex optimization", " Covariance selection", " Global convergence", " Linear rate convergence", " l(1)-penalization", " Maximum likelihood estimation"], "paper_abstract": "We consider a class of unconstrained nonsmooth convex optimization problems, in which the objective function is the sum of a convex smooth function on an open subset of matrices and a separable convex function on a set of matrices. This problem includes the covariance selection problem that can be expressed as an l(1)-penalized maximum likelihood estimation problem. In this paper, we propose a block coordinate gradient descent method (abbreviated as BCGD) for solving this class of nonsmooth separable problems with the coordinate block chosen by a Gauss-Seidel rule. The method is simple, highly parallelizable, and suited for large-scale problems. We establish global convergence and, under a local Lipschizian error bound assumption, linear rate of convergence for this method. For the covariance selection problem, the method can terminate in O(n(3)/epsilon) iterations with an epsilon-optimal solution. We compare the performance of the BCGD method with the first-order methods proposed by Lu (SIAM J Optim 19:1807-1827, 2009; SIAM J Matrix Anal Appl 31:2000-2016, 2010) for solving the covariance selection problem on randomly generated instances. Our numerical experience suggests that the BCGD method can be efficient for large-scale covariance selection problems with constraints.", "paper_title": "A block coordinate gradient descent method for regularized convex separable optimization and covariance selection", "paper_id": "WOS:000295785000008"}