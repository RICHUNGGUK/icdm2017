{"auto_keywords": [{"score": 0.0405231147798184, "phrase": "total_length"}, {"score": 0.004814970514323876, "phrase": "tsp."}, {"score": 0.004712397190881752, "phrase": "first_constant-factor_approximation_algorithm"}, {"score": 0.004661940802328815, "phrase": "orienteering"}, {"score": 0.004574927037264451, "phrase": "new_problem"}, {"score": 0.004513767384609526, "phrase": "discounted-reward_traveling_salesman_problem"}, {"score": 0.004429508899912265, "phrase": "robot_navigation"}, {"score": 0.004118925784964089, "phrase": "orienteering_problem"}, {"score": 0.0038924376662459424, "phrase": "hard_limit"}, {"score": 0.00378898953201521, "phrase": "discounted-reward_tsp"}, {"score": 0.003738297182416934, "phrase": "length_limit"}, {"score": 0.003688280527594519, "phrase": "discount_factor"}, {"score": 0.0035902386595439146, "phrase": "total_discounted_reward"}, {"score": 0.00332932058781309, "phrase": "planning_problem"}, {"score": 0.003302511130068653, "phrase": "markov_decision_process"}, {"score": 0.0032407895547247254, "phrase": "commonly_employed_infinite_horizon"}, {"score": 0.003112373698668253, "phrase": "exponentially_large_state_spaces"}, {"score": 0.003054194767034299, "phrase": "one-time_events"}, {"score": 0.0029890310448474266, "phrase": "package_deliveries"}, {"score": 0.002941069587970774, "phrase": "tree_and_multiple-path_variants"}, {"score": 0.002832122581073037, "phrase": "unrooted_orienteering_problem"}, {"score": 0.002786671812199566, "phrase": "fixed_start"}, {"score": 0.0026906751042173444, "phrase": "related_problems"}, {"score": 0.0024748609903955656, "phrase": "rooted_question"}, {"score": 0.0024483022321037714, "phrase": "open_problem"}, {"score": 0.0024220277948247973, "phrase": "j._s._b._mitchell"}, {"score": 0.002402506729202441, "phrase": "g._narasimhan"}, {"score": 0.0023575658069040885, "phrase": "computational_geometry"}, {"score": 0.0022217110026330066, "phrase": "s._vempala"}, {"score": 0.0022098460002469207, "phrase": "siam"}, {"score": 0.0022038008840831783, "phrase": "j._comput"}], "paper_keywords": ["approximation algorithms", " traveling salesman problem", " prize-collecting traveling", " salesman problem", " orienteering", " robot navigation", " Markov decision processes"], "paper_abstract": "In this paper, we give the first constant-factor approximation algorithm for the rooted ORIENTEERING problem, as well as a new problem that we call the DISCOUNTED-REWARD traveling salesman problem (TSP), motivated by robot navigation. In both problems, we are given a graph with lengths on edges and rewards on nodes, and a start node s. In the ORIENTEERING problem, the goal is to find a path starting at s that maximizes the reward collected, subject to a hard limit on the total length of the path. In the DISCOUNTED-REWARD TSP, instead of a length limit we are given a discount factor gamma, and the goal is to maximize the total discounted reward collected, where the reward for a node reached at time t is discounted by gamma(t). This problem is motivated by an approximation to a planning problem in the Markov decision process (MDP) framework under the commonly employed infinite horizon discounted reward optimality criterion. The approximation arises from a need to deal with exponentially large state spaces that emerge when trying to model one-time events and nonrepeatable rewards (such as for package deliveries). We also consider tree and multiple-path variants of these problems and provide approximations for those as well. Although the unrooted ORIENTEERING problem, where there is no fixed start node s, has been known to be approximable using algorithms for related problems such as kappa-TSP (in which the amount of reward to be collected is fixed and the total length is approximately minimized), ours is the first to approximate the rooted question, solving an open problem in [E. M. Arkin, J. S. B. Mitchell, and G. Narasimhan, Proceedings of the 14th ACM Symposium on Computational Geometry, 1998, pp. 307-316] and [B. Awerbuch, Y. Azar, A. Blum, and S. Vempala, SIAM J. Comput., 28 ( 1998), pp. 254-262]. We complement our approximation result for Orienteering by showing that the problem is APX- hard.", "paper_title": "Approximation algorithms for orienteering and discounted-reward TSP", "paper_id": "WOS:000247647200014"}