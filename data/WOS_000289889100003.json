{"auto_keywords": [{"score": 0.03245716955912307, "phrase": "atr"}, {"score": 0.00481495049065317, "phrase": "adaptive_timekeeping_replacement"}, {"score": 0.004659951478263083, "phrase": "shared_cmp_caches"}, {"score": 0.004603115982404017, "phrase": "chip_multiprocessors"}, {"score": 0.004382591802405206, "phrase": "shared_last-level_cache"}, {"score": 0.0043114466990006334, "phrase": "degraded_and_unpredictable_memory_performance"}, {"score": 0.004276306563103443, "phrase": "multiprogrammed_and_parallel_workloads"}, {"score": 0.004189698902282913, "phrase": "recent_schemes"}, {"score": 0.0041555469216463855, "phrase": "cache_bandwidth"}, {"score": 0.004038183610525504, "phrase": "better_aggregate_performance"}, {"score": 0.003828912548622257, "phrase": "relatively_coarse-grained_capacity_management"}, {"score": 0.0037667216301666196, "phrase": "operating_system_process_priority_levels"}, {"score": 0.0034422330919613294, "phrase": "prior_work"}, {"score": 0.00335867620389808, "phrase": "system_priority_levels"}, {"score": 0.003304097742490624, "phrase": "capacity_management"}, {"score": 0.003237116128682586, "phrase": "capacity_management_mechanism"}, {"score": 0.0031975786158658158, "phrase": "timekeeping_techniques"}, {"score": 0.003145609726536791, "phrase": "time_interval"}, {"score": 0.003107186371289819, "phrase": "cached_data"}, {"score": 0.0030193416820392554, "phrase": "aggregate_cache_occupancies"}, {"score": 0.0028744731986533076, "phrase": "key_novelties"}, {"score": 0.002759058732719621, "phrase": "complete_cache_capacity_management_framework"}, {"score": 0.0027253441561528495, "phrase": "account_application_priorities"}, {"score": 0.0027030963394733916, "phrase": "memory_characteristics"}, {"score": 0.0025315126622525424, "phrase": "parallel_workloads"}, {"score": 0.0024903420152789135, "phrase": "sequential_ones"}, {"score": 0.00241989408309933, "phrase": "full-system_simulator"}, {"score": 0.0023610946923597405, "phrase": "sequential_and_parallel_applications"}, {"score": 0.0023131855385489764, "phrase": "first_detailed_study"}, {"score": 0.0022477377970612847, "phrase": "thread_behaviors"}, {"score": 0.002229380111189829, "phrase": "parallel_applications"}, {"score": 0.002184137736856711, "phrase": "unmanaged_system"}], "paper_keywords": ["Design", " Performance", " Cache decay", " capacity management", " shared resource management"], "paper_abstract": "In chip multiprocessors (CMPs), several high-performance cores typically compete for capacity in a shared last-level cache. This causes degraded and unpredictable memory performance for multiprogrammed and parallel workloads. In response, recent schemes apportion cache bandwidth and capacity in ways that offer better aggregate performance for the workloads. These schemes, however, focus primarily on relatively coarse-grained capacity management without concern for operating system process priority levels. In this work, we explore capacity management approaches that are both temporally and spatially more fine-grained than prior work. We also consider operating system priority levels as part of capacity management. We propose a capacity management mechanism based on timekeeping techniques that track the time interval since the last access to cached data. This Adaptive Timekeeping Replacement (ATR) scheme maintains aggregate cache occupancies that reflect the priority and footprint of each application. The key novelties of our work are (1) ATR offers a complete cache capacity management framework taking into account application priorities and memory characteristics, and (2) ATR's fine-grained cache capacity control is demonstrated to be effective and important in improving the performance of parallel workloads in addition to sequential ones. We evaluate our ideas using a full-system simulator and multiprogrammed workloads of both sequential and parallel applications. This is the first detailed study of shared cache capacity management considering thread behaviors in parallel applications. ATR outperforms an unmanaged system by as much as 1.63X and by an average of 1.19X. ATR's fine-grained temporal control is particularly important for parallel applications, which are expected to be increasingly prevalent in years to come.", "paper_title": "Adaptive Timekeeping Replacement: Fine-Grained Capacity Management for Shared CMP Caches", "paper_id": "WOS:000289889100003"}