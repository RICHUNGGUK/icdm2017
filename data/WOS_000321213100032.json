{"auto_keywords": [{"score": 0.03838240989170165, "phrase": "long_memory_latencies"}, {"score": 0.00481495049065317, "phrase": "aware_scheduling_techniques"}, {"score": 0.004749824776508639, "phrase": "gpgpu_performance"}, {"score": 0.004706895428234964, "phrase": "emerging_gpgpu_architectures"}, {"score": 0.0046221918332842995, "phrase": "programming_models"}, {"score": 0.004580496468695434, "phrase": "cuda"}, {"score": 0.004539039298641187, "phrase": "opencl"}, {"score": 0.004457309741979921, "phrase": "cost-effective_platform"}, {"score": 0.004337502738649814, "phrase": "high_thread_level_parallelism"}, {"score": 0.00429828390000229, "phrase": "lower_energy_budgets"}, {"score": 0.003925004567341077, "phrase": "lost_opportunity"}, {"score": 0.0038194503801108324, "phrase": "major_cause"}, {"score": 0.0036998728605395384, "phrase": "current_warp_scheduling_policies"}, {"score": 0.0034876091188625535, "phrase": "scheduling_decisions"}, {"score": 0.0033175095965171674, "phrase": "thread_array"}, {"score": 0.0030847461428168614, "phrase": "coordinated_cta-aware_scheduling_policy"}, {"score": 0.002816556934231128, "phrase": "two-level_warp_scheduling"}, {"score": 0.0027159094186967247, "phrase": "per-core_performance"}, {"score": 0.00266693888510356, "phrase": "cache_contention"}, {"score": 0.0026307897899760383, "phrase": "latency_hiding_capability"}, {"score": 0.0025951294037626174, "phrase": "third_scheme"}, {"score": 0.002468450551616404, "phrase": "overall_gpgpu_performance"}, {"score": 0.0024349853033241663, "phrase": "dram_bank-level_parallelism"}, {"score": 0.0023694065215394593, "phrase": "opportunistic_memory-side_prefetching"}, {"score": 0.0022743274702883456, "phrase": "open_dram_rows"}, {"score": 0.002203016940021265, "phrase": "highly_memory-intensive_applications"}, {"score": 0.0021049977753042253, "phrase": "commonly-employed_round-robin_warp_scheduling_policy"}], "paper_keywords": ["Design", " Performance", " GPGPUs", " Scheduling", " Prefetching", " Latency Tolerance"], "paper_abstract": "Emerging GPGPU architectures, along with programming models like CUDA and OpenCL, offer a cost-effective platform for many applications by providing high thread level parallelism at lower energy budgets. Unfortunately, for many general-purpose applications, available hardware resources of a GPGPU are not efficiently utilized, leading to lost opportunity in improving performance. A major cause of this is the inefficiency of current warp scheduling policies in tolerating long memory latencies. In this paper, we identify that the scheduling decisions made by such policies are agnostic to thread-block, or cooperative thread array (CTA), behavior, and as a result inefficient. We present a coordinated CTA-aware scheduling policy that utilizes four schemes to minimize the impact of long memory latencies. The first two schemes, CTA-aware two-level warp scheduling and locality aware warp scheduling, enhance per-core performance by effectively reducing cache contention and improving latency hiding capability. The third scheme, bank-level parallelism aware warp scheduling, improves overall GPGPU performance by enhancing DRAM bank-level parallelism. The fourth scheme employs opportunistic memory-side prefetching to further enhance performance by taking advantage of open DRAM rows. Evaluations on a 28-core GPGPU platform with highly memory-intensive applications indicate that our proposed mechanism can provide 33% average performance improvement compared to the commonly-employed round-robin warp scheduling policy.", "paper_title": "OWL: Cooperative Thread Array Aware Scheduling Techniques for Improving GPGPU Performance", "paper_id": "WOS:000321213100032"}