{"auto_keywords": [{"score": 0.040130255517357764, "phrase": "scms_algorithm"}, {"score": 0.01501804323888077, "phrase": "mean_shift"}, {"score": 0.00481495049065317, "phrase": "convergence_properties"}, {"score": 0.004309968592440261, "phrase": "principal_curves"}, {"score": 0.004192190072470619, "phrase": "new_definition"}, {"score": 0.004096615294478729, "phrase": "hessian"}, {"score": 0.004040123129261597, "phrase": "kernel_probability_density_estimate"}, {"score": 0.00398452530840582, "phrase": "simulation_results"}, {"score": 0.003947884118204803, "phrase": "synthetic_and_real_data"}, {"score": 0.0037696519417841287, "phrase": "rigorous_study"}, {"score": 0.0035663245542174224, "phrase": "initial_steps"}, {"score": 0.003389557201018519, "phrase": "important_convergence_properties"}, {"score": 0.003118942369691329, "phrase": "density_estimate_values"}, {"score": 0.0030476702431093687, "phrase": "output_values"}, {"score": 0.0028303313661029597, "phrase": "consecutive_points"}, {"score": 0.0027913353347879507, "phrase": "output_sequence"}, {"score": 0.002652878591019016, "phrase": "gradient_vector"}, {"score": 0.0025683581446365165, "phrase": "d-d_eigenvectors"}, {"score": 0.0025212722607043546, "phrase": "d-d_largest_eigenvalues"}, {"score": 0.0024865237976623286, "phrase": "local_inverse_covariance_matrix"}, {"score": 0.0024522530645471065, "phrase": "last_two_properties"}, {"score": 0.002429668083484833, "phrase": "theoretical_guarantees"}, {"score": 0.002341384304635523, "phrase": "projection_step"}, {"score": 0.0022355167967736326, "phrase": "running_times"}, {"score": 0.0021843879253500894, "phrase": "resulting_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Unsupervised learning", " Subspace constrained mean shift", " Dimensionality reduction", " Principal curves", " Principal surfaces", " Convergence"], "paper_abstract": "Subspace constrained mean shift (SCMS) is a non-parametric, iterative algorithm that has recently been proposed to find principal curves and surfaces based on a new definition involving the gradient and Hessian of a kernel probability density estimate. Although simulation results using synthetic and real data have demonstrated the usefulness of the SCMS algorithm, a rigorous study of its convergence is still missing. This paper aims to take initial steps in this direction by showing that the SCMS algorithm inherits some important convergence properties of the mean shift (MS) algorithm. In particular, the monotonicity and convergence of the density estimate values along the sequence of output values of the algorithm is shown. Also, it is shown that the distance between consecutive points of the output sequence converges to zero, as does the projection of the gradient vector onto the subspace spanned by the D-d eigenvectors corresponding to the D-d largest eigenvalues of the local inverse covariance matrix. These last two properties provide theoretical guarantees for stopping criteria. By modifying the projection step, three variations of the SCMS algorithm are proposed and the running times and performance of the resulting algorithms are compared. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "On some convergence properties of the subspace constrained mean shift", "paper_id": "WOS:000321232900022"}