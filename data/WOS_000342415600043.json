{"auto_keywords": [{"score": 0.04743405832670909, "phrase": "general_metric_spaces"}, {"score": 0.00481495049065317, "phrase": "metric_data"}, {"score": 0.004754600386388821, "phrase": "recent_advances"}, {"score": 0.00469500313566324, "phrase": "large-margin_classification"}, {"score": 0.004435885269134359, "phrase": "hilbert_spaces"}, {"score": 0.004009894739765119, "phrase": "general_framework"}, {"score": 0.003764603272900618, "phrase": "computational_efficiency"}, {"score": 0.0036707250689857348, "phrase": "direct_bounds"}, {"score": 0.0036246643358397272, "phrase": "generalization_error"}, {"score": 0.0035120164504478437, "phrase": "new_algorithm"}, {"score": 0.0032556926399045635, "phrase": "doubling_dimension"}, {"score": 0.003194580334752371, "phrase": "data_points"}, {"score": 0.003075765038696993, "phrase": "superior_classification_performance"}, {"score": 0.002980124909321296, "phrase": "algorithmic_core"}, {"score": 0.002780025943393764, "phrase": "lipschitz_extension"}, {"score": 0.0027278178983579085, "phrase": "nearest_neighbor_search"}, {"score": 0.002676587673012276, "phrase": "algorithm's_generalization_performance"}, {"score": 0.0025933275734892508, "phrase": "fat-shattering_dimension"}, {"score": 0.0025611233323308584, "phrase": "lipschitz"}, {"score": 0.0024654516678953658, "phrase": "experimental_evidence"}, {"score": 0.0023887436047824386, "phrase": "common_kernel_methods"}, {"score": 0.0022423971916457662, "phrase": "new_perspective"}, {"score": 0.0022002633747977593, "phrase": "nearest_neighbor_classifier"}, {"score": 0.0021453112313233554, "phrase": "significantly_sharper_risk_asymptotics"}, {"score": 0.0021049977753042253, "phrase": "classic_analysis"}], "paper_keywords": ["Classification", " Lipschitz function", " metric space", " doubling dimension"], "paper_abstract": "Recent advances in large-margin classification of data residing in general metric spaces (rather than Hilbert spaces) enable classification under various natural metrics, such as string edit and earthmover distance. A general framework developed for this purpose left open the questions of computational efficiency and of providing direct bounds on generalization error. We design a new algorithm for classification in general metric spaces, whose runtime and accuracy depend on the doubling dimension of the data points, and can thus achieve superior classification performance in many common scenarios. The algorithmic core of our approach is an approximate (rather than exact) solution to the classical problems of Lipschitz extension and of nearest neighbor search. The algorithm's generalization performance is guaranteed via the fat-shattering dimension of Lipschitz classifiers, and we present experimental evidence of its superiority to some common kernel methods. As a by-product, we offer a new perspective on the nearest neighbor classifier, which yields significantly sharper risk asymptotics than the classic analysis.", "paper_title": "Efficient Classification for Metric Data", "paper_id": "WOS:000342415600043"}