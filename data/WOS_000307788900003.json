{"auto_keywords": [{"score": 0.04843830651479272, "phrase": "large_data_structures"}, {"score": 0.03392479049908957, "phrase": "ideal_sram"}, {"score": 0.00481495049065317, "phrase": "worst_case_performance_guarantee"}, {"score": 0.004367108655796158, "phrase": "memory_system"}, {"score": 0.004115697805448384, "phrase": "link_speeds"}, {"score": 0.003988487841970088, "phrase": "router_designers"}, {"score": 0.003905864553426306, "phrase": "unfortunate_trade-offs"}, {"score": 0.0038116339588032816, "phrase": "sram"}, {"score": 0.0033969686698961743, "phrase": "wirespeed_updates"}, {"score": 0.0032689949867583633, "phrase": "robust_pipelined_memory_architecture"}, {"score": 0.0031348622158167195, "phrase": "output_sequence"}, {"score": 0.003091381857342703, "phrase": "pipelined_memory_architecture"}, {"score": 0.002823050755743821, "phrase": "fixed_pipeline_delay"}, {"score": 0.0028045091222220356, "phrase": "delta"}, {"score": 0.0027645030021576926, "phrase": "fixed_pipeline_delay_abstraction"}, {"score": 0.0027356847952125433, "phrase": "interrupt_mechanism"}, {"score": 0.002641770514171764, "phrase": "write_operation"}, {"score": 0.002542174767860791, "phrase": "proposed_solution"}, {"score": 0.0024548872075607687, "phrase": "dram_banks"}, {"score": 0.002395571076685383, "phrase": "reservation_table"}, {"score": 0.002337684811187166, "phrase": "data_cache"}, {"score": 0.002297193869719679, "phrase": "prior_interleaved_memory_solutions"}, {"score": 0.0022338588371647278, "phrase": "memory_access_patterns"}, {"score": 0.0022105600003496225, "phrase": "adversarial_ones"}, {"score": 0.0021571346514418226, "phrase": "rigorous_worst_case_theoretical_analysis"}, {"score": 0.002119764367112061, "phrase": "convex_ordering"}, {"score": 0.0021049977753042253, "phrase": "large_deviation_theory"}], "paper_keywords": ["Robust memory system", " network processing", " large deviation theory", " convex ordering"], "paper_abstract": "Many network processing applications require wirespeed access to large data structures or a large amount of packet and flow-level data. Therefore, it is essential for the memory system of a router to be able to support both read and write accesses to such data at link speeds. As link speeds continue to increase, router designers are constantly grappling with the unfortunate trade-offs between the speed and cost of SRAM and DRAM. The capacity of SRAMs is woefully inadequate in many cases and it proves too costly to store large data structures entirely in SRAM, while DRAM is viewed as too slow for providing wirespeed updates at such high speed. In this paper, we analyze a robust pipelined memory architecture that can emulate an ideal SRAM by guaranteeing with very high probability that the output sequence produced by the pipelined memory architecture is the same as the one produced by an ideal SRAM under the same sequence of memory read and write operations, except time shifted by a fixed pipeline delay of Delta. Given a fixed pipeline delay abstraction, no interrupt mechanism is required to indicate when read data are ready or a write operation has completed, which greatly simplifies the use of the proposed solution. The design is based on the interleaving of DRAM banks together with the use of a reservation table that serves in part as a data cache. In contrast to prior interleaved memory solutions, our design is robust under all memory access patterns, including adversarial ones, which we demonstrate through a rigorous worst case theoretical analysis using a combination of convex ordering and large deviation theory.", "paper_title": "Robust Pipelined Memory System with Worst Case Performance Guarantee for Network Processing", "paper_id": "WOS:000307788900003"}