{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "open-response_assignments"}, {"score": 0.004736045157669388, "phrase": "preference_learning"}, {"score": 0.0046970764288494764, "phrase": "peer_assessments"}, {"score": 0.004601045179623499, "phrase": "massive_open_online_courses"}, {"score": 0.0045443671106012405, "phrase": "difficult_task"}, {"score": 0.00446987606516801, "phrase": "huge_number"}, {"score": 0.004378469469879815, "phrase": "peer_grading"}, {"score": 0.004324521292490181, "phrase": "effective_method"}, {"score": 0.004014452814186653, "phrase": "first_case"}, {"score": 0.003757480190200645, "phrase": "ordinal_approach"}, {"score": 0.00371115485111022, "phrase": "raw_materials"}, {"score": 0.0036052633308076933, "phrase": "relative_orders"}, {"score": 0.003360464999544279, "phrase": "factorization_method"}, {"score": 0.0032780806856932423, "phrase": "cardinal_and_ordinal_approaches"}, {"score": 0.0031977096047057898, "phrase": "preference_judgments"}, {"score": 0.0031064226956732497, "phrase": "numeric_grades"}, {"score": 0.0028596699752849682, "phrase": "significantly_different_average_grades"}, {"score": 0.002676418520290942, "phrase": "real_world_dataset"}, {"score": 0.0026217131194790832, "phrase": "spain"}, {"score": 0.0025892330566358503, "phrase": "coruna"}, {"score": 0.0025678829712650437, "phrase": "pablo_de_olavide"}, {"score": 0.002546709241392841, "phrase": "sevilla"}, {"score": 0.002515272856011004, "phrase": "oviedo"}, {"score": 0.002494531100339247, "phrase": "gijon"}, {"score": 0.0022492129111567824, "phrase": "similar_or_better_scores"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Peer grading", " Factorization", " Preference learning", " Ordinal and cardinal approaches", " MOOCs"], "paper_abstract": "Evaluating open-response assignments in Massive Open Online Courses is a difficult task because of the huge number of students involved. Peer grading is an effective method to address this problem. There are two basic approaches in the literature: cardinal and ordinal. The first case uses grades assigned by student-graders to a set of assignments of other colleagues. In the ordinal approach, the raw materials used by grading systems are the relative orders that graders appreciate in the assignments that they evaluate. In this paper we present a factorization method that seeks a trade-off between cardinal and ordinal approaches. The algorithm learns from preference judgments to avoid the subjectivity of the numeric grades. But in addition to preferences expressed by student-graders, we include other preferences: those induced from assignments with significantly different average grades. The paper includes a report of the results obtained using this approach in a real world dataset collected in 3 Universities of Spain, A Coruna, Pablo de Olavide at Sevilla, and Oviedo at Gijon. Additionally, we studied the sensitivity of the method with respect to the number of assignments graded by each student. Our method achieves similar or better scores than staff instructors when we measure the discrepancies with other instructor's grades. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "A factorization approach to evaluate open-response assignments in MOOCs using preference learning on peer assessments", "paper_id": "WOS:000359331000026"}