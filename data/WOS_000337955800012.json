{"auto_keywords": [{"score": 0.02496494762400715, "phrase": "ssl"}, {"score": 0.008741550712409193, "phrase": "vad"}, {"score": 0.00481495049065317, "phrase": "mid-fusion_of"}, {"score": 0.004658426838467567, "phrase": "speech_signals"}, {"score": 0.004488384083300084, "phrase": "background_noise"}, {"score": 0.00443308765153716, "phrase": "sound_sources"}, {"score": 0.004342430078306461, "phrase": "effective_communication"}, {"score": 0.004236074477264556, "phrase": "sound_source_localization"}, {"score": 0.004132312946154097, "phrase": "key_signal_processing_components"}, {"score": 0.004031082737449308, "phrase": "sound_signals"}, {"score": 0.0038678251731669865, "phrase": "visual_cues"}, {"score": 0.003773049633656302, "phrase": "lip_movements"}, {"score": 0.003590383760209319, "phrase": "crucial_design_elements"}, {"score": 0.003560807940070886, "phrase": "building_applications"}, {"score": 0.003531474887552889, "phrase": "human_speech"}, {"score": 0.0034306925765832633, "phrase": "microphone_arrays"}, {"score": 0.0033465923623290034, "phrase": "robust_speech_capture"}, {"score": 0.0033190180867511605, "phrase": "video_conferencing_applications"}, {"score": 0.003264547032549398, "phrase": "speaker_identification"}, {"score": 0.0032376465741070274, "phrase": "speech_recognition"}, {"score": 0.0032109670662222416, "phrase": "human_computer_interfaces"}, {"score": 0.0030808211830129304, "phrase": "robust_vad"}, {"score": 0.0030554300179552415, "phrase": "ssl_algorithms"}, {"score": 0.003030247482854539, "phrase": "practical_acoustic_environments"}, {"score": 0.002931569771634119, "phrase": "multiple_simultaneous_speakers"}, {"score": 0.002789528662623221, "phrase": "multimodal_approach"}, {"score": 0.0027551040319267446, "phrase": "support_vector_machines"}, {"score": 0.0025999744005191713, "phrase": "video_and_audio_modalities"}, {"score": 0.0025678829712650437, "phrase": "rgb_camera"}, {"score": 0.0025361866386341796, "phrase": "microphone_array"}, {"score": 0.0024842242921857705, "phrase": "individual_speakers'_spatio-temporal_activities"}, {"score": 0.00240328454980006, "phrase": "mid-fusion_approach"}, {"score": 0.0023249757987087055, "phrase": "multiple_active_and_inactive_speakers"}, {"score": 0.0022773309395419427, "phrase": "proposed_algorithm"}, {"score": 0.002203117204010397, "phrase": "average_vad_accuracy"}, {"score": 0.0021579641460002523, "phrase": "average_error"}, {"score": 0.0021049977753042253, "phrase": "three-dimensional_locations"}], "paper_keywords": ["Beamforming", " hidden Markov model", " multimodal fusion", " optical-flow", " sound source localization", " SRP-PHAT", " support vector machine", " voice activity detection"], "paper_abstract": "Humans can extract speech signals that they need to understand from amixture of background noise, interfering sound sources, and reverberation for effective communication. Voice Activity Detection (VAD) and Sound Source Localization (SSL) are the key signal processing components that humans perform by processing sound signals received at both ears, sometimes with the help of visual cues by locating and observing the lip movements of the speaker. Both VAD and SSL serve as the crucial design elements for building applications involving human speech. For example, systems with microphone arrays can benefit from these for robust speech capture in video conferencing applications, or for speaker identification and speech recognition in Human Computer Interfaces (HCIs). The design and implementation of robust VAD and SSL algorithms in practical acoustic environments are still challenging problems, particularly when multiple simultaneous speakers exist in the same audiovisual scene. In this work we propose a multimodal approach that uses Support Vector Machines (SVMs) and Hidden Markov Models (HMMs) for assessing the video and audio modalities through an RGB camera and a microphone array. By analyzing the individual speakers' spatio-temporal activities and mouth movements, we propose a mid-fusion approach to perform both VAD and SSL for multiple active and inactive speakers. We tested the proposed algorithm in scenarios with up to three simultaneous speakers, showing an average VAD accuracy of 95.06% with an average error of 10.9 cm when estimating the three-dimensional locations of the speakers.", "paper_title": "Simultaneous-Speaker Voice Activity Detection and Localization Using Mid-Fusion of SVM and HMMs", "paper_id": "WOS:000337955800012"}