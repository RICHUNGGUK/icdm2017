{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "visual_tracking"}, {"score": 0.004727837281331949, "phrase": "feature_matching_tasks"}, {"score": 0.004642292792558686, "phrase": "integral_part"}, {"score": 0.003920573616298175, "phrase": "feature_matches"}, {"score": 0.003867206750827, "phrase": "final_scene_estimates"}, {"score": 0.0037626342268042997, "phrase": "candidate_matches"}, {"score": 0.0036776458116294986, "phrase": "consensus_algorithms"}, {"score": 0.0036275811898112934, "phrase": "ransac"}, {"score": 0.003594570678425133, "phrase": "jcbb."}, {"score": 0.003465521267039616, "phrase": "dramatically_different_approach"}, {"score": 0.003310684108128973, "phrase": "feature_matching_search"}, {"score": 0.003265591844292821, "phrase": "global_matching"}, {"score": 0.0032358709481309913, "phrase": "far_fewer_image_processing_operations"}, {"score": 0.0032064196784562017, "phrase": "lower_overall_computational_cost"}, {"score": 0.003119659433566212, "phrase": "image_processing"}, {"score": 0.0030076088903023034, "phrase": "global_consensus"}, {"score": 0.002860062341014677, "phrase": "significant_image_ambiguity"}, {"score": 0.0028082176224611542, "phrase": "dynamic_mixture"}, {"score": 0.002782647772219627, "phrase": "gaussians_treatment"}, {"score": 0.0027197343489967296, "phrase": "active_matching"}, {"score": 0.002527788827701828, "phrase": "expected_shannon_information_gain"}, {"score": 0.0023818288346547692, "phrase": "sequential_slam_system"}, {"score": 0.002254571365634257, "phrase": "detailed_analysis"}, {"score": 0.002193509835371438, "phrase": "performance-enhancing_approximations"}, {"score": 0.002163600705898299, "phrase": "full_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Image matching", " Visual tracking", " SLAM"], "paper_abstract": "In the feature matching tasks which form an integral part of visual tracking or SLAM (Simultaneous Localisation And Mapping), there are invariably priors available on the absolute and/or relative image locations of features of interest. Usually, these priors are used post-hoc in the process of resolving feature matches and obtaining final scene estimates, via 'first get candidate matches, then resolve' consensus algorithms such as RANSAC or JCBB. In this paper we show that the dramatically different approach of using priors dynamically to guide a feature by feature matching search can achieve global matching with far fewer image processing operations and lower overall computational cost. Essentially, we put image processing into the loop of the search for global consensus. In particular, Our approach is able to cope with significant image ambiguity thanks to a dynamic mixture of Gaussians treatment. In Our fully Bayesian algorithm denoted Active Matching, the choice of the most efficient search action at each step is guided intuitively and rigorously by expected Shannon information gain. We demonstrate the algorithm in feature matching as part of a sequential SLAM system for 3D camera tracking with a range of settings, and give a detailed analysis of performance which leads to performance-enhancing approximations to the full algorithm. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Active matching for visual tracking", "paper_id": "WOS:000272605900003"}