{"auto_keywords": [{"score": 0.03523476499109691, "phrase": "enhanced_visual_content_similarity"}, {"score": 0.00481495049065317, "phrase": "associated_news_story_retrieval"}, {"score": 0.004603244077348199, "phrase": "multi-modal_approaches"}, {"score": 0.0045262653692203815, "phrase": "associated_news_stories"}, {"score": 0.004351595642056769, "phrase": "visual_domain"}, {"score": 0.004183638135671596, "phrase": "detection_method"}, {"score": 0.00413684709497912, "phrase": "local_signatures"}, {"score": 0.004022137057452406, "phrase": "mutual_visual_cues"}, {"score": 0.003823584649521733, "phrase": "visual_representation"}, {"score": 0.003717528595128948, "phrase": "semantic_signature"}, {"score": 0.0036553070575015344, "phrase": "pre-defined_semantic_visual_concepts"}, {"score": 0.003474799999519295, "phrase": "visual_concept_weighting_scheme"}, {"score": 0.0034166271604770576, "phrase": "local_and_semantic_signature_similarities"}, {"score": 0.003266201217790818, "phrase": "textual_domain"}, {"score": 0.00319348234663792, "phrase": "automatic_speech_recognition"}, {"score": 0.003157758100103665, "phrase": "asr"}, {"score": 0.003087419032091453, "phrase": "optical_character_recognition"}, {"score": 0.003052875369809362, "phrase": "ocr"}, {"score": 0.002951444412262927, "phrase": "enhanced_textual_similarity"}, {"score": 0.0029020077365708966, "phrase": "proposed_semantic_similarity_measure"}, {"score": 0.0028373742057129126, "phrase": "textual_and_visual_modalities"}, {"score": 0.002758597251689795, "phrase": "early_and_late_fusion_approaches"}, {"score": 0.0026971491199975083, "phrase": "proposed_early_fusion_approach"}, {"score": 0.00257831812581194, "phrase": "visual_semantics"}, {"score": 0.0025494354909453847, "phrase": "textual_information"}, {"score": 0.002464709648714996, "phrase": "late_fusion_approach"}, {"score": 0.0024097920845706795, "phrase": "uni-modal_similarity_scores"}, {"score": 0.0023694065215394593, "phrase": "determined_early_fusion_similarity_score"}, {"score": 0.0023166075278110237, "phrase": "final_retrieval_performance"}, {"score": 0.0022906498741695094, "phrase": "experimental_results"}, {"score": 0.0021773853011112882, "phrase": "early_fusion_approach"}], "paper_keywords": ["Semantic signature", " Scene signature", " Visual concept signature", " News story retrieval"], "paper_abstract": "In this paper, we investigate multi-modal approaches to retrieve associated news stories sharing the same main topic. In the visual domain, we employ near duplicate keyframe/scene detection method using local signatures to identify stories with mutual visual cues. Further, to improve the effectiveness of visual representation, we develop a semantic signature that contains pre-defined semantic visual concepts in a news story. We propose a visual concept weighting scheme to combine local and semantic signature similarities to obtain the enhanced visual content similarity. In the textual domain, we utilize Automatic Speech Recognition (ASR) and refined Optical Character Recognition (OCR) transcripts and determine the enhanced textual similarity using the proposed semantic similarity measure. To fuse textual and visual modalities, we investigate different early and late fusion approaches. In the proposed early fusion approach, we employ two methods to retrieve the visual semantics using textual information. Next, using a late fusion approach, we integrate uni-modal similarity scores and the determined early fusion similarity score to boost the final retrieval performance. Experimental results show the usefulness of the enhanced visual content similarity and the early fusion approach, and the superiority of our late fusion approach.", "paper_title": "Multi-modal fusion for associated news story retrieval", "paper_id": "WOS:000351692300002"}