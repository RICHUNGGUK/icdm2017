{"auto_keywords": [{"score": 0.03510165161138853, "phrase": "community_inter-annotator_agreement"}, {"score": 0.03316461064922614, "phrase": "ground_truth"}, {"score": 0.00481495049065317, "phrase": "ground_truth_generation"}, {"score": 0.004508286991825192, "phrase": "clinical_records"}, {"score": 0.004205660924155212, "phrase": "community_annotation_experiment"}, {"score": 0.004054598617367474, "phrase": "annotation_guidelines"}, {"score": 0.004010342699414496, "phrase": "small_set"}, {"score": 0.003981106369547929, "phrase": "annotated_discharge_summaries"}, {"score": 0.003754731210715724, "phrase": "discharge_summary"}, {"score": 0.0036198068824262464, "phrase": "third_annotator"}, {"score": 0.0035802796531956413, "phrase": "third_team"}, {"score": 0.0032552377213294604, "phrase": "inter-annotator_agreement"}, {"score": 0.003161271802988128, "phrase": "expert_annotators"}, {"score": 0.0031039212233596415, "phrase": "pooled_system_outputs"}, {"score": 0.002853183476661506, "phrase": "expert_inter-annotator_agreement"}, {"score": 0.0028014064098604093, "phrase": "raw_records"}, {"score": 0.0026613618933208467, "phrase": "community_ground_truth"}, {"score": 0.0026035011274172753, "phrase": "expert_ground_truth"}, {"score": 0.0025098414374397308, "phrase": "community_annotators"}, {"score": 0.0024915167458844914, "phrase": "comparable_inter-annotator_agreement"}, {"score": 0.0021282722932911427, "phrase": "high-quality_ground_truth"}, {"score": 0.0021049977753042253, "phrase": "intricate_and_domain-specific_annotation_tasks"}], "paper_keywords": [""], "paper_abstract": "Objective Within the context of the Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records, the authors (also referred to as 'the i2b2 medication challenge team' or 'the i2b2 team' for short) organized a community annotation experiment. Design For this experiment, the authors released annotation guidelines and a small set of annotated discharge summaries. They asked the participants of the Third i2b2 Workshop to annotate 10 discharge summaries per person; each discharge summary was annotated by two annotators from two different teams, and a third annotator from a third team resolved disagreements. Measurements In order to evaluate the reliability of the annotations thus produced, the authors measured community inter-annotator agreement and compared it with the inter-annotator agreement of expert annotators when both the community and the expert annotators generated ground truth based on pooled system outputs. For this purpose, the pool consisted of the three most densely populated automatic annotations of each record. The authors also compared the community inter-annotator agreement with expert inter-annotator agreement when the experts annotated raw records without using the pool. Finally, they measured the quality of the community ground truth by comparing it with the expert ground truth. Results and conclusions The authors found that the community annotators achieved comparable inter-annotator agreement to expert annotators, regardless of whether the experts annotated from the pool. Furthermore, the ground truth generated by the community obtained F-measures above 0.90 against the ground truth of the experts, indicating the value of the community as a source of high-quality ground truth even on intricate and domain-specific annotation tasks.", "paper_title": "Community annotation experiment for ground truth generation for the i2b2 medication challenge", "paper_id": "WOS:000281972200006"}