{"auto_keywords": [{"score": 0.04968898347890437, "phrase": "group_sparsity"}, {"score": 0.015719716506582538, "phrase": "local_motion"}, {"score": 0.014264356483831285, "phrase": "action_recognition"}, {"score": 0.008724296621580347, "phrase": "multiple_kernel_method"}, {"score": 0.004603115982404017, "phrase": "critical_step"}, {"score": 0.004258843584164121, "phrase": "challenging_task"}, {"score": 0.004206879557938045, "phrase": "wide_variations"}, {"score": 0.004121672175779706, "phrase": "camera_motion"}, {"score": 0.0040880724328903, "phrase": "cluttered_background"}, {"score": 0.003908091457759849, "phrase": "dense_sampling_based_approaches"}, {"score": 0.003386301445203071, "phrase": "discriminative_power"}, {"score": 0.0032637448078885016, "phrase": "clutter_motions"}, {"score": 0.003210704011271791, "phrase": "background_changes"}, {"score": 0.0031845067058084583, "phrase": "camera_motions"}, {"score": 0.003069230909006614, "phrase": "local_motions"}, {"score": 0.0030069962181403487, "phrase": "unsupervised_manner"}, {"score": 0.0028627183712016894, "phrase": "action_types"}, {"score": 0.0027817658627604653, "phrase": "local_motion_descriptors"}, {"score": 0.002759058732719621, "phrase": "full_motion_descriptors"}, {"score": 0.0026920404458807444, "phrase": "emphasized_motion_features"}, {"score": 0.002573362191631126, "phrase": "different_types"}, {"score": 0.002480154140902474, "phrase": "small_number"}, {"score": 0.002459902948098627, "phrase": "selected_local_motion_descriptors"}, {"score": 0.0024298350266697905, "phrase": "proposed_algorithm"}, {"score": 0.002322689143237591, "phrase": "popular_benchmark_datasets"}, {"score": 0.002294294558114909, "phrase": "existing_methods"}, {"score": 0.002229380111189829, "phrase": "group_sparse_representation"}, {"score": 0.002157433262687853, "phrase": "action_recognition_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Action recognition", " Motion descriptor", " Sparse representation", " Dynamic scene understanding"], "paper_abstract": "Recognizing actions in a video is a critical step for making many vision-based applications possible and has attracted much attention recently. However, action recognition in a video is a challenging task due to wide variations within an action, camera motion, cluttered background, and occlusions, to name a few. While dense sampling based approaches are currently achieving the state-of-the-art performance in action recognition, they do not perform well for many realistic video sequences since, by considering every motion found in a video equally, the discriminative power of these approaches is often reduced due to clutter motions, such as background changes and camera motions. In this paper, we robustly identify local motions of interest in an unsupervised manner by taking advantage of group sparsity. In order to robustly classify action types, we emphasize local motion by combining local motion descriptors and full motion descriptors and apply group sparsity to the emphasized motion features using the multiple kernel method. In experiments, we show that different types of actions can be well recognized using a small number of selected local motion descriptors and the proposed algorithm achieves the state-of-the-art performance on popular benchmark datasets, outperforming existing methods. We also demonstrate that the group sparse representation with the multiple kernel method can dramatically improve the action recognition performance. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Robust action recognition using local motion and group sparsity", "paper_id": "WOS:000331667400003"}