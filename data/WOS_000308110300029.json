{"auto_keywords": [{"score": 0.02965864568759797, "phrase": "learning_path"}, {"score": 0.00481495049065317, "phrase": "multiple-lookahead-levels_agent_reinforcement-learning_technique"}, {"score": 0.0044852808671980325, "phrase": "machine_learning"}, {"score": 0.00428314621261913, "phrase": "main_limitations"}, {"score": 0.004252864101987588, "phrase": "existing_rl_techniques"}, {"score": 0.0038234000205958547, "phrase": "reinforcement_learning"}, {"score": 0.003796355951305572, "phrase": "multiple_lookahead_levels"}, {"score": 0.0037428381813152532, "phrase": "autonomous_agent"}, {"score": 0.0035613768414594262, "phrase": "watkins's_q-learning_algorithm"}, {"score": 0.003329051037399227, "phrase": "mll_equation"}, {"score": 0.0031788569040714434, "phrase": "improvement_rate"}, {"score": 0.0031451673059794205, "phrase": "agent's_learning_speed"}, {"score": 0.0031229055434606003, "phrase": "different_look-ahead_levels"}, {"score": 0.002748016244165687, "phrase": "learning_path_number"}, {"score": 0.0024789326485851666, "phrase": "look-ahead_levels"}, {"score": 0.0023670015392684885, "phrase": "mll_system"}, {"score": 0.002341895788806396, "phrase": "time_domain"}, {"score": 0.0023006418600072325, "phrase": "lyapunov_theory"}, {"score": 0.002220296517089146, "phrase": "lyapunov_stability_analysis"}, {"score": 0.0021580408652391767, "phrase": "first_time"}, {"score": 0.0021275693314553143, "phrase": "circuit_architecture"}, {"score": 0.0021049977753042253, "phrase": "mll_technique's_software_configurable_hardware_system_design"}], "paper_keywords": ["Reinforcement learning", " Q-Learning", " Machine learning", " Integrated circuits", " Lyapunov stability", " Robotics"], "paper_abstract": "Reinforcement learning (RL) techniques have contributed and continue to tremendously contribute to the advancement of machine learning and its many related recent applications. As it is well known, some of the main limitations of existing RL techniques are, in general, their slow convergence and their computational complexity. The contributions of this paper are two-fold: (1) First, it introduces a technique for reinforcement learning using multiple lookahead levels that grants an autonomous agent more visibility in its environment and helps it learn faster. This technique extends the Watkins's Q-Learning algorithm by using the Multiple-Lookahead-Levels (MLL) model equation that we develop and present here. An analysis of the convergence of the MLL equation and proof of its effectiveness are performed. A method to compute the improvement rate of the agent's learning speed between different look-ahead levels is also proposed and implemented. Here, both the time and space complexities are examined. Results show that the number of steps, required to achieve the goal, per learning path exponentially decreases with the learning path number (time). Results also show that the number of steps per learning path, to some degree, is less at any time when the number of look-ahead levels is higher (space). Furthermore, we perform the analysis of the MLL system in the time domain and prove its temporal stability using Lyapunov theory. (2) Second, based on this Lyapunov stability analysis, we subsequently, and for the first time, propose a circuit architecture for the MLL technique's software configurable hardware system design for real-time applications.", "paper_title": "Towards a Multiple-Lookahead-Levels agent reinforcement-learning technique and its implementation in integrated circuits", "paper_id": "WOS:000308110300029"}