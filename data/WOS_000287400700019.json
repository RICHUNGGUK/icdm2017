{"auto_keywords": [{"score": 0.04672266571747963, "phrase": "large_number"}, {"score": 0.046363267640327714, "phrase": "inter-related_classifiers"}, {"score": 0.034912196795508985, "phrase": "visual_concept_network"}, {"score": 0.015105298965886738, "phrase": "structured_max-margin_learning_algorithm"}, {"score": 0.014114616198351701, "phrase": "classifier_training"}, {"score": 0.010582786410595867, "phrase": "inter-concept_visual_similarity_contexts"}, {"score": 0.009767533992740808, "phrase": "feature_space"}, {"score": 0.00481495049065317, "phrase": "structured_max-margin"}, {"score": 0.004757087682427765, "phrase": "inter-related_classifier_training_and_multilabel_image_annotation"}, {"score": 0.004406360418456518, "phrase": "multilabel_image_annotation_application"}, {"score": 0.0043533862892678864, "phrase": "leverage_multilabel_images"}, {"score": 0.004266501496765747, "phrase": "multilabel_image"}, {"score": 0.004147756103284082, "phrase": "image_instances"}, {"score": 0.00398380712846857, "phrase": "automatic_instance_label_identification_algorithm"}, {"score": 0.0039042694651106884, "phrase": "multiple_labels"}, {"score": 0.0037955672944969287, "phrase": "image_level"}, {"score": 0.0036898803847106023, "phrase": "k-way_min-max_cut_algorithm"}, {"score": 0.0035726811019151984, "phrase": "kernel_weight_determination"}, {"score": 0.0035296940954990964, "phrase": "multiple_base_kernels"}, {"score": 0.003403801095792972, "phrase": "huge_intra-concept_visual_diversity"}, {"score": 0.003165283275312972, "phrase": "high-dimensional_multimodal_feature_space"}, {"score": 0.0030278017562213265, "phrase": "inter-related_learning_tasks"}, {"score": 0.0029197572138773237, "phrase": "label_space"}, {"score": 0.002861402531078998, "phrase": "common_space"}, {"score": 0.0028155572612699976, "phrase": "image_classification"}, {"score": 0.002759279370360986, "phrase": "parallel_computing_platform"}, {"score": 0.002504374718975704, "phrase": "max-margin_markov_networks"}, {"score": 0.0024149629835339926, "phrase": "huge_inter-concept_visual_similarity"}, {"score": 0.0023287359930308864, "phrase": "inter-related_classifier_training"}, {"score": 0.0021220787047953093, "phrase": "object_classes"}, {"score": 0.0021049977753042253, "phrase": "image_concepts"}], "paper_keywords": ["Inter-concept visual similarity", " intra-concept visual diversity", " parallel computing", " structured max-margin learning", " visual concept network"], "paper_abstract": "In this paper, a structured max-margin learning algorithm is developed to achieve more effective training of a large number of inter-related classifiers for multilabel image annotation application. To leverage multilabel images for classifier training, each multilabel image is partitioned into a set of image instances (image regions or image patches) and an automatic instance label identification algorithm is developed to assign multiple labels (which are given at the image level) to the most relevant image instances. A K-way min-max cut algorithm is developed for automatic instance clustering and kernel weight determination, where multiple base kernels are seamlessly combined to address the issue of huge intra-concept visual diversity more effectively. Second, a visual concept network is constructed for characterizing the inter-concept visual similarity contexts more precisely in the high-dimensional multimodal feature space. The visual concept network is used to determine the inter-related learning tasks directly in the feature space rather than in the label space because feature space is the common space for classifier training and image classification. Third, a parallel computing platform is developed to achieve more effective learning of a large number of inter-related classifiers over the visual concept network. A structured max-margin learning algorithm is developed by incorporating the visual concept network, max-margin Markov networks and multitask learning to address the issue of huge inter-concept visual similarity more effectively. By leveraging the inter-concept visual similarity contexts for inter-related classifier training, our structured max-margin learning algorithm can significantly enhance the discrimination power of the inter-related classifiers. Our experiments have also obtained very positive results for a large number of object classes and image concepts.", "paper_title": "Structured Max-Margin Learning for Inter-Related Classifier Training and Multilabel Image Annotation", "paper_id": "WOS:000287400700019"}