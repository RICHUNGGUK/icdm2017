{"auto_keywords": [{"score": 0.04959024199151092, "phrase": "latent_variable_models"}, {"score": 0.039729415387648515, "phrase": "testing_dataset"}, {"score": 0.00481495049065317, "phrase": "divergence_analysis"}, {"score": 0.004597622380240611, "phrase": "machine_learning"}, {"score": 0.004550669123303854, "phrase": "pattern_recognition"}, {"score": 0.004256895619944631, "phrase": "necessary_and_strict_assumption"}, {"score": 0.004191829338769843, "phrase": "training_samples"}, {"score": 0.0041490029730869345, "phrase": "testing_samples"}, {"score": 0.0039010428828989826, "phrase": "different_domains"}, {"score": 0.0036490529923833884, "phrase": "training_dataset"}, {"score": 0.0033096502887394233, "phrase": "training_model"}, {"score": 0.0030327408069772293, "phrase": "traditional_latent_variable_models"}, {"score": 0.002910545717767559, "phrase": "transfer_learning_framework"}, {"score": 0.0028807718824400697, "phrase": "latent_variable_model"}, {"score": 0.002585906251354904, "phrase": "obtained_latent_variable_model"}, {"score": 0.0022856052476424344, "phrase": "different_datasets"}, {"score": 0.002262209824105154, "phrase": "experimental_results"}, {"score": 0.0021709933368474223, "phrase": "proposed_framework"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Dimensionality reduction", " Latent variable model", " Transfer learning", " Bregman divergence"], "paper_abstract": "Latent variable models are powerful dimensionality reduction approaches in machine learning and pattern recognition. However, this kind of methods only works well under a necessary and strict assumption that the training samples and testing samples are independent and identically distributed. When the samples come from different domains, the distribution of the testing dataset will not be identical with the training dataset. Therefore, the performance of latent variable models will be degraded for the reason that the parameters of the training model do not suit for the testing dataset. This case limits the generalization and application of the traditional latent variable models. To handle this issue, a transfer learning framework for latent variable model is proposed which can utilize the distance (or divergence) of the two datasets to modify the parameters of the obtained latent variable model. So we do not need to rebuild the model and only adjust the parameters according to the divergence, which will adopt different datasets. Experimental results on several real datasets demonstrate the advantages of the proposed framework. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Transfer latent variable model based on divergence analysis", "paper_id": "WOS:000292849000014"}