{"auto_keywords": [{"score": 0.04979001683121731, "phrase": "controlled_redundancy"}, {"score": 0.04195592845616805, "phrase": "feature_selection"}, {"score": 0.034692465572853035, "phrase": "selected_features"}, {"score": 0.00481495049065317, "phrase": "neural_framework"}, {"score": 0.004699778175535453, "phrase": "feature_selection_method"}, {"score": 0.0046432244185370605, "phrase": "multilayer_perceptron"}, {"score": 0.004397000006837371, "phrase": "fsmlp"}, {"score": 0.004357245559948974, "phrase": "essential_features"}, {"score": 0.004317848984770719, "phrase": "derogatory_and_indifferent_features"}, {"score": 0.004051828608343507, "phrase": "general_scheme"}, {"score": 0.003978866233891427, "phrase": "\"controlled_redundancy"}, {"score": 0.0038953958723049287, "phrase": "proposed_scheme"}, {"score": 0.003848484047789749, "phrase": "fsmlp-cor"}, {"score": 0.0036553070575015344, "phrase": "new_more_effective_training_scheme"}, {"score": 0.0036332251283046997, "phrase": "mfsmlp-cor."}, {"score": 0.0033579708256295847, "phrase": "synthetic_data_set"}, {"score": 0.0031129269142224194, "phrase": "linear_dependency"}, {"score": 0.0030291453183733897, "phrase": "nonlinear_measures"}, {"score": 0.00297454367746853, "phrase": "mutual_information"}, {"score": 0.0028595832692215766, "phrase": "proposed_schemes"}, {"score": 0.00280802944821096, "phrase": "explicit_evaluation"}, {"score": 0.0027826011546425366, "phrase": "feature_subsets"}, {"score": 0.0026913130726432645, "phrase": "decision-making_system"}, {"score": 0.002517600815544645, "phrase": "possible_nonlinear_subtle_interactions"}, {"score": 0.0021371774747471033, "phrase": "network's_behavior"}, {"score": 0.0021049977753042253, "phrase": "connection_weights"}], "paper_keywords": ["Dimensionality reduction", " feature selection", " neural network", " redundancy control"], "paper_abstract": "We first present a feature selection method based on a multilayer perceptron (MLP) neural network, called feature selection MLP (FSMLP). We explain how FSMLP can select essential features and discard derogatory and indifferent features. Such a method may pick up some useful but dependent (say correlated) features, all of which may not be needed. We then propose a general scheme for dealing with feature selection with \"controlled redundancy\" (CoR). The proposed scheme, named as FSMLP-CoR, can select features with a controlled redundancy both for classification and function approximation/prediction type problems. We have also proposed a new more effective training scheme named mFSMLP-CoR. The idea is general in nature and can be used with other learning schemes also. We demonstrate the effectiveness of the algorithms using several data sets including a synthetic data set. We also show that the selected features are adequate to solve the problem at hand. Here, we have considered a measure of linear dependency to control the redundancy. The use of nonlinear measures of dependency, such as mutual information, is straightforward. Here, there are some advantages of the proposed schemes. They do not require explicit evaluation of the feature subsets. Here, feature selection is integrated into designing of the decision-making system. Hence, it can look at all features together and pick up whatever is necessary. Our methods can account for possible nonlinear subtle interactions between features, as well as that between features, tools, and the problem being solved. They can also control the level of redundancy in the selected features. Of the two learning schemes, mFSMLP-CoR, not only improves the performance of the system, but also significantly reduces the dependency of the network's behavior on the initialization of connection weights.", "paper_title": "Feature Selection Using a Neural Framework With Controlled Redundancy", "paper_id": "WOS:000348854800004"}