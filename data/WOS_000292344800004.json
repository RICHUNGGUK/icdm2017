{"auto_keywords": [{"score": 0.03888852734951892, "phrase": "kalman"}, {"score": 0.00481495049065317, "phrase": "automatic_analysis"}, {"score": 0.004765482708043842, "phrase": "head_gestures"}, {"score": 0.004716520738331953, "phrase": "facial_expressions"}, {"score": 0.004644014990871423, "phrase": "challenging_research_area"}, {"score": 0.004549063619065006, "phrase": "significant_applications"}, {"score": 0.00450231514359539, "phrase": "human-computer_interfaces"}, {"score": 0.004320055598195391, "phrase": "gesture_detector"}, {"score": 0.004275650416511604, "phrase": "video_streams"}, {"score": 0.004123781267600542, "phrase": "face_landmark_paradigm"}, {"score": 0.0040186038781533946, "phrase": "configuration_information"}, {"score": 0.0037769520261285872, "phrase": "facial_landmarks"}, {"score": 0.0037381083634085424, "phrase": "adaptive_templates"}, {"score": 0.0034771228725942846, "phrase": "facial_landmark_positions"}, {"score": 0.003370907075333389, "phrase": "head_gesture"}, {"score": 0.003336225354214798, "phrase": "facial_expression"}, {"score": 0.0031845067058084583, "phrase": "landmark_coordinate_time_series"}, {"score": 0.003151736708029235, "phrase": "facial_geometric_features"}, {"score": 0.0030872017736888113, "phrase": "expressive_regions"}, {"score": 0.00257586859792573, "phrase": "matrix_factorization"}, {"score": 0.0025230952890994236, "phrase": "spatiotemporal_data"}, {"score": 0.0024333239726872604, "phrase": "seven-gesture_test_database"}, {"score": 0.0023225701080913388, "phrase": "fusion_scheme"}, {"score": 0.002298649369775623, "phrase": "promising_and_competitive_results"}, {"score": 0.0021940118003312397, "phrase": "gesture_clips"}, {"score": 0.0021714122348552747, "phrase": "lim-twotalk_corpus"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Face and head gesture classification", " Facial landmark tracking", " Time series analysis", " Fusion of classifiers"], "paper_abstract": "Automatic analysis of head gestures and facial expressions is a challenging research area and it has significant applications in human-computer interfaces. We develop a face and head gesture detector in video streams. The detector is based on face landmark paradigm in that appearance and configuration information of landmarks are used. First we detect and track accurately facial landmarks using adaptive templates, Kalman predictor and subspace regularization. Then the trajectories (time series) of facial landmark positions during the course of the head gesture or facial expression are converted in various discriminative features. Features can be landmark coordinate time series, facial geometric features or patches on expressive regions of the face. We use comparatively, two feature sequence classifiers, that is, Hidden Markov Models (HMM) and Hidden Conditional Random Fields (HCRF), and various feature subspace classifiers, that is, ICA (Independent Component Analysis) and NMF (Non-negative Matrix Factorization) on the spatiotemporal data. We achieve 87.3% correct gesture classification on a seven-gesture test database, and the performance reaches 98.2% correct detection under a fusion scheme. Promising and competitive results are also achieved on classification of naturally occurring gesture clips of LIM-TwoTalk Corpus. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Robust classification of face and head gestures in video", "paper_id": "WOS:000292344800004"}