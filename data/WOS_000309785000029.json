{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "generalized_gaussian_scale_mixture"}, {"score": 0.04845849110410783, "phrase": "existing_probabilistic_classifiers"}, {"score": 0.039299987230067404, "phrase": "ggsm"}, {"score": 0.0045464538515249085, "phrase": "sparsity-inducing_modeling"}, {"score": 0.0038492126993451337, "phrase": "flexible_probabilistic_model"}, {"score": 0.003592859432554987, "phrase": "appropriate_degree"}, {"score": 0.0032028587883593702, "phrase": "model_learning"}, {"score": 0.0029552551661591988, "phrase": "proposed_model"}, {"score": 0.0026342827320931937, "phrase": "different_types"}, {"score": 0.0025014259919355453, "phrase": "kernel-based_setup"}, {"score": 0.002416589459425828, "phrase": "improved_kernel-based_ggig"}, {"score": 0.0023079225317714815, "phrase": "proposed_method"}, {"score": 0.0022815262950738814, "phrase": "better_or_comparable_performances"}, {"score": 0.0021789194955896124, "phrase": "kernel-based_classification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Classification", " Prior distribution", " Generalized Gaussian scale mixture", " Likelihood function"], "paper_abstract": "Most of the existing probabilistic classifiers are based on sparsity-inducing modeling. However, we show that sparsity is not always desirable in practice, and only an appropriate degree of sparsity is profitable. In this work, we propose a flexible probabilistic model using a generalized Gaussian scale mixture (GGSM) prior that can provide an appropriate degree of sparsity for its model parameters, and yield either sparse or non-sparse estimates according to the intrinsic sparsity of features in a dataset. Model learning is carried out by an efficient modified maximum a posterior (MAP) estimate. We also show relationships of the proposed model to existing probabilistic classifiers as well as iteratively re-weighted l(1) and l(2) minimizations. We then study different types of likelihood working with the GGSM prior in kernel-based setup, based on which an improved kernel-based GGIG is presented. Experiments demonstrate that the proposed method has better or comparable performances in linear classifiers as well as in kernel-based classification. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Probabilistic classifiers with a generalized Gaussian scale mixture prior", "paper_id": "WOS:000309785000029"}