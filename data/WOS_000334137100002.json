{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "shared-memory_parallelism"}, {"score": 0.004719664411221952, "phrase": "multifrontal_solver"}, {"score": 0.00450456611645112, "phrase": "parallel_distributed-memory_solver"}, {"score": 0.004415395176043526, "phrase": "multi-core_architectures"}, {"score": 0.004186105377259855, "phrase": "pure_shared-memory_parallelism"}, {"score": 0.003968675046544738, "phrase": "distributed-memory_parallelism"}, {"score": 0.003813017794036879, "phrase": "deep_redesign"}, {"score": 0.0037624955853577786, "phrase": "fully_benefits"}, {"score": 0.003687960013746783, "phrase": "numerical_kernels"}, {"score": 0.0035669891404642015, "phrase": "original_code"}, {"score": 0.0034730657554172405, "phrase": "performance_models"}, {"score": 0.003404244237372396, "phrase": "coarse-grain_parallelism"}, {"score": 0.0033367819052051995, "phrase": "openmp_environment"}, {"score": 0.00312138530097218, "phrase": "third-party_optimized"}, {"score": 0.0029198522863606953, "phrase": "simple_approaches"}, {"score": 0.002824005136794786, "phrase": "numa_architectures"}, {"score": 0.0027680095380207756, "phrase": "original_optimizations"}, {"score": 0.0027131212233443137, "phrase": "thread_synchronization_costs"}, {"score": 0.0026593184154220123, "phrase": "performance_gains"}, {"score": 0.00255488432062653, "phrase": "test_problems"}, {"score": 0.0024545413637350765, "phrase": "studied_code"}, {"score": 0.002405853977978678, "phrase": "direct_solver"}, {"score": 0.0023739321566540682, "phrase": "sparse_systems"}, {"score": 0.0023424328921363585, "phrase": "linear_equations"}, {"score": 0.0021049977753042253, "phrase": "wider_range"}], "paper_keywords": ["Shared-memory", " Multi-core", " NUMA", " LU factorization", " Sparse matrix", " Multifrontal method"], "paper_abstract": "We introduce shared-memory parallelism in a parallel distributed-memory solver, targeting multi-core architectures. Our concern in this paper is pure shared-memory parallelism, although the work will also impact distributed-memory parallelism. Our approach avoids a deep redesign and fully benefits from the numerical kernels and features of the original code. We use performance models to exploit coarse-grain parallelism in an OpenMP environment while, at the same time, also relying on third-party optimized multithreaded libraries. In this context, we propose simple approaches to take advantage of NUMA architectures, and original optimizations to limit thread synchronization costs. The performance gains are analyzed in detail on test problems from various application areas. Although the studied code is a direct solver for sparse systems of linear equations, the contributions of this paper are more general and could be useful in a wider range of situations.", "paper_title": "A study of shared-memory parallelism in a multifrontal solver", "paper_id": "WOS:000334137100002"}