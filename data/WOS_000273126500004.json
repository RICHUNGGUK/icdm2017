{"auto_keywords": [{"score": 0.04792174815213851, "phrase": "singular_statistical_models"}, {"score": 0.03658660559501354, "phrase": "bayes"}, {"score": 0.0338059442674165, "phrase": "gibbs"}, {"score": 0.00481495049065317, "phrase": "singular_statistical_estimation"}, {"score": 0.004766090327062509, "phrase": "learning_machines"}, {"score": 0.0046937237797352515, "phrase": "hierarchical_structures"}, {"score": 0.004646088032529303, "phrase": "hidden_variables"}, {"score": 0.004238452771031322, "phrase": "bayes_a_posteriori_distribution"}, {"score": 0.004152818376630929, "phrase": "normal_distribution"}, {"score": 0.004068907083697369, "phrase": "maximum_likelihood_estimator"}, {"score": 0.004027586827290042, "phrase": "asymptotic_normality"}, {"score": 0.003926105480202798, "phrase": "main_reason"}, {"score": 0.0037307208089736835, "phrase": "trained_states"}, {"score": 0.0029196116767956273, "phrase": "universal_mathematical_relations"}, {"score": 0.002690406375639144, "phrase": "statistical_estimation"}, {"score": 0.0026091594819333654, "phrase": "true_distribution"}, {"score": 0.0025694584194998356, "phrase": "parametric_model"}, {"score": 0.00240426758768164, "phrase": "bayes_and_gibbs_generalization_errors"}, {"score": 0.0023079225317714815, "phrase": "training_errors"}, {"score": 0.002249672943553135, "phrase": "widely_applicable_information_criteria"}, {"score": 0.0021705796895687864, "phrase": "regular_and_singular_statistical_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": [""], "paper_abstract": "Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models. (C) 2009 Elsevier Ltd. All Fights reserved.", "paper_title": "Equations of states in singular statistical estimation", "paper_id": "WOS:000273126500004"}