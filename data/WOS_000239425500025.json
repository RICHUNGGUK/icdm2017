{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "machine_learning_based_ranking_model"}, {"score": 0.004375061788023814, "phrase": "training_set"}, {"score": 0.004271490815763164, "phrase": "annotated_examples"}, {"score": 0.004023120753104693, "phrase": "relevance_judgments"}, {"score": 0.00374399358035391, "phrase": "document_elements"}, {"score": 0.0034016119212097826, "phrase": "baseline_information"}, {"score": 0.0031277316566463978, "phrase": "ranking_loss_criterion"}, {"score": 0.0025202582748972122, "phrase": "co-focussed_and_co-thourough_tasks"}, {"score": 0.0021049977753042253, "phrase": "structured_information_retrieval"}], "paper_keywords": [""], "paper_abstract": "We present a Machine Learning based ranking model which can automatically learn its parameters using a training set of annotated examples composed of queries and relevance judgments on a subset of the document elements. Our model improves the performance of a baseline Information. Retrieval system by optimizing a ranking loss criterion and combining scores computed from doxels and from their local structural context. We analyze the performance of our algorithm on CO-Focussed and CO-Thourough tasks and compare it to the baseline model which is an adaptation of Okapi to Structured Information Retrieval.", "paper_title": "Machine Learning ranking and INEX'05", "paper_id": "WOS:000239425500025"}