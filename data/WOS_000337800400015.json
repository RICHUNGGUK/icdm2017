{"auto_keywords": [{"score": 0.02851118214106399, "phrase": "adaptive_method"}, {"score": 0.024421252430118813, "phrase": "memory_traffic"}, {"score": 0.00481495049065317, "phrase": "global_history_buffer"}, {"score": 0.004777917315737992, "phrase": "multicore_processors"}, {"score": 0.004668510065740584, "phrase": "well-known_technique"}, {"score": 0.004596960835906736, "phrase": "memory_latency"}, {"score": 0.004544016163767266, "phrase": "last-level_cache"}, {"score": 0.004338239827628188, "phrase": "ghb"}, {"score": 0.0040002088320302935, "phrase": "fixed_value"}, {"score": 0.003923671536980019, "phrase": "prefetch_degree"}, {"score": 0.003702702337768558, "phrase": "new_addresses"}, {"score": 0.0036178264212372497, "phrase": "wrong_addresses"}, {"score": 0.003548579306515579, "phrase": "constant_strides"}, {"score": 0.0034140224493254935, "phrase": "pattern_length"}, {"score": 0.003374655868348244, "phrase": "prefetching_degree"}, {"score": 0.0032972747743006603, "phrase": "aggressive_prefetcher"}, {"score": 0.003147778277286763, "phrase": "variable_pattern_length_mechanism"}, {"score": 0.0028356575042385156, "phrase": "efficient_throttling_procedure"}, {"score": 0.002792119634261638, "phrase": "negative_effects"}, {"score": 0.0027706013216537042, "phrase": "wrong_prefetching"}, {"score": 0.0027386335451725762, "phrase": "new_measure"}, {"score": 0.0027175263159330523, "phrase": "cache_pollution"}, {"score": 0.002644920521740564, "phrase": "cmp_processors"}, {"score": 0.00257424957319189, "phrase": "shared_lcc._simulation"}, {"score": 0.0025347150832077175, "phrase": "mixed_suite"}, {"score": 0.0024957862318504753, "phrase": "floating_point_benchmarks"}, {"score": 0.002429090371567499, "phrase": "single-core_processor"}, {"score": 0.0021544587602892466, "phrase": "multiprogrammed_workloads"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_throttling_methods"}], "paper_keywords": ["Computer architecture", " Cache", " Prefetching", " Multicore processor"], "paper_abstract": "Data prefetching is a well-known technique to hide the memory latency in the last-level cache (LCC). Among many prefetching methods in recent years, the Global History Buffer (GHB) proves to be efficient in terms of cost and speedup. In this paper, we show that a fixed value for detecting patterns and prefetch degree makes GHB to (1) be conservative while there are more opportunities to create new addresses and (2) generate wrong addresses in the presence of constant strides. To resolve these problems, we separate the pattern length from the prefetching degree. The result is an aggressive prefetcher that can generate more addresses with a given pattern length. Furthermore with a variable pattern length mechanism, constant strides are grouped, such that more accurate patterns are detected. As the aggressiveness of this prefetcher is relatively high, we further propose an efficient throttling procedure to reduce the negative effects of wrong prefetching using a new measure of cache pollution. This adaptive method is suitable for CMP processors where the prefetcher resides in the shared LCC. Simulation results with a mixed suite of integer and floating point benchmarks from SPEC CPU2006 show that on a single-core processor both aggressive and adaptive methods outperform existing prefetchers by 48 and 28 %, respectively, while increasing the memory traffic by 20 and 14 %, respectively. Further on an 8-core CMP with a mix of multiprogrammed workloads, the adaptive method outperforms the state-of-the-art throttling methods by 8 % in speedup, while reducing the memory traffic by 3 %.", "paper_title": "Adaptive prefetching using global history buffer in multicore processors", "paper_id": "WOS:000337800400015"}