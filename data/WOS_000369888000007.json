{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "fast_stagewise_algorithms"}, {"score": 0.004773692073488661, "phrase": "forward_stagewise_regression"}, {"score": 0.004592340708764929, "phrase": "sparse_regression_estimates"}, {"score": 0.004231699728933577, "phrase": "small_amount"}, {"score": 0.0040708542958968605, "phrase": "maximal_absolute_inner_product"}, {"score": 0.003899269293218826, "phrase": "interesting_connection"}, {"score": 0.0036395944934310524, "phrase": "forward_stagewise_estimates"}, {"score": 0.0035620354052144656, "phrase": "lasso_path"}, {"score": 0.003501175642882274, "phrase": "step_size"}, {"score": 0.003296212583539166, "phrase": "least_squares_regression"}, {"score": 0.0031982609080278643, "phrase": "differentiable_convex_loss_function"}, {"score": 0.0031032109441552287, "phrase": "stagewise_algorithm"}, {"score": 0.0029980260560551982, "phrase": "maximal_absolute_component"}, {"score": 0.002822434555587263, "phrase": "stagewise_estimates"}, {"score": 0.0027861630854233693, "phrase": "useful_approximation"}, {"score": 0.002680114113899346, "phrase": "sparse_modeling"}, {"score": 0.0023855210798264205, "phrase": "current_paper"}, {"score": 0.0022651818270925704, "phrase": "general_framework"}, {"score": 0.0022457226013538343, "phrase": "stagewise_estimation"}, {"score": 0.002150900082318562, "phrase": "group-structured_learning"}, {"score": 0.0021049977753042253, "phrase": "image_denoising"}], "paper_keywords": ["forward stagewise regression", " lasso", " epsilon-boosting", " regularization paths"], "paper_abstract": "Forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount epsilon) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size epsilon goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an l(1) norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient). Even when they do not match their l(1)-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the l(1) norm and sparsity? The current paper is an attempt to do just this. We present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising,and more.", "paper_title": "A General Framework for Fast Stagewise Algorithms", "paper_id": "WOS:000369888000007"}