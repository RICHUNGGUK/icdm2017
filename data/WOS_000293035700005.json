{"auto_keywords": [{"score": 0.028178716380754497, "phrase": "lac"}, {"score": 0.00481495049065317, "phrase": "popular_machine"}, {"score": 0.0047267100475487595, "phrase": "example_x"}, {"score": 0.004683195250422422, "phrase": "class_c"}, {"score": 0.004349131361255114, "phrase": "calibrated_classifiers"}, {"score": 0.004269391057037737, "phrase": "accurate_estimates"}, {"score": 0.004243135792625782, "phrase": "class_membership_probabilities"}, {"score": 0.0041653307620376535, "phrase": "estimated_probability"}, {"score": 0.0033873366086549735, "phrase": "necessary_property"}, {"score": 0.0033561107040568747, "phrase": "accurate_classifiers"}, {"score": 0.0032240802202623316, "phrase": "direct_accuracy_maximization_strategies"}, {"score": 0.003145417398038261, "phrase": "non-calibrated_classifiers"}, {"score": 0.0029297595441334823, "phrase": "sensible_use"}, {"score": 0.0026704721397631938, "phrase": "lazy_associative_classifiers"}, {"score": 0.002565345470805401, "phrase": "important_applications"}, {"score": 0.002367315484467406, "phrase": "naive_bayes"}, {"score": 0.0023454704375452406, "phrase": "decision_trees"}, {"score": 0.0022740957662393223, "phrase": "additional_highlights"}, {"score": 0.0022185595808679, "phrase": "reliable_predictions"}, {"score": 0.002144400314994328, "phrase": "doubtful_predictions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Classification", " MDL", " Calibration"], "paper_abstract": "Classification is a popular machine learning task. Given an example x and a class c, a classifier usually works by estimating the probability of x being member of c (i.e., membership probability). Well calibrated classifiers are those able to provide accurate estimates of class membership probabilities, that is, the estimated probability (p) over cap (c vertical bar x) is close to p(c vertical bar(p) over cap (c vertical bar x)), which is the true, (unknown) empirical probability of x being member of c given that the probability estimated by the classifier is (p) over cap (c vertical bar x). Calibration is not a necessary property for producing accurate classifiers, and, thus, most of the research has focused on direct accuracy maximization strategies rather than on calibration. However, non-calibrated classifiers are problematic in applications where the reliability associated with a prediction must be taken into account. In these applications, a sensible use of the classifier must be based on the reliability of its predictions, and, thus, the classifier must be well calibrated. In this paper we show that lazy associative classifiers (LAC) are well calibrated using an MM.:based entropy minimization method. We investigate important applications where such characteristics (i.e., accuracy and calibration) are relevant, and we demonstrate empirically that LAC outperforms other classifiers, such as SVMs, Naive Bayes, and Decision Trees (even after these classifiers are calibrated). Additional highlights of LAC include the ability to incorporate reliable predictions for improving training, and the ability to refrain from doubtful predictions. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Calibrated lazy associative classification", "paper_id": "WOS:000293035700005"}