{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "update_scheme"}, {"score": 0.04654333053127705, "phrase": "estimated_optimal_action"}, {"score": 0.029481409740312034, "phrase": "classical_pursuit_algorithms"}, {"score": 0.02778803575261581, "phrase": "proposed_schemes"}, {"score": 0.004769094364165299, "phrase": "state_probability_vector"}, {"score": 0.004634708738155558, "phrase": "la"}, {"score": 0.004517334089033334, "phrase": "pursuit_scheme"}, {"score": 0.004333777641701335, "phrase": "reverse_philosophy"}, {"score": 0.004278807087784626, "phrase": "last-position_elimination-based_learning_automata"}, {"score": 0.004104902932459142, "phrase": "estimated_performance"}, {"score": 0.003912974119795913, "phrase": "active_actions"}, {"score": 0.003826492478418294, "phrase": "nonzero_state_probability"}, {"score": 0.0037658881407605445, "phrase": "penalized_state_probability"}, {"score": 0.0037299855191553, "phrase": "last-position_action"}, {"score": 0.0036592001010420667, "phrase": "proposed_lela"}, {"score": 0.003601235776892337, "phrase": "relaxed_convergence_condition"}, {"score": 0.003566897271386661, "phrase": "optimal_action"}, {"score": 0.003476914643052379, "phrase": "state_probability_update_scheme"}, {"score": 0.0033568708392230544, "phrase": "estimated_nonoptimal_actions"}, {"score": 0.003293142761521425, "phrase": "epsilon-optimal_property"}, {"score": 0.0032617323680009217, "phrase": "proposed_algorithm"}, {"score": 0.0032203158828810244, "phrase": "last-position_elimination"}, {"score": 0.0031895978980118127, "phrase": "widespread_philosophy"}, {"score": 0.0031591719975282073, "phrase": "real_world"}, {"score": 0.0030306348247409703, "phrase": "learning_automaton"}, {"score": 0.002982597613697966, "phrase": "well-known_benchmark_environments"}, {"score": 0.0028887887866517496, "phrase": "lela"}, {"score": 0.002861224098631869, "phrase": "different_selection_strategies"}, {"score": 0.0028339216840405873, "phrase": "last_action"}, {"score": 0.0027185822791428766, "phrase": "dpri"}, {"score": 0.00269263690712392, "phrase": "discretized_generalized_pursuit_algorithm"}, {"score": 0.0026499429068956686, "phrase": "simulation_results"}, {"score": 0.002599600404892265, "phrase": "significantly_faster_convergence"}, {"score": 0.0025830324526904427, "phrase": "higher_accuracy"}, {"score": 0.0025583777916403437, "phrase": "classical_ones"}, {"score": 0.0024620842899061614, "phrase": "best_parameter"}, {"score": 0.002438581238635582, "phrase": "specific_environment"}, {"score": 0.0022656723439661163, "phrase": "practical_case"}, {"score": 0.0022297330866277738, "phrase": "convergence_curves"}, {"score": 0.0022084431489676993, "phrase": "corresponding_variance_coefficient_curves"}, {"score": 0.002125292598116514, "phrase": "analysis_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithms"}], "paper_keywords": ["Last-position elimination", " learning automata (LA)", " stationary environments", " update scheme"], "paper_abstract": "An update scheme of the state probability vector of actions is critical for learning automata (LA). The most popular is the pursuit scheme that pursues the estimated optimal action and penalizes others. This paper proposes a reverse philosophy that leads to last-position elimination-based learning automata (LELA). The action graded last in terms of the estimated performance is penalized by decreasing its state probability and is eliminated when its state probability becomes zero. All active actions, that is, actions with nonzero state probability, equally share the penalized state probability from the last-position action at each iteration. The proposed LELA is characterized by the relaxed convergence condition for the optimal action, the accelerated step size of the state probability update scheme for the estimated optimal action, and the enriched sampling for the estimated nonoptimal actions. The proof of the epsilon-optimal property for the proposed algorithm is presented. Last-position elimination is a widespread philosophy in the real world and has proved to be also helpful for the update scheme of the learning automaton via the simulations of well-known benchmark environments. In the simulations, two versions of the LELA, using different selection strategies of the last action, are compared with the classical pursuit algorithms Discretized Pursuit Reward-Inaction (DPRI) and Discretized Generalized Pursuit Algorithm (DGPA). Simulation results show that the proposed schemes achieve significantly faster convergence and higher accuracy than the classical ones. Specifically, the proposed schemes reduce the interval to find the best parameter for a specific environment in the classical pursuit algorithms. Thus, they can have their parameter tuning easier to perform and can save much more time when applied to a practical case. Furthermore, the convergence curves and the corresponding variance coefficient curves of the contenders are illustrated to characterize their essential differences and verify the analysis results of the proposed algorithms.", "paper_title": "Last-Position Elimination-Based Learning Automata", "paper_id": "WOS:000345629000020"}