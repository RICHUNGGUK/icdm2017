{"auto_keywords": [{"score": 0.04426396296309378, "phrase": "relative_alpha-entropies"}, {"score": 0.012608217541173758, "phrase": "compressed_lengths"}, {"score": 0.00481495049065317, "phrase": "relative_alpha-entropy"}, {"score": 0.0047310525657970615, "phrase": "forward_projection"}, {"score": 0.004675930527604869, "phrase": "minimization_problems"}, {"score": 0.00454090611248773, "phrase": "one-parameter_family"}, {"score": 0.004487989502479992, "phrase": "generalized_relative_entropies"}, {"score": 0.004358368398332723, "phrase": "relative_entropies"}, {"score": 0.0039449104968131655, "phrase": "mismatched_compression"}, {"score": 0.0036553070575015344, "phrase": "parametric_relative_entropies"}, {"score": 0.0035288860880035985, "phrase": "usual_relative_entropy"}, {"score": 0.0033277936960179892, "phrase": "relative_entropy"}, {"score": 0.003212663446866794, "phrase": "squared_euclidean_distance"}, {"score": 0.003138124398958758, "phrase": "pythagorean_property"}, {"score": 0.0030118058266380503, "phrase": "closed_and_convex_sets"}, {"score": 0.0028568174308720167, "phrase": "maximum_renyi"}, {"score": 0.0028235522980914877, "phrase": "tsallis"}, {"score": 0.0028069426877159664, "phrase": "entropy_principle"}, {"score": 0.0027579362569638945, "phrase": "minimizing_probability_distribution"}, {"score": 0.002570296816631395, "phrase": "linear_family"}, {"score": 0.0023953929817357882, "phrase": "statistical_inference"}, {"score": 0.0023674192513753996, "phrase": "namely_subspace_transitivity"}, {"score": 0.0022192891418394535, "phrase": "companion_paper"}, {"score": 0.0021049977753042253, "phrase": "robust_statistics"}], "paper_keywords": ["Best approximant", " exponential family", " information geometry", " Kullback-Leibler divergence", " linear family", " power-law family", " projection", " Pythagorean property", " relative entropy", " Renyi entropy", " Tsallis entropy"], "paper_abstract": "Minimization problems with respect to a one-parameter family of generalized relative entropies are studied. These relative entropies, which we term relative alpha-entropies (denoted I-alpha), arise as redundancies under mismatched compression when cumulants of compressed lengths are considered instead of expected compressed lengths. These parametric relative entropies are a generalization of the usual relative entropy (Kullback-Leibler divergence). Just like relative entropy, these relative alpha-entropies behave like squared Euclidean distance and satisfy the Pythagorean property. Minimizers of these relative alpha-entropies on closed and convex sets are shown to exist. Such minimizations generalize the maximum Renyi or Tsallis entropy principle. The minimizing probability distribution (termed forward I-alpha-projection) for a linear family is shown to obey a power-law. Other results in connection with statistical inference, namely subspace transitivity and iterated projections, are also established. In a companion paper, a related minimization problem of interest in robust statistics that leads to a reverse I-alpha-projection is studied.", "paper_title": "Minimization Problems Based on Relative alpha-Entropy I: Forward Projection", "paper_id": "WOS:000360015900037"}