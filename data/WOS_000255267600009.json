{"auto_keywords": [{"score": 0.04971747610320425, "phrase": "classical_test_theory"}, {"score": 0.041153750933824076, "phrase": "generalizability_theory"}, {"score": 0.00481495049065317, "phrase": "ir_test_collections"}, {"score": 0.004742511243894382, "phrase": "theoretically_derived_reliability_measures"}, {"score": 0.004688894532233354, "phrase": "cronbach's_alpha"}, {"score": 0.00444655284354007, "phrase": "information_retrieval_test_results"}, {"score": 0.004346547997061901, "phrase": "item_analysis"}, {"score": 0.004200724305692705, "phrase": "test's_reliability"}, {"score": 0.0038644521996539466, "phrase": "even_richer_set"}, {"score": 0.0034096661914349577, "phrase": "test's_design"}, {"score": 0.0032087739900195232, "phrase": "test_collection"}, {"score": 0.0030082398041055003, "phrase": "help_test_designers"}, {"score": 0.002874257080906824, "phrase": "test_designs"}, {"score": 0.0028095140358025, "phrase": "relevance_assessors"}, {"score": 0.0026741832921475667, "phrase": "empirical_analysis"}, {"score": 0.0026040262154057607, "phrase": "out-_data"}, {"score": 0.002368120987066921, "phrase": "per-document_performance_measure"}, {"score": 0.002314752262613559, "phrase": "document-set_based_performance_measure"}, {"score": 0.0022284577808744316, "phrase": "implicit_debate"}, {"score": 0.0022115879730677, "phrase": "ir_literature"}, {"score": 0.002153540857619362, "phrase": "relevance_judgments"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["evaluation", " test collections", " test theory", " generalizabilty theory"], "paper_abstract": "Classical test theory offers theoretically derived reliability measures such as Cronbach's alpha, which call be applied to measure the reliability of a set of Information Retrieval test results. The theory also supports item analysis, which identifies queries that are hampering the test's reliability, and which may be candidates for refinement or removal. A generalization of Classical Test Theory, called Generalizability Theory, provides all even richer set of tools. It allows its to estimate the reliability of it test as a function of the number of queries, assessors (relevance judges), and other aspects of the test's design. One novel aspect of Generalizability Theory is that it allows this estimation of reliability even before the test collection exists, based purely oil the numbers of queries and assessors that it will contain. These calculations call help test designers in advance, by allowing them to compare the reliability of test designs with various numbers of queries and relevance assessors, and to spend their limited budgets oil a design that maximizes reliability. Empirical analysis shows that in cases for which out- data is representative, having more queries is more helpful for reliability than having more assessors. It also suggests that reliability may be improved with a per-document performance measure, as opposed to a document-set based performance measure, where appropriate. The theory also clarifies the implicit debate in IR literature regarding the nature of error in relevance judgments. (C) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Test theory for evaluating reliability of IR test collections", "paper_id": "WOS:000255267600009"}