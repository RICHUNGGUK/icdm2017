{"auto_keywords": [{"score": 0.03503844837089729, "phrase": "new_environment"}, {"score": 0.004698392162272106, "phrase": "human_perception"}, {"score": 0.00436530211426514, "phrase": "current_research"}, {"score": 0.004274527415193801, "phrase": "great_challenge"}, {"score": 0.004127394241838696, "phrase": "surrounding_world"}, {"score": 0.0037286231222250065, "phrase": "large_amount"}, {"score": 0.003663857375787215, "phrase": "knowledge_transfer"}, {"score": 0.0035625556447293804, "phrase": "domain_adaptation_problem"}, {"score": 0.0035006643351909246, "phrase": "visual_place_recognition"}, {"score": 0.0033564619316337634, "phrase": "monocular_camera"}, {"score": 0.003263631515158206, "phrase": "traditional_approaches"}, {"score": 0.0032294837250130327, "phrase": "supervised_learning_perform"}, {"score": 0.0031733603553977877, "phrase": "annotated_data"}, {"score": 0.002958423758207523, "phrase": "large_variability"}, {"score": 0.002937745193225725, "phrase": "visual_information"}, {"score": 0.0028464607144683247, "phrase": "novel_transfer_learning_approach"}, {"score": 0.0027100527874262446, "phrase": "different_environments"}, {"score": 0.0025175684590726796, "phrase": "current_scenario"}, {"score": 0.0024307637980373552, "phrase": "transfer_risk_measure"}, {"score": 0.0023469450817019, "phrase": "new_visual_data"}, {"score": 0.002226592967032469, "phrase": "multiple_visual_cues"}, {"score": 0.002149799391146313, "phrase": "proposed_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Autonomous robot navigation", " Visual place recognition", " Domain adaptation", " Unsupervised learning", " Multiple cues"], "paper_abstract": "One of the most impressive characteristics of human perception is its domain adaptation capability. Humans can recognize objects and places simply by transferring knowledge from their past experience. Inspired by that, current research in robotics is addressing a great challenge: building robots able to sense and interpret the surrounding world by reusing information previously collected, gathered by other robots or obtained from the web. But, how can a robot automatically understand what is useful among a large amount of information and perform knowledge transfer? In this paper we address the domain adaptation problem in the context of visual place recognition. We consider the scenario where a robot equipped with a monocular camera explores a new environment. In this situation traditional approaches based on supervised learning perform poorly, as no annotated data are provided in the new environment and the models learned from data collected in other places are inappropriate due to the large variability of visual information. To overcome these problems we introduce a novel transfer learning approach. With our algorithm the robot is given only some training data (annotated images collected in different environments by other robots) and is able to decide whether, and how much, this knowledge is useful in the current scenario. At the base of our approach there is a transfer risk measure which quantifies the similarity between the given and the new visual data. To improve the performance, we also extend our framework to take into account multiple visual cues. Our experiments on three publicly available datasets demonstrate the effectiveness of the proposed approach. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Transferring knowledge across robots: A risk sensitive approach", "paper_id": "WOS:000349724400001"}