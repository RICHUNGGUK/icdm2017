{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "reinforcement_learning"}, {"score": 0.00437302709192313, "phrase": "traditional_value-function"}, {"score": 0.0032231139180761183, "phrase": "desired_action"}, {"score": 0.003146360480030653, "phrase": "reinforcement-learning_community"}, {"score": 0.0029268619875960715, "phrase": "improved_performance"}, {"score": 0.0026792166577171476, "phrase": "focused_learning"}, {"score": 0.0025736694380861604, "phrase": "classification-based_rl."}, {"score": 0.0024524733595572084, "phrase": "useful_notation"}, {"score": 0.0024133533748653033, "phrase": "state_importance"}, {"score": 0.0022812715029149216, "phrase": "rigorous_bounds"}, {"score": 0.002244876254113051, "phrase": "policy_loss"}, {"score": 0.0021049977753042253, "phrase": "classification-based_rl_agent"}], "paper_keywords": ["reinforcement learning", " function approximation", " generalization", " attention"], "paper_abstract": "Classification-based reinforcement learning (RL) methods have recently been proposed as an alternative to the traditional value-function based methods. These methods use a classifier to represent a policy, where the input (features) to the classifier is the state and the output (class label) for that state is the desired action. The reinforcement-learning community knows that focusing on more important states can lead to improved performance. In this paper, we investigate the idea of focused learning in the context of classification-based RL. Specifically, we define a useful notation of state importance, which we use to prove rigorous bounds on policy loss. Furthermore, we show that a classification-based RL agent may behave arbitrarily poorly if it treats all states as equally important.", "paper_title": "Focus of attention in reinforcement learning", "paper_id": "WOS:000253616000007"}