{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "video_event"}, {"score": 0.04934325842114903, "phrase": "web_images"}, {"score": 0.004692227830462433, "phrase": "annotating_events"}, {"score": 0.004644014990871423, "phrase": "uncontrolled_videos"}, {"score": 0.0045726187330434025, "phrase": "challenging_task"}, {"score": 0.004456044928556639, "phrase": "previous_work"}, {"score": 0.004320055598195391, "phrase": "numerous_labeled_videos"}, {"score": 0.0040186038781533946, "phrase": "large_amount"}, {"score": 0.0039772848945723435, "phrase": "required_labeled_videos"}, {"score": 0.003936389070602614, "phrase": "modeling_events"}, {"score": 0.0035866734383189366, "phrase": "abundant_web_images"}, {"score": 0.003513264130359459, "phrase": "rich_source"}, {"score": 0.0031845067058084583, "phrase": "new_discriminative_structural_model"}, {"score": 0.003151736708029235, "phrase": "cross-domain_structural_model"}, {"score": 0.0031193028729331667, "phrase": "cdsm"}, {"score": 0.002931569771634119, "phrase": "consumer_videos"}, {"score": 0.0025892330566358503, "phrase": "common_feature_subspace"}, {"score": 0.0025230952890994236, "phrase": "feature_distribution"}, {"score": 0.002471400498582011, "phrase": "video_domain"}, {"score": 0.0024333239726872604, "phrase": "image_domain"}, {"score": 0.0023346234464901978, "phrase": "weak_semantic_attributes"}, {"score": 0.0021826828321952615, "phrase": "experimental_results"}, {"score": 0.0021601997088758957, "phrase": "challenging_video_datasets"}], "paper_keywords": ["Video annotation", " Knowledge transfer", " Video analysis"], "paper_abstract": "Annotating events in uncontrolled videos is a challenging task. Most of the previous work focuses on obtaining concepts from numerous labeled videos. But it is extremely time consuming and labor expensive to collect a large amount of required labeled videos for modeling events under various circumstances. In this paper, we try to learn models for video event annotation by leveraging abundant Web images which contains a rich source of information with many events taken under various conditions and roughly annotated as well. Our method is based on a new discriminative structural model called Cross-Domain Structural Model (CDSM) to transfer knowledge from Web images (source domain) to consumer videos (target domain), by jointly modeling the interaction between videos and images. Specifically, under this framework we build a common feature subspace to deal with the feature distribution mismatching between the video domain and the image domain. Further, we propose to use weak semantic attributes to describe events, which can be obtained with no or little labor. Experimental results on challenging video datasets demonstrate the effectiveness of our transfer learning method.", "paper_title": "Cross-domain structural model for video event annotation via web images", "paper_id": "WOS:000364493700013"}