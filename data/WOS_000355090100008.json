{"auto_keywords": [{"score": 0.0401247080718564, "phrase": "multivariate_adtree"}, {"score": 0.03724961953707463, "phrase": "proposed_sparse"}, {"score": 0.00481495049065317, "phrase": "decision_tree"}, {"score": 0.004668957916795845, "phrase": "special_decision_tree_representation"}, {"score": 0.004481132747216135, "phrase": "well-established_ensemble_algorithm"}, {"score": 0.004322967486451877, "phrase": "wide_applications"}, {"score": 0.004235095881928715, "phrase": "existing_variants"}, {"score": 0.004191831207705965, "phrase": "adtree"}, {"score": 0.004127753467337283, "phrase": "univariate_decision_nodes"}, {"score": 0.004085579055228594, "phrase": "potential_interactions"}, {"score": 0.003724810609448597, "phrase": "sparse_version"}, {"score": 0.0034308688638878286, "phrase": "uci_datasets"}, {"score": 0.003361069456060196, "phrase": "spectral_datasets"}, {"score": 0.0032926863719985352, "phrase": "uef"}, {"score": 0.003192700856094016, "phrase": "sparse_adtree"}, {"score": 0.0031116963435584982, "phrase": "univariate_decision_trees"}, {"score": 0.0029863450621333415, "phrase": "cart"}, {"score": 0.002940626369102927, "phrase": "multivariate_decision_trees"}, {"score": 0.002910545717767559, "phrase": "fisher's_decision_tree"}, {"score": 0.0028659990472580154, "phrase": "single_multivariate_decision_tree"}, {"score": 0.0028221322455573624, "phrase": "random_forest"}, {"score": 0.0027363971333961967, "phrase": "best_average_rank"}, {"score": 0.0026806884201358515, "phrase": "prediction_accuracy"}, {"score": 0.0025992391074270097, "phrase": "decision_tree_size"}, {"score": 0.0025726416100664853, "phrase": "faster_induction_time"}, {"score": 0.002546315584783112, "phrase": "existing_adtree"}, {"score": 0.002393907896783117, "phrase": "correlated_features"}, {"score": 0.002357249810661667, "phrase": "uef_spectral_datasets"}, {"score": 0.002193447487477984, "phrase": "wider_variety"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Alternating decision tree", " Decision tree", " Boosting", " Sparse discriminant analysis", " Feature selection"], "paper_abstract": "Alternating decision tree (ADTree) is a special decision tree representation that brings interpretability to boosting, a well-established ensemble algorithm. This has found success in wide applications. However, existing variants of ADTree are implementing univariate decision nodes where potential interactions between features are ignored. To date, there has been no multivariate ADTree. We propose a sparse version of multivariate ADTree such that it remains comprehensible. The proposed sparse ADTree is empirically tested on UCI datasets as well as spectral datasets from the University of Eastern Finland (UEF). We show that sparse ADTree is competitive against both univariate decision trees (original ADTree, C4.5, and CART) and multivariate decision trees (Fisher's decision tree and a single multivariate decision tree from oblique Random Forest). It achieves the best average rank in terms of prediction accuracy, second in terms of decision tree size and faster induction time than existing ADTree. In addition, it performs especially well on datasets with correlated features such as UEF spectral datasets. Thus, the proposed sparse ADTree extends the applicability of ADTree to a wider variety of applications. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Sparse alternating decision tree", "paper_id": "WOS:000355090100008"}