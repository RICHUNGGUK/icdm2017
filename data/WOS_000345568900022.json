{"auto_keywords": [{"score": 0.03843856552198327, "phrase": "first_pass"}, {"score": 0.004815275110112491, "phrase": "generic"}, {"score": 0.004641351004344458, "phrase": "large_acoustic_model"}, {"score": 0.004208282550386197, "phrase": "scalable_hardware_accelerator"}, {"score": 0.004157058883925715, "phrase": "speech_recognition"}, {"score": 0.0038624981944755813, "phrase": "viterbi_beam_search"}, {"score": 0.0037921662574137535, "phrase": "observation_probability_calculation"}, {"score": 0.003545024951402821, "phrase": "bigram_language_model"}, {"score": 0.0033961782274326948, "phrase": "word_lattice_output"}, {"score": 0.0031942814007173254, "phrase": "second_pass"}, {"score": 0.003116913644889395, "phrase": "trigram_language_model"}, {"score": 0.0030601162942933665, "phrase": "proposed_design"}, {"score": 0.0030043508031729277, "phrase": "logic-on-memory_approach"}, {"score": 0.002791239714439304, "phrase": "random_read_performance"}, {"score": 0.0027572166867780275, "phrase": "senone_scoring"}, {"score": 0.002723607240517261, "phrase": "first_pass_decoding"}, {"score": 0.0026252106881752067, "phrase": "memory_intensive_operations"}, {"score": 0.0023798115577749225, "phrase": "cmu_sphinx_speech_recognition_software"}, {"score": 0.0022382002476510573, "phrase": "hardware_accelerator"}, {"score": 0.002210903392596343, "phrase": "improved_speech_recognition_accuracy"}, {"score": 0.0021705796895687864, "phrase": "larger_acoustic_models"}, {"score": 0.0021441057555396013, "phrase": "word_dictionaries"}, {"score": 0.0021049977753042253, "phrase": "real-time_performance"}], "paper_keywords": ["Accelerator", " beam search", " embedded", " hardware software co-design", " logic on memory", " multipass decoding", " N-best", " speech recognition", " sphinx"], "paper_abstract": "This paper describes a scalable hardware accelerator for speech recognition, which uses a two pass decoding algorithm with word dependent N-best Viterbi Beam Search. The observation probability calculation (Senone scoring) and first pass of decoding using a Bigram language model is implemented in hardware. The word lattice output from the first pass is used by software for the second pass, with a trigram language model. The proposed design uses a logic-on-memory approach to make use of high bandwidth NOR flash memory to improve random read performance for Senone scoring and first pass decoding, both of which are memory intensive operations. The proposed HW/SW co-design achieves an overall speed up of 4.3X over a 2.4-GHz Intel Core 2 Duo processor running the CMU Sphinx speech recognition software, while consuming an estimated 1.72 W of power. The hardware accelerator provides improved speech recognition accuracy by supporting larger acoustic models and word dictionaries while maintaining real-time performance.", "paper_title": "A Generic and Scalable Architecture for a Large Acoustic Model and Large Vocabulary Speech Recognition Accelerator Using Logic on Memory", "paper_id": "WOS:000345568900022"}