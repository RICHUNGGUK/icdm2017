{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bad_hubs"}, {"score": 0.009170874782450118, "phrase": "proposed_approaches"}, {"score": 0.0047026237903774895, "phrase": "numeric_scale"}, {"score": 0.004299064805926241, "phrase": "social_and_natural_sciences"}, {"score": 0.0038746849003761024, "phrase": "k_nearest_neighbor_regression"}, {"score": 0.003784209997115174, "phrase": "nearest_neighbor_approaches"}, {"score": 0.0033943947596098583, "phrase": "surprisingly_many_other_instances"}, {"score": 0.0033308077968426937, "phrase": "detrimental_effect"}, {"score": 0.0032838979278935814, "phrase": "overall_prediction_performance"}, {"score": 0.003030247482854539, "phrase": "hubness-aware_nearest_neighbor_regression_schemes"}, {"score": 0.002931569771634119, "phrase": "publicly_available_real-world_data-sets"}, {"score": 0.0027307746993761035, "phrase": "knn_regression"}, {"score": 0.002705059527911504, "phrase": "regression_trees"}, {"score": 0.002679585860537941, "phrase": "neural_networks"}, {"score": 0.0025436978068459565, "phrase": "label_noise"}, {"score": 0.002358220053411644, "phrase": "real-world_applications"}, {"score": 0.0022280223816618736, "phrase": "conventional_gaussian_label_noise"}, {"score": 0.002196609505859369, "phrase": "adapted_version"}, {"score": 0.0021656385587456952, "phrase": "recently_proposed_hubness-proportional_random_label_noise"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Nearest neighbor regression", " Hubs", " Intrinsic dimensionality", " Machine learning"], "paper_abstract": "Prediction on a numeric scale, i.e., regression, is one of the most prominent machine learning tasks with various applications in finance, medicine, social and natural sciences. Due to its simplicity, theoretical performance guarantees and successful real-world applications, one of the most popular regression techniques is the k nearest neighbor regression. However, k nearest neighbor approaches are affected by the presence of bad hubs, a recently observed phenomenon according to which some of the instances are similar to surprisingly many other instances and have a detrimental effect on the overall prediction performance. This paper is the first to study bad hubs in context of regression. We propose hubness-aware nearest neighbor regression schemes. We evaluate our approaches on publicly available real-world data-sets from various domains. Our results show that the proposed approaches outperform various other regressions schemes such as kNN regression, regression trees and neural networks. We also evaluate the proposed approaches in the presence of label noise because tolerance to noise is one of the most relevant aspects from the point of view of real-world applications. In particular, we perform experiments under the assumption of conventional Gaussian label noise and an adapted version of the recently proposed hubness-proportional random label noise. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Nearest neighbor regression in the presence of bad hubs", "paper_id": "WOS:000359887400020"}