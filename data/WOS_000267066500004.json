{"auto_keywords": [{"score": 0.03409163909893534, "phrase": "amd"}, {"score": 0.00481495049065317, "phrase": "hpc_global_file_system_performance_analysis"}, {"score": 0.004636027593561925, "phrase": "exponential_growth"}, {"score": 0.004604214511792668, "phrase": "high-fidelity_sensor"}, {"score": 0.004510073211984104, "phrase": "scientific_community"}, {"score": 0.004459831911681321, "phrase": "lustre"}, {"score": 0.00443308765153716, "phrase": "ultrascale_hpc_resources"}, {"score": 0.004152288616186981, "phrase": "balanced_fashion"}, {"score": 0.004095468830028361, "phrase": "architectural_bottleneck"}, {"score": 0.003916098498242888, "phrase": "data-intensive_applications"}, {"score": 0.003889206369317587, "phrase": "realistic_computational_settings"}, {"score": 0.003531474887552889, "phrase": "modern_parallel_file_systems"}, {"score": 0.003483120583663528, "phrase": "broad_range"}, {"score": 0.003459191419478298, "phrase": "system_architectures"}, {"score": 0.003108565704097622, "phrase": "cxfs"}, {"score": 0.002982557900944478, "phrase": "key_parameters"}, {"score": 0.0028715299874549245, "phrase": "shared-file_accesses"}, {"score": 0.0026342827320931937, "phrase": "nine_evaluated_systems"}, {"score": 0.0025625729433234644, "phrase": "experimental_results"}, {"score": 0.002527451565708784, "phrase": "computational_intensity"}, {"score": 0.0024333239726872604, "phrase": "practical_limit"}, {"score": 0.0023670716500937667, "phrase": "vast_differences"}, {"score": 0.002318566141177149, "phrase": "parallel_file_systems"}, {"score": 0.0022015970225369975, "phrase": "system_designers"}, {"score": 0.0021864526544323184, "phrase": "computational_scientists"}, {"score": 0.0021714122348552747, "phrase": "lightweight_tool"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["I/O benchmarking", " Global parallel file system", " Cosmic microwave background", " GPFS", " Lustre", " CXFS", " PVFS2"], "paper_abstract": "With the exponential growth of high-fidelity sensor and simulated data, the scientific community is increasingly reliant on ultrascale HPC resources to handle its data analysis requirements. However, to use such extreme computing power effectively, the I/O components must be designed in a balanced fashion, as any architectural bottleneck will quickly render the platform intolerably inefficient. To understand I/O performance of data-intensive applications in realistic computational settings, we develop a lightweight, portable benchmark called MADbench2, which is derived directly from a large-scale cosmic microwave background (CMB) data analysis package. Our study represents one of the most comprehensive I/O analyses of modern parallel file systems, examining a broad range of system architectures and configurations, including Lustre on the Cray XT3, XT4, and Intel Itanium2 clusters; GPFS on IBM Power5 and AMD Opteron platforms; a BlueGene/P installation using GPFS and PVFS2 file systems; and CXFS on the SGI Altix3700. We present extensive synchronous I/O performance data comparing a number of key parameters including concurrency, POSIX- versus MPI-IO, and unique- versus shared-file accesses, using both the default environment as well as highly tuned I/O parameters. Finally, we explore the potential of asynchronous I/O and show that only the two of the nine evaluated systems benefited from MPI-2's asynchronous MPI-IO. On those systems, experimental results indicate that the computational intensity required to hide I/O effectively is already close to the practical limit of BLAS3 calculations. Overall, our study quantifies vast differences in performance and functionality of parallel file systems across state-of-the-art platforms - showing I/O rates that vary up to 75x on the examined architectures - while providing system designers and computational scientists a lightweight tool for conducting further analysis. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "HPC global file system performance analysis using a scientific-application derived benchmark", "paper_id": "WOS:000267066500004"}