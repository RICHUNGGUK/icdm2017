{"auto_keywords": [{"score": 0.04137808138172336, "phrase": "high-dimensional_parallel"}, {"score": 0.00481495049065317, "phrase": "lower_complexity_bounds"}, {"score": 0.004757227624616643, "phrase": "large-scale_smooth_convex_optimization"}, {"score": 0.004533142516832776, "phrase": "black-box_oracle_complexity"}, {"score": 0.004478782833067256, "phrase": "large-scale_smooth_convex_minimization_problems"}, {"score": 0.004242085112539693, "phrase": "holder_continuous"}, {"score": 0.0034342161742428635, "phrase": "design_dimension_factors"}, {"score": 0.003252542052430779, "phrase": "substantial_extension"}, {"score": 0.0031941384895530426, "phrase": "existing_lower_complexity_bounds"}, {"score": 0.003155784719225786, "phrase": "large-scale_convex_minimization"}, {"score": 0.0030251262102203385, "phrase": "\"euclidean\"_smooth_case"}, {"score": 0.002952900365829518, "phrase": "convex_functions"}, {"score": 0.00291743479583421, "phrase": "lipschitz_continuous_gradients"}, {"score": 0.0028823939507764238, "phrase": "euclidean_balls"}, {"score": 0.0026646374739979694, "phrase": "classical_conditional_gradient_algorithm"}, {"score": 0.002508365161206412, "phrase": "information-based_complexity_theory"}, {"score": 0.002448446849997207, "phrase": "smooth_convex_functions"}, {"score": 0.0021827653135510225, "phrase": "square_matrices"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Smooth convex optimization", " Lower complexity bounds", " Optimal algorithms"], "paper_abstract": "We derive lower bounds on the black-box oracle complexity of large-scale smooth convex minimization problems, with emphasis on minimizing smooth (with Holder continuous, with a given exponent and constant, gradient) convex functions over high-dimensional parallel to . parallel to(p)-balls, 1 <= p <= infinity. Our bounds turn out to be tight (up to logarithmic in the design dimension factors), and can be viewed as a substantial extension of the existing lower complexity bounds for large-scale convex minimization covering the nonsmooth case and the \"Euclidean\" smooth case (minimization of convex functions with Lipschitz continuous gradients over Euclidean balls). As a byproduct of our results, we demonstrate that the classical Conditional Gradient algorithm is near-optimal, in the sense of Information-Based Complexity Theory, when minimizing smooth convex functions over high-dimensional parallel to . parallel to(infinity)-balls and their matrix analogies - spectral norm balls in the spaces of square matrices. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "On lower complexity bounds for large-scale smooth convex optimization", "paper_id": "WOS:000347766600001"}