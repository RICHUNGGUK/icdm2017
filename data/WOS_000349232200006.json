{"auto_keywords": [{"score": 0.04543168546916397, "phrase": "value_function"}, {"score": 0.029420333683948783, "phrase": "different_ways"}, {"score": 0.026172722064646062, "phrase": "user_preferences"}, {"score": 0.00481495049065317, "phrase": "value_functions"}, {"score": 0.004757643419056999, "phrase": "interactive_evolutionary_multiobjective_optimization"}, {"score": 0.004589763483242731, "phrase": "interactive_multiobjective_evolutionary_algorithm"}, {"score": 0.004535127046891651, "phrase": "moea"}, {"score": 0.004271490815763164, "phrase": "users'_true_preferences"}, {"score": 0.004195418010131832, "phrase": "regular_intervals"}, {"score": 0.003975200317233532, "phrase": "single_pair"}, {"score": 0.0037216229812089686, "phrase": "algorithm's_internal_value_function_model"}, {"score": 0.0035473867669654174, "phrase": "subsequent_generations"}, {"score": 0.0031845067058084583, "phrase": "pareto_front"}, {"score": 0.0024901940672239784, "phrase": "different_types"}, {"score": 0.0023592715993204796, "phrase": "learned_value_function"}, {"score": 0.0023171750718791713, "phrase": "moea._results"}, {"score": 0.0022486729928886885, "phrase": "different_scenarios"}, {"score": 0.0021953290937408807, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "benchmark_problems"}], "paper_keywords": ["Evolutionary multiobjective optimization", " interactive procedure", " ordinal regression", " preference learning"], "paper_abstract": "This paper proposes an interactive multiobjective evolutionary algorithm (MOEA) that attempts to learn a value function capturing the users' true preferences. At regular intervals, the user is asked to rank a single pair of solutions. This information is used to update the algorithm's internal value function model, and the model is used in subsequent generations to rank solutions incomparable according to dominance. This speeds up evolution toward the region of the Pareto front that is most desirable to the user. We take into account the most general additive value function as a preference model and we empirically compare different ways to identify the value function that seems to be the most representative with respect to the given preference information, different types of user preferences, and different ways to use the learned value function in the MOEA. Results on a number of different scenarios suggest that the proposed algorithm works well over a range of benchmark problems and types of user preferences.", "paper_title": "Learning Value Functions in Interactive Evolutionary Multiobjective Optimization", "paper_id": "WOS:000349232200006"}