{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "video-realistic_speech_animation"}, {"score": 0.04187714396259639, "phrase": "audio-visual_speech"}, {"score": 0.004709193603747991, "phrase": "coupled_hidden_markov_model"}, {"score": 0.00450456611645112, "phrase": "realistic_facial_animations"}, {"score": 0.004425215632459868, "phrase": "independent_continuous_speech"}, {"score": 0.004347256848088215, "phrase": "hidden_markov_model"}, {"score": 0.004195418010131832, "phrase": "single-state_chain"}, {"score": 0.0040309035566691645, "phrase": "subtle_characteristics"}, {"score": 0.0035908627899097407, "phrase": "expectation_maximization"}, {"score": 0.003527550673371394, "phrase": "conversion_algorithm"}, {"score": 0.0034194194985380268, "phrase": "acoustic_speech"}, {"score": 0.003389136094439662, "phrase": "decent_facial_animation_parameters"}, {"score": 0.003299880298525127, "phrase": "video-realistic_speech_animation_system"}, {"score": 0.0032129675521135616, "phrase": "facial_animation_parameters"}, {"score": 0.0031703706626348507, "phrase": "mouth_animation_sequence"}, {"score": 0.003073154478201771, "phrase": "performance_refinement_process"}, {"score": 0.002992195343569767, "phrase": "animated_mouth"}, {"score": 0.002952516738708181, "phrase": "background_facial_sequence"}, {"score": 0.002861961643289598, "phrase": "animation_performance"}, {"score": 0.002824005977340573, "phrase": "chmm"}, {"score": 0.0027495914857929584, "phrase": "multi-stream_hmms"}, {"score": 0.0027131212233443137, "phrase": "factorial_hmms"}, {"score": 0.002572001496778222, "phrase": "superior_animation_performance"}, {"score": 0.002537880772794665, "phrase": "ph-vi-chmm_system"}, {"score": 0.0024930878032164757, "phrase": "different_state_variables"}, {"score": 0.0023845255359681143, "phrase": "audio_and_visual_modalities"}, {"score": 0.0023113506171584157, "phrase": "proposed_approach"}, {"score": 0.002220551141101661, "phrase": "speech_animation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["speech animation", " audio-to-visual conversion", " talking faces", " facial animation", " coupled hidden Markov models (CHMMs)"], "paper_abstract": "We propose a coupled hidden Markov model (CHMM) approach to video-realistic speech animation, which realizes realistic facial animations driven by speaker independent continuous speech. Different from hidden Markov model (HMM)-based animation approaches that use a single-state chain, we use CHMMs to explicitly model the subtle characteristics of audio-visual speech, e.g., the asynchrony, temporal dependency (synchrony), and different speech classes between the two modalities. We derive an expectation maximization (EM)-based AN conversion algorithm for the CHMMs, which converts acoustic speech into decent facial animation parameters. We also present a video-realistic speech animation system. The system transforms the facial animation parameters to a mouth animation sequence, refines the animation with a performance refinement process, and finally stitches the animated mouth with a background facial sequence seamlessly. We have compared the animation performance of the CHMM with the HMMs, the multi-stream HMMs and the factorial HMMs both objectively and subjectively. Results show that the CHMMs achieve superior animation performance. The ph-vi-CHMM system, which adopts different state variables (phoneme states and viseme states) in the audio and visual modalities, performs the best. The proposed approach indicates that explicitly modelling audio-visual speech is promising for speech animation. (C) 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "A coupled HMM approach to video-realistic speech animation", "paper_id": "WOS:000246534800018"}