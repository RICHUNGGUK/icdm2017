{"auto_keywords": [{"score": 0.0396600464562225, "phrase": "nb"}, {"score": 0.031903748622217835, "phrase": "modl"}, {"score": 0.03176042316480365, "phrase": "fcaim"}, {"score": 0.031546608876331345, "phrase": "caim"}, {"score": 0.007566009445984354, "phrase": "test_instances"}, {"score": 0.00481495049065317, "phrase": "enabling_technique"}, {"score": 0.004770586038104802, "phrase": "semi-naive_bayes-based_classification"}, {"score": 0.004704801005785787, "phrase": "concern_data_sets"}, {"score": 0.004683073921949839, "phrase": "large_and_increasing_size"}, {"score": 0.004661446705304497, "phrase": "scalable_classification_algorithms"}, {"score": 0.004544261922024103, "phrase": "linear_complexity_classifiers"}, {"score": 0.004450568197899267, "phrase": "data_mining_methods"}, {"score": 0.004249188865832993, "phrase": "front-end_discretization"}, {"score": 0.004219774794569897, "phrase": "continuous_features"}, {"score": 0.00414232476071772, "phrase": "nominal_or_discrete_features"}, {"score": 0.003945688181502044, "phrase": "subsequent_classification"}, {"score": 0.003571591012750155, "phrase": "fastest_supervised_discretizers"}, {"score": 0.0033472128817103263, "phrase": "discretization_schemes"}, {"score": 0.003324021697090111, "phrase": "highest_overall_quality"}, {"score": 0.0032629585038738856, "phrase": "classification_accuracy"}, {"score": 0.0031223774204093713, "phrase": "raw_data"}, {"score": 0.0030792503971093506, "phrase": "discretization_algorithm"}, {"score": 0.0029602552900111407, "phrase": "statistically_significant_improvements"}, {"score": 0.002892411527890342, "phrase": "khiops_discretizers"}, {"score": 0.002879029968532794, "phrase": "moderate_improvements"}, {"score": 0.0028261182163214337, "phrase": "averaged_one-dependence_estimators"}, {"score": 0.002780617137719563, "phrase": "aode"}, {"score": 0.002767750940486342, "phrase": "hnb"}, {"score": 0.0026423134393535243, "phrase": "statistically_significantly_better_accuracies"}, {"score": 0.0025638364322646545, "phrase": "fnb"}, {"score": 0.002540160379502092, "phrase": "lazy_bayes"}, {"score": 0.002458999203701833, "phrase": "discretization_scheme"}, {"score": 0.0023694065215394593, "phrase": "entire_process"}, {"score": 0.0022672356020734964, "phrase": "nb-based_classification"}, {"score": 0.0022515101427469683, "phrase": "continuous_instances"}, {"score": 0.0021896891011033105, "phrase": "important_factor"}, {"score": 0.002144437447202989, "phrase": "biggest_positive_influence"}, {"score": 0.0021049977753042253, "phrase": "classification_time"}], "paper_keywords": [""], "paper_abstract": "Current classification problems that concern data sets of large and increasing size require scalable classification algorithms. In this study, we concentrate on several scalable, linear complexity classifiers that include one of the top 10 voted data mining methods, Naive Bayes (NB), and several recently proposed semi-NB classifiers. These algorithms perform front-end discretization of the continuous features since by design they work only with nominal or discrete features. We address the lack of studies that investigate the benefits and drawbacks of discretization in the context of the subsequent classification. Our comprehensive empirical study considers 12 discretizers (two unsupervised and 10 supervised), seven classifiers (two classical NB and five semi-NB), and 16 data sets. We investigate the scalability of the discretizers and show that the fastest supervised discretizers fast class-attribute interdependency maximization (FCAIM), class-attribute interdependency maximization (CAIM), and information entropy maximization (IEM) provide discretization schemes with the highest overall quality. We show that discretization improves the classification accuracy when compared against the two classical methods, NB and Flexible Naive Bayes (FNB), executed on the raw data. The choice of the discretization algorithm impacts the significance of the improvements. The MODL, FCAIM, and CAIM methods provide statistically significant improvements, while the IEM, Class-attribute contingency coefficient (CACC), and Khiops discretizers provide moderate improvements. The most accurate classification models are generated by the Averaged one-dependence estimators (AODEsr) classifier followed by AODE and HNB (Hidden Naive Bayes). AODEsr run on data discretized with MODL, FCAIM, and CAIM provides statistically significantly better accuracies than both the classical NB methods. The worst results are obtained with the NB, FNB, and LBR (Lazy Bayes rule) classifiers. We show that although the time to build the discretization scheme could be longer than the time to train the classifier, the completion of the entire process (to discretize data, compute the classifier, and predict test instances) is often faster than the NB-based classification of the continuous instances. This is because the time to classify test instances is an important factor that is positively influenced by discretization. The biggest positive influence, both on the accuracy and the classification time, is associated with the MODL, FCAIM, and CAIM algorithms.", "paper_title": "Discretization as the enabling technique for the Naive Bayes and semi-Naive Bayes-based classification", "paper_id": "WOS:000284699900003"}