{"auto_keywords": [{"score": 0.04397860317645382, "phrase": "structure_inference"}, {"score": 0.03363896917055258, "phrase": "data_association"}, {"score": 0.00481495049065317, "phrase": "bayesian_multisensory_scene"}, {"score": 0.0044991925146184025, "phrase": "multisensor_scene"}, {"score": 0.004256243266481051, "phrase": "bayesian_model_selection"}, {"score": 0.004076352180903171, "phrase": "multimodal_data"}, {"score": 0.003952513974962462, "phrase": "previous_modeling_work"}, {"score": 0.003832423454781517, "phrase": "optimal_fusion"}, {"score": 0.003625347557965931, "phrase": "machine_perception_systems"}, {"score": 0.003515162360861593, "phrase": "unifying_bayesian_solution"}, {"score": 0.0034720284451902083, "phrase": "multisensory_perception"}, {"score": 0.0032240802202623316, "phrase": "explicit_probabilistic_reasoning"}, {"score": 0.0031260524791486347, "phrase": "temporal_context"}, {"score": 0.0030497740719977835, "phrase": "multimodal_data_association"}, {"score": 0.0029753513692467315, "phrase": "intrinsic_interest"}, {"score": 0.002938821840519756, "phrase": "higher_level_understanding"}, {"score": 0.0029027394930867902, "phrase": "multisensory_data"}, {"score": 0.002762774046280398, "phrase": "probabilistic_implementation"}, {"score": 0.002678734566245688, "phrase": "multiparty_audiovisual_scenario"}, {"score": 0.002629539710839523, "phrase": "unsupervised_learning"}, {"score": 0.002426566352219336, "phrase": "individual_subjects"}, {"score": 0.0023967582707115354, "phrase": "audiovisual_sequences"}, {"score": 0.002323826502178581, "phrase": "structure-inference-based_framework"}, {"score": 0.002225426912273257, "phrase": "theoretical_foundation"}, {"score": 0.0021311850321045767, "phrase": "human_psychophysics_experiments"}, {"score": 0.0021049977753042253, "phrase": "multimodal_cue_integration"}], "paper_keywords": ["Sensor fusion", " audiovisual", " multimodal", " detection", " tracking", " graphical models", " model selection", " Bayesian inference", " speaker association"], "paper_abstract": "We investigate a solution to the problem of multisensor scene understanding by formulating it in the framework of Bayesian model selection and structure inference. Humans robustly associate multimodal data as appropriate, but previous modeling work has focused largely on optimal fusion, leaving segregation unaccounted for and unexploited by machine perception systems. We illustrate a unifying Bayesian solution to multisensory perception and tracking, which accounts for both integration and segregation by explicit probabilistic reasoning about data association in a temporal context. Such an explicit inference of multimodal data association is also of intrinsic interest for higher level understanding of multisensory data. We illustrate this by using a probabilistic implementation of data association in a multiparty audiovisual scenario, where unsupervised learning and structure inference is used to automatically segment, associate, and track individual subjects in audiovisual sequences. Indeed, the structure-inference-based framework introduced in this work provides the theoretical foundation needed to satisfactorily explain many confounding results in human psychophysics experiments involving multimodal cue integration and association.", "paper_title": "Structure Inference for Bayesian Multisensory Scene Understanding", "paper_id": "WOS:000260033900006"}