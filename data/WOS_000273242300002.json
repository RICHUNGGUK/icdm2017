{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "baseline_algorithm"}, {"score": 0.015138649662151085, "phrase": "human_motion"}, {"score": 0.014039250117668788, "phrase": "current_state"}, {"score": 0.004742179945772899, "phrase": "articulated_human_motion"}, {"score": 0.004394424341776717, "phrase": "systematic_quantitative_evaluation"}, {"score": 0.004361077072629878, "phrase": "competing_methods"}, {"score": 0.004118874671285618, "phrase": "hardware_system"}, {"score": 0.004025787455454387, "phrase": "synchronized_video"}, {"score": 0.00394981748347167, "phrase": "resulting_humaneva_datasets"}, {"score": 0.003919830848124645, "phrase": "multiple_subjects"}, {"score": 0.0038458526450015976, "phrase": "predefined_actions"}, {"score": 0.003659954070354848, "phrase": "synchronized_motion"}, {"score": 0.0036183419672924095, "phrase": "multi-view_video"}, {"score": 0.003404244237372396, "phrase": "pure_motion_capture_data"}, {"score": 0.003365529624506815, "phrase": "standard_set"}, {"score": 0.0033399639721674954, "phrase": "error_measures"}, {"score": 0.003036263091666092, "phrase": "relatively_standard_bayesian_framework"}, {"score": 0.002956272941199248, "phrase": "sequential_importance_resampling"}, {"score": 0.0027706507124407686, "phrase": "likelihood_functions"}, {"score": 0.002687366778212309, "phrase": "algorithm_parameters"}, {"score": 0.002636587060621144, "phrase": "image_observation_models"}, {"score": 0.002616544154458438, "phrase": "motion_priors"}, {"score": 0.002596653214530156, "phrase": "important_roles"}, {"score": 0.0025185863209688016, "phrase": "multi-view_laboratory_environment"}, {"score": 0.0024522007854499554, "phrase": "bayesian_filtering"}, {"score": 0.0023069440108381364, "phrase": "research_community"}, {"score": 0.002229043128610864, "phrase": "new_articulated_motion"}, {"score": 0.002203663884543687, "phrase": "estimation_algorithms"}, {"score": 0.0021049977753042253, "phrase": "new_methods"}], "paper_keywords": ["Articulated pose estimation", " Articulated tracking", " Motion capture", " Human tracking", " Datasets and evaluation"], "paper_abstract": "While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HumanEva datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.", "paper_title": "HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion", "paper_id": "WOS:000273242300002"}