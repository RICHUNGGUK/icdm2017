{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "clone_detection_tools"}, {"score": 0.004605748839642513, "phrase": "duplicated_source_code"}, {"score": 0.003465350958971198, "phrase": "space_and_time_requirements"}, {"score": 0.003198705576491802, "phrase": "six_clone_detectors"}, {"score": 0.003114449258490062, "phrase": "eight_large_c_and_java_programs"}, {"score": 0.0026534064006291853, "phrase": "independent_third_party"}, {"score": 0.00258347644721642, "phrase": "selected_techniques"}, {"score": 0.002515384834484012, "phrase": "whole_spectrum"}, {"score": 0.002220551141101659, "phrase": "lexical_and_syntactic_information"}, {"score": 0.0021049977753042253, "phrase": "program_dependency_graphs"}], "paper_keywords": ["code", " duplicated code", " software clones"], "paper_abstract": "Many techniques for detecting duplicated source code (software clones) have been proposed in the past. However, it is not yet clear how these techniques compare in terms of recall and precision as well as space and time requirements. This paper presents an experiment that evaluates six clone detectors based on eight large C and Java programs (altogether almost 850 KLOC). Their clone candidates were evaluated by one of the authors as an independent third party. The selected techniques cover the whole spectrum of the state-of-the-art in clone detection. The techniques work on text, lexical and syntactic information, software metrics, and program dependency graphs.", "paper_title": "Comparison and evaluation of clone detection tools", "paper_id": "WOS:000248885000001"}