{"auto_keywords": [{"score": 0.03992659287798476, "phrase": "mpi"}, {"score": 0.00481495049065317, "phrase": "petascale_spectral_simulations"}, {"score": 0.00478163886605204, "phrase": "transition_analysis"}, {"score": 0.00466684417181804, "phrase": "efficient_parallel_spectral_method"}, {"score": 0.004634552430216732, "phrase": "direct_numerical_simulations"}, {"score": 0.004602483094756597, "phrase": "transitional_and_turbulent_flows"}, {"score": 0.004384098916012568, "phrase": "bidimensional_domain_decomposition"}, {"score": 0.004249188865832993, "phrase": "solenoidal_fourier-chebyshev_spectral_approximation"}, {"score": 0.003936560022538909, "phrase": "classical_libraries"}, {"score": 0.0038957429132649175, "phrase": "cubic_fourier_boxes"}, {"score": 0.0038286491141253584, "phrase": "message-passing_interface"}, {"score": 0.003485744667868367, "phrase": "efficient_hybrid_programming"}, {"score": 0.0034376160364306637, "phrase": "internodes_communications"}, {"score": 0.003401954670986958, "phrase": "coarse_grain_parallelism"}, {"score": 0.0033549790559972053, "phrase": "core_shared-memory_computation"}, {"score": 0.0032971676471625646, "phrase": "classical_hybrid_programming"}, {"score": 0.003240349178289032, "phrase": "fine_granularity_parallelism"}, {"score": 0.0032067276476968032, "phrase": "loop_level"}, {"score": 0.0031845067058084583, "phrase": "openmp_directives"}, {"score": 0.0031514627590375354, "phrase": "hybrid_parallelism"}, {"score": 0.003086396772496527, "phrase": "recent_generation"}, {"score": 0.0030650070965891653, "phrase": "high-performance_parallel_supercomputers"}, {"score": 0.0029193609786176632, "phrase": "different_low-frequency_and_high-frequency_processors_massively_parallel_platforms"}, {"score": 0.0027326751144684386, "phrase": "new_generation"}, {"score": 0.0027137301650680175, "phrase": "high-performance_distributed-memory_computers"}, {"score": 0.002639251247218711, "phrase": "hybrid_programming"}, {"score": 0.0026209522797963447, "phrase": "good_scalability"}, {"score": 0.0025578967205807843, "phrase": "new_numerical_experiments"}, {"score": 0.002505054869094183, "phrase": "petascale_computers"}, {"score": 0.002461852774571558, "phrase": "attractive_features"}, {"score": 0.0024447807655923926, "phrase": "spectral_methods"}, {"score": 0.0024026156525362684, "phrase": "exponential_convergence"}, {"score": 0.0023859534478427313, "phrase": "computational_efficiency"}, {"score": 0.0023694065215394593, "phrase": "conservative_properties"}, {"score": 0.002312388685547026, "phrase": "direct_numerical_simulation"}, {"score": 0.002264607102799332, "phrase": "boundary_layers"}, {"score": 0.002233301265743633, "phrase": "entrance_section"}, {"score": 0.0022101055924375725, "phrase": "plane_channel"}, {"score": 0.0021569129153653777, "phrase": "fully_turbulent_flow"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["spectral method", " high-order methods", " petascale computing", " hybrid parallel programming strategy", " transition"], "paper_abstract": "An efficient parallel spectral method for direct numerical simulations of transitional and turbulent flows is described in this paper. The parallelization is classically based on a bidimensional domain decomposition, but has been specifically developed for a solenoidal Fourier-Chebyshev spectral approximation where in one Fourier direction, the number of modes is very large compared with the two other directions. The approach therefore differs from classical libraries developed for cubic Fourier boxes. The strategy uses message-passing interface (MPI) for message-passing among nodes and is fairly portable. One of the originalities of this paper is the use of an efficient hybrid programming with MPI for internodes communications and a coarse grain parallelism using OpenMP for core shared-memory computation, instead of the classical hybrid programming with MPI and a fine granularity parallelism at the loop level with OpenMP directives. This hybrid parallelism has been tested on the recent generation of high-performance parallel supercomputers involving a few tens of cores per node. Performances are evaluated on different low-frequency and high-frequency processors massively parallel platforms. We demonstrate that spectral methods, which are known to be inherently ill-fitted for the new generation of high-performance distributed-memory computers, can be implemented efficiently using this hybrid programming with good scalability and a very fast wall-clock time per iteration. New numerical experiments are therefore now accessible on petascale computers, while keeping the attractive features of spectral methods such as accuracy, exponential convergence, computational efficiency and conservative properties. This is illustrated by a direct numerical simulation of the transition of the boundary layers developing from the entrance section of a plane channel and interacting to merge into a fully turbulent flow. Copyright (c) 2012 John Wiley & Sons, Ltd.", "paper_title": "Towards petascale spectral simulations for transition analysis in wall bounded flow", "paper_id": "WOS:000319832500001"}