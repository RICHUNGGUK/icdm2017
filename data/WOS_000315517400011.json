{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sum_space"}, {"score": 0.004623224002010226, "phrase": "least_square_regularized_regression_algorithm"}, {"score": 0.004439097815805505, "phrase": "kernel_hilbert_spaces"}, {"score": 0.0042912493195237725, "phrase": "nonflat_function_approximation"}, {"score": 0.0039028497942174777, "phrase": "linear_equations"}, {"score": 0.0037219886365287085, "phrase": "low-and_high-frequency_component"}, {"score": 0.00357362651129581, "phrase": "large_and_small_scale_kernels"}, {"score": 0.0034079696245884073, "phrase": "learning_rate"}, {"score": 0.003036832649117453, "phrase": "covering_number"}, {"score": 0.0028569549040495163, "phrase": "covering_numbers"}, {"score": 0.0028184476518849015, "phrase": "basic_rkhss"}, {"score": 0.002687702958515561, "phrase": "gaussian_kernels"}, {"score": 0.0026157259734105, "phrase": "appropriate_parameters"}, {"score": 0.0025284524249379265, "phrase": "sample_error"}, {"score": 0.002494361792853833, "phrase": "regularization_error"}, {"score": 0.0024111277643627154, "phrase": "polynomial_learning_rate"}, {"score": 0.0022682274656069643, "phrase": "single_rkhs."}, {"score": 0.0021049977753042253, "phrase": "five_real-life_databases"}], "paper_keywords": ["Learning rate", " least square regularized regression (LSRR)", " multiscale kernel", " reproducing kernel Hilbert space (RKHS)", " sum space"], "paper_abstract": "This paper proposes a least square regularized regression algorithm in sum space of reproducing kernel Hilbert spaces (RKHSs) for nonflat function approximation, and obtains the solution of the algorithm by solving a system of linear equations. This algorithm can approximate the low-and high-frequency component of the target function with large and small scale kernels, respectively. The convergence and learning rate are analyzed. We measure the complexity of the sum space by its covering number and demonstrate that the covering number can be bounded by the product of the covering numbers of basic RKHSs. For sum space of RKHSs with Gaussian kernels, by choosing appropriate parameters, we tradeoff the sample error and regularization error, and obtain a polynomial learning rate, which is better than that in any single RKHS. The utility of this method is illustrated with two simulated data sets and five real-life databases.", "paper_title": "Least Square Regularized Regression in Sum Space", "paper_id": "WOS:000315517400011"}