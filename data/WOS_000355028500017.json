{"auto_keywords": [{"score": 0.049118287236398724, "phrase": "labeled_examples"}, {"score": 0.017653135187062224, "phrase": "cle_lmnn"}, {"score": 0.010612392224356166, "phrase": "lmnn"}, {"score": 0.007416124110348042, "phrase": "mahalanobis_distance"}, {"score": 0.00615028582803024, "phrase": "cle"}, {"score": 0.004670722261414736, "phrase": "distance_metric_learning"}, {"score": 0.004395040468458808, "phrase": "task-specific_distance_functions"}, {"score": 0.0040525061916685924, "phrase": "mahalanobis_distance_metric"}, {"score": 0.003971111060168551, "phrase": "nearest_neighbor_classification"}, {"score": 0.0037555742428544096, "phrase": "novel_framework"}, {"score": 0.0036801209922064817, "phrase": "mahalanobis_distance_learning"}, {"score": 0.003569763795153386, "phrase": "recently_proposed_framework"}, {"score": 0.003308085469148074, "phrase": "fine-grained_way"}, {"score": 0.0030655302236416502, "phrase": "fine-grained_learning_way"}, {"score": 0.0028120022457134267, "phrase": "novel_algorithm"}, {"score": 0.0026863605504566924, "phrase": "traditional_unsupervised_clustering_algorithms"}, {"score": 0.0026057256900479026, "phrase": "class_information"}, {"score": 0.002426842369975213, "phrase": "different_subsets"}, {"score": 0.0023301793939902015, "phrase": "extensive_experiments"}, {"score": 0.002271725588755076, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Distance metric learning", " Classification", " Clustering"], "paper_abstract": "Distance metric learning is the task that aims to automate this process of learning task-specific distance functions in a supervised manner. In this paper, we study how to learn a Mahalanobis distance metric that can improve nearest neighbor classification. Our paper makes two contributions. First, we propose a novel framework named CLE_LMNN for Mahalanobis distance learning. CLE_LMNN builds on a recently proposed framework known as large margin nearest neighbor (LMNN) classification. Compared with LMNN, CLE_LMNN learns a Mahalanobis distance in a fine-grained way by first partitioning the labeled examples into subsets. As shown by our experiments, this fine-grained learning way is inclined to obtain a Mahalanobis distance more suitable for classification. Second, we present a novel algorithm named CLE for clustering labeled examples. Different from traditional unsupervised clustering algorithms, CLE fully employ the class information of labeled examples to effectively partition the set of examples with the same class into different subsets. To evaluate our proposed framework, we conduct extensive experiments on three real datasets. The experimental results show the effectiveness of CLE_LMNN when applied to classification. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "CLE_LMNN: A novel framework of LMNN based on clustering labeled examples", "paper_id": "WOS:000355028500017"}