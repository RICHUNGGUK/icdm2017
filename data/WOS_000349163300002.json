{"auto_keywords": [{"score": 0.049636680610423214, "phrase": "large_scale_image_search"}, {"score": 0.00481495049065317, "phrase": "data-independent_codebook"}, {"score": 0.00455413214216076, "phrase": "sift"}, {"score": 0.004408456735066782, "phrase": "large-scale_image_retrieval_applications"}, {"score": 0.004307286039184164, "phrase": "vector_quantization"}, {"score": 0.004247696749118451, "phrase": "crucial_role"}, {"score": 0.004208427318712072, "phrase": "bow_model"}, {"score": 0.00413096972023037, "phrase": "visual_words"}, {"score": 0.004073809704022463, "phrase": "high-dimensional_sift_features"}, {"score": 0.0039252049353279556, "phrase": "inverted_file_structure"}, {"score": 0.0038708810091769856, "phrase": "scalable_retrieval"}, {"score": 0.003644001325962353, "phrase": "visual_codebook_training"}, {"score": 0.003610292749660581, "phrase": "limited_reliability"}, {"score": 0.0033515736929172644, "phrase": "novel_feature_quantization_scheme"}, {"score": 0.003244275497488318, "phrase": "sift_descriptor"}, {"score": 0.003199345207727474, "phrase": "descriptive_and_discriminative_bit-vector"}, {"score": 0.0029838294169759663, "phrase": "image_collections"}, {"score": 0.0028350413546400703, "phrase": "bsift"}, {"score": 0.0028087949095629955, "phrase": "code_word"}, {"score": 0.0027698788736535865, "phrase": "generated_bsift"}, {"score": 0.0026687115458522326, "phrase": "classic_inverted_file_structure"}, {"score": 0.0025712297362596574, "phrase": "quantization_error"}, {"score": 0.002523828967909659, "phrase": "feature_filtering"}, {"score": 0.002500456462028735, "phrase": "code_word_expansion"}, {"score": 0.0024658019116387845, "phrase": "query_sensitive_mask_shielding"}, {"score": 0.002420339995088838, "phrase": "explicit_codebook"}, {"score": 0.0023103100997344072, "phrase": "image_search"}, {"score": 0.002278284823857861, "phrase": "resource-limited_scenarios"}, {"score": 0.0022258906408344973, "phrase": "proposed_algorithm"}, {"score": 0.0021246816854197732, "phrase": "index_efficiency"}, {"score": 0.0021049977753042253, "phrase": "retrieval_accuracy"}], "paper_keywords": ["Large scale image retrieval", " scalar quantization", " binary SIFT", " visual matching", " feature filtering"], "paper_abstract": "Bag-of-Words (BoWs) model based on Scale Invariant Feature Transform (SIFT) has been widely used in large-scale image retrieval applications. Feature quantization by vector quantization plays a crucial role in BoW model, which generates visual words from the high-dimensional SIFT features, so as to adapt to the inverted file structure for the scalable retrieval. Traditional feature quantization approaches suffer several issues, such as necessity of visual codebook training, limited reliability, and update inefficiency. To avoid the above problems, in this paper, a novel feature quantization scheme is proposed to efficiently quantize each SIFT descriptor to a descriptive and discriminative bit-vector, which is called binary SIFT (BSIFT). Our quantizer is independent of image collections. In addition, by taking the first 32 bits out from BSIFT as code word, the generated BSIFT naturally lends itself to adapt to the classic inverted file structure for image indexing. Moreover, the quantization error is reduced by feature filtering, code word expansion, and query sensitive mask shielding. Without any explicit codebook for quantization, our approach can be readily applied in image search in some resource-limited scenarios. We evaluate the proposed algorithm for large scale image search on two public image data sets. Experimental results demonstrate the index efficiency and retrieval accuracy of our approach.", "paper_title": "BSIFT: Toward Data-Independent Codebook for Large Scale Image Search", "paper_id": "WOS:000349163300002"}