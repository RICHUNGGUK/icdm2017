{"auto_keywords": [{"score": 0.04367893340409897, "phrase": "mapreduce_jobs"}, {"score": 0.04013856515910984, "phrase": "hadoop"}, {"score": 0.03746416104347038, "phrase": "task_execution_mechanism"}, {"score": 0.03486952426079635, "phrase": "mapreduce_job"}, {"score": 0.00481495049065317, "phrase": "mapreduce_performance"}, {"score": 0.004775466707014647, "phrase": "job_execution_mechanism"}, {"score": 0.004749323365717064, "phrase": "hadoop_clusters"}, {"score": 0.004697463242638162, "phrase": "widely-used_parallel_computing_framework"}, {"score": 0.004434305003382215, "phrase": "job_execution"}, {"score": 0.004338063990049372, "phrase": "mapreduce"}, {"score": 0.004094859572002327, "phrase": "great_significance"}, {"score": 0.003802135020791801, "phrase": "job_scheduling"}, {"score": 0.003781299948366952, "phrase": "job_parameter_optimization_level"}, {"score": 0.003618662721828278, "phrase": "hadoop_mapreduce_framework"}, {"score": 0.0034345606706788273, "phrase": "mapreduce_framework"}, {"score": 0.003378385493203297, "phrase": "execution_performance"}, {"score": 0.0032867878190398466, "phrase": "task_execution_mechanisms"}, {"score": 0.0031367173902609887, "phrase": "time_cost"}, {"score": 0.003110952512844786, "phrase": "initialization_and_termination_stages"}, {"score": 0.003018273305489126, "phrase": "loose_heartbeat-based_communication_mechanism"}, {"score": 0.0029042887008048717, "phrase": "instant_messaging_communication_mechanism"}, {"score": 0.0028804274673073713, "phrase": "performance-sensitive_task_scheduling"}, {"score": 0.002703869368554536, "phrase": "execution_time_cost"}, {"score": 0.0026596133775123538, "phrase": "short_jobs"}, {"score": 0.0026450225118952713, "phrase": "experimental_results"}, {"score": 0.002601727192945944, "phrase": "standard_hadoop"}, {"score": 0.0025874530719850156, "phrase": "shadoop"}, {"score": 0.002566188216663453, "phrase": "stable_performance_improvement"}, {"score": 0.0025241800390039828, "phrase": "comprehensive_benchmarks"}, {"score": 0.0024489385825401536, "phrase": "production-level_test"}, {"score": 0.0024355300434855936, "phrase": "intel"}, {"score": 0.002389044640563906, "phrase": "intel_distributed_hadoop"}, {"score": 0.002292451418825592, "phrase": "first_effort"}, {"score": 0.002254914147694574, "phrase": "execution_mechanism"}, {"score": 0.0021696949408934857, "phrase": "job_scheduling_optimizations"}, {"score": 0.002140044760360403, "phrase": "job_execution_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Parallel computing", " MapReduce", " Performance optimization", " Distributed processing", " Cloud computing"], "paper_abstract": "As a widely-used parallel computing framework for big data processing today, the Hadoop MapReduce framework puts more emphasis on high-throughput of data than on low-latency of job execution. However, today more and more big data applications developed with MapReduce require quick response time. As a result, improving the performance of MapReduce jobs, especially for short jobs,. is of great significance in practice and has attracted more and more attentions from both academia and industry. A lot of efforts have been made to improve the performance of Hadoop from job scheduling or job parameter optimization level. In this paper, we explore an approach to improve the performance of the Hadoop MapReduce framework by optimizing the job and task execution mechanism. First of all, by analyzing the job and task execution mechanism in MapReduce framework we reveal two critical limitations to job execution performance. Then we propose two major optimizations to the MapReduce job and task execution mechanisms: first, we optimize the setup and cleanup tasks of a MapReduce job to reduce the time cost during the initialization and termination stages of the job; second, instead of adopting the loose heartbeat-based communication mechanism to transmit all messages between the JobTracker and TaskTrackers, we introduce an instant messaging communication mechanism for accelerating performance-sensitive task scheduling and execution. Finally, we implement SHadoop, an optimized and fully compatible version of Hadoop that aims at shortening the execution time cost of MapReduce jobs, especially for short jobs. Experimental results show that compared to the standard Hadoop, SHadoop can achieve stable performance improvement by around 25% on average for comprehensive benchmarks without losing scalability and speedup. Our optimization work has passed a production-level test in Intel and has been integrated into the Intel Distributed Hadoop (IDH). To the best of our knowledge, this work is the first effort that explores on optimizing the execution mechanism inside map/reduce tasks of a job. The advantage is that it can complement job scheduling optimizations to further improve the job execution performance. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "SHadoop: Improving MapReduce performance by optimizing job execution mechanism in Hadoop clusters", "paper_id": "WOS:000331023800003"}