{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "kernel_banach_spaces"}, {"score": 0.03616457540369227, "phrase": "hypothesis_error"}, {"score": 0.004581162468419365, "phrase": "norm_ii"}, {"score": 0.004358676214574977, "phrase": "regularized_least_square_regression"}, {"score": 0.004230382915464492, "phrase": "typical_approach"}, {"score": 0.004065156038847781, "phrase": "learning_rate"}, {"score": 0.003945467978172845, "phrase": "regularized_learning_scheme"}, {"score": 0.003753738096719609, "phrase": "approximation_error"}, {"score": 0.0035358770665604657, "phrase": "sampling_error"}, {"score": 0.0031372369299648203, "phrase": "reproducing_kernel_space"}, {"score": 0.003014576010931602, "phrase": "linear_representer_theorem"}, {"score": 0.0021907299375981356, "phrase": "learning_rate_estimate"}, {"score": 0.0021049977753042253, "phrase": "machine_learning"}], "paper_keywords": [""], "paper_abstract": "A typical approach in estimating the learning rate of a regularized learning scheme is to bound the approximation error by the sum of the sampling error, the hypothesis error, and the regularization error. Using a reproducing kernel space that satisfies the linear representer theorem brings the advantage of discarding the hypothesis error from the sum automatically. Following this direction, we illustrate how reproducing kernel Banach spaces with the l(1) norm can be applied to improve the learning rate estimate of l(1)-regularization in machine learning.", "paper_title": "Reproducing Kernel Banach Spaces with the l(1) Norm II: Error Analysis for Regularized Least Square Regression", "paper_id": "WOS:000294549600009"}