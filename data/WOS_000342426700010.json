{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayesian_hierarchical_reinforcement_learning"}, {"score": 0.04745938608113117, "phrase": "hierarchical_action_decomposition"}, {"score": 0.0360107386851525, "phrase": "lower-level_subtasks"}, {"score": 0.004476882241896009, "phrase": "bayesian"}, {"score": 0.004207230913063179, "phrase": "larger_problems"}, {"score": 0.004044822781630218, "phrase": "partially_observable_semi-markov_decision_process"}, {"score": 0.003910595307634916, "phrase": "main_posmdp_task"}, {"score": 0.0037595947142663997, "phrase": "posmdp_subtasks"}, {"score": 0.0033405704229694656, "phrase": "macro_actions"}, {"score": 0.003303177222901723, "phrase": "higher-level_subtasks"}, {"score": 0.0030017207229983385, "phrase": "formulated_posmdp"}, {"score": 0.002951444412262927, "phrase": "continuous_state_space"}, {"score": 0.002853396755102163, "phrase": "prior_belief"}, {"score": 0.0027898428590137515, "phrase": "approximate_model"}, {"score": 0.002651960559564796, "phrase": "recently_introduced_monte_carlo_value_iteration"}, {"score": 0.0026222551419772867, "phrase": "macro-actions_solver"}, {"score": 0.002535115422089855, "phrase": "monte_carlo"}, {"score": 0.002450864311898833, "phrase": "simulation_results"}, {"score": 0.0023694065215394593, "phrase": "action_hierarchy"}, {"score": 0.0022777800735119405, "phrase": "flat_bayesian_reinforcement"}], "paper_keywords": ["Reinforcement learning", " Bayesian model-based RL", " Bayesian reinforcement learning", " Model-based reinforcement learning", " Partially observable Markov decision process (POMDP)", " Partially observable semi-MDP (POSDMP)"], "paper_abstract": "In this paper, we propose to use hierarchical action decomposition to make Bayesian model-based reinforcement learning more efficient and feasible for larger problems. We formulate Bayesian hierarchical reinforcement learning as a partially observable semi-Markov decision process (POSMDP). The main POSMDP task is partitioned into a hierarchy of POSMDP subtasks. Each subtask might consist of only primitive actions or hierarchically call other subtasks' policies, since the policies of lower-level subtasks are considered as macro actions in higher-level subtasks. A solution for this hierarchical action decomposition is to solve lower-level subtasks first, then higher-level ones. Because each formulated POSMDP has a continuous state space, we sample from a prior belief to build an approximate model for them, then solve by using a recently introduced Monte Carlo Value Iteration with Macro-Actions solver. We name this method Monte Carlo Bayesian Hierarchical Reinforcement Learning. Simulation results show that our algorithm exploiting the action hierarchy performs significantly better than that of flat Bayesian reinforcement learning in terms of both reward, and especially solving time, in at least one order of magnitude.", "paper_title": "Approximate planning for bayesian hierarchical reinforcement learning", "paper_id": "WOS:000342426700010"}