{"auto_keywords": [{"score": 0.0365582565831686, "phrase": "huge_data_sets"}, {"score": 0.00481495049065317, "phrase": "data_mining"}, {"score": 0.0046875099587735825, "phrase": "statistical_machine_learning"}, {"score": 0.004612662942073808, "phrase": "robust_statistics"}, {"score": 0.004165497627464024, "phrase": "massive_data_sets"}, {"score": 0.0040334640350288, "phrase": "data_points"}, {"score": 0.003947765701573287, "phrase": "robust_and_non-parametric_confidence_intervals"}, {"score": 0.003802135020791801, "phrase": "fitted_models"}, {"score": 0.0036815763368083197, "phrase": "simple_but_general_method"}, {"score": 0.003050508020268648, "phrase": "computation_time"}, {"score": 0.002969628818856786, "phrase": "distribution-free_confidence_intervals"}, {"score": 0.0028293966058473476, "phrase": "main_focus"}, {"score": 0.0027841357057119295, "phrase": "general_support_vector_machines"}, {"score": 0.002681315076008658, "phrase": "regularized_risks"}, {"score": 0.0025273383208209922, "phrase": "modem_statistical_machine_learning"}, {"score": 0.0025003054002226944, "phrase": "i.e._kernel_logistic_regression"}, {"score": 0.002473560912156024, "phrase": "epsilon-support_vector_regression"}, {"score": 0.0022213218042340735, "phrase": "robust_estimators"}, {"score": 0.0021975548867457606, "phrase": "parametric_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b._v."}], "paper_keywords": ["breakdown point", " convex risk minimization", " data mining", " distributed computing", " influence function", " logistic regression", " robustness", " scalability", " statistical machine learning", " support vector machine"], "paper_abstract": "Some methods from statistical machine learning and from robust statistics have two drawbacks. Firstly, they are computer-intensive such that they can hardly be used for massive data sets, say with millions of data points. Secondly, robust and non-parametric confidence intervals for the predictions according to the fitted models are often unknown. A simple but general method is proposed to overcome these problems in the context of huge data sets. An implementation of the method is scalable to the memory of the computer and can be distributed on several processors to reduce the computation time. The method offers distribution-free confidence intervals for the median of the predictions. The main focus is on general support vector machines (SVM) based on minimizing regularized risks. As an example, a combination of two methods from modem statistical machine learning, i.e. kernel logistic regression and epsilon-support vector regression, is used to model a data set from several insurance companies. The approach can also be helpful to fit robust estimators in parametric models for huge data sets. (c) 2006 Elsevier B. V. All rights reserved.", "paper_title": "Robust learning from bites for data mining", "paper_id": "WOS:000249912400029"}