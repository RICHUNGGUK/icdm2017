{"auto_keywords": [{"score": 0.044374218223558116, "phrase": "testing_data"}, {"score": 0.037643583038832924, "phrase": "co-occurrence_labels"}, {"score": 0.035704982048606206, "phrase": "training_data"}, {"score": 0.025346851316156833, "phrase": "test_image"}, {"score": 0.00481495049065317, "phrase": "multilabel_image_annotation"}, {"score": 0.004653339900995785, "phrase": "jointly_unsupervised_feature_representation"}, {"score": 0.00448179813172021, "phrase": "specific_labels"}, {"score": 0.004436116555817464, "phrase": "practical_cases"}, {"score": 0.004331320910631647, "phrase": "big_gap"}, {"score": 0.004185874747231179, "phrase": "label_combination"}, {"score": 0.0038695436922618876, "phrase": "semantic_label"}, {"score": 0.0037523473563976246, "phrase": "discriminative_feature_representation"}, {"score": 0.0036015684625793775, "phrase": "semantic_relevance"}, {"score": 0.0034450303951025704, "phrase": "discriminative_representation"}, {"score": 0.0032952735750239924, "phrase": "overlapped_groups"}, {"score": 0.0032728171827367068, "phrase": "graph_shift"}, {"score": 0.0032283609741926445, "phrase": "exclusive_label_graph"}, {"score": 0.003141246279336443, "phrase": "exclusive_labels"}, {"score": 0.003077451642616653, "phrase": "multiple_label-specific_dictionaries"}, {"score": 0.003025277245730244, "phrase": "feature_representation"}, {"score": 0.002963830787280652, "phrase": "joint_optimization_approach"}, {"score": 0.002903628734069691, "phrase": "fisher_discrimination_criterion"}, {"score": 0.002805989498127897, "phrase": "context_information"}, {"score": 0.002720916895509682, "phrase": "semantic_relationship"}, {"score": 0.002702363975375762, "phrase": "visual_words"}, {"score": 0.0026384167183586015, "phrase": "multitask_learning_way"}, {"score": 0.0025936662520897992, "phrase": "reconstruction_coefficients"}, {"score": 0.002532284627202572, "phrase": "annotation_stage"}, {"score": 0.002497862210672172, "phrase": "discriminative_dictionaries"}, {"score": 0.002480826434008026, "phrase": "exclusive_label_groups"}, {"score": 0.0024387423375226507, "phrase": "group_sparsity_constraint"}, {"score": 0.002293051006249274, "phrase": "label_propagation_scheme"}, {"score": 0.0021782980803068505, "phrase": "experimental_results"}, {"score": 0.0021194582289403925, "phrase": "significant_performance_gains"}, {"score": 0.0021049977753042253, "phrase": "existing_methods"}], "paper_keywords": [""], "paper_abstract": "Most existing methods on weakly supervised image annotation rely on jointly unsupervised feature representation, the components of which are not directly correlated with specific labels. In practical cases, however, there is a big gap between the training and the testing data, say the label combination of the testing data is not always consistent with that of the training. To bridge the gap, this paper presents a semantic label embedding dictionary representation that not only achieves the discriminative feature representation for each label in the image, but also mines the semantic relevance between co-occurrence labels for context information. More specifically, to enhance the discriminative representation of labels, the training data is first divided into a set of overlapped groups by graph shift based on the exclusive label graph. Afterward, given a group of exclusive labels, we try to learn multiple label-specific dictionaries to explicitly decorrelate the feature representation of each label. A joint optimization approach is proposed according to the Fisher discrimination criterion for seeking its solution. Then, to discover the context information hidden in the co-occurrence labels, we explore the semantic relationship between visual words in dictionaries and labels in a multitask learning way with respect to the reconstruction coefficients of the training data. In the annotation stage, with the discriminative dictionaries and exclusive label groups as well as a group sparsity constraint, the reconstruction coefficients of a test image can be easily obtained. Finally, we introduce a label propagation scheme to compute the score of each label for the test image based on its reconstruction coefficients. Experimental results on three challenging data sets demonstrate that our proposed method leads to significant performance gains over existing methods.", "paper_title": "SLED: Semantic Label Embedding Dictionary Representation for Multilabel Image Annotation", "paper_id": "WOS:000355210700009"}