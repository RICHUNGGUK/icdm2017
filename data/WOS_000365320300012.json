{"auto_keywords": [{"score": 0.04488170016920937, "phrase": "augmented_system"}, {"score": 0.017916269690444825, "phrase": "reference_trajectory"}, {"score": 0.00481495049065317, "phrase": "unknown_discrete-time_linear_systems"}, {"score": 0.004767261858417823, "phrase": "input-output_measured_data"}, {"score": 0.004604023526673226, "phrase": "output-feedback_solution"}, {"score": 0.004209369306023422, "phrase": "system_dynamics"}, {"score": 0.0041469500929423595, "phrase": "reference_trajectory_dynamics"}, {"score": 0.0038869467872381957, "phrase": "limited_number"}, {"score": 0.0035358770665604657, "phrase": "novel_bellman_equation"}, {"score": 0.0034317185678527672, "phrase": "value_function"}, {"score": 0.0033639840192975835, "phrase": "fixed_policy"}, {"score": 0.0031845067058084583, "phrase": "approximate_dynamic_programming"}, {"score": 0.0031061129850110994, "phrase": "reinforcement_learning_methods"}, {"score": 0.0030600032877646263, "phrase": "lqt_problem"}, {"score": 0.002925728709297618, "phrase": "augmented_system_dynamics"}, {"score": 0.0026745503978347143, "phrase": "policy_iteration"}, {"score": 0.002648215742917402, "phrase": "pi"}, {"score": 0.0026086775334428617, "phrase": "value_iteration"}, {"score": 0.0024941578231560055, "phrase": "optimal_controller"}, {"score": 0.0022799458141165587, "phrase": "proposed_pi"}, {"score": 0.0022573076362326135, "phrase": "vi_algorithms"}, {"score": 0.0022016885701010088, "phrase": "simulation_example"}, {"score": 0.0021049977753042253, "phrase": "proposed_control_scheme"}], "paper_keywords": ["Approximate dynamic programming (ADP)", " linear quadratic tracking (LQT)", " reinforcement learning (RL)"], "paper_abstract": "In this paper, an output-feedback solution to the infinite-horizon linear quadratic tracking (LQT) problem for unknown discrete-time systems is proposed. An augmented system composed of the system dynamics and the reference trajectory dynamics is constructed. The state of the augmented system is constructed from a limited number of measurements of the past input, output, and reference trajectory in the history of the augmented system. A novel Bellman equation is developed that evaluates the value function related to a fixed policy by using only the input, output, and reference trajectory data from the augmented system. By using approximate dynamic programming, a class of reinforcement learning methods, the LQT problem is solved online without requiring knowledge of the augmented system dynamics only by measuring the input, output, and reference trajectory from the augmented system. We develop both policy iteration (PI) and value iteration (VI) algorithms that converge to an optimal controller that require only measuring the input, output, and reference trajectory data. The convergence of the proposed PI and VI algorithms is shown. A simulation example is used to verify the effectiveness of the proposed control scheme.", "paper_title": "Optimal Tracking Control of Unknown Discrete-Time Linear Systems Using Input-Output Measured Data", "paper_id": "WOS:000365320300012"}