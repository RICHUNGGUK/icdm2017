{"auto_keywords": [{"score": 0.04290504428036135, "phrase": "fortran"}, {"score": 0.03672315986539298, "phrase": "java"}, {"score": 0.00481495049065317, "phrase": "computer_science_research"}, {"score": 0.004777214679969785, "phrase": "industry_product_development"}, {"score": 0.0045748683993128425, "phrase": "key_questions"}, {"score": 0.004468117364177126, "phrase": "complex_run-time_tradeoffs"}, {"score": 0.004363846362238565, "phrase": "garbage_collection"}, {"score": 0.004312622975366672, "phrase": "java_programs"}, {"score": 0.00406538572296651, "phrase": "spec"}, {"score": 0.0036987521452843987, "phrase": "evaluation_methodologies"}, {"score": 0.003626626504149897, "phrase": "dacapo_benchmarks"}, {"score": 0.003555902292752528, "phrase": "open_source"}, {"score": 0.0033917213078141373, "phrase": "complex_interactions"}, {"score": 0.0028630535573013686, "phrase": "new_value"}, {"score": 0.002752390133272192, "phrase": "static_and_dynamic_properties"}, {"score": 0.0027200305495826797, "phrase": "code_complexity"}, {"score": 0.0026986685585382347, "phrase": "code_size"}, {"score": 0.0026774738852854427, "phrase": "heap_composition"}, {"score": 0.0026045914281028473, "phrase": "benchmark_suite"}, {"score": 0.0025137856138000014, "phrase": "dacapo"}, {"score": 0.0024842242921857705, "phrase": "spec_java"}, {"score": 0.00238816839721704, "phrase": "richer_object_behaviors"}, {"score": 0.002155412043665635, "phrase": "system_design"}], "paper_keywords": ["measurement", " performance", " methodology", " benchmark", " DaCapo", " Java", " SPEC"], "paper_abstract": "Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex run-time tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.", "paper_title": "The DaCapo benchmarks: Java benchmarking development and analysis", "paper_id": "WOS:000202972500013"}