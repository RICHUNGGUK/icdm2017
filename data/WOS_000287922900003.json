{"auto_keywords": [{"score": 0.048886391350397976, "phrase": "fake_content"}, {"score": 0.00481495049065317, "phrase": "artificial_texts"}, {"score": 0.004754600386388821, "phrase": "statistical_machine_learning_techniques"}, {"score": 0.004435885269134359, "phrase": "basic_random_word_salads"}, {"score": 0.004009894739765119, "phrase": "fake_web_sites"}, {"score": 0.003909923690310641, "phrase": "search_engine_indexes"}, {"score": 0.0037409118365761894, "phrase": "search_engine"}, {"score": 0.0036707250689857348, "phrase": "automatically_generated_texts"}, {"score": 0.0034461093496505127, "phrase": "existing_pages"}, {"score": 0.003194580334752371, "phrase": "natural_texts"}, {"score": 0.0031544754107802413, "phrase": "artificially_generated_ones"}, {"score": 0.003095257144856771, "phrase": "first_method"}, {"score": 0.00305639530600272, "phrase": "basic_lexicometric_features"}, {"score": 0.002999012736727594, "phrase": "second_one"}, {"score": 0.0029613556844064713, "phrase": "standard_language_models"}, {"score": 0.002905752244360772, "phrase": "third_one"}, {"score": 0.002815383690003621, "phrase": "relative_entropy_measure"}, {"score": 0.0027625134766418266, "phrase": "short_range_dependencies"}, {"score": 0.00264296841319955, "phrase": "lexicometric_features"}, {"score": 0.002609770322855842, "phrase": "language_models"}, {"score": 0.0023587312574818208, "phrase": "high_order_markov_models"}, {"score": 0.0022002633747977593, "phrase": "large_corpus"}, {"score": 0.0021049977753042253, "phrase": "\"hard\"_text_generators"}], "paper_keywords": ["Web spam filtering", " Statistical language models", " Artificial languages"], "paper_abstract": "Fake content is flourishing on the Internet, ranging from basic random word salads to web scraping. Most of this fake content is generated for the purpose of nourishing fake web sites aimed at biasing search engine indexes: at the scale of a search engine, using automatically generated texts render such sites harder to detect than using copies of existing pages. In this paper, we present three methods aimed at distinguishing natural texts from artificially generated ones: the first method uses basic lexicometric features, the second one uses standard language models and the third one is based on a relative entropy measure which captures short range dependencies between words. Our experiments show that lexicometric features and language models are efficient to detect most generated texts, but fail to detect texts that are generated with high order Markov models. By comparison our relative entropy scoring algorithm, especially when trained on a large corpus, allows us to detect these \"hard\" text generators with a high degree of accuracy.", "paper_title": "Filtering artificial texts with statistical machine learning techniques", "paper_id": "WOS:000287922900003"}