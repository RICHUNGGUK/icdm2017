{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "multiple_annotators"}, {"score": 0.009135024355747437, "phrase": "annotator_expertise"}, {"score": 0.004777214679969785, "phrase": "varying_expertise"}, {"score": 0.004665764181113345, "phrase": "knowledge_sources"}, {"score": 0.004592905436175987, "phrase": "important_problem"}, {"score": 0.004556901872357751, "phrase": "machine_learning"}, {"score": 0.004521179256835894, "phrase": "data_mining"}, {"score": 0.0041625173012740544, "phrase": "common_goal"}, {"score": 0.004081385289516914, "phrase": "data_source"}, {"score": 0.0040334640350288, "phrase": "additionally_the_need"}, {"score": 0.003908396188878932, "phrase": "collected_information"}, {"score": 0.003772306776343196, "phrase": "probabilistic_approaches"}, {"score": 0.003742711659195878, "phrase": "statistical_learning"}, {"score": 0.00327356326590461, "phrase": "better_knowledge"}, {"score": 0.003247868216333148, "phrase": "different_parts"}, {"score": 0.00320970209394799, "phrase": "input_space"}, {"score": 0.0031100973266506163, "phrase": "task_domain"}, {"score": 0.00290855128461017, "phrase": "classification_and_annotator_models"}, {"score": 0.002807178116892454, "phrase": "true_labels"}, {"score": 0.0026148807877309417, "phrase": "proposed_models"}, {"score": 0.0025638362414466278, "phrase": "related_practical_problems"}, {"score": 0.002242127152093798, "phrase": "real_tasks"}, {"score": 0.0022070310530794097, "phrase": "presented_approaches"}, {"score": 0.0021896891011033088, "phrase": "clear_advantages"}, {"score": 0.0021384748235753425, "phrase": "input-independent_annotator_characteristics"}, {"score": 0.0021049977753042253, "phrase": "alternative_approaches"}], "paper_keywords": ["Multiple labelers", " Crowdsourcing", " Opinion aggregation", " Graphical models", " Classification", " Adversarial annotators"], "paper_abstract": "Learning from multiple annotators or knowledge sources has become an important problem in machine learning and data mining. This is in part due to the ease with which data can now be shared/collected among entities sharing a common goal, task, or data source; and additionally the need to aggregate and make inferences about the collected information. This paper focuses on the development of probabilistic approaches for statistical learning in this setting. It specially considers the case when annotators may be unreliable, but also when their expertise vary depending on the data they observe. That is, annotators may have better knowledge about different parts of the input space and therefore be inconsistently accurate across the task domain. The models developed address both the supervised and the semi-supervised settings and produce classification and annotator models that allow us to provide estimates of the true labels and annotator expertise when no ground-truth is available. In addition, we provide an analysis of the proposed models, tasks, and related practical problems under various scenarios. In particular, we address how to evaluate annotators and how to consider cases where some ground-truth may be available. We show experimentally that annotator expertise can indeed vary in real tasks and that the presented approaches provide clear advantages over previously introduced multi-annotator methods, which only consider input-independent annotator characteristics, and over alternative approaches that do not model multiple annotators.", "paper_title": "Learning from multiple annotators with varying expertise", "paper_id": "WOS:000336034000003"}