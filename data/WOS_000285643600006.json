{"auto_keywords": [{"score": 0.04970673234765248, "phrase": "reinforcement_learning"}, {"score": 0.04563946205873015, "phrase": "optimal_control"}, {"score": 0.042047027839498104, "phrase": "path_integrals"}, {"score": 0.041117082962420635, "phrase": "rl"}, {"score": 0.00481495049065317, "phrase": "generalized_path_integral_control_approach"}, {"score": 0.004613825044585923, "phrase": "higher_efficiency"}, {"score": 0.00440394242852293, "phrase": "classical_techniques"}, {"score": 0.004336115760817672, "phrase": "dynamic_programming"}, {"score": 0.004302593474473502, "phrase": "modern_learning_techniques"}, {"score": 0.004269329235934791, "phrase": "statistical_estimation_theory"}, {"score": 0.004043541331944951, "phrase": "stochastic_optimal_control"}, {"score": 0.003950453462399077, "phrase": "novel_approach"}, {"score": 0.003889583851551197, "phrase": "parameterized_policies"}, {"score": 0.003800026771283179, "phrase": "value_function_estimation"}, {"score": 0.0035850198321277418, "phrase": "approximation_problem"}, {"score": 0.0034753508746887957, "phrase": "open_algorithmic_parameters"}, {"score": 0.0034217766191752628, "phrase": "exploration_noise"}, {"score": 0.0033821367649595254, "phrase": "resulting_algorithm"}, {"score": 0.003069112841132911, "phrase": "learning_problem"}, {"score": 0.003010063382882408, "phrase": "update_equations"}, {"score": 0.002952146666681891, "phrase": "numerical_instabilities"}, {"score": 0.002917931350071647, "phrase": "matrix_inversions"}, {"score": 0.002895341091441627, "phrase": "gradient_learning_rates"}, {"score": 0.002828611294209297, "phrase": "interesting_similarities"}, {"score": 0.0028067105937184954, "phrase": "previous_rl_research"}, {"score": 0.002752695791351096, "phrase": "probability_matching"}, {"score": 0.0026892447164623247, "phrase": "slightly_heuristically_motivated_probability_matching_approach"}, {"score": 0.0026272523687965615, "phrase": "empirical_evaluations"}, {"score": 0.0026069066266394118, "phrase": "significant_performance_improvements"}, {"score": 0.0025867180355950816, "phrase": "gradient-based_policy_learning"}, {"score": 0.0025468074858862964, "phrase": "high-dimensional_control_problems"}, {"score": 0.0024977819584875573, "phrase": "learning_experiment"}, {"score": 0.00238392747425758, "phrase": "complex_robot_learning_scenario"}, {"score": 0.0023380300046435187, "phrase": "policy_improvement"}, {"score": 0.0021049977753042253, "phrase": "trajectory_roll-outs"}], "paper_keywords": ["stochastic optimal control", " reinforcement learning", " parameterized policies"], "paper_abstract": "With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI(2)) offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs.", "paper_title": "A Generalized Path Integral Control Approach to Reinforcement Learning", "paper_id": "WOS:000285643600006"}