{"auto_keywords": [{"score": 0.048482301099111176, "phrase": "wrong_conjectures"}, {"score": 0.015719716506582538, "phrase": "u-shaped_learning"}, {"score": 0.012425481275730671, "phrase": "correct_grammars"}, {"score": 0.011300703767070229, "phrase": "learning_power"}, {"score": 0.004770627232703404, "phrase": "paper_deals"}, {"score": 0.00459735824483322, "phrase": "full_power"}, {"score": 0.004569095351567258, "phrase": "algorithmic_learning"}, {"score": 0.004362564831572539, "phrase": "old_correct_conjectures"}, {"score": 0.00428257928723, "phrase": "classical_models"}, {"score": 0.004191106618363275, "phrase": "positive_data"}, {"score": 0.004013957450249313, "phrase": "correct_grammar"}, {"score": 0.0037621210719690594, "phrase": "target_concept"}, {"score": 0.0035808664384880213, "phrase": "full_learning_power"}, {"score": 0.0033768959341690524, "phrase": "inverted_u-shaped_learning_behaviour"}, {"score": 0.0032945169562318575, "phrase": "old_wrong_conjecture"}, {"score": 0.0032641439000713306, "phrase": "correct_conjecture"}, {"score": 0.0030216505272259884, "phrase": "old_\"overinclusive\"_conjectures"}, {"score": 0.0029570303644956128, "phrase": "target_language"}, {"score": 0.002867098884583832, "phrase": "vacillatory_learning"}, {"score": 0.002797121441432477, "phrase": "finite_number"}, {"score": 0.002565345470805401, "phrase": "old_wrong_conjectures"}, {"score": 0.0024340759772801847, "phrase": "old_overinclusive_conjectures"}, {"score": 0.0023746421791791717, "phrase": "old_overgeneralizing_conjectures"}, {"score": 0.0021576973674470997, "phrase": "old_conjectures-wrong"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": [""], "paper_abstract": "The paper deals with the following problem: is returning to wrong conjectures necessary to achieve full power of algorithmic learning? Returning to wrong conjectures complements the paradigm of U-shaped learning when a learner returns to old correct conjectures. We explore our problem for classical models of learning in the limit from positive data: explanatory learning (when a learner stabilizes in the limit on a correct grammar) and behaviourally correct learning (when a learner stabilizes in the limit on a sequence of correct grammars representing the target concept). In both cases we show that returning to wrong conjectures is necessary to achieve full learning power. In contrast, one can modify learners (without losing learning power) such that they never show inverted U-shaped learning behaviour, that is, never return to old wrong conjecture with a correct conjecture in-between. Furthermore, one can also modify a learner (without losing learning power) such that it does not return to old \"overinclusive\" conjectures containing non-elements of the target language. We also consider our problem in the context of vacillatory learning (when a learner stabilizes on a finite number of correct grammars) and show that each of the following four constraints is restrictive (that is, reduces learning power): the learner does not return to old wrong conjectures; the learner is not inverted U-shaped; the learner does not return to old overinclusive conjectures; the learner does not return to old overgeneralizing conjectures. We also show that learners that are consistent with the input seen so far can be made decisive: on any text, they do not return to any old conjectures-wrong or right. (C) 2006 Elsevier Inc. All rights reserved.", "paper_title": "Variations on U-shaped learning", "paper_id": "WOS:000239670300002"}