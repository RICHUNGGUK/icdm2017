{"auto_keywords": [{"score": 0.04854498731652113, "phrase": "system_identification"}, {"score": 0.013112189542674351, "phrase": "mee_criterion"}, {"score": 0.009682431510265347, "phrase": "discrete_entropy"}, {"score": 0.007715992922858133, "phrase": "delta-entropy_criterion"}, {"score": 0.0048149508482222, "phrase": "delta-entropy"}, {"score": 0.00462291551375938, "phrase": "quantized_data"}, {"score": 0.004529776351030488, "phrase": "minimum_error"}, {"score": 0.00434906528856802, "phrase": "traditional_mean_square_error_criterion"}, {"score": 0.004175533280178052, "phrase": "machine_learning"}, {"score": 0.004141663280057689, "phrase": "signal_processing"}, {"score": 0.0038803710796509227, "phrase": "training_data"}, {"score": 0.003755949722774176, "phrase": "error's_discrete_entropy"}, {"score": 0.003695237613399466, "phrase": "error's_dispersion"}, {"score": 0.003504595508262064, "phrase": "discrete-valued_data_cases"}, {"score": 0.003433909720151841, "phrase": "new_entropy_definition"}, {"score": 0.003392182098020722, "phrase": "discrete_random_variables"}, {"score": 0.0032567057896906314, "phrase": "riemann_sums"}, {"score": 0.0032302647179383915, "phrase": "finite_size_partitions"}, {"score": 0.0031910037920846817, "phrase": "probability_weighted_formula"}, {"score": 0.0031139032290048788, "phrase": "average_partition"}, {"score": 0.003076052206618737, "phrase": "new_entropy"}, {"score": 0.0030386598795433474, "phrase": "important_properties"}, {"score": 0.0030017207229983385, "phrase": "differential_entropy"}, {"score": 0.0028006988338192375, "phrase": "dynamic_range"}, {"score": 0.002677837900473396, "phrase": "superior_optimality_criterion"}, {"score": 0.0026560841114619147, "phrase": "system_identification_problems"}, {"score": 0.002581324549141829, "phrase": "plug-in_estimate"}, {"score": 0.0024281181691350085, "phrase": "m-spacing_based_estimates"}, {"score": 0.0022933216359142736, "phrase": "coarsely_quantized_input-output_data"}, {"score": 0.00224701149477484, "phrase": "optimum_parameter_set"}, {"score": 0.0022287497627555895, "phrase": "monte_carlo_simulations"}, {"score": 0.00220163445285539, "phrase": "performance_improvement"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Delta-Entropy", " Minimum error entropy criterion", " System identification", " Estimation of distribution algorithm"], "paper_abstract": "Recently, the minimum error entropy criterion, an information theoretic alternative to the traditional mean square error criterion, has been successfully used in the contexts of machine learning and signal processing. For system identification, however, the MEE criterion will be no longer suitable if the training data are discrete-valued, since minimizing error's discrete entropy cannot constrain error's dispersion. In this paper, to make the MEE criterion suitable for the discrete-valued data cases, we give a new entropy definition for the discrete random variables, i.e. the Delta-entropy, based on Riemann sums for finite size partitions. A probability weighted formula is established to calculate the average partition. This new entropy retains some important properties of the differential entropy and reduces to discrete entropy under certain conditions. Unlike discrete entropy, the Delta-entropy is sensitive to the dynamic range of the data, and can be used as a superior optimality criterion in system identification problems. Also, we present a plug-in estimate of Delta-entropy, analyze its asymptotic behavior and explore the links to the kernel based and m-spacing based estimates for differential entropy. Finally, the Delta-entropy criterion is applied in system identification with coarsely quantized input-output data to search for the optimum parameter set. Monte Carlo simulations demonstrate the performance improvement that may be achieved with the Delta-entropy criterion. Published by Elsevier Inc.", "paper_title": "Delta-Entropy: Definition, properties and applications in system identification with quantized data", "paper_id": "WOS:000287269600012"}