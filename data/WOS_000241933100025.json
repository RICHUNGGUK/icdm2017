{"auto_keywords": [{"score": 0.042664382141952804, "phrase": "mca"}, {"score": 0.00481495049065317, "phrase": "self-stabilizing_mca_learning_algorithms"}, {"score": 0.004321118442104989, "phrase": "self-stabilizing_learning_algorithms"}, {"score": 0.003247868216333148, "phrase": "weight_vector_length_change"}, {"score": 0.003091767402370757, "phrase": "presented_input_vector"}, {"score": 0.002914288956035066, "phrase": "rigorous_global_convergence_proof"}, {"score": 0.0027741761880779535, "phrase": "convergence_rate"}, {"score": 0.0025638362414466278, "phrase": "positive_properties"}, {"score": 0.002416589459425828, "phrase": "new_learning_algorithm"}], "paper_keywords": ["eigenvector", " feature extraction", " global convergence", " minor component analysis", " neural networks"], "paper_abstract": "In this letter, we propose a class of self-stabilizing learning algorithms for minor component analysis (MCA), which includes a few well-known MCA learning algorithms. Self-stabilizing means that the sign of the weight vector length change is independent of the presented input vector. For these algorithms, rigorous global convergence proof is given and the convergence rate is also discussed. By combining the positive properties of these algorithms, a new learning algorithm is proposed which can improve the performance. Simulations are employed to confirm our theoretical results.", "paper_title": "Class of self-stabilizing MCA learning algorithms", "paper_id": "WOS:000241933100025"}