{"auto_keywords": [{"score": 0.03644451033122438, "phrase": "convergence_speed"}, {"score": 0.00481495049065317, "phrase": "sparse_updates"}, {"score": 0.004556901872357751, "phrase": "recurrent_kernel_algorithm"}, {"score": 0.004501453956464956, "phrase": "selectively_sparse_updates"}, {"score": 0.004446677711302801, "phrase": "online_learning"}, {"score": 0.00428629884026304, "phrase": "linear_recurrent_term"}, {"score": 0.0041316803676012155, "phrase": "current_output"}, {"score": 0.0037459885494311217, "phrase": "recurrent_gradient_term"}, {"score": 0.003545024951402821, "phrase": "recurrent_gradient"}, {"score": 0.0033961782274326948, "phrase": "novel_hybrid_recurrent_training"}, {"score": 0.0031942814007173254, "phrase": "recurrent_information"}, {"score": 0.0030601162942933665, "phrase": "current_training_error"}, {"score": 0.002913650880082273, "phrase": "data-dependent_adaptive_learning_rate"}, {"score": 0.0028430608076324727, "phrase": "guaranteed_system_weight_convergence"}, {"score": 0.002791239714439304, "phrase": "training_iteration"}, {"score": 0.0027403605798830984, "phrase": "learning_rate"}, {"score": 0.00257735013358606, "phrase": "derived_convergence_conditions"}, {"score": 0.002499508839669243, "phrase": "algorithm_updating_process"}, {"score": 0.002453934333239467, "phrase": "theoretical_analyses"}, {"score": 0.002409188797323273, "phrase": "weight_convergence"}, {"score": 0.0023507916951658455, "phrase": "experimental_results"}, {"score": 0.0023079225317714815, "phrase": "good_performance"}, {"score": 0.002265833357217618, "phrase": "proposed_algorithm"}, {"score": 0.002183938718432764, "phrase": "estimation_accuracy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Kernel methods", " Linear recurrent", " Hybrid recurrent training", " Weight convergence"], "paper_abstract": "In this paper, we propose a recurrent kernel algorithm with selectively sparse updates for online learning. The algorithm introduces a linear recurrent term in the estimation of the current output. This makes the past information reusable for updating of the algorithm in the form of a recurrent gradient term. To ensure that the reuse of this recurrent gradient indeed accelerates the convergence speed, a novel hybrid recurrent training is proposed to switch on or off learning the recurrent information according to the magnitude of the current training error. Furthermore, the algorithm includes a data-dependent adaptive learning rate which can provide guaranteed system weight convergence at each training iteration. The learning rate is set as zero when the training violates the derived convergence conditions, which makes the algorithm updating process sparse. Theoretical analyses of the weight convergence are presented and experimental results show the good performance of the proposed algorithm in terms of convergence speed and estimation accuracy. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "A linear recurrent kernel online learning algorithm with sparse updates", "paper_id": "WOS:000330009800013"}