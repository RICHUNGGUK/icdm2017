{"auto_keywords": [{"score": 0.04156226931091792, "phrase": "global_optimization"}, {"score": 0.013826959989438423, "phrase": "temporal_consistency"}, {"score": 0.013390013101296276, "phrase": "depth_videos"}, {"score": 0.004815607935434171, "phrase": "bayesian"}, {"score": 0.004756805752518494, "phrase": "dense_depth_estimation"}, {"score": 0.0046993598473345395, "phrase": "spatial-temporal_correlation"}, {"score": 0.0041784516085055035, "phrase": "main_concerns"}, {"score": 0.004045157344249484, "phrase": "previous_works"}, {"score": 0.003996271202811961, "phrase": "stereo_matching_methods"}, {"score": 0.003916098498242888, "phrase": "accurate_and_dense_depth_videos"}, {"score": 0.0036257863613963245, "phrase": "optimization_procedure"}, {"score": 0.003524322230452833, "phrase": "bayesian_framework"}, {"score": 0.003453584878128719, "phrase": "accurate_and_temporal_consistent_dense_depth_videos"}, {"score": 0.003329804436286295, "phrase": "spatial_and_temporal_correlations"}, {"score": 0.0032629585038738856, "phrase": "different_viewpoints"}, {"score": 0.0030455297429878873, "phrase": "extracted_features"}, {"score": 0.0029126002350975634, "phrase": "initial_depth"}, {"score": 0.002831037222426215, "phrase": "two-stage_decision_method"}, {"score": 0.002751751956416832, "phrase": "depth_value"}, {"score": 0.0027184540871935284, "phrase": "initial_depth_map"}, {"score": 0.002685558057339722, "phrase": "minimum_risk_probability"}, {"score": 0.002599763359352916, "phrase": "improved_graph_cuts_algorithm"}, {"score": 0.002537216271965659, "phrase": "improved_graph_construction_method"}, {"score": 0.0024761702541879213, "phrase": "graph_cuts_algorithm"}, {"score": 0.002311047934798247, "phrase": "experimental_results"}, {"score": 0.0022738201751000865, "phrase": "proposed_algorithm"}, {"score": 0.002246292543825748, "phrase": "accurate_depth_videos"}, {"score": 0.0022281257694320433, "phrase": "higher_efficiency"}, {"score": 0.002183347623351868, "phrase": "traditional_methods"}, {"score": 0.002165688807780663, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Bayesian method", " Depth estimation", " Spatial and temporal correlation", " Depth video", " 3-D video"], "paper_abstract": "Depth video is vital to the representation of the dynamic 3-D video, which is the fundamental for the rapidly growing 3-D video applications. The issues of accuracy and temporal consistency are the main concerns of the researches on depth videos. In previous works, the stereo matching methods with global optimization can generate accurate and dense depth videos. However, the global optimization is computationally intensive, and the temporal consistency is obtained with difficulty in the optimization procedure. In this paper, a Bayesian framework is proposed to generate accurate and temporal consistent dense depth videos in an efficient way. Firstly, the spatial and temporal correlations in 3-D videos from different viewpoints are used to generate the candidates for depth, and these correlations are further measured by extracted features. These features are adopted to estimate the risk of initial depth in our Bayesian framework. Then, a two-stage decision method is designed to select candidates of depth value in the initial depth map with the minimum risk probability. Finally, depth videos are refined by improved graph cuts algorithm with global optimization. An improved graph construction method is designed and applied on graph cuts algorithm to reduce the number of nodes in graph and thus the complexity of global optimization. The experimental results demonstrate that the proposed algorithm can achieve accurate depth videos with higher efficiency up to 68.14% than traditional methods. Crown Copyright (C) 2012 Published by Elsevier B.V. All rights reserved.", "paper_title": "A Bayesian framework for dense depth estimation based on spatial-temporal correlation", "paper_id": "WOS:000316163500001"}