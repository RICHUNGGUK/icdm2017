{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "universal_detector"}, {"score": 0.04937491786272555, "phrase": "small_semantic_gaps"}, {"score": 0.04486536086457028, "phrase": "object_concepts"}, {"score": 0.03274311169285322, "phrase": "unseen_concepts"}, {"score": 0.004779406424192843, "phrase": "mining_concepts"}, {"score": 0.004554650954322244, "phrase": "unseen_objects"}, {"score": 0.00450429688285697, "phrase": "training_exemplars"}, {"score": 0.004260719592945006, "phrase": "human_vocabulary"}, {"score": 0.003941592232278979, "phrase": "training_data"}, {"score": 0.0038263829059336564, "phrase": "visual_variance"}, {"score": 0.0037283301619292636, "phrase": "realistic_small-semantic-gap"}, {"score": 0.0036327808996444904, "phrase": "large-scale_image_corpus"}, {"score": 0.003579322363502143, "phrase": "imagenet"}, {"score": 0.003461758868675955, "phrase": "discovered_ssg_concepts"}, {"score": 0.003385555209074879, "phrase": "visual_models"}, {"score": 0.003323330859027428, "phrase": "reasonably_satisfactory_recognition_accuracies"}, {"score": 0.0032743731084995515, "phrase": "distinctive_visual_models"}, {"score": 0.0032022811145186974, "phrase": "semantic_ontology_knowledge"}, {"score": 0.0031786037839399055, "phrase": "co-occurrence_statistics"}, {"score": 0.003120171161064748, "phrase": "visual_recognition"}, {"score": 0.002951220859491642, "phrase": "real-life_image"}, {"score": 0.0028436862612859896, "phrase": "concept_recognition"}, {"score": 0.0028017745468000587, "phrase": "visual_learning"}, {"score": 0.0027810502577708105, "phrase": "image_exemplars"}, {"score": 0.00267969955652659, "phrase": "unseen_realistic_concepts"}, {"score": 0.00265987582861194, "phrase": "training_samples"}, {"score": 0.002543967677678384, "phrase": "first_research"}, {"score": 0.002497172413754263, "phrase": "semantic_gap_measuring"}, {"score": 0.002469508424323385, "phrase": "large_amount"}, {"score": 0.0024330980793680337, "phrase": "leverage_visually_learnable_concepts"}, {"score": 0.002379484250348599, "phrase": "training_images"}, {"score": 0.0023357085513887365, "phrase": "nus-wide"}, {"score": 0.0022927343007881846, "phrase": "selected_concepts"}, {"score": 0.0021846599115561832, "phrase": "promising_results"}, {"score": 0.0021684903968454267, "phrase": "comparable_accuracy"}, {"score": 0.002152440300818582, "phrase": "preliminary_training-based_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Semantic gap", " Concept detection", " Concept selection", " Concept transfer"], "paper_abstract": "Can we have a universal detector that could visually recognize unseen objects with no training exemplars available? Such a detector is so desirable, as there are hundreds of thousands of object concepts in human vocabulary but few labeled image examples available. In this study, we attempt to build such a universal detector to predict concepts in the absence of training data. First, by considering both semantic relatedness and visual variance, we mine a set of realistic small-semantic-gap (SSG) concepts from a large-scale image corpus, i.e., ImageNet, which comprises 4961 concepts and nearly 4 million images. The discovered SSG concepts can be depicted well by visual models and their detectors can deliver reasonably satisfactory recognition accuracies. From these distinctive visual models, we then leverage the semantic ontology knowledge and co-occurrence statistics of concepts to extend visual recognition to unseen concepts. The rational is that object concepts generally co-occur in a real-life image. Their visual co-occurrence and semantic ontology provide the possibility for concept recognition to transcend the visual learning of image exemplars, and therefore, enable the detector to predict unseen realistic concepts without training samples. To the best of our knowledge, this work presents the first research attempting to substantiate the semantic gap measuring of a large amount of concepts and leverage visually learnable concepts to predicate those with no training images available. Testings on NUS-WIDE dataset demonstrate that the selected concepts with small semantic gaps can be well modeled and the prediction of unseen concepts delivers promising results with comparable accuracy to preliminary training-based methods. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Towards a universal detector by mining concepts with small semantic gaps", "paper_id": "WOS:000305863300103"}