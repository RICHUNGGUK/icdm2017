{"auto_keywords": [{"score": 0.02734982127890264, "phrase": "insvr"}, {"score": 0.015719716506582538, "phrase": "nu-support_vector_regression"}, {"score": 0.009537133748106214, "phrase": "initial_adjustments"}, {"score": 0.004611013257082199, "phrase": "effective_regression_learning_algorithm"}, {"score": 0.004329630571352367, "phrase": "support_vectors"}, {"score": 0.00409748471133883, "phrase": "nu-support_vector_classification"}, {"score": 0.003817137134650275, "phrase": "additional_linear_term"}, {"score": 0.003684213587410217, "phrase": "accurate_on-line_nu-svc_algorithm"}, {"score": 0.0035141291835319682, "phrase": "effective_initial_solution"}, {"score": 0.003445591238183763, "phrase": "main_challenge"}, {"score": 0.0033917213078141373, "phrase": "incremental_nu-svr_learning_algorithm"}, {"score": 0.003260690513595527, "phrase": "special_procedure"}, {"score": 0.0029899137403982027, "phrase": "initial_solution"}, {"score": 0.0029547698888028697, "phrase": "incremental_learning"}, {"score": 0.002851790448644913, "phrase": "aonsvm"}, {"score": 0.00281826552772148, "phrase": "exact_and_effective_incremental_nu-svr_learning_algorithm"}, {"score": 0.0023787690061742566, "phrase": "infeasible_updating_paths"}, {"score": 0.0022777800735119405, "phrase": "optimal_solution"}, {"score": 0.002181069180139225, "phrase": "batch_nu-svr_algorithms"}, {"score": 0.002155412043665635, "phrase": "cold_and_warm_starts"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Incremental learning", " Online learning", " nu-Support Vector Regression", " Support vector machine"], "paper_abstract": "The nu-Support Vector Regression (nu-SVR) is an effective regression learning algorithm, which has the advantage of using a parameter nu on controlling the number of support vectors and adjusting the width of the tube automatically. However, compared to nu-Support Vector Classification (nu-SVC) (Scholkopf et al., 2000), nu-SVR introduces an additional linear term into its objective function. Thus, directly applying the accurate on-line nu-SVC algorithm (AONSVM) to nu-SVR will not generate an effective initial solution. It is the main challenge to design an incremental nu-SVR learning algorithm. To overcome this challenge, we propose a special procedure called initial adjustments in this paper. This procedure adjusts the weights of nu-SVC based on the Karush-Kuhn-Tucker (KKT) conditions to prepare an initial solution for the incremental learning. Combining the initial adjustments with the two steps of AONSVM produces an exact and effective incremental nu-SVR learning algorithm (INSVR). Theoretical analysis has proven the existence of the three key inverse matrices, which are the cornerstones of the three steps of INSVR (including the initial adjustments), respectively. The experiments on benchmark datasets demonstrate that INSVR can avoid the infeasible updating paths as far as possible, and successfully converges to the optimal solution. The results also show that INSVR is faster than batch nu-SVR algorithms with both cold and warm starts. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Incremental learning for nu-Support Vector Regression", "paper_id": "WOS:000356198400012"}