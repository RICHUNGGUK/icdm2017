{"auto_keywords": [{"score": 0.0464374488056209, "phrase": "prior_and_sampling_distributions"}, {"score": 0.00481495049065317, "phrase": "bayesian_reasoning"}, {"score": 0.0047395356621991935, "phrase": "bayesian_model"}, {"score": 0.004568108391981922, "phrase": "probability_distributions"}, {"score": 0.0043797373807446015, "phrase": "wide_range"}, {"score": 0.00433386573276633, "phrase": "fundamental_machine_learning_tasks"}, {"score": 0.003962779958955344, "phrase": "bayesian_models"}, {"score": 0.0038597866217852353, "phrase": "new_probabilistic_programming_abstraction"}, {"score": 0.0037803485166412643, "phrase": "bayesian"}, {"score": 0.003623352647973122, "phrase": "probabilistic_expressions"}, {"score": 0.0033480035183418642, "phrase": "synthetic_data"}, {"score": 0.0031098631445657158, "phrase": "probabilistic_inference"}, {"score": 0.0029038817737006405, "phrase": "generic_programming_pattern"}, {"score": 0.0028734230458881903, "phrase": "model-based_inference"}, {"score": 0.002798662730881037, "phrase": "uniform_expression"}, {"score": 0.002769304577201566, "phrase": "common_tasks"}, {"score": 0.002740253546178098, "phrase": "model_testing"}, {"score": 0.002640947693258573, "phrase": "mixture_models"}, {"score": 0.002613239644974825, "phrase": "evidence-based_model_averaging"}, {"score": 0.0025052769325080255, "phrase": "formal_semantics"}, {"score": 0.0024529757621396717, "phrase": "model_equivalence"}, {"score": 0.0024272350606639147, "phrase": "implementation_correctness"}, {"score": 0.0022783523466965187, "phrase": "exact_inference"}, {"score": 0.0022544399432457164, "phrase": "factor_graphs"}, {"score": 0.0022196605202113163, "phrase": "markov"}, {"score": 0.0021957487795963666, "phrase": "monte_carlo"}, {"score": 0.0021385822825864425, "phrase": "broad_applicability"}, {"score": 0.0021049977753042253, "phrase": "new_programming_pattern"}], "paper_keywords": ["Bayesian reasoning", " machine learning", " model-learner pattern", " probabilistic programming"], "paper_abstract": "A Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for model-based inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidence-based model averaging, and mixtures of experts. A formal semantics supports reasoning about model equivalence and implementation correctness. By developing a series of examples and three learner implementations based on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applicability of this new programming pattern.", "paper_title": "A Model-Learner Pattern for Bayesian Reasoning", "paper_id": "WOS:000318629900035"}