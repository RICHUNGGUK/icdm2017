{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "distributed"}, {"score": 0.004740009255053372, "phrase": "random_vector_functional-link_networks"}, {"score": 0.004569630374630379, "phrase": "distributed_learning_algorithms"}, {"score": 0.004522080178250592, "phrase": "random_vector_functional-link"}, {"score": 0.004359500411441827, "phrase": "training_data"}, {"score": 0.004246947739596565, "phrase": "decentralized_information_structure"}, {"score": 0.0040942189695983185, "phrase": "decentralized_average_consensus"}, {"score": 0.003988487841970088, "phrase": "alternating_direction_method_of_multipliers"}, {"score": 0.0037261429556468217, "phrase": "fully_distributed_fashion"}, {"score": 0.003554661425339528, "phrase": "central_agent"}, {"score": 0.0034992667091713813, "phrase": "learning_process"}, {"score": 0.003444732262403694, "phrase": "distributed_learning"}, {"score": 0.003303412571939192, "phrase": "common_learner_model"}, {"score": 0.003234934827553253, "phrase": "system_performance"}, {"score": 0.0031845067058084583, "phrase": "whole_set"}, {"score": 0.0031513240597392843, "phrase": "local_data"}, {"score": 0.0029284999172756103, "phrase": "initial_weights"}, {"score": 0.0028828348726682965, "phrase": "input_layer"}, {"score": 0.0028378798711397235, "phrase": "output_weights"}, {"score": 0.002808298910480358, "phrase": "local_rvfl_networks"}, {"score": 0.0027356847952125433, "phrase": "communication_channels"}, {"score": 0.0027071661844502992, "phrase": "neighboring_nodes"}, {"score": 0.002651015392318126, "phrase": "local_datasets"}, {"score": 0.00256895975568914, "phrase": "proposed_learning_algorithms"}, {"score": 0.0025156683465194967, "phrase": "five_benchmark_datasets"}, {"score": 0.0024894376091589244, "phrase": "experimental_results"}, {"score": 0.002412371116984224, "phrase": "dac-based_learning_algorithm"}, {"score": 0.002289180040240783, "phrase": "computational_complexity"}, {"score": 0.0022299587520401747, "phrase": "admm-based_learning_algorithm"}, {"score": 0.002172266188075892, "phrase": "higher_computational_burden"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Random Vector Functional-Link", " Distributed learning", " Consensus", " Distributed Optimization"], "paper_abstract": "This paper aims to develop distributed learning algorithms for Random Vector Functional-Link (RVFL) networks, where training data is distributed under a decentralized information structure. Two algorithms are proposed by using Decentralized Average Consensus (DAC) and Alternating Direction Method of Multipliers (ADMM) strategies, respectively. These algorithms work in a fully distributed fashion and have no requirement on coordination from a central agent during the learning process. For distributed learning, the goal is to build a common learner model which optimizes the system performance over the whole set of local data. In this work, it is assumed that all stations know the initial weights of the input layer, the output weights of local RVFL networks can be shared through communication channels among neighboring nodes only, and local datasets are blocked strictly. The proposed learning algorithms are evaluated over five benchmark datasets. Experimental results with comparisons show that the DAC-based learning algorithm performs favorably in terms of effectiveness, efficiency and computational complexity, followed by the ADMM-based learning algorithm with promising accuracy but higher computational burden. (c) 2015 Elsevier Inc. All rights reserved.", "paper_title": "Distributed learning for Random Vector Functional-Link networks", "paper_id": "WOS:000350929100018"}