{"auto_keywords": [{"score": 0.028498454462912447, "phrase": "cholesky"}, {"score": 0.007530146668965983, "phrase": "factorization_routines"}, {"score": 0.006242409737331162, "phrase": "sixteen_spes"}, {"score": 0.005772910227059537, "phrase": "svd"}, {"score": 0.00481495049065317, "phrase": "parallel_matrix_factorization_library"}, {"score": 0.0047587234712024775, "phrase": "cell_broadband_engine"}, {"score": 0.004721602309430702, "phrase": "matrix_factorization"}, {"score": 0.004558091785667046, "phrase": "frequently_used_kernel"}, {"score": 0.004504850679320975, "phrase": "large_number"}, {"score": 0.004365881789903319, "phrase": "data_clustering"}, {"score": 0.004331811712726697, "phrase": "machine_learning"}, {"score": 0.004281202319048493, "phrase": "central_contribution"}, {"score": 0.0041817430405326145, "phrase": "thorough_performance_study"}, {"score": 0.003943047070755799, "phrase": "sti_cell_broadband_engine"}, {"score": 0.003806395038279074, "phrase": "implementation_challenges"}, {"score": 0.0037471850931318942, "phrase": "cell_chip-multiprocessor"}, {"score": 0.003561017335579207, "phrase": "factorization_techniques"}, {"score": 0.003491894048789576, "phrase": "matrix_sizes"}, {"score": 0.0033444934455768597, "phrase": "bottleneck_kernels"}, {"score": 0.003056029284235528, "phrase": "largest_data_sets"}, {"score": 0.002405549400304955, "phrase": "dense_lu"}, {"score": 0.002223927963267319, "phrase": "interesting_interactions"}, {"score": 0.0021383149182448522, "phrase": "two-node_non-uniform_memory_access"}, {"score": 0.0021049977753042253, "phrase": "cell_blade"}], "paper_keywords": ["Parallel matrix factorization", " cell broadband engine", " scalability", " LU", " Cholesky", " QR", " singular value decomposition", " new data structures"], "paper_abstract": "Matrix factorization (or often called decomposition) is a frequently used kernel in a large number of applications ranging from linear solvers to data clustering and machine learning. The central contribution of this paper is a thorough performance study of four popular matrix factorization techniques, namely, LU, Cholesky, QR and SVD on the STI Cell broadband engine. The paper explores algorithmic as well as implementation challenges related to the Cell chip-multiprocessor and explains how we achieve near-linear speedup on most of the factorization techniques for a range of matrix sizes. For each of the factorization routines, we identify the bottleneck kernels and explain how we have attempted to resolve the bottleneck and to what extent we have been successful. Our implementations, for the largest data sets that we use, running on a two-node 3.2 GHz Cell BladeCenter (exercising a total of sixteen SPEs), on average, deliver 203.9, 284.6, 81.5, 243.9 and 54.0 GFLOPS for dense LU, dense Cholesky, sparse Cholesky, QR and SVD, respectively. The implementations achieve speedup of 11.2, 12.8, 10.6, 13.0 and 6.2, respectively for dense LU, dense Cholesky, sparse Cholesky, QR and SVD, when running on sixteen SPEs. We discuss the interesting interactions that result from parallelization of the factorization routines on a two-node non-uniform memory access (NUMA) Cell Blade cluster.", "paper_title": "Implementing a parallel matrix factorization library on the cell broadband engine", "paper_id": "WOS:000272227800002"}