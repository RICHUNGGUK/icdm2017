{"auto_keywords": [{"score": 0.04648981269156613, "phrase": "spatial_phrases"}, {"score": 0.00481495049065317, "phrase": "multimodal_connectionist_architecture"}, {"score": 0.004768378525561471, "phrase": "unsupervised_grounding"}, {"score": 0.004722254884093624, "phrase": "spatial_language"}, {"score": 0.004608879242346434, "phrase": "bio-inspired_unsupervised_connectionist_architecture"}, {"score": 0.004390192910013225, "phrase": "two-layer_architecture"}, {"score": 0.004161553532799662, "phrase": "phonological_inputs"}, {"score": 0.004081385289516914, "phrase": "first_layer"}, {"score": 0.0040222701316738295, "phrase": "visual_pathway"}, {"score": 0.003983334573043943, "phrase": "separate_'what'_and_'where'_subsystems"}, {"score": 0.003685067218455088, "phrase": "bitmap_images"}, {"score": 0.0035965032173588753, "phrase": "artificial_retina"}, {"score": 0.0035443861412929006, "phrase": "phonologically_encoded_five-word_sentences"}, {"score": 0.00342568771304873, "phrase": "phonological_input"}, {"score": 0.003376037544631762, "phrase": "visual_scene"}, {"score": 0.003200045185608395, "phrase": "phonological_description"}, {"score": 0.0029747028229638625, "phrase": "five-word_sentences"}, {"score": 0.0029031614127920232, "phrase": "blue_ball"}, {"score": 0.002875027551180213, "phrase": "red_cup"}, {"score": 0.0028195727965553367, "phrase": "primary_representations"}, {"score": 0.002778682854371595, "phrase": "first-layer_modules"}, {"score": 0.0026986685585382347, "phrase": "multimodal_second-layer_module"}, {"score": 0.0026337862438869184, "phrase": "som"}, {"score": 0.00259554612218815, "phrase": "'neural_gas'_algorithms"}, {"score": 0.0025085435314540837, "phrase": "proper_lexical_and_visual_features"}, {"score": 0.002320449502337321, "phrase": "spatial_location"}, {"score": 0.002286781334850213, "phrase": "object_shape"}, {"score": 0.0021674482571326283, "phrase": "quantitative_experimental_results"}], "paper_keywords": ["Unsupervised learning", " Self-organizing map", " Symbol grounding", " Spatial phrases", " Multimodal representations"], "paper_abstract": "We propose a bio-inspired unsupervised connectionist architecture and apply it to grounding the spatial phrases. The two-layer architecture combines by concatenation the information from the visual and the phonological inputs. In the first layer, the visual pathway employs separate 'what' and 'where' subsystems that represent the identity and spatial relations of two objects in 2D space, respectively. The bitmap images are presented to an artificial retina and the phonologically encoded five-word sentences describing the image serve as the phonological input. The visual scene is hence represented by several self-organizing maps (SOMs) and the phonological description is processed by the Recursive SOM that learns to topographically represent the spatial phrases, represented as five-word sentences (e.g., 'blue ball above red cup'). Primary representations from the first-layer modules are unambiguously integrated in a multimodal second-layer module, implemented by the SOM or the 'neural gas' algorithms. The system learns to bind proper lexical and visual features without any prior knowledge. The simulations reveal that separate processing and representation of the spatial location and the object shape significantly improve the performance of the model. We provide quantitative experimental results comparing three models in terms of their accuracy.", "paper_title": "A Multimodal Connectionist Architecture for Unsupervised Grounding of Spatial Language", "paper_id": "WOS:000332454800008"}