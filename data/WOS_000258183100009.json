{"auto_keywords": [{"score": 0.03189920454613392, "phrase": "hdp"}, {"score": 0.011162339505328057, "phrase": "critic_nn"}, {"score": 0.004814964793146253, "phrase": "hjb"}, {"score": 0.004752221133109136, "phrase": "approximate_dynamic_programming"}, {"score": 0.004588891098290524, "phrase": "value-iteration-based_heuristic_dynamic_programming"}, {"score": 0.004373398434399966, "phrase": "general_nonlinear_systems"}, {"score": 0.0041316803676012155, "phrase": "optimal_control"}, {"score": 0.00407781620412982, "phrase": "optimal_value_function"}, {"score": 0.004007083686875716, "phrase": "hamilton-jacobi-bellman_equation"}, {"score": 0.003954837422946628, "phrase": "infinite-horizon_discrete-time"}, {"score": 0.0033197446406648626, "phrase": "value_function"}, {"score": 0.0032621183241997777, "phrase": "action_network"}, {"score": 0.0031775430407770026, "phrase": "optimal_control_policy"}, {"score": 0.002962543869356265, "phrase": "internal_dynamics"}, {"score": 0.0028857130140890787, "phrase": "exact_solution_assumption"}, {"score": 0.0028108690791085536, "phrase": "nonlinear_systems"}, {"score": 0.002725994306480105, "phrase": "specific_case"}, {"score": 0.002690406375639144, "phrase": "dt_linear_quadratic_regulator"}, {"score": 0.002552628669352939, "phrase": "value_quadratic"}, {"score": 0.002497325365598002, "phrase": "nns"}, {"score": 0.002475532161577715, "phrase": "zero_approximation_error"}, {"score": 0.0023798149238292857, "phrase": "lqr"}, {"score": 0.0021049977753042253, "phrase": "dt_lqr"}], "paper_keywords": ["adaptive critics", " approximate dynamic programming (ADP)", " Hamilton Jacobi Bellman (HJB)", " policy iteration", " value iteration"], "paper_abstract": "Convergence of the value-iteration-based heuristic dynamic programming (HDP) algorithm is proven in the case of general nonlinear systems. That is, it is shown that HDP converges to the optimal control and the optimal value function that solves the Hamilton-Jacobi-Bellman equation appearing in infinite-horizon discrete-time (DT). nonlinear optimal control. It is assumed that, at each iteration, the value. and action update equations can be exactly solved. The following two standard neural networks (NN) are used: a critic NN is used to approximate the value function, whereas an action network is used to approximate the optimal control policy. It is stressed that this approach allows the implementation of HDP without knowing the internal dynamics of the system. The exact solution assumption holds for some classes of nonlinear systems and, specifically, in the specific case of the DT linear quadratic regulator (LQR), where the action is linear and the value quadratic in the states and NNs have zero approximation error. It is stressed that, for the LQR, HDP may be implemented without knowing the system A matrix by using two NNs. This fact is not generally appreciated in the folklore of HDP for the DT LQR, where only one critic NN is generally used.", "paper_title": "Discrete-time nonlinear HJB solution using approximate dynamic programming: Convergence proof", "paper_id": "WOS:000258183100009"}