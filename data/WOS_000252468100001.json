{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multilingual_information_retrieval"}, {"score": 0.04759132288543124, "phrase": "user_query"}, {"score": 0.03720431371429641, "phrase": "retrieved_documents"}, {"score": 0.014409351512929292, "phrase": "different_languages"}, {"score": 0.013813363167184116, "phrase": "information_sources"}, {"score": 0.013668204364448055, "phrase": "monolingual_search"}, {"score": 0.011358455377941494, "phrase": "final_ranked_list"}, {"score": 0.00979220177189862, "phrase": "comparable_document_scores"}, {"score": 0.008132046619701988, "phrase": "cross-language_evaluation_forum"}, {"score": 0.0077936421557197036, "phrase": "query-specific_and_source-specific_results"}, {"score": 0.0047893552527040195, "phrase": "federated_search_environments"}, {"score": 0.004650982915563737, "phrase": "relevant_information"}, {"score": 0.004626255242038005, "phrase": "multiple_target_languages"}, {"score": 0.004528646311967683, "phrase": "single_source_language"}, {"score": 0.004480613386372829, "phrase": "multilingual_federated_search_environment"}, {"score": 0.004456787453123763, "phrase": "different_information_sources"}, {"score": 0.004374385639298755, "phrase": "general_search_strategy"}, {"score": 0.0043511221096418475, "phrase": "multilingual_federated_search_environments"}, {"score": 0.004147216579100177, "phrase": "information_source"}, {"score": 0.004048860957751714, "phrase": "single_ranked_document_list"}, {"score": 0.003777581820724243, "phrase": "previous_research"}, {"score": 0.003727527379253624, "phrase": "simple_approach"}, {"score": 0.0037076911119175455, "phrase": "normalizing_source-specific_document_scores"}, {"score": 0.0035056552382170575, "phrase": "source_language"}, {"score": 0.003404244237372396, "phrase": "search_client"}, {"score": 0.003377097707054532, "phrase": "latter_method"}, {"score": 0.0032969458013463807, "phrase": "large_amount"}, {"score": 0.0032793933580835574, "phrase": "online_communication"}, {"score": 0.003201552914322119, "phrase": "effective_and_efficient_approach"}, {"score": 0.0031422859551159506, "phrase": "multilingual_ranked_lists"}, {"score": 0.0029394074636168435, "phrase": "document-based_translation_method"}, {"score": 0.002908181499274949, "phrase": "query-specific_and_source-specific_transformation_models"}, {"score": 0.002801472611575172, "phrase": "downloaded_documents"}, {"score": 0.0027791193706247267, "phrase": "transformation_models"}, {"score": 0.0026275488733903426, "phrase": "merging_approach"}, {"score": 0.0025109095718850376, "phrase": "extensive_set"}, {"score": 0.002323733841676433, "phrase": "new_research"}, {"score": 0.0022928991183989115, "phrase": "different_variants"}, {"score": 0.002202822893621243, "phrase": "thorough_experimental_results"}, {"score": 0.002179404723362044, "phrase": "detailed_analysis"}, {"score": 0.0021276181317652163, "phrase": "preliminary_research"}, {"score": 0.002105007493414928, "phrase": "callan"}], "paper_keywords": ["results merging", " federated search", " multilingual information retrieval"], "paper_abstract": "Multilingual information retrieval is generally understood to mean the retrieval of relevant information in multiple target languages in response to a user query in a single source language. In a multilingual federated search environment, different information sources contain documents in different languages. A general search strategy in multilingual federated search environments is to translate the user query to each language of the information sources and run a monolingual search in each information source. It is then necessary to obtain a single ranked document list by merging the individual ranked lists from the information sources that are in different languages. This is known as the results merging problem for multilingual information retrieval. Previous research has shown that the simple approach of normalizing source-specific document scores is not effective. On the other side, a more effective merging method was proposed to download and translate all retrieved documents into the source language and generate the final ranked list by running a monolingual search in the search client. The latter method is more effective but is associated with a large amount of online communication and computation costs. This paper proposes an effective and efficient approach for the results merging task of multilingual ranked lists. Particularly, it downloads only a small number of documents from the individual ranked lists of each user query to calculate comparable document scores by utilizing both the query-based translation method and the document-based translation method. Then, query-specific and source-specific transformation models can be trained for individual ranked lists by using the information of these downloaded documents. These transformation models are used to estimate comparable document scores for all retrieved documents and thus the documents can be sorted into a final ranked list. This merging approach is efficient as only a subset of the retrieved documents are downloaded and translated online. Furthermore, an extensive set of experiments on the Cross-Language Evaluation Forum (CLEF) (http://www.clef-campaign.org/) data has demonstrated the effectiveness of the query-specific and source-specific results merging algorithm against other alternatives. The new research in this paper proposes different variants of the query-specific and source-specific results merging algorithm with different transformation models. This paper also provides thorough experimental results as well as detailed analysis. All of the work substantially extends the preliminary research in (Si and Callan, in: Peters (ed.) Results of the cross-language evaluation forum-CLEF 2005, 2005).", "paper_title": "An effective and efficient results merging strategy for multilingual information retrieval in federated search environments", "paper_id": "WOS:000252468100001"}