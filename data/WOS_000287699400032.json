{"auto_keywords": [{"score": 0.03557337756607459, "phrase": "long_range_dependencies"}, {"score": 0.004723322467153553, "phrase": "probabilistic_models"}, {"score": 0.004501774379030109, "phrase": "central_role"}, {"score": 0.0034725269588525534, "phrase": "real-world_sequence_data"}, {"score": 0.002977061391551628, "phrase": "natural_processes"}, {"score": 0.002864628676257225, "phrase": "power-law_properties"}, {"score": 0.0027830917623582903, "phrase": "common_sequence_models"}, {"score": 0.002601727192945944, "phrase": "sequence_memoizer"}, {"score": 0.002527654365712841, "phrase": "new_hierarchical_bayesian_model"}, {"score": 0.00247944491239733, "phrase": "discrete_sequence_data"}, {"score": 0.00236289633510808, "phrase": "power-law_characteristics"}, {"score": 0.002145942273032774, "phrase": "language_model"}, {"score": 0.0021049977753042253, "phrase": "general_purpose_lossless_compressor"}], "paper_keywords": [""], "paper_abstract": "Probabilistic models of sequences play a central role in most machine translation, automated speech recognition, lossless compression, spell-checking, and gene identification applications to name but a few. Unfortunately, real-world sequence data often exhibit long range dependencies which can only be captured by computationally challenging, complex models. Sequence data arising from natural processes also often exhibits power-law properties, yet common sequence models do not capture such properties. The sequence memoizer is a new hierarchical Bayesian model for discrete sequence data that captures long range dependencies and power-law characteristics, while remaining computationally attractive. Its utility as a language model and general purpose lossless compressor is demonstrated.", "paper_title": "The Sequence Memoizer", "paper_id": "WOS:000287699400032"}