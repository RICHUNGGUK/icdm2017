{"auto_keywords": [{"score": 0.043265073684849525, "phrase": "aeg"}, {"score": 0.007740489013190948, "phrase": "affective_content"}, {"score": 0.004708056866527125, "phrase": "gaussian_mixture"}, {"score": 0.004401342842183475, "phrase": "music_information_retrieval"}, {"score": 0.0043619799095455415, "phrase": "affective_human_computer_interaction"}, {"score": 0.0042459818200777846, "phrase": "novel_generative_model"}, {"score": 0.004170394506041784, "phrase": "gaussians"}, {"score": 0.0040777192615062815, "phrase": "computational_modeling"}, {"score": 0.003933736547198066, "phrase": "music_excerpt"}, {"score": 0.003660788124282824, "phrase": "valence-arousal_space"}, {"score": 0.003563370084662192, "phrase": "gaussian_mixture_model"}, {"score": 0.003406713877432236, "phrase": "subjective_nature"}, {"score": 0.003376216135716629, "phrase": "emotion_perception"}, {"score": 0.0031845067058084613, "phrase": "audio_and_emotion_data"}, {"score": 0.003141830113006653, "phrase": "fitting_algorithm"}, {"score": 0.003099723664407622, "phrase": "gmm_parameters"}, {"score": 0.003058179790439378, "phrase": "model_learning_process"}, {"score": 0.002845809662579165, "phrase": "emotion_distribution"}, {"score": 0.002820319063320601, "phrase": "music_audio_data"}, {"score": 0.0027452053065200152, "phrase": "comprehensive_performance_study"}, {"score": 0.0026600905156986317, "phrase": "new_insights"}, {"score": 0.00249767641333441, "phrase": "\"affective_diversity"}, {"score": 0.002398529964696261, "phrase": "effective_means"}, {"score": 0.0023770362063833903, "phrase": "emotion_modeling"}, {"score": 0.002282667669149476, "phrase": "publicly_available_codes"}, {"score": 0.0022419349154472806, "phrase": "aeg_model"}, {"score": 0.0021049977753042253, "phrase": "affective_or_other_highly_subjective_information"}], "paper_keywords": ["Music information retrieval", " music emotion recognition", " valence", " arousal", " Gaussian mixture model", " subjectivity"], "paper_abstract": "Modeling the association between music and emotion has been considered important for music information retrieval and affective human computer interaction. This paper presents a novel generative model called acoustic emotion Gaussians (AEG) for computational modeling of emotion. Instead of assigning a music excerpt with a deterministic (hard) emotion label, AEG treats the affective content of music as a (soft) probability distribution in the valence-arousal space and parameterizes it with a Gaussian mixture model (GMM). In this way, the subjective nature of emotion perception is explicitly modeled. Specifically, AEG employs two GMMs to characterize the audio and emotion data. The fitting algorithm of the GMM parameters makes the model learning process transparent and interpretable. Based on AEG, a probabilistic graphical structure for predicting the emotion distribution from music audio data is also developed. A comprehensive performance study over two emotion-labeled datasets demonstrates that AEG offers new insights into the relationship between music and emotion (e.g., to assess the \"affective diversity\" of a corpus) and represents an effective means of emotion modeling. Readers can easily implement AEG via the publicly available codes. As the AEG model is generic, it holds the promise of analyzing any signal that carries affective or other highly subjective information.", "paper_title": "Modeling the Affective Content of Music with a Gaussian Mixture Model", "paper_id": "WOS:000350741300005"}