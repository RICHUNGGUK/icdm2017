{"auto_keywords": [{"score": 0.04849105041601803, "phrase": "shh"}, {"score": 0.028198264663901264, "phrase": "neural_network"}, {"score": 0.011932015375043192, "phrase": "shf"}, {"score": 0.004774530554776604, "phrase": "smooth_hinge_functions"}, {"score": 0.004463096299091981, "phrase": "well-known_hinging_hyperplane"}, {"score": 0.004260800171269249, "phrase": "useful_features"}, {"score": 0.004225055716170054, "phrase": "hh"}, {"score": 0.004171891198117439, "phrase": "hh's_drawback"}, {"score": 0.0040334640350288, "phrase": "formal_characterization"}, {"score": 0.003999578057088828, "phrase": "smooth_hinge_function"}, {"score": 0.0036759314172445934, "phrase": "general_construction"}, {"score": 0.0033926758710459866, "phrase": "functional_approximation"}, {"score": 0.0032938942910948096, "phrase": "general_function"}, {"score": 0.0031710909624086884, "phrase": "hh."}, {"score": 0.002939007076062123, "phrase": "sigmoidal_functions"}, {"score": 0.00284137141260141, "phrase": "corresponding_shh"}, {"score": 0.0027009493171069763, "phrase": "sigmoidal_function"}, {"score": 0.0025458479274370832, "phrase": "approximation_error"}, {"score": 0.0021773853011112882, "phrase": "simulation_experiments"}, {"score": 0.002122866345896679, "phrase": "theoretical_conclusions"}, {"score": 0.0021049977753042253, "phrase": "possible_extent"}], "paper_keywords": ["Function approximation", " hinging hyperplanes (HHs)", " neural networks", " nonlinear identification", " sigmoidal functions", " smooth hinging hyperplanes (SHHs)"], "paper_abstract": "Smooth hinging hyperplane (SHH) has been proposed as an improvement over the well-known hinging hyperplane (HH) by the fact that it retains the useful features of HH while overcoming HH's drawback of nondifferentiability. This paper introduces a formal characterization of smooth hinge function (SHF), which can be used to generate SHH as a neural network. A method for the general construction of SHF is also given. Furthermore, the work proves that SHH is better than HH in functional approximation, i.e., the optimal error of SHH approximating a general function is always smaller or equal to that of HH. Particularly, in the case that the SHF is generated via the integration of a class of sigmoidal functions, it is further proven that the corresponding SHH of the 2m SHFs would outperform a neural network with m of the sigmoidal function from which the SHF is derived. Any upper bound established on the approximation error of a neural network of m sigmoidal activation functions can hence be translated to the SHH of m SHFs by replacing m with m/2. The work also includes an algorithm for the identification of SHH making use of its differentiability property. Simulation experiments are presented to validate the theoretical conclusions to possible extent.", "paper_title": "A Neural Network of Smooth Hinge Functions", "paper_id": "WOS:000283231200001"}