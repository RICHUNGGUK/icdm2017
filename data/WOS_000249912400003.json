{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "best_subset_regression_models"}, {"score": 0.0047797212183363835, "phrase": "large-scale_problems"}, {"score": 0.004506968388996092, "phrase": "modified_versions"}, {"score": 0.004473982381245469, "phrase": "existing_regression-tree_methods"}, {"score": 0.004344424495876066, "phrase": "first_algorithm"}, {"score": 0.004296809240805996, "phrase": "best_subset_models"}, {"score": 0.004172360444377281, "phrase": "reduced_search_space"}, {"score": 0.004081385289516914, "phrase": "computationally_the_existing_branch-and-bound_algorithm"}, {"score": 0.003948613642680196, "phrase": "proposed_algorithm"}, {"score": 0.0038483286199081, "phrase": "second_new_algorithm"}, {"score": 0.0037643920253363776, "phrase": "regression_tree"}, {"score": 0.003334309053314909, "phrase": "smaller_distance"}, {"score": 0.0032021226849872054, "phrase": "efficient_method"}, {"score": 0.003097865379563127, "phrase": "experimental_results"}, {"score": 0.0027437235598539904, "phrase": "large-scale_subset-selection_problems"}, {"score": 0.0026543514413663893, "phrase": "conventional_exhaustive-selection_methods"}, {"score": 0.0026059610060579145, "phrase": "new_heuristic_strategies"}, {"score": 0.0024660058541899647, "phrase": "different_tolerance_value"}, {"score": 0.0024389277933858054, "phrase": "subset_model_size"}, {"score": 0.0023944552329463035, "phrase": "different_kind"}, {"score": 0.0023335495163073544, "phrase": "exhaustive_and_heuristic_subset-selection_strategies"}, {"score": 0.002224510044064184, "phrase": "noncontiguous_size"}, {"score": 0.002175913488493802, "phrase": "flexible_tool"}, {"score": 0.0021520139248317333, "phrase": "large_scale_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["best-subset regression", " regression tree", " branch-and-bound algorithm"], "paper_abstract": "Several strategies for computing the best subset regression models are proposed. Some of the algorithms are modified versions of existing regression-tree methods, while others are new. The first algorithm selects the best subset models within a given size range. It uses a reduced search space and is found to outperform computationally the existing branch-and-bound algorithm. The properties and computational aspects of the proposed algorithm are discussed in detail. The second new algorithm preorders the variables inside the regression tree. A radius is defined in order to measure the distance of a node from the root of the tree. The algorithm applies the preordering to all nodes which have a smaller distance than a certain radius that is given a priori. An efficient method of preordering the variables is employed. The experimental results indicate that the algorithm performs best when preordering is employed on a radius of between one quarter and one third of the number of variables. The algorithm has been applied with such a radius to tackle large-scale subset-selection problems that are considered to be computationally infeasible by conventional exhaustive-selection methods. A class of new heuristic strategies is also proposed. The most important of these is one that assigns a different tolerance value to each subset model size. This strategy with different kind of tolerances is equivalent to all exhaustive and heuristic subset-selection strategies. In addition the strategy can be used to investigate submodels having noncontiguous size ranges. Its implementation provides a flexible tool for tackling large scale models. (c) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Efficient algorithms for computing the best subset regression models for large-scale problems", "paper_id": "WOS:000249912400003"}