{"auto_keywords": [{"score": 0.04475762650789477, "phrase": "power_savings"}, {"score": 0.00481495049065317, "phrase": "autonomously_power-managed_components"}, {"score": 0.00461575768501871, "phrase": "hierarchical_dynamic_power_management"}, {"score": 0.004572647942983489, "phrase": "dpm"}, {"score": 0.004466519142879166, "phrase": "reinforcement_learning"}, {"score": 0.004424875933824536, "phrase": "rl"}, {"score": 0.00422177384286051, "phrase": "computer_system"}, {"score": 0.004085221464783815, "phrase": "heterogeneous_applications"}, {"score": 0.004028054020045027, "phrase": "proposed_framework"}, {"score": 0.003953068310598579, "phrase": "cpu_scheduler"}, {"score": 0.003897742942358768, "phrase": "effective_application-level_scheduling"}, {"score": 0.0037188360359702182, "phrase": "non-stationary_workloads"}, {"score": 0.0036324746826127997, "phrase": "service_request_generation_rates"}, {"score": 0.003548111729206587, "phrase": "online_adaptive_dpm_technique"}, {"score": 0.003229741076887151, "phrase": "component-level_pm_policy"}, {"score": 0.002995677265350124, "phrase": "semi-markov_decision_process"}, {"score": 0.002953711377604328, "phrase": "model-free_rl_technique"}, {"score": 0.0028312983170567948, "phrase": "heterogeneous_application_pool"}, {"score": 0.002752513434446804, "phrase": "proposed_approach"}, {"score": 0.002675914969626753, "phrase": "good_performance_levels"}, {"score": 0.0025770807002632877, "phrase": "proposed_rl-based_dpm_approach"}, {"score": 0.0023127544795063263, "phrase": "deep_power-performance_tradeoff_curves"}, {"score": 0.0022589720760078274, "phrase": "multiple_service_providers"}, {"score": 0.0022168460543913787, "phrase": "maximum_energy_saving"}, {"score": 0.002196077907454079, "phrase": "service_provider"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Power management", " Reinforcement learning", " Temporal difference learning", " Semi-Markov decision process"], "paper_abstract": "This paper presents a hierarchical dynamic power management (DPM) framework based on reinforcement learning (RL) technique, which aims at power savings in a computer system with multiple I/O devices running a number of heterogeneous applications. The proposed framework interacts with the CPU scheduler to perform effective application-level scheduling, thereby enabling further power savings. Moreover, it considers non-stationary workloads and differentiates between the service request generation rates of various software application. The online adaptive DPM technique consists of two layers: component-level local power manager and system-level global power manager. The component-level PM policy is pre-specified and fixed whereas the system-level PM employs temporal difference learning on semi-Markov decision process as the model-free RL technique, and it is specifically optimized for a heterogeneous application pool. Experiments show that the proposed approach considerably enhances power savings while maintaining good performance levels. In comparison with other reference systems, the proposed RL-based DPM approach, further enhances power savings, performs well under various workloads, can simultaneously consider power and performance, and achieves wide and deep power-performance tradeoff curves. Experiments conducted with multiple service providers confirm that up to 63% maximum energy saving per service provider can be achieved. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Hierarchical power management of a system with autonomously power-managed components using reinforcement learning", "paper_id": "WOS:000345541100002"}