{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cross-domain_multicue_fusion_for_concept-based_video_indexing"}, {"score": 0.004500891637057374, "phrase": "video_retrieval_needs"}, {"score": 0.004351595642056769, "phrase": "concept-based_video_indexing"}, {"score": 0.004067635935640407, "phrase": "video_segment"}, {"score": 0.003982741546775355, "phrase": "objective_linguistic_description"}, {"score": 0.003883194688426525, "phrase": "semantic_gap"}, {"score": 0.0037861265016041813, "phrase": "machine-extracted_low-level_features"}, {"score": 0.0037543106253910313, "phrase": "human_high-level_conceptual_interpretation"}, {"score": 0.0034504423179340738, "phrase": "low-level_features"}, {"score": 0.003349984456443527, "phrase": "diverse_cues"}, {"score": 0.0031845067058084613, "phrase": "learned_knowledge"}, {"score": 0.003104849179057156, "phrase": "new_domain"}, {"score": 0.0029266219967426224, "phrase": "multiple_cues"}, {"score": 0.0029020077365708966, "phrase": "multiple_video_domains"}, {"score": 0.002853396755102163, "phrase": "recursive_algorithms"}, {"score": 0.00266693888510356, "phrase": "concept_labels"}, {"score": 0.0025674490885184173, "phrase": "single_fusion_model"}, {"score": 0.002524428047147766, "phrase": "unseen_shots"}, {"score": 0.0024302416436559867, "phrase": "contextual_and_temporal_relationships"}, {"score": 0.002349467661235186, "phrase": "additional_human_effort"}, {"score": 0.0022522565004588113, "phrase": "video_sets"}, {"score": 0.002233301265743633, "phrase": "domain_change"}, {"score": 0.002195866991284673, "phrase": "popular_benchmarks"}, {"score": 0.0021228663458966813, "phrase": "significant_improvements"}, {"score": 0.0021049977753042253, "phrase": "popular_baselines"}], "paper_keywords": ["Video annotation", " concept detection", " cross-domain learning", " contextual correlation", " temporal dependency", " TRECVID"], "paper_abstract": "The success of query-by-concept, proposed recently to cater to video retrieval needs, depends greatly on the accuracy of concept-based video indexing. Unfortunately, it remains a challenge to recognize the presence of concepts in a video segment or to extract an objective linguistic description from it because of the semantic gap, that is, the lack of correspondence between machine-extracted low-level features and human high-level conceptual interpretation. This paper studies three issues with the aim to reduce such a gap: 1) how to explore cues beyond low-level features, 2) how to combine diverse cues to improve performance, and 3) how to utilize the learned knowledge when applying it to a new domain. To solve these problems, we propose a framework that jointly exploits multiple cues across multiple video domains. First, recursive algorithms are proposed to learn both interconcept and intershot relationships from annotations. Second, all concept labels for all shots are simultaneously refined in a single fusion model. Additionally, unseen shots are assigned pseudolabels according to their initial prediction scores so that contextual and temporal relationships can be learned, thus requiring no additional human effort. Integration of cues embedded within training and testing video sets accommodates domain change. Experiments on popular benchmarks show that our framework is effective, achieving significant improvements over popular baselines.", "paper_title": "Cross-Domain Multicue Fusion for Concept-Based Video Indexing", "paper_id": "WOS:000307522700005"}