{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "minimal_approximation_error"}, {"score": 0.004726382602809228, "phrase": "principal_components"}, {"score": 0.004388035083357706, "phrase": "classical_pca"}, {"score": 0.00411182818527434, "phrase": "mean_square_error"}, {"score": 0.003961842092184655, "phrase": "lower-dimensional_approximation"}, {"score": 0.0035110198699787013, "phrase": "mean_square_error_function"}, {"score": 0.0032593915869553714, "phrase": "presented_methods"}, {"score": 0.003140401552000472, "phrase": "commonly_employed_process"}, {"score": 0.0027062100658653485, "phrase": "optimization_problem"}, {"score": 0.0025121156016001864, "phrase": "optimal_basis"}, {"score": 0.0024429654627197393, "phrase": "minimum_error"}, {"score": 0.0021049977753042253, "phrase": "recent_solution"}], "paper_keywords": ["principal components analysis", " eigenvalue", " matrix trace"], "paper_abstract": "We introduce two new methods of deriving the classical PCA in the framework of minimizing the mean square error upon performing a lower-dimensional approximation of the data. These methods are based on two forms of the mean square error function. One of the novelties of the presented methods is that the commonly employed process of subtraction of the mean of the data becomes part of the solution of the optimization problem and not a pre-analysis heuristic. We also derive the optimal basis and the minimum error of approximation in this framework and demonstrate the elegance of our solution in comparison with a recent solution in the framework.", "paper_title": "New routes from minimal approximation error to principal components", "paper_id": "WOS:000255196500001"}