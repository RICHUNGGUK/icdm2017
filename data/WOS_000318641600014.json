{"auto_keywords": [{"score": 0.03148448317610218, "phrase": "proposed_algorithm"}, {"score": 0.01138340829859759, "phrase": "ranking_accuracies"}, {"score": 0.00481495049065317, "phrase": "sparse_learning-to-rank"}, {"score": 0.004749824776508639, "phrase": "efficient_primal-dual_algorithm"}, {"score": 0.0045804107341714, "phrase": "information_retrieval"}, {"score": 0.004518442937844318, "phrase": "increasing_interest"}, {"score": 0.004337502738649814, "phrase": "sparse_models"}, {"score": 0.0040334640350288, "phrase": "learned_ranking_models"}, {"score": 0.003802135020791801, "phrase": "sparse_learning-to-rank_problem"}, {"score": 0.0035194572615570977, "phrase": "critical_issue"}, {"score": 0.003378385493203297, "phrase": "optimization_problem"}, {"score": 0.0032282396745967504, "phrase": "learning_algorithm"}, {"score": 0.0031845067058084583, "phrase": "primal_dual_perspective"}, {"score": 0.0028813415821988156, "phrase": "epsilon-accurate_solution"}, {"score": 0.0028422946420821075, "phrase": "convergence_rate"}, {"score": 0.002753224894944965, "phrase": "popular_subgradient_descent_algorithm"}, {"score": 0.0026548344763838213, "phrase": "empirical_evaluation"}, {"score": 0.0024460897937643032, "phrase": "dense_models"}, {"score": 0.002401972657236928, "phrase": "ranking_model"}, {"score": 0.0023802126605244438, "phrase": "sparsity_constraints"}, {"score": 0.002133937537670314, "phrase": "sparser_models"}, {"score": 0.0021049977753042253, "phrase": "superior_performance_gain"}], "paper_keywords": ["Learning-to-rank", " sparse models", " ranking algorithm", " Fenchel duality"], "paper_abstract": "Learning-to-rank for information retrieval has gained increasing interest in recent years. Inspired by the success of sparse models, we consider the problem of sparse learning-to-rank, where the learned ranking models are constrained to be with only a few nonzero coefficients. We begin by formulating the sparse learning-to-rank problem as a convex optimization problem with a sparse-inducing l(1) constraint. Since the l(1) constraint is nondifferentiable, the critical issue arising here is how to efficiently solve the optimization problem. To address this issue, we propose a learning algorithm from the primal dual perspective. Furthermore, we prove that, after at most O(1/epsilon) iterations, the proposed algorithm can guarantee the obtainment of an epsilon-accurate solution. This convergence rate is better than that of the popular subgradient descent algorithm. i.e., O(1/epsilon(2)). Empirical evaluation on several public benchmark data sets demonstrates the effectiveness of the proposed algorithm: 1) Compared to the methods that learn dense models, learning a ranking model with sparsity constraints significantly improves the ranking accuracies. 2) Compared to other methods for sparse learning-to-rank, the proposed algorithm tends to obtain sparser models and has superior performance gain on both ranking accuracies and training time. 3) Compared to several state-of-the-art algorithms, the ranking accuracies of the proposed algorithm are very competitive and stable.", "paper_title": "Sparse Learning-to-Rank via an Efficient Primal-Dual Algorithm", "paper_id": "WOS:000318641600014"}