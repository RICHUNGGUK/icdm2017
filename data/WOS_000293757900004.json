{"auto_keywords": [{"score": 0.04749520186100097, "phrase": "multiple_kernels"}, {"score": 0.046363267640327735, "phrase": "single_one"}, {"score": 0.00481495049065317, "phrase": "multiple_kernel_learning_algorithms"}, {"score": 0.0043753813457279404, "phrase": "different_kernels"}, {"score": 0.004266501496765747, "phrase": "different_notions"}, {"score": 0.004056769841308405, "phrase": "multiple_sources"}, {"score": 0.0033832583769880576, "phrase": "real_data"}, {"score": 0.003332440548684892, "phrase": "better_illustration"}, {"score": 0.0032658649336454923, "phrase": "existing_algorithms"}, {"score": 0.003105168935158648, "phrase": "large_differences"}, {"score": 0.002821247621250663, "phrase": "stored_support_vectors"}, {"score": 0.0026420518944303716, "phrase": "used_kernels"}, {"score": 0.0026023381988430666, "phrase": "training_time_complexity"}, {"score": 0.0023287359930308864, "phrase": "nonlinear_or_data-dependent_way"}, {"score": 0.0022706700510423954, "phrase": "linear_combination"}, {"score": 0.002202894673025544, "phrase": "simple_linear_kernels"}, {"score": 0.0021697677572213086, "phrase": "linear_methods"}, {"score": 0.0021049977753042253, "phrase": "complex_gaussian_kernels"}], "paper_keywords": ["support vector machines", " kernel machines", " multiple kernel learning"], "paper_abstract": "In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels.", "paper_title": "Multiple Kernel Learning Algorithms", "paper_id": "WOS:000293757900004"}