{"auto_keywords": [{"score": 0.03783329245327835, "phrase": "dpillar"}, {"score": 0.00481495049065317, "phrase": "large_scale_data_centers"}, {"score": 0.004724571550328985, "phrase": "huge_demands"}, {"score": 0.004688894532233354, "phrase": "computation_power"}, {"score": 0.004653485665080485, "phrase": "storage_space"}, {"score": 0.004600870784042767, "phrase": "future_data_center"}, {"score": 0.004396267325338022, "phrase": "conventional_hierarchical_tree-based_data_center_network_architecture"}, {"score": 0.004184825434585617, "phrase": "previous_research_effort"}, {"score": 0.004106225442735525, "phrase": "server-centric_architecture"}, {"score": 0.003983512316698646, "phrase": "storage_workstations"}, {"score": 0.00393844255111604, "phrase": "intermediate_nodes"}, {"score": 0.0037774893672602506, "phrase": "data_center"}, {"score": 0.00373474182086791, "phrase": "huge_number"}, {"score": 0.00362308996613223, "phrase": "server-centric_data_center_network"}, {"score": 0.0034881908940439213, "phrase": "classic_butterfly_network"}, {"score": 0.0033582976109650608, "phrase": "topological_scalability"}, {"score": 0.0033329032920236994, "phrase": "network_performance"}, {"score": 0.0031845067058084583, "phrase": "large_scale"}, {"score": 0.0031724417545242315, "phrase": "future_data_centers"}, {"score": 0.002940487679224863, "phrase": "dpillar_network"}, {"score": 0.002874257080906824, "phrase": "network_bottleneck"}, {"score": 0.002820202660128789, "phrase": "architectural_level"}, {"score": 0.002643888053328987, "phrase": "communication_intensive_distributed_applications"}, {"score": 0.0025843206540392184, "phrase": "interconnection_features"}, {"score": 0.0024043803470189455, "phrase": "extensive_simulation_experiments"}, {"score": 0.0021948455916770233, "phrase": "large_number"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Data center network", " Multi-path routing", " Network topology"], "paper_abstract": "To meet the huge demands of computation power and storage space, a future data center may have to include up to millions of servers. The conventional hierarchical tree-based data center network architecture faces several challenges in scaling a data center to that size. Previous research effort has shown that a server-centric architecture, where servers are not only computation and storage workstations but also intermediate nodes relaying traffic for other servers, performs well in scaling a data center to a huge number of servers. This paper presents a server-centric data center network called DPillar, whose topology is inspired by the classic butterfly network. DPillar provides several nice properties and achieves the balance between topological scalability, network performance, and cost efficiency, which make it suitable for building large scale future data centers. Using only commodity hardware, a DPillar,network can easily accommodate millions of servers. The structure of a DPillar network is symmetric so that any network bottleneck is eliminated at the architectural level. With each server having only two ports, DPillar is able to provide the bandwidth to support communication intensive distributed applications. This paper studies the interconnection features of DPillar, how to compute routes in DPillar, and how to forward packets in DPillar. Extensive simulation experiments have been performed to evaluate the performance of DPillar. The results show that DPillar performs well even in the presence of a large number of server and switch failures. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "DPillar: Dual-port server interconnection network for large scale data centers", "paper_id": "WOS:000303946000004"}