{"auto_keywords": [{"score": 0.041813362211231636, "phrase": "ssn"}, {"score": 0.00481495049065317, "phrase": "classification_data"}, {"score": 0.004775726266706856, "phrase": "privacy_preservation"}, {"score": 0.00467905143301368, "phrase": "fundamental_problem"}, {"score": 0.004640929126263433, "phrase": "data_analysis"}, {"score": 0.004491506786670695, "phrase": "large_collection"}, {"score": 0.0044005599565517875, "phrase": "person-specific_data"}, {"score": 0.004329124434198367, "phrase": "customer_data"}, {"score": 0.004293840838533456, "phrase": "patient_records"}, {"score": 0.0041555469216463855, "phrase": "individual's_privacy"}, {"score": 0.0040713749949601915, "phrase": "explicit_identifying_information"}, {"score": 0.0037055370870874484, "phrase": "nonidentifying_attributes"}, {"score": 0.00357146811369898, "phrase": "useful_approach"}, {"score": 0.0032637448078885016, "phrase": "k_released_records"}, {"score": 0.003223883109446599, "phrase": "value_combination"}, {"score": 0.0031845067058084583, "phrase": "linking_attributes"}, {"score": 0.003158522476399068, "phrase": "previous_work"}, {"score": 0.0030944828434567966, "phrase": "optimal_k-anonymization"}, {"score": 0.0030441844088874366, "phrase": "data_distortion_metric"}, {"score": 0.002921975748248106, "phrase": "training_data"}, {"score": 0.0028510114763213596, "phrase": "classification_goal"}, {"score": 0.0027365364488693656, "phrase": "\"future\"_data"}, {"score": 0.002637433799290365, "phrase": "k-anonymization_solution"}, {"score": 0.0024398167078355224, "phrase": "data_distortion"}, {"score": 0.0023903140267755576, "phrase": "classification_structure"}, {"score": 0.0023514343164101332, "phrase": "intensive_experiments"}, {"score": 0.0022477377970612847, "phrase": "future_data"}, {"score": 0.0022111720236287547, "phrase": "real-life_data"}, {"score": 0.0021049977753042253, "phrase": "highly_restrictive_anonymity_requirements"}], "paper_keywords": ["privacy protection", " anonymity", " security", " integrity", " data mining", " classification", " data sharing"], "paper_abstract": "Classification is a fundamental problem in data analysis. Training a classifier requires accessing a large collection of data. Releasing person-specific data, such as customer data or patient records, may pose a threat to an individual's privacy. Even after removing explicit identifying information such as Name and SSN, it is still possible to link released records back to their identities by matching some combination of nonidentifying attributes such as {Sex; Zip; Birthdate}. A useful approach to combat such linking attacks, called k-anonymization [1], is anonymizing the linking attributes so that at least k released records match each value combination of the linking attributes. Previous work attempted to find an optimal k-anonymization that minimizes some data distortion metric. We argue that minimizing the distortion to the training data is not relevant to the classification goal that requires extracting the structure of predication on the \"future\" data. In this paper, we propose a k-anonymization solution for classification. Our goal is to find a k-anonymization, not necessarily optimal in the sense of minimizing data distortion, which preserves the classification structure. We conducted intensive experiments to evaluate the impact of anonymization on the classification on future data. Experiments on real-life data show that the quality of classification can be preserved even for highly restrictive anonymity requirements.", "paper_title": "Anonymizing classification data for privacy preservation", "paper_id": "WOS:000244895200009"}