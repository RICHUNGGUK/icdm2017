{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "conventional_weighted_voting_methods"}, {"score": 0.004483585361583022, "phrase": "soft_class_probability_outputs"}, {"score": 0.004192484908247302, "phrase": "class-specific_weights"}, {"score": 0.004071121244999291, "phrase": "combinative_performance"}, {"score": 0.003712011863784581, "phrase": "cssv_scheme"}, {"score": 0.0036809741830924796, "phrase": "multiple_extreme_learning_machines"}, {"score": 0.003589404014552974, "phrase": "designed_two_models"}, {"score": 0.0035148319209810128, "phrase": "accuracy_and_sparsity_aspects"}, {"score": 0.0033561632967173856, "phrase": "condition_number"}, {"score": 0.00323166859966514, "phrase": "linear_equation"}, {"score": 0.003138030910104256, "phrase": "base_elm_classifiers"}, {"score": 0.0030089355635315005, "phrase": "randomly_input_parameters"}, {"score": 0.002971249577042111, "phrase": "single_elm"}, {"score": 0.002909482525367989, "phrase": "ill-conditioned_problem"}, {"score": 0.002873038567884343, "phrase": "linear_system_structure"}, {"score": 0.002789763110395331, "phrase": "sparse_ensemble_methods"}, {"score": 0.0027548146060608902, "phrase": "memory_requirement"}, {"score": 0.0026975346834867313, "phrase": "classification_process"}, {"score": 0.0026414426081951734, "phrase": "classifier-specific_weight_level"}, {"score": 0.002586513876943306, "phrase": "spacssv-elm_method"}, {"score": 0.0025221012517852907, "phrase": "weight_optimization_problem"}, {"score": 0.0024904974828343033, "phrase": "sparse_coding_problem"}, {"score": 0.002438699976481979, "phrase": "sparse_representation_technique"}, {"score": 0.0024081387633065206, "phrase": "classification_performance"}, {"score": 0.002318728604587962, "phrase": "twenty_uci_data_sets"}, {"score": 0.002299313860419974, "phrase": "finance_event_series_data"}, {"score": 0.002270495425498003, "phrase": "experimental_results"}, {"score": 0.0022420373745572837, "phrase": "superior_performance"}, {"score": 0.002204646157481869, "phrase": "based_elm_algorithms"}, {"score": 0.002158780936704905, "phrase": "state-of-the-art_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme learning machine", " Soft voting", " Condition number", " Sparse ensemble"], "paper_abstract": "Compared with conventional weighted voting methods, class-specific soft voting (CSSV) system has several advantages. On one hand, it not only deals with the soft class probability outputs but also refines the weights from classifiers to classes. On the other hand, the class-specific weights can be used to improve the combinative performance without increasing much computational load. This paper proposes two weight optimization based ensemble methods (CSSV-ELM and SpaCSSV-ELM) under the framework of CSSV scheme for multiple extreme learning machines (ELMs). The designed two models are in terms of accuracy and sparsity aspects, respectively. Firstly. CSSV-ELM takes advantage of the condition number of matrix, which reveals the stability of linear equation, to determine the weights of base ELM classifiers. This model can reduce the unreliability induced by randomly input parameters of a single ELM, and solve the ill-conditioned problem caused by linear system structure of ELM simultaneously. Secondly, sparse ensemble methods can lower memory requirement and speed up the classification process, but only for classifier-specific weight level. Therefore, a SpaCSSV-ELM method is proposed by transforming the weight optimization problem to a sparse coding problem, which uses the sparse representation technique for maintaining classification performance with less nonzero weight coefficients. Experiments are carried out on twenty UCI data sets and Finance event series data and the experimental results show the superior performance of the CSSV based ELM algorithms by comparing with the state-of-the-art algorithms. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Class-specific soft voting based multiple extreme learning machines ensemble", "paper_id": "WOS:000360028800034"}