{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "cluster_tree_estimation"}, {"score": 0.004608284537368354, "phrase": "density_f"}, {"score": 0.004442824994700736, "phrase": "high-density_cluster"}, {"score": 0.0043464021636703066, "phrase": "connected_component"}, {"score": 0.003700170452390401, "phrase": "high-density_clusters"}, {"score": 0.003291188255149539, "phrase": "cluster_tree"}, {"score": 0.003081272473508291, "phrase": "robust_variant"}, {"score": 0.0030143100083252516, "phrase": "single_linkage_algorithm"}, {"score": 0.002970476379751357, "phrase": "hierarchical_clustering"}, {"score": 0.0028014064098604093, "phrase": "k-nearest_neighbor_graph"}, {"score": 0.0026613618933208467, "phrase": "finite-sample_convergence_rates"}, {"score": 0.002419542935774618, "phrase": "lower_bounds"}, {"score": 0.0023669270875629205, "phrase": "sample_complexity"}, {"score": 0.0021049977753042253, "phrase": "milder_conditions"}], "paper_keywords": ["Clustering algorithms", " convergence"], "paper_abstract": "For a density f on R-d, a high-density cluster is any connected component of {x : f (x) >= lambda}, for some lambda > 0. The set of all high-density clusters forms a hierarchy called the cluster tree of f. We present two procedures for estimating the cluster tree given samples from f. The first is a robust variant of the single linkage algorithm for hierarchical clustering. The second is based on the k-nearest neighbor graph of the samples. We give finite-sample convergence rates for these algorithms, which also imply consistency, and we derive lower bounds on the sample complexity of cluster tree estimation. Finally, we study a tree pruning procedure that guarantees, under milder conditions than usual, to remove clusters that are spurious while recovering those that are salient.", "paper_title": "Consistent Procedures for Cluster Tree Estimation and Pruning", "paper_id": "WOS:000345511400032"}