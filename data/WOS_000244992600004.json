{"auto_keywords": [{"score": 0.0492663932388163, "phrase": "geographic_named_entity_annotation"}, {"score": 0.04302766577757209, "phrase": "location_normalization"}, {"score": 0.00481495049065317, "phrase": "phrasal_context_and_error_correction_heuristics"}, {"score": 0.0046928556063004214, "phrase": "geographic_named_entities"}, {"score": 0.00443878620915958, "phrase": "information_extraction"}, {"score": 0.004216434794603139, "phrase": "high-performance_bootstrapping_algorithm"}, {"score": 0.003988055287053139, "phrase": "seven_sub-types"}, {"score": 0.0037238193456845124, "phrase": "initial_stage"}, {"score": 0.0036448787596988423, "phrase": "raw_corpus"}, {"score": 0.003598317040926701, "phrase": "large_set"}, {"score": 0.003331134183901191, "phrase": "specific_training_corpus"}, {"score": 0.003274499008699115, "phrase": "initial_annotation"}, {"score": 0.0032465423581390625, "phrase": "boundary_patterns"}, {"score": 0.003218823621926739, "phrase": "phrasal_context"}, {"score": 0.0030705342386533083, "phrase": "new_annotation"}, {"score": 0.0030183163057929687, "phrase": "error_correction_heuristics"}, {"score": 0.00296698375549856, "phrase": "bootstrapping_loop"}, {"score": 0.0029165216647792924, "phrase": "annotated_instances"}, {"score": 0.002842428606954139, "phrase": "learned_boundary_patterns"}, {"score": 0.0026538960171433985, "phrase": "syntactic_constraints"}, {"score": 0.0026199603775129516, "phrase": "named_entity"}, {"score": 0.002564360070426064, "phrase": "annotation_errors"}, {"score": 0.0025315664299815537, "phrase": "incomplete_boundary_patterns"}, {"score": 0.002404522326122611, "phrase": "learning_curve"}, {"score": 0.0023233800372985686, "phrase": "newspaper_corpus"}, {"score": 0.0021599099979586946, "phrase": "instance_level"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["bootstrapping", " geographic named entity annotation", " phrasal context", " error correcting heuristics", " location normalization"], "paper_abstract": "Geographic named entities can be classified into many sub-types that are useful for applications such as information extraction and question answering. In this paper, we present a high-performance bootstrapping algorithm with error correction heuristics and location normalization for the task of geographic named entity annotation with seven sub-types. Location normalization additionally resolves ambiguities of entities with same name and sub-types. In the initial stage, we annotate a raw corpus using a large set of seeds which is automatically selected from a gazetteer so that its quality does not depend on a specific training corpus. From the initial annotation, boundary patterns reflecting phrasal context are learned and applied to the corpus again to obtain new annotation which passes through error correction heuristics. As the bootstrapping loop proceeds, the annotated instances are gradually increased and the learned boundary patterns become gradually richer and more accurate. Through experiments, we explore inter/intra-phrasal context which reflects syntactic constraints of a named entity and several heuristic knowledge for correcting annotation errors introduced by incomplete boundary patterns. The experiments show the effect of the strategies on the learning curve. When our bootstrapping approach was applied to a newspaper corpus, it could achieve 89 F1 value. And the method suggested for location normalization could achieve 95% accuracy at instance level. (C) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Exploring phrasal context and error correction heuristics in bootstrapping for geographic named entity annotation", "paper_id": "WOS:000244992600004"}