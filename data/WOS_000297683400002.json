{"auto_keywords": [{"score": 0.04417829785526971, "phrase": "strassen"}, {"score": 0.02543383379857634, "phrase": "matrix_additions"}, {"score": 0.00481495049065317, "phrase": "matrix-computation_kernels"}, {"score": 0.004771672073078768, "phrase": "symmetric_multiprocessor_systems_matrix-multiplication"}, {"score": 0.004728780808284175, "phrase": "matrix-addition_algorithm_optimizations"}, {"score": 0.00468627326602559, "phrase": "software_pipelining"}, {"score": 0.0045200127552893704, "phrase": "simple_and_efficient_methodology"}, {"score": 0.004320421465896323, "phrase": "matrix_algorithms"}, {"score": 0.004167090708942862, "phrase": "winograd"}, {"score": 0.003947186611084043, "phrase": "complex_matrices"}, {"score": 0.0037052054802048707, "phrase": "highly_tuned_blas_matrix_multiplication"}, {"score": 0.003415681637907093, "phrase": "algorithm_design"}, {"score": 0.00338493754743055, "phrase": "careful_and_natural_parallelism_exploitation"}, {"score": 0.003235305151177897, "phrase": "function-call_parallelism"}, {"score": 0.003148709618042979, "phrase": "function_software"}, {"score": 0.00298238929037041, "phrase": "performance_overview"}, {"score": 0.002955533639862811, "phrase": "double-and_double-complex-precision_matrices"}, {"score": 0.0029289191071848403, "phrase": "state-of-the-art_smp_systems"}, {"score": 0.0026634937939104177, "phrase": "winograd's_matrix_multiplication"}, {"score": 0.0025572160146590623, "phrase": "regular_matrix_multiplication"}, {"score": 0.0025000115574335693, "phrase": "latter_winograd's_algorithms"}, {"score": 0.0023148964315569866, "phrase": "computation_latency"}], "paper_keywords": ["Algorithms", " Performance", " Matrix multiplications", " fast algorithms", " software pipeline", " parallelism"], "paper_abstract": "We present a simple and efficient methodology for the development, tuning, and installation of matrix algorithms such as the hybrid Strassen's and Winograd's fast matrix multiply or their combination with the 3M algorithm for complex matrices (i.e., hybrid: a recursive algorithm as Strassen's until a highly tuned BLAS matrix multiplication allows performance advantages). We investigate how modern Symmetric Multiprocessor (SMP) architectures present old and new challenges that can be addressed by the combination of an algorithm design with careful and natural parallelism exploitation at the function level (optimizations) such as function-call parallelism, function percolation, and function software pipelining. We have three contributions: first, we present a performance overview for double-and double-complex-precision matrices for state-of-the-art SMP systems; second, we introduce new algorithm implementations: a variant of the 3M algorithm and two new different schedules of Winograd's matrix multiplication (achieving up to 20% speedup with respect to regular matrix multiplication). About the latter Winograd's algorithms: one is designed to minimize the number of matrix additions and the other to minimize the computation latency of matrix additions; third, we apply software pipelining and threads allocation to all the algorithms and we show how this yields up to 10% further performance improvements.", "paper_title": "Exploiting Parallelism in Matrix-Computation Kernels for Symmetric Multiprocessor Systems Matrix-Multiplication and Matrix-Addition Algorithm Optimizations by Software Pipelining and Threads Allocation", "paper_id": "WOS:000297683400002"}