{"auto_keywords": [{"score": 0.030162973691103215, "phrase": "pre-trained_detector"}, {"score": 0.00481495049065317, "phrase": "detector_adaptation"}, {"score": 0.004565876339189493, "phrase": "different_target_scenes"}, {"score": 0.0040334640350288, "phrase": "novel_approach"}, {"score": 0.003986103192038901, "phrase": "detection_adaptation"}, {"score": 0.003916098498242888, "phrase": "scene_transformation"}, {"score": 0.0037133478610481994, "phrase": "automatic_parameter_estimation"}, {"score": 0.003584025547693836, "phrase": "pre-trained_detectors"}, {"score": 0.0033584796908285894, "phrase": "yaw_view_variations"}, {"score": 0.003299460925313939, "phrase": "human_interactions"}, {"score": 0.0030017207229983385, "phrase": "spatial_temporal_voting"}, {"score": 0.0026986685585382347, "phrase": "new_samples"}, {"score": 0.00266693888510356, "phrase": "manual_camera_calibration"}, {"score": 0.002573965014439041, "phrase": "manual_interactions"}, {"score": 0.002498961351132696, "phrase": "real-world_videos"}, {"score": 0.002426137929553724, "phrase": "significant_improvements"}, {"score": 0.002181069180139225, "phrase": "fully_labeled_sequences"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Vehicle detection", " Detector adaptation", " Scene transformation", " Automatic parameter estimation"], "paper_abstract": "This paper focuses on detecting vehicles in different target scenes with the same pre-trained detector which is very challenging due to view variations. To address this problem, we propose a novel approach for detection adaptation based on scene transformation, which contributes in both view transformation and automatic parameter estimation. Instead of modifying the pre-trained detectors, we transform scenes into frontal/rear view handling with pitch and yaw view variations. Without human interactions but only some general prior knowledge, the transformation parameters are automatically initialized, and then online optimized with spatial temporal voting, which guarantees that the transformation matches the pre-trained detector. Since there is no need of labeling new samples and manual camera calibration, our approach can considerably reduce manual interactions. Experiments on challenging real-world videos demonstrate that our approach achieves significant improvements over the pre-trained detector, and it is even comparable to the performance of the detector trained on fully labeled sequences. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Scene transformation for detector adaptation", "paper_id": "WOS:000329145400019"}