{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "normalized_radial_basis_function_networks"}, {"score": 0.004481132747216135, "phrase": "strong_universal_consistency"}, {"score": 0.003951453722517865, "phrase": "nonlinear_regression_function_learning_algorithms"}, {"score": 0.0031845067058084583, "phrase": "covariance_matrices"}, {"score": 0.003071965703296204, "phrase": "synaptic_weights"}, {"score": 0.0028076592725295646, "phrase": "empirical_risk_minimization"}, {"score": 0.0021049977753042253, "phrase": "complexity_regularization"}], "paper_keywords": [""], "paper_abstract": "We study strong universal consistency and the rates of convergence of nonlinear regression function learning algorithms using normalized radial basis function networks. The parameters of the network including centers, covariance matrices and synaptic weights are trained by the empirical risk minimization. We show the rates of convergence for the networks whose parameters are learned by the complexity regularization.", "paper_title": "Nonlinear function learning by the normalized radial basis function networks", "paper_id": "WOS:000239600000006"}