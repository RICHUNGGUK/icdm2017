{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "contextual_human_motion_recognition"}, {"score": 0.0047113960612140335, "phrase": "human_motion"}, {"score": 0.004682218325765133, "phrase": "monocular_video_sequences"}, {"score": 0.004624401316503576, "phrase": "discriminative_conditional_random_fields"}, {"score": 0.004553128404612054, "phrase": "maximum_entropy_markov_models"}, {"score": 0.004482949028773004, "phrase": "existing_approaches"}, {"score": 0.004400153852462583, "phrase": "generative_structures"}, {"score": 0.004359328949187473, "phrase": "hidden_markov_model"}, {"score": 0.004186734905572505, "phrase": "often_unrealistic_assumptions"}, {"score": 0.0041478818670853115, "phrase": "conditional_independence"}, {"score": 0.004083923108969583, "phrase": "motion_class_labels"}, {"score": 0.00402094658848301, "phrase": "rich_overlapping_features"}, {"score": 0.003958937342925219, "phrase": "long-term_contextual_dependencies"}, {"score": 0.003910016472705627, "phrase": "multiple_timesteps"}, {"score": 0.003825849505589868, "phrase": "myopic_failures"}, {"score": 0.0037551444630619015, "phrase": "even_the_transition"}, {"score": 0.0037318666008429014, "phrase": "simple_human_activities"}, {"score": 0.0036972190829870906, "phrase": "temporal_segments"}, {"score": 0.003617616153730564, "phrase": "correct_interpretation"}, {"score": 0.003452742572806342, "phrase": "particular_timestep"}, {"score": 0.003399466007862412, "phrase": "longer_windows"}, {"score": 0.0031062294139798273, "phrase": "observation_sequences"}, {"score": 0.003039311963787733, "phrase": "context_window"}, {"score": 0.0029645930408561086, "phrase": "different_philosophy"}, {"score": 0.0029007179823144357, "phrase": "complex_image_generation_process"}, {"score": 0.002700348931172875, "phrase": "conditional_models"}, {"score": 0.002675252546913683, "phrase": "proposed_crfs"}, {"score": 0.00265038878247831, "phrase": "contextual_dependencies"}, {"score": 0.0026257554931689076, "phrase": "computationally_attractive_properties"}, {"score": 0.0025059724686541263, "phrase": "convex_optimization"}, {"score": 0.002474961375646351, "phrase": "conditional_graphical_models"}, {"score": 0.0024595996811420566, "phrase": "complementary_tools"}, {"score": 0.002444333100610621, "phrase": "human_motion_recognition"}, {"score": 0.002414082940963702, "phrase": "extensive_set"}, {"score": 0.002325554982412043, "phrase": "diverse_human_activities"}, {"score": 0.00219196314559129, "phrase": "subtle_motion_styles"}, {"score": 0.0021783541405925766, "phrase": "normal_walks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Markov random fields", " discriminative models", " hidden Markov models", " human motion recognition", " multiclass logistic regression", " feature selection", " conditional models", " optimization"], "paper_abstract": "We describe algorithms for recognizing human motion in monocular video sequences, based on discriminative conditional random fields (CRFs) and maximum entropy Markov models (MEMMs). Existing approaches to this problem typically use generative structures like the hidden Markov model (HMM). Therefore, they have to make simplifying, often unrealistic assumptions on the conditional independence of observations given the motion class labels and cannot accommodate rich overlapping features of the observation or long-term contextual dependencies among observations at multiple timesteps. This makes them prone to myopic failures in recognizing many human motions, because even the transition between simple human activities naturally has temporal segments of ambiguity and overlap. The correct interpretation of these sequences requires more holistic, contextual decisions, where the estimate of an activity at a particular timestep could be constrained by longer windows of observations, prior and even posterior to that timestep. This would not be computationally feasible with a HMM which requires the enumeration of a number of observation sequences exponential in the size of the context window. In this work we follow a different philosophy: instead of restrictively modeling the complex image generation process - the observation, we work with models that can unrestrictedly take it as an input, hence condition on it. Conditional models like the proposed CRFs seamlessly represent contextual dependencies and have computationally attractive properties: they support efficient, exact recognition using dynamic programming, and their parameters can be learned using convex optimization. We introduce conditional graphical models as complementary tools for human motion recognition and present an extensive set of experiments that show not only how these can successfully classify diverse human activities like walking, jumping, running, picking or dancing, but also how they can discriminate among subtle motion styles like normal walks and wander walks. (c) 2006 Published by Elsevier Inc.", "paper_title": "Conditional models for contextual human motion recognition", "paper_id": "WOS:000241689800008"}