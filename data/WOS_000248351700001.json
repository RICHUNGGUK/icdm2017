{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "decision_trees"}, {"score": 0.0047051051659828275, "phrase": "existing_algorithms"}, {"score": 0.004597754207658615, "phrase": "greedy-a_tree"}, {"score": 0.004431041346291019, "phrase": "locally_optimal_decisions"}, {"score": 0.004211594861013822, "phrase": "constructed_tree"}, {"score": 0.004096493540086106, "phrase": "even_the_few_non-greedy_learners"}, {"score": 0.003787108326100196, "phrase": "fixed_amount"}, {"score": 0.003616108223034427, "phrase": "better_tree"}, {"score": 0.0035828427627676075, "phrase": "additional_time"}, {"score": 0.0034368818187904744, "phrase": "anytime_induction"}, {"score": 0.0032968474833776906, "phrase": "computation_speed"}, {"score": 0.0032665093519968083, "phrase": "better_tree_quality"}, {"score": 0.0031625006830124512, "phrase": "novel_strategy"}, {"score": 0.003118942369691329, "phrase": "candidate_splits"}, {"score": 0.0030759821501544224, "phrase": "biased_sampling"}, {"score": 0.0030056887638095883, "phrase": "consistent_trees"}, {"score": 0.0028303313661029597, "phrase": "minimal_tree"}, {"score": 0.002702415623060517, "phrase": "smallest_expected_tree"}, {"score": 0.0026042472337367015, "phrase": "anytime_induction_algorithms"}, {"score": 0.0025096359514694523, "phrase": "sample_size"}, {"score": 0.0024409345168575833, "phrase": "pre-given_allocation"}, {"score": 0.0023091093387071593, "phrase": "greedy_tree"}, {"score": 0.0022458849724980904, "phrase": "additional_sampling"}, {"score": 0.002225196379204285, "phrase": "experimental_results"}, {"score": 0.0021344259291927914, "phrase": "good_anytime_behavior"}, {"score": 0.0021049977753042253, "phrase": "significantly_better_decision_trees"}], "paper_keywords": ["anytime algorithms", " decision tree induction", " lookahead", " hard concepts", " resource-bounded reasoning"], "paper_abstract": "The majority of existing algorithms for learning decision trees are greedy-a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difficult. Furthermore, they require a fixed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.", "paper_title": "Anytime learning of decision trees", "paper_id": "WOS:000248351700001"}