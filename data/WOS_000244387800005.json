{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "unconstrained_optimization"}, {"score": 0.00413684709497912, "phrase": "conjugate_gradient_methods"}, {"score": 0.0034944094552943, "phrase": "presented_conjugate_gradient_methods"}, {"score": 0.002853396755102163, "phrase": "efficient_hybrid_methods"}, {"score": 0.0027123819773059127, "phrase": "touati-ahmed-storey"}, {"score": 0.0026222551419772867, "phrase": "dai-yuan"}, {"score": 0.0024926348205500715, "phrase": "mild_conditions"}, {"score": 0.0023694065215394593, "phrase": "convergent_results"}, {"score": 0.0022145052062614514, "phrase": "primary_numerical_results"}, {"score": 0.0021049977753042253, "phrase": "new_methods"}], "paper_keywords": ["unconstrained optimization", " conjugate gradient method", " Wolfe line search conditions", " sufficient descent property", " global convergence"], "paper_abstract": "In this paper we propose two kinds of conjugate gradient methods for unconstrained optimization, based on the combinations of the presented conjugate gradient methods. The methods can be regarded as the modifications of the efficient hybrid methods proposed by Touati-Ahmed-Storey and Dai-Yuan. Under mild conditions, globally convergent results are proved. Primary numerical results of the new methods are encouraging.", "paper_title": "Hybrid conjugate gradient methods for unconstrained optimization", "paper_id": "WOS:000244387800005"}