{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "speaker-specific_characteristics"}, {"score": 0.00475254914916922, "phrase": "deep_neural_architecture"}, {"score": 0.0047113960612140335, "phrase": "speech_signals"}, {"score": 0.00457013354092966, "phrase": "speaker-specific_information"}, {"score": 0.0044524122425287005, "phrase": "acoustic_representations"}, {"score": 0.004394688617163029, "phrase": "different_kinds"}, {"score": 0.00417115038996633, "phrase": "speaker_recognition"}, {"score": 0.004028452405970376, "phrase": "better_performance"}, {"score": 0.0038737214010638745, "phrase": "novel_deep_neural_architecture"}, {"score": 0.0037249113156851013, "phrase": "mel-frequency_cepstral_coefficients"}, {"score": 0.003581797255879607, "phrase": "speech_recognition"}, {"score": 0.003459191419478298, "phrase": "speaker-specific_overcomplete_representation"}, {"score": 0.0033699898113395328, "phrase": "intrinsic_speaker-specific_characteristics"}, {"score": 0.00326881479036966, "phrase": "objective_function"}, {"score": 0.0032263862712733934, "phrase": "contrastive_losses"}, {"score": 0.0031431690368229443, "phrase": "data_reconstruction_losses"}, {"score": 0.0030223381739019894, "phrase": "non-speaker-related_information"}, {"score": 0.002931569771634119, "phrase": "hybrid_learning_strategy"}, {"score": 0.002855934865505357, "phrase": "deep_neural_networks"}, {"score": 0.0026986685585382347, "phrase": "global_supervised_learning"}, {"score": 0.002663620639539737, "phrase": "ultimate_discriminative_goal"}, {"score": 0.00226693109377084, "phrase": "extensive_comparative_studies"}, {"score": 0.0022180544041790697, "phrase": "favorite_results"}, {"score": 0.0021987994275190314, "phrase": "speaker_verification"}], "paper_keywords": ["Deep neural architecture", " hybrid learning strategy", " overcomplete representation", " speaker comparison", " speaker segmentation", " speaker verification", " speaker-specific characteristics"], "paper_abstract": "Speech signals convey various yet mixed information ranging from linguistic to speaker-specific information. However, most of acoustic representations characterize all different kinds of information as whole, which could hinder either a speech or a speaker recognition (SR) system from producing a better performance. In this paper, we propose a novel deep neural architecture (DNA) especially for learning speaker-specific characteristics from mel-frequency cepstral coefficients, an acoustic representation commonly used in both speech recognition and SR, which results in a speaker-specific overcomplete representation. In order to learn intrinsic speaker-specific characteristics, we come up with an objective function consisting of contrastive losses in terms of speaker similarity/dissimilarity and data reconstruction losses used as regularization to normalize the interference of non-speaker-related information. Moreover, we employ a hybrid learning strategy for learning parameters of the deep neural networks: i.e., local yet greedy layerwise unsupervised pretraining for initialization and global supervised learning for the ultimate discriminative goal. With four Linguistic Data Consortium (LDC) benchmarks and two non-English corpora, we demonstrate that our overcomplete representation is robust in characterizing various speakers, no matter whether their utterances have been used in training our DNA, and highly insensitive to text and languages spoken. Extensive comparative studies suggest that our approach yields favorite results in speaker verification and segmentation. Finally, we discuss several issues concerning our proposed approach.", "paper_title": "Learning Speaker-Specific Characteristics With a Deep Neural Architecture", "paper_id": "WOS:000296469500006"}