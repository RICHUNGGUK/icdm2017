{"auto_keywords": [{"score": 0.048199190574404506, "phrase": "approximate_inverse_preconditioner"}, {"score": 0.03310810282697231, "phrase": "pcg_solver"}, {"score": 0.00481495049065317, "phrase": "mpi-cuda"}, {"score": 0.00467677555573346, "phrase": "conjugate_gradient_method"}, {"score": 0.004575740812937594, "phrase": "preconditioned_conjugate_gradient"}, {"score": 0.004364224683011796, "phrase": "sparse_linear_systems"}, {"score": 0.004332559296185432, "phrase": "symmetric_and_positive_definite_matrix"}, {"score": 0.004162445117081609, "phrase": "incompressible_flows"}, {"score": 0.004028220740657591, "phrase": "basic_linear_algebra_operations"}, {"score": 0.003969957613513855, "phrase": "overall_performance"}, {"score": 0.0037725685596495355, "phrase": "basic_operations"}, {"score": 0.003624361893008392, "phrase": "parallel_computing_systems"}, {"score": 0.0034692911858848893, "phrase": "computing_power"}, {"score": 0.0033696019683761274, "phrase": "graphics_processing_units"}, {"score": 0.003308751289007707, "phrase": "math_co"}, {"score": 0.0031671412657333364, "phrase": "mpi-cuda_implementation"}, {"score": 0.0030761074505030184, "phrase": "multiple_cpus"}, {"score": 0.0030315735229977958, "phrase": "special_attention"}, {"score": 0.00296597499768551, "phrase": "sparse_matrix-vector_multiplication"}, {"score": 0.0028702214190712036, "phrase": "execution_time"}, {"score": 0.002648935542912504, "phrase": "cg_solver"}, {"score": 0.002582159550634798, "phrase": "spmv_operation"}, {"score": 0.002535492891830764, "phrase": "data_transfer"}, {"score": 0.0024447527798201357, "phrase": "mpi"}, {"score": 0.00241805924231814, "phrase": "cpu-gpu_communications"}, {"score": 0.0023830293456003765, "phrase": "parallel_spmvs"}, {"score": 0.0023229408675543147, "phrase": "considerable_improvement"}, {"score": 0.002256117165288155, "phrase": "hybrid_implementation"}, {"score": 0.0021992217442375157, "phrase": "significant_speedup"}, {"score": 0.002167355210897559, "phrase": "cpu-only_implementation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["MPI-CUDA", " Sparse matrix vector multiplication", " Conjugate gradient", " Approximate inverse preconditioner", " CPU"], "paper_abstract": "The preconditioned conjugate gradient (PCG) is one of the most prominent iterative methods for the solution of sparse linear systems with symmetric and positive definite matrix that arise, for example, in the modeling of incompressible flows. The method relies on a set of basic linear algebra operations which determine the overall performance. Therefore, to achieve improvements in the performance, implementations of these basic operations must be adapted to the changes in the architecture of parallel computing systems. In the last years, one of the strategies to increase the computing power of supercomputers has been the usage of Graphics Processing Units (GPUs) as math co-processors in addition to CPUs. This paper presents a MPI-CUDA implementation of the PCG solver for such hybrid computing systems composed of multiple CPUs and CPUs. Special attention has been paid to the sparse matrix-vector multiplication (SpMV), because most of the execution time of the solver is spent on this operation. The approximate inverse preconditioner, which is used to improve the convergence of the CG solver, is also based on the SpMV operation. An overlapping of data transfer and computations is proposed in order to hide the MPI and the CPU-GPU communications needed to perform parallel SpMVs. This strategy has shown a considerable improvement and, as a result, the hybrid implementation of the PCG solver has demonstrated a significant speedup compared to the CPU-only implementation. (C) 2014 Published by Elsevier Ltd.", "paper_title": "MPI-CUDA sparse matrix-vector multiplication for the conjugate gradient method with an approximate inverse preconditioner", "paper_id": "WOS:000332264700022"}