{"auto_keywords": [{"score": 0.04216792154193272, "phrase": "surface_word"}, {"score": 0.010499029522559334, "phrase": "lda"}, {"score": 0.009628342150274463, "phrase": "document_collection"}, {"score": 0.007186888701127447, "phrase": "different_contexts"}, {"score": 0.006661389572596234, "phrase": "word_senses"}, {"score": 0.00481495049065317, "phrase": "aware_topic_models"}, {"score": 0.00456020621016909, "phrase": "semantic_relation"}, {"score": 0.0045108906244892165, "phrase": "surface_words"}, {"score": 0.004437910956996051, "phrase": "semantic_information"}, {"score": 0.004225950334493496, "phrase": "topic_distribution"}, {"score": 0.003290237137651949, "phrase": "different_signatures"}, {"score": 0.0031329184724341592, "phrase": "different_senses"}, {"score": 0.0029830992938695033, "phrase": "topic_models"}, {"score": 0.0027943943921568456, "phrase": "joint_model"}, {"score": 0.0027341763737751467, "phrase": "document_topics"}, {"score": 0.0026033756286033285, "phrase": "pre-defined_word_sense_resources"}, {"score": 0.0025334249463305875, "phrase": "word_sense_information"}, {"score": 0.0024923577071656014, "phrase": "latent_variable"}, {"score": 0.0023990982005125763, "phrase": "fully_unsupervised_manner"}, {"score": 0.0023346234464901978, "phrase": "experimental_results"}, {"score": 0.002284290601837099, "phrase": "proposed_joint_model"}, {"score": 0.002210814150677161, "phrase": "document_clustering"}, {"score": 0.0021631447601689444, "phrase": "word_sense_induction"}, {"score": 0.0021049977753042253, "phrase": "stand-alone_non-parametric_model"}], "paper_keywords": ["Topic modeling", " Word sense induction", " Document representation", " Document clustering"], "paper_abstract": "LDA has been proved effective in modeling the semantic relation between surface words. This semantic information in the document collection is useful to measure the topic distribution for a document. In general, a surface word may significantly contribute to several topics in a document collection. LDA measures the contribution of a surface word to each topic and considers a surface word to be identical across all documents. However, a surface word may present different signatures in different contexts, i.e., polysemous words can be used with different senses in different contexts. Intuitively, disambiguating word senses for topic models can enhance their discriminative capabilities. In this work, we propose a joint model to automatically induce document topics and word senses simultaneously. Instead of using some pre-defined word sense resources, we capture the word sense information via a latent variable and directly induce them in a fully unsupervised manner from the corpora. Experimental results show that the proposed joint model outperforms the baselines significantly in document clustering and improves the word sense induction as well against a stand-alone non-parametric model.", "paper_title": "Statistical word sense aware topic models", "paper_id": "WOS:000347407500003"}