{"auto_keywords": [{"score": 0.035149400889238576, "phrase": "ic"}, {"score": 0.00481495049065317, "phrase": "compact_rbf_network"}, {"score": 0.004654210559456924, "phrase": "radial_basis_function"}, {"score": 0.0034867406208212146, "phrase": "compact_network"}, {"score": 0.003370190834214415, "phrase": "improved_clustering"}, {"score": 0.002975099633631871, "phrase": "corresponding_widths"}, {"score": 0.0028111220617485985, "phrase": "learning_algorithms"}, {"score": 0.0024395426929046415, "phrase": "conventional_gradient_descent_method"}, {"score": 0.0021778895748464024, "phrase": "couple_recursive_equations"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithm"}], "paper_keywords": [""], "paper_abstract": "In the formulation of radial basis function (RBF) network, there are three factors mainly considered, i.e., centers, widths, and weights, which significantly affect the performance of the network. Within thus three factors, the placement of centers is proved theoretically and practically to be critical. In order to obtain a compact network, this paper presents an improved clustering (IC) scheme to obtain the location of the centers. What is more, since the location of the corresponding widths does affect the performance of the networks, a learning algorithms referred to as anisotropic gradient descent (AGD) method for designing the widths is presented as well. In the context of this paper, the conventional gradient descent method for learning the weights of the networks is combined with that of the widths to form an array of couple recursive equations. The implementation of the proposed algorithm shows that it is as efficient and practical as GGAP-RBF.", "paper_title": "Improved clustering and anisotropic gradient descent algorithm for compact RBF network", "paper_id": "WOS:000241753100089"}