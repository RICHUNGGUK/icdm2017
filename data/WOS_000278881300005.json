{"auto_keywords": [{"score": 0.04596243308241487, "phrase": "fil_algorithms"}, {"score": 0.00481495049065317, "phrase": "feature_interval_learning_algorithms"}, {"score": 0.004633429997452688, "phrase": "multi-concept_descriptions"}, {"score": 0.0044873778175404475, "phrase": "disjoint_feature_intervals"}, {"score": 0.00429057323623293, "phrase": "inductive_learning_algorithms"}, {"score": 0.004208881856146758, "phrase": "feature_projections"}, {"score": 0.0041287394089001405, "phrase": "training_instances"}, {"score": 0.004050116768809519, "phrase": "induced_classification_knowledge"}, {"score": 0.0039729853486821995, "phrase": "concept_description"}, {"score": 0.0035856674209019234, "phrase": "disjoint_intervals"}, {"score": 0.003450329424861292, "phrase": "unseen_instance"}, {"score": 0.00334144503360907, "phrase": "weighted-majority_voting"}, {"score": 0.0032777653153451265, "phrase": "feature_predictions"}, {"score": 0.003215295260684715, "phrase": "basic_fil_algorithm"}, {"score": 0.003133844153806967, "phrase": "adaptive_interval"}, {"score": 0.0030938932434437178, "phrase": "feature_weight_schemes"}, {"score": 0.00299622333197717, "phrase": "noisy_and_irrelevant_features"}, {"score": 0.0028463059289874637, "phrase": "twelve_data_sets"}, {"score": 0.0027920359090544107, "phrase": "uci_repository"}, {"score": 0.002585081657512565, "phrase": "nbc_classification_algorithms"}, {"score": 0.0024088451312725924, "phrase": "irrelevant_features"}, {"score": 0.00236289633510808, "phrase": "feature_values"}, {"score": 0.0022159566043745724, "phrase": "existing_algorithms"}, {"score": 0.0021876814398268775, "phrase": "significantly_less_average_running_times"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Classification learning", " Inductive learning", " Feature partitioning", " Adaptive feature weights"], "paper_abstract": "This paper presents Feature Interval Learning algorithms (FIL) which represent multi-concept descriptions in the form of disjoint feature intervals. The FIL algorithms are batch supervised inductive learning algorithms and use feature projections of the training instances to represent induced classification knowledge. The concept description is learned separately for each feature and is in the form of a set of disjoint intervals. The class of an unseen instance is determined by the weighted-majority voting of the feature predictions. The basic FIL algorithm is enhanced with adaptive interval and feature weight schemes in order to handle noisy and irrelevant features. The algorithms are empirically evaluated on twelve data sets from the UCI repository and are compared with k-NN, k-NNFP, and NBC classification algorithms. The experiments demonstrate that the FIL algorithms are robust to irrelevant features and missing feature values, achieve accuracy comparable to the best of the existing algorithms with significantly less average running times. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Feature interval learning algorithms for classification", "paper_id": "WOS:000278881300005"}