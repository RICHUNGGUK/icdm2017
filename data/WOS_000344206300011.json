{"auto_keywords": [{"score": 0.04559087461131432, "phrase": "smote"}, {"score": 0.008042380808947173, "phrase": "noisy_and_borderline_examples"}, {"score": 0.00481495049065317, "phrase": "smote-ipf"}, {"score": 0.00475437061368627, "phrase": "noisy_and_borderline_examples_problem"}, {"score": 0.004664920223650814, "phrase": "re-sampling_method"}, {"score": 0.004606219233601922, "phrase": "classification_datasets"}, {"score": 0.004548253531664032, "phrase": "unequal_class_distribution"}, {"score": 0.004420472010542188, "phrase": "imbalanced_classification"}, {"score": 0.004162328961227939, "phrase": "different_number"}, {"score": 0.004032551494961167, "phrase": "recent_works"}, {"score": 0.003994409920585572, "phrase": "class_imbalance"}, {"score": 0.003894446653225716, "phrase": "performance_degradation"}, {"score": 0.0035412818367359378, "phrase": "class_boundaries"}, {"score": 0.0033661687998511737, "phrase": "current_generalizations"}, {"score": 0.0031693980719538317, "phrase": "new_element"}, {"score": 0.0030704837930305745, "phrase": "iterative-partitioning_filter"}, {"score": 0.002782989122345306, "phrase": "comprehensive_experimental_study"}, {"score": 0.0027218799769229596, "phrase": "basic_smote"}, {"score": 0.0026036473098881344, "phrase": "synthetic_datasets"}, {"score": 0.002587180385280577, "phrase": "different_levels"}, {"score": 0.002538400273615248, "phrase": "borderline_examples"}, {"score": 0.0025063909490744855, "phrase": "real-world_datasets"}, {"score": 0.0024435751724545193, "phrase": "additional_different_types"}, {"score": 0.0023899010501421186, "phrase": "real-world_data"}, {"score": 0.0023226161707785064, "phrase": "new_proposal"}, {"score": 0.0022933216359142736, "phrase": "existing_smote_generalizations"}, {"score": 0.002179785463550654, "phrase": "ipf"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Imbalanced classification", " Borderline examples", " Noisy data", " Noise filters", " SMOTE"], "paper_abstract": "Classification datasets often have an unequal class distribution among their examples. This problem is known as imbalanced classification. The Synthetic Minority Over-sampling Technique (SMOTE) is one of the most well-know data pre-processing methods to cope with it and to balance the different number of examples of each class. However, as recent works claim, class imbalance is not a problem in itself and performance degradation is also associated with other factors related to the distribution of the data. One of these is the presence of noisy and borderline examples, the latter lying in the areas surrounding class boundaries. Certain intrinsic limitations of SMOTE can aggravate the problem produced by these types of examples and current generalizations of SMOTE are not correctly adapted to their treatment. This paper proposes the extension of SMOTE through a new element, an iterative ensemble-based noise filter called Iterative-Partitioning Filter (IPF), which can overcome the problems produced by noisy and borderline examples in imbalanced datasets. This extension results in SMOTE IPF. The properties of this proposal are discussed in a comprehensive experimental study. It is compared against a basic SMOTE and its most well-known generalizations. The experiments are carried out both on a set of synthetic datasets with different levels of noise and shapes of borderline examples as well as real-world datasets. Furthermore, the impact of introducing additional different types and levels of noise into these real-world data is studied. The results show that the new proposal performs better than existing SMOTE generalizations for all these different scenarios. The analysis of these results also helps to identify the characteristics of IPF which differentiate it from other filtering approaches. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "SMOTE-IPF: Addressing the noisy and borderline examples problem in imbalanced classification by a re-sampling method with filtering", "paper_id": "WOS:000344206300011"}