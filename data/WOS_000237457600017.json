{"auto_keywords": [{"score": 0.04926219622874721, "phrase": "emg_energy"}, {"score": 0.03308128556916549, "phrase": "future_head_motion"}, {"score": 0.00481495049065317, "phrase": "virtual-environment_applications"}, {"score": 0.004692227830462433, "phrase": "real-time_human-computer_interaction"}, {"score": 0.004305203085016233, "phrase": "user's_head_motion"}, {"score": 0.004268293864953256, "phrase": "today's_ve_systems"}, {"score": 0.004231699728933577, "phrase": "head-mounted_inertial_sensors"}, {"score": 0.0039841418890102925, "phrase": "head-mounted_display"}, {"score": 0.0037028514035425037, "phrase": "stabilization_scheme"}, {"score": 0.0035162927451540065, "phrase": "manual_control"}, {"score": 0.003426556090707852, "phrase": "real_and_virtual_objects"}, {"score": 0.003382547312072468, "phrase": "half-transparent_display"}, {"score": 0.003212074324593725, "phrase": "novel_methods"}, {"score": 0.003037047249598515, "phrase": "major_neck_muscles"}, {"score": 0.0030109772626153797, "phrase": "head_kinematics"}, {"score": 0.0027861630854233693, "phrase": "emg_signals"}, {"score": 0.0027150087516999047, "phrase": "neural_network"}, {"score": 0.002680114113899346, "phrase": "emg_data"}, {"score": 0.0026456667695073043, "phrase": "present_head_kinematics"}, {"score": 0.0025780912201045555, "phrase": "trained_network"}, {"score": 0.0025230952890994236, "phrase": "real_time"}, {"score": 0.0025014259919355453, "phrase": "head-motion_anticipation"}, {"score": 0.002416589459425828, "phrase": "time_advantage"}, {"score": 0.0023548511815224098, "phrase": "inherent_latencies"}, {"score": 0.0023245746777044766, "phrase": "main_contribution"}, {"score": 0.0022168460543913787, "phrase": "head_acceleration"}, {"score": 0.0021416404045850224, "phrase": "improved_performance"}, {"score": 0.0021049977753042253, "phrase": "previous_work"}], "paper_keywords": ["electromyogram", " head-mounted display", " neural networks", " pattern recognition", " virtual environment"], "paper_abstract": "Real-time human-computer interaction plays an important role in virtual-environment (VE) applications. Such interaction can be improved by detecting and reacting to the user's head motion. Today's VE systems use head-mounted inertial sensors to update and spatially stabilize the image displayed to a user through a head-mounted display. Since motion can only be detected after it has already occurred, latencies in the stabilization scheme can only be reduced but never eliminated. Such latencies slow down manual control, cause inaccuracies in matching real and virtual objects through a half-transparent display, and reduce the sense of presence. This paper presents novel methods for reducing VE latencies by anticipating future head motion based on electromyographic (EMG) signals originating from the major neck muscles and head kinematics; it also reports results for anticipation of 17.5 and 35 ms. Features extracted from the EMG signals are used to train a neural network in mapping EMG data, given present head kinematics, into future head motion. The trained network is then used in real time for head-motion anticipation, which gives the VE system the time advantage necessary to compensate for the inherent latencies. The main contribution of this work is the use of EMG energy and bounded head acceleration as the key input/output information. which results in improved performance compared to the previous work.", "paper_title": "Head motion anticipation for virtual-environment applications using kinematics and EMG energy", "paper_id": "WOS:000237457600017"}