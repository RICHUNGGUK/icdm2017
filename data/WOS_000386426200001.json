{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "high-speed_networks"}, {"score": 0.04380262464631336, "phrase": "distributed_query_processing"}, {"score": 0.03541239344095569, "phrase": "distributed_query_engine"}, {"score": 0.004774324145784758, "phrase": "modern_database_clusters"}, {"score": 0.00459568664065473, "phrase": "numa_regions"}, {"score": 0.004537631845923196, "phrase": "single_server"}, {"score": 0.004480307124735185, "phrase": "small_and_multiple_servers"}, {"score": 0.0043678116213858, "phrase": "huge_performance_gap"}, {"score": 0.0039620308501590795, "phrase": "single_many-core_server"}, {"score": 0.0039119492240482, "phrase": "increased_main-memory_capacity"}, {"score": 0.003813669883040509, "phrase": "sole_benefit"}, {"score": 0.0036864465771943933, "phrase": "high-speed_interconnects"}, {"score": 0.0036398422096466415, "phrase": "infiniband"}, {"score": 0.003578600071301459, "phrase": "performance_gap"}, {"score": 0.003503497128638099, "phrase": "infiniband's_higher_network_bandwidth"}, {"score": 0.0034299649219748513, "phrase": "query_performance"}, {"score": 0.0031912706211861324, "phrase": "tcp_overheads"}, {"score": 0.0031110402375026016, "phrase": "uncoordinated_communication"}, {"score": 0.0029691377785333872, "phrase": "classic_exchange_operator_model"}, {"score": 0.0025810006125483835, "phrase": "local_and_distributed_parallelism"}, {"score": 0.002360796860462937, "phrase": "analytical_database_workloads"}, {"score": 0.0023408288327263316, "phrase": "remote_direct_memory_access"}, {"score": 0.002321032415418357, "phrase": "rdma"}, {"score": 0.00229164285862567, "phrase": "low-latency_network_scheduling"}, {"score": 0.0022722584151582616, "phrase": "high-speed_communication"}, {"score": 0.0022530375696567136, "phrase": "almost_no_cpu"}, {"score": 0.002215081186105616, "phrase": "extensive_evaluation"}, {"score": 0.0021870332034407817, "phrase": "hyper_database_system"}, {"score": 0.0021593396036472777, "phrase": "tpc-h_benchmark"}, {"score": 0.0021049977753042253, "phrase": "high-speed_query_processing"}], "paper_keywords": [""], "paper_abstract": "Modern database clusters entail two levels of networks: connecting CPUs and NUMA regions inside a single server in the small and multiple servers in the large. The huge performance gap between these two types of networks used to slow down distributed query processing to such an extent that a cluster of machines actually performed worse than a single many-core server. The increased main-memory capacity of the cluster remained the sole benefit of such a scale-out. The economic viability of high-speed interconnects such as InfiniBand has narrowed this performance gap considerably. However, InfiniBand's higher network bandwidth alone does not improve query performance as expected when the distributed query engine is left unchanged. The scalability of distributed query processing is impaired by TCP overheads, switch contention due to uncoordinated communication, and load imbalances resulting from the inflexibility of the classic exchange operator model. This paper presents the blueprint for a distributed query engine that addresses these problems by considering both levels of networks holistically. It consists of two parts: First, hybrid parallelism that distinguishes local and distributed parallelism for better scalability in both the number of cores as well as servers. Second, a novel communication multiplexer tailored for analytical database workloads using remote direct memory access (RDMA) and low-latency network scheduling for high-speed communication with almost no CPU overhead. An extensive evaluation within the HyPer database system using the TPC-H benchmark shows that our holistic approach indeed enables high-speed query processing over high-speed networks.", "paper_title": "High-Speed Query Processing over High-Speed Networks", "paper_id": "WOS:000386426200001"}