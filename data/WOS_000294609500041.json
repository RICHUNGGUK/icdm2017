{"auto_keywords": [{"score": 0.015719716506582538, "phrase": "matrix_multiplication"}, {"score": 0.0047468516620285525, "phrase": "existing_work"}, {"score": 0.004548253531664032, "phrase": "complicated_reductions"}, {"score": 0.0036990033106718183, "phrase": "compiler_construction"}, {"score": 0.003544088803300751, "phrase": "complicated_loops"}, {"score": 0.003493900927782043, "phrase": "existing_techniques"}, {"score": 0.0027025233739857374, "phrase": "parallelized_programs"}, {"score": 0.0022933216359142736, "phrase": "existing_compilers"}], "paper_keywords": ["Experimentation", " Languages", " Design", " Algorithms", " automatic parallelization", " loop", " reduction", " scan", " matrix multiplication", " semiring", " linear recurrence equation"], "paper_abstract": "Existing work that deals with parallelization of complicated reductions and scans focuses only on formalism and hardly dealt with implementation. To bridge the gap between formalism and implementation, we have integrated parallelization via matrix multiplication into compiler construction. Our framework can deal with complicated loops that existing techniques in compilers cannot parallelize. Moreover, we have sophisticated our framework by developing two sets of techniques. One enhances its capability for parallelization by extracting max-operators automatically, and the other improves the performance of parallelized programs by eliminating redundancy. We have also implemented our framework and techniques as a parallelizer in a compiler. Experiments on examples that existing compilers cannot parallelize have demonstrated the scalability of programs parallelized by our implementation.", "paper_title": "Automatic Parallelization via Matrix Multiplication", "paper_id": "WOS:000294609500041"}