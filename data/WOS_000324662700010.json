{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "renyi"}, {"score": 0.004719473826242326, "phrase": "commonly_used_univariate_continuous_distributions"}, {"score": 0.003785836059995376, "phrase": "particular_importance"}, {"score": 0.0036370297611938796, "phrase": "renyi_divergence_measures"}, {"score": 0.00354700018079049, "phrase": "closed-form_expressions"}, {"score": 0.0034592063385840014, "phrase": "kullback-leibler"}, {"score": 0.003407549468745519, "phrase": "nineteen_commonly_used_univariate_continuous_distributions"}, {"score": 0.003306559295382181, "phrase": "multivariate_gaussian_and_dirichlet_distributions"}, {"score": 0.003097865379563127, "phrase": "zero-mean_stationary_gaussian_processes"}, {"score": 0.002976019715894587, "phrase": "kullback-leibler_divergence"}, {"score": 0.002816246391862638, "phrase": "renyi_divergence"}, {"score": 0.0027327327233891865, "phrase": "log-likelihood_ratio"}, {"score": 0.0025989960809479104, "phrase": "previous_result"}, {"score": 0.0025730638262762766, "phrase": "song"}, {"score": 0.0025473475682739784, "phrase": "j._stat"}, {"score": 0.002362620540317279, "phrase": "renyi_entropy"}, {"score": 0.002327310963701377, "phrase": "log-likelihood_function"}, {"score": 0.0022582634725291225, "phrase": "corresponding_variance_expressions"}, {"score": 0.002224510044064184, "phrase": "univariate_distributions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Renyi divergence", " Renyi divergence rate", " Kullback divergence", " Probabilistic distances", " log-likelihood ratio", " Continuous distributions"], "paper_abstract": "Probabilistic 'distances' (also called divergences), which in some sense assess how 'close' two probability distributions are from one another, have been widely employed in probability, statistics, information theory, and related fields. Of particular importance due to their generality and applicability are the Renyi divergence measures. This paper presents closed-form expressions for the Renyi and Kullback-Leibler divergences for nineteen commonly used univariate continuous distributions as well as those for multivariate Gaussian and Dirichlet distributions. In addition, a table summarizing four of the most important information measure rates for zero-mean stationary Gaussian processes, namely Renyi entropy, differential Shannon entropy, Renyi divergence, and Kullback-Leibler divergence, is presented. Lastly, a connection between the Renyi divergence and the variance of the log-likelihood ratio of two distributions is established, thereby extending a previous result by Song [J. Stat. Plan. Infer. 93 (2001)] on the relation between Renyi entropy and the log-likelihood function. A table with the corresponding variance expressions for the univariate distributions considered here is also included. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Renyi divergence measures for commonly used univariate continuous distributions", "paper_id": "WOS:000324662700010"}