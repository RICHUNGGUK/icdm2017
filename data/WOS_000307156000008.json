{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "asynchronous_data_transfers"}, {"score": 0.034034183564694315, "phrase": "optimal_number"}, {"score": 0.004776484866276242, "phrase": "consumer_graphics_processing_units"}, {"score": 0.004738325072560588, "phrase": "graphics_processing_units"}, {"score": 0.00458869631642791, "phrase": "general-purpose_coprocessors"}, {"score": 0.00455202992136173, "phrase": "high_performance_computing_applications"}, {"score": 0.00426900754050533, "phrase": "inherent_performance_bottleneck"}, {"score": 0.003845994935767022, "phrase": "cuda_application_programming_interface"}, {"score": 0.0038153090485632635, "phrase": "api"}, {"score": 0.003769567397860168, "phrase": "asynchronous_transfers"}, {"score": 0.0036651042289628324, "phrase": "staged_execution"}, {"score": 0.0034647524666821614, "phrase": "precise_manner"}, {"score": 0.00340954056723008, "phrase": "possible_improvement"}, {"score": 0.002857143106182505, "phrase": "cuda_streams"}, {"score": 0.0028342734482926677, "phrase": "different_cpu_architectures"}, {"score": 0.0023463854193479274, "phrase": "highest_performance_improvements"}, {"score": 0.0021825754039596855, "phrase": "sdk"}, {"score": 0.002156402793481102, "phrase": "successful_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["GPU", " CUDA", " Asynchronous transfers", " Streams", " Overlapping of communication and computation"], "paper_abstract": "Graphics Processing Units (CPU) have impressively arisen as general-purpose coprocessors in high performance computing applications, since the launch of the Compute Unified Device Architecture (CUDA). However, they present an inherent performance bottleneck in the fact that communication between two separate address spaces (the main memory of the CPU and the memory of the CPU) is unavoidable. The CUDA Application Programming Interface (API) provides asynchronous transfers and streams, which permit a staged execution, as a way to overlap communication and computation. Nevertheless, a precise manner to estimate the possible improvement due to overlapping does not exist, neither a rule to determine the optimal number of stages or streams in which computation should be divided. In this work, we present a methodology that is applied to model the performance of asynchronous data transfers of CUDA streams on different CPU architectures. Thus, we illustrate this methodology by deriving expressions of performance for two different consumer graphic architectures belonging to the more recent generations. These models permit programmers to estimate the optimal number of streams in which the computation on the CPU should be broken up, in order to obtain the highest performance improvements. Finally, we have checked the suitability of our performance models with three applications based on codes from the CUDA Software Development Kit (SDK) with successful results. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "Performance models for asynchronous data transfers on consumer Graphics Processing Units", "paper_id": "WOS:000307156000008"}