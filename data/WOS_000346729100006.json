{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cooperative_agents"}, {"score": 0.004675768396728975, "phrase": "optimal_rewards"}, {"score": 0.004630270043271386, "phrase": "single_agents"}, {"score": 0.004518442937844318, "phrase": "multiagent_optimal_rewards_problem"}, {"score": 0.004474469913842941, "phrase": "orp"}, {"score": 0.00413767314434795, "phrase": "new_problem"}, {"score": 0.004077394032903726, "phrase": "individual_agent_reward_functions"}, {"score": 0.003978866233891427, "phrase": "better_overall_team_performance"}, {"score": 0.003661275004971829, "phrase": "multiagent_architecture"}, {"score": 0.0035553221885904467, "phrase": "good_reward_functions"}, {"score": 0.0034693659976716197, "phrase": "gradient-based_algorithm"}, {"score": 0.003368948014436942, "phrase": "usual_task"}, {"score": 0.003319830633865628, "phrase": "good_policies"}, {"score": 0.002852256089630964, "phrase": "agent's_reward-learning_problem"}, {"score": 0.002729246328390722, "phrase": "reward_functions"}, {"score": 0.0026243534128245886, "phrase": "proposed_architecture"}, {"score": 0.0025860636370864084, "phrase": "conventional_approach"}, {"score": 0.0024264776194357993, "phrase": "resource_overhead"}, {"score": 0.002391067964886602, "phrase": "reward_learning"}, {"score": 0.002333193766510394, "phrase": "learning_algorithm"}, {"score": 0.0022216045996270974, "phrase": "individual_reward_functions"}, {"score": 0.0021784745589727246, "phrase": "better_specialization"}, {"score": 0.0021049977753042253, "phrase": "shared_reward"}], "paper_keywords": ["Intrinsic motivation", " multiagent learning", " optimal rewards", " reinforcement learning"], "paper_abstract": "Following work on designing optimal rewards for single agents, we define a multiagent optimal rewards problem (ORP) in cooperative (specifically, common-payoff or team) settings. This new problem solves for individual agent reward functions that guide agents to better overall team performance relative to teams in which all agents guide their behavior with the same given team-reward function. We present a multiagent architecture in which each agent learns good reward functions from experience using a gradient-based algorithm in addition to performing the usual task of planning good policies (except in this case with respect to the learned rather than the given reward function). Multiagency introduces the challenge of nonstationarity: because the agents learn simultaneously, each agent's reward-learning problem is nonstationary and interdependent on the other agents evolving reward functions. We demonstrate on two simple domains that the proposed architecture outperforms the conventional approach in which all the agents use the same given team-reward function (even when accounting for the resource overhead of the reward learning); that the learning algorithm performs stably despite the nonstationarity; and that learning individual reward functions can lead to better specialization of roles than is possible with shared reward, whether learned or given.", "paper_title": "Optimal Rewards for Cooperative Agents", "paper_id": "WOS:000346729100006"}