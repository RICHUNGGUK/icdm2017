{"auto_keywords": [{"score": 0.04586846484671963, "phrase": "k-nn_algorithm"}, {"score": 0.0313210843742285, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "kappa-nn_algorithm"}, {"score": 0.0047273577269638725, "phrase": "self-recombination_learning"}, {"score": 0.004473982381245469, "phrase": "existing_reduction_techniques"}, {"score": 0.004157058883925715, "phrase": "whole_training_data"}, {"score": 0.0035887342563437935, "phrase": "large-scale_problems"}, {"score": 0.00327356326590461, "phrase": "new_method"}, {"score": 0.0030414140732789186, "phrase": "basic_idea"}, {"score": 0.002878140062594849, "phrase": "self-recombination_learning_strategy"}, {"score": 0.0026011706232569316, "phrase": "tip_response_time"}, {"score": 0.002461472107682778, "phrase": "base_classifiers"}, {"score": 0.0023079225317714815, "phrase": "generalization_performance"}, {"score": 0.002183938718432764, "phrase": "training_samples"}, {"score": 0.0021441057555396013, "phrase": "experimental_results"}], "paper_keywords": [""], "paper_abstract": "A difficulty faced by existing reduction techniques for k-NN algorithm is to require loading the whole training data set. Therefore, these approaches often become inefficient when they are used for solving large-scale problems. To overcome this deficiency, we propose a new method for reducing samples for k-NN algorithm. The basic idea behind the proposed method is a self-recombination learning strategy, which is originally designed for combining classifiers to speed tip response time by reducing the number of base classifiers to he checked and improve the generalization performance by rearranging the order of training samples. Experimental results on several benchmark problems indicate that the proposed method is valid and efficient.", "paper_title": "A modular reduction method for kappa-NN algorithm with self-recombination learning", "paper_id": "WOS:000238112000080"}