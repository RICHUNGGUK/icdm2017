{"auto_keywords": [{"score": 0.04970970636722942, "phrase": "ltag"}, {"score": 0.033078712669685435, "phrase": "hmm"}, {"score": 0.032343829512590856, "phrase": "elementary_tree_sequence"}, {"score": 0.00481495049065317, "phrase": "statistical_and_hand-crafted_grammars"}, {"score": 0.004723322467153553, "phrase": "rich_formalism"}, {"score": 0.004669180816745142, "phrase": "nlp_tasks"}, {"score": 0.004615656893404919, "phrase": "semantic_interpretation"}, {"score": 0.004545240514015278, "phrase": "machine_translation"}, {"score": 0.004407599980762272, "phrase": "specific_nlp_task"}, {"score": 0.004065721092116729, "phrase": "specific_features"}, {"score": 0.0040190868373730015, "phrase": "semantic_representation"}, {"score": 0.003988293689448734, "phrase": "statistical_information"}, {"score": 0.0035263833746562788, "phrase": "nlp_applications"}, {"score": 0.003445906915464945, "phrase": "statistical_model"}, {"score": 0.00334144503360907, "phrase": "natural_language"}, {"score": 0.0029428772818833166, "phrase": "target_ltag._training"}, {"score": 0.002875678729214613, "phrase": "standard_hmm_training_algorithm"}, {"score": 0.002853622236571845, "phrase": "baum-welch"}, {"score": 0.0027992118597278087, "phrase": "training_algorithm"}, {"score": 0.0027670642397888494, "phrase": "better_solution"}, {"score": 0.002735284807307981, "phrase": "initial_state"}, {"score": 0.0026421139329830755, "phrase": "novel_em-based_semi-supervised_bootstrapping_algorithm"}, {"score": 0.002532526438148958, "phrase": "xtag-group"}, {"score": 0.002484242043387661, "phrase": "mica"}, {"score": 0.002446255658166246, "phrase": "bangalore"}, {"score": 0.0022911901892022847, "phrase": "empirical_results"}, {"score": 0.002221655243524476, "phrase": "satisfactory_way"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Tree adjoining grammar", " LTAG", " Hidden Markov model", " XTAG", " MICA"], "paper_abstract": "LTAG is a rich formalism for performing NLP tasks such as semantic interpretation, parsing, machine translation and information retrieval. Depend on the specific NLP task, different kinds of LTAGs for a language may be developed. Each of these LTAGs is enriched with some specific features such as semantic representation and statistical information that make them suitable to be used in that task. The distribution of these capabilities among the LTAGs makes it difficult to get the benefit from all of them in NLP applications. This paper discusses a statistical model to bridge between two kinds LTAGs for a natural language in order to benefit from the capabilities of both kinds. To do so, an HMM was trained that links an elementary tree sequence of a source LTAG onto an elementary tree sequence of a target LTAG. Training was performed by using the standard HMM training algorithm called Baum-Welch. To lead the training algorithm to a better solution, the initial state of the HMM was also trained by a novel EM-based semi-supervised bootstrapping algorithm. The model was tested on two English LTAGs, XTAG (XTAG-Group, 2001) and MICA's grammar (Bangalore et al., 2009) as the target and source LTAGs, respectively. The empirical results confirm that the model can provide a satisfactory way for linking these LTAGs to share their capabilities together. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Bridge the gap between statistical and hand-crafted grammars", "paper_id": "WOS:000318139300003"}