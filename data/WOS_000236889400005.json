{"auto_keywords": [{"score": 0.012591891290264445, "phrase": "faq_pages"}, {"score": 0.009312160028071063, "phrase": "proposed_answer"}, {"score": 0.004696608876994947, "phrase": "factoid_questions"}, {"score": 0.004604044728767603, "phrase": "qa"}, {"score": 0.0040048667115255, "phrase": "unsupervised_approach"}, {"score": 0.0032164136388931805, "phrase": "natural_language-posed_question"}, {"score": 0.0031686717525002935, "phrase": "phrase-based_query"}, {"score": 0.0030906666356950887, "phrase": "exact_match"}, {"score": 0.003044785536429499, "phrase": "off-the-shelf_search_engine"}, {"score": 0.0027420820738468577, "phrase": "posed_question"}, {"score": 0.002687922713487696, "phrase": "answer_language_model"}, {"score": 0.0025317627242523104, "phrase": "well-formed_answer"}, {"score": 0.002444883254621769, "phrase": "modular_fashion"}, {"score": 0.0023609780669713288, "phrase": "baseline_algorithms"}, {"score": 0.0022016885701010088, "phrase": "reasonable_performance"}, {"score": 0.002158179521369965, "phrase": "answer_accuracy"}, {"score": 0.002126111716875142, "phrase": "large_variety"}], "paper_keywords": [""], "paper_abstract": "In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions.", "paper_title": "Automatic Question Answering using the Web: Beyond the factoid", "paper_id": "WOS:000236889400005"}