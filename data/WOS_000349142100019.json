{"auto_keywords": [{"score": 0.03062284688869159, "phrase": "gpu"}, {"score": 0.017041368759880087, "phrase": "cpu"}, {"score": 0.0052681846818827364, "phrase": "fft."}, {"score": 0.00481495049065317, "phrase": "rectangular_matrices"}, {"score": 0.004736434518535051, "phrase": "matrix_transposition"}, {"score": 0.004678384952779725, "phrase": "important_algorithmic_building_block"}, {"score": 0.004416713079266374, "phrase": "storage_layout"}, {"score": 0.004272306164821805, "phrase": "gpu."}, {"score": 0.004221379262804442, "phrase": "high_performance"}, {"score": 0.0039362878637371574, "phrase": "good_fit"}, {"score": 0.0039040344230094164, "phrase": "gpu_architectures"}, {"score": 0.003793206550770237, "phrase": "board_memory_capacity"}, {"score": 0.0037621210719690594, "phrase": "high_throughput"}, {"score": 0.003700709435644458, "phrase": "direct_application"}, {"score": 0.0036104596826132965, "phrase": "transposition_algorithms"}, {"score": 0.003422371752150812, "phrase": "good_performance"}, {"score": 0.0033115124244647736, "phrase": "first_known_in-place_matrix_transposition_approach"}, {"score": 0.0030623570435946977, "phrase": "elementary_tiled-wise_transposition"}, {"score": 0.0029147175927233546, "phrase": "memory_transfer"}, {"score": 0.0028086649378423357, "phrase": "transposition_overhead"}, {"score": 0.002762774046280398, "phrase": "pcie_transfer"}, {"score": 0.002673223453963085, "phrase": "larger_tiles"}, {"score": 0.002656613512900431, "phrase": "cpu."}, {"score": 0.0023430556582539805, "phrase": "asynchronous_execution_scheme"}, {"score": 0.002314271047073693, "phrase": "cpu_threads"}, {"score": 0.002298962218497207, "phrase": "gpo"}, {"score": 0.002285839246740627, "phrase": "in-place_matrix_transposition"}, {"score": 0.0021845387990972543, "phrase": "data_transfers_costs"}, {"score": 0.002139986159978777, "phrase": "current_multi-threaded_implementations"}, {"score": 0.0021224200236681498, "phrase": "in-place_transposition"}], "paper_keywords": ["GPU", " Transposition", " In-Place"], "paper_abstract": "Matrix transposition is an important algorithmic building block for many numeric algorithms such as FFT. It has also been used to convert the storage layout of arrays. With more and more algebra libraries offloaded to GPUs, a high performance in-place transposition becomes necessary. Intuitively, in-place transposition should be a good fit for GPU architectures due to limited available on-board memory capacity and high throughput. However, direct application of CPU in-place transposition algorithms lacks the amount of parallelism and locality required by GPUs to achieve good performance. In this paper we present the first known in-place matrix transposition approach for the GPUs. Our implementation is based on a novel 3-stage transposition algorithm where each stage is performed using an elementary tiled-wise transposition. Additionally, when transposition is done as part of the memory transfer between GPU and host, our staged approach allows hiding transposition overhead by overlap with PCIe transfer. We show that the 3-stage algorithm allows larger tiles and achieves 3X speedup over a traditional 4-stage algorithm, with both algorithms based on our high-performance elementary transpositions on the GPU. We also show our proposed low-level optimizationsimprove the sustained throughput to more than 20 GB/s. Finally, we propose an asynchronous execution scheme that allows CPU threads to delegate in-place matrix transposition to GPO, achieving a throughput of more than 3.4 GB/s (including data transfers costs), and improving current multi-threaded implementations of in-place transposition on CPU.", "paper_title": "In-Place Transposition of Rectangular Matrices on Accelerators", "paper_id": "WOS:000349142100019"}