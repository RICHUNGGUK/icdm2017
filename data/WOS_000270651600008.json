{"auto_keywords": [{"score": 0.049561472167574526, "phrase": "duplicated_data"}, {"score": 0.00481495049065317, "phrase": "probabilistic_databases"}, {"score": 0.004685855684376706, "phrase": "major_source"}, {"score": 0.004462105963334103, "phrase": "duplicate_items"}, {"score": 0.004157561334726201, "phrase": "accurate_deduplication"}, {"score": 0.004090274535243564, "phrase": "difficult_task"}, {"score": 0.004046020290832207, "phrase": "imperfect_data_cleaning"}, {"score": 0.003916098498242888, "phrase": "valuable_information"}, {"score": 0.0038527044625533574, "phrase": "reasonable_alternative_approach"}, {"score": 0.0037087324991876727, "phrase": "correct_cleaning_strategy"}, {"score": 0.0035507457285983268, "phrase": "efficient_probabilistic_query-answering_technique"}, {"score": 0.003167217223469026, "phrase": "flexible_modular_framework"}, {"score": 0.0030821633718368206, "phrase": "probabilistic_database"}, {"score": 0.0030157628075954588, "phrase": "dirty_relation"}, {"score": 0.0028096544129939277, "phrase": "large_relations"}, {"score": 0.002779217021776702, "phrase": "string_data"}, {"score": 0.0025611773887867255, "phrase": "state-of-the-art_scalable_approximate_join_methods"}, {"score": 0.0023219374444738723, "phrase": "new_clustering_algorithms"}, {"score": 0.00217496508306238, "phrase": "inferred_probabilities"}, {"score": 0.0021049977753042253, "phrase": "duplicate_records"}], "paper_keywords": ["Probabilistic databases", " Duplicate detection", " String databases"], "paper_abstract": "A major source of uncertainty in databases is the presence of duplicate items, i.e., records that refer to the same real-world entity. However, accurate deduplication is a difficult task and imperfect data cleaning may result in loss of valuable information. A reasonable alternative approach is to keep duplicates when the correct cleaning strategy is not certain, and utilize an efficient probabilistic query-answering technique to return query results along with probabilities of each answer being correct. In this paper, we present a flexible modular framework for scalably creating a probabilistic database out of a dirty relation of duplicated data and overview the challenges raised in utilizing this framework for large relations of string data. We study the problem of associating probabilities with duplicates that are detected using state-of-the-art scalable approximate join methods. We argue that standard thresholding techniques are not sufficiently robust for this task, and propose new clustering algorithms suitable for inferring duplicates and their associated probabilities. We show that the inferred probabilities accurately reflect the error in duplicate records.", "paper_title": "Creating probabilistic databases from duplicated data", "paper_id": "WOS:000270651600008"}