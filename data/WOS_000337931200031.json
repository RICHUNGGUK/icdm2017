{"auto_keywords": [{"score": 0.049698358871674383, "phrase": "data_transfer_time"}, {"score": 0.04932092433893311, "phrase": "hybrid_clouds"}, {"score": 0.00481495049065317, "phrase": "job_execution"}, {"score": 0.004553265070135597, "phrase": "pareto_frontier"}, {"score": 0.004203803478570494, "phrase": "cloud_application"}, {"score": 0.003927848423072668, "phrase": "gene_probes"}, {"score": 0.003684634192576662, "phrase": "heavy_computational_task"}, {"score": 0.0033077610063168093, "phrase": "different_characteristics"}, {"score": 0.0027741761880779535, "phrase": "sensitive_information"}, {"score": 0.002644179117033107, "phrase": "execution_time"}, {"score": 0.002560907803820395, "phrase": "transfer_time"}, {"score": 0.0025101968585850474, "phrase": "hybrid_cloud"}, {"score": 0.002421428191952395, "phrase": "bi-objective_optimization_problem"}, {"score": 0.002363995836370621, "phrase": "particle_swarm_optimization"}, {"score": 0.0022987068521717765, "phrase": "pso-parfnt"}, {"score": 0.0022531762659857507, "phrase": "relevant_pareto_frontier"}, {"score": 0.0021821915946356168, "phrase": "new_insights"}, {"score": 0.0021561514733516676, "phrase": "complex_problem"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Big data", " Private data", " Cloud bursting", " Particle swarm optimization", " Pareto frontier"], "paper_abstract": "This paper proposes a solution to calculate the Pareto frontier for the execution of a batch of jobs versus data transfer time for hybrid clouds. Based on the nature of the cloud application, jobs are assumed to require a number of data-files from either public or private clouds. For example, gene probes can be used to identify various infection agents such as bacteria, viruses, etc. The heavy computational task of aligning probes of a patient's DNA (private-data) with normal sequences (public-data) with various data sizes is the key to this process. Such files have different characteristics depends on their nature and could be either allowed for replication or not in the cloud. Files could be too big to replicate (big data), others might be small enough to be replicated but they cannot be replicated as they contain sensitive information (private data). To show the relationship between the execution time of a batch of jobs and the transfer time needed for their required data in hybrid cloud, we first model this problem as a bi-objective optimization problem, and then propose a Particle Swarm Optimization (PSO)-based approach, called here PSO-ParFnt, to find the relevant Pareto frontier. The results are promising and provide new insights into this complex problem. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Pareto frontier for job execution and data transfer time in hybrid clouds", "paper_id": "WOS:000337931200031"}