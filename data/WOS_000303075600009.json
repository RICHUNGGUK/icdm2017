{"auto_keywords": [{"score": 0.032464786803174536, "phrase": "trajectory_data"}, {"score": 0.014340833089687784, "phrase": "parallel_computer"}, {"score": 0.01362792353067151, "phrase": "inter-node_communication"}, {"score": 0.01049868341480209, "phrase": "massively_parallel_computers"}, {"score": 0.009076711208296385, "phrase": "scattering_calculations"}, {"score": 0.008918963982889107, "phrase": "wide_variety"}, {"score": 0.00846184294297026, "phrase": "distributed_program"}, {"score": 0.008427657513334724, "phrase": "test_data"}, {"score": 0.006311820965873618, "phrase": "physical_equations"}, {"score": 0.00481495049065317, "phrase": "sassena_-_x-ray"}, {"score": 0.004775982436102607, "phrase": "molecular_dynamics_trajectories"}, {"score": 0.004724512842884256, "phrase": "molecular_dynamics"}, {"score": 0.00468627326602559, "phrase": "multi-million_atom_systems"}, {"score": 0.004610715835407973, "phrase": "subsequent_analysis"}, {"score": 0.004592016716046632, "phrase": "resulting_simulation_trajectories"}, {"score": 0.004561019117546361, "phrase": "high_performance_computing_problem"}, {"score": 0.0044935556350002475, "phrase": "x-ray_and_neutron_scattering_intensities"}, {"score": 0.00448153161355249, "phrase": "md"}, {"score": 0.004439097815805505, "phrase": "massively_parallel_supercomputers"}, {"score": 0.004409127897028587, "phrase": "data_staging"}, {"score": 0.004343901281737735, "phrase": "io_bandwidth_requirements"}, {"score": 0.004326279546609189, "phrase": "strong_scaling"}, {"score": 0.004297068016286485, "phrase": "petaflop_cray"}, {"score": 0.0042796354512206, "phrase": "oak_ridge_national_laboratory"}, {"score": 0.004268052880043907, "phrase": "virtually_linear"}, {"score": 0.004137075934831388, "phrase": "scaling_demands"}, {"score": 0.004125877593529025, "phrase": "different_types"}, {"score": 0.004081385289516914, "phrase": "high_performance_tool"}, {"score": 0.00405931864967869, "phrase": "large-scale_supercomputing"}, {"score": 0.004010851690259294, "phrase": "program"}, {"score": 0.003993829133258082, "phrase": "sassena_catalogue"}, {"score": 0.0039347219763836315, "phrase": "cpc_program_library"}, {"score": 0.003924069213584732, "phrase": "queen's_university"}, {"score": 0.003913445372405096, "phrase": "belfast"}, {"score": 0.0039028497942174777, "phrase": "n._ireland"}, {"score": 0.003881744671780119, "phrase": "gnu_general_public_license"}, {"score": 0.0036175055096536726, "phrase": "high_performance_network"}, {"score": 0.003578502608957923, "phrase": "linux"}, {"score": 0.0035687846293837805, "phrase": "osx"}, {"score": 0.0034827269754814593, "phrase": "mpi_directives"}, {"score": 0.003384937883076095, "phrase": "cmake"}, {"score": 0.003294350220187021, "phrase": "molecular_dynamics_simulations"}, {"score": 0.0032809721277161988, "phrase": "large_trajectories"}, {"score": 0.0032323807458348474, "phrase": "structural_and_dynamical_analysis"}, {"score": 0.0032105315278837423, "phrase": "analysis_algorithms"}, {"score": 0.003197492724448062, "phrase": "parallel_computation"}, {"score": 0.0031888295268343186, "phrase": "io_schemes"}, {"score": 0.003171573259648412, "phrase": "computational_task"}, {"score": 0.003158692174494277, "phrase": "practical_amount"}, {"score": 0.003137339485314611, "phrase": "particular_computational_and_io_requirements"}, {"score": 0.0031119061114737267, "phrase": "particular_analysis_algorithm"}, {"score": 0.0030409556209597404, "phrase": "different_projections"}, {"score": 0.0030163012350270025, "phrase": "single_scattering_function"}, {"score": 0.0029959082125682918, "phrase": "good_performance"}, {"score": 0.0029196605859915237, "phrase": "volatile_version"}, {"score": 0.0029077996390372996, "phrase": "whole_trajectory"}, {"score": 0.002884221684971253, "phrase": "high_performance"}, {"score": 0.0028764048059439205, "phrase": "good_scalability"}, {"score": 0.002829945165524915, "phrase": "data_locality"}, {"score": 0.0027281287776514665, "phrase": "scattering_calculation"}, {"score": 0.0026840576423314668, "phrase": "high_performance_computing_clusters"}, {"score": 0.002640696562204307, "phrase": "file_storage"}, {"score": 0.002480854266451858, "phrase": "necessary_data"}, {"score": 0.0024640724199520877, "phrase": "excellent_scalability"}, {"score": 0.002454057717474422, "phrase": "partitioning_scheme"}, {"score": 0.0024111277643627154, "phrase": "use_cases"}, {"score": 0.0023948164534491273, "phrase": "achieved_performance"}, {"score": 0.0023657330062474996, "phrase": "independent_scattering_vectors"}, {"score": 0.0023529203904298864, "phrase": "independent_parallel_partitions"}, {"score": 0.0022929995458532385, "phrase": "distribution_file"}, {"score": 0.002228541063618486, "phrase": "html_file"}, {"score": 0.0021658906393953736, "phrase": "usual_runtime"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["X-ray", " Neutron", " Scattering", " Molecular dynamics", " Massively parallel"], "paper_abstract": "Massively parallel computers now permit the molecular dynamics (MD) simulation of multi-million atom systems on time scales up to the microsecond. However, the subsequent analysis of the resulting simulation trajectories has now become a high performance computing problem in itself. Here, we present software for calculating X-ray and neutron scattering intensities from MD simulation data that scales well on massively parallel supercomputers. The calculation and data staging schemes used maximize the degree of parallelism and minimize the IO bandwidth requirements. The strong scaling tested on the jaguar Petaflop Cray XT5 at Oak Ridge National Laboratory exhibits virtually linear scaling up to 7000 cores for most benchmark systems. Since both MPI and thread parallelism is supported, the software is flexible enough to cover scaling demands for different types of scattering calculations. The result is a high performance tool capable of unifying large-scale supercomputing and a wide variety of neutron/synchrotron technology. Program summary Program title: Sassena Catalogue identifier: AELW_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AELW_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 1 003 742 No. of bytes in distributed program, including test data, etc.: 798 Distribution format: tar.gz Programming language: C++, OpenMPI Computer: Distributed Memory, Cluster of Computers with high performance network, Supercomputer Operating system: UNIX, LINUX, OSX Has the code been vectorized or parallelized?: Yes, the code has been parallelized using MPI directives. Tested with up to 7000 processors RAM: Up to 1 Gbytes/core Classification: 6.5, 8 External routines: Boost Library, FFTW3, CMAKE, GNU C++ Compiler, OpenMPI, LibXML, LAPACK Nature of problem: Recent developments in supercomputing allow molecular dynamics simulations to generate large trajectories spanning millions of frames and thousands of atoms. The structural and dynamical analysis of these trajectories requires analysis algorithms which use parallel computation and IO schemes to solve the computational task in a practical amount of time. The particular computational and IO requirements very much depend on the particular analysis algorithm. In scattering calculations a very frequent pattern is that the trajectory data is used multiple times to compute different projections and aggregates this into a single scattering function. Thus, for good performance the trajectory data has to be kept in memory and the parallel computer has to have enough RAM to store a volatile version of the whole trajectory. In order to achieve high performance and good scalability the mapping of the physical equations to a parallel computer needs to consider data locality and reduce the amount of the inter-node communication. Solution method: The physical equations for scattering calculations were analyzed and two major calculation schemes were developed to support any type of scattering calculation (all/self). Certain hardware aspects were taken into account, e.g. high performance computing clusters and supercomputers usually feature a 2 tier network system, with Ethernet providing the file storage and infiniband the inter-node communication via MPI calls. The time spent loading the trajectory data into memory is minimized by letting each core only read the trajectory data it requires. The performance of inter-node communication is maximized by exclusively utilizing the appropriate MPI calls to exchange the necessary data, resulting in an excellent scalability. The partitioning scheme developed to map the calculation onto a parallel computer covers a wide variety of use cases without negatively effecting the achieved performance. This is done through a 2D partitioning scheme where independent scattering vectors are assigned to independent parallel partitions and all communication is local to the partition. Additional comments: !!!!! The distribution file for this program is approximately 36 Mbytes and therefore is not delivered directly when download or E-mail is requested. Instead an html file giving details of how the program can be obtained is sent. !!!!! Running time: Usual runtime spans from 1 min on 20 nodes to 2 h on 2000 nodes. That is 0.5-4000 CPU hours per execution. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Sassena - X-ray and neutron scattering calculated from molecular dynamics trajectories using massively parallel computers", "paper_id": "WOS:000303075600009"}