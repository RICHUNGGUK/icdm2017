{"auto_keywords": [{"score": 0.04262239385376984, "phrase": "mobile_robot"}, {"score": 0.00481495049065317, "phrase": "robust_visual_human_detection_approach"}, {"score": 0.004779085953614184, "phrase": "ukf-based_motion_tracking"}, {"score": 0.004708152527097594, "phrase": "robust_tracking"}, {"score": 0.0046037124365759215, "phrase": "video_sequence"}, {"score": 0.004552360432689039, "phrase": "essential_prerequisite"}, {"score": 0.004501578643090429, "phrase": "increasing_number"}, {"score": 0.00430402915573404, "phrase": "human_user"}, {"score": 0.004224286804379592, "phrase": "human-inhabited_environment"}, {"score": 0.004130535674260505, "phrase": "robust_approach"}, {"score": 0.003934456513187142, "phrase": "onboard_rgb-d_sensor"}, {"score": 0.0036781781340563748, "phrase": "proposed_approach"}, {"score": 0.0036507498735369576, "phrase": "real-time_computation_power"}, {"score": 0.003569681826459428, "phrase": "new_ideas"}, {"score": 0.003543059748318891, "phrase": "well-established_techniques"}, {"score": 0.0034904076588243315, "phrase": "proposed_method"}, {"score": 0.0034643747383816164, "phrase": "background_subtraction"}, {"score": 0.003412887952564431, "phrase": "depth_segmentation_detector"}, {"score": 0.0033747739383101095, "phrase": "matching_method"}, {"score": 0.003324614337607016, "phrase": "human_tracking"}, {"score": 0.003275197814021, "phrase": "novel_concept"}, {"score": 0.003226513431715486, "phrase": "hand_creation"}, {"score": 0.0030617295196636173, "phrase": "human_silhouette"}, {"score": 0.0030275252142439213, "phrase": "dynamic_environment"}, {"score": 0.002606264765409507, "phrase": "relatively_high_computation_time"}, {"score": 0.0025771355599741915, "phrase": "silhouette-matching-based_method"}, {"score": 0.002463830258460528, "phrase": "matching-based_method"}, {"score": 0.002391067964886602, "phrase": "unscented_kalman_filter"}, {"score": 0.002337906603857861, "phrase": "human_location"}, {"score": 0.002311769781106355, "phrase": "image_frame"}, {"score": 0.002251911996999611, "phrase": "robot_motion"}, {"score": 0.0021609589091422608, "phrase": "real_experiment"}, {"score": 0.0021049977753042253, "phrase": "indoor_environment"}], "paper_keywords": ["Distance transform (DT)", " head and hand creation", " human silhouette", " projection histogram", " unscented Kalman filter (UKF)"], "paper_abstract": "Robust tracking of a human in a video sequence is an essential prerequisite to an increasing number of applications, where a robot needs to interact with a human user or operates in a human-inhabited environment. This paper presents a robust approach that enables a mobile robot to detect and track a human using an onboard RGB-D sensor. Such robots could be used for security, surveillance, and assistive robotics applications. The proposed approach has real-time computation power through a unique combination of new ideas and well-established techniques. In the proposed method, background subtraction is combined with depth segmentation detector and template matching method to initialize the human tracking automatically. A novel concept of head and hand creation based on depth of interest is introduced in this paper to track the human silhouette in a dynamic environment, when the robot is moving. To make the algorithm robust, a series of detectors (e.g., height, size, and shape) is utilized to distinguish target human from other objects. Because of the relatively high computation time of the silhouette-matching-based method, a confidence level is defined, which allows using the matching-based method only where it is imperative. An unscented Kalman filter is used to predict the human location in the image frame to maintain the continuity of the robot motion. The efficacy of the approach is demonstrated through a real experiment on a mobile robot navigating in an indoor environment.", "paper_title": "A Robust Visual Human Detection Approach With UKF-Based Motion Tracking for a Mobile Robot", "paper_id": "WOS:000365406200025"}