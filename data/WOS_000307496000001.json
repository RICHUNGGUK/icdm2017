{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "irrelevant_variables"}, {"score": 0.004523865550323285, "phrase": "relevant_and_irrelevant_boolean_variables"}, {"score": 0.003693533648584145, "phrase": "natural_family"}, {"score": 0.0034700069041302003, "phrase": "relevant_variables"}, {"score": 0.003389704997952315, "phrase": "small_advantage"}, {"score": 0.0033372020049592726, "phrase": "random_guessing"}, {"score": 0.0032599633778910516, "phrase": "main_result"}, {"score": 0.0030387767523076528, "phrase": "error_probabilities"}, {"score": 0.002559072326933071, "phrase": "positive_constant"}, {"score": 0.0024418934977061876, "phrase": "accurate_learning"}, {"score": 0.002188874450946842, "phrase": "high_confidence"}, {"score": 0.0021049977753042253, "phrase": "individual_variable"}], "paper_keywords": ["feature selection", " generalization", " learning theory"], "paper_abstract": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.", "paper_title": "On the Necessity of Irrelevant Variables", "paper_id": "WOS:000307496000001"}