{"auto_keywords": [{"score": 0.03733316966031278, "phrase": "search_space"}, {"score": 0.034575479494292, "phrase": "glc"}, {"score": 0.033271145227488884, "phrase": "bayesian_networks"}, {"score": 0.02388389503667773, "phrase": "dominated_models"}, {"score": 0.00481495049065317, "phrase": "generalized_latent_class_analysis_based_on_model_dominance_theory._latent_class_analysis"}, {"score": 0.00447341965028879, "phrase": "generalized_latent_class"}, {"score": 0.004299621153829837, "phrase": "huge_space"}, {"score": 0.004132546937996706, "phrase": "computational_cost"}, {"score": 0.0040171817997972335, "phrase": "model_topology"}, {"score": 0.0036073064072936626, "phrase": "strong_pruning"}, {"score": 0.0035065523764817143, "phrase": "significant_reduction"}, {"score": 0.0030955250132409964, "phrase": "categorical_data"}, {"score": 0.0030606302942851027, "phrase": "glc_models"}, {"score": 0.0029920129598858545, "phrase": "local_dependence_problem"}, {"score": 0.002958281631770322, "phrase": "latent_class_analysis"}, {"score": 0.0028111220617485985, "phrase": "flexible_topology"}, {"score": 0.0027325449069867222, "phrase": "large_increase"}, {"score": 0.0026864540069792275, "phrase": "learning_complexity"}, {"score": 0.0025383451160985488, "phrase": "related_theoretical_results"}, {"score": 0.0023313149689465386, "phrase": "efficient_learning_algorithm"}, {"score": 0.002305014991156598, "phrase": "glc."}, {"score": 0.002279011030331704, "phrase": "core_technique"}, {"score": 0.0021049977753042253, "phrase": "significant_pruning"}], "paper_keywords": ["Latent class", " dominance", " learning", " Bayesian networks"], "paper_abstract": "Latent class analysis is a popular statistical learning approach. A major challenge for learning generalized latent class is the complexity in searching the huge space of models and parameters. The computational cost is higher when the model topology is more flexible. In this paper, we propose the notion of dominance which can lead to strong pruning of the search space and significant reduction of learning complexity, and apply this notion to the Generalized Latent Class (GLC) models, a class of Bayesian networks for clustering categorical data. GLC models can address the local dependence problem in latent class analysis by assuming a very general graph structure. However, The flexible topology of GLC leads to large increase of the learning complexity. We first propose the concept of dominance and related theoretical results which is general for all Bayesian networks. Based on dominance, we propose an efficient learning algorithm for GLC. A core technique to prune dominated models is regularization, which can eliminate dominated models, leading to significant pruning of the search space. Significant improvements on the model", "paper_title": "GENERALIZED LATENT CLASS ANALYSIS BASED ON MODEL DOMINANCE THEORY", "paper_id": "WOS:000271353000007"}