{"auto_keywords": [{"score": 0.04705125463422358, "phrase": "zou"}, {"score": 0.01050189597957353, "phrase": "hastie"}, {"score": 0.00481495049065317, "phrase": "learning_theory"}, {"score": 0.004690952711665194, "phrase": "statistical_learning_theory"}, {"score": 0.00457013354092966, "phrase": "so-called_elastic-net_regularization_scheme"}, {"score": 0.00443308765153716, "phrase": "h._zou"}, {"score": 0.004394688617163029, "phrase": "t._hastie"}, {"score": 0.004244375827831527, "phrase": "elastic_net"}, {"score": 0.00420760449108585, "phrase": "j._r._stat"}, {"score": 0.003806866174028117, "phrase": "correlated_variables"}, {"score": 0.0037249113156851013, "phrase": "statistical_properties"}, {"score": 0.0035199624440843892, "phrase": "suitable_mathematical_framework"}, {"score": 0.003459191419478298, "phrase": "random-design_regression"}, {"score": 0.003384695931421288, "phrase": "response_variable"}, {"score": 0.0032546105164772995, "phrase": "prediction_functions"}, {"score": 0.00321236577162671, "phrase": "linear_combinations"}, {"score": 0.0031023662967184216, "phrase": "infinite-dimensional_dictionary"}, {"score": 0.0030092017034068666, "phrase": "regression_function"}, {"score": 0.0029701328654852246, "phrase": "sparse_representation"}, {"score": 0.002831158147480665, "phrase": "particular_\"elastic-net_representation"}, {"score": 0.002675252546913683, "phrase": "data_increases"}, {"score": 0.002640507929183363, "phrase": "elastic-net_estimator"}, {"score": 0.0024842242921857705, "phrase": "finite-sample_bounds"}, {"score": 0.0024519545383399773, "phrase": "adaptive_scheme"}, {"score": 0.0024095777415804346, "phrase": "regularization_parameter"}, {"score": 0.002357632753637638, "phrase": "convex_analysis_tools"}, {"score": 0.0023068049914566975, "phrase": "iterative_thresholding_algorithm"}, {"score": 0.002266931093770838, "phrase": "elastic-net_solution"}, {"score": 0.0022084059770026416, "phrase": "optimization_procedure"}, {"score": 0.0021607883988873492, "phrase": "above-cited_work"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Learning", " Regularization", " Sparsity", " Elastic net"], "paper_abstract": "Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie [H. Zou,T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B, 67(2) (2005) 301-320] for the selection of groups of correlated variables. To investigate the statistical properties of this scheme and in particular its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combinations of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular \"elastic-net representation\" of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in the above-cited work. (C) 2009 Elsevier Inc. All rights reserved.", "paper_title": "Elastic-net regularization in learning theory", "paper_id": "WOS:000265303600007"}