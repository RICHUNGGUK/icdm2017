{"auto_keywords": [{"score": 0.04972711494665694, "phrase": "mobile_devices"}, {"score": 0.046339294991504756, "phrase": "user_intent"}, {"score": 0.00481495049065317, "phrase": "mobile_task_recommendation"}, {"score": 0.004623503587186056, "phrase": "personal_concierge"}, {"score": 0.004374593906165761, "phrase": "meaningful_and_personalized_suggestions"}, {"score": 0.004294625285753825, "phrase": "existing_efforts"}, {"score": 0.0040783736687590635, "phrase": "voice_query"}, {"score": 0.003989046949801574, "phrase": "new_and_alternative_perspective"}, {"score": 0.003858697684681798, "phrase": "visual_signal"}, {"score": 0.003802135020791801, "phrase": "built-in_camera"}, {"score": 0.0036914757355166966, "phrase": "\"visual_intent"}, {"score": 0.003557653536102669, "phrase": "visual_form"}, {"score": 0.003466865016432561, "phrase": "visual_intent"}, {"score": 0.003378385493203297, "phrase": "taptell"}, {"score": 0.003292395087148129, "phrase": "windows"}, {"score": 0.003196292402522254, "phrase": "user_interaction"}, {"score": 0.0031727643282003976, "phrase": "rich_context"}, {"score": 0.0031377955308198634, "phrase": "interactive_visual_searches"}, {"score": 0.0031146967095233586, "phrase": "contextual_recommendations"}, {"score": 0.003069006373093756, "phrase": "taptell_system"}, {"score": 0.003035177667751495, "phrase": "mobile_user"}, {"score": 0.002850385670169814, "phrase": "different_drawing_patterns"}, {"score": 0.0027673496368158545, "phrase": "search-based_recognition"}, {"score": 0.0027368368719283298, "phrase": "proposed_large-scale_context-embedded_vocabulary_tree"}, {"score": 0.002541817937194794, "phrase": "mobile_tasks"}, {"score": 0.0021522261505574035, "phrase": "image_retrieval"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Visual intent", " Mobile visual search", " Interactive visual search", " Image retrieval", " Mobile recommendation", " Natural user interface", " Mobile user intention", " Visual vocabulary"], "paper_abstract": "Mobile devices are becoming ubiquitous. People use them as personal concierge to search information and make decisions. Therefore, understanding user intent and subsequently provide meaningful and personalized suggestions is important. While existing efforts have predominantly focused on understanding the intent expressed by a textual or a voice query, this paper presents a new and alternative perspective which understands user intent visually, i.e., via visual signal captured by the built-in camera. We call this kind of intent \"visual intent\" as it can be naturally expressed through a visual form. To accomplish the discovery of visual intent on the phone, we develop TapTell, an exemplary real application on Windows Phone seven, by taking advantages of user interaction and rich context to enable interactive visual searches and contextual recommendations. Through the TapTell system, a mobile user can take a photo and indicate an object-of-interest within the photo via different drawing patterns. Then, the system performs a search-based recognition using a proposed large-scale context-embedded vocabulary tree. Finally, contextually relevant entities (i.e., local businesses) are recommended to the user for completing mobile tasks (those tasks which are natural to be raised and subsequently executed when the user utilizes mobile devices). We evaluated TapTell in a variety of scenarios with millions of images, and compared our results to state-of-the-art algorithms for image retrieval. (C) 2015 Elsevier Inc. All rights reserved.", "paper_title": "TapTell: Interactive visual search for mobile task recommendation", "paper_id": "WOS:000352119100011"}