{"auto_keywords": [{"score": 0.048160672145929125, "phrase": "perry's_conjugate_gradient_method"}, {"score": 0.04579719782409401, "phrase": "perry's_method"}, {"score": 0.04257746263992029, "phrase": "new_class"}, {"score": 0.038366351582223346, "phrase": "spectral_scaling_parameter"}, {"score": 0.03321250447563154, "phrase": "k.h._phua"}, {"score": 0.032837811446900414, "phrase": "unconstrained_multivariate_functions"}, {"score": 0.03265199947883932, "phrase": "acm_transactions"}, {"score": 0.032467203627889435, "phrase": "mathematical_software"}, {"score": 0.00481495049065317, "phrase": "efficient_training_algorithms"}, {"score": 0.0047590526875526335, "phrase": "conjugate_gradient_optimization_methods"}, {"score": 0.004676414661927138, "phrase": "existing_conjugate_gradient_training_algorithms"}, {"score": 0.0045418465969859064, "phrase": "a._perry"}, {"score": 0.004502232973788714, "phrase": "modified_conjugate_gradient_algorithm"}, {"score": 0.0042344044308802445, "phrase": "unconstrained_optimization"}, {"score": 0.00412453424255527, "phrase": "mlp_training"}, {"score": 0.004041046382656947, "phrase": "conjugate_gradient"}, {"score": 0.003913354746030144, "phrase": "cg"}, {"score": 0.0038005481380011677, "phrase": "hestenes-stiefel"}, {"score": 0.003778402618021389, "phrase": "fletcher-reeves"}, {"score": 0.0037563834177836287, "phrase": "polak-ribiere"}, {"score": 0.0035952667506642033, "phrase": "j.m._borwein"}, {"score": 0.0035534843024440706, "phrase": "ima"}, {"score": 0.0035224543064259553, "phrase": "numerical_analysis"}, {"score": 0.0033911075605164804, "phrase": "second_order_information"}, {"score": 0.003351683787996572, "phrase": "hessian_matrix"}, {"score": 0.0032742013937442182, "phrase": "cg_training"}, {"score": 0.0032456080226031417, "phrase": "efficient_line_search_technique"}, {"score": 0.0032078703296827436, "phrase": "wolfe_conditions"}, {"score": 0.003179854375044433, "phrase": "safeguarded_cubic_interpolation"}, {"score": 0.00316131270466841, "phrase": "d.f._shanno"}, {"score": 0.002964332290433157, "phrase": "initial_learning_rate_parameter"}, {"score": 0.0029212989109921594, "phrase": "line_search_technique"}, {"score": 0.0028370919298236724, "phrase": "closed_formula"}, {"score": 0.0026758704908948563, "phrase": "d.g._sotiropoulos"}, {"score": 0.0026602596982627, "phrase": "a.e._kostopoulos"}, {"score": 0.0026447397361111045, "phrase": "t.n._grapsa"}, {"score": 0.0025385994743190796, "phrase": "d.t._tsahalis"}, {"score": 0.002523886659439616, "phrase": "ed"}, {"score": 0.0024944216175108126, "phrase": "fourth_gracm_congress"}, {"score": 0.0024798667494928795, "phrase": "computational_mechanics"}, {"score": 0.002352626922459313, "phrase": "efficient_restarting_procedure"}, {"score": 0.0022714400852767045, "phrase": "cg_training_algorithms"}, {"score": 0.002258183361711707, "phrase": "experimental_results"}, {"score": 0.0021422934905256956, "phrase": "better_success_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural network", " Training", " Self-scaled conjugate gradient", " Perry's method", " Line search"], "paper_abstract": "This article presents some efficient training algorithms, based on conjugate gradient optimization methods. In addition to the existing conjugate gradient training algorithms, we introduce Perry's conjugate gradient method as a training algorithm [A. Perry, A modified conjugate gradient algorithm, Operations Research 26 (1978) 26-43]. Perry's method has been proven to be a very efficient method in the context of unconstrained optimization, but it has never been used in MLP training. Furthermore, a new class of conjugate gradient (CG) methods is proposed, called self-scaled CG methods, which are derived from the principles of Hestenes-Stiefel, Fletcher-Reeves, Polak-Ribiere and Perry's method. This class is based on the spectral scaling parameter introduced in [J. Barzilai, J.M. Borwein, Two point step size gradient methods, IMA journal of Numerical Analysis 8 (1988) 141-148]. The spectral scaling parameter contains second order information without estimating the Hessian matrix. Furthermore, we incorporate to the CG training algorithms an efficient line search technique based on the Wolfe conditions and on safeguarded cubic interpolation [D.F. Shanno, K.H. Phua, Minimization of unconstrained multivariate functions, ACM Transactions on Mathematical Software 2 (1976) 87-94]. In addition, the initial learning rate parameter, fed to the line search technique, was automatically adapted at each iteration by a closed formula proposed in [D.F. Shanno, K.H. Phua, Minimization of unconstrained multivariate functions, ACM Transactions on Mathematical Software 2 (1976) 87-94; D.G. Sotiropoulos, A.E. Kostopoulos, T.N. Grapsa, A spectral version of Perry's conjugate gradient method for neural network training, in: D.T. Tsahalis (Ed.), Fourth GRACM Congress on Computational Mechanics, vol. 1, 2002, pp. 172-179]. Finally, an efficient restarting procedure was employed in order to further improve the effectiveness of the CG training algorithms. Experimental results show that, in general, the new class of methods can perform better with a much lower computational cost and better success performance. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Self-scaled conjugate gradient training algorithms", "paper_id": "WOS:000268733700029"}