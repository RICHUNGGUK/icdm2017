{"auto_keywords": [{"score": 0.04687100580934199, "phrase": "latent_variables"}, {"score": 0.02150427124185177, "phrase": "model_parameters"}, {"score": 0.0074836562080680385, "phrase": "model_evidence"}, {"score": 0.00481495049065317, "phrase": "variational_bayesian_expectation-maximization"}, {"score": 0.004597754207658615, "phrase": "probabilistic_models"}, {"score": 0.004349955692802962, "phrase": "standard_technique"}, {"score": 0.004309968592440261, "phrase": "practical_bayesian_inference"}, {"score": 0.004096493540086106, "phrase": "conjugate-exponential_family_models"}, {"score": 0.0036496814103475174, "phrase": "exact_way"}, {"score": 0.003468796799802612, "phrase": "lsvb_approach"}, {"score": 0.0034368818187904744, "phrase": "better_estimates"}, {"score": 0.0032215231177304513, "phrase": "vbem_approach"}, {"score": 0.002950608985750058, "phrase": "practical_implementation"}, {"score": 0.00274017282519193, "phrase": "approximate_distribution"}, {"score": 0.0025447065763362984, "phrase": "folsvb_algorithm"}, {"score": 0.0024750474634399797, "phrase": "vbem_algorithm"}, {"score": 0.0023741093119014436, "phrase": "lsvb"}, {"score": 0.002341384304635523, "phrase": "recently_proposed_collapsed_variational_methods"}, {"score": 0.0023198180380964305, "phrase": "general_conjugate-exponential_families"}, {"score": 0.002245914733515068, "phrase": "gaussians"}, {"score": 0.0022046982718304862, "phrase": "bernoullis"}, {"score": 0.0021843879253500894, "phrase": "synthetic_and_real-world_data_sets"}], "paper_keywords": ["Bayesian inference", " conjugate-exponential family", " variational method", " mixture of Gaussians", " mixture of Bernoullis"], "paper_abstract": "Variational Bayesian Expectation-Maximization (VBEM), an approximate inference method for probabilistic models based on factorizing over latent variables and model parameters, has been a standard technique for practical Bayesian inference. In this paper, we introduce a more general approximate inference framework for conjugate-exponential family models. which we call Latent-Space Variational Bayes (LSVB). In this approach, we integrate out the model parameters in an exact way, leaving only the latent variables. It can be shown that the LSVB approach gives better estimates of the model evidence as well as the distribution over the latent variables than the VBEM approach, but, in practice, the distribution over the latent variables has to be approximated. As a practical implementation, we present a First-order LSVB (FoLSVB) algorithm to approximate the distribution over the latent variables. From this approximate distribution, one can also estimate the model evidence and the posterior over the model parameters. The FoLSVB algorithm is directly comparable to the VBEM algorithm and has the same computational complexity. We discuss how LSVB generalizes the recently proposed collapsed variational methods to general conjugate-exponential families. Examples based on mixtures of Gaussians and mixtures of Bernoullis with synthetic and real-world data sets are used to illustrate some advantages of our method over VBEM.", "paper_title": "Latent-Space Variational Bayes", "paper_id": "WOS:000260033900013"}