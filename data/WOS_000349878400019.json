{"auto_keywords": [{"score": 0.04305504521414548, "phrase": "mop"}, {"score": 0.03591803764291837, "phrase": "task_allocation"}, {"score": 0.00481495049065317, "phrase": "concurrent_markov_decision"}, {"score": 0.004568326972997448, "phrase": "multi-agent_learning"}, {"score": 0.00443308765153716, "phrase": "decision_theoretic_sense"}, {"score": 0.004174451361776433, "phrase": "single_markov_decision_process"}, {"score": 0.00396050388346298, "phrase": "model_agent_behaviour"}, {"score": 0.003591733916159209, "phrase": "multi-agent_learning_problem"}, {"score": 0.003459191419478298, "phrase": "individual_learning"}, {"score": 0.003232779373179631, "phrase": "concurrent_mdp"}, {"score": 0.002976019715894587, "phrase": "decentralized_mdp."}, {"score": 0.002931569771634119, "phrase": "individual_mop_problem"}, {"score": 0.0028233196851239753, "phrase": "q-learning_algorithm"}, {"score": 0.002678433668156676, "phrase": "locally_optimal_reward_maximization_policy"}, {"score": 0.0024287485834314027, "phrase": "heterogeneous_team_foraging_case_study"}, {"score": 0.0023040642263522505, "phrase": "cmdp-based_learning_mechanisms"}, {"score": 0.0022022878843247257, "phrase": "total_agent_learning_effort"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multi-agent learning", " Robot team", " Heterogeneous team", " Reinforcement learning", " Markov decision process"], "paper_abstract": "Multi-agent learning, in a decision theoretic sense, may run into deficiencies if a single Markov decision process (MOP) is used to model agent behaviour. This paper discusses an approach to overcoming such deficiencies by considering a multi-agent learning problem as a concurrence between individual learning and task allocation MDPs. This approach, called Concurrent MDP (CMDP), is contrasted with other MDP models, including decentralized MDP. The individual MOP problem is solved by a Q-Learning algorithm, guaranteed to settle on a locally optimal reward maximization policy. For the task allocation MOP, several different concurrent individual and social learning solutions are considered. Through a heterogeneous team foraging case study, it is shown that the CMDP-based learning mechanisms reduce both simulation time and total agent learning effort. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Concurrent Markov decision processes for robot team learning", "paper_id": "WOS:000349878400019"}