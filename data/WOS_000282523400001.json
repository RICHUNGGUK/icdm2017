{"auto_keywords": [{"score": 0.047844536713056586, "phrase": "margin_losses"}, {"score": 0.045915154498909476, "phrase": "proper_loss"}, {"score": 0.045288645062415214, "phrase": "link_function"}, {"score": 0.03945629000021421, "phrase": "composite_binary_losses"}, {"score": 0.037851725528819796, "phrase": "proper_losses"}, {"score": 0.00481495049065317, "phrase": "binary_classification"}, {"score": 0.004769182299692333, "phrase": "class_probability_estimation"}, {"score": 0.004546755482549647, "phrase": "general_composite_losses"}, {"score": 0.004172094622123562, "phrase": "proper_composite_losses"}, {"score": 0.00401560412621627, "phrase": "symmetric_loss"}, {"score": 0.003791763445590174, "phrase": "intrinsic_parametrisation"}, {"score": 0.0036845490951857617, "phrase": "complete_characterisation"}, {"score": 0.0035632774896594524, "phrase": "\"classification_calibrated\"_losses"}, {"score": 0.003413183361838603, "phrase": "\"best\"_surrogate_binary_loss"}, {"score": 0.003332537728389913, "phrase": "precise_notion"}, {"score": 0.0032074366230426727, "phrase": "exist_situations"}, {"score": 0.003072285184441679, "phrase": "complete_explicit_characterisation"}, {"score": 0.002887028262617443, "phrase": "weight_function"}, {"score": 0.00276534038425712, "phrase": "composite_loss"}, {"score": 0.0026999603129904902, "phrase": "new_ways"}, {"score": 0.0026742419973883134, "phrase": "\"surrogate_tuning"}, {"score": 0.0025861368747446324, "phrase": "explicit_characterisation"}, {"score": 0.0025492922498807943, "phrase": "bregman"}, {"score": 0.002500927169609655, "phrase": "unit_interval"}, {"score": 0.0023388183537341213, "phrase": "new_algorithm-independent_results"}, {"score": 0.002187194307878705, "phrase": "binary_losses"}, {"score": 0.0021049977753042253, "phrase": "non-robust_to_misclassification_noise"}], "paper_keywords": ["surrogate loss", " convexity", " probability estimation", " classification", " Fisher consistency", " classification-calibrated", " regret bound", " proper scoring rule", " Bregman divergence", " robustness", " misclassification noise"], "paper_abstract": "We study losses for binary classification and class probability estimation and extend the understanding of them from margin losses to general composite losses which are the composition of a proper loss with a link function. We characterise when margin losses can be proper composite losses, explicitly show how to determine a symmetric loss in full from half of one of its partial losses, introduce an intrinsic parametrisation of composite binary losses and give a complete characterisation of the relationship between proper losses and \"classification calibrated\" losses. We also consider the question of the \"best\" surrogate binary loss. We introduce a precise notion of \"best\" and show there exist situations where two convex surrogate losses are incommensurable. We provide a complete explicit characterisation of the convexity of composite binary losses in terms of the link function and the weight function associated with the proper loss which make up the composite loss. This characterisation suggests new ways of \"surrogate tuning\" as well as providing an explicit characterisation of when Bregman divergences on the unit interval are convex in their second argument. Finally, in an appendix we present some new algorithm-independent results on the relationship between properness, convexity and robustness to misclassification noise for binary losses and show that all convex proper losses are non-robust to misclassification noise.", "paper_title": "Composite Binary Losses", "paper_id": "WOS:000282523400001"}