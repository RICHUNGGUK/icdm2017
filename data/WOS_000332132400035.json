{"auto_keywords": [{"score": 0.04252420525659764, "phrase": "sparse_signals"}, {"score": 0.015719716506582538, "phrase": "compressive_sampling"}, {"score": 0.0038810583785936505, "phrase": "first_model"}, {"score": 0.00374399358035391, "phrase": "standard_recovery"}, {"score": 0.0035794251590584563, "phrase": "second_one"}, {"score": 0.00333097896868937, "phrase": "noisy_observations"}, {"score": 0.0029901699209266435, "phrase": "convergence_behavior"}, {"score": 0.0026600905156986317, "phrase": "first_case"}, {"score": 0.00249767641333441, "phrase": "global_minimum"}, {"score": 0.0024311346712789553, "phrase": "objective_function"}, {"score": 0.0023451553255837317, "phrase": "second_case"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural circuit", " Stability"], "paper_abstract": "This paper develops two neural network models, based on Lagrange programming neural networks (LPNNs), for recovering sparse signals in compressive sampling. The first model is for the standard recovery of sparse signals. The second one is for the recovery of sparse signals from noisy observations. Their properties, including the optimality of the solutions and the convergence behavior of the networks, are analyzed. We show that for the first case, the network converges to the global minimum of the objective function. For the second case, the convergence is locally stable. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Recurrent networks for compressive sampling", "paper_id": "WOS:000332132400035"}