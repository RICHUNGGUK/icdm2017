{"auto_keywords": [{"score": 0.040102648840296407, "phrase": "clir"}, {"score": 0.004814953351200263, "phrase": "cross-language"}, {"score": 0.004701015188740582, "phrase": "latent_topic_models"}, {"score": 0.004645057834614705, "phrase": "document-aligned_comparable_corpora"}, {"score": 0.0044990584725586764, "phrase": "different_applications"}, {"score": 0.004463278123628969, "phrase": "cross-language_latent_topic_models"}, {"score": 0.004340263267055143, "phrase": "first_focus"}, {"score": 0.004237512575106522, "phrase": "cross-language_information_retrieval"}, {"score": 0.004137184278868391, "phrase": "bilingual_latent_dirichlet_allocation_model"}, {"score": 0.003758981686762617, "phrase": "additional_translation_resources"}, {"score": 0.003684634192576662, "phrase": "second_focus"}, {"score": 0.003583002730249742, "phrase": "translation_candidates"}, {"score": 0.0035544816252832375, "phrase": "semantically_related_words"}, {"score": 0.0034841647157189985, "phrase": "cross-language_latent_topic_model"}, {"score": 0.003255295296893174, "phrase": "per-document_topic_distributions"}, {"score": 0.0032164910932827168, "phrase": "per-topic_word_distributions"}, {"score": 0.0030904433394138963, "phrase": "extracted_lexicon"}, {"score": 0.003005151718290603, "phrase": "novel_evidence-rich_statistical_model"}, {"score": 0.002752075650871066, "phrase": "standard_test_collections"}, {"score": 0.0026336242881032645, "phrase": "alternative_evaluation"}, {"score": 0.002530359917257626, "phrase": "known-item_search"}, {"score": 0.0025001755089445092, "phrase": "test_subset"}, {"score": 0.0024802523752944536, "phrase": "wikipedia_articles"}, {"score": 0.0024506642262508735, "phrase": "main_importance"}, {"score": 0.0023451553255837317, "phrase": "translation_resources"}, {"score": 0.002326464619238653, "phrase": "comparable_document-aligned_corpora"}, {"score": 0.0022987068521717765, "phrase": "novel_clir_statistical_models"}, {"score": 0.002199725773957207, "phrase": "better_clir_results"}, {"score": 0.002147540520631409, "phrase": "additional_external_resources"}, {"score": 0.002121913011761504, "phrase": "parallel_corpora"}, {"score": 0.0021049977753042253, "phrase": "machine-readable_dictionaries"}], "paper_keywords": ["Cross-language information retrieval", " Unsupervised cross-language lexicon extraction", " Probabilistic latent topic models", " Evidence-rich retrieval models"], "paper_abstract": "In this paper, we study different applications of cross-language latent topic models trained on comparable corpora. The first focus lies on the task of cross-language information retrieval (CLIR). The Bilingual Latent Dirichlet allocation model (BiLDA) allows us to create an interlingual, language-independent representation of both queries and documents. We construct several BiLDA-based document models for CLIR, where no additional translation resources are used. The second focus lies on the methods for extracting translation candidates and semantically related words using only per-topic word distributions of the cross-language latent topic model. As the main contribution, we combine the two former steps, blending the evidences from the per-document topic distributions and the per-topic word distributions of the topic model with the knowledge from the extracted lexicon. We design and evaluate the novel evidence-rich statistical model for CLIR, and prove that such a model, which combines various (only internal) evidences, obtains the best scores for experiments performed on the standard test collections of the CLEF 2001-2003 campaigns. We confirm these findings in an alternative evaluation, where we automatically generate queries and perform the known-item search on a test subset of Wikipedia articles. The main importance of this work lies in the fact that we train translation resources from comparable document-aligned corpora and provide novel CLIR statistical models that exhaustively exploit as many cross-lingual clues as possible in the quest for better CLIR results, without use of any additional external resources such as parallel corpora or machine-readable dictionaries.", "paper_title": "Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora", "paper_id": "WOS:000319273400002"}