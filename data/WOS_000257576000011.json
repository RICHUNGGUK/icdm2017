{"auto_keywords": [{"score": 0.04806645810598007, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "high-dimensional_survival_data"}, {"score": 0.004662427600500598, "phrase": "support_vector_machines"}, {"score": 0.004143196873246147, "phrase": "partly_censored_survival_data"}, {"score": 0.003990385737207714, "phrase": "coxs_proportional_hazards_model"}, {"score": 0.0039055990919333082, "phrase": "partial_likelihood"}, {"score": 0.0038431888954930083, "phrase": "proportional_hazards"}, {"score": 0.0036815763368083197, "phrase": "inner_products"}, {"score": 0.003526735797659508, "phrase": "kernelized_proportional_hazards_model"}, {"score": 0.003167441161387607, "phrase": "key_features"}, {"score": 0.0030178961017306646, "phrase": "sparse_solution"}, {"score": 0.0029221312451219203, "phrase": "small_fraction"}, {"score": 0.002875391177926809, "phrase": "training_data"}, {"score": 0.0027102997281261733, "phrase": "geometric_idea"}, {"score": 0.0026384167183586015, "phrase": "vector_classificationthe_margin"}, {"score": 0.0025962029048520324, "phrase": "failed_observation"}, {"score": 0.0023694065215394593, "phrase": "sparse_model"}, {"score": 0.002284452809384848, "phrase": "ivm"}, {"score": 0.0021975548867457606, "phrase": "data_examples"}, {"score": 0.0021049977753042253, "phrase": "competing_approaches"}], "paper_keywords": [""], "paper_abstract": "Sparse kernel methods like support vector machines (SVM) have been applied with great success to classification and (standard) regression settings. Existing support vector classification and regression techniques however are not suitable for partly censored survival data, which are typically analysed using Coxs proportional hazards model. As the partial likelihood of the proportional hazards model only depends on the covariates through inner products, it can be kernelized. The kernelized proportional hazards model however yields a solution that is dense, i.e. the solution depends on all observations. One of the key features of an SVM is that it yields a sparse solution, depending only on a small fraction of the training data. We propose two methods. One is based on a geometric idea, whereakin to support vector classificationthe margin between the failed observation and the observations currently at risk is maximised. The other approach is based on obtaining a sparse model by adding observations one after another akin to the Import Vector Machine (IVM). Data examples studied suggest that both methods can outperform competing approaches.", "paper_title": "Sparse kernel methods for high-dimensional survival data", "paper_id": "WOS:000257576000011"}