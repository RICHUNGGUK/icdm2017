{"auto_keywords": [{"score": 0.030776703302311397, "phrase": "certainty_map"}, {"score": 0.01538020907790006, "phrase": "visual_sensor_networks"}, {"score": 0.004815133171259446, "phrase": "distributed"}, {"score": 0.004739630859016419, "phrase": "progressive_certainty_map"}, {"score": 0.004325339932975582, "phrase": "sensor_node"}, {"score": 0.004073665268036686, "phrase": "target_localization"}, {"score": 0.004009894739765119, "phrase": "challenging_computer_vision_problem"}, {"score": 0.00389760315371842, "phrase": "extremely_higher_data_rate"}, {"score": 0.0038608730929994696, "phrase": "directional_sensing_characteristics"}, {"score": 0.0038365781437074017, "phrase": "limited_field"}, {"score": 0.003500945350055404, "phrase": "visual_occlusion"}, {"score": 0.003317970131738248, "phrase": "target_existence"}, {"score": 0.0032148229832850215, "phrase": "non-occupied_areas"}, {"score": 0.003144527847739642, "phrase": "so-called_certainty_map"}, {"score": 0.0029613556844064713, "phrase": "sensor_nodes"}, {"score": 0.002933422608537915, "phrase": "unresolved_regions"}, {"score": 0.002626317051411829, "phrase": "limited_number"}, {"score": 0.002609770322855842, "phrase": "camera_nodes"}, {"score": 0.0025526713533068793, "phrase": "dynamic_itinerary"}, {"score": 0.0025365874386986493, "phrase": "certainty_map_integration"}, {"score": 0.002512650904797186, "phrase": "entire_map"}, {"score": 0.002336469133929162, "phrase": "remaining_unresolved_regions"}, {"score": 0.0022423971916457684, "phrase": "real_experiments"}, {"score": 0.0022212306049833397, "phrase": "proposed_progressive_method"}, {"score": 0.0021933182759089364, "phrase": "detection_accuracy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Target localization", " Dynamic itinerary", " Certainty map"], "paper_abstract": "Collaboration in visual sensor networks (VSNs) is essential not only to compensate for the processing, sensing, energy, and bandwidth limitations of each sensor node but also to improve the accuracy and robustness of the network. In this paper, we study target localization in VSNs, a challenging computer vision problem because of two unique features of cameras, including the extremely higher data rate and the directional sensing characteristics with limited field of view. Traditionally, the problem is solved by localizing the targets at the intersections of the back-projected 2D cones of each target. However, the existence of visual occlusion among targets would generate many false alarms. In this work, instead of resolving the uncertainty about target existence at the intersections, we identify and study the non-occupied areas in the cone and generate the so-called certainty map of non-existence of targets. As a result, after fusing inputs from a set of sensor nodes, the unresolved regions on the certainty map would be the location of targets. This paper focuses on the design of a light-weight, energy-efficient, and robust solution where not only each camera node transmits a very limited amount of data but that a limited number of camera nodes is involved. We propose a dynamic itinerary for certainty map integration where the entire map is progressively clarified from sensor to sensor. When the confidence of the certainty map is satisfied, targets are localized at the remaining unresolved regions in the certainty map. Based on results obtained from both simulation and real experiments, the proposed progressive method shows effectiveness in detection accuracy as well as energy and bandwidth efficiency. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Distributed target localization using a progressive certainty map in visual sensor networks", "paper_id": "WOS:000288522700009"}