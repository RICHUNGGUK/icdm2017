{"auto_keywords": [{"score": 0.03874050686675407, "phrase": "dissimilarity_functions"}, {"score": 0.00481495049065317, "phrase": "algorithm_for_learning_with_dissimilarity_functions"}, {"score": 0.004195418010131832, "phrase": "data_samples"}, {"score": 0.004028796143006953, "phrase": "feature_vectors"}, {"score": 0.00374531366198605, "phrase": "sufficient_conditions"}, {"score": 0.0035965032173588753, "phrase": "building_accurate_classifiers"}, {"score": 0.003398015125667192, "phrase": "learning_paradigm"}, {"score": 0.003236596168242036, "phrase": "simple_classifiers"}, {"score": 0.0029126002350975634, "phrase": "convex_combination"}, {"score": 0.0027741761880779535, "phrase": "large_margin"}, {"score": 0.0026423134393535265, "phrase": "practical_algorithm"}, {"score": 0.0025578967205807843, "phrase": "dissimilarity-based_boosting"}, {"score": 0.0023776656246300063, "phrase": "theoretical_guidance"}, {"score": 0.0021745003291048356, "phrase": "dboost_algorithm"}], "paper_keywords": [""], "paper_abstract": "We study the problem of classification when only a dissimilarity function between objects is accessible. That is, data samples are represented not by feature vectors but in terms of their pairwise dissimilarities. We establish sufficient conditions for dissimilarity functions to allow building accurate classifiers. The theory immediately suggests a learning paradigm: construct an ensemble of simple classifiers, each depending on a pair of examples; then find a convex combination of them to achieve a large margin. We next develop a practical algorithm referred to as dissimilarity-based boosting (DBoost) for learning with dissimilarity functions under theoretical guidance. Experiments on a variety of databases demonstrate that the DBoost algorithm is promising for several dissimilarity measures widely used in practice.", "paper_title": "Theory and Algorithm for Learning with Dissimilarity Functions", "paper_id": "WOS:000266106800010"}