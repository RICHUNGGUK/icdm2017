{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "maximum_causal_entropy"}, {"score": 0.004664352260912174, "phrase": "interacting_processes"}, {"score": 0.0044237033788643715, "phrase": "maximum_entropy"}, {"score": 0.004285289574869884, "phrase": "powerful_framework"}, {"score": 0.003150900886669123, "phrase": "maximum_causal_entropy-an_approach"}, {"score": 0.002863921785266393, "phrase": "unknown_process"}, {"score": 0.002687235334499192, "phrase": "known_process"}, {"score": 0.0022674379203603224, "phrase": "inverse_optimal_control"}, {"score": 0.002219790619936024, "phrase": "decision_processes"}, {"score": 0.0021731423856364003, "phrase": "computing_equilibrium"}, {"score": 0.0021049977753042253, "phrase": "sequential_games"}], "paper_keywords": ["Causal entropy", " correlated equilibrium (CE)", " directed information", " inverse optimal control", " inverse reinforcement learning", " maximum entropy", " statistical estimation"], "paper_abstract": "The principle of maximum entropy provides a powerful framework for estimating joint, conditional, and marginal probability distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This paper presents the principle of maximum causal entropy-an approach based on directed information theory for estimating an unknown process based on its interactions with a known process. We demonstrate the breadth of the approach using two applications: a predictive solution for inverse optimal control in decision processes and computing equilibrium strategies in sequential games.", "paper_title": "The Principle of Maximum Causal Entropy for Estimating Interacting Processes", "paper_id": "WOS:000316407900002"}