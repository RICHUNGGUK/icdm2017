{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "supervised_learning"}, {"score": 0.004777392826700429, "phrase": "language_evolution"}, {"score": 0.004648222388468414, "phrase": "shared_representations"}, {"score": 0.004487241576747549, "phrase": "supervised_classification_task"}, {"score": 0.004181743040532619, "phrase": "agents'_classification_hypothesis_space"}, {"score": 0.003974071684351861, "phrase": "classification_game"}, {"score": 0.0038817197332605647, "phrase": "complexity_regularisation"}, {"score": 0.0037915057447460133, "phrase": "improved_generalisation"}, {"score": 0.003561017335579207, "phrase": "improved_language-task_fit_springs"}, {"score": 0.003424107910734022, "phrase": "collective_learning"}, {"score": 0.003357633232360232, "phrase": "simple_representations"}, {"score": 0.0032667240408191015, "phrase": "classification_task"}, {"score": 0.0029849474387154827, "phrase": "shared_representation"}, {"score": 0.0028588847851995533, "phrase": "artificial_neural_networks"}, {"score": 0.0028143720888311537, "phrase": "classification_tasks"}, {"score": 0.0026535183547537655, "phrase": "form-meaning_mapping"}, {"score": 0.0025614418906438744, "phrase": "compositional_and_holistic_languages"}, {"score": 0.0024150092189446424, "phrase": "noisy_data"}, {"score": 0.002197891020396052, "phrase": "simple_recurrent_networks"}, {"score": 0.0021721582450016, "phrase": "temporal_classification_tasks"}, {"score": 0.0021049977753042253, "phrase": "rudimentary_grammar"}], "paper_keywords": ["language evolution", " languages games", " classification game", " learnability", " functionality", " expressivity", " complexity regularisation"], "paper_abstract": "We study the emergence of shared representations in a population of agents engaged in a supervised classification task, using a model called the classification game. We connect languages with tasks by treating the agents' classification hypothesis space as an information channel. We show that by learning through the classification game, agents can implicitly perform complexity regularisation, which improves generalisation. Improved generalisation also means that the languages that emerge are well adapted to the given task. The improved language-task fit springs from the interplay of two opposing forces: the dynamics of collective learning impose a preference for simple representations, while the intricacy of the classification task imposes a pressure towards representations that are more complex. The push-pull of these two forces results in the emergence of a shared representation that is simple but not too simple. Our agents use artificial neural networks to solve the classification tasks they face, and a simple counting algorithm to learn a language as a form-meaning mapping. We present several experiments to demonstrate that both compositional and holistic languages can emerge in our system. We also demonstrate that the agents avoid overfitting on noisy data, and can learn some very difficult tasks through interaction, which they are unable to learn individually. Further, when the agents use simple recurrent networks to solve temporal classification tasks, we see the emergence of a rudimentary grammar, which does not have to be explicitly learned.", "paper_title": "The classification game: combining supervised learning and language evolution", "paper_id": "WOS:000274672400001"}