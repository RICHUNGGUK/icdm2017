{"auto_keywords": [{"score": 0.0298726137212918, "phrase": "yang"}, {"score": 0.00481495049065317, "phrase": "density_estimation"}, {"score": 0.004342430078306461, "phrase": "aggregation_procedure"}, {"score": 0.004182301599359094, "phrase": "lower_bounds"}, {"score": 0.004028054020045027, "phrase": "model_selection_type"}, {"score": 0.0038431888954930083, "phrase": "kullback-leibler_divergence"}, {"score": 0.0036324746826127997, "phrase": "hellinger's_distance"}, {"score": 0.0032145922365231093, "phrase": "kl_distance"}, {"score": 0.0030097975788892896, "phrase": "on-line_type_estimate"}, {"score": 0.002456097492315454, "phrase": "tsybakov"}, {"score": 0.002447101794397243, "phrase": "optimal_rate"}, {"score": 0.0021049977753042253, "phrase": "sample_size"}], "paper_keywords": ["aggregation", " optimal rates", " Kullback-Leibler divergence"], "paper_abstract": "In this paper we prove the optimality of an aggregation procedure. We prove lower bounds for aggregation of model selection type of M density estimators for the Kullback-Leibler divergence (KL), the Hellinger's distance and the L-1-distance. The lower bound, with respect to the KL distance, can be achieved by the on-line type estimate suggested, among others, by Yang (2000a). Combining these results, we state that logM/n is an optimal rate of aggregation in the sense of Tsybakov (2003), where n is the sample size.", "paper_title": "Lower bounds and aggregation in density estimation", "paper_id": "WOS:000245388400003"}