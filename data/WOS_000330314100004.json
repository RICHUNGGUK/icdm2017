{"auto_keywords": [{"score": 0.03696244992309851, "phrase": "bi-modal_words"}, {"score": 0.008619780482237804, "phrase": "bipartite_graph"}, {"score": 0.005820269810143765, "phrase": "multiple_kernel"}, {"score": 0.00481495049065317, "phrase": "joint_audio-visual_codewords"}, {"score": 0.004653900688892822, "phrase": "complex_events"}, {"score": 0.004564291332315995, "phrase": "intrinsically_a_multimodal_problem"}, {"score": 0.004390192910013225, "phrase": "conventional_methods"}, {"score": 0.004263972370406983, "phrase": "superficial_level"}, {"score": 0.004101281992566145, "phrase": "new_representation-called_bi-modal_words"}, {"score": 0.0040222701316738295, "phrase": "representative_joint_audio-visual_patterns"}, {"score": 0.0037942165191367366, "phrase": "quantized_words"}, {"score": 0.003721098220825489, "phrase": "visual_and_audio_modalities"}, {"score": 0.0034423989472667756, "phrase": "joint_patterns"}, {"score": 0.003376037544631762, "phrase": "different_pooling_strategies"}, {"score": 0.00323134923196813, "phrase": "visual_and_audio_words"}, {"score": 0.0029031614127920232, "phrase": "suitable_number"}, {"score": 0.0026986685585382347, "phrase": "different_sizes"}, {"score": 0.002570385604065795, "phrase": "resulting_multiple_representations"}, {"score": 0.0024963543716522087, "phrase": "experimental_results"}, {"score": 0.002424450174817922, "phrase": "proposed_method"}, {"score": 0.002400944221804575, "phrase": "statistically_significant_performance"}, {"score": 0.0023431692392929353, "phrase": "individual_visual_and_audio_feature"}, {"score": 0.0023091721866305426, "phrase": "existing_popular_multi-modal_fusion_methods"}, {"score": 0.002242647404307089, "phrase": "average_pooling"}, {"score": 0.0021886732310288128, "phrase": "bi-modal_representation"}, {"score": 0.0021049977753042253, "phrase": "multi-modal_representations"}], "paper_keywords": ["Bi-modal words", " Multimodal fusion", " Multiple kernel learning", " Event detection"], "paper_abstract": "Detecting complex events in videos is intrinsically a multimodal problem since both audio and visual channels provide important clues. While conventional methods fuse both modalities at a superficial level, in this paper we propose a new representation-called bi-modal words-to explore representative joint audio-visual patterns. We first build a bipartite graph to model relation across the quantized words extracted from the visual and audio modalities. Partitioning over the bipartite graph is then applied to produce the bi-modal words that reveal the joint patterns across modalities. Different pooling strategies are then employed to re-quantize the visual and audio words into the bi-modal words and form bi-modal Bag-of-Words representations. Since it is difficult to predict the suitable number of bi-modal words, we generate bi-modal words at different levels (i.e., codebooks with different sizes), and use multiple kernel learning to combine the resulting multiple representations during event classifier learning. Experimental results on three popular datasets show that the proposed method achieves statistically significant performance gains over methods using individual visual and audio feature alone and existing popular multi-modal fusion methods. We also find that average pooling is particularly suitable for bi-modal representation, and using multiple kernel learning to combine multi-modal representations at various granularities is helpful.", "paper_title": "Discovering joint audio-visual codewords for video event detection", "paper_id": "WOS:000330314100004"}