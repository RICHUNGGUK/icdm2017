{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "sparse_representation"}, {"score": 0.04115970230529493, "phrase": "proposed_method"}, {"score": 0.03925691033100968, "phrase": "representation_coefficients"}, {"score": 0.004621984064678252, "phrase": "pose-robust_face_recognition_method"}, {"score": 0.004364696693320128, "phrase": "face_recognition"}, {"score": 0.004189698902282913, "phrase": "large_pose_difference"}, {"score": 0.003828912548622257, "phrase": "sparse_property"}, {"score": 0.0036453427552753533, "phrase": "face_image"}, {"score": 0.003145609726536791, "phrase": "probe_image"}, {"score": 0.002970260970134645, "phrase": "smaller_pose_difference"}, {"score": 0.0026482660253173075, "phrase": "pose_variations"}, {"score": 0.0025211567863488962, "phrase": "synthesized_face_image"}, {"score": 0.00241989408309933, "phrase": "extensive_experiments"}, {"score": 0.002380534385718454, "phrase": "cmu_multi-pie_face_database"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Pose-robust face recognition", " Pose-normalization", " Face synthesis", " Sparse representation", " Latent sparse modeling"], "paper_abstract": "We propose a pose-robust face recognition method to handle the challenging task of face recognition in the presence of large pose difference between gallery and probe faces. The proposed method exploits the sparse property of the representation coefficients of a face image over its corresponding view-dictionary. By assuming the representation coefficients are invariant to pose, we can synthesize for the probe image a novel face image which has smaller pose difference with the gallery faces. Furthermore, face recognition in the presence of pose variations is achieved based on the synthesized face image again via sparse representation. Extensive experiments on CMU Multi-PIE face database are conducted to verify the efficacy of the proposed method. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Pose-robust face recognition via sparse representation", "paper_id": "WOS:000315313600023"}