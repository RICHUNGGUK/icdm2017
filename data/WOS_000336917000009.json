{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "nonconvex_regularizations"}, {"score": 0.0495290999002817, "phrase": "feature_selection"}, {"score": 0.004650896226390719, "phrase": "sparse_svm._feature_selection"}, {"score": 0.004389745162172933, "phrase": "crucial_issue"}, {"score": 0.003955926499405168, "phrase": "learning_process"}, {"score": 0.003755307753574745, "phrase": "general_framework"}, {"score": 0.0035855066449812273, "phrase": "support_vector_machines"}, {"score": 0.003523822585025555, "phrase": "sparse_regularization_term"}, {"score": 0.0034233569308012982, "phrase": "classical_convex_regularizations"}, {"score": 0.0031937337757356526, "phrase": "nonconvex_regularization_terms"}, {"score": 0.003120657021758196, "phrase": "log_penalty"}, {"score": 0.0030847461428168614, "phrase": "minimax_concave_penalty"}, {"score": 0.00270023138867066, "phrase": "convex_problems"}, {"score": 0.002432971714608409, "phrase": "intensive_experiments"}, {"score": 0.0024049552650867935, "phrase": "nine_datasets"}, {"score": 0.0021543421286324945, "phrase": "resulting_models"}, {"score": 0.0021049977753042253, "phrase": "prediction_performance"}], "paper_keywords": ["Feature selection", " forward-backward splitting algorithms", " learning to rank", " nonconvex regularizations", " regularized support vector machines", " sparsity"], "paper_abstract": "Feature selection in learning to rank has recently emerged as a crucial issue. Whereas several preprocessing approaches have been proposed, only a few have focused on integrating feature selection into the learning process. In this paper, we propose a general framework for feature selection in learning to rank using support vector machines with a sparse regularization term. We investigate both classical convex regularizations, such as l(1) or weighted l(1), and nonconvex regularization terms, such as log penalty, minimax concave penalty, or l(p) pseudo-norm with p < 1. Two algorithms are proposed: the first, an accelerated proximal approach for solving the convex problems, and, the second, a reweighted l(1) scheme to address nonconvex regularizations. We conduct intensive experiments on nine datasets from Letor 3.0 and Letor 4.0 corpora. Numerical results show that the use of nonconvex regularizations we propose leads to more sparsity in the resulting models while preserving the prediction performance. The number of features is decreased by up to a factor of 6 compared to the l(1) regularization. In addition, the software is publicly available on the web.(1)", "paper_title": "Nonconvex Regularizations for Feature Selection in Ranking With Sparse SVM", "paper_id": "WOS:000336917000009"}