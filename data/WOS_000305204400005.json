{"auto_keywords": [{"score": 0.04769152759457355, "phrase": "stochastic_estimator"}, {"score": 0.04133226577429628, "phrase": "short-term_perspective"}, {"score": 0.03653477583148174, "phrase": "stationary_random_environments"}, {"score": 0.00481495049065317, "phrase": "inertial_estimator_learning_automata_scheme"}, {"score": 0.0043656173407379895, "phrase": "running_reward-probability_estimates"}, {"score": 0.0037287578312533596, "phrase": "estimator_algorithms"}, {"score": 0.0036813725040313002, "phrase": "proposed_automata"}, {"score": 0.003634587156159668, "phrase": "inertial_weight_estimator"}, {"score": 0.003527712410706358, "phrase": "rapid_and_accurate_convergence"}, {"score": 0.003394891095032025, "phrase": "proposed_inertial_estimator_scheme"}, {"score": 0.003309124298009635, "phrase": "reward_probabilities"}, {"score": 0.003211788912325178, "phrase": "last_response"}, {"score": 0.003038532168038562, "phrase": "positive_response"}, {"score": 0.0029744042270542655, "phrase": "short_time"}, {"score": 0.0025183881730011597, "phrase": "optimal_action"}, {"score": 0.0024863459251358217, "phrase": "asymptotic_behavior"}, {"score": 0.0024547103576350233, "phrase": "proposed_scheme"}, {"score": 0.0023123057299992587, "phrase": "stationary_random_environment"}, {"score": 0.0022926465161639633, "phrase": "extensive_simulation_results"}, {"score": 0.002253826965566618, "phrase": "proposed_algorithm"}, {"score": 0.0022062235065459274, "phrase": "traditional_stochastic-estimator-based_s_e-ri_scheme"}, {"score": 0.0021230516601794687, "phrase": "dcpa"}, {"score": 0.0021049980887993046, "phrase": "dpri"}], "paper_keywords": ["learning automata", " epsilon-optimality", " stationary environments", " stochastic estimator", " inertial weight"], "paper_abstract": "This paper presents an inertial estimator learning automata scheme by which both the short-term and long-term perspectives of the environment can be incorporated in the stochastic estimator-the long term information crystallized in terms of the running reward-probability estimates, and the short term information used by considering whether the most recent response was a reward or a penalty. Thus, when the short-term perspective is considered, the stochastic estimator becomes pertinent in the context of the estimator algorithms. The proposed automata employ an inertial weight estimator as the short-term perspective to achieve a rapid and accurate convergence when operating in stationary random environments. According to the proposed inertial estimator scheme, the estimates of the reward probabilities of actions are affected by the last response from environment. In this way, actions that have gotten the positive response from environment in the short time, have the opportunity to be estimated as \"optimal\", to increase their choice probability and consequently, to be selected. The estimates become more reliable and consequently, the automaton rapidly and accurately converges to the optimal action. The asymptotic behavior of the proposed scheme is analyzed and it is proved to be epsilon-optimal in every stationary random environment. Extensive simulation results indicate that the proposed algorithm converges faster than the traditional stochastic-estimator-based S E-RI scheme, and the deterministic-estimator-based DCPA and DPRI schemes when operating in stationary random environments.", "paper_title": "Inertial Estimator Learning Automata", "paper_id": "WOS:000305204400005"}