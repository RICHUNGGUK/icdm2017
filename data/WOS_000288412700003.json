{"auto_keywords": [{"score": 0.02660316717219819, "phrase": "rkhs"}, {"score": 0.00481495049065317, "phrase": "cluster-based_gating_functions"}, {"score": 0.004718314112245913, "phrase": "multiple_kernel_learning"}, {"score": 0.00457696724030363, "phrase": "increasing_attention"}, {"score": 0.00446240364863009, "phrase": "traditional_single_kernel"}, {"score": 0.004156590395333602, "phrase": "mkl_methods"}, {"score": 0.00395101799538683, "phrase": "relative_weights"}, {"score": 0.0036988415150699945, "phrase": "\"non-uniform\"_mkl_method"}, {"score": 0.0036429626734678314, "phrase": "data-dependent_gating_mechanism"}, {"score": 0.00351582806482809, "phrase": "kernel_weights"}, {"score": 0.0033759367489122716, "phrase": "soft_clustering_algorithm"}, {"score": 0.0030655302236416502, "phrase": "cluster_structures"}, {"score": 0.0025533148412439166, "phrase": "out-of-sample_data"}, {"score": 0.0023659694724243764, "phrase": "quantitative_studies"}, {"score": 0.0023301793939902015, "phrase": "proposed_method"}, {"score": 0.002294929467537814, "phrase": "representative_mkl_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Kernel based learning", " Multi-kernel learning", " Graph embedding"], "paper_abstract": "Recently, multiple kernel learning (MKL) has gained increasing attention due to its empirical superiority over traditional single kernel based methods. However, most of state-of-the-art MKL methods are \"uniform\" in the sense that the relative weights of kernels keep fixed among all data. Here we propose a \"non-uniform\" MKL method with a data-dependent gating mechanism, i.e., adaptively determine the kernel weights for the samples. We utilize a soft clustering algorithm and then tune the weight for each cluster under the graph embedding (GE) framework. The idea of exploiting cluster structures is based on the observation that data from the same cluster tend to perform consistently, which thus increases the resistance to noises and results in more reliable estimate. Moreover, it is computationally simple to handle out-of-sample data, whose implicit RKHS representations are modulated by the posterior to each cluster. Quantitative studies between the proposed method and some representative MKL methods are conducted on both synthetic and widely used public data sets. The experimental results well validate its superiorities. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Non-uniform multiple kernel learning with cluster-based gating functions", "paper_id": "WOS:000288412700003"}