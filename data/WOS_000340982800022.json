{"auto_keywords": [{"score": 0.04860525980080879, "phrase": "bpnn"}, {"score": 0.04329937098605951, "phrase": "artificial_samples"}, {"score": 0.029980650698789916, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "small_data"}, {"score": 0.00470862827326028, "phrase": "back-propagation_neural_networks"}, {"score": 0.0046046429665581555, "phrase": "effective_learning_tools"}, {"score": 0.004543352284098364, "phrase": "non-linear_models"}, {"score": 0.004081385289516914, "phrase": "virtual_samples"}, {"score": 0.003991196463806465, "phrase": "learning_robustness"}, {"score": 0.0038855864345030563, "phrase": "novel_method"}, {"score": 0.0038510053729959074, "phrase": "virtual_sample_generation"}, {"score": 0.0037490909410974166, "phrase": "genetic_algorithm-based_virtual_sample_generation"}, {"score": 0.0036173729405604674, "phrase": "integrated_effects"}, {"score": 0.0035532533960012298, "phrase": "data_attributes"}, {"score": 0.003459191419478298, "phrase": "acceptable_range"}, {"score": 0.00341309513558946, "phrase": "mega-trend_diffusion"}, {"score": 0.003278447235176858, "phrase": "feasibility-based_programming"}, {"score": 0.003191636645925296, "phrase": "bpnn."}, {"score": 0.0031632120077478066, "phrase": "genetic_algorithm"}, {"score": 0.002997886346903894, "phrase": "most-feasible_virtual_samples"}, {"score": 0.002633063577807091, "phrase": "svr"}, {"score": 0.002473129577319634, "phrase": "experimental_results"}, {"score": 0.0023968383387667404, "phrase": "gabvsg_method"}, {"score": 0.002322895056238477, "phrase": "original_training_data"}, {"score": 0.002162316557411786, "phrase": "small_samples"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Small data set", " Genetic algorithm-based virtual sample generation (GABVSG)", " Feasibility-based programming (FBP) model"], "paper_abstract": "While back-propagation neural networks (BPNN) are effective learning tools for building non-linear models, they are often unstable when using small-data-sets. Therefore, in order to solve this problem, we construct artificial samples, called virtual samples, to improve the learning robustness. This research develops a novel method of virtual sample generation (VSG), named genetic algorithm-based virtual sample generation (GABVSG), which considers the integrated effects and constraints of data attributes. We first determine the acceptable range by using mega-trend diffusion (MTD) functions, and construct the feasibility-based programming (FBP) model with BPNN. A genetic algorithm (GA) is then applied to accelerate the generation of the most-feasible virtual samples. Finally, we use two real cases to verify the performance of the proposed method by comparing the results with two well-known forecasting models, BPNN, support vector machine for regression (SVR) and one newly published approach MTD method [1]. The experimental results indicate that the performance of the GABVSG method is superior to that of using original training data without artificial samples. Consequently, the proposed method can improve learning performance significantly when working with small samples. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A genetic algorithm-based virtual sample generation technique to improve small data set learning", "paper_id": "WOS:000340982800022"}