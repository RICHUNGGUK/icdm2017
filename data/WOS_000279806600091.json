{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "inverse_reinforcement_learning"}, {"score": 0.004508286991825192, "phrase": "behavior_styles"}, {"score": 0.004442824994700736, "phrase": "character_controllers"}, {"score": 0.0043464021636703066, "phrase": "small_set"}, {"score": 0.004099340908974491, "phrase": "rich_set"}, {"score": 0.004039792863335641, "phrase": "behavior_variations"}, {"score": 0.0038380787059463075, "phrase": "appropriate_reward_function"}, {"score": 0.003754731210715724, "phrase": "reinforcement_learning_framework"}, {"score": 0.0035934073774126856, "phrase": "discovered_reward_function"}, {"score": 0.0034642599795044445, "phrase": "different_environments"}, {"score": 0.0032671775301001483, "phrase": "new_algorithm"}, {"score": 0.0031728680535262083, "phrase": "unknown_reward_function"}, {"score": 0.0030587884803931964, "phrase": "original_apprenticeship_learning_algorithm"}, {"score": 0.002927278301452369, "phrase": "reward_function"}, {"score": 0.002863652837038965, "phrase": "behavior_style"}, {"score": 0.0027006464509273806, "phrase": "different_tasks"}, {"score": 0.002584494385587982, "phrase": "key_features"}, {"score": 0.0023669270875629205, "phrase": "adaptive_process"}, {"score": 0.0021049977753042253, "phrase": "better_generalization_properties"}], "paper_keywords": ["Optimal Control", " Data Driven Animation", " Human Animation", " Inverse Reinforcement Learning", " Apprenticeship Learning"], "paper_abstract": "We present a method for inferring the behavior styles of character controllers from a small set of examples. We show that a rich set of behavior variations can be captured by determining the appropriate reward function in the reinforcement learning framework, and show that the discovered reward function can be applied to different environments and scenarios. We also introduce a new algorithm to recover the unknown reward function that improves over the original apprenticeship learning algorithm. We show that the reward function representing a behavior style can be applied to a variety of different tasks, while still preserving the key features of the style present in the given examples. We describe an adaptive process where an author can, with just a few additional examples, refine the behavior so that it has better generalization properties.", "paper_title": "Learning Behavior Styles with Inverse Reinforcement Learning", "paper_id": "WOS:000279806600091"}