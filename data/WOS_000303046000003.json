{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "random_search"}, {"score": 0.02800366786508098, "phrase": "hyper-parameter_optimization"}, {"score": 0.008591527302015104, "phrase": "grid_search"}, {"score": 0.008531045646517528, "phrase": "manual_search"}, {"score": 0.00738017634740074, "phrase": "neural_networks"}, {"score": 0.007328159189757083, "phrase": "deep_belief_networks"}, {"score": 0.0044852808671980325, "phrase": "randomly_chosen_trials"}, {"score": 0.004298367681427065, "phrase": "empirical_evidence"}, {"score": 0.004192937957960497, "phrase": "large_previous_study"}, {"score": 0.003933508945770709, "phrase": "pure_grid_search"}, {"score": 0.0036380471217986065, "phrase": "small_fraction"}, {"score": 0.0035995085881892464, "phrase": "computation_time"}, {"score": 0.003486316700608598, "phrase": "better_models"}, {"score": 0.003329051037399227, "phrase": "thoughtful_combination"}, {"score": 0.0033054920459613018, "phrase": "manual_search_and_grid_search"}, {"score": 0.003201516237269705, "phrase": "statistically_equal_performance"}, {"score": 0.0031563574387360465, "phrase": "seven_data_sets"}, {"score": 0.0031229055434606003, "phrase": "superior_performance"}, {"score": 0.0030462191741660346, "phrase": "gaussian_process_analysis"}, {"score": 0.0029608739165086257, "phrase": "set_performance"}, {"score": 0.002777463594061196, "phrase": "different_hyper-parameters"}, {"score": 0.0027382698136572785, "phrase": "different_data_sets"}, {"score": 0.00266152927283908, "phrase": "poor_choice"}, {"score": 0.0026146595609362715, "phrase": "new_data_sets"}, {"score": 0.0025413743933379185, "phrase": "recent_\"high_throughput\"_methods"}, {"score": 0.0024439412207268355, "phrase": "large_number"}, {"score": 0.002333586381274878, "phrase": "growing_interest"}, {"score": 0.002317055705600351, "phrase": "large_hierarchical_models"}, {"score": 0.0022843440226146005, "phrase": "increasing_burden"}, {"score": 0.0021811799507898792, "phrase": "natural_baseline"}], "paper_keywords": ["global optimization", " model selection", " neural networks", " deep learning", " response surface modeling"], "paper_abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.", "paper_title": "Random Search for Hyper-Parameter Optimization", "paper_id": "WOS:000303046000003"}