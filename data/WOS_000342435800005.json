{"auto_keywords": [{"score": 0.03812390148224935, "phrase": "eye-tracking_data"}, {"score": 0.015231419194298608, "phrase": "visual_attention"}, {"score": 0.00481495049065317, "phrase": "image_visual_attention_computation"}, {"score": 0.00458997570994803, "phrase": "salient_subset"}, {"score": 0.004546252387071551, "phrase": "visual_input"}, {"score": 0.0044742996274375404, "phrase": "redundant_data"}, {"score": 0.004431673309295777, "phrase": "dominant_view"}, {"score": 0.0042245308336121635, "phrase": "visual_saliency"}, {"score": 0.004184274042752558, "phrase": "local_contrast"}, {"score": 0.004157648639294148, "phrase": "interest_points"}, {"score": 0.004065781456356258, "phrase": "scene_viewing"}, {"score": 0.003753882510162702, "phrase": "novel_framework"}, {"score": 0.0037180939187449937, "phrase": "image_visual_attention"}, {"score": 0.0031591719975282073, "phrase": "computation_model"}, {"score": 0.003139048969111903, "phrase": "image_categorization"}, {"score": 0.0030598265870531004, "phrase": "object_bank"}, {"score": 0.002973081656751767, "phrase": "object_detectors"}, {"score": 0.0028703830632744133, "phrase": "feature_descriptor"}, {"score": 0.002675477598833661, "phrase": "interesting_objects"}, {"score": 0.002599600404892265, "phrase": "surrounding_objects"}, {"score": 0.0025665698207861533, "phrase": "first_problem"}, {"score": 0.0025178072071268534, "phrase": "computational_model"}, {"score": 0.002485813299166851, "phrase": "second_problem"}, {"score": 0.0023922444655593046, "phrase": "object_attributes"}, {"score": 0.0022802093550930245, "phrase": "proposed_pixel-level_visual_attention_model"}, {"score": 0.0022297330866277738, "phrase": "comprehensive_evaluations"}, {"score": 0.00221551716210208, "phrase": "publicly_available_benchmarks"}, {"score": 0.0021049977753042253, "phrase": "proposed_models"}], "paper_keywords": ["Visual attention", " Eye tracking", " Object bank", " Image categorization"], "paper_abstract": "Visual attention aims at selecting a salient subset from the visual input for further processing while ignoring redundant data. The dominant view for the computation of visual attention is based on the assumption that bottom-up visual saliency such as local contrast and interest points drives the allocation of attention in scene viewing. However, we advocate in this paper that the deployment of attention is primarily and directly guided by objects and thus propose a novel framework to explore image visual attention via the learning of object attributes from eye-tracking data. We mainly aim to solve three problems: (1) the pixel-level visual attention computation (the saliency map); (2) the image-level visual attention computation; (3) the application of the computation model in image categorization. We first adopt the algorithm of object bank to acquire the responses to a number of object detectors at each location in an image and thus form a feature descriptor to indicate the occurrences of various objects at a pixel or in an image. Next, we integrate the inference of interesting objects from fixations in eye-tracking data with the competition among surrounding objects to solve the first problem. We further propose a computational model to solve the second problem and estimate the interestingness of each image via the mapping between object attributes and the inter-observer visual congruency obtained from eye-tracking data. Finally, we apply the proposed pixel-level visual attention model to the image categorization task. Comprehensive evaluations on publicly available benchmarks and comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed models.", "paper_title": "Image visual attention computation and application via the learning of object attributes", "paper_id": "WOS:000342435800005"}