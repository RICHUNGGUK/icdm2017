{"auto_keywords": [{"score": 0.03879777167718846, "phrase": "agent_subset"}, {"score": 0.014593739423908547, "phrase": "coordinate_team"}, {"score": 0.014182157645544868, "phrase": "joint_action_policy"}, {"score": 0.012289860870307265, "phrase": "coordinate_tree"}, {"score": 0.00481495049065317, "phrase": "novel_coordination_tree_frame"}, {"score": 0.0045709675813472884, "phrase": "team_markov_games"}, {"score": 0.004264697351852424, "phrase": "main_problems"}, {"score": 0.004119313626110943, "phrase": "first_problem"}, {"score": 0.004048483412089098, "phrase": "dynamic_team"}, {"score": 0.0038654774211631564, "phrase": "novel_coordinate_tree_frame"}, {"score": 0.0031753062090452864, "phrase": "minimum_cost"}, {"score": 0.0029281780903782284, "phrase": "belief_allocation_studies"}, {"score": 0.0026691455928407022, "phrase": "optimum_solution"}, {"score": 0.0025780134737796085, "phrase": "multiple_simulation_environments"}, {"score": 0.002518989593177879, "phrase": "proposed_algorithm"}, {"score": 0.0024899850926034567, "phrase": "similar_ones"}, {"score": 0.002461313735611462, "phrase": "experimental_results"}, {"score": 0.0024049552650867935, "phrase": "proposed_algorithms"}, {"score": 0.002309408211203032, "phrase": "corporative_teams"}, {"score": 0.0022565201625076876, "phrase": "optimum_joint_action_policy"}, {"score": 0.0021921063460682293, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multi-agent", " Coordination tree", " Markov games", " Belief propagation", " Q learning"], "paper_abstract": "In the research of team Markov games, computing the coordinate team dynamically and determining the joint action policy are the main problems. To deal with the first problem, a dynamic team partitioning method is proposed based on a novel coordinate tree frame. We build a coordinate tree with coordinate agent subset and define two breaching weights to represent the weights of an agent to corporate with the agent subset. Each agent chooses the agent subset with a minimum cost as the coordinate team based on coordinate tree. The Q-learning based on belief allocation studies multi-agents joint action policy which helps corporative multi-agents joint action policy to converge to the optimum solution. We perform experiments on multiple simulation environments and compare the proposed algorithm with similar ones. Experimental results show that the proposed algorithms are able to dynamically compute the corporative teams and design the optimum joint action policy for corporative teams. Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.", "paper_title": "Collaborative multi-agent reinforcement learning based on a novel coordination tree frame with dynamic partition", "paper_id": "WOS:000328725600016"}