{"auto_keywords": [{"score": 0.04955805081100005, "phrase": "facial_expression_recognition"}, {"score": 0.04509939867683567, "phrase": "facial_expressions"}, {"score": 0.03537870345379016, "phrase": "lbp-top"}, {"score": 0.00481495049065317, "phrase": "near-infrared_videos"}, {"score": 0.0046340196745259694, "phrase": "emotional_state"}, {"score": 0.004363277689798392, "phrase": "existing_datasets"}, {"score": 0.004199247711875386, "phrase": "visible_light_spectrum"}, {"score": 0.004085856939524333, "phrase": "visible_light"}, {"score": 0.003826007045267751, "phrase": "significant_variations"}, {"score": 0.0035630518377071916, "phrase": "novel_research"}, {"score": 0.0035049738376240567, "phrase": "dynamic_facial_expression_recognition"}, {"score": 0.00315875951953507, "phrase": "nir"}, {"score": 0.0030229485340586473, "phrase": "illumination_invariant_description"}, {"score": 0.002989991779830814, "phrase": "face_video_sequences"}, {"score": 0.0029573932605287947, "phrase": "appearance_and_motion_features"}, {"score": 0.002861708609922034, "phrase": "expression_classification"}, {"score": 0.0027691111795848183, "phrase": "discriminative_weights"}, {"score": 0.002709045632654716, "phrase": "training_examples"}, {"score": 0.002650279521276126, "phrase": "component-based_facial_features"}, {"score": 0.0025786066704445304, "phrase": "geometric_and_appearance_information"}, {"score": 0.002522662955545598, "phrase": "effective_way"}, {"score": 0.0024410093024417527, "phrase": "experimental_results"}, {"score": 0.0023362245884046176, "phrase": "support_vector_machine"}, {"score": 0.0023107372754933887, "phrase": "sparse_representation_classifiers"}, {"score": 0.002285527382381719, "phrase": "good_and_robust_results"}, {"score": 0.002163535133532765, "phrase": "future_research"}, {"score": 0.0021399277552314067, "phrase": "nir-based_facial_expression_recognition"}], "paper_keywords": ["Facial expression recognition", " Spatiotemporal descriptors", " Near-infrared (NIR)", " Visible light (VIS)", " Component-based facial features"], "paper_abstract": "Facial expression recognition is to determine the emotional state of the face regardless of its identity. Most of the existing datasets for facial expressions are captured in a visible light spectrum. However, the visible light (VIS) can change with time and location, causing significant variations in appearance and texture. In this paper, we present a novel research on a dynamic facial expression recognition, using near-infrared (NIR) video sequences and LBP-TOP (Local binary patterns from three orthogonal planes) feature descriptors. NIR imaging combined with LBP-TOP features provide an illumination invariant description of face video sequences. Appearance and motion features in slices are used for expression classification, and for this, discriminative weights are learned from training examples. Furthermore, component-based facial features are presented to combine geometric and appearance information, providing an effective way for representing the facial expressions. Experimental results of facial expression recognition using a novel Oulu-CASIA NIR&VIS facial expression database, a support vector machine and sparse representation classifiers show good and robust results against illumination variations. This provides a baseline for future research on NIR-based facial expression recognition. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Facial expression recognition from near-infrared videos", "paper_id": "WOS:000296123200004"}