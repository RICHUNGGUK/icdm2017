{"auto_keywords": [{"score": 0.04930231023249967, "phrase": "support_vector_machines"}, {"score": 0.00481495049065317, "phrase": "variable_selection"}, {"score": 0.0037667216301666196, "phrase": "input_space"}, {"score": 0.0036753171449945654, "phrase": "even_better_results"}, {"score": 0.003145609726536791, "phrase": "support_vector_machine"}, {"score": 0.0030193416820392554, "phrase": "predictive_performance"}, {"score": 0.002759058732719621, "phrase": "existing_variable_selection_techniques"}, {"score": 0.0026920404458807444, "phrase": "simulation_study"}, {"score": 0.002626645763725465, "phrase": "simulation_results"}, {"score": 0.002541910967437926, "phrase": "new_criteria"}, {"score": 0.00241989408309933, "phrase": "generalization_error_rate"}, {"score": 0.0021049977753042253, "phrase": "real-world_benchmark_data_sets"}], "paper_keywords": ["information criterion", " supervised classification", " support vector machine", " variable selection"], "paper_abstract": "Support vector machines for classification have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the definition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same findings for comparison on some real-world benchmark data sets.", "paper_title": "An information criterion for variable selection in support vector machines", "paper_id": "WOS:000256642000008"}