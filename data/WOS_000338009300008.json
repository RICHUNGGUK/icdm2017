{"auto_keywords": [{"score": 0.04127853893061471, "phrase": "safe_agi."}, {"score": 0.00481495049065317, "phrase": "artificial_general_intelligence"}, {"score": 0.00475070135279933, "phrase": "rational_agents"}, {"score": 0.004593787100061613, "phrase": "maximise_rewards"}, {"score": 0.004266501496765747, "phrase": "undesirable_ways"}, {"score": 0.00415333528095237, "phrase": "prior_reward_function"}, {"score": 0.004043158538239064, "phrase": "value_learning"}, {"score": 0.0038314621564116192, "phrase": "generalised_states"}, {"score": 0.003704797078215222, "phrase": "valuable_-_not_rewards"}, {"score": 0.0035107582226155012, "phrase": "aixi"}, {"score": 0.0031738126998349775, "phrase": "modified_aixi_agent"}, {"score": 0.0030688220103201836, "phrase": "multi-agent_environment"}, {"score": 0.0025761937402886954, "phrase": "ageneral_framework"}, {"score": 0.0024909230668523847, "phrase": "ethical_bias"}, {"score": 0.0023603078194781965, "phrase": "universal_intelligence_model"}, {"score": 0.0021917966722180132, "phrase": "simple_markov_environment"}], "paper_keywords": ["AIXI", " safe AGI", " empathy", " representations", " multi-agent environment"], "paper_abstract": "Rational agents are usually built to maximise rewards. However, artificial general intelligence (AGI) agents can find undesirable ways of maximising any prior reward function. Therefore, value learning is crucial for safe AGI. We assume that generalised states of the world are valuable - not rewards themselves, and propose an extension of AIXI, in which rewards are used only to bootstrap hierarchical value learning. The modified AIXI agent is considered in the multi-agent environment, where other agents can be either humans or other 'mature' agents, the values of which should be revealed and adopted by the 'infant' AGI agent. Ageneral framework for designing such empathic agent with ethical bias is proposed as an extension of the universal intelligence model as well. Moreover, we perform experiments in the simple Markov environment, which demonstrate feasibility of our approach to value learning in safe AGI.", "paper_title": "Universal empathy and ethical bias for artificial general intelligence", "paper_id": "WOS:000338009300008"}