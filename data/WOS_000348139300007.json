{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "rbf_networks"}, {"score": 0.006839754555901929, "phrase": "fisher_information_matrix"}, {"score": 0.00437281181878921, "phrase": "learning_process"}, {"score": 0.004241771072194993, "phrase": "slow_learning_speed"}, {"score": 0.0040525061916685924, "phrase": "natural_gradient_learning_method"}, {"score": 0.003551694229680741, "phrase": "probability_density_function"}, {"score": 0.003393115177189967, "phrase": "activation_function"}, {"score": 0.003258090521354736, "phrase": "natural_gradient_learning"}, {"score": 0.003144345057121367, "phrase": "explicit_forms"}, {"score": 0.0027835848648596513, "phrase": "hidden_units"}, {"score": 0.0026057256900479026, "phrase": "adaptive_method"}, {"score": 0.0025663181430010686, "phrase": "natural_gradient_learning_algorithms"}, {"score": 0.0024766636161181544, "phrase": "explicit_form"}, {"score": 0.0024392031587145728, "phrase": "adaptive_natural_gradient_learning_algorithm"}, {"score": 0.0023659694724243764, "phrase": "conventional_gradient_descent_method"}, {"score": 0.002294929467537814, "phrase": "proposed_adaptive_natural_gradient_method"}, {"score": 0.00217017119491777, "phrase": "good_performance"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_functions_approximation"}], "paper_keywords": [""], "paper_abstract": "Radial basis function (RBF) networks are one of the most widely used models for function approximation and classification. There are many strange behaviors in the learning process of RBF networks, such as slow learning speed and the existence of the plateaus. The natural gradient learning method can overcome these disadvantages effectively. It can accelerate the dynamics of learning and avoid plateaus. In this letter, we assume that the probability density function (pdf) of the input and the activation function are gaussian. First, we introduce natural gradient learning to the RBF networks and give the explicit forms of the Fisher information matrix and its inverse. Second, since it is difficult to calculate the Fisher information matrix and its inverse when the numbers of the hidden units and the dimensions of the input are large, we introduce the adaptive method to the natural gradient learning algorithms. Finally, we give an explicit form of the adaptive natural gradient learning algorithm and compare it to the conventional gradient descent method. Simulations show that the proposed adaptive natural gradient method, which can avoid the plateaus effectively, has a good performance when RBF networks are used for nonlinear functions approximation.", "paper_title": "Natural Gradient Learning Algorithms for RBF Networks", "paper_id": "WOS:000348139300007"}