{"auto_keywords": [{"score": 0.049270547314558835, "phrase": "least_squares"}, {"score": 0.0432381282696211, "phrase": "uniform_boundedness"}, {"score": 0.00481495049065317, "phrase": "optimal_learning_rates"}, {"score": 0.004575053987970206, "phrase": "unbounded_sampling"}, {"score": 0.004459605573798577, "phrase": "standard_assumption"}, {"score": 0.0043842558719324526, "phrase": "theoretical_study"}, {"score": 0.004060566667226606, "phrase": "output_sample_values"}, {"score": 0.003891140864902669, "phrase": "common_case"}, {"score": 0.0038253590626087237, "phrase": "gaussian_noise"}, {"score": 0.0035731269727067496, "phrase": "learning_algorithm"}, {"score": 0.0033660587983313536, "phrase": "regularization_scheme"}, {"score": 0.0032810180076973806, "phrase": "kernel_hilbert_spaces"}, {"score": 0.0029617414861784525, "phrase": "incremental_conditions"}, {"score": 0.002838033152426417, "phrase": "output_variable"}, {"score": 0.002561752783323717, "phrase": "regression_function"}, {"score": 0.0024547103576350233, "phrase": "hypothesis_space"}, {"score": 0.0022926465161639633, "phrase": "new_covering_number_argument"}, {"score": 0.002215663255682209, "phrase": "sample_error"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Learning theory", " Least squares regression", " Regularization in reproducing kernel Hilbert spaces", " Covering number"], "paper_abstract": "A standard assumption in theoretical study of learning algorithms for regression is uniform boundedness of output sample values. This excludes the common case with Gaussian noise. In this paper we investigate the learning algorithm for regression generated by the least squares regularization scheme in reproducing kernel Hilbert spaces without the assumption of uniform boundedness for sampling. By imposing some incremental conditions on moments of the output variable, we derive learning rates in terms of regularity of the regression function and capacity of the hypothesis space. The novelty of our analysis is a new covering number argument for bounding the sample error. (c) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Optimal learning rates for least squares regularized regression with unbounded sampling", "paper_id": "WOS:000286082100004"}