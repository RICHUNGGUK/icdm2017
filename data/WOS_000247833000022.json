{"auto_keywords": [{"score": 0.031134205568030458, "phrase": "discriminative_learning"}, {"score": 0.015719716506582524, "phrase": "cl-net"}, {"score": 0.013227256952832514, "phrase": "second_stage"}, {"score": 0.004600870784042767, "phrase": "two-stage_classification_learning_algorithm"}, {"score": 0.004363058588000252, "phrase": "class-conditional_distribution"}, {"score": 0.004264923517966271, "phrase": "discrete_space"}, {"score": 0.004168986485488861, "phrase": "separate_mixture_model"}, {"score": 0.003953409087444568, "phrase": "class_posterior_probabilities"}, {"score": 0.003748937184382714, "phrase": "first_stage"}, {"score": 0.0036645642553325215, "phrase": "generative_information"}, {"score": 0.0032951702142329357, "phrase": "high-dimensional_probability"}, {"score": 0.0032209766681045365, "phrase": "tree_structure"}, {"score": 0.0031010029460927864, "phrase": "dependence_tree"}, {"score": 0.002788257824525862, "phrase": "resulting_learning_algorithm"}, {"score": 0.0025453546203590364, "phrase": "cl_dependence-tree_estimation"}, {"score": 0.0023952639120005193, "phrase": "empirical_tests"}, {"score": 0.0023235629819760018, "phrase": "proposed_learning_algorithm"}, {"score": 0.002288519595829552, "phrase": "significant_improvements"}, {"score": 0.002203200914130907, "phrase": "related_classifiers"}, {"score": 0.0021049977753042253, "phrase": "generative_learning"}], "paper_keywords": ["chow-liu (CL) algorithm", " classification", " dependence tree", " machine learning", " neural networks", " probability estimation"], "paper_abstract": "This correspondence presents a two-stage classification learning algorithm. The first stage approximates the class-conditional distribution of a discrete space using a separate mixture model, and the second stage investigates the class posterior probabilities by training a network. The first stage explores the generative information that is inherent in each class by using the Chow-Liu (CL) method, which approximates high-dimensional probability with a tree structure, namely, a dependence tree, whereas the second stage concentrates on discriminative learning to distinguish between classes. The resulting learning algorithm integrates the advantages of both generative learning and discriminative learning. Because it uses CL dependence-tree estimation, we call our algorithm CL-Net. Empirical tests indicate that the proposed learning algorithm makes significant improvements when compared with the related classifiers that are constructed by either generative learning or discriminative learning.", "paper_title": "Generative and discriminative learning by CL-Net", "paper_id": "WOS:000247833000022"}