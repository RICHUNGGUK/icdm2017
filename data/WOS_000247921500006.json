{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "hjb_equation"}, {"score": 0.00476323161560059, "phrase": "neural_network_value-function_approximators"}, {"score": 0.004537254814246843, "phrase": "empirical_study"}, {"score": 0.004488505301335364, "phrase": "iterative_least_squares"}, {"score": 0.004392822192973707, "phrase": "hamilton"}, {"score": 0.004139128578531134, "phrase": "neural_network"}, {"score": 0.003985487777785535, "phrase": "value_function"}, {"score": 0.0038375280115736958, "phrase": "optimal_control_problem"}, {"score": 0.003796267885124202, "phrase": "nn_approximator"}, {"score": 0.0037554497046697432, "phrase": "theoretical_guarantees"}, {"score": 0.0036553070575015344, "phrase": "numerical_instabilities"}, {"score": 0.003245359940011418, "phrase": "first_method"}, {"score": 0.003193129886123359, "phrase": "gradual_increase"}, {"score": 0.0031417377608722, "phrase": "horizon_time_scale"}, {"score": 0.003074495313651187, "phrase": "corresponding_gradual_increase"}, {"score": 0.0030414140732789186, "phrase": "value_function_complexity"}, {"score": 0.002992456421856031, "phrase": "second_method"}, {"score": 0.0029126002350975634, "phrase": "stochastic_dynamics"}, {"score": 0.0028502479839354637, "phrase": "regularizing_second_derivative_term"}, {"score": 0.0027592065435732955, "phrase": "gradual_reduction"}, {"score": 0.0023841092222805253, "phrase": "initial_stabilizing_policy"}, {"score": 0.002345707314211983, "phrase": "restrictive_assumptions"}, {"score": 0.0022830706430588482, "phrase": "cost_function"}, {"score": 0.002234165013865669, "phrase": "plant_dynamics"}, {"score": 0.0021049977753042253, "phrase": "first-_and_second-order_differential_backpropagation"}], "paper_keywords": ["differential neural networks (NNs)", " dynamic programming", " feedforward neural networks", " Hamilton-Jacoby-Bellman (HJB) equation", " optimal control", " viscosity solution"], "paper_abstract": "In this paper, we present an empirical study of iterative least squares minimization of the Hamilton-jacobi-Bellman (HJB) residual with a neural network (NN) approximation of the value function. Although the nonlinearities in the optimal control problem and NN approximator preclude theoretical guarantees and raise concerns of numerical instabilities, we present two simple methods for promoting convergence, the effectiveness of which is presented in a series of experiments. The first method involves the gradual increase of the horizon time scale, with a corresponding gradual increase in value function complexity. The second method involves the assumption of stochastic dynamics which introduces a regularizing second derivative term to the HJB equation. A gradual reduction of this term provides further stabilization of the convergence. We demonstrate the solution of several problems, including the 4-D inverted-pendulum system with bounded control. Our approach requires no initial stabilizing policy or any restrictive assumptions on the plant or cost function, only knowledge of the plant dynamics. In the Appendix, we provide the equations for first- and second-order differential backpropagation.", "paper_title": "Least squares solutions of the HJB equation with neural network value-function approximators", "paper_id": "WOS:000247921500006"}