{"auto_keywords": [{"score": 0.049496721057450455, "phrase": "extreme_scale"}, {"score": 0.00481495049065317, "phrase": "implicit_finite_element"}, {"score": 0.004698514007994158, "phrase": "parallel_simulations"}, {"score": 0.004392565073466714, "phrase": "large_number"}, {"score": 0.00428629884026304, "phrase": "equal_work_load"}, {"score": 0.004234129588974058, "phrase": "minimum_inter-part_communications"}, {"score": 0.0036553070575015344, "phrase": "global_implementation"}, {"score": 0.0036107898615445797, "phrase": "current_approaches"}, {"score": 0.003459191419478298, "phrase": "vertex_imbalance"}, {"score": 0.0033548062793973144, "phrase": "individual_cores"}, {"score": 0.0030043508031729277, "phrase": "iterative_improvement_algorithm"}, {"score": 0.0029677378488677983, "phrase": "liipbmod"}, {"score": 0.002878140062594849, "phrase": "previous_study"}, {"score": 0.0028430608076324727, "phrase": "zhou_et_al"}, {"score": 0.002791239714439304, "phrase": "siam_j._sci"}, {"score": 0.0026091594819333654, "phrase": "current_work"}, {"score": 0.0025615908073796027, "phrase": "combined_partition_strategy"}, {"score": 0.0021049977753042253, "phrase": "combined_partition_algorithm"}], "paper_keywords": ["Partition improvement", " Extreme scale", " Unstructured mesh", " Finite element"], "paper_abstract": "Parallel simulations at extreme scale require that the mesh is distributed across a large number of processors with equal work load and minimum inter-part communications. A number of algorithms have been developed to meet these goals and graph/hypergraph-based methods are by far the most powerful ones. However, the global implementation of current approaches can fail on very large core counts and the vertex imbalance is not optimal where individual cores are lightly loaded. Those issues are resolved by combination of global and local partitioning and an iterative improvement algorithm, LIIPBMod, developed in the previous study (Zhou et al. in SIAM J. Sci. Comput. 32:3201-3227, 2010). In the current work, this combined partition strategy is applied to the simulations at extreme scale with up to O(10(10)) elements and up to O(300K) cores. Strong scaling studies on IBM BlueGene/P and Cray XT5 systems demonstrate the effectiveness of this combined partition algorithm.", "paper_title": "Unstructured mesh partition improvement for implicit finite element at extreme scale", "paper_id": "WOS:000299509500006"}