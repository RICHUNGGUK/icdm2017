{"auto_keywords": [{"score": 0.04137808138172336, "phrase": "mdp"}, {"score": 0.012230341488553039, "phrase": "pomdp"}, {"score": 0.00481495049065317, "phrase": "modified_memory-based_reinforcement_learning_method"}, {"score": 0.004728624975677976, "phrase": "pomdp_problems"}, {"score": 0.004671932143798307, "phrase": "partially_observable_markov_decision_processes"}, {"score": 0.004505881071218599, "phrase": "mathematical_framework"}, {"score": 0.004451846834142146, "phrase": "agent_planning"}, {"score": 0.004216566615939546, "phrase": "classic_bayesian_optimal_solution"}, {"score": 0.00399367108161192, "phrase": "markov_decision_process"}, {"score": 0.0038749521906474593, "phrase": "belief_states"}, {"score": 0.0037371212381934853, "phrase": "belief_state_space"}, {"score": 0.003232956879952914, "phrase": "complete_pomdp_model"}, {"score": 0.002952900365829518, "phrase": "modified_memory-based_reinforcement"}, {"score": 0.0028650312174624635, "phrase": "u-tree"}, {"score": 0.0027463775689957255, "phrase": "raw_sensor_experiences"}, {"score": 0.0027133856739937133, "phrase": "minimum_prior_knowledge"}, {"score": 0.0025697460140513932, "phrase": "original_u-tree's_state_generation_process"}, {"score": 0.002508365161206412, "phrase": "generated_model"}, {"score": 0.0023470051993423483, "phrase": "statistical_test"}, {"score": 0.002318799760022913, "phrase": "reward_estimation"}, {"score": 0.002169607777109063, "phrase": "traditional_model-based_algorithms"}, {"score": 0.0021049977753042253, "phrase": "well_known_pomdp_problems"}], "paper_keywords": ["Memory-based reinforcement learning", " Markov decision processes", " Partially observable Markov decision processes", " Reinforcement learning"], "paper_abstract": "Partially observable Markov decision processes (POMDP) provide a mathematical framework for agent planning under stochastic and partially observable environments. The classic Bayesian optimal solution can be obtained by transforming the problem into Markov decision process (MDP) using belief states. However, because the belief state space is continuous and multi-dimensional, the problem is highly intractable. Many practical heuristic based methods are proposed, but most of them require a complete POMDP model of the environment, which is not always practical. This article introduces a modified memory-based reinforcement learning algorithm called modified U-Tree that is capable of learning from raw sensor experiences with minimum prior knowledge. This article describes an enhancement of the original U-Tree's state generation process to make the generated model more compact, and also proposes a modification of the statistical test for reward estimation, which allows the algorithm to be benchmarked against some traditional model-based algorithms with a set of well known POMDP problems.", "paper_title": "A Modified Memory-Based Reinforcement Learning Method for Solving POMDP Problems", "paper_id": "WOS:000290583100005"}