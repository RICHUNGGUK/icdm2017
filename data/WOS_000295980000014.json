{"auto_keywords": [{"score": 0.04775244230872765, "phrase": "training_data"}, {"score": 0.00481495049065317, "phrase": "statistical_modeling"}, {"score": 0.0045357787854931894, "phrase": "quantitative_comparison"}, {"score": 0.004490842671590769, "phrase": "modeling_techniques"}, {"score": 0.004209369306023422, "phrase": "model_probability_density_function"}, {"score": 0.003753738096719609, "phrase": "large-numbers_limit"}, {"score": 0.003347259612209552, "phrase": "unknown_pdf"}, {"score": 0.0032004205378150354, "phrase": "model_pdf"}, {"score": 0.003075296862372287, "phrase": "artificial_data"}, {"score": 0.0029698211157405618, "phrase": "limiting_large-number_relations"}, {"score": 0.0028966970007550824, "phrase": "good_quantitative_and_qualitative_predictions"}, {"score": 0.0028113140998735366, "phrase": "measured_specificity"}, {"score": 0.0027557911038816256, "phrase": "small_numbers"}, {"score": 0.0027284410545226306, "phrase": "training_examples"}, {"score": 0.0026745503978347143, "phrase": "extreme_cases"}, {"score": 0.0024693979425773993, "phrase": "previous_graph-based_techniques"}, {"score": 0.0023846534541336326, "phrase": "real_data_sets"}, {"score": 0.0023143284925318916, "phrase": "proper_theoretical_basis"}, {"score": 0.0022799458141165587, "phrase": "previously_ad_hoc_concept"}, {"score": 0.0022127018991294047, "phrase": "useful_insights"}, {"score": 0.0021049977753042253, "phrase": "real_data"}], "paper_keywords": ["Specificity", " generalization", " assessment of modeling", " graph-based estimators", " entropy estimation", " estimation of statistical distance", " estimation of divergence", " nearest-neighbor estimators", " cross entropy", " Kullback-Leibler divergence"], "paper_abstract": "In statistical modeling, there are various techniques used to build models from training data. Quantitative comparison of modeling techniques requires a method for evaluating the quality of the fit between the model probability density function (pdf) and the training data. One graph-based measure that has been used for this purpose is the specificity. We consider the large-numbers limit of the specificity, and derive expressions which show that it can be considered as an estimator of the divergence between the unknown pdf from which the training data was drawn and the model pdf built from the training data. Experiments using artificial data enable us to show that these limiting large-number relations enable us to obtain good quantitative and qualitative predictions of the behavior of the measured specificity, even for small numbers of training examples and in some extreme cases. We demonstrate that specificity can provide a more sensitive measure of difference between various modeling methods than some previous graph-based techniques. Key points are illustrated using real data sets. We thus establish a proper theoretical basis for the previously ad hoc concept of specificity, and obtain useful insights into the application of specificity in the analysis of real data.", "paper_title": "Specificity: A Graph-Based Estimator of Divergence", "paper_id": "WOS:000295980000014"}