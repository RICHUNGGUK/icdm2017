{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaussian_mixtures"}, {"score": 0.04330964116308674, "phrase": "negative_entropy"}, {"score": 0.0399683188090249, "phrase": "model_selection"}, {"score": 0.004600870784042767, "phrase": "novel_automatic_model_selection_algorithm"}, {"score": 0.00450075384484102, "phrase": "gaussian"}, {"score": 0.004363461101496622, "phrase": "em"}, {"score": 0.003983512316698646, "phrase": "latent_variables"}, {"score": 0.0038644521996539466, "phrase": "indirect_effect"}, {"score": 0.0032209766681045365, "phrase": "insufficient_data"}, {"score": 0.002746225320405654, "phrase": "parameter_estimation_and_model_selection"}, {"score": 0.0024691719264774165, "phrase": "parametric_mixture_model"}, {"score": 0.0023952639120005193, "phrase": "em_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Harmonious competition learning", " Gaussian mixture model", " Model selection", " Expectation maximization"], "paper_abstract": "This paper proposes a novel automatic model selection algorithm for learning Gaussian mixtures. Unlike EM, we shall further increase the negative entropy of the posterior of latent variables to exert an indirect effect on model selection. The increase of negative entropy can be interpreted as a competition, which corresponds to an annihilation of those components with insufficient data to support. More importantly, this competition only depends on the data itself. Additionally, we seamlessly integrate parameter estimation and model selection into a single algorithm, which can be applied to any kind of parametric mixture model solved by an EM algorithm. Experiments involving Gaussian mixtures show the effectiveness of our approach on model selection. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Harmonious competition learning for Gaussian mixtures", "paper_id": "WOS:000361256000023"}