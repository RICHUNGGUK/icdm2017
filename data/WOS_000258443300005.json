{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "parsimonious_neural_network_models"}, {"score": 0.03116522728244387, "phrase": "objective_function"}, {"score": 0.004507882471601552, "phrase": "resulting_sparse_networks"}, {"score": 0.00437281181878921, "phrase": "simple_rules"}, {"score": 0.004198965651975536, "phrase": "greater_insight"}, {"score": 0.003971111060168551, "phrase": "neural_networks"}, {"score": 0.0038325685550319863, "phrase": "estimated_schwartz_model_selection_criterion"}, {"score": 0.0036988415150699945, "phrase": "sparse_network"}, {"score": 0.003291336101694653, "phrase": "model_parameters"}, {"score": 0.0030038988468808845, "phrase": "schwartz_information_criterion"}, {"score": 0.002618995204424126, "phrase": "differential_evolution"}, {"score": 0.0025533148412439166, "phrase": "global_optimization"}, {"score": 0.002203509076886664, "phrase": "sequential_unconstrained_optimization_technique"}], "paper_keywords": ["model selection", " differential evolution", " genetic algorithm", " neural network"], "paper_abstract": "The aim of this work is to avoid overfitting by seeking parsimonious neural network models and hence to provide better out-of-sample predictions. The resulting sparse networks are easier to interpret as simple rules which, in turn, could give greater insight into the structure of the data. Fully connected feedforward neural networks are pruned through optimization of an estimated Schwartz model selection criterion using differential evolution to produce a sparse network. A quantity, alpha, which indicates how close a parameter is to zero is used to estimate the number of model parameters which are being pruned out. The value of alpha is incorporated into a function of the Schwartz information criterion to form an objective function whose maxima, as alpha tends to zero, define parsimonious neural network models for a given data set. Since there is a multiplicity of maxima, differential evolution, with its greater capacity for global optimization, is used to optimize this objective function. The value of alpha is progressively reduced during the evolution of the population of models in the manner of a sequential unconstrained optimization technique. The method is illustrated by results on four sets of data.", "paper_title": "Differential evolution and sparse neural networks", "paper_id": "WOS:000258443300005"}