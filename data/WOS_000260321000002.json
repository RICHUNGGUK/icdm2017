{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "overview"}, {"score": 0.004559661374976129, "phrase": "automatic_medical_annotation_task"}, {"score": 0.0041827333568982055, "phrase": "task_setup"}, {"score": 0.0040334640350288, "phrase": "evaluation_campaign"}, {"score": 0.003925004567341077, "phrase": "medical_automatic_image_annotation_task"}, {"score": 0.003871869188502677, "phrase": "imageclef"}, {"score": 0.0038368442918503072, "phrase": "increasing_complexity"}, {"score": 0.003600351201298387, "phrase": "completely_automatic_annotation"}, {"score": 0.0035677736570659813, "phrase": "medical_images"}, {"score": 0.0035194572615570977, "phrase": "visual_properties"}, {"score": 0.003056819704624705, "phrase": "realistic_task"}, {"score": 0.003015401937358271, "phrase": "large_number"}, {"score": 0.002988101395789399, "phrase": "possible_classes"}, {"score": 0.0029610472910087176, "phrase": "different_levels"}, {"score": 0.0029076695661972114, "phrase": "detailed_analysis"}, {"score": 0.0028422946420821075, "phrase": "participating_groups"}, {"score": 0.0025716240685409513, "phrase": "class_hierarchy"}, {"score": 0.00245724480030654, "phrase": "local_image_descriptors"}, {"score": 0.0024349853033241663, "phrase": "discriminative_models"}, {"score": 0.0023802126605244438, "phrase": "good_predictions"}, {"score": 0.0023479408542211875, "phrase": "image_classes"}, {"score": 0.0022231606165229235, "phrase": "machine_learning"}, {"score": 0.002203016940021265, "phrase": "computer_vision_domain"}, {"score": 0.0021830553825770097, "phrase": "object_recognition"}, {"score": 0.0021632743048552536, "phrase": "non-medical_images"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Automatic image annotation", " Medical images", " Benchmark", " Evaluation"], "paper_abstract": "In this paper, the automatic medical annotation task of the 2007 CLEF cross language image retrieval campaign (ImageCLEF) is described. The paper focusses on the images used, the task setup, and the results obtained in the evaluation campaign. Since 2005, the medical automatic image annotation task exists in ImageCLEF with increasing complexity to evaluate the performance of state-of-the-art methods for completely automatic annotation of medical images based on visual properties. The paper also describes the evolution of the task from its origin in 2005-2007. The 2007 task, comprising 11,000 fully annotated training images and 1000 test images to be annotated, is a realistic task with a large number of possible classes at different levels of detail. Detailed analysis of the methods across participating groups is presented with respect to the (i) image representation, (ii) classification method, and (iii) use of the class hierarchy. The results show that methods which build on local image descriptors and discriminative models are able to provide good predictions of the image classes, mostly by using techniques that were originally developed in the machine learning and computer vision domain for object recognition in non-medical images. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Automatic medical image annotation in ImageCLEF 2007: Overview, results, and discussion", "paper_id": "WOS:000260321000002"}