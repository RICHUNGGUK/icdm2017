{"auto_keywords": [{"score": 0.037269594768434666, "phrase": "natural_stimuli"}, {"score": 0.00481495049065317, "phrase": "translation-invariant_neurons"}, {"score": 0.004704844560325455, "phrase": "human_visual_system"}, {"score": 0.00462773520532301, "phrase": "complex_objects"}, {"score": 0.004447766706144458, "phrase": "higher_cortical_areas"}, {"score": 0.004403874076980828, "phrase": "sensory_neurons"}, {"score": 0.004317378385832431, "phrase": "complex_visual_features"}, {"score": 0.004204676524824095, "phrase": "image_translation"}, {"score": 0.0041357305373417455, "phrase": "strong_nonlinearities"}, {"score": 0.003961694867182568, "phrase": "response_patterns"}, {"score": 0.0038838500580958744, "phrase": "formidable_computational_challenge"}, {"score": 0.003845500924733243, "phrase": "related_problem"}, {"score": 0.0037450707321601963, "phrase": "randomized_inputs"}, {"score": 0.0036958398682001015, "phrase": "white_noise"}, {"score": 0.003587415196933649, "phrase": "complex_high-order_correlations"}, {"score": 0.0034706569037527774, "phrase": "novel_two-step_optimization_technique"}, {"score": 0.0033465923623290034, "phrase": "position_invariance"}, {"score": 0.0033245147666408157, "phrase": "neural_responses"}, {"score": 0.0031739835218110015, "phrase": "maximally_informative_dimension"}, {"score": 0.0031426214970263137, "phrase": "estimated_spatial_location"}, {"score": 0.0028930017387859804, "phrase": "next_step"}, {"score": 0.0028267211555015295, "phrase": "monotonic_relationship"}, {"score": 0.002798780784517325, "phrase": "firing_rate"}, {"score": 0.0026281103468378856, "phrase": "largest_projection"}, {"score": 0.0025340874787959195, "phrase": "quick_convergence"}, {"score": 0.0024842242921857705, "phrase": "estimation_results"}, {"score": 0.0024192592458651204, "phrase": "small_signal-to-noise_ratios"}, {"score": 0.00234043119560776, "phrase": "complex_cells"}, {"score": 0.0023172863267744703, "phrase": "primary_visual_cortex"}, {"score": 0.002279217896185919, "phrase": "natural_movies"}, {"score": 0.002161542155696987, "phrase": "translation-invariant_models"}, {"score": 0.002118993962770973, "phrase": "position-specific_models"}], "paper_keywords": [""], "paper_abstract": "The human visual system is capable of recognizing complex objects even when their appearances change drastically under various viewing conditions. Especially in the higher cortical areas, the sensory neurons reflect such functional capacity in their selectivity for complex visual features and invariance to certain object transformations, such as image translation. Due to the strong nonlinearities necessary to achieve both the selectivity and invariance, characterizing and predicting the response patterns of these neurons represents a formidable computational challenge. A related problem is that such neurons are poorly driven by randomized inputs, such as white noise, and respond strongly only to stimuli with complex high-order correlations, such as natural stimuli. Here we describe a novel two-step optimization technique that can characterize both the shape selectivity and the range and coarseness of position invariance from neural responses to natural stimuli. One step in the optimization is finding the template as the maximally informative dimension given the estimated spatial location where the response could have been triggered within each image. The estimates of the locations that triggered the response are updated in the next step. Under the assumption of a monotonic relationship between the firing rate and stimulus projections on the template at a given position, the most likely location is the one that has the largest projection on the estimate of the template. The algorithm shows quick convergence during optimization, and the estimation results are reliable even in the regime of small signal-to-noise ratios. When we apply the algorithm to responses of complex cells in the primary visual cortex (V1) to natural movies, we find that responses of the majority of cells were significantly better described by translation-invariant models based on one template compared with position-specific models with several relevant features.", "paper_title": "Characterizing Responses of Translation-Invariant Neurons to Natural Stimuli: Maximally Informative Invariant Dimensions", "paper_id": "WOS:000307125000005"}