{"auto_keywords": [{"score": 0.03483220185213116, "phrase": "base_classifiers"}, {"score": 0.009076312083079118, "phrase": "margin_distribution"}, {"score": 0.00564594822739079, "phrase": "svm"}, {"score": 0.00516112115021393, "phrase": "bagging"}, {"score": 0.00481495049065317, "phrase": "simple_and_effective_technique"}, {"score": 0.004481132747216135, "phrase": "redundant_base_classifiers"}, {"score": 0.004421156723819163, "phrase": "original_bagging"}, {"score": 0.004322967486451877, "phrase": "pruning_approach"}, {"score": 0.0041703614123283165, "phrase": "proposed_technique"}, {"score": 0.004096082254543022, "phrase": "based_classification_loss"}, {"score": 0.004041238873048836, "phrase": "optimization_objective"}, {"score": 0.003933736547198066, "phrase": "training_samples"}, {"score": 0.0038290829316346654, "phrase": "optimal_margin_distribution"}, {"score": 0.003677280504320636, "phrase": "sparse_ensemble"}, {"score": 0.00333097896868937, "phrase": "sparse_weight_vector"}, {"score": 0.003071965703296204, "phrase": "large_weights"}, {"score": 0.0029236680654022664, "phrase": "mad-bagging"}, {"score": 0.0028844769395018595, "phrase": "simple_voting"}, {"score": 0.0028586409810976367, "phrase": "weighted_voting"}, {"score": 0.0027575842698911173, "phrase": "selected_base_classifiers"}, {"score": 0.0026841368358805407, "phrase": "pruned_ensemble"}, {"score": 0.0025316298556765445, "phrase": "cart"}, {"score": 0.0024864612830452254, "phrase": "nearest_neighbor"}, {"score": 0.0023557346035701096, "phrase": "margin_distribution_based_cart"}, {"score": 0.002292965705677312, "phrase": "classification_accuracies"}, {"score": 0.002231872088802303, "phrase": "inn"}, {"score": 0.00216263228969761, "phrase": "single_classifiers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Classification", " Bagging", " Margin distribution", " Sparse ensemble"], "paper_abstract": "Bagging is a simple and effective technique for generating an ensemble of classifiers. It is found there are a lot of redundant base classifiers in the original Bagging. We design a pruning approach to bagging for improving its generalization power. The proposed technique introduces the margin distribution based classification loss as the optimization objective and minimizes the loss on training samples, which leads to an optimal margin distribution. Meanwhile, in order to derive a sparse ensemble, l(1) regularization is introduced to control the size of ensembles. By this way, we can obtain a sparse weight vector of base classifiers. Then we rank the base classifiers with respect to their weights and combine the base classifiers with large weights. We call this technique MArgin Distribution base Bagging pruning (MAD-Bagging). Simple voting and weighted voting are tried to combine the outputs of selected base classifiers. The performance of this pruned ensemble is evaluated with several UCI benchmark tasks, where base classifiers are trained with SVM, CART, and the nearest neighbor (1NN) rule, respectively. The results show that margin distribution based CART pruned Bagging can significantly improve classification accuracies. However, SVM and INN pruned Bagging improve little compared with single classifiers. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Margin distribution based bagging pruning", "paper_id": "WOS:000302972300003"}