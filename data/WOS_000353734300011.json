{"auto_keywords": [{"score": 0.048946208597162526, "phrase": "bic"}, {"score": 0.03324892655765073, "phrase": "hbic"}, {"score": 0.00481495049065317, "phrase": "hierarchical_bic."}, {"score": 0.004757643419056999, "phrase": "bayesian_information_criterion"}, {"score": 0.004481132747216135, "phrase": "model_selection"}, {"score": 0.0044277810648955624, "phrase": "finite_mixture_models"}, {"score": 0.00407161650109722, "phrase": "whole_sample_size"}, {"score": 0.003951453722517865, "phrase": "clustered_structure"}, {"score": 0.0035901685646972585, "phrase": "novel_criterion"}, {"score": 0.0033812801309408514, "phrase": "component_complexity"}, {"score": 0.0032423089921154503, "phrase": "clustered_data_structure"}, {"score": 0.0030353401945270755, "phrase": "variational_bayesian"}, {"score": 0.002910545717767559, "phrase": "sample_size"}, {"score": 0.0028245516301681713, "phrase": "widely_used_bic"}, {"score": 0.002724696729311532, "phrase": "empirical_study"}, {"score": 0.002628362641980801, "phrase": "theoretical_result"}, {"score": 0.0023592730043928093, "phrase": "bic."}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Model selection", " Mixture model", " EM", " Maximum likelihood estimation", " BIC", " Hierarchical BIC", " Clustering"], "paper_abstract": "The Bayesian information criterion (BIC) is one of the most popular criteria for model selection in finite mixture models. However, it implausibly penalizes the complexity of each component using the whole sample size and completely ignores the clustered structure inherent in the data, resulting in over-penalization. To overcome this problem, a novel criterion called hierarchical BIC (HBIC) is proposed which penalizes the component complexity only using its local sample size and matches the clustered data structure well. Theoretically, HBIC is an approximation of the variational Bayesian (VB) lower bound when sample size is large and the widely used BIC is a less accurate approximation. An empirical study is conducted to verify this theoretical result and a series of experiments is performed on simulated and real data sets to compare HBIC and BIC. The results show that HBIC outperforms BIC substantially and BIC suffers from underestimation. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Mixture model selection via hierarchical BIC", "paper_id": "WOS:000353734300011"}