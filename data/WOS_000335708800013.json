{"auto_keywords": [{"score": 0.049655608595442415, "phrase": "layered_model"}, {"score": 0.04923607408107278, "phrase": "multiple_cues"}, {"score": 0.03249743674047246, "phrase": "group_actions"}, {"score": 0.00481495049065317, "phrase": "human_group_action"}, {"score": 0.004688321299229397, "phrase": "human_actions"}, {"score": 0.004646852519870358, "phrase": "important_contents"}, {"score": 0.004544770921520536, "phrase": "video_analysis"}, {"score": 0.004425215632459868, "phrase": "notable_methods"}, {"score": 0.004308791766002289, "phrase": "individual_actions"}, {"score": 0.004270665568018583, "phrase": "pair's_interactions"}, {"score": 0.004158290806404304, "phrase": "multiple_persons"}, {"score": 0.0038556354709511818, "phrase": "small_group"}, {"score": 0.00378767277402891, "phrase": "countable_persons"}, {"score": 0.0037043953071235155, "phrase": "correlative_purposes"}, {"score": 0.003606866938661143, "phrase": "varying_number"}, {"score": 0.003527550673371394, "phrase": "inherent_interactions"}, {"score": 0.0033442114331500407, "phrase": "discriminative_characteristics"}, {"score": 0.003314591878914152, "phrase": "different_granularities"}, {"score": 0.0032272929113113203, "phrase": "uniform_statistical_representation"}, {"score": 0.0030459283281463953, "phrase": "arbitrary_features"}, {"score": 0.0030189426536422577, "phrase": "different_action_scales"}, {"score": 0.002992195343569767, "phrase": "gaussian_processes"}, {"score": 0.0029263562233195423, "phrase": "motion_trajectories"}, {"score": 0.0028875481626754696, "phrase": "probabilistic_perspective"}, {"score": 0.0027010719929751, "phrase": "discriminative_appearance_information"}, {"score": 0.002641621667026613, "phrase": "participants'_visual_\"style\"_features"}, {"score": 0.0026182085765020548, "phrase": "group's_\"shape\"_characters"}, {"score": 0.002537880772794665, "phrase": "different_levels"}, {"score": 0.002427372728532664, "phrase": "recognition_accuracy"}, {"score": 0.0021910825909008946, "phrase": "nus-hga_dataset"}, {"score": 0.0021716540012278744, "phrase": "satisfactory_results"}, {"score": 0.002152397315548125, "phrase": "behave_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Human group action", " Layered model", " Gaussian processes", " Motion trajectory", " Action style", " Group shape"], "paper_abstract": "Human actions are important contents which are helpful for video analysis and interpretation. Recently, notable methods have been proposed to recognize individual actions and pair's interactions, whereas recognizing more complex actions involving multiple persons remains a challenge. In this paper, we focus on the actions performed by a small group that consists of countable persons who generally act with correlative purposes. To cope with the varying number of participants and the inherent interactions within the group, we propose a layered model to describe the discriminative characteristics at different granularities and present each layer with uniform statistical representation. Depending on this model, we can flexibly represent group actions with arbitrary features at different action scales. Gaussian processes are employed to represent motion trajectories from a probabilistic perspective to handle the variability of movements within the group. Moreover, we take discriminative appearance information into account and depict participants' visual \"style\" features and group's \"shape\" characters. Taking advantage of multiple cues from different levels, our approach can better represent group actions and improve the recognition accuracy. Experiments on two human group action datasets demonstrate the validity of our approach, as we achieve the state-of-the-art performance on NUS-HGA dataset and satisfactory results on Behave dataset (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Recognizing human group action by layered model with multiple cues", "paper_id": "WOS:000335708800013"}