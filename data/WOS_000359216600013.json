{"auto_keywords": [{"score": 0.032996840512363755, "phrase": "pascal_voc"}, {"score": 0.029802260146187, "phrase": "object_detection"}, {"score": 0.00481495049065317, "phrase": "spatial_pyramid_pooling"}, {"score": 0.004780334370089564, "phrase": "deep_convolutional_networks_for_visual_recognition"}, {"score": 0.004745965928357563, "phrase": "existing_deep_convolutional_neural_networks"}, {"score": 0.004544859094594241, "phrase": "input_image"}, {"score": 0.004352236750212992, "phrase": "recognition_accuracy"}, {"score": 0.004049096735491828, "phrase": "pooling_strategy"}, {"score": 0.003849502288990788, "phrase": "new_network_structure"}, {"score": 0.003712964709503484, "phrase": "fixed-length_representation"}, {"score": 0.003646516176920946, "phrase": "pyramid_pooling"}, {"score": 0.003416963042033614, "phrase": "cnn-based_image_classification_methods"}, {"score": 0.003190265467214105, "phrase": "cnn_architectures"}, {"score": 0.0030219357765636497, "phrase": "state-of-the-art_classification_results"}, {"score": 0.0027310567737368033, "phrase": "feature_maps"}, {"score": 0.0027015970441828267, "phrase": "entire_image"}, {"score": 0.0026151058979100596, "phrase": "arbitrary_regions"}, {"score": 0.0025405463228033486, "phrase": "fixed-length_representations"}, {"score": 0.0024326641072463157, "phrase": "convolutional_features"}, {"score": 0.002406415352892016, "phrase": "processing_test_images"}, {"score": 0.002337791830793841, "phrase": "r-cnn_method"}, {"score": 0.0023042157412517333, "phrase": "better_or_comparable_accuracy"}, {"score": 0.0022547515816852266, "phrase": "imagenet_large_scale_visual_recognition_challenge"}, {"score": 0.002127965733093433, "phrase": "image_classification"}], "paper_keywords": ["Convolutional neural networks", " spatial pyramid pooling", " image classification", " object detection"], "paper_abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is \"artificial\" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \"spatial pyramid pooling\", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.", "paper_title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "paper_id": "WOS:000359216600013"}