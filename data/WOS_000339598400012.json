{"auto_keywords": [{"score": 0.03934488867229197, "phrase": "blas_library"}, {"score": 0.00481495049065317, "phrase": "execution_time"}, {"score": 0.004774115617883476, "phrase": "scientific_software"}, {"score": 0.004733625412289433, "phrase": "numa_systems"}, {"score": 0.004594582559945568, "phrase": "large_parallel_systems"}, {"score": 0.004440648141548589, "phrase": "non-uniform_memory_access"}, {"score": 0.004273601449537163, "phrase": "large_number"}, {"score": 0.0041657281535389615, "phrase": "hierarchically_organized_memory"}, {"score": 0.004112812699639956, "phrase": "main_basic_component"}, {"score": 0.004060566667226606, "phrase": "scientific_codes"}, {"score": 0.004008981656795115, "phrase": "matrix_multiplication"}, {"score": 0.003941215575554871, "phrase": "efficient_development"}, {"score": 0.0038090874465452214, "phrase": "matrix_multiplication_routine"}, {"score": 0.003512702405367876, "phrase": "free_implementations"}, {"score": 0.003468053168607071, "phrase": "latest_versions"}, {"score": 0.0032950412841674026, "phrase": "multicore_systems"}, {"score": 0.0031845067058084583, "phrase": "parallel_codes"}, {"score": 0.002936577024096783, "phrase": "auto-tuning_method"}, {"score": 0.002850168497994985, "phrase": "optimum_number"}, {"score": 0.0027662954811372175, "phrase": "parallel_level"}, {"score": 0.0025947642929906407, "phrase": "simple_but_effective_theoretical_model"}, {"score": 0.0025183881730011597, "phrase": "two-level_routines"}, {"score": 0.002433843373780471, "phrase": "two-level_matrix-matrix_multiplication"}, {"score": 0.0022828795331819025, "phrase": "traditional_schemes"}, {"score": 0.0022346638243276717, "phrase": "multithreaded_routine"}, {"score": 0.002217042955870026, "phrase": "bias"}, {"score": 0.002123051344001408, "phrase": "multithreaded_dgemm"}], "paper_keywords": ["Auto-tuning", " Linear algebra", " Performance modeling", " NUMA"], "paper_abstract": "The most computationally demanding scientific problems are solved with large parallel systems. In some cases these systems are Non-Uniform Memory Access (NUMA) multiprocessors made up of a large number of cores which share a hierarchically organized memory. The main basic component of these scientific codes is often matrix multiplication, and the efficient development of other linear algebra packages is directly based on the matrix multiplication routine implemented in the BLAS library. BLAS library is used in the form of packages implemented by the vendors or free implementations. The latest versions of this library are multithreaded and can be used efficiently in multicore systems, but when they are used inside parallel codes, the two parallelism levels can interfere and produce a degradation of the performance. In this work, an auto-tuning method is proposed to select automatically the optimum number of threads to use at each parallel level when multithreaded linear algebra routines are called from OpenMP parallel codes. The method is based on a simple but effective theoretical model of the execution time of the two-level routines. The methodology is applied to a two-level matrix-matrix multiplication and to different matrix factorizations (LU, QR and Cholesky) by blocks. Traditional schemes which directly use the multithreaded routine of BIAS, dgemm, are compared with schemes combining the multithreaded dgemm with OpenMP. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Auto-tuned nested parallelism: A way to reduce the execution time of scientific software in NUMA systems", "paper_id": "WOS:000339598400012"}