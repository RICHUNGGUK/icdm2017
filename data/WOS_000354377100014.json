{"auto_keywords": [{"score": 0.04693443770971553, "phrase": "random_forest"}, {"score": 0.04047293105117697, "phrase": "decision_trees"}, {"score": 0.0047559432344272, "phrase": "de-correlated_trees"}, {"score": 0.004611554879923759, "phrase": "conceptually_radical_approach"}, {"score": 0.00428257928723, "phrase": "prior_distribution"}, {"score": 0.0041269626762569095, "phrase": "weighted_ensemble"}, {"score": 0.004076352180903171, "phrase": "predictive_probabilities"}, {"score": 0.003284361433827464, "phrase": "bayesian_inference"}, {"score": 0.0031260524791486347, "phrase": "power_likelihood"}, {"score": 0.0029215242891888083, "phrase": "bayesian"}, {"score": 0.0027457584014804574, "phrase": "bayesian_random_forest"}, {"score": 0.002678734566245688, "phrase": "built-in_safety"}, {"score": 0.0025495426598675583, "phrase": "good_predictive_performance"}, {"score": 0.002487296495786493, "phrase": "underlying_probabilistic_model"}, {"score": 0.0023241799929929396, "phrase": "smc"}, {"score": 0.002295277579105177, "phrase": "bayesian_decision_trees"}, {"score": 0.002157697367447098, "phrase": "competitive_performance"}, {"score": 0.002105017525051423, "phrase": "gini"}], "paper_keywords": ["Bayesian methods", " random forest", " decision trees"], "paper_abstract": "Random forests works by averaging several predictions of de-correlated trees. We show a conceptually radical approach to generate a random forest: random sampling of many trees from a prior distribution, and subsequently performing a weighted ensemble of predictive probabilities. Our approach uses priors that allow sampling of decision trees even before looking at the data, and a power likelihood that explores the space spanned by combination of decision trees. While each tree performs Bayesian inference to compute its predictions, our aggregation procedure uses the power likelihood rather than the likelihood and is therefore strictly speaking not Bayesian. Nonetheless, we refer to it as a Bayesian random forest but with a built-in safety. The safeness comes as it has good predictive performance even if the underlying probabilistic model is wrong. We demonstrate empirically that our Safe-Bayesian random forest outperforms MCMC or SMC based Bayesian decision trees in term of speed and accuracy, and achieves competitive performance to entropy or Gini optimised random forest, yet is very simple to construct.", "paper_title": "A Very Simple Safe-Bayesian Random Forest", "paper_id": "WOS:000354377100014"}