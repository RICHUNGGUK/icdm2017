{"auto_keywords": [{"score": 0.04979106692834644, "phrase": "human_perceptions"}, {"score": 0.015719716506582538, "phrase": "shape_signatures"}, {"score": 0.015533304978687707, "phrase": "geometric_similarity"}, {"score": 0.008207754851026145, "phrase": "human_perception"}, {"score": 0.004728624975677976, "phrase": "manual_indexing"}, {"score": 0.004700193486903964, "phrase": "large_databases"}, {"score": 0.004671932143798307, "phrase": "geometric_information"}, {"score": 0.004505881071218599, "phrase": "automated_retrieval"}, {"score": 0.004478782833067256, "phrase": "indexing_schemes"}, {"score": 0.004306556130853961, "phrase": "relatively_small_number"}, {"score": 0.004191200981137508, "phrase": "ill-defined_properties"}, {"score": 0.00415343716016457, "phrase": "\"geometric_similarity"}, {"score": 0.003615071654787124, "phrase": "human_subjects"}, {"score": 0.003507567662772987, "phrase": "drexel_benchmark_datasets"}, {"score": 0.003423862933454743, "phrase": "twelve_manual_inspections"}, {"score": 0.003117890043128058, "phrase": "competitive_neural_network"}, {"score": 0.0030434561939032597, "phrase": "\"similar\"_clusters"}, {"score": 0.0030069062905274976, "phrase": "human_and_machine"}, {"score": 0.0029351141718104725, "phrase": "similar_components"}, {"score": 0.0027463775689957255, "phrase": "firstly_the_results"}, {"score": 0.0027215964067013924, "phrase": "human_perception_test"}, {"score": 0.0026889014020738578, "phrase": "drexel_dataset"}, {"score": 0.0026326249204932733, "phrase": "recorded_spectrum"}, {"score": 0.002478225374613311, "phrase": "low_rate"}, {"score": 0.0024632912250657636, "phrase": "false_positives"}, {"score": 0.002419025278429845, "phrase": "false_negative_rate"}, {"score": 0.0023541098780336255, "phrase": "perceived_similarity"}, {"score": 0.002202651051682622, "phrase": "direct_proportion"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["geometric similarity", " shape perception", " D2 shape distribution", " spherical harmonics", " surface partitioning spectrum", " artificial neural networks"], "paper_abstract": "Manual indexing of large databases of geometric information is both costly and difficult. Because of this, research into automated retrieval and indexing schemes has focused on the development of methods for characterising 3D shapes with a relatively small number of parameters (e.g. histograms) that allow ill-defined properties such as \"geometric similarity\" to be computed. However although many methods of generating these so called shape signatures have been proposed, little work on assessing how closely these measures match human perceptions of geometric similarity has been reported. This paper details the results of a trial that compared the part families identified by both human subjects and three published shape signatures. To do this a similarity matrix for the Drexel benchmark datasets was created by averaging the results of twelve manual inspections. Three different shape signatures (D2 shape distribution, spherical harmonics and surface portioning spectrum) were computed for each component in the dataset, and then used as input to a competitive neural network that sorted the objects into numbers of \"similar\" clusters. Comparison of human and machine generated clusters (i.e. families) of similar components allows the effectiveness of the signatures at duplicating human perceptions of shapes to be quantified. The work reported makes two contributions. Firstly the results of the human perception test suggest that the Drexel dataset contains objects whose perceived similarity levels ranged across the recorded spectrum (i.e. 0.1 to 0.9); Secondly the results obtained from benchmarking the three shape signatures against human perception demonstrate a low rate of false positives for all three signatures and a false negative rate that varied almost linearly with the amount of perceived similarity. In other words the shape signatures studied were reasonably effective at matching human perception in that they returned few wrong results and excluded parts in direct proportion to the level of similarity demanded by the user. (C) 2006 Elsevier Ltd. All rights reserved.", "paper_title": "Benchmarking shape signatures against human perceptions of geometric similarity", "paper_id": "WOS:000241350800008"}