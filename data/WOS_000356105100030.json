{"auto_keywords": [{"score": 0.04497074066422271, "phrase": "data_points"}, {"score": 0.04124584638960657, "phrase": "training_process"}, {"score": 0.040034968123333736, "phrase": "prediction_accuracy"}, {"score": 0.03730689531156782, "phrase": "kernel_function"}, {"score": 0.03273308790548843, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "support_vector_machine"}, {"score": 0.0047882655545381835, "phrase": "svm"}, {"score": 0.004747771713778112, "phrase": "good_generalization_ability"}, {"score": 0.004539005582217309, "phrase": "mathematical_model"}, {"score": 0.004450568197899267, "phrase": "kernel_matrix"}, {"score": 0.0041484957687138784, "phrase": "improved_algorithms"}, {"score": 0.00357395617690842, "phrase": "main_computation"}, {"score": 0.0035240305920962766, "phrase": "linear_feature_space"}, {"score": 0.003474799999519295, "phrase": "computational_cost"}, {"score": 0.003407026489727143, "phrase": "concept_\"relative_density"}, {"score": 0.0033218213825698417, "phrase": "support_vectors"}, {"score": 0.0031666307941887093, "phrase": "new_method"}, {"score": 0.0031223774204093713, "phrase": "existing_methods"}, {"score": 0.003018668521745676, "phrase": "good_local_characteristics"}, {"score": 0.0029681091946289757, "phrase": "original_space"}, {"score": 0.0027741761880779535, "phrase": "noise_data"}, {"score": 0.002735392281320413, "phrase": "inseparable_problem"}, {"score": 0.0026895647154863405, "phrase": "separable_problem"}, {"score": 0.00266693888510356, "phrase": "cross_validation"}, {"score": 0.002556625752785268, "phrase": "existing_svm_methods"}, {"score": 0.0023035922232490106, "phrase": "time_complexity"}, {"score": 0.0022522565004588113, "phrase": "training_time"}, {"score": 0.0021408862709571615, "phrase": "artificial_and_public_datasets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Relative density", " Support vector machine", " Boundary"], "paper_abstract": "As a support vector machine (SVM) has good generalization ability, it has been implemented in various applications. Yet in the process of resolving the mathematical model, it needs to compute the kernel matrix, the dimension of which is equal to the number of data points in the dataset, thereby costing a very high amount of memory. Some improved algorithms are proposed to extract the boundary of the dataset so that the number of data points participating in the training process decreases and the training process can be accelerated. But the prediction accuracy of most of these algorithms is so low that many support vectors are discarded. Moreover, those methods all need to perform the main computation by the kernel function in linear feature space, which increases the computational cost. In this paper, the concept \"relative density\" is proposed to extract the subset containing support vectors. The measure \"relative density\" is designed to be more meticulous so that the new method performs more precisely than existing methods. The proposed method makes use of the fact that it has good local characteristics to perform the computations in original space without having to use any kernel function. Therefore, efficiency is also improved. Furthermore, the proposed method can be used to detect noise data, by which an inseparable problem can be transformed into a separable problem so that cross validation can be avoided in various SVM algorithms. This is an advantage that none of the existing SVM methods has. Yet another advantage of this method is that it can be considered as a framework to be used in various SVM methods. This paper presents the details of the proposed accelerated algorithm, having a time complexity of O(n log n), that decreases training time significantly without decreasing prediction accuracy. The effectiveness and efficiency of the method is demonstrated through experiments on artificial and public datasets. (c) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Relative density based support vector machine", "paper_id": "WOS:000356105100030"}