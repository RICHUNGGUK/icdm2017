{"auto_keywords": [{"score": 0.030300638905853418, "phrase": "hybrid_features"}, {"score": 0.030019925790468825, "phrase": "decision-level_fusion"}, {"score": 0.028387849147691928, "phrase": "recognition_rate"}, {"score": 0.00481495049065317, "phrase": "anova_feature_selection_method"}, {"score": 0.004769182299692333, "phrase": "multi-classifier_neural_networks"}, {"score": 0.004678940805063634, "phrase": "human-computer_interaction"}, {"score": 0.004397230847865867, "phrase": "human's_affective_states"}, {"score": 0.003773680962197766, "phrase": "human_perception"}, {"score": 0.0036845490951857617, "phrase": "emotion-related_information"}, {"score": 0.0035632774896594524, "phrase": "speech_emotion_recognition_system"}, {"score": 0.00349577373145384, "phrase": "prosody_features"}, {"score": 0.003300813776412652, "phrase": "short-term_power_spectrum"}, {"score": 0.003131637033429892, "phrase": "integrated_time_motion_image"}, {"score": 0.0031018195951638882, "phrase": "quantized_image_matrix"}, {"score": 0.0029569247903654477, "phrase": "temporal_templates"}, {"score": 0.002928765962664163, "phrase": "experimental_results"}, {"score": 0.00276534038425712, "phrase": "unimodal_systems"}, {"score": 0.0025861368747446324, "phrase": "speech_unimodal_system"}, {"score": 0.002488985155689794, "phrase": "facial_expression_system"}, {"score": 0.0024301229420875155, "phrase": "proposed_multi-classifier_system"}, {"score": 0.0023840346303791032, "phrase": "improved_hybrid_system"}, {"score": 0.0022295126846388325, "phrase": "rbf"}, {"score": 0.0021560018573288666, "phrase": "speech-based_system"}, {"score": 0.0021049977753042253, "phrase": "facial_expression-based_system"}], "paper_keywords": ["Human-computer interaction", " Audiovisual emotion recognition", " Feature-level fusion", " Decision-level fusion", " Multi-classifier"], "paper_abstract": "To make human-computer interaction more naturally and friendly, computers must enjoy the ability to understand human's affective states the same way as human does. There are many modals such as face, body gesture and speech that people use to express their feelings. In this study, we simulate human perception of emotion through combining emotion-related information using facial expression and speech. Speech emotion recognition system is based on prosody features, mel-frequency cepstral coefficients (a representation of the short-term power spectrum of a sound) and facial expression recognition based on integrated time motion image and quantized image matrix, which can be seen as an extension to temporal templates. Experimental results showed that using the hybrid features and decision-level fusion improves the outcome of unimodal systems. This method can improve the recognition rate by about 15 % with respect to the speech unimodal system and by about 30 % with respect to the facial expression system. By using the proposed multi-classifier system that is an improved hybrid system, recognition rate would increase up to 7.5 % over the hybrid features and decision-level fusion with RBF, up to 22.7 % over the speech-based system and up to 38 % over the facial expression-based system.", "paper_title": "Audiovisual emotion recognition using ANOVA feature selection method and multi-classifier neural networks", "paper_id": "WOS:000330318500016"}