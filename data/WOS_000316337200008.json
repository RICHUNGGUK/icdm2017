{"auto_keywords": [{"score": 0.0434105433415452, "phrase": "discretized_intervals"}, {"score": 0.0364985323839451, "phrase": "ddu"}, {"score": 0.03181452972222957, "phrase": "non-parametric_normalized_discretization_criteria"}, {"score": 0.00481495049065317, "phrase": "discretization_techniques"}, {"score": 0.004741845603430361, "phrase": "important_role"}, {"score": 0.0047057084749407485, "phrase": "machine_learning"}, {"score": 0.004669845451682529, "phrase": "data_mining"}, {"score": 0.004529093447865156, "phrase": "training_data"}, {"score": 0.004443276312972159, "phrase": "data_discretization_unification"}, {"score": 0.004309323675697185, "phrase": "state-of-the-art_discretization_techniques"}, {"score": 0.0037259623657693794, "phrase": "large_number"}, {"score": 0.0036553070575015344, "phrase": "good_results"}, {"score": 0.0035179764329354877, "phrase": "optimal_solution"}, {"score": 0.003347105144425849, "phrase": "inconsistent_records"}, {"score": 0.003246091321090681, "phrase": "unnecessary_information_loss"}, {"score": 0.0031121406603482112, "phrase": "uni_versal_dis_cretization_technique"}, {"score": 0.00307657476125683, "phrase": "unidis"}, {"score": 0.002938317647182056, "phrase": "relatively_large_difference"}, {"score": 0.0029158847555394714, "phrase": "classification_errors"}, {"score": 0.00283870598459543, "phrase": "discretization_results"}, {"score": 0.0027529929913806066, "phrase": "new_entropy-based_measure"}, {"score": 0.002711109302199029, "phrase": "multi-dimensional_variables"}, {"score": 0.0026698611202305715, "phrase": "information_loss"}, {"score": 0.002629238854352447, "phrase": "concise_summarization"}, {"score": 0.0026091594819333654, "phrase": "continuous_variables"}, {"score": 0.002540078552743003, "phrase": "heuristic_algorithm"}, {"score": 0.0025110337774260773, "phrase": "better_discretization"}, {"score": 0.002444544487772349, "phrase": "entropy-based_inconsistency"}, {"score": 0.002416589459425828, "phrase": "theoretical_analysis"}, {"score": 0.0023981301755351607, "phrase": "experimental_results"}, {"score": 0.002290291401956985, "phrase": "popular_statistical_test"}, {"score": 0.002246799280512825, "phrase": "better_discretization_scheme"}, {"score": 0.0021705796895687864, "phrase": "previously_other_known_discretization_methods"}, {"score": 0.0021049977753042253, "phrase": "naive_bayes_classifier"}], "paper_keywords": ["Discretization", " Inconsistency", " Entropy", " J4.8 decision tree", " Naive Bayes classifier"], "paper_abstract": "Discretization techniques have played an important role in machine learning and data mining as most methods in such areas require that the training data set contains only discrete attributes. Data discretization unification (DDU), one of the state-of-the-art discretization techniques, trades off classification errors and the number of discretized intervals, and unifies existing discretization criteria. However, it suffers from two deficiencies. First, the efficiency of DDU is very low as it conducts a large number of parameters to search good results, which does not still guarantee to obtain an optimal solution. Second, DDU does not take into account the number of inconsistent records produced by discretization, which leads to unnecessary information loss. To overcome the above deficiencies, this paper presents a Uni versal Dis cretization technique, namely UniDis. We first develop a non-parametric normalized discretization criteria which avoids the effect of relatively large difference between classification errors and the number of discretized intervals on discretization results. In addition, we define a new entropy-based measure of inconsistency for multi-dimensional variables to effectively control information loss while producing a concise summarization of continuous variables. Finally, we propose a heuristic algorithm to guarantee better discretization based on the non-parametric normalized discretization criteria and the entropy-based inconsistency. Besides theoretical analysis, experimental results demonstrate that our approach is statistically comparable to DDU evaluated by a popular statistical test and it yields a better discretization scheme which significantly improves the accuracy of classification than previously other known discretization methods except for DDU by running J4.8 decision tree and Naive Bayes classifier.", "paper_title": "UniDis: a universal discretization technique", "paper_id": "WOS:000316337200008"}