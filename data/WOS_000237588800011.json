{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "support_vector_machines"}, {"score": 0.0047026237903774895, "phrase": "high-dimensional_feature_vectors"}, {"score": 0.0046364836476864325, "phrase": "high_computational_cost"}, {"score": 0.004401794742140806, "phrase": "feature_selection"}, {"score": 0.004339867166576126, "phrase": "dimensionality_reduction_problem"}, {"score": 0.004218602458288531, "phrase": "available_features"}, {"score": 0.003986103192038901, "phrase": "novel_feature_selection_method"}, {"score": 0.0038930368103015467, "phrase": "sequential_forward_search"}, {"score": 0.0036095259108028105, "phrase": "conventional_wrapper_methods"}, {"score": 0.003541923233726252, "phrase": "sfs_strategy"}, {"score": 0.0035085965086415474, "phrase": "fs_sfs"}, {"score": 0.0030446120560494155, "phrase": "training_process"}, {"score": 0.0030017207229983385, "phrase": "computational_cost"}, {"score": 0.0029454679553901613, "phrase": "single_svm_classifier"}, {"score": 0.0028495431343111897, "phrase": "new_criterion"}, {"score": 0.0025678829712650437, "phrase": "nonessential_features"}, {"score": 0.0024842242921857705, "phrase": "total_number"}, {"score": 0.002391938519840648, "phrase": "overfitting_problem"}, {"score": 0.002336004938424957, "phrase": "proposed_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["feature selection", " sequential forward search (SFS)", " support vector machines (SVM)"], "paper_abstract": "In many pattern recognition applications, high-dimensional feature vectors impose a high computational cost as well as the risk of \"overfitting\". Feature Selection addresses the dimensionality reduction problem by determining a subset of available features which is most essential for classification. This paper presents a novel feature selection method named filtered and supported sequential forward search (FS_SFS) in the context of support vector machines (SVM). In comparison with conventional wrapper methods that employ the SFS strategy, FS_SFS has two important properties to reduce the time of computation. First, it dynamically maintains a subset of samples for the training of SVM. Because not all the available samples participate in the training process, the computational cost to obtain a single SVM classifier is decreased. Secondly, a new criterion, which takes into consideration both the discriminant ability of individual features and the correlation between them, is proposed to effectively filter out nonessential features. As a result, the total number of training is significantly reduced and the overfitting problem is alleviated. The proposed approach is tested on both synthetic and real data to demonstrate its effectiveness and efficiency. (c) 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "FS_SFS: A novel feature selection method for support vector machines", "paper_id": "WOS:000237588800011"}