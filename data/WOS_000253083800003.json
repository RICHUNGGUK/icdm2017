{"auto_keywords": [{"score": 0.004841935098280483, "phrase": "nerf"}, {"score": 0.00481495049065317, "phrase": "approximate_clustering"}, {"score": 0.004750804958816491, "phrase": "key_challenge"}, {"score": 0.004725385523804256, "phrase": "pattern_recognition"}, {"score": 0.004649936560866145, "phrase": "computational_efficiency"}, {"score": 0.0045879792359519375, "phrase": "large_data_sets"}, {"score": 0.004143196873246147, "phrase": "distinguished_features"}, {"score": 0.004109968477407982, "phrase": "progressive_sampling"}, {"score": 0.003802135020791801, "phrase": "literal_nerf"}, {"score": 0.0036324746826127997, "phrase": "relational_data"}, {"score": 0.0035078427258024613, "phrase": "truly_large_data_sets"}, {"score": 0.0032624429135093237, "phrase": "sample_size"}, {"score": 0.003125175084962886, "phrase": "whole_purpose"}, {"score": 0.0030178961017306646, "phrase": "sampling_scheme"}, {"score": 0.002969628818856786, "phrase": "different_parameters"}, {"score": 0.0029299945756268756, "phrase": "modified_sampling_scheme"}, {"score": 0.002875391177926809, "phrase": "simple_random_sampling"}, {"score": 0.0027991418754844347, "phrase": "enerf"}, {"score": 0.002717594582706144, "phrase": "clustering_tendency"}, {"score": 0.002631333840588461, "phrase": "original_progressive_sampling_scheme"}, {"score": 0.0025822818714499795, "phrase": "truly_vl_data"}, {"score": 0.002568435291444709, "phrase": "numerical_experiments"}, {"score": 0.0025478041555687036, "phrase": "distance_matrix"}, {"score": 0.0024209250169088575, "phrase": "proposed_sampling_method"}, {"score": 0.0023503703309169406, "phrase": "data_set"}, {"score": 0.0022880115261460212, "phrase": "computation_time"}, {"score": 0.0021392418081444798, "phrase": "vl_data"}, {"score": 0.0021049977753042253, "phrase": "wiley_periodicals"}], "paper_keywords": [""], "paper_abstract": "A key challenge in pattern recognition is how to scale the computational efficiency of clustering algorithms on large data sets. The extension of non-Euclidean relational fuzzy c-means (NERF) clustering to very large (VL = unloadable) relational data is called the extended NERF (eNERF) clustering algorithm, which comprises four phases: (i) finding distinguished features that monitor progressive sampling; (ii) progressively sampling from a N x N relational matrix R(N) to obtain a n x n sample matrix R(n); (iii) clustering R(n) with literal NERF; and (iv) extending the clusters in R(n) to the remainder of the relational data. Previously published examples on several fairly small data sets suggest that eNERF is feasible for truly large data sets. However, it seems that phases (i) and (ii), i.e., finding R(n) are not very practical because the sample size n often turns out to be roughly 50% of N, and this over-sampling defeats the whole purpose of eNERF. In this paper, we examine the performance of the sampling scheme of eNERF with respect to different parameters. We propose a modified sampling scheme for use with eNERF that combines simple random sampling with (parts of) the sampling procedures used by eNERF and a related algorithm sVAT (scalable visual assessment of clustering tendency). We demonstrate that our modified sampling scheme can eliminate over-sampling of the original progressive sampling scheme, thus enabling the processing of truly VL data. Numerical experiments on a distance matrix of a set of 3,000,000 vectors drawn from a mixture of 5 bivariate normal distributions demonstrate the feasibility and effectiveness of the proposed sampling method. We also find that actually running eNERF on a data set of this size is very costly in terms of computation time. Thus, our results demonstrate that further modification of eNERF, especially the extension stage, will be needed before it is truly practical for VL data. (c) 2008 Wiley Periodicals, Inc.", "paper_title": "Selective sampling for approximate clustering of very large data sets", "paper_id": "WOS:000253083800003"}