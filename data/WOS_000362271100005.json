{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "human_touch"}, {"score": 0.004681226806682058, "phrase": "pet_cat"}, {"score": 0.004637478829869904, "phrase": "dog's_ability"}, {"score": 0.004487541157100498, "phrase": "interaction_channel"}, {"score": 0.00444559516711374, "phrase": "high_visceral_impact"}, {"score": 0.004383406949799696, "phrase": "social_robots"}, {"score": 0.004123781267600542, "phrase": "understudied_element"}, {"score": 0.003897742942358768, "phrase": "furry_robot_pet"}, {"score": 0.00375395061615019, "phrase": "nine_emotions"}, {"score": 0.003531474887552889, "phrase": "lap-sized_robot_prototype"}, {"score": 0.003369322866499541, "phrase": "overall_correct_classification"}, {"score": 0.0028715299874549245, "phrase": "high_arousal_zone"}, {"score": 0.0028047899594686003, "phrase": "classifier_performance"}, {"score": 0.002726740692380339, "phrase": "new_metrics"}, {"score": 0.0026384167183586015, "phrase": "gestural_expressions"}, {"score": 0.002447101794397243, "phrase": "gesture_recognition"}, {"score": 0.0023789822182914877, "phrase": "unique_first_insight"}, {"score": 0.002301896758256575, "phrase": "affective_touch"}, {"score": 0.0022378101636842296, "phrase": "design_tool"}, {"score": 0.002196077907454079, "phrase": "unintrusive_affect"}, {"score": 0.0021652891317252994, "phrase": "deployed_interactions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Affective interfaces", " Haptic", " Human robot interaction", " Affect recognition", " Gesture recognition"], "paper_abstract": "A pet cat or dog's ability to respond to our emotional state opens an interaction channel with high visceral impact, which social robots may also be able to access. Touch is a key but understudied element; here, we explore its emotional content in the context of a furry robot pet. We asked participants to imagine feeling nine emotions located in a 2-D arousal-valence affect space, then to express them by touching a lap-sized robot prototype equipped with pressure sensors and accelerometer. We found overall correct classification (Random Forests) within the 2-D grid of 36% (all participants combined) and 48% (average of participants classified individually); chance 11%. Rates rose to 56% in the high arousal zone. To better understand classifier performance, we defined and analyzed new metrics that better indicate closeness of the gestural expressions. We also present a method to combine direct affect recognition with affect inferred from gesture recognition. This analysis provides a unique first insight into the nature and quality of affective touch, with implications as a design tool and for incorporating unintrusive affect sensing into deployed interactions. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Recognizing affect in human touch of a robot", "paper_id": "WOS:000362271100005"}