{"auto_keywords": [{"score": 0.02533429978532225, "phrase": "boolean"}, {"score": 0.00481495049065317, "phrase": "machine_learning"}, {"score": 0.0041316803676012155, "phrase": "single_large_one"}, {"score": 0.003545024951402821, "phrase": "highly_constrained_environments"}, {"score": 0.0032836104153151973, "phrase": "human_experts"}, {"score": 0.0022041312401435346, "phrase": "sample_complexity_bounds"}, {"score": 0.0021049977753042253, "phrase": "different_steps"}], "paper_keywords": ["concept learning", " example decomposition", " concept decomposition", " probably approximately correct", " computational learning theory", " learning by decomposition"], "paper_abstract": "It is widely accepted in machine learning that it is easier to learn several smaller decomposed concepts than a single large one. Typically, such decomposition of concepts is achieved in highly constrained environments, or aided by human experts. In this article, we investigate concept learning by example decomposition in a general probably approximately correct setting for Boolean learning. We develop sample complexity bounds for the different steps involved in the process. We formally show that if the cost of example partitioning is kept low then it is highly advantageous to learn by example decomposition.", "paper_title": "Concept learning by example decomposition", "paper_id": "WOS:000274673700001"}