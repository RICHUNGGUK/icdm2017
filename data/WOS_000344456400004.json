{"auto_keywords": [{"score": 0.038181865145166416, "phrase": "tesseract"}, {"score": 0.010528312813115679, "phrase": "hypervisor_swap"}, {"score": 0.0047864177420244795, "phrase": "hypervisor_swapping"}, {"score": 0.0045641317216991205, "phrase": "virtual_machines"}, {"score": 0.004076533120753605, "phrase": "expensive_sequence"}, {"score": 0.003933609372601274, "phrase": "hypervisor_swapfile"}, {"score": 0.0035236026196355783, "phrase": "double-paging_problem"}, {"score": 0.0033597650090676032, "phrase": "existing_disk_blocks"}, {"score": 0.0033299015941662302, "phrase": "page_contents"}, {"score": 0.003232254571848865, "phrase": "guest_disks"}, {"score": 0.0030819219279832224, "phrase": "guest_pages"}, {"score": 0.0030183628643645436, "phrase": "disk_blocks"}, {"score": 0.003000444353938951, "phrase": "file_contents"}, {"score": 0.0029649248741435512, "phrase": "common_manner"}, {"score": 0.002801849371896573, "phrase": "guest's_notion"}, {"score": 0.0027852126030055305, "phrase": "disk_layout"}, {"score": 0.002639845784408318, "phrase": "indirection_results"}, {"score": 0.0026241684260044414, "phrase": "significant_performance_loss"}, {"score": 0.002608583927848071, "phrase": "subsequent_guest"}, {"score": 0.0025395882359574005, "phrase": "lost_locality"}, {"score": 0.0024577274522206436, "phrase": "indirected_blocks"}, {"score": 0.0024431289979171505, "phrase": "persistent_storage"}, {"score": 0.0022881443200668886, "phrase": "synthetic_benchmark"}, {"score": 0.002168664249034289, "phrase": "traditional_disks"}, {"score": 0.002130237749180594, "phrase": "ssd."}, {"score": 0.0021049977753042253, "phrase": "worst_case_application_responsiveness"}], "paper_keywords": ["Virtualization", " memory overcommitment", " swapping", " paging", " virtual machines", " hypervisor"], "paper_abstract": "Double-paging is an often-cited, if unsubstantiated, problem in multi-level scheduling of memory between virtual machines (VMs) and the hypervisor. This problem occurs when both a virtualized guest and the hypervisor overcommit their respective physical address-spaces. When the guest pages out memory previously swapped out by the hypervisor, it initiates an expensive sequence of steps causing the contents to be read in from the hypervisor swapfile only to be written out again, significantly lengthening the time to complete the guest I/O request. As a result, performance rapidly drops. We present Tesseract, a system that directly and transparently addresses the double-paging problem. Tesseract tracks when guest and hypervisor I/O operations are redundant and modifies these I/Os to create indirections to existing disk blocks containing the page contents. Although our focus is on reconciling I/Os between the guest disks and hypervisor swap, our technique is general and can reconcile, or deduplicate, I/Os for guest pages read or written by the VM. Deduplication of disk blocks for file contents accessed in a common manner is well-understood. One challenge that our approach faces is that the locality of guest I/Os (reflecting the guest's notion of disk layout) often differs from that of the blocks in the hypervisor swap. This loss of locality through indirection results in significant performance loss on subsequent guest reads. We propose two alternatives to recovering this lost locality, each based on the idea of asynchronously reorganizing the indirected blocks in persistent storage. We evaluate our system and show that it can significantly reduce the costs of double-paging. We focus our experiments on a synthetic benchmark designed to highlight its effects. In our experiments we observe Tesseract can improve our benchmark's throughput by as much as 200% when using traditional disks and by as much as 30% when using SSD. At the same time worst case application responsiveness can be improved by a factor of 5.", "paper_title": "Tesseract: Reconciling Guest I/O and Hypervisor Swapping in a VM", "paper_id": "WOS:000344456400004"}