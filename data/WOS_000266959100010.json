{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "gaussian_processes"}, {"score": 0.004562362497952548, "phrase": "bayesian_methods"}, {"score": 0.004481132747216135, "phrase": "classification_and_regression_problems"}, {"score": 0.004284302478991387, "phrase": "gp_classifier"}, {"score": 0.0035156341294017685, "phrase": "sparse_gp_classifiers"}, {"score": 0.0030171910207671205, "phrase": "validation-based_method"}, {"score": 0.002963390095037466, "phrase": "sparse_gp_classifier_design"}, {"score": 0.0028844769395018595, "phrase": "proposed_method"}, {"score": 0.0028076592725295646, "phrase": "negative_log"}, {"score": 0.0025892330566358503, "phrase": "gp_models"}, {"score": 0.0024093494406195386, "phrase": "basis_vector_selection"}, {"score": 0.002366361492754389, "phrase": "hyperparameter_adaptation"}, {"score": 0.0023033100935570755, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "existing_methods"}], "paper_keywords": [""], "paper_abstract": "Gaussian processes (GPs) are promising Bayesian methods for classification and regression problems. Design of a GP classifier and making predictions using it is, however, computationally demanding, especially when the training set size is large. Sparse GP classifiers are known to overcome this limitation. In this letter, we propose and study a validation-based method for sparse GP classifier design. The proposed method uses a negative log predictive (NLP) loss measure, which is easy to compute for GP models. We use this measure for both basis vector selection and hyperparameter adaptation. The experimental results on several real-world benchmark data sets show better or comparable generalization performance over existing methods.", "paper_title": "Validation-Based Sparse Gaussian Process Classifier Design", "paper_id": "WOS:000266959100010"}