{"auto_keywords": [{"score": 0.037694919794756004, "phrase": "k-"}, {"score": 0.00481495049065317, "phrase": "location-invariant_word_recognition_network"}, {"score": 0.004635881163634672, "phrase": "feedforward_network"}, {"score": 0.004531637625717547, "phrase": "dandurand_et_al"}, {"score": 0.004264923517966271, "phrase": "location-specific_letter_inputs"}, {"score": 0.004200724305692705, "phrase": "location-invariant_word_outputs"}, {"score": 0.004075198669927074, "phrase": "hidden_layer"}, {"score": 0.0038062581748678245, "phrase": "hidden_patterns"}, {"score": 0.0034749794034364197, "phrase": "single_letter_patterns"}, {"score": 0.0032702516771609957, "phrase": "semi-location-invariant_letter_representations"}, {"score": 0.0030775481952376987, "phrase": "superseding_bigram_representations"}, {"score": 0.002940487679224863, "phrase": "linear_regressions"}, {"score": 0.0028525124381208705, "phrase": "word_pattern"}, {"score": 0.0027254466299651936, "phrase": "linear_combination"}, {"score": 0.002564763827975352, "phrase": "overlapping_holographic_representations"}, {"score": 0.002413531395475291, "phrase": "surprisingly_acute_and_useful_correspondence"}, {"score": 0.0022711961447468114, "phrase": "broken_symmetry"}, {"score": 0.0022200068884960836, "phrase": "connection_weight_matrix"}, {"score": 0.002137236968970666, "phrase": "group-invariance_theorem"}], "paper_keywords": [""], "paper_abstract": "We studied the feedforward network proposed by Dandurand et al. (2010), which maps location-specific letter inputs to location-invariant word outputs, probing the hidden layer to determine the nature of the code. Hidden patterns for words were densely distributed, and K-means clustering on single letter patterns produced evidence that the network had formed semi-location-invariant letter representations during training. The possible confound with superseding bigram representations was ruled out, and linear regressions showed that any word pattern was well approximated by a linear combination of its constituent letter patterns. Emulating this code using overlapping holographic representations (Plate, 1995) uncovered a surprisingly acute and useful correspondence with the network, stemming from a broken symmetry in the connection weight matrix and related to the group-invariance theorem (Minsky & Papert, 1969). These results also explain how the network can reproduce relative and transposition priming effects found in humans.", "paper_title": "Broken Symmetries in a Location-Invariant Word Recognition Network", "paper_id": "WOS:000285136900008"}