{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cross-evaluation"}, {"score": 0.004716520738331953, "phrase": "interactive_qa_systems"}, {"score": 0.004224418476742716, "phrase": "rigorous_evaluation"}, {"score": 0.003435426082776884, "phrase": "statistical_techniques"}, {"score": 0.002931569771634119, "phrase": "meaningful_measurements"}, {"score": 0.002793380001054064, "phrase": "user_task_performance"}, {"score": 0.00271735110571002, "phrase": "surprisingly_small_number"}, {"score": 0.0025892330566358503, "phrase": "predetermined_judgments"}, {"score": 0.002224510044064184, "phrase": "end-to-end_qa_systems"}, {"score": 0.0021049977753042253, "phrase": "high_efficiency"}], "paper_keywords": [""], "paper_abstract": "In this article, we report on an experiment to assess the possibility of rigorous evaluation of interactive question-answering (QA) systems using the cross-evaluation method. This method takes into account the effects of tasks and context, and of the users of the systems. Statistical techniques are used to remove these effects, isolating the effect of the system itself. The results show that this approach yields meaningful measurements of the impact of systems on user task performance, using a surprisingly small number of subjects and without relying on predetermined judgments of the quality, or of the relevance of materials. We conclude that the method is indeed effective for comparing end-to-end QA systems, and for comparing interactive systems with high efficiency.", "paper_title": "Using Cross-Evaluation to Evaluate Interactive QA Systems", "paper_id": "WOS:000294255200001"}