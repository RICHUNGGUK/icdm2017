{"auto_keywords": [{"score": 0.02961086686282144, "phrase": "gpu"}, {"score": 0.00481495049065317, "phrase": "based_scene-space_video_processing"}, {"score": 0.004682917599491709, "phrase": "per-pixel_depth_information"}, {"score": 0.004290969772225093, "phrase": "\"scene-space\"_information"}, {"score": 0.004026598712747509, "phrase": "high-quality_scene-space_video_effects"}, {"score": 0.0036747384051186937, "phrase": "key_idea"}, {"score": 0.003573859492987647, "phrase": "high_redundancy"}, {"score": 0.0035455471410184404, "phrase": "approximate_scene_information"}, {"score": 0.0034482025128821548, "phrase": "visible_multiple_times"}, {"score": 0.0032614317990293695, "phrase": "novel_pixel_gathering"}, {"score": 0.0031845067058084583, "phrase": "gathering_step"}, {"score": 0.0031217859267564344, "phrase": "pixel_samples"}, {"score": 0.0030481444896915504, "phrase": "filtering_step"}, {"score": 0.00295264307195965, "phrase": "desired_output_video"}, {"score": 0.002917604283241353, "phrase": "gathered_sample_sets"}, {"score": 0.002726740692380339, "phrase": "full_advantage"}, {"score": 0.002705121052747945, "phrase": "large_volumes"}, {"score": 0.0026836723685046407, "phrase": "video_data"}, {"score": 0.0026518169452004465, "phrase": "practical_runtimes"}, {"score": 0.0026307897899760383, "phrase": "hd_video"}, {"score": 0.002599560451267203, "phrase": "standard_desktop_computer"}, {"score": 0.002488183412451029, "phrase": "video_processing_applications"}, {"score": 0.0021049977753042253, "phrase": "uncontrolled_environments"}], "paper_keywords": ["Video processing", " Sampling", " Inpainting", " Denoising", " Computational Shutters"], "paper_abstract": "Many compelling video processing effects can be achieved if per-pixel depth information and 3D camera calibrations are known. However, the success of such methods is highly dependent on the accuracy of this \"scene-space\" information. We present a novel, sampling-based framework for processing video that enables high-quality scene-space video effects in the presence of inevitable errors in depth and camera pose estimation. Instead of trying to improve the explicit 3D scene representation, the key idea of our method is to exploit the high redundancy of approximate scene information that arises due to most scene points being visible multiple times across many frames of video. Based on this observation, we propose a novel pixel gathering and filtering approach. The gathering step is general and collects pixel samples in scene-space, while the filtering step is application-specific and computes a desired output video from the gathered sample sets. Our approach is easily parallelizable and has been implemented on GPU, allowing us to take full advantage of large volumes of video data and facilitating practical runtimes on HD video using a standard desktop computer. Our generic scene-space formulation is able to comprehensively describe a multitude of video processing applications such as denoising, deblurring, super resolution, object removal, computational shutter functions, and other scene-space camera effects. We present results for various casually captured, hand-held, moving, compressed, monocular videos depicting challenging scenes recorded in uncontrolled environments.", "paper_title": "Sampling Based Scene-Space Video Processing", "paper_id": "WOS:000358786600033"}