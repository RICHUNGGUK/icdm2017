{"auto_keywords": [{"score": 0.03201243105298459, "phrase": "gpu"}, {"score": 0.00457553517865013, "phrase": "power_consumption"}, {"score": 0.004506048850130758, "phrase": "key_issue"}, {"score": 0.0044150318724093226, "phrase": "sustained_performance"}, {"score": 0.004238452771031322, "phrase": "fundamental_tradeoffs"}, {"score": 0.0038467574225322086, "phrase": "important_class"}, {"score": 0.0032172054825137866, "phrase": "general_matrix-matrix_multiplication"}, {"score": 0.0030569913145731408, "phrase": "general-purpose_cpu"}, {"score": 0.00290473242917264, "phrase": "magnitude_improvements"}, {"score": 0.0028170314398963704, "phrase": "relatively_simple_customizations"}, {"score": 0.0027600360731762997, "phrase": "memory_hierarchy_configurations"}, {"score": 0.0025247885558459895, "phrase": "lap"}, {"score": 0.002453934333239467, "phrase": "current_cpus"}, {"score": 0.002367676766637248, "phrase": "linear_algebra_processor"}, {"score": 0.0023079225317714815, "phrase": "double-precision_gemm"}], "paper_keywords": ["Low-power design", " energy-aware systems", " performance analysis and design aids", " matrix multiplication", " memory hierarchy", " level-3 BLAS", " special-purpose hardware"], "paper_abstract": "As technology is reaching physical limits, reducing power consumption is a key issue on our path to sustained performance. In this paper, we study fundamental tradeoffs and limits in efficiency (as measured in energy per operation) that can be achieved for an important class of kernels, namely the level-3 Basic Linear Algebra Subprograms (BLAS). It is well-accepted that specialization is the key to efficiency. This paper establishes a baseline by studying GEneral Matrix-matrix Multiplication (GEMM) on a variety of custom and general-purpose CPU and GPU architectures. Our analysis shows that orders of magnitude improvements in efficiency are possible with relatively simple customizations and fine-tuning of memory hierarchy configurations. We argue that these customizations can be generalized to perform other representative linear algebra operations. In addition to exposing the sources of inefficiencies in current CPUs and GPUs, our results show our prototype Linear Algebra Processor (LAP) implementing Double-precision GEMM (DGEMM) can achieve 600 GFLOPS while consuming less than 25 Watts in standard 45 nm technology, which is up to 50x more energy efficient than cutting-edge CPUs.", "paper_title": "Codesign Tradeoffs for High-Performance, Low-Power Linear Algebra Architectures", "paper_id": "WOS:000310385000006"}