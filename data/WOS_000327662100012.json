{"auto_keywords": [{"score": 0.03580958092226666, "phrase": "gmrace"}, {"score": 0.010612387000973441, "phrase": "gpu_programs"}, {"score": 0.008055005198591151, "phrase": "data_races"}, {"score": 0.007049455972372316, "phrase": "static_analysis"}, {"score": 0.0047464608267006395, "phrase": "low-overhead_scheme"}, {"score": 0.004525088883501587, "phrase": "extremely_cost-effective_means"}, {"score": 0.004334684156070974, "phrase": "cuda"}, {"score": 0.004293445247794074, "phrase": "opencl"}, {"score": 0.004232331639807526, "phrase": "gpu_programming"}, {"score": 0.004192078165260057, "phrase": "nongraphical_applications"}, {"score": 0.00409310463335204, "phrase": "explicitly_parallel_languages"}, {"score": 0.004034841146538891, "phrase": "parallel_programmers"}, {"score": 0.0036845490951857617, "phrase": "multithreaded_environment"}, {"score": 0.003529364798514874, "phrase": "program_reliability"}, {"score": 0.003146652729098066, "phrase": "carefully_designed_dynamic_checker"}, {"score": 0.0029853535424953595, "phrase": "gpus_memory_hierarchy"}, {"score": 0.002942811795757911, "phrase": "runtime_data_accesses"}, {"score": 0.002549269480908849, "phrase": "thread_scheduling"}, {"score": 0.002512926336312949, "phrase": "execution_model"}, {"score": 0.0024771000236931836, "phrase": "underlying_gpus"}, {"score": 0.002372649454733249, "phrase": "false_positives"}, {"score": 0.0022834993174767016, "phrase": "previous_approaches"}, {"score": 0.002166349705365602, "phrase": "evaluated_cases"}, {"score": 0.0021049977753042253, "phrase": "space_overhead"}], "paper_keywords": ["GPU", " CUDA", " data race", " concurrency", " multithreading"], "paper_abstract": "In recent years, GPUs have emerged as an extremely cost-effective means for achieving high performance. While languages like CUDA and OpenCL have eased GPU programming for nongraphical applications, they are still explicitly parallel languages. All parallel programmers, particularly the novices, need tools that can help ensuring the correctness of their programs. Like any multithreaded environment, data races on GPUs can severely affect the program reliability. In this paper, we propose GMRace, a new mechanism for detecting races in GPU programs. GMRace combines static analysis with a carefully designed dynamic checker for logging and analyzing information at runtime. Our design utilizes GPUs memory hierarchy to log runtime data accesses efficiently. To improve the performance, GMRace leverages static analysis to reduce the number of statements that need to be instrumented. Additionally, by exploiting the knowledge of thread scheduling and the execution model in the underlying GPUs, GMRace can accurately detect data races with no false positives reported. Our experimental results show that comparing to previous approaches, GMRace is more effective in detecting races in the evaluated cases, and incurs much less runtime and space overhead.", "paper_title": "GMRace: Detecting Data Races in GPU Programs via a Low-Overhead Scheme", "paper_id": "WOS:000327662100012"}