{"auto_keywords": [{"score": 0.03029870759132602, "phrase": "regularized_regression_model"}, {"score": 0.00481495049065317, "phrase": "feature_selection"}, {"score": 0.004760765880314089, "phrase": "laplacian_regularization"}, {"score": 0.004251210982670317, "phrase": "meaningful_feature_subset"}, {"score": 0.00417961150804265, "phrase": "original_features"}, {"score": 0.0037745459666402915, "phrase": "feature_selection_problem"}, {"score": 0.0037320255482878365, "phrase": "unsupervised_learning_scenarios"}, {"score": 0.0035065523764817143, "phrase": "class_labels"}, {"score": 0.003370190834214415, "phrase": "relevant_information"}, {"score": 0.0032946562900625187, "phrase": "laplacian_regularized_least_squares"}, {"score": 0.0032026066791743866, "phrase": "smooth_function"}, {"score": 0.003148612096891635, "phrase": "data_manifold"}, {"score": 0.003078028357912228, "phrase": "empirical_loss"}, {"score": 0.002908394246754633, "phrase": "expected_prediction_error"}, {"score": 0.0026411384812121503, "phrase": "parameter_covariance_matrix"}, {"score": 0.0025097154697345096, "phrase": "experimental_design"}, {"score": 0.0024534188907003726, "phrase": "trace_and_determinant_operators"}, {"score": 0.0023579143175099324, "phrase": "covariance_matrix"}, {"score": 0.0023313149689465386, "phrase": "efficient_computational_schemes"}, {"score": 0.0022405528759521856, "phrase": "corresponding_optimization_problems"}, {"score": 0.002215274527909304, "phrase": "extensive_experimental_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithms"}], "paper_keywords": ["Feature selection", " dimensionality reduction", " manifold", " regularization", " regression", " clustering"], "paper_abstract": "In many information processing tasks, one is often confronted with very high-dimensional data. Feature selection techniques are designed to find the meaningful feature subset of the original features which can facilitate clustering, classification, and retrieval. In this paper, we consider the feature selection problem in unsupervised learning scenarios, which is particularly difficult due to the absence of class labels that would guide the search for relevant information. Based on Laplacian regularized least squares, which finds a smooth function on the data manifold and minimizes the empirical loss, we propose two novel feature selection algorithms which aim to minimize the expected prediction error of the regularized regression model. Specifically, we select those features such that the size of the parameter covariance matrix of the regularized regression model is minimized. Motivated from experimental design, we use trace and determinant operators to measure the size of the covariance matrix. Efficient computational schemes are also introduced to solve the corresponding optimization problems. Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithms.", "paper_title": "A Variance Minimization Criterion to Feature Selection Using Laplacian Regularization", "paper_id": "WOS:000293969000009"}