{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "hybrid_similarity_measure"}, {"score": 0.037851725528819796, "phrase": "visual_similarity"}, {"score": 0.015571915698046524, "phrase": "image_retrieval"}, {"score": 0.014574931753677809, "phrase": "existing_approaches"}, {"score": 0.011666521018137655, "phrase": "semantic_similarity"}, {"score": 0.004723847090183984, "phrase": "learning_similarity_measure"}, {"score": 0.004678940805063634, "phrase": "relevance_feedback"}, {"score": 0.00459039896577789, "phrase": "promising_way"}, {"score": 0.004503525064934708, "phrase": "image_retrieval_performance"}, {"score": 0.004355415885815204, "phrase": "short-term_learning_experience"}, {"score": 0.004272969980602508, "phrase": "visual_similarity_measure"}, {"score": 0.004212157020917929, "phrase": "single_query_session"}, {"score": 0.004132411680987228, "phrase": "long-term_learning_methodology"}, {"score": 0.004054169947971483, "phrase": "semantic_similarity_measure"}, {"score": 0.00401560412621627, "phrase": "multiple_query_sessions"}, {"score": 0.00386496062825374, "phrase": "big_room"}, {"score": 0.003791763445590174, "phrase": "retrieval_effectiveness"}, {"score": 0.0033806943112103397, "phrase": "novel_hybrid_similarity"}, {"score": 0.0032850648549389025, "phrase": "visual_and_semantic_resemblance"}, {"score": 0.0032074366230426727, "phrase": "long-term_learning_processes"}, {"score": 0.003131637033429892, "phrase": "proposed_scheme"}, {"score": 0.0030285084113651035, "phrase": "users'_query_log"}, {"score": 0.0029147869733574844, "phrase": "prior_knowledge"}, {"score": 0.002791932344920906, "phrase": "labeled_and_unlabeled_images"}, {"score": 0.0027259252868643926, "phrase": "unlabeled_images"}, {"score": 0.002661474607275484, "phrase": "relevant_and_irrelevant_classes"}, {"score": 0.0024417832296664698, "phrase": "visual_and_semantic_similarities"}, {"score": 0.002406968743305622, "phrase": "nonlinear_way"}, {"score": 0.0023840346303791032, "phrase": "image_ranking"}, {"score": 0.002350041573006264, "phrase": "empirical_study"}, {"score": 0.0022188370388602813, "phrase": "proposed_algorithm"}, {"score": 0.0021976915360493628, "phrase": "better_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Image retrieval", " Relevance feedback", " Hybrid similarity measure", " Short-term learning", " Long-term learning"], "paper_abstract": "Learning similarity measure from relevance feedback has become a promising way to enhance the image retrieval performance. Existing approaches mainly focus on taking short-term learning experience to identify a visual similarity measure within a single query session, or applying long-term learning methodology to infer a semantic similarity measure crossing multiple query sessions. However, there is still a big room to elevate the retrieval effectiveness, because little is known in taking the relationship between visual similarity and semantic similarity into account. In this paper, we propose a novel hybrid similarity learning scheme to preserve both visual and semantic resemblance by integrating short-term with long-term learning processes. Concretely, the proposed scheme first learns a semantic similarity from the users' query log, and then, taking this as prior knowledge, learns a visual similarity from a mixture of labeled and unlabeled images. In particular, unlabeled images are exploited for the relevant and irrelevant classes differently and the visual similarity is learned incrementally. Finally, a hybrid similarity measure is produced by fusing the visual and semantic similarities in a nonlinear way for image ranking. An empirical study shows that using hybrid similarity measure for image retrieval is beneficial, and the proposed algorithm achieves better performance than some existing approaches. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Learning a hybrid similarity measure for image retrieval", "paper_id": "WOS:000321232900006"}