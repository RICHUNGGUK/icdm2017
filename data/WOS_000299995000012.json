{"auto_keywords": [{"score": 0.024375780701861423, "phrase": "bfs"}, {"score": 0.01913426818033786, "phrase": "gpgpu"}, {"score": 0.00481495049065317, "phrase": "gpgpu_scheduling_in_control_flow_bound_applications"}, {"score": 0.004554178982565098, "phrase": "mum"}, {"score": 0.004505471345843642, "phrase": "massively_data_parallel_applications"}, {"score": 0.004449688143230257, "phrase": "predictable_memory_access_patterns"}, {"score": 0.004010785080336424, "phrase": "intensive_control_flow"}, {"score": 0.003977594755967813, "phrase": "unpredictable_memory"}, {"score": 0.00396110232316167, "phrase": "access_patterns"}, {"score": 0.003831591603702392, "phrase": "current_hardware"}, {"score": 0.0036756196381448015, "phrase": "low_hardware_utilization"}, {"score": 0.003630073540438812, "phrase": "relatively_low_performance"}, {"score": 0.003540661500026122, "phrase": "root_causes"}, {"score": 0.0035113478590679133, "phrase": "execution_inefficacies"}, {"score": 0.003439117493141663, "phrase": "intensive_cuda_applications"}, {"score": 0.0034106416719768035, "phrase": "nvidia_gpgpu_hardware"}, {"score": 0.0032446544415922615, "phrase": "local_thread_scheduling"}, {"score": 0.0032177835714470027, "phrase": "inherent_limitations"}, {"score": 0.0031254704616199614, "phrase": "high_rate"}, {"score": 0.0030995835539697893, "phrase": "branch_divergence"}, {"score": 0.0029733203800202303, "phrase": "hierarchical_warp_scheduling"}, {"score": 0.002948689937518561, "phrase": "global_warps_reconstruction"}, {"score": 0.00288800021194675, "phrase": "ideal_hierarchical_warp_scheduling_mechanism"}, {"score": 0.0028521858876446654, "phrase": "odgs"}, {"score": 0.002828556054338357, "phrase": "oracle_dynamic_global_scheduling"}, {"score": 0.0027703320456201074, "phrase": "machine_utilization"}, {"score": 0.0027473784878300133, "phrase": "global_warp_reconstruction"}, {"score": 0.002657445352746803, "phrase": "bound_applications"}, {"score": 0.002591927716261631, "phrase": "shared_memory"}, {"score": 0.0025070703395635133, "phrase": "substantial_potential"}, {"score": 0.002486292595645607, "phrase": "performance_improvement"}, {"score": 0.002268769196700254, "phrase": "parallel_graph_algorithms"}, {"score": 0.00224061675049049, "phrase": "significant_branch_divergence"}, {"score": 0.0021225958499898182, "phrase": "performance_gain"}], "paper_keywords": ["Design", " Algorithms", " Performance", " GPGPU", " parallel machines", " scheduling algorithm"], "paper_abstract": "GPGPUs are optimized for graphics, for that reason the hardware is optimized for massively data parallel applications characterized by predictable memory access patterns and little control flow. For such applications' e.g., matrix multiplication, GPGPU based system can achieve very high performance. However, many general purpose data parallel applications are characterized as having intensive control flow and unpredictable memory access patterns. Optimizing the code in such problems for current hardware is often ineffective and even impractical since it exhibits low hardware utilization leading to relatively low performance. This work tracks the root causes of execution inefficacies when running control flow intensive CUDA applications on NVIDIA GPGPU hardware. We show both analytically and by simulations of various benchmarks that local thread scheduling has inherent limitations when dealing with applications that have high rate of branch divergence. To overcome those limitations we propose to use hierarchical warp scheduling and global warps reconstruction. We implement an ideal hierarchical warp scheduling mechanism we term ODGS (Oracle Dynamic Global Scheduling) designed to maximize machine utilization via global warp reconstruction. We show that in control flow bound applications that make no use of shared memory (1) there is still a substantial potential for performance improvement (2) we demonstrate, based on various synthetic and real benchmarks the feasible performance improvement. For example, MUM and BFS are parallel graph algorithms suffering from significant branch divergence. We show that in those algorithms it's possible to achieve performance gain of up to x4.4 and x2.6 relative to previously applied scheduling methods.", "paper_title": "Exploring the Limits of GPGPU Scheduling In Control Flow Bound Applications", "paper_id": "WOS:000299995000012"}