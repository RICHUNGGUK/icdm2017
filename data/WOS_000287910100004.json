{"auto_keywords": [{"score": 0.04239913611090826, "phrase": "neural_networks"}, {"score": 0.00481495049065317, "phrase": "dictionary-based_and_linear_computational_models"}, {"score": 0.004594137808658537, "phrase": "traditional_linear_regression"}, {"score": 0.00432208488976429, "phrase": "linear_combinations"}, {"score": 0.004313869805063343, "phrase": "linear_approximator"}, {"score": 0.004281678802569817, "phrase": "fixed_sets"}, {"score": 0.004162703509434786, "phrase": "orthogonal_polynomials"}, {"score": 0.004123781267600542, "phrase": "hermite_functions"}, {"score": 0.003701401653850141, "phrase": "useful_properties"}, {"score": 0.0036667768092524576, "phrase": "linear_approximators"}, {"score": 0.00348202932445197, "phrase": "best_approximation_operators"}, {"score": 0.0031695693589575916, "phrase": "linear_regression"}, {"score": 0.003139903872290182, "phrase": "experimental_results"}, {"score": 0.002995677265350124, "phrase": "substantially_lower_model_complexity"}, {"score": 0.0028850668183221947, "phrase": "high-dimensional_cases"}, {"score": 0.002818013056816282, "phrase": "theoretical_results"}, {"score": 0.002765490759189057, "phrase": "model_complexity"}, {"score": 0.0025171669791790438, "phrase": "upper_bounds"}, {"score": 0.002493592357827492, "phrase": "worst-case_errors"}, {"score": 0.002470237978694949, "phrase": "variable-basis_approximation"}, {"score": 0.002323663295811215, "phrase": "nonlinear_approximation"}, {"score": 0.002301896758256575, "phrase": "integral_representations"}, {"score": 0.0022696277881969896, "phrase": "computational_units"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Linear approximation schemes", " Variable-basis approximation schemes", " Model complexity", " Worst-case errors", " Neural networks", " Kernel models"], "paper_abstract": "Neural networks provide a more flexible approximation of functions than traditional linear regression. In the latter, one can only adjust the coefficients in linear combinations of fixed sets of functions, such as orthogonal polynomials or Hermite functions, while for neural networks, one may also adjust the parameters of the functions which are being combined. However, some useful properties of linear approximators (such as uniqueness, homogeneity, and continuity of best approximation operators) are not satisfied by neural networks. Moreover, optimization of parameters in neural networks becomes more difficult than in linear regression. Experimental results suggest that these drawbacks of neural networks are offset by substantially lower model complexity, allowing accuracy of approximation even in high-dimensional cases. We give some theoretical results comparing requirements on model complexity for two types of approximators, the traditional linear ones and so called variable-basis types, which include neural networks, radial, and kernel models. We compare upper bounds on worst-case errors in variable-basis approximation with lower bounds on such errors for any linear approximator. Using methods from nonlinear approximation and integral representations tailored to computational units, we describe some cases where neural networks outperform any linear approximator. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Some comparisons of complexity in dictionary-based and linear computational models", "paper_id": "WOS:000287910100004"}