{"auto_keywords": [{"score": 0.04945540937697203, "phrase": "connection_weight_matrices"}, {"score": 0.009623383624798918, "phrase": "global_exponential_stability"}, {"score": 0.008725735873631669, "phrase": "globally_exponentially_stable_drnns"}, {"score": 0.00731417775233968, "phrase": "upper_bounds"}, {"score": 0.00481495049065317, "phrase": "robustness_analysis"}, {"score": 0.004690305164360866, "phrase": "global_exponential_stable_time"}, {"score": 0.004598933498016774, "phrase": "recurrent_neural_networks"}, {"score": 0.004306968571854912, "phrase": "delayed_recurrent_neural_networks"}, {"score": 0.003584025547693836, "phrase": "neural_network"}, {"score": 0.003247868216333148, "phrase": "parameter_uncertainty"}, {"score": 0.003021502415143067, "phrase": "parameter_uncertainty_intensity"}, {"score": 0.0029238768315330305, "phrase": "transcendental_equations"}, {"score": 0.00268452026617243, "phrase": "additive_parameter_uncertainties"}, {"score": 0.0025638362414466278, "phrase": "derived_supper_bounds"}, {"score": 0.0024485643104830814, "phrase": "perturbed_drnns"}, {"score": 0.0022777800735119405, "phrase": "numerical_example"}, {"score": 0.0021896891011033105, "phrase": "theoretical_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Recurrent neural networks", " Global exponential stability", " Delayed", " Robustness"], "paper_abstract": "This paper analyzes the robustness of global exponential stability of delayed recurrent neural networks (DRNNs) subject to parameter uncertainty in connection weight matrices. Given a globally exponentially stable DRNNs, the problem to be addressed herein is how much parameter uncertainty in the connection weight matrices that the neural network can remain to be globally exponentially stable. We characterize the upper bounds of the parameter uncertainty for the DRNNs to sustain global exponential stability. The upper bounds of parameter uncertainty intensity are characterized by using transcendental equations. Moreover, we prove theoretically that, for globally exponentially stable DRNNs, if additive parameter uncertainties in connection weight matrices are smaller than the derived supper bounds arrived at here, then the perturbed DRNNs are guaranteed to also be globally exponentially stable. A numerical example is provided to illustrate the theoretical results. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Robustness analysis for connection weight matrices of global exponential stable time varying delayed recurrent neural networks", "paper_id": "WOS:000319952700021"}