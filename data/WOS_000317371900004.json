{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "performance_analysis"}, {"score": 0.00476697427418509, "phrase": "passive_target_synchronization"}, {"score": 0.0046027811584139315, "phrase": "pgas"}, {"score": 0.004488946121663966, "phrase": "convenient_abstraction"}, {"score": 0.004444203710097837, "phrase": "shared_memory"}, {"score": 0.004269630481719451, "phrase": "multi-threaded_programming"}, {"score": 0.004227064760262924, "phrase": "large-scale_systems"}, {"score": 0.004184921610672245, "phrase": "physically_distributed_memory"}, {"score": 0.004000396172153709, "phrase": "pgas_languages"}, {"score": 0.003940706575253453, "phrase": "appropriate_tool_support"}, {"score": 0.0036007478932719417, "phrase": "performance_problems"}, {"score": 0.00354700018079049, "phrase": "tool_support"}, {"score": 0.0033905067962602515, "phrase": "underlying_one-sided_communication_substrate"}, {"score": 0.003306559295382181, "phrase": "aggregate_remote_memory_copy_interface"}, {"score": 0.0031765314093444956, "phrase": "waiting_time"}, {"score": 0.003113441660343867, "phrase": "asynchronous_data_transfers"}, {"score": 0.003021141590548526, "phrase": "software_intervention"}, {"score": 0.002976019715894587, "phrase": "target_side"}, {"score": 0.0028446460307503343, "phrase": "reduced_operating-system_kernels"}, {"score": 0.0027327327233891865, "phrase": "progress_threads"}, {"score": 0.0024348510123716424, "phrase": "scalasca_trace-analysis_infrastructure"}, {"score": 0.0023390219342052623, "phrase": "progress-related_waiting_times"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Performance analysis", " Event tracing", " One-sided communication", " Remote memory access"], "paper_abstract": "Partitioned global address space (PGAS) languages combine the convenient abstraction of shared memory with the notion of affinity, extending multi-threaded programming to large-scale systems with physically distributed memory. However, in spite of their obvious advantages, PGAS languages still lack appropriate tool support for performance analysis, one of the reasons why their adoption is still in its infancy. Some of the performance problems for which tool support is needed occur at the level of the underlying one-sided communication substrate, such as the Aggregate Remote Memory Copy Interface (ARMCI). One such example is the waiting time in situations where asynchronous data transfers cannot be completed without software intervention at the target side. This is not uncommon on systems with reduced operating-system kernels such as IBM Blue Gene/P where the use of progress threads would double the number of cores necessary to run an application. In this paper, we present an extension of the Scalasca trace-analysis infrastructure aimed at the identification and quantification of progress-related waiting times at larger scales. We demonstrate its utility and scalability using a benchmark running with up to 32,768 processes. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "A scalable infrastructure for the performance analysis of passive target synchronization", "paper_id": "WOS:000317371900004"}