{"auto_keywords": [{"score": 0.049465088868450455, "phrase": "stochastic_factorization"}, {"score": 0.03463681921366539, "phrase": "pisf"}, {"score": 0.009256579162951117, "phrase": "proposed_algorithm"}, {"score": 0.00481495049065317, "phrase": "policy_iteration"}, {"score": 0.00467207889925365, "phrase": "transition_probability_matrix"}, {"score": 0.004286708921931472, "phrase": "transition_matrix"}, {"score": 0.004213519983188574, "phrase": "fundamental_characteristics"}, {"score": 0.004088421020408183, "phrase": "derived_matrix"}, {"score": 0.003816197777388722, "phrase": "compact_version"}, {"score": 0.003767203575060969, "phrase": "markov_decision_process"}, {"score": 0.003608370966473902, "phrase": "computational_cost"}, {"score": 0.003577414339646767, "phrase": "dynamic_programming"}, {"score": 0.0034118234596993836, "phrase": "approximate_policy_iteration_algorithm"}, {"score": 0.0031845067058084583, "phrase": "computational_complexity"}, {"score": 0.003130077012135034, "phrase": "standard_policy_iteration's_cubic_dependence"}, {"score": 0.0030502112070879815, "phrase": "mdp"}, {"score": 0.0027861630854233693, "phrase": "nice_theoretical_properties"}, {"score": 0.002703327151461985, "phrase": "finite_number"}, {"score": 0.0026342827320931937, "phrase": "decision_policy"}, {"score": 0.0024586427394152196, "phrase": "approximation_error"}, {"score": 0.0023447155524245655, "phrase": "optimal_value_function"}, {"score": 0.0023145733687225845, "phrase": "mdp."}, {"score": 0.0022651818270925704, "phrase": "practical_ways"}, {"score": 0.0021049977753042253, "phrase": "large-scale_decision_problem"}], "paper_keywords": [""], "paper_abstract": "When a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix that retains some fundamental characteristics of the original. Since the derived matrix can be much smaller than its precursor, this property can be exploited to create a compact version of a Markov decision process (MDP), and hence to reduce the computational cost of dynamic programming. Building on this idea, this paper presents an approximate policy iteration algorithm called policy iteration based on stochastic factorization, or PISF for short. In terms of computational complexity, PISF replaces standard policy iteration's cubic dependence on the size of the MDP with a function that grows only linearly with the number of states in the model. The proposed algorithm also enjoys nice theoretical properties: it always terminates after a finite number of iterations and returns a decision policy whose performance only depends on the quality of the stochastic factorization. In particular, if the approximation error in the factorization is sufficiently small, PISF computes the optimal value function of the MDP. The paper also discusses practical ways of factoring an MDP and illustrates the usefulness of the proposed algorithm with an application involving a large-scale decision problem of real economical interest.", "paper_title": "Policy Iteration Based on Stochastic Factorization", "paper_id": "WOS:000341179100001"}