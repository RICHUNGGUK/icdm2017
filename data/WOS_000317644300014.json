{"auto_keywords": [{"score": 0.0319925883999622, "phrase": "sail"}, {"score": 0.028568036093229787, "phrase": "clustering_quality"}, {"score": 0.00481495049065317, "phrase": "information-theoretic_text_clustering"}, {"score": 0.004773692073488661, "phrase": "information-theoretic_clustering"}, {"score": 0.004692227830462433, "phrase": "information-theoretic_measures"}, {"score": 0.004632039024076267, "phrase": "clustering_criteria"}, {"score": 0.0045726187330434025, "phrase": "common_practice"}, {"score": 0.004456044928556639, "phrase": "so-called_info-kmeans"}, {"score": 0.004380081030297162, "phrase": "k-means"}, {"score": 0.004268293864953256, "phrase": "proximity_function"}, {"score": 0.004213519983188569, "phrase": "expert_efforts"}, {"score": 0.004123781267600542, "phrase": "promising_results"}, {"score": 0.0040708542958968605, "phrase": "remaining_challenge"}, {"score": 0.0039841418890102925, "phrase": "high-dimensional_sparse_data"}, {"score": 0.0037028514035425037, "phrase": "high-dimensional_text_vectors"}, {"score": 0.003623949225126944, "phrase": "infinite_kl-divergence_values"}, {"score": 0.003441352119167824, "phrase": "iteration_process"}, {"score": 0.0031845180757169517, "phrase": "info"}, {"score": 0.0030501665085457606, "phrase": "equivalent_objective_function"}, {"score": 0.0029214769181390653, "phrase": "incremental_computation"}, {"score": 0.0028963961037320805, "phrase": "shannon_entropy"}, {"score": 0.002822434555587263, "phrase": "zero-feature_dilemma"}, {"score": 0.002622947550220266, "phrase": "variable_neighborhood_search_scheme"}, {"score": 0.0025780912201045555, "phrase": "v-sail_algorithm"}, {"score": 0.0024906610581090223, "phrase": "multithreaded_scheme"}, {"score": 0.0023145690608400425, "phrase": "clustering_performance"}, {"score": 0.002294686545883412, "phrase": "info-kmeans"}, {"score": 0.0021049977753042253, "phrase": "lower_cost"}], "paper_keywords": ["Information-theoretic clustering", " KL-divergence", " K-means distance", " multithreaded parallel computing", " variable neighborhood search (VNS)"], "paper_abstract": "Information-theoretic clustering aims to exploit information-theoretic measures as the clustering criteria. A common practice on this topic is the so-called Info-Kmeans, which performs K-means clustering with KL-divergence as the proximity function. While expert efforts on Info-Kmeans have shown promising results, a remaining challenge is to deal with high-dimensional sparse data such as text corpora. Indeed, it is possible that the centroids contain many zero-value features for high-dimensional text vectors, which leads to infinite KL-divergence values and creates a dilemma in assigning objects to centroids during the iteration process of Info-Kmeans. To meet this challenge, in this paper, we propose a Summation-bAsed Incremental Learning (SAIL) algorithm for Info-Kmeans clustering. Specifically, by using an equivalent objective function, SAIL replaces the computation of KL-divergence by the incremental computation of Shannon entropy. This can avoid the zero-feature dilemma caused by the use of KL-divergence. To improve the clustering quality, we further introduce the variable neighborhood search scheme and propose the V-SAIL algorithm, which is then accelerated by a multithreaded scheme in PV-SAIL. Our experimental results on various real-world text collections have shown that, with SAIL as a booster, the clustering performance of Info-Kmeans can be significantly improved. Also, V-SAIL and PV-SAIL indeed help improve the clustering quality at a lower cost of computation.", "paper_title": "SAIL: Summation-bAsed Incremental Learning for Information-Theoretic Text Clustering", "paper_id": "WOS:000317644300014"}