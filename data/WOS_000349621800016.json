{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multicategory_crowdsourcing_accounting_for_variable_task_difficulty"}, {"score": 0.004770211811866158, "phrase": "worker_skill"}, {"score": 0.004595356579614969, "phrase": "instant_recruitment"}, {"score": 0.00436530211426514, "phrase": "document_databases"}, {"score": 0.00428451995524549, "phrase": "worker_unreliability"}, {"score": 0.004205226397573506, "phrase": "worker's_responses"}, {"score": 0.004166129325245761, "phrase": "\"face_value"}, {"score": 0.004032118497047498, "phrase": "multiple_workers"}, {"score": 0.003884212965525665, "phrase": "ground-truth_answers"}, {"score": 0.0037768417338850274, "phrase": "crowd_aggregation"}, {"score": 0.003741712498615481, "phrase": "multicategory_answer_spaces"}, {"score": 0.0034884152484429207, "phrase": "answer_generation"}, {"score": 0.0033919477901198716, "phrase": "worker_skills"}, {"score": 0.003313592421224647, "phrase": "task_difficulties"}, {"score": 0.0032522091171602557, "phrase": "broad_range"}, {"score": 0.0032219438233946312, "phrase": "worker_types"}, {"score": 0.0031182092708983184, "phrase": "average_aggregate_confidence"}, {"score": 0.002839812512030847, "phrase": "individual_workers"}, {"score": 0.0027612315954598085, "phrase": "improved_crowd_aggregation"}, {"score": 0.0026722904884984348, "phrase": "unsupervised_and_semi-supervised_settings"}, {"score": 0.0024912073108138613, "phrase": "multiple_domains"}, {"score": 0.002456486927713543, "phrase": "task-dependent_answer_spaces"}, {"score": 0.00237733844863506, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "large_crowd"}], "paper_keywords": ["Crowdsourcing", " expectation-maximization", " ensemble classification", " inference", " multicategory"], "paper_abstract": "Crowdsourcing allows instant recruitment of workers on the web to annotate image, webpage, or document databases. However, worker unreliability prevents taking a worker's responses at \"face value\". Thus, responses from multiple workers are typically aggregated to more reliably infer ground-truth answers. We study two approaches for crowd aggregation on multicategory answer spaces: stochastic modeling-based and deterministic objective function-based. Our stochastic model for answer generation plausibly captures the interplay between worker skills, intentions, and task difficulties and captures a broad range of worker types. Our deterministic objective-based approach aims to maximize the average aggregate confidence of weighted plurality crowd decision making. In both approaches, we explicitly model the skill and intention of individual workers, which is exploited for improved crowd aggregation. Our methods are applicable in both unsupervised and semi-supervised settings, and also when the batch of tasks is heterogeneous, i.e., from multiple domains, with task-dependent answer spaces. As observed experimentally, the proposed methods can defeat \"tyranny of the masses\", i.e., they are especially advantageous when there is an (a priori unknown) minority of skilled workers amongst a large crowd of unskilled (and malicious) workers.", "paper_title": "Multicategory Crowdsourcing Accounting for Variable Task Difficulty, Worker Skill, and Worker Intention", "paper_id": "WOS:000349621800016"}