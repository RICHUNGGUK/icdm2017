{"auto_keywords": [{"score": 0.035851235908081446, "phrase": "collective_operations"}, {"score": 0.00481495049065317, "phrase": "numa_clusters"}, {"score": 0.00475413908468027, "phrase": "increasing_number"}, {"score": 0.004615202066192795, "phrase": "manycore-based_systems"}, {"score": 0.004461359547152617, "phrase": "multiple_levels"}, {"score": 0.0043863636475983845, "phrase": "non_uniform_memory_access"}, {"score": 0.004349346119448046, "phrase": "numa"}, {"score": 0.004276216827737399, "phrase": "processor_cores"}, {"score": 0.004186534737631288, "phrase": "complex_interconnects"}, {"score": 0.004081385289516914, "phrase": "increasing_amount"}, {"score": 0.003978866233891427, "phrase": "processing_elements"}, {"score": 0.0038953958723049287, "phrase": "efficient_and_scalable_provision"}, {"score": 0.00378145978325955, "phrase": "collective_communication_operations"}, {"score": 0.0034886660821793576, "phrase": "unnecessary_synchronization"}, {"score": 0.003301461490509176, "phrase": "point_functions"}, {"score": 0.0031375577552605533, "phrase": "good_performance"}, {"score": 0.002994449398035894, "phrase": "hierarchical_trees"}, {"score": 0.0029691377785333872, "phrase": "overlapping_one-sided_communications"}, {"score": 0.0028944757265206332, "phrase": "available_numa_binding_features"}, {"score": 0.0027978304423349246, "phrase": "unified_parallel_c"}, {"score": 0.002658863179011018, "phrase": "shared_memory_view"}, {"score": 0.0025591748412278174, "phrase": "private_memory_regions"}, {"score": 0.002505412457777352, "phrase": "performance_evaluation"}, {"score": 0.0024736973328071026, "phrase": "proposed_implementation"}, {"score": 0.0024320326084093465, "phrase": "five_representative_systems"}, {"score": 0.0023708445073819277, "phrase": "finis_terrae"}, {"score": 0.002350794145263235, "phrase": "svg"}, {"score": 0.0023309080951251335, "phrase": "superdome"}, {"score": 0.002281930100792185, "phrase": "generally_good_performance"}, {"score": 0.002224586888628006, "phrase": "mpi"}, {"score": 0.0021229583685841405, "phrase": "developed_algorithms"}, {"score": 0.0021049977753042253, "phrase": "manycore_architectures"}], "paper_keywords": ["Manycore architectures", " Collective operations", " NUMA", " UPC", " PGAS", " MPI", " High performance computing", " Communication algorithms"], "paper_abstract": "The increasing number of cores per processor is turning manycore-based systems in pervasive. This involves dealing with multiple levels of memory in non uniform memory access (NUMA) systems and processor cores hierarchies, accessible via complex interconnects in order to dispatch the increasing amount of data required by the processing elements. The key for efficient and scalable provision of data is the use of collective communication operations that minimize the impact of bottlenecks. Leveraging one sided communications becomes more important in these systems, to avoid unnecessary synchronization between pairs of processes in collective operations implemented in terms of two sided point to point functions. This work proposes a series of algorithms that provide a good performance and scalability in collective operations, based on the use of hierarchical trees, overlapping one-sided communications, message pipelining and the available NUMA binding features. An implementation has been developed for Unified Parallel C, a Partitioned Global Address Space language, which presents a shared memory view across the nodes for programmability, while keeping private memory regions for performance. The performance evaluation of the proposed implementation, conducted on five representative systems (JuRoPA, JUDGE, Finis Terrae, SVG and Superdome), has shown generally good performance and scalability, even outperforming MPI in some cases, which confirms the suitability of the developed algorithms for manycore architectures.", "paper_title": "Scalable PGAS collective operations in NUMA clusters", "paper_id": "WOS:000345077400027"}