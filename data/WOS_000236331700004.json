{"auto_keywords": [{"score": 0.023933953433183882, "phrase": "pac"}, {"score": 0.00481495049065317, "phrase": "classical_approach"}, {"score": 0.004763565662615503, "phrase": "multi-class_pattern_classification"}, {"score": 0.0045879792359519375, "phrase": "probability_distributions"}, {"score": 0.00441883624011048, "phrase": "label_class"}, {"score": 0.004301834610818624, "phrase": "new_instances"}, {"score": 0.004210458358142504, "phrase": "bayes_classifier"}, {"score": 0.004143196873246147, "phrase": "estimated_distributions"}, {"score": 0.0038226070564362697, "phrase": "conditional_distribution"}, {"score": 0.003781772207256374, "phrase": "class_labels"}, {"score": 0.0036422425344358037, "phrase": "class_overlap"}, {"score": 0.0033965836912510385, "phrase": "accurate_classifiers"}, {"score": 0.003150466780962537, "phrase": "distinct_labels"}, {"score": 0.002859977408815691, "phrase": "pac_learnability"}, {"score": 0.0027543636132088332, "phrase": "pac_learning_framework"}, {"score": 0.0025546627686955656, "phrase": "standard_pac_learning"}, {"score": 0.002447101794397243, "phrase": "interesting_algorithms"}, {"score": 0.0021975548867457623, "phrase": "alternative_slightly_milder_restriction"}, {"score": 0.0021049977753042253, "phrase": "unrestricted_pac_learning"}], "paper_keywords": ["computational learning theory", " computational complexity", " pattern classification"], "paper_abstract": "A classical approach in multi-class pattern classification is the following. Estimate the probability distributions that generated the observations for each label class, and then label new instances by applying the Bayes classifier to the estimated distributions. That approach provides more useful information than just a class label; it also provides estimates of the conditional distribution of class labels, in situations where there is class overlap. We would like to know whether it is harder to build accurate classifiers via this approach, than by techniques that may process all data with distinct labels together. In this paper we make that question precise by considering it in the context of PAC learnability. We propose two restrictions on the PAC learning framework that are intended to correspond with the above approach, and consider their relationship with standard PAC learning. Our main restriction of interest leads to some interesting algorithms that show that the restriction is not stronger (more restrictive) than various other well-known restrictions on PAC learning. An alternative slightly milder restriction turns out to be almost equivalent to unrestricted PAC learning.", "paper_title": "Some discriminant-based PAC algorithms", "paper_id": "WOS:000236331700004"}