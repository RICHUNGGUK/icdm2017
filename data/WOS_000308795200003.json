{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high-dimensional_gaussian_graphical_model_selection"}, {"score": 0.004396600680348434, "phrase": "efficient_estimation_algorithm"}, {"score": 0.004014452814186653, "phrase": "empirical_conditional_covariances"}, {"score": 0.003820144626025562, "phrase": "transparent_conditions"}, {"score": 0.0036958398682001015, "phrase": "structural_consistency"}, {"score": 0.0034879261743468574, "phrase": "proposed_algorithm"}, {"score": 0.0025892330566358503, "phrase": "sufficient_conditions"}, {"score": 0.0022492129111567802, "phrase": "sparse_local_vertex_separators"}, {"score": 0.0021940118003312397, "phrase": "underlying_graph"}, {"score": 0.0021049977753042253, "phrase": "novel_non-asymptotic_necessary_conditions"}], "paper_keywords": ["Gaussian graphical model selection", " high-dimensional learning", " local-separation property", " walk-summability", " necessary conditions for model selection"], "paper_abstract": "We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n = Omega (J(min)(-2) log p), where p is the number of variables and J(min) is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency.", "paper_title": "High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion", "paper_id": "WOS:000308795200003"}