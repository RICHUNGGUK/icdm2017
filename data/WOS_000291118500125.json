{"auto_keywords": [{"score": 0.042105172167053985, "phrase": "prototype_reduction"}, {"score": 0.00481495049065317, "phrase": "different_approaches"}, {"score": 0.004771593452798216, "phrase": "main_two_drawbacks"}, {"score": 0.004742904841489477, "phrase": "nearest_neighbor_based_classifiers"}, {"score": 0.004700193486903964, "phrase": "high_cpu_costs"}, {"score": 0.004588158931776543, "phrase": "training_set"}, {"score": 0.004332616870895206, "phrase": "pattern_recognition_field"}, {"score": 0.004267757384195032, "phrase": "adequate_subset"}, {"score": 0.003945754887845885, "phrase": "good_set"}, {"score": 0.0035824802350703376, "phrase": "considered_approaches"}, {"score": 0.003539479990299223, "phrase": "different_nearest_neighbor_based_classifiers"}, {"score": 0.0030897670291134645, "phrase": "different_reduced_set"}, {"score": 0.0029887957779347394, "phrase": "supervised_optimization_function"}, {"score": 0.0028736995088566083, "phrase": "training_phase"}, {"score": 0.0027298319171606498, "phrase": "test_pattern"}, {"score": 0.0026485829604700833, "phrase": "\"vote_rule"}, {"score": 0.002616762777413686, "phrase": "reported_results"}, {"score": 0.0025312097004307013, "phrase": "well_known_bagging_approach"}, {"score": 0.0024117251593529883, "phrase": "\"supervised\"_bagging_ensemble"}, {"score": 0.002202651051682622, "phrase": "best_method"}, {"score": 0.002176176622950153, "phrase": "different_classifiers"}, {"score": 0.002143529653780496, "phrase": "genetic_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Prototype reduction", " Nearest neighbor based classifiers", " Learning prototypes and distances", " Particle swarm optimization", " Genetic algorithm"], "paper_abstract": "The main two drawbacks of nearest neighbor based classifiers are: high CPU costs when the number of samples in the training set is high and performance extremely sensitive to outliers. Several attempts of overcoming such drawbacks have been proposed in the pattern recognition field aimed at selecting/generating an adequate subset of prototypes from the training set. The problem addressed in this paper concerns the comparison of methods for prototype reduction; several methods for finding a good set of prototypes are evaluated: particle swarm optimization; clustering algorithm; genetic algorithm; learning prototypes and distances. Experiments are carried out on several classification problems in order to evaluate the considered approaches in conjunction with different nearest neighbor based classifiers: 1-nearest-neighbor classifier, 5-nearest-neighbor classifier, nearest feature plane based classifier, nearest feature line based classifier. Moreover, we propose a method for creating an ensemble of the classifiers, where each classifier is trained with a different reduced set of prototypes. Since these prototypes are generated using a supervised optimization function, we have called our ensemble: \"supervised bagging\". The training phase consists in repeating N times the prototype generation, then the scores resulting from classifying a test pattern using each set of prototypes are combined by the \"vote rule\". The reported results show the superiority of this method with respect to the well known bagging approach for building ensembles of classifiers. Our results are obtained when 1-nearest-neighbor classifier is coupled with a \"supervised\" bagging ensemble of learning prototypes and distances. As expected, the approaches for prototype reduction proposed for 1-nearest-neighbor classifier do not work so well when other classifiers are tested. In our experiments the best method for prototype reduction when different classifiers are used is the genetic algorithm. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Prototype reduction techniques: A comparison among different approaches", "paper_id": "WOS:000291118500125"}