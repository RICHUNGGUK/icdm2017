{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "efficient_parallelization"}, {"score": 0.04470537402089973, "phrase": "memory_management"}, {"score": 0.030881013294335926, "phrase": "deep_simulations"}, {"score": 0.004778258911940497, "phrase": "amr_mhd_multiphysics_calculations"}, {"score": 0.004511799033574768, "phrase": "compute_engines"}, {"score": 0.004443276312972159, "phrase": "amr_simulations"}, {"score": 0.004342430078306461, "phrase": "new_levels"}, {"score": 0.004163427731140163, "phrase": "new_techniques"}, {"score": 0.003976709319372198, "phrase": "amr"}, {"score": 0.003931118518507637, "phrase": "ghost_cells"}, {"score": 0.0038713805573219297, "phrase": "hyperbolic_advances"}, {"score": 0.0035997484400118965, "phrase": "astrobear"}, {"score": 0.0033599501775954024, "phrase": "finer_level_grids"}, {"score": 0.003296212583539166, "phrase": "global_load"}, {"score": 0.003221316290858497, "phrase": "level_load_balancing"}, {"score": 0.0031723302551782324, "phrase": "greater_parallelization"}, {"score": 0.0031360786664165093, "phrase": "physical_space"}, {"score": 0.0031121406603482112, "phrase": "amr_level"}, {"score": 0.0030648097361278856, "phrase": "level_advances"}, {"score": 0.002599177223533834, "phrase": "larger_scale_simulations"}, {"score": 0.0025110337774260773, "phrase": "distributed_tree_algorithm"}, {"score": 0.002435190484324197, "phrase": "local_sections"}, {"score": 0.0024073421672242486, "phrase": "amr_tree_structure"}, {"score": 0.0023889533502525527, "phrase": "neighboring_processors"}, {"score": 0.0023525950486853937, "phrase": "distributed_approach"}, {"score": 0.0022990901047984197, "phrase": "reasonable_scaling_efficiency"}, {"score": 0.0021956951486095805, "phrase": "amr_-_independent"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Adaptive mesh refinement", " Parallel", " High performance computing", " Distributed tree", " Threading"], "paper_abstract": "Current adaptive mesh refinement (AMR) simulations require algorithms that are highly parallelized and manage memory efficiently. As compute engines grow larger, AMR simulations will require algorithms that achieve new levels of efficient parallelization and memory management. We have attempted to employ new techniques to achieve both of these goals. Patch or grid based AMR often employs ghost cells to decouple the hyperbolic advances of each grid on a given refinement level. This decoupling allows each grid to be advanced independently. In AstroBEAR we utilize this independence by threading the grid advances on each level with preference going to the finer level grids. This allows for global load balancing instead of level by level load balancing and allows for greater parallelization across both physical space and AMR level. Threading of level advances can also improve performance by interleaving communication with computation, especially in deep simulations with many levels of refinement. While we see improvements of up to 30% on deep simulations run on a few cores, the speedup is typically more modest (5-20%) for larger scale simulations. To improve memory management we have employed a distributed tree algorithm that requires processors to only store and communicate local sections of the AMR tree structure with neighboring processors. Using this distributed approach we are able to get reasonable scaling efficiency (>80%) out to 12288 cores and up to 8 levels of AMR - independent of the use of threading. (c) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Efficient parallelization for AMR MHD multiphysics calculations; implementation in AstroBEAR", "paper_id": "WOS:000314801500028"}