{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "unreliable_computers"}, {"score": 0.004751216062917875, "phrase": "computing_systems"}, {"score": 0.004626255242038005, "phrase": "million_core_architectures"}, {"score": 0.004327981760540394, "phrase": "power_consumption"}, {"score": 0.0038900709793274484, "phrase": "million_cores"}, {"score": 0.0036715973701254823, "phrase": "operational_availability"}, {"score": 0.003465350958971198, "phrase": "traditional_techniques"}, {"score": 0.0032561349010603734, "phrase": "central_overseer"}, {"score": 0.0031703706626348507, "phrase": "final_judgement"}, {"score": 0.0031422859551159506, "phrase": "system_availability"}, {"score": 0.0030868583963299698, "phrase": "single_point"}, {"score": 0.003032405563826263, "phrase": "large_systems"}, {"score": 0.002824005136794786, "phrase": "isolated_points"}, {"score": 0.0027495914857929584, "phrase": "gross_behaviour"}, {"score": 0.002629889135655667, "phrase": "meaningful_way"}, {"score": 0.002482013330276732, "phrase": "neural_network_architecture"}, {"score": 0.0024490834773668153, "phrase": "million-core_machine"}, {"score": 0.002427372728532664, "phrase": "layered_fault-tolerance"}, {"score": 0.0022404162042797262, "phrase": "canonical_distributed_heat_diffusion_equation"}, {"score": 0.0021049977753042253, "phrase": "partial_system_failure"}], "paper_keywords": [""], "paper_abstract": "As computing systems continue their unquenchable rise towards and through million core architectures, two considerations that used to be unimportant become more and more dominant: power consumption (be it FLOPS/W or W/mm2) and reliability. This study is concerned with the latter: in a system of a million cores, it is unrealistic to expect 100% functionality on power-up; equally, operational availability degrades with time. Monitoring and maintaining the health of such a system using traditional techniques is costly, and most rely on the concept of some sort of central overseer or monitor to make a final judgement about system availability, giving a single point of failure. Large systems of the future will consist of hardware and software that work synergistically to cope with isolated points of failure, allowing the gross behaviour of the system to degrade gracefully and in a meaningful way in the face of faults. This study describes one such system: spiking neural network architecture is a million-core machine with layered fault-tolerance built in at many levels. The authors show how the system may be used to solve the canonical distributed heat diffusion equation, and how the quality of solution is modulated by the effects of partial system failure.", "paper_title": "Reliable computation with unreliable computers", "paper_id": "WOS:000356419900009"}