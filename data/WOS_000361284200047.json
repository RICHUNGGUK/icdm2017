{"auto_keywords": [{"score": 0.049338666620453345, "phrase": "dynamic_environments"}, {"score": 0.04201419729599904, "phrase": "hardware_resources"}, {"score": 0.00481495049065317, "phrase": "experts_approach"}, {"score": 0.004777569299319633, "phrase": "runtime_mapping"}, {"score": 0.004685375117376838, "phrase": "program_parallelism"}, {"score": 0.004648995193787914, "phrase": "platform_parallelism"}, {"score": 0.004612896435506914, "phrase": "thread_selection"}, {"score": 0.004488734023485018, "phrase": "available_resources"}, {"score": 0.0044192837023190445, "phrase": "existing_compiler_or_runtime_approaches"}, {"score": 0.004040211152784802, "phrase": "new_external_workloads"}, {"score": 0.003885668358145471, "phrase": "best_number"}, {"score": 0.0038106195546165574, "phrase": "parallel_application"}, {"score": 0.0037079710789139305, "phrase": "new_scheme"}, {"score": 0.0034564928013639125, "phrase": "existing_policies"}, {"score": 0.003324203391715804, "phrase": "particular_environment"}, {"score": 0.0031351720217878917, "phrase": "novel_environment_predictor"}, {"score": 0.0030269369940823902, "phrase": "expert_thread_selection_policy"}, {"score": 0.00300339527261653, "phrase": "additional_expert_policies"}, {"score": 0.0027995677499976406, "phrase": "varying_external_workloads"}, {"score": 0.002671406231520296, "phrase": "affinity_scheduling"}, {"score": 0.002519402443228795, "phrase": "existing_schemes"}, {"score": 0.0024803459795996116, "phrase": "workload_performance"}, {"score": 0.0023946630984825207, "phrase": "openmp_default"}, {"score": 0.00234834406550549, "phrase": "online_scheme"}, {"score": 0.0023029188952824685, "phrase": "offline_policy"}, {"score": 0.0022583704173318123, "phrase": "state-of-art_analytic_model"}, {"score": 0.002154930852763048, "phrase": "open_problem"}], "paper_keywords": ["Parallelism Mapping", " Dynamic Environment", " Machine Learning", " Mixture of Experts"], "paper_abstract": "Matching program parallelism to platform parallelism using thread selection is difficult when the environment and available resources dynamically change. Existing compiler or runtime approaches are typically based on a one-size fits all policy. There is little ability to either evaluate or adapt the policy when encountering new external workloads or hardware resources. This paper focuses on selecting the best number of threads for a parallel application in dynamic environments. It develops a new scheme based on a mixture of experts approach. It learns online which, of a number of existing policies, or experts, is best suited for a particular environment without having to try out each policy. It does this by using a novel environment predictor as a proxy for the quality of an expert thread selection policy. Additional expert policies can easily be added and are selected only when appropriate. We evaluate our scheme in environments with varying external workloads and hardware resources. We then consider the case when workloads use affinity scheduling or are themselves adaptive and show that our approach, in all cases, outperforms existing schemes and surprisingly improves workload performance. On average, we improve 1.66x over OpenMP default, 1.34x over an online scheme, 1.25x over an offline policy and 1.2x over a state-of-art analytic model. Determining the right number and type of experts is an open problem and our initial analysis shows that adding more experts improves accuracy and performance.", "paper_title": "Celebrating Diversity: A Mixture of Experts Approach for Runtime Mapping in Dynamic Environments", "paper_id": "WOS:000361284200047"}