{"auto_keywords": [{"score": 0.0401960790312752, "phrase": "scene_tokens"}, {"score": 0.03947447987613366, "phrase": "kflann"}, {"score": 0.029063297113478832, "phrase": "memory_units"}, {"score": 0.00481495049065317, "phrase": "visual_place_recognition"}, {"score": 0.004778923541463994, "phrase": "mobile_robotic_navigation"}, {"score": 0.0045855181084455444, "phrase": "mobile_robotic_localization"}, {"score": 0.004551199992546761, "phrase": "visual_indoor_image_sequences"}, {"score": 0.00450020161335541, "phrase": "biologically_inspired_spatio-temporal_neural_network_approach"}, {"score": 0.0039308449157320815, "phrase": "scene_quantization_module"}, {"score": 0.003785836059995376, "phrase": "k-iteration_fast_learning_artificial_neural_network"}, {"score": 0.003673675864705682, "phrase": "core_unit"}, {"score": 0.0036324746826127997, "phrase": "quantization_module"}, {"score": 0.003591733916159209, "phrase": "kflann_network"}, {"score": 0.0035381202759091784, "phrase": "intrinsic_statistics"}, {"score": 0.0034984340449789745, "phrase": "data_stream"}, {"score": 0.0032694619052789768, "phrase": "kflann_performance"}, {"score": 0.0032085525502105836, "phrase": "data_presentation"}, {"score": 0.0031606404966938568, "phrase": "popular_clustering_methods"}, {"score": 0.003043957650600757, "phrase": "consistent_number"}, {"score": 0.003021141590548526, "phrase": "stable_centroids"}, {"score": 0.002953711377604328, "phrase": "topological_structure"}, {"score": 0.002708843102301451, "phrase": "ltm_architecture"}, {"score": 0.0025989960809479104, "phrase": "visual_input_stream"}, {"score": 0.0025124343366192954, "phrase": "spatio-temporal_learning"}, {"score": 0.002493592357827492, "phrase": "namely_error_tolerance"}, {"score": 0.002437907960209364, "phrase": "primary_objective"}, {"score": 0.0023040686862868583, "phrase": "ltm"}, {"score": 0.0022610992177831643, "phrase": "visual_topological_localization_problem"}, {"score": 0.0021775526153883355, "phrase": "proposed_framework"}, {"score": 0.0021530945641993152, "phrase": "challenging_cosy_localization_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Visual place recognition", " Spatio-temporal neural networks", " Long-term Memory", " Topological robotic mapping"], "paper_abstract": "This paper proposes a solution to the problem of mobile robotic localization using visual indoor image sequences with a biologically inspired spatio-temporal neural network approach. The system contains three major subsystems: a feature extraction module, a scene quantization module and a spatio-temporal long-term memory (LTM) module. During learning, the scene quantization module clusters the visual images set into scene tokens. A K-Iteration Fast Learning Artificial Neural Network (KFLANN) is employed as the core unit of the quantization module. The KFLANN network is driven by intrinsic statistics of the data stream and therefore does not require the number of clusters to be predefined. In addition, the KFLANN performance is less sensitive to data presentation ordering compared to popular clustering methods such as k-means, and can therefore produce a consistent number of stable centroids. Using scene tokens, the topological structure of the environment can be composed into sequences of tokens. These sequences are then learnt and stored in memory units in an LTM architecture, which is able to continuously and robustly recognize the visual input stream. The design of memory units addresses two critical problems in spatio-temporal learning, namely error tolerance and memory forgetting. The primary objective of this work is to explore the synergy between the strength of KFLANN and LTM models to address the visual topological localization problem. We demonstrate the efficiency and efficacy of the proposed framework on the challenging COsy Localization Dataset. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "A spatio-temporal Long-term Memory approach for visual place recognition in mobile robotic navigation", "paper_id": "WOS:000328178200038"}