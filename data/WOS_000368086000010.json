{"auto_keywords": [{"score": 0.04762987004578423, "phrase": "scientific_workflow"}, {"score": 0.012314085283447428, "phrase": "data-intensive_scientific_workflow"}, {"score": 0.006142831452813064, "phrase": "workflow_tasks"}, {"score": 0.005803886781631125, "phrase": "multi-dc_environment"}, {"score": 0.005580189695658785, "phrase": "overall_data_transfer_time"}, {"score": 0.00481495049065317, "phrase": "optimized_scheduling"}, {"score": 0.004608997772926119, "phrase": "big_data_era"}, {"score": 0.004470071518923524, "phrase": "data_intensity"}, {"score": 0.004354315061804687, "phrase": "scientific_domains"}, {"score": 0.004316396683332752, "phrase": "efficient_scheduling"}, {"score": 0.004149792080414689, "phrase": "long-standing_challenge"}, {"score": 0.00407781620412982, "phrase": "previous_work"}, {"score": 0.004042295865725559, "phrase": "data-intensive_scientific_workflow_scheduling"}, {"score": 0.003852371777780562, "phrase": "data_transfer"}, {"score": 0.0037198532511087566, "phrase": "novel_scheduling_strategies"}, {"score": 0.003423042581201533, "phrase": "novel_dc_selection_approach"}, {"score": 0.0031498398446368025, "phrase": "optimized_inter-dc_network_bandwidth"}, {"score": 0.0029886047149545025, "phrase": "clustering-based_data_placement_strategy"}, {"score": 0.002898378776839434, "phrase": "initial_data"}, {"score": 0.0027985846558073457, "phrase": "initial_data_transfer"}, {"score": 0.0027741761880779535, "phrase": "different_dcs"}, {"score": 0.002737960968372391, "phrase": "multilevel_task_replication_scheduling_strategy"}, {"score": 0.0026436755508085223, "phrase": "intermediate_data_transfer"}, {"score": 0.002497319603143104, "phrase": "broad_range"}, {"score": 0.002453934333239467, "phrase": "multi-dc_settings"}, {"score": 0.0023694065215394593, "phrase": "proposed_approaches"}, {"score": 0.0023384629381463054, "phrase": "numerical_results"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["scientific workflow", " scheduling", " multiple datacenter"], "paper_abstract": "In the big data era, scientific workflow exhibits the characteristics of data intensity and becomes increasingly popular in scientific domains. Efficient scheduling of data-intensive scientific workflow in a multiple datacenter (DC) environment has been a long-standing challenge. Most of previous work on data-intensive scientific workflow scheduling primarily focused on the optimization of reducing the volumes of data transfer between workflow tasks. In this paper, novel scheduling strategies for the execution of data-intensive scientific workflow in multi-DC environment are proposed aiming at the optimization of the overall data transfer time. A novel DC selection approach is proposed to minimize the number of DCs having enough storage capacity for the execution of scientific workflow as well as optimized inter-DC network bandwidth for efficient data transfer between workflow tasks. A k-means clustering-based data placement strategy is adopted to intelligently place the initial data of scientific workflow thereby reducing the volume of initial data transfer between different DCs. A multilevel task replication scheduling strategy is invented to reduce the volumes of intermediate data transfer between DCs during the runtime of the scientific workflow. Simulations spanning a broad range of scientific workflow and multi-DC settings are performed in order to verify the proposed approaches. The numerical results show that our combined scheduling strategy significantly reduces the overall data transfer time and data transfer volume when scientific workflow is scheduled in multi-DC environment. Copyright (C) 2015 John Wiley & Sons, Ltd.", "paper_title": "Towards optimized scheduling for data-intensive scientific workflow in multiple datacenter environment", "paper_id": "WOS:000368086000010"}