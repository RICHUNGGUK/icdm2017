{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "latent_dirichlet_allocation"}, {"score": 0.015512225806948927, "phrase": "document_categorization"}, {"score": 0.010470313620074442, "phrase": "sl-lda"}, {"score": 0.004563028873351776, "phrase": "modeling_approaches"}, {"score": 0.004471978831302331, "phrase": "considerable_attention"}, {"score": 0.0038833278847237858, "phrase": "pre-assigned_labels"}, {"score": 0.0037297918570822876, "phrase": "potentially_lost_labels"}, {"score": 0.003679969180334367, "phrase": "common_semantics"}, {"score": 0.0027370835853738626, "phrase": "label_sampling"}, {"score": 0.002474209809122618, "phrase": "gibbs_expectation-maximization_algorithm"}, {"score": 0.0024084679931209514, "phrase": "sl-lda_model"}, {"score": 0.0023762535034673017, "phrase": "quantitative_experimental_results"}, {"score": 0.0021049977753042253, "phrase": "multi-label_corpora"}], "paper_keywords": ["Supervised", " Topic modeling", " Latent Dirichlet allocation", " Multi-label classification"], "paper_abstract": "Recently, supervised topic modeling approaches have received considerable attention. However, the representative labeled latent Dirichlet allocation (L-LDA) method has a tendency to over-focus on the pre-assigned labels, and does not give potentially lost labels and common semantics sufficient consideration. To overcome these problems, we propose an extension of L-LDA, namely supervised labeled latent Dirichlet allocation (SL-LDA), for document categorization. Our model makes two fundamental assumptions, i.e., Prior 1 and Prior 2, that relax the restriction of label sampling and extend the concept of topics. In this paper, we develop a Gibbs expectation-maximization algorithm to learn the SL-LDA model. Quantitative experimental results demonstrate that SL-LDA is competitive with state-of-the-art approaches on both single-label and multi-label corpora.", "paper_title": "Supervised labeled latent Dirichlet allocation for document categorization", "paper_id": "WOS:000351110600013"}