{"auto_keywords": [{"score": 0.0398577732669491, "phrase": "throughput_metrics"}, {"score": 0.034466371082712124, "phrase": "different_assumptions"}, {"score": 0.00481495049065317, "phrase": "multiprogram_throughput_metrics"}, {"score": 0.004676345706416371, "phrase": "multiple_programs"}, {"score": 0.004374267836149339, "phrase": "meaningful_throughput_metrics"}, {"score": 0.004319830179921363, "phrase": "simulation_environment"}, {"score": 0.004057596233938867, "phrase": "ongoing_debate"}, {"score": 0.003924284136338428, "phrase": "multiprogram_workloads"}, {"score": 0.0037480753312398754, "phrase": "systematic_way"}, {"score": 0.0036248955128033084, "phrase": "job_size"}, {"score": 0.0035947360742084253, "phrase": "job_distribution"}, {"score": 0.0034332736827447654, "phrase": "theoretical_throughput_experiment"}, {"score": 0.0033905067962602515, "phrase": "throughput_metric"}, {"score": 0.003334309053314909, "phrase": "average_throughput"}, {"score": 0.0032246835734680377, "phrase": "different_metrics"}, {"score": 0.0029909851863915283, "phrase": "specific_metric"}, {"score": 0.0028926147550022607, "phrase": "explicit_assumptions"}, {"score": 0.0028091907694771613, "phrase": "better_understanding"}, {"score": 0.002627407106087633, "phrase": "multiple_metrics"}, {"score": 0.0024573577356299765, "phrase": "commonly_used_weighted_speedup"}, {"score": 0.002337066031028783, "phrase": "actual_throughput_metrics"}, {"score": 0.00223196703759329, "phrase": "new_throughput_metrics"}, {"score": 0.0021405207258001118, "phrase": "closed_formula"}, {"score": 0.0021049977753042253, "phrase": "real_experimental_data"}], "paper_keywords": ["Performance", " Measurement", " Experimentation", " Multi-program workloads", " throughput metrics", " multicore", " simultaneous multithreading"], "paper_abstract": "Running multiple programs on a processor aims at increasing the throughput of that processor. However, defining meaningful throughput metrics in a simulation environment is not as straightforward as reporting execution time. This has led to an ongoing debate on what forms a meaningful throughput metric for multiprogram workloads. We present a method to construct throughput metrics in a systematic way: we start by expressing assumptions on job size, job distribution, scheduling, and so forth that together define a theoretical throughput experiment. The throughput metric is then the average throughput of this experiment. Different assumptions lead to different metrics, so one should be aware of these assumptions when making conclusions based on results using a specific metric. Throughput metrics should always be defined from explicit assumptions, because this leads to a better understanding of the implications and limits of the results obtained with that metric. We elaborate multiple metrics based on different assumptions. In particular, we identify the assumptions that lead to the commonly used weighted speedup and harmonic mean of speedups. Our study clarifies that they are actual throughput metrics, which was recently questioned. We also propose some new throughput metrics, which cannot always be expressed as a closed formula. We use real experimental data to characterize metrics and show how they relate to each other.", "paper_title": "Multiprogram Throughput Metrics: A Systematic Approach", "paper_id": "WOS:000344830900012"}