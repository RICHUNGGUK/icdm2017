{"auto_keywords": [{"score": 0.03878676535760206, "phrase": "show-through_techniques"}, {"score": 0.03386841066632519, "phrase": "proxemic_behavior"}, {"score": 0.030580277430729972, "phrase": "social_protocols"}, {"score": 0.00481495049065317, "phrase": "referential_awareness"}, {"score": 0.004781357546476652, "phrase": "collaborative_virtual_reality"}, {"score": 0.004748015486240577, "phrase": "multi-user"}, {"score": 0.0047314049780079765, "phrase": "virtual_reality_systems"}, {"score": 0.004698392162272106, "phrase": "natural_collaboration"}, {"score": 0.004665608609734034, "phrase": "shared_virtual_worlds"}, {"score": 0.00444241196374042, "phrase": "virtual_scenery"}, {"score": 0.004127394241838696, "phrase": "whereon_objects"}, {"score": 0.003861596041315646, "phrase": "viewing_position"}, {"score": 0.0038346288310750804, "phrase": "specialized_individual_views"}, {"score": 0.00379452936798156, "phrase": "shared_virtual_scene"}, {"score": 0.0033564619316337634, "phrase": "spatial_understanding"}, {"score": 0.003263631515158206, "phrase": "mutual_information_exchange"}, {"score": 0.003096417317829492, "phrase": "user_study"}, {"score": 0.003064013732721966, "phrase": "co-located_stereoscopic_multi-user_setup"}, {"score": 0.0029274600162609654, "phrase": "user_acceptance"}, {"score": 0.002866498582226632, "phrase": "spatial_understanding_and_mutual_information_exchange"}, {"score": 0.002662932198158107, "phrase": "distributed_virtual_environments"}, {"score": 0.0026166287606163145, "phrase": "distributed_setup"}, {"score": 0.0025264171945423254, "phrase": "minimalist_avatar_representation"}, {"score": 0.002422249273477746, "phrase": "mutual_awareness"}, {"score": 0.0023551955270737215, "phrase": "other's_pointing_gestures"}, {"score": 0.0021725557397878565, "phrase": "collaborative_interaction_tasks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Evaluation/methodology", " Interaction techniques", " 3D pointing", " Collaborative virtual reality"], "paper_abstract": "Multi-user virtual reality systems enable natural collaboration in shared virtual worlds. Users can talk to each other, gesture and point into the virtual scenery as if it were real. As in reality, referring to objects by pointing results often in a situation whereon objects are occluded from the other users' viewpoints. While in reality this problem can only be solved by adapting the viewing position, specialized individual views of the shared virtual scene enable various other solutions. As one such solution we propose show-through techniques to make sure that the objects one is pointing to can always be seen by others. We first study the impact of such augmented viewing techniques on the spatial understanding of the scene, the rapidity of mutual information exchange as well as the proxemic behavior of users. To this end we conducted a user study in a co-located stereoscopic multi-user setup. Our study revealed advantages for show-through techniques in terms of comfort, user acceptance and compliance to social protocols while spatial understanding and mutual information exchange is retained. Motivated by these results we further analyze whether show-through techniques may also be beneficial in distributed virtual environments. We investigated a distributed setup for two users, each participant having its own display screen and a minimalist avatar representation for each participant. In such a configuration there is a lack of mutual awareness, which hinders the understanding of each other's pointing gestures and decreases the relevance of social protocols in terms of proxemic behavior. Nevertheless, we found that show-through techniques can improve collaborative interaction tasks even in such situations. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "See-through techniques for referential awareness in collaborative virtual reality", "paper_id": "WOS:000291455100004"}