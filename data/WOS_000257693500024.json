{"auto_keywords": [{"score": 0.03617360814426437, "phrase": "new_extension"}, {"score": 0.019056963867325853, "phrase": "state-action_pairs"}, {"score": 0.014537808721995088, "phrase": "xcsf"}, {"score": 0.00481495049065317, "phrase": "environmental_reinforcement_signal"}, {"score": 0.004773904938276846, "phrase": "non-linear_polynomials"}, {"score": 0.00473320761888448, "phrase": "learning_classifier_systems"}, {"score": 0.004672808182229058, "phrase": "classifier_systems"}, {"score": 0.004632973742977787, "phrase": "lcss"}, {"score": 0.004573842508130535, "phrase": "evolutionary_learning_mechanisms"}, {"score": 0.004515467468684217, "phrase": "genetic_algorithms"}, {"score": 0.00440093312826378, "phrase": "power_of_the_reinforcement_learning_paradigm"}, {"score": 0.004289291421348352, "phrase": "state-action-reward_mappings"}, {"score": 0.004144810537437608, "phrase": "environmental_state"}, {"score": 0.004074397977869244, "phrase": "achieved_reward"}, {"score": 0.004005176784823524, "phrase": "early_versions"}, {"score": 0.003837205620832367, "phrase": "constant_real-valued_reward"}, {"score": 0.0037238193456845124, "phrase": "fairly_complex_environment"}, {"score": 0.0036137713873827374, "phrase": "identical_state-action_pairs"}, {"score": 0.00350696416714684, "phrase": "environmental_reward"}, {"score": 0.003388745581055085, "phrase": "well-known_lcs"}, {"score": 0.003288567185099276, "phrase": "classifier_system"}, {"score": 0.0032604932428246637, "phrase": "xcs"}, {"score": 0.0030705342386533083, "phrase": "linear_reward_function"}, {"score": 0.0028916124463906983, "phrase": "original_xcs."}, {"score": 0.0027114323601885666, "phrase": "proper_mappings"}, {"score": 0.0026538960171433985, "phrase": "particular_intervals"}, {"score": 0.0024252458095689847, "phrase": "non-linear_reward_function"}, {"score": 0.002216251512062482, "phrase": "produced_population"}, {"score": 0.0021599099979586946, "phrase": "final_approximation"}, {"score": 0.0021322775033565805, "phrase": "desired_range"}, {"score": 0.0021049977753042253, "phrase": "input_parameters"}], "paper_keywords": ["evolutionary function approximation", " learning classifier systems", " XCS", " XCSF"], "paper_abstract": "Learning classifier systems (LCSS) are evolutionary learning mechanisms that combine genetic algorithms (GAs) with the power of the reinforcement learning paradigm. LCSs try to evolve state-action-reward mappings to propose the best action for each environmental state and maximize the achieved reward. In the early versions of LCSs, state-action pairs had been mapped to a constant real-valued reward. Thus, to model a fairly complex environment, LCSs had to develop some identical state-action pairs to show different levels of the environmental reward. Recently, a new extension to a well-known LCS, called the accuracy based learning classifier system or XCS, has been developed, which is able to map state-action pairs to a linear reward function. This new extension, called XCSF, can develop a more compact population than the original XCS. However, further research studies have shown that this new extension is not able to develop the proper mappings when its input parameters come from particular intervals. In this paper, we propose a new extension to XCSF that is able to map the state-action pairs to a non-linear reward function. The experimental results show that this extension can outperform other extensions to XCSF with respect to the compactness of the produced population and the accuracy of the final approximation for any desired range of the input parameters.", "paper_title": "Approximating the environmental reinforcement signal with non-linear polynomials using learning classifier systems", "paper_id": "WOS:000257693500024"}