{"auto_keywords": [{"score": 0.04500352715676157, "phrase": "hpc"}, {"score": 0.03327530097944352, "phrase": "tuccompi"}, {"score": 0.00481495049065317, "phrase": "distributed_heterogeneous_computing"}, {"score": 0.004761494044422926, "phrase": "tuning_capabilities"}, {"score": 0.004630423009638876, "phrase": "parallel_processing_architectures"}, {"score": 0.004528156929805637, "phrase": "powerful_tool"}, {"score": 0.004428139418750855, "phrase": "massively-parallel_problems"}, {"score": 0.004354572391793908, "phrase": "high_performance_computing"}, {"score": 0.004211069233133584, "phrase": "last_trend"}, {"score": 0.004049590638590415, "phrase": "heterogeneous_environments"}, {"score": 0.003960101604693806, "phrase": "different_computational_processing_devices"}, {"score": 0.003829548069657859, "phrase": "graphics_processing_units"}, {"score": 0.0036214184763804034, "phrase": "gpu_parallel_implementation"}, {"score": 0.003501991125315889, "phrase": "in-depth_knowledge"}, {"score": 0.0034437574674180365, "phrase": "gpu_underlying_architecture"}, {"score": 0.003367611042980526, "phrase": "tedious_manual_effort"}, {"score": 0.003293142761521422, "phrase": "experienced_programmers"}, {"score": 0.0029282926103224717, "phrase": "heterogeneous_systems"}, {"score": 0.002895721231882758, "phrase": "hardware_accelerators"}, {"score": 0.0026478260192958924, "phrase": "optimal_values"}, {"score": 0.002574788013427524, "phrase": "kernel_characterization"}, {"score": 0.0021049977753042253, "phrase": "all-pair_shortest-path_problem"}], "paper_keywords": ["Abstract parallel model", " Auto-tuning", " CUDA", " GPU", " Heterogeneous system", " HPC framework", " MPI", " OpenMP"], "paper_abstract": "During the last decade, parallel processing architectures have become a powerful tool to deal with massively-parallel problems that require high performance computing (HPC). The last trend of HPC is the use of heterogeneous environments, that combine different computational processing devices, such as CPU-cores and graphics processing units (GPUs). Maximizing the performance of any GPU parallel implementation of an algorithm requires an in-depth knowledge about the GPU underlying architecture, becoming a tedious manual effort only suited for experienced programmers. In this paper, we present TuCCompi, a multi-layer abstract model that simplifies the programming on heterogeneous systems including hardware accelerators, by hiding the details of synchronization, deployment, and tuning. TuCCompi chooses optimal values for their configuration parameters using a kernel characterization provided by the programmer. This model is very useful to tackle problems characterized by independent, high computational-load independent tasks, such as embarrassingly-parallel problems. We have evaluated TuCCompi in different, real-world, heterogeneous environments using the all-pair shortest-path problem as a case study.", "paper_title": "TuCCompi: A Multi-layer Model for Distributed Heterogeneous Computing with Tuning Capabilities", "paper_id": "WOS:000358648600012"}