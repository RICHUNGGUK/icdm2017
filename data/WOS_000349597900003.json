{"auto_keywords": [{"score": 0.0500767939470904, "phrase": "distributed_wireless_networks"}, {"score": 0.041846704768780815, "phrase": "rl"}, {"score": 0.007901970373712179, "phrase": "rl-based_routing_schemes"}, {"score": 0.004641005983799053, "phrase": "node_mobility"}, {"score": 0.004610058444728656, "phrase": "dynamic_network_topology"}, {"score": 0.004503350877279651, "phrase": "major_challenge"}, {"score": 0.004384407114103125, "phrase": "traditional_routing_schemes"}, {"score": 0.004297256607382827, "phrase": "wireless_node"}, {"score": 0.004211831095629, "phrase": "predefined_set"}, {"score": 0.004019024264498125, "phrase": "reinforcement_learning"}, {"score": 0.0038867116254051363, "phrase": "routing_challenge"}, {"score": 0.0038478705530517296, "phrase": "wireless_nodes"}, {"score": 0.00368397861425962, "phrase": "efficient_routing_decisions"}, {"score": 0.003331828027460222, "phrase": "wireless_networks"}, {"score": 0.0032985137109323827, "phrase": "routing_challenges"}, {"score": 0.003265531406540679, "phrase": "different_types"}, {"score": 0.003013237619840709, "phrase": "rl_models"}, {"score": 0.0029041917003677234, "phrase": "network_performance"}, {"score": 0.002752950443338712, "phrase": "markov"}, {"score": 0.0026887369417065957, "phrase": "extensive_review"}, {"score": 0.0026707727531746447, "phrase": "new_features"}, {"score": 0.0026529282690379128, "phrase": "rl-based_routing"}, {"score": 0.0025568888889527792, "phrase": "rl."}, {"score": 0.002514387630447547, "phrase": "real_hardware_implementation"}, {"score": 0.0024892263318773704, "phrase": "rl-based_routing_scheme"}, {"score": 0.0024396547357855777, "phrase": "performance_enhancements"}, {"score": 0.0022585847551229274, "phrase": "new_research_directions"}, {"score": 0.0021622381640810442, "phrase": "tutorial_manner"}], "paper_keywords": ["Q-routing", " Routing", " Wireless networks", " Q-learning", " Reinforcement learning"], "paper_abstract": "The dynamicity of distributed wireless networks caused by node mobility, dynamic network topology, and others has been a major challenge to routing in such networks. In the traditional routing schemes, routing decisions of a wireless node may solely depend on a predefined set of routing policies, which may only be suitable for a certain network circumstances. Reinforcement Learning (RL) has been shown to address this routing challenge by enabling wireless nodes to observe and gather information from their dynamic local operating environment, learn, and make efficient routing decisions on the fly. In this article, we focus on the application of the traditional, as well as the enhanced, RL models, to routing in wireless networks. The routing challenges associated with different types of distributed wireless networks, and the advantages brought about by the application of RL to routing are identified. In general, three types of RL models have been applied to routing schemes in order to improve network performance, namely Q-routing, multi-agent reinforcement learning, and partially observable Markov decision process. We provide an extensive review on new features in RL-based routing, and how various routing challenges and problems have been approached using RL. We also present a real hardware implementation of a RL-based routing scheme. Subsequently, we present performance enhancements achieved by the RL-based routing schemes. Finally, we discuss various open issues related to RL-based routing schemes in distributed wireless networks, which help to explore new research directions in this area. Discussions in this article are presented in a tutorial manner in order to establish a foundation for further research in this field.", "paper_title": "Application of reinforcement learning to routing in distributed wireless networks: a review", "paper_id": "WOS:000349597900003"}