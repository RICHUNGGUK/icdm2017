{"auto_keywords": [{"score": 0.03218738484270805, "phrase": "dnb_algorithm"}, {"score": 0.020136431429012017, "phrase": "distributed_applications"}, {"score": 0.005333709895000044, "phrase": "network_topology"}, {"score": 0.004814966938368356, "phrase": "collaborative"}, {"score": 0.0046432244185370605, "phrase": "human_society"}, {"score": 0.0042209023469249205, "phrase": "distributed_learning"}, {"score": 0.003942877273973905, "phrase": "local_model"}, {"score": 0.003683097597914604, "phrase": "low_communication_cost"}, {"score": 0.003378385493203297, "phrase": "new_distributed_learning_method"}, {"score": 0.0032135959043774085, "phrase": "learned_hypotheses"}, {"score": 0.0031556799865964974, "phrase": "neighboring_sites"}, {"score": 0.0031271133933467575, "phrase": "learning_process"}, {"score": 0.0030988045934942587, "phrase": "theoretical_analysis"}, {"score": 0.0030017207229983385, "phrase": "cost_function"}, {"score": 0.00297454367746853, "phrase": "collaborative_functional_gradient_descent"}, {"score": 0.0028682668647074397, "phrase": "upper_bounds"}, {"score": 0.0028422946420821075, "phrase": "training_error"}, {"score": 0.002816556934231128, "phrase": "generalization_error"}, {"score": 0.0026069623057065664, "phrase": "different_sizes"}, {"score": 0.002536763634539386, "phrase": "proposed_algorithm"}, {"score": 0.0022847009758393405, "phrase": "random_graphs"}, {"score": 0.0022640009579466924, "phrase": "scale-free_networks"}, {"score": 0.0022434880661725493, "phrase": "bias-variance_decomposition"}, {"score": 0.0021731423856364003, "phrase": "important_role"}, {"score": 0.0021049977753042253, "phrase": "learned_classifier_ensemble"}], "paper_keywords": ["Ensemble learning", " distributed network boosting", " collaborative functional gradient descent", " bias-variance decomposition"], "paper_abstract": "In human society, people learn from each other and knowledge is accumulated from generation to generation. This provides some hints to distributed learning. For distributed applications, each site has its own data. If we can build a local model for each site and improve the model based on models learned by its neighbor sites with low communication cost, then it would be very helpful to the distributed applications. In this paper, we propose a new distributed learning method called distributed network boosting (DNB) algorithm for distributed applications. The learned hypotheses are exchanged between neighboring sites during learning process. Theoretical analysis shows that the DNB algorithm minimizes the cost function through collaborative functional gradient descent in hypotheses space. We also give upper bounds of training error and generalization error of the DNB algorithm. Comparison results of the DNB algorithm with other algorithms on real data sets with different sizes show the effectiveness of the proposed algorithm for distributed applications. In order to show the influence of network topology on the performance of the DNB algorithm, we tested it on random graphs and scale-free networks. Bias-variance decomposition shows that the network topology plays an important role in controlling the diversity of the learned classifier ensemble.", "paper_title": "COLLABORATIVE LEARNING BY BOOSTING IN DISTRIBUTED ENVIRONMENTS", "paper_id": "WOS:000281194300005"}