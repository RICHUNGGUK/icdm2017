{"auto_keywords": [{"score": 0.04333983479125857, "phrase": "direct_physical_interaction"}, {"score": 0.00481495049065317, "phrase": "interaction_modalities"}, {"score": 0.0047804843375940835, "phrase": "mobile_indoor_robot_guidance"}, {"score": 0.004449045430179066, "phrase": "indoor_environment"}, {"score": 0.004322967486451877, "phrase": "quantitative_metrics"}, {"score": 0.0042004472903194616, "phrase": "first_interaction_modality"}, {"score": 0.004096082254543022, "phrase": "human_user"}, {"score": 0.003923144245133288, "phrase": "second_and_third_interaction_modalities"}, {"score": 0.003611751858608048, "phrase": "desired_location"}, {"score": 0.0035601676907962626, "phrase": "first_task"}, {"score": 0.0034220655476652683, "phrase": "different_rooms"}, {"score": 0.003385336776758738, "phrase": "simulated_physical_apartment"}, {"score": 0.003361069456060196, "phrase": "rough_movement"}, {"score": 0.0033011569787033297, "phrase": "designated_areas"}, {"score": 0.00326572165784246, "phrase": "second_task"}, {"score": 0.0032423089921154503, "phrase": "robot_guidance"}, {"score": 0.003105305189036267, "phrase": "accurate_movements"}, {"score": 0.0030171910207671205, "phrase": "generic_differential_drive_mobile_platform"}, {"score": 0.0029740732099993706, "phrase": "pan-tilt_system"}, {"score": 0.002942138511829811, "phrase": "kinect_camera"}, {"score": 0.0029210388852074208, "phrase": "task_completion_time"}, {"score": 0.0028177825595998023, "phrase": "users'_performance"}, {"score": 0.0027775064989253575, "phrase": "nasa-tlx_questionnaire"}, {"score": 0.0027181663098561066, "phrase": "users'_workload"}, {"score": 0.0026220625042810706, "phrase": "interaction_modality"}, {"score": 0.0026032523182547003, "phrase": "significant_effect"}, {"score": 0.002584576723303367, "phrase": "completion_time"}, {"score": 0.0021049977753042253, "phrase": "person-following_interaction_modality"}], "paper_keywords": ["Direct physical interaction (DPI)", " gesture recognition", " human-robot interaction (HRI)", " mobile robots", " person following", " person tracking"], "paper_abstract": "Three advanced natural interaction modalities for mobile robot guidance in an indoor environment were developed and compared using two tasks and quantitative metrics to measure performance and workload. The first interaction modality is based on direct physical interaction requiring the human user to push the robot in order to displace it. The second and third interaction modalities exploit a 3-D vision-based human-skeleton tracking allowing the user to guide the robot by either walking in front of it or by pointing toward a desired location. In the first task, the participants were asked to guide the robot between different rooms in a simulated physical apartment requiring rough movement of the robot through designated areas. The second task evaluated robot guidance in the same environment through a set of waypoints, which required accurate movements. The three interaction modalities were implemented on a generic differential drive mobile platform equipped with a pan-tilt system and a Kinect camera. Task completion time and accuracy were used as metrics to assess the users' performance, while the NASA-TLX questionnaire was used to evaluate the users' workload. A study with 24 participants indicated that choice of interaction modality had significant effect on completion time (F (2, 61) = 84.874, p < 0.001), accuracy (F (2, 29) = 4.937, p = 0.016), and workload (F (2, 68) = 11.948, p < 0.001). The direct physical interaction required less time, provided more accuracy and less workload than the two contactless interaction modalities. Between the two contactless interaction modalities, the person-following interaction modality was systematically better than the pointing-control one: The participants completed the tasks faster with less workload.", "paper_title": "Comparison of Interaction Modalities for Mobile Indoor Robot Guidance: Direct Physical Interaction, Person Following, and Pointing Control", "paper_id": "WOS:000366893700001"}