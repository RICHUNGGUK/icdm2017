{"auto_keywords": [{"score": 0.04158809801348612, "phrase": "remo"}, {"score": 0.00481495049065317, "phrase": "increasing_popularity"}, {"score": 0.004779876533667754, "phrase": "large-scale_distributed_applications"}, {"score": 0.004659108142668152, "phrase": "growing_demand"}, {"score": 0.004625164125506718, "phrase": "distributed_application_state_monitoring"}, {"score": 0.004574709094672837, "phrase": "application_state_monitoring_tasks"}, {"score": 0.004426607895888934, "phrase": "large_number"}, {"score": 0.004221072004655573, "phrase": "monitoring_overlay"}, {"score": 0.0041294419262414995, "phrase": "cost_effectiveness"}, {"score": 0.0037273513742154237, "phrase": "overlay_construction"}, {"score": 0.003646399625591736, "phrase": "existing_works"}, {"score": 0.003528244907944199, "phrase": "intertask_cost-sharing_opportunities"}, {"score": 0.0035025108234594668, "phrase": "node-level_resource_constraints"}, {"score": 0.003401431460601017, "phrase": "per-message_processing_overhead"}, {"score": 0.003279160988534828, "phrase": "previous_works"}, {"score": 0.0031728680535262083, "phrase": "optimized_monitoring_trees"}, {"score": 0.003081272473508291, "phrase": "cost-sharing_opportunities"}, {"score": 0.002959617637433, "phrase": "resource-sensitive_construction_schemes"}, {"score": 0.0028741605025505435, "phrase": "adaptive_algorithm"}, {"score": 0.0027911639820564897, "phrase": "overlay_adaptation"}, {"score": 0.002720505061804933, "phrase": "large_systems"}, {"score": 0.002680932410366204, "phrase": "monitoring_tasks"}, {"score": 0.0024915167458844914, "phrase": "extension_techniques"}, {"score": 0.002455266773537278, "phrase": "extensive_experiments"}, {"score": 0.002384345584772295, "phrase": "ibm"}, {"score": 0.002323953608863511, "phrase": "system-system_s."}, {"score": 0.0021282722932911427, "phrase": "collected_attributes"}, {"score": 0.0021049977753042253, "phrase": "existing_schemes"}], "paper_keywords": ["Resource-aware", " state monitoring", " distributed monitoring", " datacenter monitoring", " adaptation", " data-intensive"], "paper_abstract": "The increasing popularity of large-scale distributed applications in datacenters has led to the growing demand of distributed application state monitoring. These application state monitoring tasks often involve collecting values of various status attributes from a large number of nodes. One challenge in such large-scale application state monitoring is to organize nodes into a monitoring overlay that achieves monitoring scalability and cost effectiveness at the same time. In this paper, we present REMO, a REsource-aware application state MOnitoring system, to address the challenge of monitoring overlay construction. REMO distinguishes itself from existing works in several key aspects. First, it jointly considers intertask cost-sharing opportunities and node-level resource constraints. Furthermore, it explicitly models the per-message processing overhead which can be substantial but is often ignored by previous works. Second, REMO produces a forest of optimized monitoring trees through iterations of two phases. One phase explores cost-sharing opportunities between tasks, and the other refines the tree with resource-sensitive construction schemes. Finally, REMO also employs an adaptive algorithm that balances the benefits and costs of overlay adaptation. This is particularly useful for large systems with constantly changing monitoring tasks. Moreover, we enhance REMO in terms of both performance and applicability with a series of optimization and extension techniques. We perform extensive experiments including deploying REMO on a BlueGene/P rack running IBM's large-scale distributed streaming system-System S. Using REMO in the context of collecting over 200 monitoring tasks for an application deployed across 200 nodes results in a 35-45 percent decrease in the percentage error of collected attributes compared to existing schemes.", "paper_title": "Resource-Aware Application State Monitoring", "paper_id": "WOS:000310382800012"}