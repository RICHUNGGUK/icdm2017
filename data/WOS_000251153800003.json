{"auto_keywords": [{"score": 0.047510470298773595, "phrase": "ann"}, {"score": 0.00481495049065317, "phrase": "back-propagation_neural_networks"}, {"score": 0.004620093797504386, "phrase": "artificial_neural_networks"}, {"score": 0.004342430078306461, "phrase": "global_approximation_tools"}, {"score": 0.004195418010131832, "phrase": "approximate_optimization"}, {"score": 0.004025531820226182, "phrase": "absolute_difference"}, {"score": 0.003970439654470374, "phrase": "target_outputs"}, {"score": 0.003916098498242888, "phrase": "approximate_outputs"}, {"score": 0.003809628721278519, "phrase": "approximate_optimal_solutions"}, {"score": 0.003483120583663528, "phrase": "inequality_constraint_functions"}, {"score": 0.00327356326590461, "phrase": "efficient_back-propagation_neural_network"}, {"score": 0.0031193028729331667, "phrase": "constraint_feasibility"}, {"score": 0.00307657476125683, "phrase": "approximate_optimal_solution"}, {"score": 0.0030135741897058844, "phrase": "bpn_architecture"}, {"score": 0.0028715299874549245, "phrase": "derivative-based_method"}, {"score": 0.00271735110571002, "phrase": "interconnection_weights"}, {"score": 0.002518746501941657, "phrase": "proposed_approach"}, {"score": 0.00241658945942583, "phrase": "standard_ten-bar_truss_problem"}, {"score": 0.0023346234464901978, "phrase": "ga-based_approximate_optimization"}, {"score": 0.002255431275670053, "phrase": "optical_flying_head"}, {"score": 0.002163930783583212, "phrase": "shock_resistance_capability"}, {"score": 0.0021049977753042253, "phrase": "dynamic_characteristics"}], "paper_keywords": ["constrained approximate optimization", " back-propagation neural network", " inequality constraints", " genetic algorithm"], "paper_abstract": "Artificial neural networks (ANN) have been extensively used as global approximation tools in the context of approximate optimization. ANN traditionally minimizes the absolute difference between target outputs and approximate outputs thereby resulting in approximate optimal solutions being sometimes actually infeasible when it is used as a metamodel for inequality constraint functions. The paper explores the development of the efficient back-propagation neural network (BPN)-based metamodel that ensures the constraint feasibility of approximate optimal solution. The BPN architecture is optimized via two approaches of both derivative-based method and genetic algorithm (GA) to determine interconnection weights between layers in the network. The verification of the proposed approach is examined by adopting a standard ten-bar truss problem. Finally, a GA-based approximate optimization of suspension with an optical flying head is conducted to enhance the shock resistance capability in addition to dynamic characteristics.", "paper_title": "Derivative and GA-based methods in metamodeling of back-propagation neural networks for constrained approximate optimization", "paper_id": "WOS:000251153800003"}