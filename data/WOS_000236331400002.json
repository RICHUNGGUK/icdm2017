{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "hierarchical_classification"}, {"score": 0.004396426822203258, "phrase": "new_algorithm"}, {"score": 0.0043103495043308755, "phrase": "linear-threshold_classifier"}, {"score": 0.004062057297453209, "phrase": "trained_node_classifiers"}, {"score": 0.004014135205423942, "phrase": "top-down_fashion"}, {"score": 0.0038431888954930083, "phrase": "new_hierarchical_loss_function"}, {"score": 0.0036794956254952126, "phrase": "classification_mistake"}, {"score": 0.00344007531836105, "phrase": "additional_mistake"}, {"score": 0.0032161835102640372, "phrase": "data_instances"}, {"score": 0.003153140905312787, "phrase": "linear_noise_model"}, {"score": 0.0029362173680378624, "phrase": "reference_classifier"}, {"score": 0.002901538668877986, "phrase": "true_parameters"}, {"score": 0.002867268370615848, "phrase": "label-generating_process"}, {"score": 0.0027559226203857316, "phrase": "excess_cumulative_h-loss"}, {"score": 0.002648889316519512, "phrase": "data_sequence"}, {"score": 0.002576442680766742, "phrase": "precise_dependence"}, {"score": 0.0023334673218688437, "phrase": "texual_corpora"}, {"score": 0.002164216680934624, "phrase": "perceptron-based_hierarchical_classifiers"}, {"score": 0.0021049977753042253, "phrase": "hierarchical_support_vector_machine"}], "paper_keywords": ["incremental algorithms", " online learning", " hierarchical classification", " second order perceptron", " support vector machines", " regret bound", " loss function"], "paper_abstract": "We study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed. We introduce a new algorithm that incrementally learns a linear-threshold classifier for each node of the taxonomy. A hierarchical classification is obtained by evaluating the trained node classifiers in a top-down fashion. To evaluate classifiers in our multipath framework, we define a new hierarchical loss function, the H-loss, capturing the intuition that whenever a classification mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node. Making no assumptions on the mechanism generating the data instances, and assuming a linear noise model for the labels, we bound the H-loss of our on-line algorithm in terms of the H-loss of a reference classifier knowing the true parameters of the label-generating process. We show that, in expectation, the excess cumulative H-loss grows at most logarithmically in the length of the data sequence. Furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes. Our theoretical results are complemented by a number of experiments on texual corpora. In these experiments we show that, after only one epoch of training, our algorithm performs much better than Perceptron-based hierarchical classifiers, and reasonably close to a hierarchical support vector machine.", "paper_title": "Incremental algorithms for hierarchical classification", "paper_id": "WOS:000236331400002"}