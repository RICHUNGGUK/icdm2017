{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "decision_tree"}, {"score": 0.01293696589560445, "phrase": "class_membership"}, {"score": 0.003772306776343196, "phrase": "receiver_operating_characteristics_curve"}, {"score": 0.003684234403858852, "phrase": "auc"}, {"score": 0.003405109584265324, "phrase": "ranking_performance"}, {"score": 0.0027307746993761035, "phrase": "simple_voting"}, {"score": 0.0026045914281028473, "phrase": "test_instance"}, {"score": 0.0024842242921857705, "phrase": "similarity-weighted_voting"}, {"score": 0.0024262056455509303, "phrase": "bayes"}, {"score": 0.0023507916951658455, "phrase": "empirical_experiments"}, {"score": 0.0022070310530794097, "phrase": "recent_decision_tree"}, {"score": 0.0021049988725343354, "phrase": "auc."}], "paper_keywords": ["Ranking", " Class probability estimation", " Decision trees", " Voting", " Similarity-weighted voting", " Naive Bayes"], "paper_abstract": "Decision tree is one of the most effective and widely used methods for classification. However, many real-world applications require instances to be ranked by the probability of class membership. The area under the receiver operating characteristics curve, simply AUC, has been recently used as a measure for ranking performance of learning algorithms. In this paper, we present two novel class probability estimation algorithms to improve the ranking performance of decision tree. Instead of estimating the probability of class membership using simple voting at the leaf where the test instance falls into, our algorithms use similarity-weighted voting and naive Bayes. We design empirical experiments to verify that our new algorithms significantly outperform the recent decision tree ranking algorithm C4.4 in terms of AUC.", "paper_title": "Learning decision tree for ranking", "paper_id": "WOS:000267367500006"}