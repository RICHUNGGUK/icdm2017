{"auto_keywords": [{"score": 0.04218988994298027, "phrase": "elm_theories"}, {"score": 0.009237384209453308, "phrase": "hidden_neurons"}, {"score": 0.007459151336828217, "phrase": "elm"}, {"score": 0.007406068917426027, "phrase": "local_connections"}, {"score": 0.00481495049065317, "phrase": "extreme_learning_machine"}, {"score": 0.0046457802872573025, "phrase": "\"generalized\"_single-hidden_layer_feedforward_neural_networks"}, {"score": 0.00441883624011048, "phrase": "feature_learning"}, {"score": 0.004248294537944227, "phrase": "common_understanding"}, {"score": 0.0041580507929967645, "phrase": "neural_networks"}, {"score": 0.004055176557424983, "phrase": "training_stage"}, {"score": 0.003775008774288817, "phrase": "hidden_nodes"}, {"score": 0.003708033700676924, "phrase": "training_samples"}, {"score": 0.003616252896285766, "phrase": "continuous_probability_distribution"}, {"score": 0.0035648266669315943, "phrase": "obtained_elm_networks"}, {"score": 0.0035393874762182486, "phrase": "universal_approximation"}, {"score": 0.0035141291835319682, "phrase": "classification_capability"}, {"score": 0.00347657813672945, "phrase": "fully_connected_elm_architecture"}, {"score": 0.0032246835734680377, "phrase": "general_architecture"}, {"score": 0.0032016639006812826, "phrase": "locally_connected_elm"}, {"score": 0.003023307182674672, "phrase": "local_receptive_fields"}, {"score": 0.0029909851863915283, "phrase": "input_layer"}, {"score": 0.00266693888510356, "phrase": "different_local_receptive_fields"}, {"score": 0.002647890142618442, "phrase": "true_biological_receptive_fields"}, {"score": 0.002610198789888055, "phrase": "exact_shapes"}, {"score": 0.0025455220351023465, "phrase": "human_beings"}, {"score": 0.002509284173541882, "phrase": "specific_example"}, {"score": 0.002473560912156024, "phrase": "random_convolutional_nodes"}, {"score": 0.002447101794397243, "phrase": "pooling_structure"}, {"score": 0.0023864566654035924, "phrase": "experimental_results"}, {"score": 0.0023609270767241215, "phrase": "norb_dataset"}, {"score": 0.0023189818169615135, "phrase": "object_recognition"}, {"score": 0.0022696277881969896, "phrase": "conventional_deep_learning_solutions"}, {"score": 0.0022453451864218477, "phrase": "proposed_local_receptive_fields"}, {"score": 0.0021662599012740127, "phrase": "error_rate"}, {"score": 0.0021049977753042253, "phrase": "learning_speed"}], "paper_keywords": [""], "paper_abstract": "Extreme learning machine (ELM), which was originally proposed for \"generalized\" single-hidden layer feedforward neural networks (SLFNs), provides efficient unified learning solutions for the applications of feature learning, clustering, regression and classification. Different from the common understanding and tenet that hidden neurons of neural networks need to be iteratively adjusted during training stage, ELM theories show that hidden neurons are important but need not be iteratively tuned. In fact, all the parameters of hidden nodes can be independent of training samples and randomly generated according to any continuous probability distribution. And the obtained ELM networks satisfy universal approximation and classification capability. The fully connected ELM architecture has been extensively studied. However, ELM with local connections has not attracted much research attention yet. This paper studies the general architecture of locally connected ELM, showing that: 1) ELM theories are naturally valid for local connections, thus introducing local receptive fields to the input layer; 2) each hidden node in ELM can be a combination of several hidden nodes (a subnetwork), which is also consistent with ELM theories. ELM theories may shed a light on the research of different local receptive fields including true biological receptive fields of which the exact shapes and formula may be unknown to human beings. As a specific example of such general architectures, random convolutional nodes and a pooling structure are implemented in this paper. Experimental results on the NORB dataset, a benchmark for object recognition, show that compared with conventional deep learning solutions, the proposed local receptive fields based ELM (ELM-LRF) reduces the error rate from 6.5% to 2.7% and increases the learning speed up to 200 times.", "paper_title": "Local Receptive Fields Based Extreme Learning Machine", "paper_id": "WOS:000352902900002"}