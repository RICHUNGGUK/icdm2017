{"auto_keywords": [{"score": 0.04107868567004354, "phrase": "ale"}, {"score": 0.004647442352794694, "phrase": "general_agents"}, {"score": 0.00443308765153716, "phrase": "arcade_learning_environment"}, {"score": 0.0039626308903511764, "phrase": "domain-independent_ai_technology"}, {"score": 0.0034388111429203222, "phrase": "human_players"}, {"score": 0.0033584796908285894, "phrase": "significant_research_challenges"}, {"score": 0.0033190180867511605, "phrase": "reinforcement_learning"}, {"score": 0.0032800186243040663, "phrase": "model_learning"}, {"score": 0.0032414759269196493, "phrase": "model-based_planning"}, {"score": 0.00314708280192844, "phrase": "transfer_learning"}, {"score": 0.003091767402370757, "phrase": "intrinsic_motivation"}, {"score": 0.0029489527539649737, "phrase": "rigorous_testbed"}, {"score": 0.002620040660435039, "phrase": "domain-independent_agents"}, {"score": 0.002573965014439041, "phrase": "well-established_ai_techniques"}, {"score": 0.0023415390666563177, "phrase": "evaluation_methodology"}, {"score": 0.002246553137021003, "phrase": "empirical_results"}, {"score": 0.0021049977753042253, "phrase": "benchmark_agents"}], "paper_keywords": [""], "paper_abstract": "In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.", "paper_title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "paper_id": "WOS:000320362300001"}