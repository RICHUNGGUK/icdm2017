{"auto_keywords": [{"score": 0.037208692166501474, "phrase": "hybrid_systems"}, {"score": 0.031878918313121896, "phrase": "sse"}, {"score": 0.0316770877352543, "phrase": "avx"}, {"score": 0.00481495049065317, "phrase": "emerging_architectures"}, {"score": 0.0047527118741586055, "phrase": "massively_parallel_data_mining"}, {"score": 0.004721893473387288, "phrase": "adaptive_sparse_grids"}, {"score": 0.004630626571109149, "phrase": "vast_datasets"}, {"score": 0.004585653726916024, "phrase": "main_challenge"}, {"score": 0.0045559136361908795, "phrase": "data-driven_applications"}, {"score": 0.004511663118269957, "phrase": "sparse_grids"}, {"score": 0.004467840466181518, "phrase": "numerical_method"}, {"score": 0.004367228576339114, "phrase": "data_mining"}, {"score": 0.004241178291414502, "phrase": "data_points"}, {"score": 0.004132178452877669, "phrase": "huge_amounts"}, {"score": 0.004039094712469991, "phrase": "recursive_nature"}, {"score": 0.004012885020428863, "phrase": "sparse_grid_algorithms"}, {"score": 0.003859146369720225, "phrase": "modern_hardware_architectures"}, {"score": 0.0035923719610602245, "phrase": "vector_units"}, {"score": 0.0034322572683276654, "phrase": "algorithmical_point"}, {"score": 0.0031638248208086263, "phrase": "parallel_programming_languages"}, {"score": 0.0029836109362668586, "phrase": "vector_extensions"}, {"score": 0.0029545864975783023, "phrase": "nvidia's_fermi_architecture"}, {"score": 0.0028414933754660086, "phrase": "gpu"}, {"score": 0.002679386232765167, "phrase": "parallel_programming"}, {"score": 0.002477753633057247, "phrase": "classically_parallelized_sparse_grid_algorithms"}, {"score": 0.0023826870912207303, "phrase": "single_and_double_precision_measurements"}, {"score": 0.0023594945794976493, "phrase": "huge_data_sets"}, {"score": 0.0023213398100616132, "phrase": "real-life_dataset"}, {"score": 0.0022838006190016263, "phrase": "artificial_ones"}, {"score": 0.0021747784376752776, "phrase": "excellent_results"}, {"score": 0.00212569405594774, "phrase": "single_precision"}, {"score": 0.0021049977753042253, "phrase": "hybrid_system"}], "paper_keywords": ["Sparse grids", " Adaptivity", " SIMD", " Many-core", " Multi-core", " Accelerators", " GPGPU", " Hybrid acceleration"], "paper_abstract": "Gaining knowledge out of vast datasets is a main challenge in data-driven applications nowadays. Sparse grids provide a numerical method for both classification and regression in data mining which scales only linearly in the number of data points and is thus well-suited for huge amounts of data. Due to the recursive nature of sparse grid algorithms and their classical random memory access pattern, they impose a challenge for the parallelization on modern hardware architectures such as accelerators. In this paper, we present the parallelization on several current task- and data-parallel platforms, covering multi-core CPUs with vector units, GPUs, and hybrid systems. We demonstrate that a less efficient implementation from an algorithmical point of view can be beneficial if it allows vectorization and a higher degree of parallelism instead. Furthermore, we analyze the suitability of parallel programming languages for the implementation. Considering hardware, we restrict ourselves to the x86 platform with SSE and AVX vector extensions and to NVIDIA's Fermi architecture for GPUs. We consider both multi-core CPU and GPU architectures independently, as well as hybrid systems with up to 12 cores and 2 Fermi GPUs. With respect to parallel programming, we examine both the open standard OpenCL and Intel Array Building Blocks, a recently introduced high-level programming approach, and comment on their ease of use. As the baseline, we use the best results obtained with classically parallelized sparse grid algorithms and their OpenMP-parallelized intrinsics counterpart (SSE and AVX instructions), reporting both single and double precision measurements. The huge data sets we use are a real-life dataset stemming from astrophysics and artificial ones, all of which exhibit challenging properties. In all settings, we achieve excellent results, obtaining speedups of up to 188 x using single precision on a hybrid system.", "paper_title": "Emerging Architectures Enable to Boost Massively Parallel Data Mining Using Adaptive Sparse Grids", "paper_id": "WOS:000315038800002"}