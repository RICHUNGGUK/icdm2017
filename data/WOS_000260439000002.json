{"auto_keywords": [{"score": 0.0453438369329394, "phrase": "negative_samples"}, {"score": 0.03727378592872019, "phrase": "bda"}, {"score": 0.015719699187829488, "phrase": "one-class_classification_problems"}, {"score": 0.01546359828026655, "phrase": "biased_discriminant_analysis"}, {"score": 0.010159081129126467, "phrase": "positive_samples"}, {"score": 0.004561950666122737, "phrase": "object_verification"}, {"score": 0.004505471345843642, "phrase": "conventional_linear_discriminant_analysis"}, {"score": 0.004376378707877484, "phrase": "inappropriate_assumption"}, {"score": 0.004198323646273642, "phrase": "gaussian_distribution"}, {"score": 0.00402748353567517, "phrase": "sufficient_number"}, {"score": 0.003863568356785813, "phrase": "mean_value"}, {"score": 0.0032044314332538154, "phrase": "negative_sample"}, {"score": 0.0030357976028714557, "phrase": "first_extension"}, {"score": 0.0029981559422442693, "phrase": "saturation_technique"}, {"score": 0.002816814445002924, "phrase": "decision_boundary"}, {"score": 0.0027818804382539444, "phrase": "second_one"}, {"score": 0.002559775585192185, "phrase": "multi-class_classification_problems"}, {"score": 0.0023750478314945303, "phrase": "negative_effect"}, {"score": 0.002231310272815607, "phrase": "better_classification_performances"}, {"score": 0.002176275802465835, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "conventional_methods"}], "paper_keywords": ["Classification", " One-class", " One-against-rest", " BDA"], "paper_abstract": "In many one-class classification problems such as face detection and object verification, the conventional linear discriminant analysis sometimes fails because it makes an inappropriate assumption on negative samples that they are distributed according to a Gaussian distribution. In addition, it sometimes cannot extract sufficient number of features because it merely makes Use of the mean Value of each class. In order to resolve these problems, in this paper, we extend the biased discriminant analysis (BDA) which was originally developed for one-class classification problems. The BDA makes no assumption oil the distribution of negative samples and tries to separate each negative sample as far away from the center of positive samples as possible. The first extension uses a saturation technique to suppress the influence of the samples which are located far away front the decision boundary. The second one utilizes the L1 norm instead of the L2 norm. Also we present a method to extend BDA and its variants to multi-class classification problems. Our approach is considered useful in the sense that without much complexity, it successfully reduces the negative effect of negative samples which are far away from the center of positive samples, resulting in better classification performances. We have applied the proposed methods to several classification problems and compared the performance with conventional methods. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Feature extraction for one-class classification problems: Enhancements to biased discriminant analysis", "paper_id": "WOS:000260439000002"}