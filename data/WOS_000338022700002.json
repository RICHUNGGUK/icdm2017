{"auto_keywords": [{"score": 0.04911945324917723, "phrase": "web_spam"}, {"score": 0.014335343739057224, "phrase": "web_pages"}, {"score": 0.013336361447823323, "phrase": "new_techniques"}, {"score": 0.00481495049065317, "phrase": "web_spam_evolution"}, {"score": 0.004661446705304497, "phrase": "ongoing_battle"}, {"score": 0.00445830200258645, "phrase": "search_engines"}, {"score": 0.004333642054036135, "phrase": "modern_sharing"}, {"score": 0.004298666679374568, "phrase": "web_links"}, {"score": 0.004263972370406983, "phrase": "social_networks"}, {"score": 0.004212453011596424, "phrase": "common_challenge"}, {"score": 0.003964007795174772, "phrase": "legitimate_and_spam_web_pages"}, {"score": 0.00386876598761639, "phrase": "legitimate_web_pages"}, {"score": 0.003640516896245813, "phrase": "spam_web_pages"}, {"score": 0.0035100601949451028, "phrase": "webb_spam_corpus"}, {"score": 0.0031974502006267163, "phrase": "standard_corpus"}, {"score": 0.00303319936111073, "phrase": "spam_filtering_techniques"}, {"score": 0.0028541057064952876, "phrase": "multiple_aspects"}, {"score": 0.002774189193414484, "phrase": "http"}, {"score": 0.002547535564922075, "phrase": "last_webb_spam_corpus"}, {"score": 0.0023970487263732737, "phrase": "social_media"}, {"score": 0.0021049977753042253, "phrase": "http_header_information"}], "paper_keywords": ["Web spam", " evolution", " spam corpus"], "paper_abstract": "Identifying and detecting web spam is an ongoing battle between spam-researchers and spammers which has been going on since search engines allowed searching of web pages to the modern sharing of web links via social networks. A common challenge faced by spam-researchers is the fact that new techniques depend on requiring a corpus of legitimate and spam web pages. Although large corpora of legitimate web pages are available to researchers, the same cannot be said about web spam or spam web pages. In this paper, we introduce the Webb Spam Corpus 2011 - a corpus of approximately 330,000 spam web pages - which we make available to researchers in the fight against spam. By having a standard corpus available, researchers can collaborate better on developing and reporting results of spam filtering techniques. The corpus contains web pages crawled from links found in over 6.3 million spam emails. We analyze multiple aspects of this corpus including redirection, HTTP headers, web page content, and classification evaluation. We also provide insights into changes in web spam since the last Webb Spam Corpus was released in 2006. These insights include: (1) spammers manipulate social media in spreading spam; (2) HTTP headers and content also change over time; (3) spammers have evolved and adopted new techniques to avoid the detection based on HTTP header information.", "paper_title": "A Perspective of Evolution After Five Years: A Large-Scale Study of Web Spam Evolution", "paper_id": "WOS:000338022700002"}