{"auto_keywords": [{"score": 0.03598871877647395, "phrase": "gradient_evolution"}, {"score": 0.00481495049065317, "phrase": "new_metaheuristic_method"}, {"score": 0.004682708592184678, "phrase": "gradient-based_search_method"}, {"score": 0.004596561972944185, "phrase": "exact_optimization_method"}, {"score": 0.0044289730051264116, "phrase": "extreme_points"}, {"score": 0.004307286039184164, "phrase": "optimal_point"}, {"score": 0.00418892838079239, "phrase": "gradient_method"}, {"score": 0.004092774981286263, "phrase": "metaheuristic_method"}, {"score": 0.004017437417375057, "phrase": "gradient_theorem"}, {"score": 0.0039252049353279556, "phrase": "new_method"}, {"score": 0.0037820003983334476, "phrase": "search_space"}, {"score": 0.0034947408949397127, "phrase": "vector_updating"}, {"score": 0.003446353878654566, "phrase": "main_updating_rule"}, {"score": 0.003367188029221534, "phrase": "search_direction"}, {"score": 0.0032898346872897383, "phrase": "newton-raphson_method"}, {"score": 0.0032637705374211966, "phrase": "vector"}, {"score": 0.003125835572347772, "phrase": "local_optima"}, {"score": 0.0029838294169759663, "phrase": "gradient_evolution_method"}, {"score": 0.0028882701690741467, "phrase": "fifteen_test_functions"}, {"score": 0.0028482561434303886, "phrase": "first_experiment"}, {"score": 0.0027827907725673845, "phrase": "parameter_settings"}, {"score": 0.0025474193855847074, "phrase": "basic_and_improved_metaheuristic_methods"}, {"score": 0.0023103100997344072, "phrase": "particle_swarm_optimization"}, {"score": 0.0022889103976458437, "phrase": "differential_evolution"}, {"score": 0.0022571810910697013, "phrase": "artificial_bee_colony"}, {"score": 0.002236272403824371, "phrase": "continuous_genetic_algorithm"}, {"score": 0.0021746987512551693, "phrase": "benchmark_problems"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Gradient-based method", " Metaheuristic method", " Optimization"], "paper_abstract": "This study presents a new metaheuristic method that is derived from the gradient-based search method. In an exact optimization method, the gradient is used to find extreme points, as well as the optimal point. This study modifies a gradient method, and creates a metaheuristic method that uses a gradient theorem as its basic updating rule. This new method, named gradient evolution, explores the search space using a set of vectors and includes three major operators: vector updating, jumping and refreshing. Vector updating is the main updating rule in gradient evolution. The search direction is determined using the Newton-Raphson method. Vector jumping and refreshing enable this method to avoid local optima. In order to evaluate the performance of the gradient evolution method, three different experiments are conducted, using fifteen test functions. The first experiment determines the influence of parameter settings on the result. It also determines the best parameter setting. There follows a comparison between the basic and improved metaheuristic methods. The experimental results show that gradient evolution performs better than, or as well as, other methods, such as particle swarm optimization, differential evolution, an artificial bee colony and continuous genetic algorithm, for most of the benchmark problems tested. (C) 2015 Elsevier Inc. All rights reserved.", "paper_title": "The gradient evolution algorithm: A new metaheuristic", "paper_id": "WOS:000356732600016"}