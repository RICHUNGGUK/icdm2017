{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "large_datasets"}, {"score": 0.00465944657565638, "phrase": "fast_algorithms"}, {"score": 0.004558564684763264, "phrase": "high_dimensional_data"}, {"score": 0.004484332781147261, "phrase": "important_challenge"}, {"score": 0.004435514622798228, "phrase": "computational_statistics"}, {"score": 0.004363277689798392, "phrase": "new_class"}, {"score": 0.004315771670213371, "phrase": "recursive_stochastic_gradient_algorithms"}, {"score": 0.004222299135545249, "phrase": "k-medians_loss_criterion"}, {"score": 0.00380511089035825, "phrase": "large_samples"}, {"score": 0.0035049738376240567, "phrase": "stochastic_gradient_algorithm"}, {"score": 0.0033546701239540555, "phrase": "stationary_points"}, {"score": 0.003299977511820374, "phrase": "underlying_loss_criterion"}, {"score": 0.003246173672295418, "phrase": "particular_attention"}, {"score": 0.0031584367879334114, "phrase": "averaged_versions"}, {"score": 0.0030562674358159945, "phrase": "better_performances"}, {"score": 0.0030064251340251196, "phrase": "data-driven_procedure"}, {"score": 0.00294122712771669, "phrase": "fully_automatic_selection"}, {"score": 0.0028460640294160383, "phrase": "descent_step"}, {"score": 0.002709045632654716, "phrase": "averaged_sequential_estimator"}, {"score": 0.0026357877401570764, "phrase": "simulation_study"}, {"score": 0.002550481822137976, "phrase": "computation_speed"}, {"score": 0.0023749823753904204, "phrase": "trimmed_k-means"}, {"score": 0.002223696715077018, "phrase": "new_online_clustering_technique"}, {"score": 0.002163535133532765, "phrase": "television_audience_profiles"}], "paper_keywords": ["Averaging", " High dimensional data", " k-medoids", " Online clustering", " Partitioning around medoids", " Recursive estimators", " Robbins-Monro", " Stochastic approximation", " Stochastic gradient"], "paper_abstract": "Clustering with fast algorithms large samples of high dimensional data is an important challenge in computational statistics. A new class of recursive stochastic gradient algorithms designed for the k-medians loss criterion is proposed. By their recursive nature, these algorithms are very fast and are well adapted to deal with large samples of data that are allowed to arrive sequentially. It is proved that the stochastic gradient algorithm converges almost surely to the set of stationary points of the underlying loss criterion. A particular attention is paid to the averaged versions which are known to have better performances. A data-driven procedure that permits a fully automatic selection of the value of the descent step is also proposed. The performance of the averaged sequential estimator is compared on a simulation study, both in terms of computation speed and accuracy of the estimations, with more classical partitioning techniques such as k-means, trimmed k-means and PAM (partitioning around medoids). Finally, this new online clustering technique is illustrated on determining television audience profiles with a sample of more than 5000 individual television audiences measured every minute over a period of 24 hours. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A fast and recursive algorithm for clustering large datasets with k-medians", "paper_id": "WOS:000302033200010"}