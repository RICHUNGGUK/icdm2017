{"auto_keywords": [{"score": 0.03744658332079375, "phrase": "auc"}, {"score": 0.00481495049065317, "phrase": "defect_prediction"}, {"score": 0.004779564521350969, "phrase": "static_code"}, {"score": 0.004726970265271088, "phrase": "current_results"}, {"score": 0.004657739551687038, "phrase": "new_approaches"}, {"score": 0.004606479650820579, "phrase": "quality_software"}, {"score": 0.004539005582217309, "phrase": "software_quality_assurance"}, {"score": 0.004423284899590467, "phrase": "data_miners"}, {"score": 0.004374593906165761, "phrase": "defect_predictors"}, {"score": 0.004200582085706493, "phrase": "qa_resources"}, {"score": 0.003945117054416623, "phrase": "recent_results"}, {"score": 0.0039016690475620185, "phrase": "better_data_mining_technology"}, {"score": 0.003830312347175889, "phrase": "better_defect_predictors"}, {"score": 0.003664315990489347, "phrase": "standard_learning_goal"}, {"score": 0.003479692058963528, "phrase": "false_alarms"}, {"score": 0.0032081215771180664, "phrase": "false_alarm"}, {"score": 0.003069006373093756, "phrase": "standard_goal"}, {"score": 0.002925077525258983, "phrase": "smallest_set"}, {"score": 0.0028085616610192456, "phrase": "meta-learner_framework"}, {"score": 0.0027368368719283298, "phrase": "different_goals"}, {"score": 0.0024860616730750158, "phrase": "new_goal"}, {"score": 0.002422552850701705, "phrase": "simple_manual_methods"}, {"score": 0.0023519508354777215, "phrase": "indiscriminate_use"}, {"score": 0.0022005118194652704, "phrase": "right_architecture"}, {"score": 0.0021363670305843403, "phrase": "specific_local_business_goals"}, {"score": 0.0021049977753042253, "phrase": "simple_task"}], "paper_keywords": ["Defect prediction", " Static code features", " WHICH"], "paper_abstract": "Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective. Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection \"AUC(pd, pf)\"; i.e. the area under the curve of a probability of false alarm versus probability of detection. Accordingly, we explore changing the standard goal. Learners that maximize \"AUC(effort, pd)\" find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods. Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.", "paper_title": "Defect prediction from static code features: current results, limitations, new approaches", "paper_id": "WOS:000280257100002"}