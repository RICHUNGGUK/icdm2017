{"auto_keywords": [{"score": 0.04553177755087418, "phrase": "lpcc"}, {"score": 0.04215833196561754, "phrase": "mfcc"}, {"score": 0.04116037487774433, "phrase": "temporal_and_frequential_representations"}, {"score": 0.039337462586774265, "phrase": "cepstral_transformation"}, {"score": 0.02443831865970788, "phrase": "residual_features"}, {"score": 0.00481495049065317, "phrase": "lp-residual_representations"}, {"score": 0.004748887399153146, "phrase": "feature_extraction"}, {"score": 0.004709683359341165, "phrase": "essential_and_important_step"}, {"score": 0.004683726454379078, "phrase": "speaker_recognition_systems"}, {"score": 0.004493535274957768, "phrase": "conventional_features"}, {"score": 0.00445642983394405, "phrase": "mel_frequency_cepstral_coding"}, {"score": 0.004383131582126381, "phrase": "predictive_cepstral_coding"}, {"score": 0.004311033701151224, "phrase": "-conventional_ones"}, {"score": 0.0039898841992120695, "phrase": "lpcc."}, {"score": 0.0037130544070027305, "phrase": "similar_way"}, {"score": 0.0036823699601344962, "phrase": "lpc-lpcc_transformation"}, {"score": 0.003591824245643109, "phrase": "non-linear_nature"}, {"score": 0.00356213805517703, "phrase": "speech_signals"}, {"score": 0.003503497128638099, "phrase": "second_and_third-order_statistics"}, {"score": 0.003305727596568728, "phrase": "ar_model"}, {"score": 0.0031538090440906873, "phrase": "frequential_approach"}, {"score": 0.0031104662578334435, "phrase": "filter_bank_method"}, {"score": 0.003025559481569312, "phrase": "pdss"}, {"score": 0.002983968624542411, "phrase": "spectral_flatness"}, {"score": 0.0029348176536617013, "phrase": "resulting_features"}, {"score": 0.002854686750765905, "phrase": "proposed_schemes"}, {"score": 0.0028154431906571944, "phrase": "speaker_identification_problem"}, {"score": 0.002776737611968975, "phrase": "first_one"}, {"score": 0.0027537694407469356, "phrase": "gaudi_database"}, {"score": 0.002708399936381754, "phrase": "main_interest"}, {"score": 0.0026785685146367513, "phrase": "controlled_acquisition_conditions"}, {"score": 0.0026198851984058735, "phrase": "interval_sessions"}, {"score": 0.0025982110733562135, "phrase": "second_database"}, {"score": 0.0025767157935702814, "phrase": "well-known_ntimit_corpus"}, {"score": 0.002478726402271019, "phrase": "larger_corpus"}, {"score": 0.002417705233170991, "phrase": "traditional_features"}, {"score": 0.0024043497996540653, "phrase": "residual_ones"}, {"score": 0.002281087456676369, "phrase": "speaker-dependent_features"}, {"score": 0.0022126280490696363, "phrase": "global_improvements"}, {"score": 0.002176154620525061, "phrase": "different_mismatches"}, {"score": 0.0021225662995668446, "phrase": "opinion_fusion_framework"}, {"score": 0.0021049977753042253, "phrase": "useful_information"}], "paper_keywords": ["Feature extraction", " Speaker identification", " LP-residue", " Non-linear speech processing"], "paper_abstract": "Feature extraction is an essential and important step for speaker recognition systems. In this paper, we propose to improve these systems by exploiting both conventional features such as mel frequency cepstral coding (MFCC), linear predictive cepstral coding (LPCC) and non-conventional ones. The method exploits information present in the linear predictive (LP) residual signal. The features extracted from the LP-residue are then combined to the MFCC or the LPCC. We investigate two approaches termed as temporal and frequential representations. The first one consists of an auto-regressive (AR) modelling of the signal followed by a cepstral transformation in a similar way to the LPC-LPCC transformation. In order to take into account the non-linear nature of the speech signals we used two estimation methods based on second and third-order statistics. They are, respectively, termed as R-SOS-LPCC (residual plus second-order statistic based estimation of the AR model plus cepstral transformation) and R-HOS-LPCC (higher order). Concerning the frequential approach, we exploit a filter bank method called the power difference of spectra in sub-band (PDSS) which measures the spectral flatness over the sub-bands. The resulting features are named R-PDSS. The analysis of these proposed schemes are done over a speaker identification problem with two different databases. The first one is the Gaudi database and contains 49 speakers. The main interest lies in the controlled acquisition conditions: mismatch between the microphones and the interval sessions. The second database is the well-known NTIMIT corpus with 630 speakers. The performances of the features are confirmed over this larger corpus. In addition, we propose to compare traditional features and residual ones by the fusion of recognizers (feature extractor + classifier). The results show that residual features carry speaker-dependent features and the combination with the LPCC or the MFCC shows global improvements in terms of robustness under different mismatches. A comparison between the residual features under the opinion fusion framework gives us useful information about the potential of both temporal and frequential representations. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Investigation on LP-residual representations for speaker identification", "paper_id": "WOS:000261290400016"}