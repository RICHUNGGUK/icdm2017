{"auto_keywords": [{"score": 0.03715473136328279, "phrase": "dictionary_knowledge"}, {"score": 0.029447592125930233, "phrase": "gold-standard_annotations"}, {"score": 0.015719716506582538, "phrase": "entity_recognition"}, {"score": 0.011686570588253509, "phrase": "heterogeneous_models"}, {"score": 0.010288640713372219, "phrase": "drug_names"}, {"score": 0.00799321289806771, "phrase": "gold-standard_data"}, {"score": 0.0047739754059430055, "phrase": "aggregate_classifier"}, {"score": 0.004666406953322213, "phrase": "ner"}, {"score": 0.004613516457919661, "phrase": "critical_step"}, {"score": 0.004587300172928083, "phrase": "complex_biomedical_nlp_tasks"}, {"score": 0.004420472010542188, "phrase": "large_quantities"}, {"score": 0.0043953478557600565, "phrase": "high_quality_training_data"}, {"score": 0.004296264970612285, "phrase": "supervised_machine-learning_techniques"}, {"score": 0.004259684990957109, "phrase": "high_classification_performance"}, {"score": 0.004199406257273994, "phrase": "human_labour"}, {"score": 0.004093037185145671, "phrase": "significant_limitation"}, {"score": 0.003899385109646589, "phrase": "manual_annotations"}, {"score": 0.0038332390893263844, "phrase": "drug_ner"}, {"score": 0.003631062193606885, "phrase": "voting_system"}, {"score": 0.0035088824684910962, "phrase": "gold-standard_corpora"}, {"score": 0.0033619033669956317, "phrase": "genetic_programming"}, {"score": 0.0033048445033866795, "phrase": "common_drug_suffixes"}, {"score": 0.0029567784489935777, "phrase": "ukpmc_database"}, {"score": 0.002931569771634119, "phrase": "raw_biomedical_text"}, {"score": 0.0029148831519944358, "phrase": "gold-standard_and_silver_annotated_data"}, {"score": 0.0028490783932465288, "phrase": "multinomial_logistic_regression_classifiers"}, {"score": 0.0028087048250773766, "phrase": "drug_ner_methods"}, {"score": 0.002622713543589907, "phrase": "maximum_f-score"}, {"score": 0.0025416235470290286, "phrase": "silver_annotations"}, {"score": 0.002477140909481497, "phrase": "comparable_performance"}, {"score": 0.002414290278501595, "phrase": "main_reason"}, {"score": 0.00238006327644733, "phrase": "morphological_similarities"}, {"score": 0.002286781334850213, "phrase": "hard_requirement"}, {"score": 0.0022737565087825058, "phrase": "drug_ner."}, {"score": 0.0022223932033729396, "phrase": "similar_or_comparable_classification_performance"}, {"score": 0.0021908807203548345, "phrase": "best_performing_model"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Named entity annotation sparsity", " Gold-standard vs. silver-standard annotations", " Named entity recogniser aggregation", " Genetic-programming-evolved string-similarity patterns", " Drug named entity recognition"], "paper_abstract": "Objective: Drug named entity recognition (NER) is a critical step for complex biomedical NLP tasks such as the extraction of pharmacogenomic, pharmacodynamic and pharmacokinetic parameters. Large quantities of high quality training data are almost always a prerequisite for employing supervised machine-learning techniques to achieve high classification performance. However, the human labour needed to produce and maintain such resources is a significant limitation. In this study, we improve the performance of drug NER without relying exclusively on manual annotations. Methods: We perform drug NER using either a small gold-standard corpus (120 abstracts) or no corpus at all. In our approach, we develop a voting system to combine a number of heterogeneous models, based on dictionary knowledge, gold-standard corpora and silver annotations, to enhance performance. To improve recall, we employed genetic programming to evolve 11 regular-expression patterns that capture common drug suffixes and used them as an extra means for recognition. Materials: Our approach uses a dictionary of drug names, i.e. DrugBank, a small manually annotated corpus, i.e. the pharmacokinetic corpus, and a part of the UKPMC database, as raw biomedical text. Gold-standard and silver annotated data are used to train maximum entropy and multinomial logistic regression classifiers. Results: Aggregating drug NER methods, based on gold-standard annotations, dictionary knowledge and patterns, improved the performance on models trained on gold-standard annotations, only, achieving a maximum F-score of 95%. In addition, combining models trained on silver annotations, dictionary knowledge and patterns are shown to achieve comparable performance to models trained exclusively on gold-standard data. The main reason appears to be the morphological similarities shared among drug names. Conclusion: We conclude that gold-standard data are not a hard requirement for drug NER. Combining heterogeneous models build on dictionary knowledge can achieve similar or comparable classification performance with that of the best performing model trained on gold-standard annotations. (C) 2015 The Authors. Published by Elsevier B.V.", "paper_title": "Boosting drug named entity recognition using an aggregate classifier", "paper_id": "WOS:000363826900007"}