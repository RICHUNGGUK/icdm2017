{"auto_keywords": [{"score": 0.00466016686289196, "phrase": "value-added_services"}, {"score": 0.004573953147933065, "phrase": "sophisticated_data_collection_and_analysis_capabilities"}, {"score": 0.004108161406159407, "phrase": "significant_strain"}, {"score": 0.004050996777117918, "phrase": "web_server"}, {"score": 0.0039024014688338964, "phrase": "web_robots"}, {"score": 0.003812299519486331, "phrase": "undesirable_ones"}, {"score": 0.003570889780957037, "phrase": "robot_traffic"}, {"score": 0.0034398445347109396, "phrase": "capacity_planning"}, {"score": 0.003407839062159497, "phrase": "web_servers"}, {"score": 0.003313592421224647, "phrase": "web_robot_detection_techniques"}, {"score": 0.0031919592786945126, "phrase": "single_technique"}, {"score": 0.0031475032122226, "phrase": "even_a_specific_\"type"}, {"score": 0.0028664985822266344, "phrase": "practically_applicable_robot_detection_technique"}, {"score": 0.002787181295054319, "phrase": "critical_analysis"}, {"score": 0.002722758109991685, "phrase": "prevalent_detection_approaches"}, {"score": 0.0026105159490849364, "phrase": "existing_detection_techniques"}, {"score": 0.0024912073108138613, "phrase": "different_classes"}, {"score": 0.0023662411481713704, "phrase": "effective_robot_detection_scheme"}, {"score": 0.0022793032191981404, "phrase": "contemporary_techniques"}, {"score": 0.002226592967032469, "phrase": "general_solution"}, {"score": 0.002195552442011313, "phrase": "robot_detection_problem"}, {"score": 0.0021347607855562102, "phrase": "key_ingredients"}, {"score": 0.0021049977753042253, "phrase": "strong_web_robot_detection"}], "paper_keywords": ["Web Crawler", " Web Robot", " WWW", " Web Robot Detection", " Web User Classification"], "paper_abstract": "Most modern Web robots that crawl the Internet to support value-added services and technologies possess sophisticated data collection and analysis capabilities. Some of these robots, however, may be ill-behaved or malicious, and hence, may impose a significant strain on a Web server. It is thus necessary to detect Web robots in order to block undesirable ones from accessing the server. Such detection is also essential to ensure that the robot traffic is considered appropriately in the performance and capacity planning of Web servers. Despite a variety of Web robot detection techniques, there is no consensus regarding a single technique, or even a specific \"type\" of technique, that performs well in practice. Therefore, to aid in the development of a practically applicable robot detection technique, this survey presents a critical analysis and comparison of the prevalent detection approaches. We propose a framework to classify the existing detection techniques into four categories based on their underlying detection philosophy. We compare the different classes to gain insights into those characteristics that make up an effective robot detection scheme. Finally, we discuss why the contemporary techniques fail to offer a general solution to the robot detection problem and propose a set of key ingredients necessary for strong Web robot detection.", "paper_title": "Web robot detection techniques: overview and limitations", "paper_id": "WOS:000286001100006"}