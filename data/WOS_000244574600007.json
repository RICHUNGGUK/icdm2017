{"auto_keywords": [{"score": 0.03337540840020653, "phrase": "fusion_performance"}, {"score": 0.032725200680271634, "phrase": "input_conditions"}, {"score": 0.00481495049065317, "phrase": "signal-level_image_fusion"}, {"score": 0.0046993598473345395, "phrase": "considerable_research_attention"}, {"score": 0.004440277230269978, "phrase": "image_processing_and_information_fusion_techniques"}, {"score": 0.004333642054036135, "phrase": "optimal_information_fusion_strategy"}, {"score": 0.004161553532799662, "phrase": "multi-sensor_data"}, {"score": 0.003916098498242888, "phrase": "fusion_algorithms"}, {"score": 0.003822003373331997, "phrase": "small_number"}, {"score": 0.003791141540467091, "phrase": "available_objective_metrics"}, {"score": 0.00374531366198605, "phrase": "large_set"}, {"score": 0.0037150687722055727, "phrase": "relevant_sample_data"}, {"score": 0.0034676182423067307, "phrase": "optimal_performance"}, {"score": 0.003384262486955773, "phrase": "sample_data"}, {"score": 0.003236596168242036, "phrase": "powerful_framework"}, {"score": 0.003210446135114967, "phrase": "objectively_adaptive_image_fusion"}, {"score": 0.003120568497402472, "phrase": "broad_range"}, {"score": 0.0029602552900111407, "phrase": "objective_image_fusion_evaluation"}, {"score": 0.002900806476592644, "phrase": "fusion_process"}, {"score": 0.0028425481299483254, "phrase": "specific_focus"}, {"score": 0.002740607717745287, "phrase": "broad_appeal"}, {"score": 0.002707444334463544, "phrase": "wide_range"}, {"score": 0.002685558057339722, "phrase": "fusion_applications"}, {"score": 0.0026530590435527527, "phrase": "night_vision"}, {"score": 0.0026103363762901423, "phrase": "medical_imaging"}, {"score": 0.002568299907890696, "phrase": "objective_fusion_metrics"}, {"score": 0.0024963543716522087, "phrase": "conventional_fusion"}, {"score": 0.002416589459425828, "phrase": "fusion_parameters"}, {"score": 0.002387337550919433, "phrase": "optimal_fusion_display"}, {"score": 0.002320449502337321, "phrase": "proposed_framework"}, {"score": 0.0022923586577645143, "phrase": "considerable_improvement"}, {"score": 0.0022011501095573747, "phrase": "wide_array"}, {"score": 0.00218334762335187, "phrase": "multi-sensor_images"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["image fusion", " signal-level fusion", " optimal fusion", " adaptive fusion"], "paper_abstract": "Signal-level image fusion has been the focus of considerable research attention in recent years with a plethora of algorithms proposed, using a host of image processing and information fusion techniques. Yet what is an optimal information fusion strategy or spectral decomposition that should precede it for any multi-sensor data cannot be defined a priori. This could be learned by either evaluating fusion algorithms subjectively or indeed through a small number of available objective metrics on a large set of relevant sample data. This is not practical however and is limited in that it provides no guarantee of optimal performance should realistic input conditions be different from the sample data. This paper proposes and examines the viability of a powerful framework for objectively adaptive image fusion that explicitly optimises fusion performance for a broad range of input conditions. The idea is to employ the concepts used in objective image fusion evaluation to optimally adapt the fusion process to the input conditions. Specific focus is on fusion for display, which has broad appeal in a wide range of fusion applications such as night vision, avionics and medical imaging. By integrating objective fusion metrics shown to be subjectively relevant into conventional fusion algorithms the framework is used to adapt fusion parameters to achieve optimal fusion display. The results show that the proposed framework achieves a considerable improvement in both level and robustness of fusion performance on a wide array of multi-sensor images and image sequences. (C) 2005 Elsevier B.V. All rights reserved.", "paper_title": "Objectively adaptive image fusion", "paper_id": "WOS:000244574600007"}