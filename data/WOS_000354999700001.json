{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "mallows_models"}, {"score": 0.02772513079748339, "phrase": "pairwise_evidence"}, {"score": 0.004763565662615503, "phrase": "pairwise-preference_data"}, {"score": 0.0046875099587735825, "phrase": "preference_distributions"}, {"score": 0.004612662942073808, "phrase": "critical_problem"}, {"score": 0.0040334640350288, "phrase": "user_preferences"}, {"score": 0.003701401653850141, "phrase": "arbitrary_pairwise_comparisons"}, {"score": 0.0035648266669315943, "phrase": "fundamental_building_blocks"}, {"score": 0.003526735797659508, "phrase": "ordinal_rankings"}, {"score": 0.0034332736827447654, "phrase": "first_algorithms"}, {"score": 0.0032536902910768957, "phrase": "pairwise_comparison_data"}, {"score": 0.003100085797251317, "phrase": "new_algorithm"}, {"score": 0.002859977408815691, "phrase": "arbitrary_ranking_distributions"}, {"score": 0.0027991493739366632, "phrase": "mallows"}, {"score": 0.0026384167183586015, "phrase": "mallows_model"}, {"score": 0.0024868972942701582, "phrase": "approximate_samplers"}, {"score": 0.002382182688788248, "phrase": "provable_bounds"}, {"score": 0.0022453451864218477, "phrase": "mallows_mixtures"}, {"score": 0.002209406458446019, "phrase": "non-parametric_estimation"}, {"score": 0.002162379426839795, "phrase": "real-world_data_sets"}], "paper_keywords": ["preference learning", " ranking", " incomplete data", " Mallows models", " mixture models"], "paper_abstract": "Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the first algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally difficult in general, we develop approximate samplers that are exact for many important special cases and have provable bounds with pairwise evidence and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the effectiveness of our approach.", "paper_title": "Effective Sampling and Learning for Mallows Models with Pairwise-Preference Data", "paper_id": "WOS:000354999700001"}