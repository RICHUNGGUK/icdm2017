{"auto_keywords": [{"score": 0.036653305954379496, "phrase": "proposed_loss_function"}, {"score": 0.00481495049065317, "phrase": "pattern_classification"}, {"score": 0.004674952036988355, "phrase": "new_loss_function"}, {"score": 0.00462919213721188, "phrase": "neural_network_classification"}, {"score": 0.004516733575701128, "phrase": "recently_proposed_similarity_measure"}, {"score": 0.0044725233829143235, "phrase": "correntropy"}, {"score": 0.004236908273106904, "phrase": "conventional_square_loss"}, {"score": 0.004073359218158693, "phrase": "decision_boundary"}, {"score": 0.004013662619667083, "phrase": "small_errors"}, {"score": 0.003531474887552889, "phrase": "kernel_size_parameter"}, {"score": 0.003312486229143855, "phrase": "close_approximation"}, {"score": 0.0032639039638211347, "phrase": "misclassification_loss"}, {"score": 0.003091767402370757, "phrase": "discriminant_function"}, {"score": 0.0025262061502713683, "phrase": "prolonged_training"}, {"score": 0.002404706573379854, "phrase": "close_competitor"}, {"score": 0.0023694195742181546, "phrase": "svm."}, {"score": 0.0023346234464901978, "phrase": "proposed_method"}, {"score": 0.0022890371899188466, "phrase": "simple_gradient_based_online_learning"}, {"score": 0.002233301265743633, "phrase": "practical_way"}, {"score": 0.0021682027434117095, "phrase": "neural_network_classifiers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Classification", " Correntropy", " Neural network", " Loss function", " Backprojection"], "paper_abstract": "This paper presents a new loss function for neural network classification, inspired by the recently proposed similarity measure called Correntropy. We show that this function essentially behaves like the conventional square loss for samples that are well within the decision boundary and have small errors, and L-o or counting norm for samples that are outliers or are difficult to classify. Depending on the value of the kernel size parameter, the proposed loss function moves smoothly from convex to non-convex and becomes a close approximation to the misclassification loss (ideal 0-1 loss). We show that the discriminant function obtained by optimizing the proposed loss function in the neighborhood of the ideal 0-1 loss function to train a neural network is immune to overfitting, more robust to outliers, and has consistent and better generalization performance as compared to other commonly used loss functions, even after prolonged training. The results also show that it is a close competitor to the SVM. Since the proposed method is compatible with simple gradient based online learning, it is a practical way of improving the performance of neural network classifiers. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "The C-loss function for pattern classification", "paper_id": "WOS:000326903500035"}