{"auto_keywords": [{"score": 0.039774496364856735, "phrase": "basis_functions"}, {"score": 0.02973090540945096, "phrase": "training_points"}, {"score": 0.00481495049065317, "phrase": "response_prediction"}, {"score": 0.004640398886726948, "phrase": "engineering_design_problems"}, {"score": 0.004534517530868195, "phrase": "computation_efficiency"}, {"score": 0.004492841495994535, "phrase": "surrogate_models"}, {"score": 0.004390312150344245, "phrase": "expensive_simulations"}, {"score": 0.004349955692802962, "phrase": "engineering_problems"}, {"score": 0.004231089088705995, "phrase": "new_high-fidelity_surrogate_modeling_approach"}, {"score": 0.004077616905584829, "phrase": "polynomial_response_surface"}, {"score": 0.003947884118204803, "phrase": "spprs_model"}, {"score": 0.0038577428353448596, "phrase": "legendre_polynomials"}, {"score": 0.0036496814103475174, "phrase": "sample_size"}, {"score": 0.0035498822293046884, "phrase": "expression_ability"}, {"score": 0.003517223847512266, "phrase": "complex_functional_relationships"}, {"score": 0.0033428820402140683, "phrase": "\"sparsity-promoting\"_regression_approach"}, {"score": 0.0030056887638095883, "phrase": "function_relationship"}, {"score": 0.0027528751037412128, "phrase": "sparsity-promoting_regression_approach"}, {"score": 0.002640636493247601, "phrase": "functional_variation"}, {"score": 0.0026042472337367015, "phrase": "reasonable_local_accuracy"}, {"score": 0.00249805321224923, "phrase": "latin_hypercube_design"}, {"score": 0.002475049305784789, "phrase": "lhd"}, {"score": 0.0023851187337905412, "phrase": "predictive_capability"}, {"score": 0.0023305762840188145, "phrase": "spprs"}, {"score": 0.0022878396720586044, "phrase": "seven_benchmark_test_functions"}, {"score": 0.0022563011263787847, "phrase": "complex_engineering_problem"}, {"score": 0.0021945194903838132, "phrase": "promising_benefits"}, {"score": 0.002164264600379492, "phrase": "novel_surrogate_modeling_technique"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Sparsity-promoting", " Legendre polynomials", " Overfitting", " Surrogate model", " Response prediction"], "paper_abstract": "Computation-intensive analyses/simulations are becoming increasingly common in engineering design problems. To improve the computation efficiency, surrogate models are used to replace expensive simulations of engineering problems. This paper proposes a new high-fidelity surrogate modeling approach which is called the Sparsity-promoting Polynomial Response Surface (SPPRS). In the SPPRS model, a series of Legendre polynomials is selected as basis functions, and its number is compatible with the sample size so as to enhance the expression ability for complex functional relationships. The coefficients associated with basis functions are estimated using a \"sparsity-promoting\" regression approach which is an ensemble of two techniques: least squares and l(1)-norm regularization. As a result, only these basis functions relevant to explain the function relationship are picked out, and that dedicates to ease the problem of overfitting for training points. With the sparsity-promoting regression approach, such a surrogate model intends to capture both the global trend of the functional variation and a reasonable local accuracy in the neighborhood of training points. Additionally, Latin hypercube design (LHD) is proved conducive to improving the predictive capability of our model. The SPPRS is applied to seven benchmark test functions and a complex engineering problem. The results illustrate the promising benefits of this novel surrogate modeling technique. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Sparsity-promoting polynomial response surface: A new surrogate model for response prediction", "paper_id": "WOS:000343361600005"}