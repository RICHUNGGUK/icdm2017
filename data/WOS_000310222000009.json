{"auto_keywords": [{"score": 0.038128500901754125, "phrase": "expected_fitness"}, {"score": 0.008594417515429484, "phrase": "sampling_distribution"}, {"score": 0.007746945713527822, "phrase": "expected_fitness_maximization"}, {"score": 0.0075011993620778435, "phrase": "natural_gradient"}, {"score": 0.007211327394819845, "phrase": "fisher_information_matrix"}, {"score": 0.00481495049065317, "phrase": "theoretical_foundation_for_cma-es"}, {"score": 0.0047804843375940835, "phrase": "information_geometry_perspective"}, {"score": 0.00467855218490191, "phrase": "theoretical_basis"}, {"score": 0.004628400269774981, "phrase": "covariance_matrix_adaptation_evolution_strategy"}, {"score": 0.004497262692458022, "phrase": "information_geometry_viewpoint"}, {"score": 0.00441718685609235, "phrase": "theoretical_foundation"}, {"score": 0.004276610897002087, "phrase": "geometric_structure"}, {"score": 0.004230749292148046, "phrase": "riemannian_manifold"}, {"score": 0.0042004472903194616, "phrase": "probability_distributions"}, {"score": 0.004140490129683107, "phrase": "fisher_metric"}, {"score": 0.0035601676907962626, "phrase": "steepest_ascent"}, {"score": 0.003459191419478298, "phrase": "steepest_ascent_direction"}, {"score": 0.003230665474330377, "phrase": "conventional_gradient"}, {"score": 0.003038983069081333, "phrase": "multivariate_normal_distribution"}, {"score": 0.0028076592725295646, "phrase": "distribution_parameters"}, {"score": 0.0025938977569302177, "phrase": "learning_rates"}, {"score": 0.0025021765811679446, "phrase": "exact_natural_gradient"}, {"score": 0.002396371892825691, "phrase": "close_relation"}, {"score": 0.0023536149458842992, "phrase": "natural_gradient_learning"}, {"score": 0.00232832698253493, "phrase": "default_setting"}, {"score": 0.0022298571016688335, "phrase": "monotone_improvement"}, {"score": 0.002135542806459067, "phrase": "expectation-maximization_framework"}, {"score": 0.0021049977753042253, "phrase": "information_geometric_interpretation"}], "paper_keywords": ["CMA-ES", " Information geometry", " Natural gradient learning", " Expectation-maximization algorithms", " Expected fitness maximization", " Continuous optimization"], "paper_abstract": "This paper explores the theoretical basis of the covariance matrix adaptation evolution strategy (CMA-ES) from the information geometry viewpoint. To establish a theoretical foundation for the CMA-ES, we focus on a geometric structure of a Riemannian manifold of probability distributions equipped with the Fisher metric. We define a function on the manifold which is the expectation of fitness over the sampling distribution, and regard the goal of update of the parameters of sampling distribution in the CMA-ES as maximization of the expected fitness. We investigate the steepest ascent learning for the expected fitness maximization, where the steepest ascent direction is given by the natural gradient, which is the product of the inverse of the Fisher information matrix and the conventional gradient of the function. Our first result is that we can obtain under some types of parameterization of multivariate normal distribution the natural gradient of the expected fitness without the need for inversion of the Fisher information matrix. We find that the update of the distribution parameters in the CMA-ES is the same as natural gradient learning for expected fitness maximization. Our second result is that we derive the range of learning rates such that a step in the direction of the exact natural gradient improves the parameters in the expected fitness. We see from the close relation between the CMA-ES and natural gradient learning that the default setting of learning rates in the CMA-ES seems suitable in terms of monotone improvement in expected fitness. Then, we discuss the relation to the expectation-maximization framework and provide an information geometric interpretation of the CMA-ES.", "paper_title": "Theoretical Foundation for CMA-ES from Information Geometry Perspective", "paper_id": "WOS:000310222000009"}