{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "perception"}, {"score": 0.004741395005218184, "phrase": "features_for_audio_key_determination"}, {"score": 0.004668957916795845, "phrase": "musical_key"}, {"score": 0.004527371799277529, "phrase": "fundamental_knowledge"}, {"score": 0.004435363840720883, "phrase": "automatic_transcription"}, {"score": 0.004390060342250211, "phrase": "chord_detection"}, {"score": 0.0043452175611323335, "phrase": "automatic_play_list_generation"}, {"score": 0.00421340730888561, "phrase": "novel_features"}, {"score": 0.004023120753104693, "phrase": "musical_knowledge"}, {"score": 0.003782655283668953, "phrase": "chromatic_representation"}, {"score": 0.0035932440062595252, "phrase": "tone_ratings"}, {"score": 0.0034308688638878286, "phrase": "extensive_results"}, {"score": 0.0033096502887394233, "phrase": "originally_recorded_music"}, {"score": 0.003275807170434343, "phrase": "diverse_genres"}, {"score": 0.003048370487544551, "phrase": "band-pass-filter_type_selection"}, {"score": 0.002955782731675529, "phrase": "gating_time"}, {"score": 0.0028221322455573624, "phrase": "traditional_correlation-based_approaches"}, {"score": 0.0027646829399573434, "phrase": "data-driven_model"}, {"score": 0.0027363971333961967, "phrase": "support_vector_machines"}, {"score": 0.00266693888510356, "phrase": "proposed_features"}, {"score": 0.00263965046277432, "phrase": "correlation_results"}, {"score": 0.0025726416100664853, "phrase": "data-driven_audio_key_determination_accuracy"}, {"score": 0.00248167018931442, "phrase": "learnt_models"}, {"score": 0.002443671484766342, "phrase": "correlation-based_determination"}, {"score": 0.0022161333611134806, "phrase": "maximum_accuracy"}, {"score": 0.0021049977753042253, "phrase": "diverse_recordings"}], "paper_keywords": [""], "paper_abstract": "The musical key of a piece is the fundamental knowledge for many Music Information Retrieval tasks as automatic transcription, chord detection or automatic play list generation. To this end, novel features are proposed and evaluated on the basis of musical knowledge, in this article-a total of 13 feature groups based on chromatic representation include scales, chords, major and minor Probe Tone Ratings, and further derived variations. We present extensive results on a database containing 35 h of originally recorded music covering diverse genres as popular, classical, and Jazz music. In the pre-processing, adjustment of reference pitch classes, dB(A)-weighting, band-pass-filter type selection, frequency selection, and gating time are discussed in detail. In addition to traditional correlation-based approaches we employ a data-driven model using Support Vector Machines. Overall, the proposed features enhance correlation results and their combination boosts data-driven audio key determination accuracy. The latter approach based on learnt models significantly outperforms correlation-based determination by 5.0% and 6.7% absolute for the determination of 12 and 24 major and minor keys (clustered or separately), respectively. Maximum accuracy reaches 77.3% and 62.1% for these 12 or 24 keys over the diverse recordings.", "paper_title": "Music Theoretic and Perception-based Features for Audio Key Determination", "paper_id": "WOS:000306111000003"}