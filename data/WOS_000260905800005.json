{"auto_keywords": [{"score": 0.04637570254730582, "phrase": "mpi"}, {"score": 0.00481495049065317, "phrase": "unbalanced_applications"}, {"score": 0.004678785623021003, "phrase": "distributed_memory_parallel_implementation"}, {"score": 0.004598933498016774, "phrase": "unbalanced_tree_search"}, {"score": 0.00436742595270452, "phrase": "mpi's_ability"}, {"score": 0.004268293864953256, "phrase": "irregular_and_nested_parallelism"}, {"score": 0.004219571240696568, "phrase": "continuous_dynamic_load_balancing"}, {"score": 0.0039841418890102925, "phrase": "centralized_work_server"}, {"score": 0.0038922720138048804, "phrase": "uts"}, {"score": 0.0038492126993451337, "phrase": "explicit_polling"}, {"score": 0.003471135453226184, "phrase": "load_balancing"}, {"score": 0.003334309053314909, "phrase": "additional_techniques"}, {"score": 0.003166259444961177, "phrase": "run-time_overhead"}, {"score": 0.0031121406603482112, "phrase": "additional_parameters"}, {"score": 0.0029214769181390653, "phrase": "parallel_performance"}, {"score": 0.0025158514649823724, "phrase": "simpler_work_sharing_approach"}, {"score": 0.002472822097373027, "phrase": "single_work_server"}, {"score": 0.002444544487772349, "phrase": "good_performance"}, {"score": 0.002129356014992349, "phrase": "particular_workload"}, {"score": 0.0021049977753042253, "phrase": "load_balancing_parameters"}], "paper_keywords": ["Unbalanced tree search benchmark", " Dynamic load balancing", " MPI", " Work stealing", " Work sharing"], "paper_abstract": "We present a distributed memory parallel implementation of the unbalanced tree search (UTS) benchmark using MPI and investigate MPI's ability to efficiently support irregular and nested parallelism through continuous dynamic load balancing. Two load balancing methods are explored: work sharing using a centralized work server and distributed work stealing using explicit polling to service steal requests. Experiments indicate that in addition to a parameter defining the granularity of load balancing, message-passing paradigms require additional techniques to manage the volume of communication and mitigate run-time overhead. Using additional parameters, we observed an improvement of up to 3-4X in parallel performance. We report results for three distributed memory parallel computer systems and use UTS to characterize the performance and scalability on these systems. Overall, we find that the simpler work sharing approach with a single work server achieves good performance on hundreds of processors and that our distributed work stealing implementation scales to thousands of processors and delivers more robust performance that is less sensitive to the particular workload and load balancing parameters. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "A message passing benchmark for unbalanced applications", "paper_id": "WOS:000260905800005"}