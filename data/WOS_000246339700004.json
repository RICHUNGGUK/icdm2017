{"auto_keywords": [{"score": 0.004337710093279712, "phrase": "entire_audio-visual_lifetime"}, {"score": 0.004081385289516914, "phrase": "digital_information"}, {"score": 0.003170667618004276, "phrase": "today's_retrieval_cues"}, {"score": 0.0030092017034068666, "phrase": "additional_cues"}, {"score": 0.002906138793910919, "phrase": "body-worn_sensors"}, {"score": 0.002831158147480665, "phrase": "associative_retrieval"}, {"score": 0.002462664392109308, "phrase": "experimental_evaluation"}, {"score": 0.0022967713922705, "phrase": "acceleration_sensors"}, {"score": 0.0022180544041790697, "phrase": "audio_sensors"}, {"score": 0.0021049977753042253, "phrase": "multiple_sensors"}], "paper_keywords": ["context-awareness", " information retrieval", " sensing systems", " context recognition", " wearable computing"], "paper_abstract": "In the near future, it will be possible to continuously record and store the entire audio-visual lifetime of a person together with all digital information that the person perceives or creates. While the storage of this data will be possible soon, retrieval and indexing into such large data sets are unsolved challenges. Since today's retrieval cues seem insufficient we argue that additional cues, obtained from body-worn sensors, make associative retrieval by humans possible. We present three approaches to create such cues, each along with an experimental evaluation: the user's physical activity from acceleration sensors, his social environment from audio sensors, and his interruptibility from multiple sensors.", "paper_title": "Recognizing context for annotating a live Me recording", "paper_id": "WOS:000246339700004"}