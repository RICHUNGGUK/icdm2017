{"auto_keywords": [{"score": 0.04710550010426661, "phrase": "ce"}, {"score": 0.027605858312973828, "phrase": "sce"}, {"score": 0.024647000260337774, "phrase": "proposed_algorithms"}, {"score": 0.00481495049065317, "phrase": "cliff-edge_problems"}, {"score": 0.004684633969993391, "phrase": "efficient_agent"}, {"score": 0.004627848087788984, "phrase": "cliff-edge"}, {"score": 0.004447983733147892, "phrase": "common_interactions"}, {"score": 0.004407474941517865, "phrase": "sealed-bid_auctions"}, {"score": 0.004327555970055785, "phrase": "ultimatum_game"}, {"score": 0.004134015084535274, "phrase": "success_increases"}, {"score": 0.00404667000994945, "phrase": "sce_interactions"}, {"score": 0.0039975862967816765, "phrase": "simultaneous_auctions"}, {"score": 0.0037609217735040715, "phrase": "one-shot_interactions"}, {"score": 0.003703969862557761, "phrase": "different_human_opponents"}, {"score": 0.0036367606686493227, "phrase": "general_pattern"}, {"score": 0.0036036131261225936, "phrase": "population's_behavior"}, {"score": 0.0033901931552297792, "phrase": "generic_approach"}, {"score": 0.0033083952949044173, "phrase": "unknown_opponents"}, {"score": 0.003288255061396602, "phrase": "different_environments"}, {"score": 0.0032682370318207503, "phrase": "ce_and_sce_interactions"}, {"score": 0.003189372286182993, "phrase": "relatively_large_number"}, {"score": 0.003065243071851165, "phrase": "underlying_mechanism"}, {"score": 0.0030280272943359813, "phrase": "ce_interactions"}, {"score": 0.003000411392166266, "phrase": "new_meta-algorithm"}, {"score": 0.0029012842615708215, "phrase": "existing_methods"}, {"score": 0.0028312491793328043, "phrase": "large_number"}, {"score": 0.002814005426276146, "phrase": "alternative_decisions"}, {"score": 0.002788335965186124, "phrase": "decision_point"}, {"score": 0.0027629000130849598, "phrase": "competitive_approach"}, {"score": 0.002737695457580011, "phrase": "bayesian_approach"}, {"score": 0.002696196402171568, "phrase": "opponents'_statistical_distribution"}, {"score": 0.0026715987201787286, "phrase": "prior_knowledge"}, {"score": 0.002544141963686395, "phrase": "virtual_reinforcement_learning_algorithm"}, {"score": 0.0025286424059485745, "phrase": "sdvrl"}, {"score": 0.0024979252889176756, "phrase": "segmentation_meta-algorithm"}, {"score": 0.0024525461181496753, "phrase": "different_basic_algorithms"}, {"score": 0.00218364206035443, "phrase": "average_payoff"}, {"score": 0.0021049977753042253, "phrase": "optimal_action"}], "paper_keywords": ["Cliff-Edge problems", " Human-agent interactions", " Ultimatum game", " Dynamic pricing", " Reinforcement learning", " Virtual learning"], "paper_abstract": "In this paper, we propose an efficient agent for competing in Cliff-Edge (CE) and simultaneous Cliff-Edge (SCE) situations. In CE interactions, which include common interactions such as sealed-bid auctions, dynamic pricing and the ultimatum game (UG), the probability of success decreases monotonically as the reward for success increases. This trade-off exists also in SCE interactions, which include simultaneous auctions and various multi-player ultimatum games, where the agent has to decide about more than one offer or bid simultaneously. Our agent competes repeatedly in one-shot interactions, each time against different human opponents. The agent learns the general pattern of the population's behavior, and its performance is evaluated based on all of the interactions in which it participates. We propose a generic approach which may help the agent compete against unknown opponents in different environments where CE and SCE interactions exist, where the agent has a relatively large number of alternatives and where its achievements in the first several dozen interactions are important. The underlying mechanism we propose for CE interactions is a new meta-algorithm, deviated virtual learning (DVL), which extends existing methods to efficiently cope with environments comprising a large number of alternative decisions at each decision point. Another competitive approach is the Bayesian approach, which learns the opponents' statistical distribution, given prior knowledge about the type of distribution. For the SCE, we propose the simultaneous deviated virtual reinforcement learning algorithm (SDVRL), the segmentation meta-algorithm as a method for extending different basic algorithms, and a heuristic called fixed success probabilities (FSP). Experiments comparing the performance of the proposed algorithms with algorithms taken from the literature, as well as other intuitive meta-algorithms, reveal superiority of the proposed algorithms in average payoff and stability as well as in accuracy in converging to the optimal action, both in CE and SCE problems.", "paper_title": "Efficient bidding strategies for Cliff-Edge problems", "paper_id": "WOS:000329941000005"}