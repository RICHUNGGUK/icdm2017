{"auto_keywords": [{"score": 0.04005808804923863, "phrase": "rrf"}, {"score": 0.037059816074360756, "phrase": "grrf"}, {"score": 0.013840030800420818, "phrase": "rf"}, {"score": 0.008934303862402782, "phrase": "small_number"}, {"score": 0.007255864559502806, "phrase": "accuracy_performance"}, {"score": 0.006105258247394492, "phrase": "minimal_regularization"}, {"score": 0.0056333810433410415, "phrase": "feature_subset"}, {"score": 0.00481495049065317, "phrase": "guided_regularized_random_forest"}, {"score": 0.004772060936021876, "phrase": "regularized_random_forest"}, {"score": 0.004659538620628888, "phrase": "feature_selection"}, {"score": 0.0044423558024348, "phrase": "training_data"}, {"score": 0.004402770481509365, "phrase": "tree_node"}, {"score": 0.004337573851941615, "phrase": "upper_bound"}, {"score": 0.004273338516164498, "phrase": "distinct_gini_information_gain_values"}, {"score": 0.003989844368560806, "phrase": "large_number"}, {"score": 0.003669888501738697, "phrase": "enhanced_rrf"}, {"score": 0.003488238679515793, "phrase": "importance_scores"}, {"score": 0.00345712586265106, "phrase": "ordinary_random_forest"}, {"score": 0.0033654369798577378, "phrase": "feature_selection_process"}, {"score": 0.0033453930912354796, "phrase": "rrf._experiments"}, {"score": 0.003067778605343437, "phrase": "compact_feature_subsets"}, {"score": 0.0030313341227908687, "phrase": "competitive_accuracy_performance"}, {"score": 0.002959816926840323, "phrase": "lasso"}, {"score": 0.002681783466572198, "phrase": "data_sets"}, {"score": 0.002379548054048268, "phrase": "feature_selection_methods"}, {"score": 0.002344240943768209, "phrase": "weak_classifiers"}, {"score": 0.0022015695920570167, "phrase": "\"rrf\"_r_package"}, {"score": 0.0021819119049109516, "phrase": "cran"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Classification", " Feature selection", " Random forest", " Variable selection"], "paper_abstract": "The regularized random forest (RRF) was recently proposed for feature selection by building only one ensemble. In RRF the features are evaluated on a part of the training data at each tree node. We derive an upper bound for the number of distinct Gini information gain values in a node, and show that many features can share the same information gain at a node with a small number of instances and a large number of features. Therefore, in a node with a small number of instances, RRF is likely to select a feature not strongly relevant. Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In GRRF, the importance scores from an ordinary random forest (RF) are used to guide the feature selection process in RRF. Experiments on 10 gene data sets show that the accuracy performance of GRRF is, in general, more robust than RRF when their parameters change. GRRF is computationally efficient, can select compact feature subsets, and has competitive accuracy performance, compared to RRF, varSelRF and LASSO logistic regression (with evaluations from an RE classifier). Also, RF applied to the features selected by RRF with the minimal regularization outperforms RF applied to all the features for most of the data sets considered here. Therefore, if accuracy is considered more important than the size of the feature subset, RRF with the minimal regularization may be considered. We use the accuracy performance of RF, a strong classifier, to evaluate feature selection methods, and illustrate that weak classifiers are less capable of capturing the information contained in a feature subset. Both RRF and GRRF were implemented in the \"RRF\" R package available at CRAN, the official R package archive. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Gene selection with guided regularized random forest", "paper_id": "WOS:000323804100027"}