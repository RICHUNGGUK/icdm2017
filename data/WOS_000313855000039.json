{"auto_keywords": [{"score": 0.03407694553852572, "phrase": "temporal_signatures"}, {"score": 0.00481495049065317, "phrase": "tracked_facial_actions"}, {"score": 0.00439777299939725, "phrase": "facial_expressions"}, {"score": 0.0043511221096418475, "phrase": "continuous_videos"}, {"score": 0.004191690483929023, "phrase": "classifiers_performance"}, {"score": 0.004081385289516914, "phrase": "independent_temporal_facial_action_parameters"}, {"score": 0.0034407781868385423, "phrase": "first_scheme"}, {"score": 0.0033861224940737846, "phrase": "dynamic_time_warping_technique"}, {"score": 0.0032969458013463807, "phrase": "training_data"}, {"score": 0.0031760175583084274, "phrase": "different_universal_facial_expressions"}, {"score": 0.0031255543155487234, "phrase": "second_scheme"}, {"score": 0.003043218974273827, "phrase": "facial_actions"}, {"score": 0.003010893527930816, "phrase": "fixed_length_feature_vectors"}, {"score": 0.0027643161113894018, "phrase": "displayed_expression"}, {"score": 0.002662871928330334, "phrase": "different_schemes"}, {"score": 0.00256514094268541, "phrase": "cmu_video_sequences"}, {"score": 0.002537880772794665, "phrase": "home-made_video_sequences"}, {"score": 0.002418742283474052, "phrase": "dimension_reduction_techniques"}, {"score": 0.0023802825369325354, "phrase": "extracted_time_series"}, {"score": 0.00232995025271599, "phrase": "classification_performance"}, {"score": 0.002220551141101661, "phrase": "best_recognition_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Visual face tracking", " 3D deformable models", " Facial actions", " Dynamic facial expression recognition", " Human-computer interaction"], "paper_abstract": "In this paper, we address the analysis and recognition of facial expressions in continuous videos. More precisely, we study classifiers performance that exploit head pose independent temporal facial action parameters. These are provided by an appearance-based 3D face tracker that simultaneously provides the 3D head pose and facial actions. The use of such tracker makes the recognition pose- and texture-independent. Two different schemes are studied. The first scheme adopts a dynamic time warping technique for recognizing expressions where training data are given by temporal signatures associated with different universal facial expressions. The second scheme models temporal signatures associated with facial actions with fixed length feature vectors (observations), and uses some machine learning algorithms in order to recognize the displayed expression. Experiments quantified the performance of different schemes. These were carried out on CMU video sequences and home-made video sequences. The results show that the use of dimension reduction techniques on the extracted time series can improve the classification performance. Moreover, these experiments show that the best recognition rate can be above 90%. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Facial expression recognition using tracked facial actions: Classifier performance analysis", "paper_id": "WOS:000313855000039"}