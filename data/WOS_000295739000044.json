{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high-dimensional_matrix_prediction"}, {"score": 0.004198722289738563, "phrase": "new_entry"}, {"score": 0.00406214911915987, "phrase": "output_y"}, {"score": 0.0038930368103015467, "phrase": "high-dimensional_setting"}, {"score": 0.0037486121194392564, "phrase": "matrix_completion_problem"}, {"score": 0.0035587046945919788, "phrase": "linear_prediction_procedures"}, {"score": 0.0035085965086415474, "phrase": "different_penalizations"}, {"score": 0.0032998397149704467, "phrase": "frobenius"}, {"score": 0.0031174595932694036, "phrase": "sharp_oracle_inequalities"}, {"score": 0.0030590445140753213, "phrase": "statistical_learning_theory_point"}, {"score": 0.002987557906013826, "phrase": "surprising_fact"}, {"score": 0.0026922926476971453, "phrase": "usually_considered_incoherency_condition"}, {"score": 0.0026543514413663893, "phrase": "unknown_matrix"}, {"score": 0.0026169435194789772, "phrase": "isometry_condition"}, {"score": 0.002580061425699194, "phrase": "sampling_operator"}, {"score": 0.0023470862747075228, "phrase": "nuclear_norm_penalization"}, {"score": 0.0023030731312573246, "phrase": "regularization_algorithm"}, {"score": 0.0022070310530794097, "phrase": "prediction_accuracy"}, {"score": 0.0021656385587456952, "phrase": "deterministic_oracle"}, {"score": 0.0021049977753042253, "phrase": "reguralization_parameters"}], "paper_keywords": ["Empirical process theory", " empirical risk minimization", " high-dimensional matrix", " matrix completion", " oracle inequalities", " nuclear norm", " Schatten norms", " sparsity"], "paper_abstract": "We observe (xi, Y(i))(i=1)(n) where the Y(i)'s are real valued outputs and the X(i)'s are m x T matrices. We observe a new entry and we want to predict the output Y associated with it. We focus on the high-dimensional setting, where mT >> n. This includes the matrix completion problem with noise, as well as other problems. We consider linear prediction procedures based on different penalizations, involving a mixture of several norms: the nuclear norm, the Frobenius norm and the l(1)-norm. For these procedures, we prove sharp oracle inequalities, using a statistical learning theory point of view. A surprising fact in our results is that the rates of convergence do not depend on and directly. The analysis is conducted without the usually considered incoherency condition on the unknown matrix or restricted isometry condition on the sampling operator. Moreover, our results are the first to give for this problem an analysis of penalization (such as nuclear norm penalization) as a regularization algorithm: our oracle inequalities prove that these procedures have a prediction accuracy close to the deterministic oracle one, given that the reguralization parameters are well-chosen.", "paper_title": "Sharp Oracle Inequalities for High-Dimensional Matrix Prediction", "paper_id": "WOS:000295739000044"}