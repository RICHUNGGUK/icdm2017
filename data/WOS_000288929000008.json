{"auto_keywords": [{"score": 0.04785918158121603, "phrase": "acoustic_gestures"}, {"score": 0.046081391722127746, "phrase": "acoustic_waveforms"}, {"score": 0.040650574499495426, "phrase": "articulatory_trajectories"}, {"score": 0.031938816775762525, "phrase": "acoustic-to-articulatory_inversion"}, {"score": 0.00481495049065317, "phrase": "acoustic_and_articulatory_gestures"}, {"score": 0.004448892453489832, "phrase": "measured_articulatory_trajectories"}, {"score": 0.0043103495043308755, "phrase": "simultaneously_recorded_acoustic-articulatory_database"}, {"score": 0.004159617445102161, "phrase": "critical_points"}, {"score": 0.0040300463421312225, "phrase": "acoustic_and_articulatory_representations"}, {"score": 0.0038431888954930083, "phrase": "essentially_the_horizontal_and_vertical_movements"}, {"score": 0.003812896124654341, "phrase": "electromagnetic_articulography"}, {"score": 0.0037828575308923957, "phrase": "ema"}, {"score": 0.0035931556794840027, "phrase": "midsagittal_plane"}, {"score": 0.0035507457285983268, "phrase": "articulatory_movements"}, {"score": 0.0032934951320964276, "phrase": "detected_acoustic_and_articulatory_gestures"}, {"score": 0.002971309306027602, "phrase": "gmm-based_regression"}, {"score": 0.0027999339042953076, "phrase": "state-of-the-art_frame-based_methods"}, {"score": 0.002408623172021922, "phrase": "estimated_critical_points"}, {"score": 0.0022876877275935757, "phrase": "estimated_articulatory_trajectories"}, {"score": 0.0022606512324475584, "phrase": "acoustic-to-articulatory_inversion_methods"}, {"score": 0.002172811149894415, "phrase": "perceptual_tolerance"}, {"score": 0.002155656133617155, "phrase": "audio-visual_asynchrony"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Acoustic gestures", " Articulatory gestures", " Acoustic-to-articulatory inversion", " Critical trajectory error"], "paper_abstract": "This paper proposes a definition for articulatory as well as acoustic gestures along with a method to segment the measured articulatory trajectories and acoustic waveforms into gestures. Using a simultaneously recorded acoustic-articulatory database, the gestures are detected based on finding critical points in the utterance, both in the acoustic and articulatory representations. The acoustic gestures are parameterized using 2-D cepstral coefficients. The articulatory trajectories arc essentially the horizontal and vertical movements of Electromagnetic Articulography (EMA) coils placed on the tongue, jaw and lips along the midsagittal plane. The articulatory movements are parameterized using 2D-DCT using the same transformation that is applied on the acoustics. The relationship between the detected acoustic and articulatory gestures in terms of the timing as well as the shape is studied. In order to study this relationship further, acoustic-to-articulatory inversion is performed using GMM-based regression. The accuracy of predicting the articulatory trajectories from the acoustic waveforms are at par with state-of-the-art frame-based methods with dynamical constraints (with an average error of 1.45-1.55 mm for the two speakers in the database). In order to evaluate the acoustic-to-articulatory inversion in a more intuitive manner, a method based on the error in estimated critical points is suggested. Using this method, it was noted that the estimated articulatory trajectories using the acoustic-to-articulatory inversion methods were still not accurate enough to be within the perceptual tolerance of audio-visual asynchrony. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Mapping between acoustic and articulatory gestures", "paper_id": "WOS:000288929000008"}