{"auto_keywords": [{"score": 0.04280136321445764, "phrase": "existing_dataset"}, {"score": 0.00481495049065317, "phrase": "importance_weighting"}, {"score": 0.004621671570727774, "phrase": "novel_method"}, {"score": 0.004200197763119297, "phrase": "computing_weights"}, {"score": 0.0038960785110831162, "phrase": "confidentiality_issues"}, {"score": 0.003173636240475185, "phrase": "importance_sampling_weights"}, {"score": 0.002805989498127897, "phrase": "statistical_queries"}, {"score": 0.0026565317818265394, "phrase": "differential_privacy"}, {"score": 0.002515014676664348, "phrase": "asymptotic_variance"}, {"score": 0.002463906556781983, "phrase": "approximate_answers"}, {"score": 0.002364777610365689, "phrase": "new_mechanism"}, {"score": 0.0022696277881969896, "phrase": "privacy_budget"}, {"score": 0.0021634370496463793, "phrase": "public_and_private_datasets"}, {"score": 0.0021049977753042253, "phrase": "different_populations"}], "paper_keywords": ["Privacy", " Differential privacy", " Importance weighting"], "paper_abstract": "This paper analyzes a novel method for publishing data while still protecting privacy. The method is based on computing weights that make an existing dataset, for which there are no confidentiality issues, analogous to the dataset that must be kept private. The existing dataset may be genuine but public already, or it may be synthetic. The weights are importance sampling weights, but to protect privacy, they are regularized and have noise added. The weights allow statistical queries to be answered approximately while provably guaranteeing differential privacy. We derive an expression for the asymptotic variance of the approximate answers. Experiments show that the new mechanism performs well even when the privacy budget is small, and when the public and private datasets are drawn from different populations.", "paper_title": "Differential privacy based on importance weighting", "paper_id": "WOS:000323061200009"}