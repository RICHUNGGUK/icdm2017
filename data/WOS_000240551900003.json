{"auto_keywords": [{"score": 0.05007837343017578, "phrase": "complex_questions"}, {"score": 0.03658862465035158, "phrase": "pourpre"}, {"score": 0.025716674648363212, "phrase": "trec"}, {"score": 0.0046937237797352515, "phrase": "major_driving_force"}, {"score": 0.004483120978156604, "phrase": "language_technologies"}, {"score": 0.004216880725292, "phrase": "machine_output"}, {"score": 0.004152818376630929, "phrase": "preferred_method"}, {"score": 0.0038862297330905836, "phrase": "human_judgments"}, {"score": 0.0038271712887391015, "phrase": "recent_developments"}, {"score": 0.003769006950727621, "phrase": "automatic_evaluation"}, {"score": 0.0037307208089736835, "phrase": "machine_translation"}, {"score": 0.0036928221419965253, "phrase": "document_summarization"}, {"score": 0.0035997452281985465, "phrase": "similar_approach"}, {"score": 0.0032668821950788502, "phrase": "n-gram_co-occurrences"}, {"score": 0.0031682817605340028, "phrase": "human-generated_answer_key"}, {"score": 0.0029196116767956273, "phrase": "manual_determination"}, {"score": 0.002860546817196148, "phrase": "information_\"nugget"}, {"score": 0.002788388542538351, "phrase": "system's_response"}, {"score": 0.002718045523483502, "phrase": "automatic_methods"}, {"score": 0.00267669209359436, "phrase": "system_output"}, {"score": 0.0023197517832705297, "phrase": "qa_tracks"}, {"score": 0.0021928902836049384, "phrase": "official_rankings"}, {"score": 0.002126635713519053, "phrase": "direct_application"}, {"score": 0.0021049977753042253, "phrase": "existing_metrics"}], "paper_keywords": ["question answering", " evaluation"], "paper_abstract": "Evaluation is a major driving force in advancing the state of the art in language technologies. In particular, methods for automatically assessing the quality of machine output is the preferred method for measuring progress, provided that these metrics have been validated against human judgments. Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called POURPRE, an automatic technique for evaluating answers to complex questions based on n-gram co-occurrences between machine output and a human-generated answer key. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information \"nugget\" appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003, TREC 2004, and TREC 2005 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that POURPRE outperforms direct application of existing metrics.", "paper_title": "Methods for automatically evaluating answers to complex questions", "paper_id": "WOS:000240551900003"}