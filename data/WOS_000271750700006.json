{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "structured_prediction"}, {"score": 0.008690891075841286, "phrase": "rl"}, {"score": 0.004752873752686246, "phrase": "reinforcement_learning_task"}, {"score": 0.004631099806015235, "phrase": "structured_prediction_markov_decision_process"}, {"score": 0.004435006247087171, "phrase": "markov_decision_processes"}, {"score": 0.004284100953217494, "phrase": "optimal_policy"}, {"score": 0.004120435930514554, "phrase": "empirical_loss"}, {"score": 0.004014800383343256, "phrase": "supervised_learning_formulation"}, {"score": 0.003945879572348638, "phrase": "reinforcement_learning"}, {"score": 0.003828091961373454, "phrase": "approximate_rl_methods"}, {"score": 0.0037138072936226, "phrase": "proposed_model"}, {"score": 0.0036817833826578395, "phrase": "weak_assumptions"}, {"score": 0.003571850980647625, "phrase": "structured_prediction_problem"}, {"score": 0.003510507340837656, "phrase": "supervision_process"}, {"score": 0.003332704619499476, "phrase": "loss_functions"}, {"score": 0.0032896745636186824, "phrase": "data_encoding"}, {"score": 0.00319141242685261, "phrase": "optimal_policies"}, {"score": 0.003029722317983915, "phrase": "large_range"}, {"score": 0.003003579442221871, "phrase": "structured_prediction_problems"}, {"score": 0.0028267701593809877, "phrase": "complex_and_large-scale_real-world_problems"}, {"score": 0.0027068691342344545, "phrase": "first_one"}, {"score": 0.0026259694526633037, "phrase": "classical_sequence_prediction_benchmarks"}, {"score": 0.0025696647013416863, "phrase": "state-of-the-art_sp_algorithms"}, {"score": 0.0024928553725344933, "phrase": "tree_transformation_problem"}, {"score": 0.0024078737745407614, "phrase": "complex_instance"}, {"score": 0.0023767561522363367, "phrase": "general_labeled_tree_mapping_problem"}, {"score": 0.0023257824499250653, "phrase": "rl_exploration"}, {"score": 0.002266051712795851, "phrase": "successful_results"}, {"score": 0.002236762786982225, "phrase": "challenging_task"}, {"score": 0.002188784900536406, "phrase": "clear_confirmation"}, {"score": 0.0021233361369222344, "phrase": "large_size"}, {"score": 0.0021049977753042253, "phrase": "complex_structured_prediction_problems"}], "paper_keywords": ["Structured prediction", " Reinforcement learning", " Sequence labeling", " Tree transformation", " HTML to XML"], "paper_abstract": "We formalize the problem of Structured Prediction as a Reinforcement Learning task. We first define a Structured Prediction Markov Decision Process (SP-MDP), an instantiation of Markov Decision Processes for Structured Prediction and show that learning an optimal policy for this SP-MDP is equivalent to minimizing the empirical loss. This link between the supervised learning formulation of structured prediction and reinforcement learning (RL) allows us to use approximate RL methods for learning the policy. The proposed model makes weak assumptions both on the nature of the Structured Prediction problem and on the supervision process. It does not make any assumption on the decomposition of loss functions, on data encoding, or on the availability of optimal policies for training. It then allows us to cope with a large range of structured prediction problems. Besides, it scales well and can be used for solving both complex and large-scale real-world problems. We describe two series of experiments. The first one provides an analysis of RL on classical sequence prediction benchmarks and compares our approach with state-of-the-art SP algorithms. The second one introduces a tree transformation problem where most previous models fail. This is a complex instance of the general labeled tree mapping problem. We show that RL exploration is effective and leads to successful results on this challenging task. This is a clear confirmation that RL could be used for large size and complex structured prediction problems.", "paper_title": "Structured prediction with reinforcement learning", "paper_id": "WOS:000271750700006"}