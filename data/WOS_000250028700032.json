{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "feature_points"}, {"score": 0.004743594110762319, "phrase": "fundamental_task"}, {"score": 0.004673290248999786, "phrase": "non-rigid_articulated_motion"}, {"score": 0.004581162468419365, "phrase": "unstructured_feature_points"}, {"score": 0.004424268136268051, "phrase": "feature_correspondence"}, {"score": 0.0043804319008535555, "phrase": "motion_estimation"}, {"score": 0.004230382915464492, "phrase": "high-dimensional_configuration_spaces"}, {"score": 0.0040449598280454645, "phrase": "general_model-based_dynamic_point_matching_algorithm"}, {"score": 0.003984968827278219, "phrase": "freeform_non-rigid_articulated_movements"}, {"score": 0.0038676325043529524, "phrase": "sparse_feature_points"}, {"score": 0.0037724857647796813, "phrase": "key-frame-based_self-initialising_hierarchial_segmental_matching"}, {"score": 0.0034834098902271626, "phrase": "data_noise"}, {"score": 0.0034317185678527672, "phrase": "dynamic_scheme"}, {"score": 0.003397683082821229, "phrase": "motion_verification"}, {"score": 0.0033639840192975835, "phrase": "dynamic_keyframe-shift_identification"}, {"score": 0.0033306180744375616, "phrase": "backward_parent-segment_correction"}, {"score": 0.0031061129850110994, "phrase": "segment-based_spatial_matching"}, {"score": 0.002925728709297618, "phrase": "single_frame"}, {"score": 0.0028966970007550798, "phrase": "performance_evaluation"}, {"score": 0.002797329638474873, "phrase": "empirical_analyses"}, {"score": 0.002769568481822277, "phrase": "synthetic_data"}, {"score": 0.0027148677103585985, "phrase": "motion_capture_data"}, {"score": 0.0026745503978347143, "phrase": "common_articulated_motion"}, {"score": 0.0025827837420330816, "phrase": "feature-point_identification"}, {"score": 0.0024941578231560055, "phrase": "manual_intervention"}, {"score": 0.0024571101010891347, "phrase": "buffered_real-time"}, {"score": 0.0023846534541336326, "phrase": "proposed_algorithm"}, {"score": 0.0023143284925318916, "phrase": "feature-based_real-time_reconstruction_tasks"}, {"score": 0.002234893735225432, "phrase": "articulated_motion"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["non-rigid articulated motion", " point pattern matching", " non-rigid pose estimation", " motion tracking and object recognition"], "paper_abstract": "A fundamental task of reconstructing non-rigid articulated motion from sequences of unstructured feature points is to solve the problem of feature correspondence and motion estimation. This problem is challenging in high-dimensional configuration spaces. In this paper, we propose a general model-based dynamic point matching algorithm to reconstruct freeform non-rigid articulated movements from data presented solely by sparse feature points. The algorithm integrates key-frame-based self-initialising hierarchial segmental matching with inter-frame tracking to achieve computation effectiveness and robustness in the presence of data noise. A dynamic scheme of motion verification, dynamic keyframe-shift identification and backward parent-segment correction, incorporating temporal coherency embedded in inter-frames, is employed to enhance the segment-based spatial matching. Such a spatial-temporal approach ultimately reduces the ambiguity of identification inherent in a single frame. Performance evaluation is provided by a series of empirical analyses using synthetic data. Testing on motion capture data for a common articulated motion, namely human motion, gave feature-point identification and matching without the need for manual intervention, in buffered real-time. These results demonstrate the proposed algorithm to be a candidate for feature-based real-time reconstruction tasks involving self-resuming tracking for articulated motion. (C) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Articulated motion reconstruction from feature points", "paper_id": "WOS:000250028700032"}