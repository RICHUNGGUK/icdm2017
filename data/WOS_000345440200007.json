{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "big_data"}, {"score": 0.015595500194448667, "phrase": "uncertainty_distribution"}, {"score": 0.004358550946735829, "phrase": "data_sets"}, {"score": 0.004134137523409649, "phrase": "potential_outcomes"}, {"score": 0.004068302313449334, "phrase": "rapid_developments"}, {"score": 0.004035777314816881, "phrase": "data_collection"}, {"score": 0.004003511299646736, "phrase": "distribution_storage_technologies"}, {"score": 0.003908246778916674, "phrase": "bigger-than-ever_problem"}, {"score": 0.003694652993546994, "phrase": "big_data_research"}, {"score": 0.0035635256137029592, "phrase": "parallel_sampling_method"}, {"score": 0.0035208552733545463, "phrase": "hyper_surface"}, {"score": 0.003423261046279463, "phrase": "pshs"}, {"score": 0.0033552055241931346, "phrase": "universal_concept"}, {"score": 0.0033283624928018177, "phrase": "minimal_consistent_subset"}, {"score": 0.0033017646890192215, "phrase": "mcs"}, {"score": 0.0032621876512517398, "phrase": "hyper_surface_classification"}, {"score": 0.0027778947555117243, "phrase": "mcs._pshs"}, {"score": 0.002722634472231842, "phrase": "mapreduce_framework"}, {"score": 0.002668470532683947, "phrase": "current_and_powerful_parallel_programming_technique"}, {"score": 0.0025224615049571427, "phrase": "real_world_data"}, {"score": 0.002502264043067568, "phrase": "uci_repository"}, {"score": 0.0024822279007111255, "phrase": "synthetic_data"}, {"score": 0.002374855916752648, "phrase": "identical_distribution"}, {"score": 0.002299689297293321, "phrase": "inherent_structure"}, {"score": 0.0022268964676199292, "phrase": "evaluation_criterions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Fuzzy boundary set", " Uncertainty", " Minimal consistent subset", " Sampling", " MapReduce"], "paper_abstract": "Data are inherently uncertain in most applications. Uncertainty is encountered when an experiment such as sampling is to proceed, the result of which is not known to us while leading to variety of potential outcomes. With the rapid developments of data collection and distribution storage technologies, big data have become a bigger-than-ever problem. And dealing with big data with uncertainty distribution is one of the most important issues of big data research. In this paper, we propose a Parallel Sampling method based on Hyper Surface for big data with uncertainty distribution, namely PSHS, which adopts a universal concept of Minimal Consistent Subset (MCS) of Hyper Surface Classification (HSC). Our inspiration for handling uncertainties in sampling from big data depends on (1) the inherent structure of the original sample set is uncertain for us, (2) boundary set formed of all the possible separating hyper surfaces is a fuzzy set and (3) the uncertainty of elements in MCS. PSHS is implemented based on MapReduce framework, which is a current and powerful parallel programming technique used in many fields. Experiments have been carried out on several data sets including real world data from UCI repository and synthetic data. The results show that our algorithm shrinks data sets while maintaining identical distribution, which is useful for obtaining the inherent structure of the data sets. Furthermore, the evaluation criterions of speedup, scaleup and sizeup validate its efficiency. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Parallel sampling from big data with uncertainty distribution", "paper_id": "WOS:000345440200007"}