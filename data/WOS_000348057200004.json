{"auto_keywords": [{"score": 0.03101954444927483, "phrase": "kinect"}, {"score": 0.006244660566993056, "phrase": "rgb-d"}, {"score": 0.00481495049065317, "phrase": "borrowing_information"}, {"score": 0.004778594263962987, "phrase": "video_modalities"}, {"score": 0.004724571550328985, "phrase": "recent_advances"}, {"score": 0.004688894532233354, "phrase": "imaging_devices"}, {"score": 0.004497410940849537, "phrase": "video_content_analysis"}, {"score": 0.004429727701202369, "phrase": "next-generation_cameras"}, {"score": 0.004248782702870078, "phrase": "diverse_information"}, {"score": 0.004075198669927074, "phrase": "yielded_multimodal_videos"}, {"score": 0.003983512316698646, "phrase": "related_applications"}, {"score": 0.0038644521996539466, "phrase": "emerging_cameras"}, {"score": 0.0038062581748678245, "phrase": "short_effective_distances"}, {"score": 0.0037774893672602506, "phrase": "expensive_costs"}, {"score": 0.00373474182086791, "phrase": "long_response_time"}, {"score": 0.0035415392642831616, "phrase": "practical_use"}, {"score": 0.0034226302212225206, "phrase": "alternative_scenario"}, {"score": 0.003245520963944678, "phrase": "human_actions"}, {"score": 0.0031010029460927864, "phrase": "action_recognition"}, {"score": 0.0030775481952377017, "phrase": "rgb_videos"}, {"score": 0.002841701708055512, "phrase": "surveillance_system"}, {"score": 0.00265394824073552, "phrase": "depth_maps"}, {"score": 0.002623881637290192, "phrase": "skeleton_data"}, {"score": 0.0025453546203590364, "phrase": "proposed_approach"}, {"score": 0.0025069746344846397, "phrase": "interdatabase_variations"}, {"score": 0.0024411935317269705, "phrase": "visual_knowledge"}, {"score": 0.0024227171881502636, "phrase": "different_video_modalities"}, {"score": 0.002350196395175188, "phrase": "rgb_representation"}, {"score": 0.0022972306948217548, "phrase": "borrowed_depth_and_skeleton_features"}, {"score": 0.0022284577808744316, "phrase": "five_benchmark_data_sets"}, {"score": 0.0021782296784218923, "phrase": "promising_results"}, {"score": 0.0021453734588569823, "phrase": "borrowed_information"}, {"score": 0.002121056250381627, "phrase": "remarkable_boost"}, {"score": 0.0021049977753042253, "phrase": "recognition_accuracy"}], "paper_keywords": ["Action recognition", " next-generation cameras", " transfer learning", " feature borrowing"], "paper_abstract": "The recent advances in imaging devices have opened the opportunity of better solving the tasks of video content analysis and understanding. Next-generation cameras, such as the depth or binocular cameras, capture diverse information, and complement the conventional 2D RGB cameras. Thus, investigating the yielded multimodal videos generally facilitates the accomplishment of related applications. However, the limitations of the emerging cameras, such as short effective distances, expensive costs, or long response time, degrade their applicability, and currently make these devices not online accessible in practical use. In this paper, we provide an alternative scenario to address this problem, and illustrate it with the task of recognizing human actions. In particular, we aim at improving the accuracy of action recognition in RGB videos with the aid of one additional RGB-D camera. Since RGB-D cameras, such as Kinect, are typically not applicable in a surveillance system due to its short effective distance, we instead offline collect a database, in which not only the RGB videos but also the depth maps and the skeleton data of actions are available jointly. The proposed approach can adapt the interdatabase variations, and activate the borrowing of visual knowledge across different video modalities. Each action to be recognized in RGB representation is then augmented with the borrowed depth and skeleton features. Our approach is comprehensively evaluated on five benchmark data sets of action recognition. The promising results manifest that the borrowed information leads to remarkable boost in recognition accuracy.", "paper_title": "Robust Action Recognition via Borrowing Information Across Video Modalities", "paper_id": "WOS:000348057200004"}