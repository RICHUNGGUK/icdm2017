{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "rapid_model_adaptation_technique"}, {"score": 0.04953663923501101, "phrase": "emotional_speech_recognition"}, {"score": 0.004706453355638515, "phrase": "style_estimation_based"}, {"score": 0.004653120760786245, "phrase": "multiple-regression_hmm."}, {"score": 0.004247560712327667, "phrase": "paralinguistic_information"}, {"score": 0.004128192012388041, "phrase": "linguistic_information"}, {"score": 0.004058180509184539, "phrase": "speech_signals"}, {"score": 0.003899385109646589, "phrase": "style_estimation"}, {"score": 0.0038551626276406175, "phrase": "style_adaptation"}, {"score": 0.003789763907758991, "phrase": "multiple-regression_hmm"}, {"score": 0.003641433168173135, "phrase": "mrhmm"}, {"score": 0.0035796471115594553, "phrase": "mean_parameters"}, {"score": 0.0035189057079817285, "phrase": "output_probability_density_function"}, {"score": 0.003419943984098415, "phrase": "low-dimensional_parameter_vector"}, {"score": 0.0031573626285069157, "phrase": "explanatory_variables"}, {"score": 0.0031037644581516973, "phrase": "multiple_regression"}, {"score": 0.003051073364818384, "phrase": "recognition_process"}, {"score": 0.002931569771634119, "phrase": "first_stage"}, {"score": 0.002881793492919646, "phrase": "style_vector"}, {"score": 0.0028167336373711494, "phrase": "emotional_expression_category"}, {"score": 0.002690975370565776, "phrase": "input_speech"}, {"score": 0.0026152363154073707, "phrase": "sentence-by-sentence_basis"}, {"score": 0.0025416235470290286, "phrase": "acoustic_models"}, {"score": 0.0024700776739384977, "phrase": "estimated_style_vector"}, {"score": 0.002414290278501595, "phrase": "standard_hmm-based_speech_recognition"}, {"score": 0.0023463203637929466, "phrase": "second_stage"}, {"score": 0.00224151735457093, "phrase": "proposed_technique"}, {"score": 0.002178400923036365, "phrase": "simulated_emotional_speech"}, {"score": 0.0021291870676682406, "phrase": "professional_narrators"}, {"score": 0.0021049977753042253, "phrase": "non-professional_speakers"}], "paper_keywords": ["emotional speech", " speaking style", " style estimation", " multiple-regression HMM", " style adaptation", " speaker adaptation"], "paper_abstract": "In this paper, we propose a rapid model adaptation technique for emotional speech recognition which enables us to extract paralinguistic information a, well as linguistic information contained in speech signals. This technique is based on style estimation and style adaptation using a multiple-regression HMM (MRHMM). In the MRHMM, the mean parameters of the Output probability density function are controlled by a low-dimensional parameter vector. called a style vector, which corresponds to a set of the explanatory variables of the multiple regression. The recognition process consists of two stages. In the first stage, the style vector that represents the emotional expression category and the intensity of its expressiveness for the input speech is estimated on a sentence-by-sentence basis. Next, the acoustic models are adapted using the estimated style vector, and then standard HMM-based speech recognition is performed in the second stage. We assess the performance of the proposed technique in the recognition of simulated emotional speech uttered by both professional narrators and non-professional speakers.", "paper_title": "A Rapid Model Adaptation Technique for Emotional Speech Recognition with Style Estimation Based on Multiple-Regression HMM", "paper_id": "WOS:000274506200014"}