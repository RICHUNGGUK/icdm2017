{"auto_keywords": [{"score": 0.04860525980080879, "phrase": "face_recognition"}, {"score": 0.0429291679538017, "phrase": "linear_combination"}, {"score": 0.04256195279736391, "phrase": "training_samples"}, {"score": 0.03298871349453001, "phrase": "ecnr"}, {"score": 0.031449918411730886, "phrase": "generic_subjects"}, {"score": 0.031042443925142032, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "robust_single-sample_face_recognition"}, {"score": 0.004543352284098364, "phrase": "dominant_techniques"}, {"score": 0.004383852039913171, "phrase": "feature_space"}, {"score": 0.004211069233133584, "phrase": "probe_image"}, {"score": 0.003973398317120165, "phrase": "real-world_application"}, {"score": 0.0036173729405604674, "phrase": "insufficient_training_samples"}, {"score": 0.0035532533960012298, "phrase": "poor_generalization_ability"}, {"score": 0.003505908076430666, "phrase": "existing_state-of-the-art_appearance-based_methods"}, {"score": 0.0032492519841546682, "phrase": "collaborative_representation-based_classification"}, {"score": 0.003149094460617212, "phrase": "collaborative_neighbor_representation"}, {"score": 0.0030520148188164084, "phrase": "representational_ability"}, {"score": 0.0029978863469038914, "phrase": "intrapersonal_variation"}, {"score": 0.0028411768240129585, "phrase": "intrapersonal_variations"}, {"score": 0.0027659126653647712, "phrase": "training_dictionary"}, {"score": 0.0027047136845332917, "phrase": "test_sample"}, {"score": 0.002644865209801606, "phrase": "optimal_neighbor_bases"}, {"score": 0.0026095920086044145, "phrase": "single_class-specific_training_dictionary"}, {"score": 0.0025178072071268534, "phrase": "auxiliary_generic_subjects"}, {"score": 0.0024842242921857705, "phrase": "multiple_samples"}, {"score": 0.002418393046947528, "phrase": "possible_variations"}, {"score": 0.0023333173799963795, "phrase": "resulting_coding_scheme"}, {"score": 0.002271476019266231, "phrase": "original_bases"}, {"score": 0.0021915573326125428, "phrase": "real-world_face_data_sets"}, {"score": 0.0021049977753042253, "phrase": "single-sample_face_recognition_tasks"}], "paper_keywords": ["Single-sample face recognition", " Linear representation", " Sparse representation", " Classification", " Linear regression"], "paper_abstract": "Appearance-based methods in face recognition have become one of the dominant techniques in recent years. Several studies have demonstrated that feature space is no longer critical in face recognition when the probe image can be approximated by a linear combination of training samples. However, in real-world application of face recognition, there are limited or even a single training sample per subject available. The availability of insufficient training samples often results in poor generalization ability for the existing state-of-the-art appearance-based methods. In this work, to address single-sample per person problem, we extend collaborative representation-based classification using minimization approach, named extended collaborative neighbor representation (ECNR) that exploits the representational ability of training samples and intrapersonal variation from generic subjects. The proposed method is based on the assumption that intrapersonal variations from generic subjects are sharable across training dictionary. ECNR expresses a test sample as the linear combination of optimal neighbor bases from the single class-specific training dictionary and the intrapersonal variant bases from an auxiliary generic subjects dictionary with multiple samples per subject to represent the possible variations in testing and training samples. The resulting coding scheme is sparse in terms of original bases and is computationally efficient. Experiments on real-world face data sets have demonstrated the usefulness of the proposed method in single-sample face recognition tasks.", "paper_title": "Extended collaborative neighbor representation for robust single-sample face recognition", "paper_id": "WOS:000361397500017"}