{"auto_keywords": [{"score": 0.04916387935172275, "phrase": "sparse_representation"}, {"score": 0.011398149600675691, "phrase": "reconstruction_error"}, {"score": 0.010435409021063343, "phrase": "class-specific_subdictionary"}, {"score": 0.00970113879991039, "phrase": "discriminative_fidelity"}, {"score": 0.00481495049065317, "phrase": "learning_discriminative_dictionary_for_group_sparse_representation"}, {"score": 0.004614745158543967, "phrase": "object_recognition_applications"}, {"score": 0.00445712058827745, "phrase": "key_issue"}, {"score": 0.0043718794256931435, "phrase": "popular_method"}, {"score": 0.004222515162635615, "phrase": "sparsity_measurement"}, {"score": 0.004190019394361629, "phrase": "representation_coefficients"}, {"score": 0.00415777266387528, "phrase": "dictionary_learning"}, {"score": 0.0039085400187964196, "phrase": "learned_dictionary"}, {"score": 0.0038189638340666936, "phrase": "multisubspaces_structural_information"}, {"score": 0.0036884198249734863, "phrase": "learned_subdictionary"}, {"score": 0.0035899674590142653, "phrase": "common_atoms"}, {"score": 0.0035212516951180946, "phrase": "discriminative_ability"}, {"score": 0.0033616344666486725, "phrase": "new_dictionary"}, {"score": 0.0032718761046588835, "phrase": "image_classification"}, {"score": 0.00292477450010107, "phrase": "weighted_group_sparse_constraint"}, {"score": 0.002675797331220082, "phrase": "corresponding_class"}, {"score": 0.002644920521740564, "phrase": "weighted_group_sparse_constraint_term"}, {"score": 0.0025942468561537682, "phrase": "structural_information"}, {"score": 0.0025347150832077175, "phrase": "subdictionary_incoherence_term"}, {"score": 0.0022222152430170254, "phrase": "extensive_experiments"}, {"score": 0.0021544587602892466, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["Dictionary learning", " group sparse", " incoherence", " classification"], "paper_abstract": "In recent years, sparse representation has been widely used in object recognition applications. How to learn the dictionary is a key issue to sparse representation. A popular method is to use l(1) norm as the sparsity measurement of representation coefficients for dictionary learning. However, the l1 norm treats each atom in the dictionary independently, so the learned dictionary cannot well capture the multisubspaces structural information of the data. In addition, the learned subdictionary for each class usually shares some common atoms, which weakens the discriminative ability of the reconstruction error of each subdictionary. This paper presents a new dictionary learning model to improve sparse representation for image classification, which targets at learning a class-specific subdictionary for each class and a common subdictionary shared by all classes. The model is composed of a discriminative fidelity, a weighted group sparse constraint, and a subdictionary incoherence term. The discriminative fidelity encourages each class-specific subdictionary to sparsely represent the samples in the corresponding class. The weighted group sparse constraint term aims at capturing the structural information of the data. The subdictionary incoherence term is to make all subdictionaries independent as much as possible. Because the common subdictionary represents features shared by all classes, we only use the reconstruction error of each class-specific subdictionary for classification. Extensive experiments are conducted on several public image databases, and the experimental results demonstrate the power of the proposed method, compared with the state-of-the-arts.", "paper_title": "Learning Discriminative Dictionary for Group Sparse Representation", "paper_id": "WOS:000348366100006"}