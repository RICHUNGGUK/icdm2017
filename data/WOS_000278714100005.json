{"auto_keywords": [{"score": 0.04968898347890437, "phrase": "evolutionary_algorithms"}, {"score": 0.01409203852589941, "phrase": "different_evolutionary_approaches"}, {"score": 0.011645722870906141, "phrase": "overall_search_process"}, {"score": 0.00481495049065317, "phrase": "hybridization_strategies"}, {"score": 0.004698229304089357, "phrase": "powerful_optimization_techniques"}, {"score": 0.0045284072672385975, "phrase": "complex_mathematical_functions"}, {"score": 0.004491506786670695, "phrase": "real-world_applications"}, {"score": 0.0044005599565517875, "phrase": "performance_improvements"}, {"score": 0.004054745478848129, "phrase": "evolutionary_approaches"}, {"score": 0.0037055370870874484, "phrase": "regulatory_component"}, {"score": 0.0036008372531664726, "phrase": "evolutionary_approach"}, {"score": 0.0033449479263613848, "phrase": "on-line_adaptation"}, {"score": 0.003145609726536791, "phrase": "reinforcement_learning"}, {"score": 0.003119979714649394, "phrase": "rl"}, {"score": 0.0028046593475268174, "phrase": "state-of-the-art_rl_algorithms"}, {"score": 0.0027817718657579325, "phrase": "q-learning"}, {"score": 0.0025945450101602825, "phrase": "best-response_mixed_strategy"}, {"score": 0.0023131855385489764, "phrase": "rl_control_mechanisms"}, {"score": 0.0022849068677290836, "phrase": "optimal_patterns"}, {"score": 0.00219311232069446, "phrase": "proposed_functions"}, {"score": 0.0021049977753042253, "phrase": "individual_and_non_rl_hybrid_algorithms"}], "paper_keywords": ["Multiple offspring sampling", " reinforcement learning", " Q-Learning", " hybrid evolutionary algorithms"], "paper_abstract": "Evolutionary Algorithms are powerful optimization techniques which have been applied to many different problems, from complex mathematical functions to real-world applications. Some studies report performance improvements through the combination of different evolutionary approaches within the same hybrid algorithm. However, the mechanisms used to control this combination of evolutionary approaches are not as satisfactory as would be desirable. In most cases, there is no feedback from the algorithm nor any regulatory component that modifies the participation of each evolutionary approach in the overall search process. In some cases, the algorithm makes use of some information for an on-line adaptation of the participation of each algorithm. In this paper, the use of Reinforcement Learning (RL) is proposed as a mechanism to control how the different evolutionary approaches contribute to the overall search process. In particular, three learning policies based on one of the state-of-the-art RL algorithms, Q-Learning, have been considered and used to control the participation of each algorithm by learning the best-response mixed strategy. To test this approach, a benchmark made up of six large-scale (500 dimensions) continuous optimization functions has been considered. The experimentation carried out has proved that RL control mechanisms successfully learn optimal patterns for the combination of Evolutionary Algorithms in most of the proposed functions, being able to improve the performance of both individual and non RL hybrid algorithms.", "paper_title": "Learning hybridization strategies in evolutionary algorithms", "paper_id": "WOS:000278714100005"}