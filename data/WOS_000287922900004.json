{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "cross-language"}, {"score": 0.025904690795985716, "phrase": "cl-cng"}, {"score": 0.0046390482426405324, "phrase": "automatic_identification"}, {"score": 0.004519736937243476, "phrase": "multilingual_setting"}, {"score": 0.004403480614319231, "phrase": "suspicious_document"}, {"score": 0.003737934295442975, "phrase": "monolingual_plagiarism_detection"}, {"score": 0.0034437574674180365, "phrase": "cross-language_similarity"}, {"score": 0.003102488033561956, "phrase": "realistic_scale"}, {"score": 0.002977835845437357, "phrase": "corpora_jrc-acquis"}, {"score": 0.0029558365420125385, "phrase": "wikipedia"}, {"score": 0.0028903278985330117, "phrase": "test_document"}, {"score": 0.0028795712616923462, "phrase": "highly_similar_documents"}, {"score": 0.0028053842615436706, "phrase": "six_languages"}, {"score": 0.0027951404116759557, "phrase": "english"}, {"score": 0.002774237703583632, "phrase": "german"}, {"score": 0.002753601085823231, "phrase": "spanish"}, {"score": 0.0027331923960147624, "phrase": "french"}, {"score": 0.002712806977633262, "phrase": "dutch"}, {"score": 0.00268265913482356, "phrase": "polish"}, {"score": 0.0025844090835070986, "phrase": "ranking_tasks"}, {"score": 0.0023719332915829268, "phrase": "best_choice"}, {"score": 0.002251227801482381, "phrase": "cl-esa"}, {"score": 0.0021687806803474367, "phrase": "arbitrary_pairs"}, {"score": 0.002136651746284283, "phrase": "cl-asa"}, {"score": 0.0021049977753042253, "phrase": "\"exact\"_translations"}], "paper_keywords": ["Cross-language", " Plagiarism detection", " Similarity", " Retrieval model", " Evaluation"], "paper_abstract": "Cross-language plagiarism detection deals with the automatic identification and extraction of plagiarism in a multilingual setting. In this setting, a suspicious document is given, and the task is to retrieve all sections from the document that originate from a large, multilingual document collection. Our contributions in this field are as follows: (1) a comprehensive retrieval process for cross-language plagiarism detection is introduced, highlighting the differences to monolingual plagiarism detection, (2) state-of-the-art solutions for two important subtasks are reviewed, (3) retrieval models for the assessment of cross-language similarity are surveyed, and, (4) the three models CL-CNG, CL-ESA and CL-ASA are compared. Our evaluation is of realistic scale: it relies on 120,000 test documents which are selected from the corpora JRC-Acquis and Wikipedia, so that for each test document highly similar documents are available in all of the six languages English, German, Spanish, French, Dutch, and Polish. The models are employed in a series of ranking tasks, and more than 100 million similarities are computed with each model. The results of our evaluation indicate that CL-CNG, despite its simple approach, is the best choice to rank and compare texts across languages if they are syntactically related. CL-ESA almost matches the performance of CL-CNG, but on arbitrary pairs of languages. CL-ASA works best on \"exact\" translations but does not generalize well.", "paper_title": "Cross-language plagiarism detection", "paper_id": "WOS:000287922900004"}