{"auto_keywords": [{"score": 0.02621037288209389, "phrase": "ffnn"}, {"score": 0.00481495049065317, "phrase": "neuro-wavelet_technique"}, {"score": 0.004623608221548592, "phrase": "discrete_wavelet_transform_data_pre-processing_method"}, {"score": 0.00457696724030363, "phrase": "neural_network-based_successive-station_monthly_streamflow_prediction_models"}, {"score": 0.004328689762198895, "phrase": "coruh_river"}, {"score": 0.004285049190251076, "phrase": "turkey"}, {"score": 0.004177724522109952, "phrase": "eight_different_single-step-ahead_neural_monthly_streamflow_prediction_models"}, {"score": 0.0036988415150699945, "phrase": "best_structure"}, {"score": 0.003551694229680741, "phrase": "input_time_series"}, {"score": 0.003393115177189967, "phrase": "different_resolution_modes"}, {"score": 0.003019189898170698, "phrase": "square_error_and_nash-sutcliffe_efficiency_measures"}, {"score": 0.0028697077879305064, "phrase": "ffnn_and_nw_models"}, {"score": 0.002769483748975581, "phrase": "successive-station_prediction_strategy"}, {"score": 0.0027000395818027366, "phrase": "upstream-downstream_records"}, {"score": 0.002632332114727894, "phrase": "lagged_prediction_effect"}, {"score": 0.0026057256900479026, "phrase": "single-station_runoff-runoff_models"}, {"score": 0.00254044381202349, "phrase": "nw"}, {"score": 0.002342048999008582, "phrase": "powerful_tool"}, {"score": 0.002294929467537814, "phrase": "non-stationary_feature"}, {"score": 0.002260211578368138, "phrase": "successive-station_streamflow_process"}, {"score": 0.002226017732566121, "phrase": "comparative_performance_analysis"}, {"score": 0.002203509076886664, "phrase": "different_combinations"}, {"score": 0.0021591707887653865, "phrase": "highest_improvement"}, {"score": 0.0021049977753042253, "phrase": "simultaneous_lag-time"}], "paper_keywords": ["Feed-forward neural networks", " streamflow prediction", " successive-station forecasting", " wavelet transform"], "paper_abstract": "This study investigates the effect of discrete wavelet transform data pre-processing method on neural network-based successive-station monthly streamflow prediction models. For this aim, using data from two successive gauging stations on Coruh River, Turkey, we initially developed eight different single-step-ahead neural monthly streamflow prediction models. Typical three-layer feed-forward (FFNN) topology, trained with Levenberg-Marquardt (LM) algorithm, has been employed to develop the best structure of each model. Then, the input time series of each model were decomposed into subseries at different resolution modes using Daubechies (db4) wavelet function. At the next step, eight hybrid neuro-wavelet (NW) models were generated using the subseries of each model. Ultimately, root mean square error and Nash-Sutcliffe efficiency measures have been used to compare the performance of both FFNN and NW models. The results indicated that the successive-station prediction strategy using a pair of upstream-downstream records tends to decrease the lagged prediction effect of single-station runoff-runoff models. Higher performances of NW models compared to those of FFNN in all combinations demonstrated that the db4 wavelet transform function is a powerful tool to capture the non-stationary feature of the successive-station streamflow process. The comparative performance analysis among different combinations showed that the highest improvement for FFNN occurs when simultaneous lag-time is considered for both stations.", "paper_title": "Successive-station monthly streamflow prediction using neuro-wavelet technique", "paper_id": "WOS:000345279300001"}