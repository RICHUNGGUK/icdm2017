{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "spatial_structure"}, {"score": 0.0041287394089001405, "phrase": "non-cooperative_autonomous_system"}, {"score": 0.003750260412839128, "phrase": "new_agent_model"}, {"score": 0.003643607569483598, "phrase": "ipd_game"}, {"score": 0.00334144503360907, "phrase": "coded_parameters"}, {"score": 0.0032777653153451265, "phrase": "reinforcement_learning"}, {"score": 0.002730023736069673, "phrase": "empirical_study"}, {"score": 0.0022302306304023602, "phrase": "mutual_cooperation"}, {"score": 0.0021049977753042253, "phrase": "learning_process"}], "paper_keywords": ["game theory", " prisoner's dilemma", " small world network"], "paper_abstract": "In this study, we explore the roles of learning and evolution in a non-cooperative autonomous system through a spatial IPD (Iterated Prisoner's Dilemma) game. First, we propose a new agent model playing the IPD game; the game has a gene of the coded parameters of reinforcement learning. The agents evolve and learn during the course of the game. Second, we report an empirical study. In our simulation, we observe that the spatial structure affects learning and evolution. Learning is not effective for achieving mutual cooperation except under certain special conditions. The learning process depends on the spatial structure.", "paper_title": "Learning and evolution affected by spatial structure", "paper_id": "WOS:000240091500069"}