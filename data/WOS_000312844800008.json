{"auto_keywords": [{"score": 0.040592362494735586, "phrase": "learning_mechanism"}, {"score": 0.00481495049065317, "phrase": "success_failure_maps"}, {"score": 0.004747429622531595, "phrase": "humanoid_robot_walking"}, {"score": 0.004680851157467745, "phrase": "human_brain"}, {"score": 0.004566567411073082, "phrase": "flexible_and_adaptive_way"}, {"score": 0.004518442937844318, "phrase": "novel_stimulus"}, {"score": 0.004439354841093929, "phrase": "orbitofrontal_cortex"}, {"score": 0.004392565073466714, "phrase": "key_reward_structure"}, {"score": 0.004315670715009727, "phrase": "neurobiological_studies"}, {"score": 0.004255121395629569, "phrase": "anterior_cingulate_cortex"}, {"score": 0.004121960535174029, "phrase": "repeated_mistakes"}, {"score": 0.004064117897000019, "phrase": "vigilance_threshold"}, {"score": 0.0036553070575015344, "phrase": "important_role"}, {"score": 0.0033937776228549557, "phrase": "neurological_properties"}, {"score": 0.0033698643928647726, "phrase": "promising_inspirations"}, {"score": 0.003346119094502001, "phrase": "robot_learning"}, {"score": 0.0031286936417685178, "phrase": "negative_and_positive_feedback"}, {"score": 0.0029253546782349875, "phrase": "evaluation_phase"}, {"score": 0.0028740705307540343, "phrase": "kohonen_self-organizing_map_technique"}, {"score": 0.0028037753646260937, "phrase": "decision_making"}, {"score": 0.0027546167254869493, "phrase": "early_warning_mechanism"}, {"score": 0.002530359917257626, "phrase": "success_map"}, {"score": 0.0024947959375982614, "phrase": "adaptive_reward"}, {"score": 0.0024597305738431226, "phrase": "learned_task"}, {"score": 0.00232431758517087, "phrase": "nao_humanoid_robot"}, {"score": 0.0022835460368487233, "phrase": "bioinspired_neural_controller"}, {"score": 0.00225144317399471, "phrase": "central_pattern_generator"}, {"score": 0.0022276618873985445, "phrase": "learning_system"}, {"score": 0.0022041312401435346, "phrase": "oscillation_frequency"}, {"score": 0.002180848602315088, "phrase": "motor_neuron_gain"}, {"score": 0.0021049977753042253, "phrase": "flat_and_sloped_terrain"}], "paper_keywords": ["Experience-based learning mechanism", " humanoid learning", " humanoid robot walking", " neurorobotics"], "paper_abstract": "In the human brain, rewards are encoded in a flexible and adaptive way after each novel stimulus. Neurons of the orbitofrontal cortex are the key reward structure of the brain. Neurobiological studies show that the anterior cingulate cortex of the brain is primarily responsible for avoiding repeated mistakes. According to vigilance threshold, which denotes the tolerance to risks, we can differentiate between a learning mechanism that takes risks and one that averts risks. The tolerance to risk plays an important role in such a learning mechanism. Results have shown the differences in learning capacity between risk-taking and risk-avert behaviors. These neurological properties provide promising inspirations for robot learning based on rewards. In this paper, we propose a learning mechanism that is able to learn from negative and positive feedback with reward coding adaptively. It is composed of two phases: evaluation and decision making. In the evaluation phase, we use a Kohonen self-organizing map technique to represent success and failure. Decision making is based on an early warning mechanism that enables avoiding repeating past mistakes. The behavior to risk is modulated in order to gain experiences for success and for failure. Success map is learned with adaptive reward that qualifies the learned task in order to optimize the efficiency. Our approach is presented with an implementation on the NAO humanoid robot, controlled by a bioinspired neural controller based on a central pattern generator. The learning system adapts the oscillation frequency and the motor neuron gain in pitch and roll in order to walk on flat and sloped terrain, and to switch between them.", "paper_title": "Qualitative Adaptive Reward Learning With Success Failure Maps: Applied to Humanoid Robot Walking", "paper_id": "WOS:000312844800008"}