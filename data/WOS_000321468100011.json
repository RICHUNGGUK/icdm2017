{"auto_keywords": [{"score": 0.03869507098334236, "phrase": "feature_interaction"}, {"score": 0.003956784357357341, "phrase": "main_criteria"}, {"score": 0.0034236045013591437, "phrase": "real-world_problems"}, {"score": 0.0025892330566358503, "phrase": "squared-loss_variant"}, {"score": 0.0024586427394152196, "phrase": "selected_features"}, {"score": 0.002358917661465133, "phrase": "numerical_results"}, {"score": 0.0021049977753042253, "phrase": "non-linear_dependency"}], "paper_keywords": ["feature selection", " l(1)-regularization", " squared-loss mutual information", " density-ratio estimation", " dimensionality reduction"], "paper_abstract": "Feature selection is a technique to screen out less important features. Many existing supervised feature selection algorithms use redundancy and relevancy as the main criteria to select features. However, feature interaction, potentially a key characteristic in real-world problems, has not received much attention. As an attempt to take feature interaction into account, we propose l(1)-LSMI, an l(1)-regularization based algorithm that maximizes a squared-loss variant of mutual information between selected features and outputs. Numerical results show that l(1)-LSMI performs well in handling redundancy, detecting non-linear dependency, and considering feature interaction.", "paper_title": "Feature Selection via l(1)-Penalized Squared-Loss Mutual Information", "paper_id": "WOS:000321468100011"}