{"auto_keywords": [{"score": 0.04819731262075228, "phrase": "eye_movements"}, {"score": 0.04358955587626858, "phrase": "linear_rise"}, {"score": 0.043075933514572326, "phrase": "threshold_models"}, {"score": 0.031682290996456926, "phrase": "target_locations"}, {"score": 0.009665328194081462, "phrase": "decision-making_process"}, {"score": 0.009258946702130816, "phrase": "model_comparison"}, {"score": 0.00481495049065317, "phrase": "integrated_bayesian"}, {"score": 0.004697655699119423, "phrase": "saccadic_eye_movements"}, {"score": 0.004457762896411101, "phrase": "decision-making_processes"}, {"score": 0.004335739075947899, "phrase": "visual_stimulus"}, {"score": 0.0039040344230094164, "phrase": "peripheral_visual_target"}, {"score": 0.0034827622221683756, "phrase": "ssbjects"}, {"score": 0.0034188520484925546, "phrase": "conditional_probabilities"}, {"score": 0.0033768959341690524, "phrase": "methodological_paper"}, {"score": 0.0031649018957020337, "phrase": "previous_models"}, {"score": 0.0028848643090508205, "phrase": "maximum-likelihood_scheme"}, {"score": 0.002779894844207706, "phrase": "log_likelihood_ratios"}, {"score": 0.002728847267539957, "phrase": "integrated_model"}, {"score": 0.0026704721397631938, "phrase": "empirical_saccade_data"}, {"score": 0.0024116163689723354, "phrase": "subject-specific_learning_profiles"}, {"score": 0.0023746421791791717, "phrase": "individual_learning_profiles"}, {"score": 0.0023382335380460304, "phrase": "test_samples"}, {"score": 0.002316656574454208, "phrase": "successfully"}, {"score": 0.0022881951934069903, "phrase": "correct_subject"}, {"score": 0.0022670785913397637, "phrase": "naive_bayes_classifier"}, {"score": 0.002164376714559963, "phrase": "saccadic_decision_making"}, {"score": 0.0021049977753042253, "phrase": "statistical_inference"}], "paper_keywords": ["Saccades", " Decision making", " Reaction time", " Bayesian learning", " Model comparison"], "paper_abstract": "The neurophysiology of eye movements has been studied extensively, and several computational models have been proposed for decision-making processes that underlie the generation of eye movements towards a Visual stimulus in a Situation of uncertainty. One class of models, known as linear rise-to-threshold models, provides an economical, yet broadly applicable, explanation for the observed variability in the latency between the onset of a peripheral Visual target and the saccade towards it. So far, however, these models do not account for the dynamics of learning across a Sequence of stimuli, and they do not apply to situations in which Ssbjects are exposed to events with conditional probabilities. In this methodological paper, we extend the class of linear rise-to-threshold models to address these limitations. Specifically, we reformulate previous models in terms of a generative, hierarchical model. by combining two separate sub-models that account for the interplay between learning of target locations across trials and the decision-making process within trials. We derive a maximum-likelihood scheme for parameter estimation as well as model comparison on the basis of log likelihood ratios. The utility Of the integrated model is demonstrated by applying it to empirical saccade data acquired from three healthy subjects. Model comparison is used (i) to show that eye movements do not only reflect marginal but also conditional probabilities of target locations, and (ii) to reveal subject-specific learning profiles over trials. These individual learning profiles are Sufficiently distinct that test samples can be Successfully mapped onto the correct subject by a naive Bayes classifier. Altogether, our approach extends the class of linear rise-to-threshold models of saccadic decision making, overcomes some of their previous limitations, and enables statistical inference both about learning of target locations across trials and the decision-making process within trials. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Integrated Bayesian models of learning and decision making for saccadic eye movements", "paper_id": "WOS:000261550100006"}