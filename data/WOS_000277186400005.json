{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "reject_option"}, {"score": 0.0047057084749407485, "phrase": "convex_risk_minimization"}, {"score": 0.0043258453436258405, "phrase": "binary_classification"}, {"score": 0.003545024951402821, "phrase": "natural_loss_function"}, {"score": 0.0033599501775954024, "phrase": "empirical_risk_minimization"}, {"score": 0.0031360786664165093, "phrase": "convex_risks"}, {"score": 0.0030648097361278856, "phrase": "surrogate_convex_loss_functions"}, {"score": 0.0029951555610569225, "phrase": "necessary_and_sufficient_condition"}, {"score": 0.002949598537993663, "phrase": "infinite_sample_consistency"}, {"score": 0.0025892330566358503, "phrase": "excess_risk"}, {"score": 0.002472822097373027, "phrase": "excess_surrogate_risk"}, {"score": 0.002435190484324197, "phrase": "appropriate_conditions"}, {"score": 0.002272794656458419, "phrase": "generalized_margin_condition"}], "paper_keywords": ["classification", " convex surrogate loss", " empirical risk minimization", " generalized margin condition", " reject option"], "paper_abstract": "In this paper, we investigate the problem of binary classification with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassification. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufficient condition for infinite sample consistency ( both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions.", "paper_title": "Classification Methods with Reject Option Based on Convex Risk Minimization", "paper_id": "WOS:000277186400005"}