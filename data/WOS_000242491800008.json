{"auto_keywords": [{"score": 0.038576082979587395, "phrase": "perceptron_algorithm"}, {"score": 0.00481495049065317, "phrase": "discriminative_language_modeling"}, {"score": 0.004735519962945449, "phrase": "large_vocabulary_speech_recognition_task"}, {"score": 0.004309434349688514, "phrase": "regularized_conditional_toe-likelihood"}, {"score": 0.004099542199088835, "phrase": "finite_state_automata"}, {"score": 0.003730465168354817, "phrase": "baseline_recognizer"}, {"score": 0.003509487090932835, "phrase": "relatively_small_feature"}, {"score": 0.0033945024593626675, "phrase": "training_data"}, {"score": 0.0032470094613727433, "phrase": "regularized_likelihood"}, {"score": 0.0029544572689681934, "phrase": "perceptron's_weights"}, {"score": 0.0028260302580927856, "phrase": "word_error_rate"}, {"score": 0.002643753276264565, "phrase": "final_system"}, {"score": 0.002473203873496859, "phrase": "recognition_system"}, {"score": 0.0022880592923396714, "phrase": "wer"}, {"score": 0.0022502133699402018, "phrase": "multi-pass_recognition_system"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": [""], "paper_abstract": "This paper describes discriminative language modeling for a large vocabulary speech recognition task. We contrast two parameter estimation methods: the perceptron algorithm, and a method based on maximizing the regularized conditional toe-likelihood. The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer. The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data. We describe a method based on regularized likelihood that makes use of the feature set given by the perceptron algorithm, and initialization with the perceptron's weights; this method gives an additional 0.5% reduction in word error rate (WER) over training with the perceptron alone. The final system achieves a 1.8% absolute reduction in WER for a baseline first-pass recognition system (from 39.2% to 37.4%), and a 0.9% absolute reduction in WER for a multi-pass recognition system (from 28.9% to 28.0%). (c) 2006 Elsevier Ltd. All rights reserved.", "paper_title": "Discriminative n-gram language modeling", "paper_id": "WOS:000242491800008"}