{"auto_keywords": [{"score": 0.04016938422382131, "phrase": "positive_and_negative_prediction_errors"}, {"score": 0.004815340879031445, "phrase": "adaptive"}, {"score": 0.0047448515702010734, "phrase": "differential_learning_rates"}, {"score": 0.0046986841646790315, "phrase": "positive_and_negative_outcomes"}, {"score": 0.004562847483469705, "phrase": "reward_prediction_error"}, {"score": 0.004240116703465252, "phrase": "focal_point"}, {"score": 0.0041174821422410544, "phrase": "cognitive_science"}, {"score": 0.003940122359770492, "phrase": "reward_prediction_errors"}, {"score": 0.003863758357292741, "phrase": "single_learning_rate"}, {"score": 0.0037519679864796906, "phrase": "behavioral_data"}, {"score": 0.003520687960750365, "phrase": "expected_outcomes"}, {"score": 0.0034355662264234864, "phrase": "symmetric_impacts"}, {"score": 0.003303617297414837, "phrase": "distinct_circuits"}, {"score": 0.0032714270066711835, "phrase": "cortico-striatal_loops"}, {"score": 0.0030101325901285537, "phrase": "biased_reward_predictions"}, {"score": 0.0028244513779243107, "phrase": "static_\"bandit\"_choice_tasks"}, {"score": 0.002663208600887875, "phrase": "asymmetric_learning"}, {"score": 0.0026243534128245886, "phrase": "better_separation"}, {"score": 0.002598764579760562, "phrase": "learned_reward_probabilities"}, {"score": 0.002511147699138714, "phrase": "optimal_learning_rate_asymmetry"}, {"score": 0.0024624103660827695, "phrase": "reward_distribution"}, {"score": 0.002414616656410503, "phrase": "biologically_plausible_algorithm"}, {"score": 0.002344655708000408, "phrase": "positive_and_negative_learning_rates"}, {"score": 0.002265586601922693, "phrase": "specific_adaptive_advantages"}, {"score": 0.0022216045996270974, "phrase": "simple_reinforcement_learning_settings"}, {"score": 0.002157223877108711, "phrase": "normative_perspective"}, {"score": 0.0021049977753042253, "phrase": "associated_neural_data"}], "paper_keywords": ["Reinforcement learning", " Reward prediction error", " Decision-making", " Meta-learning", " Basal ganglia"], "paper_abstract": "The concept of the reward prediction error-the difference between reward obtained and reward predicted-continues to be a focal point for much theoretical and experimental work in psychology, cognitive science, and neuroscience. Models that rely on reward prediction errors typically assume a single learning rate for positive and negative prediction errors. However, behavioral data indicate that better-than-expected and worse-than-expected outcomes often do not have symmetric impacts on learning and decision-making. Furthermore, distinct circuits within cortico-striatal loops appear to support learning from positive and negative prediction errors, respectively. Such differential learning rates would be expected to lead to biased reward predictions and therefore suboptimal choice performance. Contrary to this intuition, we show that on static \"bandit\" choice tasks, differential learning rates can be adaptive. This occurs because asymmetric learning enables a better separation of learned reward probabilities. We show analytically how the optimal learning rate asymmetry depends on the reward distribution and implement a biologically plausible algorithm that adapts the balance of positive and negative learning rates from experience. These results suggest specific adaptive advantages for separate, differential learning rates in simple reinforcement learning settings and provide a novel, normative perspective on the interpretation of associated neural data.", "paper_title": "Adaptive properties of differential learning rates for positive and negative outcomes", "paper_id": "WOS:000328202200007"}