{"auto_keywords": [{"score": 0.047942530352327345, "phrase": "supervised_learning"}, {"score": 0.024088952772333666, "phrase": "fractal_dimension"}, {"score": 0.023614664948713598, "phrase": "redundant_features"}, {"score": 0.004815563174156561, "phrase": "fractal"}, {"score": 0.004553822674247401, "phrase": "feature_selection"}, {"score": 0.004485085675709643, "phrase": "important_role"}, {"score": 0.0044398358177815305, "phrase": "machine_learning"}, {"score": 0.004285010985001827, "phrase": "data_pre-processing_step"}, {"score": 0.0040525061916685924, "phrase": "original_set"}, {"score": 0.0038131738052230254, "phrase": "importance_criterion"}, {"score": 0.0036245239349157236, "phrase": "data_quality"}, {"score": 0.003462704420080129, "phrase": "supervised_learning_algorithms"}, {"score": 0.0033759367489122716, "phrase": "state-of-art_feature_selection_algorithms"}, {"score": 0.003019189898170698, "phrase": "important_features"}, {"score": 0.002840708910768261, "phrase": "features_redundancy"}, {"score": 0.002592523232809191, "phrase": "features'_goodness"}, {"score": 0.0024023079401125492, "phrase": "filter_algorithm"}, {"score": 0.002342048999008582, "phrase": "redundancy_analysis"}, {"score": 0.0021923400552366756, "phrase": "empirical_results"}, {"score": 0.0021049977753042253, "phrase": "appropriate_criterion"}], "paper_keywords": [""], "paper_abstract": "Feature selection plays an important role in machine learning and is often applied as a data pre-processing step. Its objective is to choose a subset from the original set of features that describes a data set, according to some importance criterion, by removing irrelevant and/or redundant features, as they may decrease data quality and reduce the comprehensibility of hypotheses induced by supervised learning algorithms. Most of the state-of-art feature selection algorithms mainly focus on finding relevant features. However, it has been shown that relevance alone is not sufficient to select important features. It is also important to deal with the problem of features redundancy. For the purpose of selecting features and discarding others, it is necessary to measure the features' goodness (importance), and many importance measures have been proposed. This work proposes a filter algorithm that decouples relevance and redundancy analysis, and introduces the use of Fractal Dimension to deal with redundant features. Empirical results on several data sets show that Fractal Dimension is an appropriate criterion to filter out redundant features for supervised learning.", "paper_title": "A Fractal dimension based filter algorithm to select features for supervised learning", "paper_id": "WOS:000242128100032"}