{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "uniform_recovery"}, {"score": 0.0494698167146389, "phrase": "random_sampling_matrices"}, {"score": 0.0044873778175404475, "phrase": "compressive_sensing"}, {"score": 0.003922378980031565, "phrase": "s-sparse_signal"}, {"score": 0.0037743802306714545, "phrase": "high_probability"}, {"score": 0.00334144503360907, "phrase": "new_and_improved_constants"}, {"score": 0.002977061391551631, "phrase": "slightly_larger_class"}, {"score": 0.002635339648135844, "phrase": "so-called_restricted_isometry_constants"}, {"score": 0.002535779689260244, "phrase": "sparse_recovery"}, {"score": 0.002145942273032774, "phrase": "sufficient_condition"}], "paper_keywords": ["Bounded orthogonal systems", " compressive sensing", " effective sparsity", " l(1)-minimization", " random sampling matrices", " restricted isometry property"], "paper_abstract": "We consider two theorems from the theory of compressive sensing. Mainly a theorem concerning uniform recovery of random sampling matrices, where the number of samples needed in order to recover an s-sparse signal from linear measurements (with high probability) is known to be m greater than or similar to s(ln s)(3) ln N. We present new and improved constants together with what we consider to be a more explicit proof. A proof that also allows for a slightly larger class of m x N-matrices, by considering what is called effective sparsity. We also present a condition on the so-called restricted isometry constants, delta s, ensuring sparse recovery via l(1)-minimization. We show that delta(2s) < 4/root 41 is sufficient and that this can be improved further to almost allow for a sufficient condition of the type delta(2s) < 2/3.", "paper_title": "On the Theorem of Uniform Recovery of Random Sampling Matrices", "paper_id": "WOS:000331902400026"}