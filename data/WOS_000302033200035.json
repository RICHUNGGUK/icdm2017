{"auto_keywords": [{"score": 0.0407565968812209, "phrase": "berger"}, {"score": 0.00481495049065317, "phrase": "highly_accurate_or_exact_p-values"}, {"score": 0.00466016686289196, "phrase": "discrete_data"}, {"score": 0.004616859702338596, "phrase": "standard_first_order_p-values"}, {"score": 0.004426882270402403, "phrase": "quite_large_sample_sizes"}, {"score": 0.004344965512058521, "phrase": "different_test_statistics"}, {"score": 0.00428451995524549, "phrase": "practically_different_results"}, {"score": 0.004267404347343794, "phrase": "boos"}, {"score": 0.004166129325245761, "phrase": "computing_p-values"}, {"score": 0.003976007772514762, "phrase": "parametric_bootstrap_p-values"}, {"score": 0.003920674807745575, "phrase": "partially_maximised_p-values"}, {"score": 0.003689628075766217, "phrase": "exact_tail_probability"}, {"score": 0.0036382660173943393, "phrase": "approximate_p-value"}, {"score": 0.0034398445347109396, "phrase": "significance_profile"}, {"score": 0.0032219438233946312, "phrase": "importance_sampling_approach"}, {"score": 0.0031328221096543823, "phrase": "major_advantage"}, {"score": 0.0029618842673339173, "phrase": "nuisance_parameter_values"}, {"score": 0.002839812512030847, "phrase": "simulation_noise"}, {"score": 0.002735522834514124, "phrase": "generalised_linear_models"}, {"score": 0.0026974065918918275, "phrase": "importance_distribution"}, {"score": 0.002550165781755603, "phrase": "optimal_point"}, {"score": 0.0023223663501882896, "phrase": "conceptually_simple_alternative_algorithm"}, {"score": 0.002185301851004467, "phrase": "importance_sampling"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Bootstrap", " Exact tests", " Logistic regression"], "paper_abstract": "Especially for discrete data, standard first order P-values can suffer from poor accuracy, even for quite large sample sizes. Moreover, different test statistics can give practically different results. There are several approaches to computing P-values which do not suffer these defects, such as parametric bootstrap P-values or the partially maximised P-values of Berger and Boos (1994). Both these methods require computing the exact tail probability of the approximate P-value as a function of the nuisance parameter/s, known as the significance profile. For most practical problems, this is not computationally feasible. I develop an importance sampling approach to this problem. A major advantage is that significance can be simultaneously estimated at a grid of nuisance parameter values, without the need for smoothing away the simulation noise. The theory is fully developed for generalised linear models. The importance distribution is selected from the same generalised linear model family but with parameters biased towards an optimal point on the boundary of the tail-set. For logistic regression at least, standard guidelines for selecting the importance distribution can fail quite badly and a conceptually simple alternative algorithm for selecting these parameters is developed. This may have application to importance sampling more generally. (c) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Computing highly accurate or exact P-values using importance sampling", "paper_id": "WOS:000302033200035"}