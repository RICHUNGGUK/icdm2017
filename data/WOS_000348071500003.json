{"auto_keywords": [{"score": 0.046273829908275756, "phrase": "base_classifiers"}, {"score": 0.03703204112725599, "phrase": "hael"}, {"score": 0.00481495049065317, "phrase": "traditional_random_subspace-based_classifier_ensemble"}, {"score": 0.004360412703519539, "phrase": "different_subspaces"}, {"score": 0.0039053194919285725, "phrase": "general_hybrid_adaptive_ensemble_learning_framework"}, {"score": 0.0036352070472201086, "phrase": "rsce."}, {"score": 0.003555902292752528, "phrase": "rsce"}, {"score": 0.00280496585980086, "phrase": "real-world_datasets"}, {"score": 0.002758907973783665, "phrase": "keel_dataset_repository"}, {"score": 0.002713604305939145, "phrase": "classification_task"}, {"score": 0.002669042572650594, "phrase": "cancer_gene_expression_profiles"}], "paper_keywords": ["Adaptive processes", " classifier ensemble", " decision tree", " optimization", " random subspace"], "paper_abstract": "Traditional random subspace-based classifier ensemble approaches (RSCE) have several limitations, such as viewing the same importance for the base classifiers trained in different subspaces, not considering how to find the optimal random subspace set. In this paper, we design a general hybrid adaptive ensemble learning framework (HAEL), and apply it to address the limitations of RSCE. As compared with RSCE, HAEL consists of two adaptive processes, i.e., base classifier competition and classifier ensemble interaction, so as to adjust the weights of the base classifiers in each ensemble and to explore the optimal random subspace set simultaneously. The experiments on the real-world datasets from the KEEL dataset repository for the classification task and the cancer gene expression profiles show that: 1) HAEL works well on both the real-world KEEL datasets and the cancer gene expression profiles and 2) it outperforms most of the state-of-the-art classifier ensemble approaches on 28 out of 36 KEEL datasets and 6 out of 6 cancer datasets.", "paper_title": "Hybrid Adaptive Classifier Ensemble", "paper_id": "WOS:000348071500003"}