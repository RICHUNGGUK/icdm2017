{"auto_keywords": [{"score": 0.03788547968512631, "phrase": "hitchhiker"}, {"score": 0.00481495049065317, "phrase": "erasure-coded_data_centers"}, {"score": 0.004418601451254075, "phrase": "higher_reliability"}, {"score": 0.004382591802405206, "phrase": "data_replication_methods"}, {"score": 0.0041725881174377345, "phrase": "network_bandwidth"}, {"score": 0.004138575034269399, "phrase": "disk_to"}, {"score": 0.0039402177950179345, "phrase": "existing_solutions"}, {"score": 0.0038603905208013482, "phrase": "additional_storage_space"}, {"score": 0.0037207397635410327, "phrase": "system_parameters"}, {"score": 0.003428164458163043, "phrase": "net_work_traffic"}, {"score": 0.0029824560656987855, "phrase": "rs-based_systems"}, {"score": 0.0028744731986533076, "phrase": "rs_codes"}, {"score": 0.0028046593475268174, "phrase": "novel_encoding_and_decoding_techniques"}, {"score": 0.002637433799290365, "phrase": "hadoop_distributed_file_system"}, {"score": 0.0025315126622525424, "phrase": "data-warehouse_cluster"}, {"score": 0.0024903420152789135, "phrase": "facehook"}, {"score": 0.0024700078414481297, "phrase": "real-time_traffic"}, {"score": 0.0023418133729330303, "phrase": "computiltion_time"}, {"score": 0.0022385401780982204, "phrase": "network_traffic"}, {"score": 0.0022111720236287547, "phrase": "io._hitchhiker"}, {"score": 0.002148604369150115, "phrase": "degraded_reads"}, {"score": 0.0021223334214910006, "phrase": "faster_recovery"}, {"score": 0.0021049977753042253, "phrase": "failed_or_decommissioned_machines"}], "paper_keywords": [""], "paper_abstract": "Erasure codes such as Reed-Solomon (RS) codes are I wing extensively deployed in data since they offer I if it oil higher reliability than data replication methods at much lower storage overheads. These codes however mandate much higher resources with respect to network bandwidth and disk TO during reconstruction of data that is missing or otherwise unavailable. Existing solutions to this problem either demand additional storage Space, or severely limit the choice of the system parameters. In this paper, we present Hitchhiker, a new erasure-coded storage system that reduces both net Work traffic and disk TO by around 25% to 45% during reconstruction of missing or otherwise unavailable data, with no additional,storage, the same fault tolerance, and at flexibility in the choice of parameters, as compared to RS-based systems. Hitchhiker \"rides\" on top of RS codes, and is based on novel encoding and decoding techniques that Will be presented in this paper. We have implemented Hitchhiker in the Hadoop Distributed File System (HDFS). When evaluating various metrics on the data-warehouse cluster in production at Facehook with real-time traffic and workloads, during reconstruction, we observe a 360% reduction in the computiltion time and a 32% reduction in the data read time, in addition to the 35% reduction in network traffic and disk IO. Hitchhiker can thus reduce the latency of degraded reads and perform faster recovery from failed or decommissioned machines.", "paper_title": "A \"Hitchhiker's\" Guide to Fast and Efficient Data Reconstruction in Erasure-coded Data Centers", "paper_id": "WOS:000350564600051"}