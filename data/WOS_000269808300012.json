{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "forward_models"}, {"score": 0.0469306417943247, "phrase": "task-irrelevant_features"}, {"score": 0.03815933846832896, "phrase": "state_space"}, {"score": 0.0046018264570209765, "phrase": "adequate_internal_representation"}, {"score": 0.004414723336530117, "phrase": "sensory_data"}, {"score": 0.004124754315882517, "phrase": "sensory_input"}, {"score": 0.004062937864702115, "phrase": "unsupervised_learning_methods"}, {"score": 0.0039124037252916055, "phrase": "dopaminergic_system"}, {"score": 0.0037959868107895053, "phrase": "reinforcernent_learning_approach"}, {"score": 0.0036691380448651443, "phrase": "single_layer_network"}, {"score": 0.0035869274480340727, "phrase": "action_space"}, {"score": 0.003480161521018509, "phrase": "feature_detection_stage"}, {"score": 0.0033008858616145205, "phrase": "learning_agent"}, {"score": 0.003263683763829895, "phrase": "memory_layer"}, {"score": 0.003214730261140886, "phrase": "state_activation"}, {"score": 0.0031784960695034645, "phrase": "previous_time"}, {"score": 0.0031072444973406586, "phrase": "previously_chosen_action"}, {"score": 0.003049086085585051, "phrase": "temporal_difference"}, {"score": 0.0029360049890445944, "phrase": "additional_inputs"}, {"score": 0.002902903244616154, "phrase": "state_layer"}, {"score": 0.0025239898026543964, "phrase": "goal-directed_forward_model"}, {"score": 0.002486103638234488, "phrase": "memory_weights"}, {"score": 0.00243033541656411, "phrase": "state-action_pairs"}, {"score": 0.002279011030331704, "phrase": "feature_detection"}, {"score": 0.002202742267484358, "phrase": "reward_systems"}, {"score": 0.0021861425611350857, "phrase": "cortical_circuits"}, {"score": 0.002169667676959813, "phrase": "goal-directed_feature_detection"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Reinforcement learning", " Forward model", " Feature learning", " Recurrent neural network"], "paper_abstract": "The brain is able to perform actions based on an adequate internal representation of the world, where task-irrelevant features are ignored and incomplete sensory data are estimated. Traditionally, it is assumed that such abstract state representations are obtained purely from the statistics of sensory input for example by unsupervised learning methods. However, more recent findings suggest an influence of the dopaminergic system, which can be modeled by a reinforcernent learning approach. Standard reinforcement learning algorithms act on a single layer network connecting the state space to the action space. Here, we involve in a feature detection stage and a memory layer, which together, construct the state space for a learning agent. The memory layer consists of the state activation at the previous time step as well as the previously chosen action. We present a temporal difference based learning rule for training the weights from these additional inputs to the state layer. As a result, the performance of the network is maintained both, in the presence of task-irrelevant features, and at randomly occurring time steps during which the input is invisible. interestingly, a goal-directed forward model emerges from the memory weights, which only covers the state-action pairs that are relevant to the task. The model presents a link between reinforcement learning, feature detection and forward models and may help to explain how reward systems recruit cortical circuits for goal-directed feature detection and prediction. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Goal-directed learning of features and forward models", "paper_id": "WOS:000269808300012"}