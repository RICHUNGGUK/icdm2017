{"auto_keywords": [{"score": 0.03600243304416855, "phrase": "newly_available_scan"}, {"score": 0.00481495049065317, "phrase": "accurate_point-to-point_correspondences"}, {"score": 0.004649302313891199, "phrase": "human_face_scans"}, {"score": 0.0045846424310611745, "phrase": "varying_expressions"}, {"score": 0.00436530211426514, "phrase": "manually_placed_markers"}, {"score": 0.0033447157263095223, "phrase": "predicted_landmarks"}, {"score": 0.003206916619690171, "phrase": "point-to-point_correspondences"}, {"score": 0.002439308179084911, "phrase": "human_faces"}, {"score": 0.002405309378904663, "phrase": "different_ethnic_groups"}, {"score": 0.002371783323468861, "phrase": "strongly_varying_expressions"}, {"score": 0.0023387234705715154, "phrase": "experimental_results"}, {"score": 0.0022739766507258105, "phrase": "obtained_point-to-point_correspondence"}], "paper_keywords": ["Non-rigid 3D registration", " Automatic landmark prediction", " Facial expression-invariant", " Blendshape model", " Energy minimization"], "paper_abstract": "We consider the problem of computing accurate point-to-point correspondences among a set of human face scans with varying expressions. Our fully automatic approach does not require any manually placed markers on the scan. Instead, the approach learns the locations of a set of landmarks present in a database and uses this knowledge to automatically predict the locations of these landmarks on a newly available scan. The predicted landmarks are then used to compute point-to-point correspondences between a template model and the newly available scan. To accurately fit the expression of the template to the expression of the scan, we use as template a blendshape model. Our algorithm was tested on a database of human faces of different ethnic groups with strongly varying expressions. Experimental results show that the obtained point-to-point correspondence is both highly accurate and consistent for most of the tested 3D face models.", "paper_title": "Fully automatic expression-invariant face correspondence", "paper_id": "WOS:000334447600003"}