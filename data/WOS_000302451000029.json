{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "maximum_likelihood_estimation"}, {"score": 0.04954033184026153, "phrase": "gaussian_mixture_models"}, {"score": 0.004707188147442449, "phrase": "stochastic_search"}, {"score": 0.004601883880242523, "phrase": "gmm"}, {"score": 0.004448169552315403, "phrase": "pattern_recognition"}, {"score": 0.004398094237005956, "phrase": "machine_learning"}, {"score": 0.004299621153829837, "phrase": "flexible_probabilistic_model"}, {"score": 0.004156012879026324, "phrase": "conventional_expectation-maximization"}, {"score": 0.003648411055689428, "phrase": "local_maxima"}, {"score": 0.0036073064072936626, "phrase": "stochastic_search_algorithms"}, {"score": 0.003546513036085932, "phrase": "popular_alternatives"}, {"score": 0.0035065523764817143, "phrase": "global_optimization"}, {"score": 0.0034279721355412285, "phrase": "gmm_estimation"}, {"score": 0.003332210257373994, "phrase": "constrained_models"}, {"score": 0.0032575241721857343, "phrase": "diagonal_covariance_matrices"}, {"score": 0.003009022147559257, "phrase": "novel_parametrization"}, {"score": 0.002975099633631871, "phrase": "arbitrary_covariance_matrices"}, {"score": 0.0029249294679606656, "phrase": "independent_updating"}, {"score": 0.0028919522313933525, "phrase": "individual_parameters"}, {"score": 0.002795228413298423, "phrase": "resultant_matrices"}, {"score": 0.0026864540069792275, "phrase": "effective_parameter"}, {"score": 0.0025097154697345096, "phrase": "multiple_candidate_solutions"}, {"score": 0.002398382089724878, "phrase": "gmm_components"}, {"score": 0.0023445770108033288, "phrase": "synthetic_and_real_data_sets"}, {"score": 0.0022919762167777427, "phrase": "proposed_framework"}, {"score": 0.002253299769886694, "phrase": "robust_performance"}, {"score": 0.002215274527909304, "phrase": "significantly_higher_likelihood_values"}, {"score": 0.0021778895748464024, "phrase": "em_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Gaussian mixture models", " Maximum likelihood estimation", " Expectation-maximization", " Covariance parametrization", " Identifiability", " Stochastic search", " Particle swarm optimization"], "paper_abstract": "Gaussian mixture models (GMM), commonly used in pattern recognition and machine learning, provide a flexible probabilistic model for the data. The conventional expectation-maximization (EM) algorithm for the maximum likelihood estimation of the parameters of GMMs is very sensitive to initialization and easily gets trapped in local maxima. Stochastic search algorithms have been popular alternatives for global optimization but their uses for GMM estimation have been limited to constrained models using identity or diagonal covariance matrices. Our major contributions in this paper are twofold. First, we present a novel parametrization for arbitrary covariance matrices that allow independent updating of individual parameters while retaining validity of the resultant matrices. Second, we propose an effective parameter matching technique to mitigate the issues related with the existence of multiple candidate solutions that are equivalent under permutations of the GMM components. Experiments on synthetic and real data sets show that the proposed framework has a robust performance and achieves significantly higher likelihood values than the EM algorithm. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Maximum likelihood estimation of Gaussian mixture models using stochastic search", "paper_id": "WOS:000302451000029"}