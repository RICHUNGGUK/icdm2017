{"auto_keywords": [{"score": 0.039674809731396525, "phrase": "information_representation"}, {"score": 0.00481495049065317, "phrase": "new_approach"}, {"score": 0.004783483480147339, "phrase": "real-time_view-based_pose_recognition"}, {"score": 0.004614038128321949, "phrase": "camera_views"}, {"score": 0.004553914287137776, "phrase": "video_sequences"}, {"score": 0.004524145463916772, "phrase": "video_streams"}, {"score": 0.0043495571869907376, "phrase": "fast_pose_recognition_process"}, {"score": 0.004086744751579196, "phrase": "new_material"}, {"score": 0.003993958025223994, "phrase": "recognition_system"}, {"score": 0.0036914757355166966, "phrase": "local_linear_models"}, {"score": 0.0035722806911457545, "phrase": "biological_systems"}, {"score": 0.003502612572302602, "phrase": "common_distortions"}, {"score": 0.0034118234596993836, "phrase": "recognition_algorithm"}, {"score": 0.0033233797886807375, "phrase": "low-level_image"}, {"score": 0.003269266730193333, "phrase": "local_orientation"}, {"score": 0.003051418904573872, "phrase": "similar_features"}, {"score": 0.0030314419455512013, "phrase": "local_image_regions"}, {"score": 0.0029722900280316216, "phrase": "query_p-channels"}, {"score": 0.00290473242917264, "phrase": "prototype_p-channels"}, {"score": 0.0028480459059007468, "phrase": "least-squares_approach"}, {"score": 0.002765077844399801, "phrase": "fisheye_camera_data"}, {"score": 0.00270221723573854, "phrase": "synthetic_images"}, {"score": 0.002649472341607234, "phrase": "nearest_view"}, {"score": 0.0026063035127641574, "phrase": "real_images"}, {"score": 0.002547043222086858, "phrase": "sift-based_methods"}, {"score": 0.0024325244760735566, "phrase": "pose-tracking_systems"}, {"score": 0.0021682027434117095, "phrase": "tracking_system"}, {"score": 0.0021398811716327273, "phrase": "sensor_fusion_unit"}, {"score": 0.0021049977753042253, "phrase": "frame-by-frame_tracking"}], "paper_keywords": ["Pose recognition", " Pose interpolation", " P-channels", " Real-time processing", " View-based computer vision"], "paper_abstract": "In this paper we propose a new approach to real-time view-based pose recognition and interpolation. Pose recognition is particularly useful for identifying camera views in databases, video sequences, video streams, and live recordings. All of these applications require a fast pose recognition process, in many cases video real-time. It should further be possible to extend the database with new material, i.e., to update the recognition system online. The method that we propose is based on P-channels, a special kind of information representation which combines advantages of histograms and local linear models. Our approach is motivated by its similarity to information representation in biological systems but its main advantage is its robustness against common distortions such as clutter and occlusion. The recognition algorithm consists of three steps: (1) low-level image features for color and local orientation are extracted in each point of the image; (2) these features are encoded into P-channels by combining similar features within local image regions; (3) the query P-channels are compared to a set of prototype P-channels in a database using a least-squares approach. The algorithm is applied in two scene registration experiments with fisheye camera data, one for pose interpolation from synthetic images and one for finding the nearest view in a set of real images. The method compares favorable to SIFT-based methods, in particular concerning interpolation. The method can be used for initializing pose-tracking systems, either when starting the tracking or when the tracking has failed and the system needs to re-initialize. Due to its real-time performance, the method can also be embedded directly into the tracking system, allowing a sensor fusion unit choosing dynamically between the frame-by-frame tracking and the pose recognition.", "paper_title": "Real-time view-based pose recognition and interpolation for tracking initialization", "paper_id": "WOS:000208118900004"}