{"auto_keywords": [{"score": 0.049764804068700534, "phrase": "fast_storage_devices"}, {"score": 0.008382515195899736, "phrase": "application_performance"}, {"score": 0.006816637932214109, "phrase": "per-request_latency"}, {"score": 0.004705278218845159, "phrase": "emerging_solution"}, {"score": 0.004659040279237174, "phrase": "data-intensive_applications"}, {"score": 0.00459809245936039, "phrase": "high_transaction_rates"}, {"score": 0.004567926994643807, "phrase": "dbms"}, {"score": 0.004537938301016805, "phrase": "low_response_times"}, {"score": 0.004508155639663496, "phrase": "web_servers"}, {"score": 0.004376523540676604, "phrase": "large_memory_footprints"}, {"score": 0.004305055407631358, "phrase": "performance-hungry_applications"}, {"score": 0.004179328666330398, "phrase": "fast_hardware"}, {"score": 0.0041518897573208785, "phrase": "modern_operating_systems"}, {"score": 0.00401736307274363, "phrase": "hardware's_full_potential"}, {"score": 0.0036034816680872877, "phrase": "traditional_storage-stack_design"}, {"score": 0.003463754811167655, "phrase": "hardware_interface"}, {"score": 0.003429675062587355, "phrase": "new_capabilities"}, {"score": 0.0033959294806803725, "phrase": "operating_system"}, {"score": 0.0033075503163659055, "phrase": "six_optimizations"}, {"score": 0.003210861212842534, "phrase": "performance_characteristics"}, {"score": 0.0031272833420309685, "phrase": "new_hardware_interfaces"}, {"score": 0.0028703465154343373, "phrase": "well-known_storage_benchmarks"}, {"score": 0.002832731124016696, "phrase": "linux_kernel"}, {"score": 0.002804842518176471, "phrase": "customized_ssd."}, {"score": 0.002758968636750223, "phrase": "context_switches"}, {"score": 0.002704906579783511, "phrase": "software_overhead"}, {"score": 0.0026171408377811687, "phrase": "new_request_merge_scheme"}, {"score": 0.0024990202385797252, "phrase": "peak_device_performance"}, {"score": 0.0024662589776071864, "phrase": "request_access_patterns"}, {"score": 0.0024179186667595436, "phrase": "performance_improvement"}, {"score": 0.0023705236064200893, "phrase": "standard_sata-based_ssd"}, {"score": 0.002308768837799101, "phrase": "relatively_high_response_times"}, {"score": 0.002263508433567761, "phrase": "future_ssds"}, {"score": 0.0022486192232506388, "phrase": "lower_response_times"}, {"score": 0.002140021296668778, "phrase": "os_community"}, {"score": 0.0021049977753042253, "phrase": "future_device_interfaces"}], "paper_keywords": ["Experimentation", " Measurement", " Performance", " I/O subsystem", " device polling", " request batching", " extended interface"], "paper_abstract": "Fast storage devices are an emerging solution to satisfy data-intensive applications. They provide high transaction rates for DBMS, low response times for Web servers, instant on-demand paging for applications with large memory footprints, and many similar advantages for performance-hungry applications. In spite of the benefits promised by fast hardware, modern operating systems are not yet structured to take advantage of the hardware's full potential. The software overhead caused by an OS, negligible in the past, adversely impacts application performance, lessening the advantage of using such hardware. Our analysis demonstrates that the overheads from the traditional storage-stack design are significant and cannot easily be overcome without modifying the hardware interface and adding new capabilities to the operating system. In this article, we propose six optimizations that enable an OS to fully exploit the performance characteristics of fast storage devices. With the support of new hardware interfaces, our optimizations minimize per-request latency by streamlining the I/O path and amortize per-request latency by maximizing parallelism inside the device. We demonstrate the impact on application performance through well-known storage benchmarks run against a Linux kernel with a customized SSD. We find that eliminating context switches in the I/O path decreases the software overhead of an I/O request from 20 microseconds to 5 microseconds and a new request merge scheme called Temporal Merge enables the OS to achieve 87% to 100% of peak device performance, regardless of request access patterns or types. Although the performance improvement by these optimizations on a standard SATA-based SSD is marginal (because of its limited interface and relatively high response times), our sensitivity analysis suggests that future SSDs with lower response times will benefit from these changes. The effectiveness of our optimizations encourages discussion between the OS community and storage vendors about future device interfaces for fast storage devices.", "paper_title": "Optimizing the Block I/O Subsystem for Fast Storage Devices", "paper_id": "WOS:000339623100003"}