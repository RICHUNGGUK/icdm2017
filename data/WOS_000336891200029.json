{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "spatial-temporal_sampling"}, {"score": 0.043982673134435366, "phrase": "spatial-temporal_boundaries"}, {"score": 0.03788313922194841, "phrase": "video_objects"}, {"score": 0.026992278395501307, "phrase": "proposed_method"}, {"score": 0.004662036750452837, "phrase": "video_object_detection"}, {"score": 0.004342430078306461, "phrase": "video_surveillance"}, {"score": 0.004286708921931472, "phrase": "event_annotation"}, {"score": 0.003992729692096799, "phrase": "visual_content"}, {"score": 0.003941478054422721, "phrase": "high-level_semantics"}, {"score": 0.0036710872046569532, "phrase": "unified_process"}, {"score": 0.0034636656250083744, "phrase": "learnt_video_object_model"}, {"score": 0.003331915219811852, "phrase": "computational_approach"}, {"score": 0.0032468685934998335, "phrase": "optimal_key-object_codebook_sequence"}, {"score": 0.0031233388358844188, "phrase": "video_clips"}, {"score": 0.002985130390111069, "phrase": "detected_video_objects"}, {"score": 0.0028715299874549245, "phrase": "learnt_codebook_sequence"}, {"score": 0.0026743419827100225, "phrase": "test_video_clip"}, {"score": 0.0024906610581090223, "phrase": "human_action_detection"}, {"score": 0.0024586427394152196, "phrase": "recognition_system"}, {"score": 0.0023958326775913165, "phrase": "experimental_results"}, {"score": 0.002304606411453748, "phrase": "good_performance"}, {"score": 0.0022168460543913787, "phrase": "detection_accuracy"}, {"score": 0.0021883400011393564, "phrase": "recognition_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Semantic video objects", " Spatial-temporal sampling", " Human action detection", " Video object model", " Dynamic programming", " Multiple alignment", " Model-based tracking", " Video object detetcion"], "paper_abstract": "For a variety of applications such as video surveillance and event annotation, the spatial-temporal boundaries between video objects are required for annotating visual content with high-level semantics. In this paper, we define spatial-temporal sampling as a unified process of extracting video objects and computing their spatial-temporal boundaries using a learnt video object model. We first :provide a computational approach for learning an optimal key-object codebook sequence from a set of training video clips to characterize the semantics of the detected video objects. Then, dynamic programming with the learnt codebook sequence is used to locate the video objects with spatial-temporal boundaries in a test video clip. To verify the performance of the proposed method, a human action detection and recognition system is constructed. Experimental results show that the proposed method gives good performance on several publicly available datasets in terms of detection accuracy and recognition rate. (c) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Model-based approach to spatial-temporal sampling of video clips for video object detection by classification", "paper_id": "WOS:000336891200029"}