{"auto_keywords": [{"score": 0.046966801547094525, "phrase": "respiration_signal"}, {"score": 0.00481495049065317, "phrase": "automatic_respiration_signal_segmentation"}, {"score": 0.0047747348769918, "phrase": "human_respiration"}, {"score": 0.004656082803151484, "phrase": "great_relevance"}, {"score": 0.004597861947580946, "phrase": "emotional_state"}, {"score": 0.00444612502361583, "phrase": "traditional_biosensors"}, {"score": 0.0043721365634105565, "phrase": "changing_emotions"}, {"score": 0.004335603324413586, "phrase": "emotion_intensities"}, {"score": 0.0042455945330942746, "phrase": "motion_artifacts"}, {"score": 0.004192484908247302, "phrase": "resulting_information_ambiguity"}, {"score": 0.004140036888016244, "phrase": "practical_application"}, {"score": 0.004003348169928476, "phrase": "reliable_affective_state_appraisal"}, {"score": 0.003920209964956682, "phrase": "present_study"}, {"score": 0.0038227112550093863, "phrase": "representative_emotion_elicited_segments"}, {"score": 0.003665552439863732, "phrase": "affective_state"}, {"score": 0.0035001037990786727, "phrase": "ees_extraction_process"}, {"score": 0.0031778255332904487, "phrase": "dynamic_time_warping_distance"}, {"score": 0.0030343239259424497, "phrase": "elicited_segment_density"}, {"score": 0.002921732652917206, "phrase": "innate_regularity"}, {"score": 0.00284899583294417, "phrase": "respiration_pattern"}, {"score": 0.002813307389456184, "phrase": "particular_emotional_state"}, {"score": 0.0026862220320327613, "phrase": "parameter-free_respiration_quasi-homogeneity_segmentation"}, {"score": 0.0025541050375256992, "phrase": "quasi-homogenous_segments"}, {"score": 0.002438699976481979, "phrase": "experimental_results"}, {"score": 0.0024081387633065206, "phrase": "five_prototypical_emotions"}, {"score": 0.002338306897057454, "phrase": "\"_\"sadness"}, {"score": 0.0023187286045879643, "phrase": "\"_\"joy"}, {"score": 0.002299313860419974, "phrase": "\"_\"anger"}, {"score": 0.0021049977753042253, "phrase": "average_classification_rate"}], "paper_keywords": ["Segmentation algorithm", " motion artifacts", " emotion classification", " dynamic time-warping", " mutual information", " constraint-based clustering analysis"], "paper_abstract": "Human respiration has been reported as having great relevance to the emotional state. However, the respiration signal obtained using traditional biosensors not only reflects changing emotions and emotion intensities, but also contains motion artifacts. The resulting information ambiguity limits the practical application of the respiration signal as a means of reliable affective state appraisal. Thus, the present study proposes a method for extracting representative Emotion Elicited Segments (EESs) from the respiration signal such that the affective state of the individual can be more reliably determined. The EES extraction process involves the combination of the following procedures, namely, 1) Mutual Information-Based Emotion Relevance Feature Ranking based on the Dynamic Time Warping Distance (MIDTW), and 2) Constraint-based Elicited Segment Density (CESD) analysis. Due to the innate regularity of the respiration signal, the respiration pattern under a particular emotional state would be relatively quasihomogeneous over time. Accordingly, a parameter-free Respiration quasi-Homogeneity Segmentation (RHS) algorithm is proposed for partitioning the respiration signal into quasi-homogenous segments from which the EESs can then be extracted. The experimental results obtained for five prototypical emotions (i.e., \"love,\" \"sadness,\" \"joy,\" \"anger,\" and \"fear\") show that the proposed segmentation/extraction methodology enables the EESs to be reliably identified and yields an average classification rate of 88 percent.", "paper_title": "Representative Segment-Based Emotion Analysis and Classification with Automatic Respiration Signal Segmentation", "paper_id": "WOS:000323642000011"}