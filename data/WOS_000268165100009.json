{"auto_keywords": [{"score": 0.04933401710855323, "phrase": "feature_subset_selection_problem"}, {"score": 0.00481495049065317, "phrase": "positive_and_unlabelled_examples"}, {"score": 0.004666228956744244, "phrase": "growing_importance"}, {"score": 0.004314127287770832, "phrase": "great_number"}, {"score": 0.004115697805448384, "phrase": "supervised_databases"}, {"score": 0.0036489343475053187, "phrase": "correlation_based_filter_selection"}, {"score": 0.003069867656852932, "phrase": "first_time"}, {"score": 0.0029593432468318745, "phrase": "positive_unlabelled_learning_context"}, {"score": 0.0028378798711397235, "phrase": "synthetic_datasets"}, {"score": 0.0027790254309734428, "phrase": "bayesian_network_models"}, {"score": 0.002542174767860791, "phrase": "real-life_databases"}, {"score": 0.002476424729269258, "phrase": "negative_examples"}, {"score": 0.002253461585851626, "phrase": "good_solutions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Positive unlabelled learning", " Partially supervised classification", " Feature subset selection", " Filter methods"], "paper_abstract": "The feature subset selection problem has a growing importance in many machine learning applications where the amount of variables is very high. There is a great number of algorithms that can approach this problem in supervised databases but, when examples from one or more classes are not available, supervised feature subset selection algorithms cannot be directly applied. One of these algorithms is the correlation based filter selection (CFS). In this work we propose an adaptation of this algorithm that can be applied when only positive and unlabelled examples are available. As far as we know, this is the first time the feature subset selection problem is studied in the positive unlabelled learning context. We have tested this adaptation on synthetic datasets obtained by sampling Bayesian network models where we know which variables are (in)dependent of the class. We have also tested our adaptations on real-life databases where the absence of negative examples has been simulated. The results show that, having enough positive examples, it is possible to obtain good solutions to the feature subset selection problem when only positive and unlabelled instances are available. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Feature subset selection from positive and unlabelled examples", "paper_id": "WOS:000268165100009"}