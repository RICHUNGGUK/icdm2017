{"auto_keywords": [{"score": 0.04034190794741309, "phrase": "fcnn_rule"}, {"score": 0.00481495049065317, "phrase": "large_data_sets_classification"}, {"score": 0.004534136929881113, "phrase": "novel_algorithm"}, {"score": 0.00442200190164008, "phrase": "fcnn"}, {"score": 0.004269630481719451, "phrase": "training-set-consistent_subset"}, {"score": 0.004205940624913343, "phrase": "nearest_neighbor_decision_rule"}, {"score": 0.004101886428528311, "phrase": "condensation_algorithms"}, {"score": 0.004040688646993605, "phrase": "nearest_neighbor_rule"}, {"score": 0.003940706575253453, "phrase": "huge_collections"}, {"score": 0.003785836059995376, "phrase": "interesting_properties"}, {"score": 0.0035292625231068517, "phrase": "small_constant_prefactor"}, {"score": 0.003306559295382181, "phrase": "decision_boundary"}, {"score": 0.0031765314093444956, "phrase": "triangle_inequality"}, {"score": 0.003051601113971322, "phrase": "computational_effort"}, {"score": 0.002976019715894587, "phrase": "even_here-enhanced_variants"}, {"score": 0.002946312377831317, "phrase": "existing_competence_preservation_methods"}, {"score": 0.002339057107835012, "phrase": "mnist"}, {"score": 0.002315658491366464, "phrase": "massachusetts_institute_of_technology"}, {"score": 0.0022926568955736093, "phrase": "mit"}, {"score": 0.0021049977753042253, "phrase": "noise-filtering_pass"}], "paper_keywords": ["classification", " large and high-dimensional data", " nearest neighbor rule", " prototype selection algorithms", " training-set-consistent subset"], "paper_abstract": "This work has two main objectives, namely, to introduce a novel algorithm, called the Fast Condensed Nearest Neighbor (FCNN) rule, for computing a training-set-consistent subset for the nearest neighbor decision rule and to show that condensation algorithms for the nearest neighbor rule can be applied to huge collections of data. The FCNN rule has some interesting properties: it is order independent, its worst-case time complexity is quadratic but often with a small constant prefactor, and it is likely to select points very close to the decision boundary. Furthermore, its structure allows for the triangle inequality to be effectively exploited to reduce the computational effort. The FCNN rule outperformed even here-enhanced variants of existing competence preservation methods both in terms of learning speed and learning scaling behavior and, often, in terms of the size of the model while it guaranteed the same prediction accuracy. Furthermore, it was three orders of magnitude faster than hybrid instance-based learning algorithms on the MNIST and Massachusetts Institute of Technology (MIT) Face databases and computed a model of accuracy comparable to that of methods incorporating a noise-filtering pass.", "paper_title": "Fast nearest neighbor condensation for large data sets classification", "paper_id": "WOS:000249563900002"}