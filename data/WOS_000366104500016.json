{"auto_keywords": [{"score": 0.04911241438594343, "phrase": "global_shape"}, {"score": 0.03605069303958734, "phrase": "local_appearance"}, {"score": 0.00481495049065317, "phrase": "detecting_surgical_tools"}, {"score": 0.004766090327062509, "phrase": "modelling_local_appearance"}, {"score": 0.004598933498016774, "phrase": "surgical_videos"}, {"score": 0.004529093447865156, "phrase": "important_ingredient"}, {"score": 0.004483120978156604, "phrase": "context-aware_computer-assisted_surgical_systems"}, {"score": 0.0042819270655224916, "phrase": "new_surgical_tool_detection_dataset"}, {"score": 0.004174064076143015, "phrase": "joint_tool_detection"}, {"score": 0.0038862297330905836, "phrase": "strong_assumptions"}, {"score": 0.0038271712887391015, "phrase": "previous_works"}, {"score": 0.0035269696779744266, "phrase": "first_stage"}, {"score": 0.003334309053314909, "phrase": "second_stage"}, {"score": 0.0032836104153151973, "phrase": "tool-specific_shape_template"}, {"score": 0.0030726480920918097, "phrase": "training_data"}, {"score": 0.002964706925144123, "phrase": "new_surgical_tool_dataset"}, {"score": 0.00290473242917264, "phrase": "neurosurgical_microscopes"}, {"score": 0.0027459678326877744, "phrase": "existing_datasets"}, {"score": 0.0025174594088331853, "phrase": "competitive_baselines"}, {"score": 0.002479150189700726, "phrase": "computer_vision_field"}, {"score": 0.002367676766637248, "phrase": "false_positives"}, {"score": 0.0022961534630650347, "phrase": "suction_tube"}, {"score": 0.0021817065300196634, "phrase": "semantic_labelling"}, {"score": 0.0021484955960643167, "phrase": "intermediate_task"}, {"score": 0.0021049977753042253, "phrase": "high_quality_detection"}], "paper_keywords": ["Microscope images", " object detection", " surgical tools", " template matching"], "paper_abstract": "Detecting tools in surgical videos is an important ingredient for context-aware computer-assisted surgical systems. To this end, we present a new surgical tool detection dataset and a method for joint tool detection and pose estimation in 2d images. Our two-stage pipeline is data-driven and relaxes strong assumptions made by previous works regarding the geometry, number, and position of tools in the image. The first stage classifies each pixel based on local appearance only, while the second stage evaluates a tool-specific shape template to enforce global shape. Both local appearance and global shape are learned from training data. Our method is validated on a new surgical tool dataset of 2 476 images from neurosurgical microscopes, which is made freely available. It improves over existing datasets in size, diversity and detail of annotation. We show that our method significantly improves over competitive baselines from the computer vision field. We achieve 15% detection miss-rate at false positives per image (for the suction tube) over our surgical tool dataset. Results indicate that performing semantic labelling as an intermediate task is key for high quality detection.", "paper_title": "Detecting Surgical Tools by Modelling Local Appearance and Global Shape", "paper_id": "WOS:000366104500016"}