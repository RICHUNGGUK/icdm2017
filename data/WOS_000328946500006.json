{"auto_keywords": [{"score": 0.03196109804000087, "phrase": "fuzzy_rules"}, {"score": 0.00481495049065317, "phrase": "incremental_learning_machine"}, {"score": 0.0046803650677267146, "phrase": "real-world_systems"}, {"score": 0.004404445045826858, "phrase": "omnipresent_neuro-fuzzy_systems"}, {"score": 0.004298666679374568, "phrase": "nonstationary_environment"}, {"score": 0.004212453011596424, "phrase": "high_degree"}, {"score": 0.003670157084732341, "phrase": "small_snapshot"}, {"score": 0.0036257863613963245, "phrase": "complete_training_data"}, {"score": 0.003567455727660992, "phrase": "computational_load"}, {"score": 0.0035386420095120706, "phrase": "memory_demand"}, {"score": 0.003495855671790616, "phrase": "low_level"}, {"score": 0.0034118234596993836, "phrase": "novel_algorithm"}, {"score": 0.0033163269450817716, "phrase": "fuzzy_inference_system"}, {"score": 0.003158776195052182, "phrase": "panfis"}, {"score": 0.003057910095857785, "phrase": "empty_rule_base"}, {"score": 0.002900806476592644, "phrase": "statistical_contributions"}, {"score": 0.002796782643988021, "phrase": "identical_fuzzy_sets"}, {"score": 0.0026209522797963447, "phrase": "transparent_rule_base"}, {"score": 0.002599763359352916, "phrase": "human's_interpretability"}, {"score": 0.002506507889886399, "phrase": "proposed_panfis"}, {"score": 0.002416589459425828, "phrase": "synthetic_datasets"}, {"score": 0.0022830706430588482, "phrase": "neuro-fuzzy_methods"}, {"score": 0.0021221627745801478, "phrase": "predictive_fidelity"}, {"score": 0.0021049977753042253, "phrase": "model_complexity"}], "paper_keywords": ["Evolving neuro-fuzzy systems (ENFSs)", " incremental learning", " sample-wise training"], "paper_abstract": "Most of the dynamics in real-world systems are compiled by shifts and drifts, which are uneasy to be overcome by omnipresent neuro-fuzzy systems. Nonetheless, learning in nonstationary environment entails a system owning high degree of flexibility capable of assembling its rule base autonomously according to the degree of nonlinearity contained in the system. In practice, the rule growing and pruning are carried out merely benefiting from a small snapshot of the complete training data to truncate the computational load and memory demand to the low level. An exposure of a novel algorithm, namely parsimonious network based on fuzzy inference system (PANFIS), is to this end presented herein. PANFIS can commence its learning process from scratch with an empty rule base. The fuzzy rules can be stitched up and expelled by virtue of statistical contributions of the fuzzy rules and injected datum afterward. Identical fuzzy sets may be alluded and blended to be one fuzzy set as a pursuit of a transparent rule base escalating human's interpretability. The learning and modeling performances of the proposed PANFIS are numerically validated using several benchmark problems from real-world or synthetic datasets. The validation includes comparisons with state-of-the-art evolving neuro-fuzzy methods and showcases that our new method can compete and in some cases even outperform these approaches in terms of predictive fidelity and model complexity.", "paper_title": "PANFIS: A Novel Incremental Learning Machine", "paper_id": "WOS:000328946500006"}