{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "functional_additive_models"}, {"score": 0.004678940805063634, "phrase": "functional_data_analysis"}, {"score": 0.004525088883501587, "phrase": "related_functions"}, {"score": 0.004418287966422041, "phrase": "learning_curves"}, {"score": 0.004313996835376177, "phrase": "brain_signals"}, {"score": 0.004212157020917929, "phrase": "spatial_maps"}, {"score": 0.003996458454723708, "phrase": "additive_model"}, {"score": 0.003920780681351006, "phrase": "individual_function"}, {"score": 0.003702206069379459, "phrase": "gaussian_processes"}, {"score": 0.0036494864560524735, "phrase": "elegant_way"}, {"score": 0.0035125292519354724, "phrase": "computational_difficulties"}, {"score": 0.0034459835582278746, "phrase": "matrix_operations"}, {"score": 0.003253791343615811, "phrase": "functional_additive_model"}, {"score": 0.0031921317979928406, "phrase": "covariance_matrices"}, {"score": 0.003131637033429892, "phrase": "specific_form"}, {"score": 0.002859533151305368, "phrase": "additional_assumptions"}, {"score": 0.00283229915062791, "phrase": "two-level_additive_model"}, {"score": 0.002500927169609655, "phrase": "matrix_factorisations"}, {"score": 0.002372649454733249, "phrase": "latent_field"}, {"score": 0.0023388183537341213, "phrase": "enormous_improvement"}, {"score": 0.002305468528385778, "phrase": "cubic_scaling"}, {"score": 0.002145703330831668, "phrase": "rqk_matrices"}, {"score": 0.0021049977753042253, "phrase": "latent_gaussian_models"}], "paper_keywords": ["Gaussian processes", " Functional data analysis", " Latent Gaussian models"], "paper_abstract": "It is common in functional data analysis to look at a set of related functions: a set of learning curves, a set of brain signals, a set of spatial maps, etc. One way to express relatedness is through an additive model, whereby each individual function is assumed to be a variation around some shared mean . Gaussian processes provide an elegant way of constructing such additive models, but suffer from computational difficulties arising from the matrix operations that need to be performed. Recently Heersink & Furrer have shown that functional additive model give rise to covariance matrices that have a specific form they called quasi-Kronecker (QK), whose inverses are relatively tractable. We show that under additional assumptions the two-level additive model leads to a class of matrices we call restricted quasi-Kronecker (rQK), which enjoy many interesting properties. In particular, we formulate matrix factorisations whose complexity scales only linearly in the number of functions in latent field, an enormous improvement over the cubic scaling of na < ve approaches. We describe how to leverage the properties of rQK matrices for inference in Latent Gaussian Models.", "paper_title": "Fast matrix computations for functional additive models", "paper_id": "WOS:000349028500009"}