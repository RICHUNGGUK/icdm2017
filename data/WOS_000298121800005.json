{"auto_keywords": [{"score": 0.04185799373875313, "phrase": "action_space"}, {"score": 0.03216284359599274, "phrase": "value_function"}, {"score": 0.030014234962223538, "phrase": "spudd"}, {"score": 0.02768352572975123, "phrase": "alp"}, {"score": 0.00481495049065317, "phrase": "local_control"}, {"score": 0.004792208732166814, "phrase": "spatial_processes"}, {"score": 0.004702305713946529, "phrase": "efficient_modelling"}, {"score": 0.00465798570297277, "phrase": "sequential_decision-making_problems"}, {"score": 0.004548996299741169, "phrase": "state_and_action_spaces"}, {"score": 0.004463635542349873, "phrase": "spatially_explicit_decision_problems"}, {"score": 0.004411096776740219, "phrase": "dedicated_solution_algorithms"}, {"score": 0.004338575266640439, "phrase": "large_factored_state_spaces"}, {"score": 0.004277359615286959, "phrase": "large_action_spaces"}, {"score": 0.004167356068778172, "phrase": "graph-based_markov_decision_processes"}, {"score": 0.004098825328193599, "phrase": "factored_mdp"}, {"score": 0.004021877785031447, "phrase": "state_space"}, {"score": 0.003965112819778777, "phrase": "decision_problem"}, {"score": 0.003863108365451395, "phrase": "transition_probabilities"}, {"score": 0.0037905691110090875, "phrase": "single_graph_structure"}, {"score": 0.0035220708047207084, "phrase": "exact_resolution"}, {"score": 0.0034641146456183976, "phrase": "approximate_solution_algorithm"}, {"score": 0.00341519498059637, "phrase": "gmdp"}, {"score": 0.003288082466985261, "phrase": "maximum_number"}, {"score": 0.0031358032682090595, "phrase": "approximate_policy_iteration"}, {"score": 0.003121023691906385, "phrase": "api"}, {"score": 0.0030623209153997116, "phrase": "mean-field_approximation"}, {"score": 0.0029623208356675556, "phrase": "suboptimal_set"}, {"score": 0.0029483034113518703, "phrase": "local_policies"}, {"score": 0.002818392856847883, "phrase": "approximate_linear_programming"}, {"score": 0.0024854034296039646, "phrase": "similar_quality"}, {"score": 0.002421372678984467, "phrase": "significantly_better_policies"}, {"score": 0.002375840949858925, "phrase": "better_approximation"}, {"score": 0.0023478184025779586, "phrase": "approximate_policies"}, {"score": 0.0023311634030822527, "phrase": "promising_results"}, {"score": 0.002309139952095626, "phrase": "gmdp_model"}, {"score": 0.0022927586999755716, "phrase": "convenient_framework"}, {"score": 0.0022603432136926137, "phrase": "large_range"}, {"score": 0.002249639986821758, "phrase": "spatial_and_structured_planning_problems"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Decision-theoretic planning", " Factored Markov decision processes", " Approximate policy iteration", " Mean-field principle", " Approximate linear programming"], "paper_abstract": "The Markov Decision Process (MDP) framework is a tool for the efficient modelling and solving of sequential decision-making problems under uncertainty. However, it reaches its limits when state and action spaces are large, as can happen for spatially explicit decision problems. Factored MDPs and dedicated solution algorithms have been introduced to deal with large factored state spaces. But the case of large action spaces remains an issue. In this article, we define graph-based Markov Decision Processes (GMDPs), a particular Factored MDP framework which exploits the factorization of the state space and the action space of a decision problem. Both spaces are assumed to have the same dimension. Transition probabilities and rewards are factored according to a single graph structure, where nodes represent pairs of state/decision variables of the problem. The complexity of this representation grows only linearly with the size of the graph, whereas the complexity of exact resolution grows exponentially. We propose an approximate solution algorithm exploiting the structure of a GMDP and whose complexity only grows quadratically with the size of the graph and exponentially with the maximum number of neighbours of any node. This algorithm, referred to as MF-API, belongs to the family of Approximate Policy Iteration (API) algorithms. It relies on a mean-field approximation of the value function of a policy and on a search limited to the suboptimal set of local policies. We compare it, in terms of performance, with two state-of-the-art algorithms for Factored MDPs: SPUDD and Approximate Linear Programming (ALP). Our experiments show that SPUDD is not generally applicable to solving GMDPs, due to the size of the action space we want to tackle. On the other hand, ALP can be adapted to solve GMDPs. We show that ALP is faster than MF-API and provides solutions of similar quality for most problems. However, for some problems MF-API provides significantly better policies, and in all cases provides a better approximation of the value function of approximate policies. These promising results show that the GMDP model offers a convenient framework for modelling and solving a large range of spatial and structured planning problems, that can arise in many different domains where processes are managed over networks: natural resources, agriculture, computer networks, etc. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "A framework and a mean-field algorithm for the local control of spatial processes", "paper_id": "WOS:000298121800005"}