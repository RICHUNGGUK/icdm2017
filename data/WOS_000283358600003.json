{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "user_preferences"}, {"score": 0.04366187180377139, "phrase": "non-verbal_behaviour"}, {"score": 0.04328800171433606, "phrase": "embodied_agent"}, {"score": 0.004772060936021876, "phrase": "facial_expressions"}, {"score": 0.004715465748738294, "phrase": "embodied_conversational_agent"}, {"score": 0.004673458200854065, "phrase": "recommender_dialogue_system"}, {"score": 0.00461802698572959, "phrase": "linguistic_content"}, {"score": 0.004590557276496284, "phrase": "automatically_generated_descriptions"}, {"score": 0.004509120242098446, "phrase": "target_user"}, {"score": 0.004402770481509365, "phrase": "effective_way"}, {"score": 0.004363536354756615, "phrase": "higher-quality_output"}, {"score": 0.004286109357236311, "phrase": "greater_impact"}, {"score": 0.004260605564118872, "phrase": "user_behaviour"}, {"score": 0.004086260758014118, "phrase": "significant_effect"}, {"score": 0.004061941280202248, "phrase": "users'_responses"}, {"score": 0.0037586021907673427, "phrase": "automatically_generated_embodied_output"}, {"score": 0.0035513038412674763, "phrase": "multimodal_dialogue_system"}, {"score": 0.003395727443433411, "phrase": "user-preference_tailoring"}, {"score": 0.003276171848389645, "phrase": "synthesised_speech"}, {"score": 0.0032179767877362512, "phrase": "multimodal_corpus"}, {"score": 0.0031797536775378327, "phrase": "annotated_facial_expressions"}, {"score": 0.0031046598804456366, "phrase": "generated_tailored_descriptions"}, {"score": 0.002950904524853741, "phrase": "artificial_talking_head"}, {"score": 0.00288982217955743, "phrase": "corpus-derived_facial_displays"}, {"score": 0.0027880284928874463, "phrase": "non-verbal_channel"}, {"score": 0.0027714139741120587, "phrase": "users'_ability"}, {"score": 0.002738480561968019, "phrase": "intended_tailoring"}, {"score": 0.0026341187000055396, "phrase": "simple_corpus-derived_rule"}, {"score": 0.002595044162728215, "phrase": "direct_use"}, {"score": 0.0025718774980145786, "phrase": "full_corpus_data"}, {"score": 0.0024738490295633194, "phrase": "rule-based_strategy"}, {"score": 0.0023302648796456452, "phrase": "data-driven_displays"}, {"score": 0.002282000081594605, "phrase": "correctly_tailored_output"}, {"score": 0.002248137100864451, "phrase": "possible_explanation"}, {"score": 0.0021688975382614366, "phrase": "future_systems"}, {"score": 0.0021049977753042253, "phrase": "user-tailored_content"}], "paper_keywords": ["Embodied conversational agents", " Evaluation of generated output", " Multimodal corpora", " User-preference modelling"], "paper_abstract": "Tailoring the linguistic content of automatically generated descriptions to the preferences of a target user has been well demonstrated to be an effective way to produce higher-quality output that may even have a greater impact on user behaviour. It is known that the non-verbal behaviour of an embodied agent can have a significant effect on users' responses to content presented by that agent. However, to date no-one has examined the contribution of non-verbal behaviour to the effectiveness of user tailoring in automatically generated embodied output. We describe a series of experiments designed to address this question. We begin by introducing a multimodal dialogue system designed to generate descriptions and comparisons tailored to user preferences, and demonstrate that the user-preference tailoring is detectable to an overhearer when the output is presented as synthesised speech. We then present a multimodal corpus consisting of the annotated facial expressions used by a speaker to accompany the generated tailored descriptions, and verify that the most characteristic positive and negative expressions used by that speaker are identifiable when resynthesised on an artificial talking head. Finally, we combine the corpus-derived facial displays with the tailored descriptions to test whether the addition of the non-verbal channel improves users' ability to detect the intended tailoring, comparing two strategies for selecting the displays: one based on a simple corpus-derived rule, and one making direct use of the full corpus data. The performance of the subjects who saw displays selected by the rule-based strategy was not significantly different than that of the subjects who got only the linguistic content, while the subjects who saw the data-driven displays were significantly worse at detecting the correctly tailored output. We propose a possible explanation for this result, and also make recommendations for developers of future systems that may make use of an embodied agent to present user-tailored content.", "paper_title": "User preferences can drive facial expressions: evaluating an embodied conversational agent in a recommender dialogue system", "paper_id": "WOS:000283358600003"}