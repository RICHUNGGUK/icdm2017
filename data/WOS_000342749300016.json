{"auto_keywords": [{"score": 0.040983364062434004, "phrase": "hosta"}, {"score": 0.010612921770384169, "phrase": "gpu"}, {"score": 0.007855211529063787, "phrase": "gpu-only_approach"}, {"score": 0.004784093722351956, "phrase": "large-scale_high-order_cfd_simulations"}, {"score": 0.004573537724320323, "phrase": "current_many-core_accelerated_hpc_systems"}, {"score": 0.0043302273768286905, "phrase": "heterogeneous_systems"}, {"score": 0.004220231044214606, "phrase": "tri-level_hybrid_and_heterogeneous_programming_model"}, {"score": 0.0038691325083517617, "phrase": "wcns"}, {"score": 0.0038443136313443126, "phrase": "hdcs"}, {"score": 0.0036986954643952203, "phrase": "dual-level_parallelization_scheme"}, {"score": 0.0036749658368075027, "phrase": "efficient_multi-block_computation"}, {"score": 0.003616303279958699, "phrase": "particular_kernel_optimizations"}, {"score": 0.0035931003474699583, "phrase": "high-order_cfd_schemes"}, {"score": 0.00339083755411773, "phrase": "greater_speedup"}, {"score": 0.0032518541963473405, "phrase": "naive_gpu-only_approach"}, {"score": 0.003199923972285777, "phrase": "novel_scheme"}, {"score": 0.0031286075376993103, "phrase": "store-poor_gpu"}, {"score": 0.0030687492784719746, "phrase": "cpu."}, {"score": 0.0030294669203999565, "phrase": "gpu_load_balance"}, {"score": 0.002961938159281582, "phrase": "maximum_simulation_problem_size"}, {"score": 0.0028588411455879037, "phrase": "collaborative_approach"}, {"score": 0.002629160426591144, "phrase": "pci-e-data_transfer_times"}, {"score": 0.0026122745966589795, "phrase": "ghost_and_singularity_data"}, {"score": 0.002554021496818708, "phrase": "collaborative_computation"}, {"score": 0.0024810246251233195, "phrase": "advanced_cuda"}, {"score": 0.002465116226447436, "phrase": "mpi"}, {"score": 0.0024413739512778136, "phrase": "scalability_tests"}, {"score": 0.002386922774536045, "phrase": "parallel_efficiency"}, {"score": 0.0022816284181888646, "phrase": "eet_high-lift_airfoil_configuration"}, {"score": 0.0022451572368358476, "phrase": "china's_large_civil_airplane_configuration"}, {"score": 0.0021669551936989886, "phrase": "largest-scale_cpu-gpu_collaborative_simulations"}, {"score": 0.0021049977753042253, "phrase": "high-order_schemes"}], "paper_keywords": ["GPU parallelization", " CFD", " CPU-GPU collaboration", " High-order finite difference scheme", " Multi-block structured grid"], "paper_abstract": "Programming and optimizing complex, real-world CFD codes on current many-core accelerated HPC systems is very challenging, especially when collaborating CPUs and accelerators to fully tap the potential of heterogeneous systems. In this paper, with a tri-level hybrid and heterogeneous programming model using MPI + OpenMP + CUDA, we port and optimize our high-order multi-block structured CFD software HOSTA on the GPU-accelerated TianHe-1A supercomputer. HOSTA adopts two self-developed high-order compact definite difference schemes WCNS and HDCS that can simulate flows with complex geometries. We present a dual-level parallelization scheme for efficient multi-block computation on GPUs and perform particular kernel optimizations for high-order CFD schemes. The GPU-only approach achieves a speedup of about 1.3 when comparing one Tesla M2050 GPU with two Xeon X5670 CPUs. To achieve a greater speedup, we collaborate CPU and GPU for HOSTA instead of using a naive GPU-only approach. We present a novel scheme to balance the loads between the store-poor GPU and the store-rich CPU. Taking CPU and GPU load balance into account, we improve the maximum simulation problem size per TianHe-1A node for HOSTA by 2.3x, meanwhile the collaborative approach can improve the performance by around 45% compared to the GPU-only approach. Further, to scale HOSTA on TianHe-1A, we propose a gather/scatter optimization to minimize PCI-e-data transfer times for ghost and singularity data of 3D grid blocks, and overlap the collaborative computation and communication as far as possible using some advanced CUDA and MPI features. Scalability tests show that HOSTA can achieve a parallel efficiency of above 60% on 1024 TianHe-1A nodes. With our method, we have successfully simulated an EET high-lift airfoil configuration containing 800M cells and China's large civil airplane configuration containing 150M cells. To our best knowledge, those are the largest-scale CPU-GPU collaborative simulations that solve realistic CFD problems with both complex configurations and high-order schemes. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Collaborating CPU and GPU for large-scale high-order CFD simulations with complex grids on the TianHe-1A supercomputer", "paper_id": "WOS:000342749300016"}