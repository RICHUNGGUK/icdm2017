{"auto_keywords": [{"score": 0.05007844442530217, "phrase": "delayed_neural_networks"}, {"score": 0.04430846047138782, "phrase": "lkf"}, {"score": 0.004363849604390773, "phrase": "lyapunov-krasovskii"}, {"score": 0.004033464035028803, "phrase": "reciprocal_convex_technique"}, {"score": 0.00376488621224566, "phrase": "new_sufficient_condition"}, {"score": 0.003091767402370757, "phrase": "previously_ignored_terms"}, {"score": 0.0025137856138000014, "phrase": "linear_matrix_inequalities"}, {"score": 0.0023003498123420237, "phrase": "addressed_dnns"}], "paper_keywords": ["Combined convex technique", " delayed neural networks (DNNs)", " global stability", " linear matrix inequality", " time-varying delay"], "paper_abstract": "In this brief, by employing an improved Lyapunov-Krasovskii functional (LKF) and combining the reciprocal convex technique with the convex one, a new sufficient condition is derived to guarantee a class of delayed neural networks (DNNs) to be globally asymptotically stable. Since some previously ignored terms can be considered during the estimation of the derivative of LKF, a less conservative stability criterion is derived in the forms of linear matrix inequalities, whose solvability heavily depends on the information of addressed DNNs. Finally, we demonstrate by two numerical examples that our results reduce the conservatism more efficiently than some currently used methods.", "paper_title": "Combined Convex Technique on Delay-Dependent Stability for Delayed Neural Networks", "paper_id": "WOS:000325980900010"}