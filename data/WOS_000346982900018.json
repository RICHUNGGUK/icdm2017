{"auto_keywords": [{"score": 0.040575248947916934, "phrase": "hybridseg"}, {"score": 0.00918805603287205, "phrase": "local_context"}, {"score": 0.004815163908620779, "phrase": "twitter"}, {"score": 0.004625164125506718, "phrase": "up-to-date_information"}, {"score": 0.004378309349451268, "phrase": "information_retrieval"}, {"score": 0.0041445748748895365, "phrase": "noisy_and_short_nature"}, {"score": 0.003981106369547929, "phrase": "novel_framework"}, {"score": 0.003952082333580496, "phrase": "tweet_segmentation"}, {"score": 0.003908940988435398, "phrase": "batch_mode"}, {"score": 0.0037823114064881357, "phrase": "meaningful_segments"}, {"score": 0.003741016338085083, "phrase": "semantic_or_context_information"}, {"score": 0.0036198068824262464, "phrase": "downstream_applications"}, {"score": 0.0035541673947328163, "phrase": "optimal_segmentation"}, {"score": 0.0034264254358037653, "phrase": "stickiness_scores"}, {"score": 0.00336428077531883, "phrase": "stickiness_score"}, {"score": 0.0032316025706874358, "phrase": "english"}, {"score": 0.00276066021951467, "phrase": "linguistic_features"}, {"score": 0.00257504297785205, "phrase": "confident_segments"}, {"score": 0.0025562434609873335, "phrase": "pseudo_feedback"}, {"score": 0.0024915167458844914, "phrase": "tweet_segmentation_quality"}, {"score": 0.002401875938287542, "phrase": "global_context"}, {"score": 0.002306982970571251, "phrase": "local_linguistic_features"}, {"score": 0.0021597049163247476, "phrase": "high_accuracy"}, {"score": 0.002128272293291141, "phrase": "named_entity_recognition"}], "paper_keywords": ["Twitter stream", " tweet segmentation", " named entity recognition", " linguistic processing", " Wikipedia"], "paper_abstract": "Twitter has attracted millions of users to share and disseminate most up-to-date information, resulting in large volumes of data produced everyday. However, many applications in Information Retrieval (IR) and Natural Language Processing (NLP) suffer severely from the noisy and short nature of tweets. In this paper, we propose a novel framework for tweet segmentation in a batch mode, called HybridSeg. By splitting tweets into meaningful segments, the semantic or context information is well preserved and easily extracted by the downstream applications. HybridSeg finds the optimal segmentation of a tweet by maximizing the sum of the stickiness scores of its candidate segments. The stickiness score considers the probability of a segment being a phrase in English (i.e., global context) and the probability of a segment being a phrase within the batch of tweets (i.e., local context). For the latter, we propose and evaluate two models to derive local context by considering the linguistic features and term-dependency in a batch of tweets, respectively. HybridSeg is also designed to iteratively learn from confident segments as pseudo feedback. Experiments on two tweet data sets show that tweet segmentation quality is significantly improved by learning both global and local contexts compared with using global context alone. Through analysis and comparison, we show that local linguistic features are more reliable for learning local context compared with term-dependency. As an application, we show that high accuracy is achieved in named entity recognition by applying segment-based part-of-speech (POS) tagging.", "paper_title": "Tweet Segmentation and Its Application to Named Entity Recognition", "paper_id": "WOS:000346982900018"}