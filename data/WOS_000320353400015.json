{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "generative_model"}, {"score": 0.004767261858417823, "phrase": "semi-supervised_classification"}, {"score": 0.004720046813265696, "phrase": "semi-supervised"}, {"score": 0.004581162468419365, "phrase": "generative_classifiers"}, {"score": 0.004358676214574977, "phrase": "unlabeled_data_points"}, {"score": 0.004230382915464492, "phrase": "far_more_unlabeled_data"}, {"score": 0.0040048667115255, "phrase": "generative_classification_model"}, {"score": 0.003906357144047631, "phrase": "predictive_deviance_criterion"}, {"score": 0.0037350832473790007, "phrase": "parsimonious_and_relevant_generative_classifier"}, {"score": 0.00367967106192446, "phrase": "semi-supervised_context"}, {"score": 0.003553540530701019, "phrase": "standard_information_criteria"}, {"score": 0.0035008975559602767, "phrase": "aic"}, {"score": 0.0034661752535134556, "phrase": "bic"}, {"score": 0.0033306180744375616, "phrase": "classification_task"}, {"score": 0.0032004205378150354, "phrase": "predictive_power"}, {"score": 0.00299958349498286, "phrase": "computational_cost"}, {"score": 0.002969821115740559, "phrase": "cross-validation_criteria"}, {"score": 0.0029111767710750117, "phrase": "repeated_use"}, {"score": 0.00286795254228461, "phrase": "em_algorithm"}, {"score": 0.0027557911038816256, "phrase": "consistency_properties"}, {"score": 0.00263548363220061, "phrase": "bec"}, {"score": 0.002481747081849503, "phrase": "numerical_experiments"}, {"score": 0.002168975681697056, "phrase": "competing_criteria"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Generative models", " Gaussian mixture models", " Maximum likelihood", " EM algorithm", " Cross-validated error rate", " Information criteria", " AIC", " BIC", " BEC"], "paper_abstract": "Semi-supervised classification can help to improve generative classifiers by taking into account the information provided by the unlabeled data points, especially when there are far more unlabeled data than labeled data. The aim is to select a generative classification model using both unlabeled and labeled data. A predictive deviance criterion, AIC(cond), aiming to select a parsimonious and relevant generative classifier in the semi-supervised context is proposed. In contrast to standard information criteria such as AIC and BIC, AIC(cond) is focused on the classification task, since it attempts to measure the predictive power of a generative model by approximating its predictive deviance. However, it avoids the computational cost of cross-validation criteria, which make repeated use of the EM algorithm. AIC(cond) is proved to have consistency properties that ensure its parsimony when compared with the Bayesian Entropy Criterion (BEC), whose focus is similar to that of AIC(cond). Numerical experiments on both simulated and real data sets show that the behavior of AIC(cond) as regards the selection of variables and models, is encouraging when it is compared to the competing criteria. (c) 2013 Elsevier B.V. All rights reserved.", "paper_title": "A predictive deviance criterion for selecting a generative model in semi-supervised classification", "paper_id": "WOS:000320353400015"}