{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "neuron_structure"}, {"score": 0.034188412736981345, "phrase": "learning_algorithm"}, {"score": 0.004656871909510407, "phrase": "general_insight"}, {"score": 0.004522796947899013, "phrase": "multilayer_perceptron"}, {"score": 0.004195418010131832, "phrase": "common_neuron_structures"}, {"score": 0.004125934074896249, "phrase": "monotonic_activation_functions"}, {"score": 0.0040916230716372265, "phrase": "linear_input_mappings"}, {"score": 0.003990385737207714, "phrase": "proposed_neuron_structure"}, {"score": 0.003940706575253453, "phrase": "nonmonotonic_activation_function"}, {"score": 0.00389164348807591, "phrase": "nonlinear_input_mapping"}, {"score": 0.003685972800046028, "phrase": "high_power_neurons"}, {"score": 0.0035947360742084253, "phrase": "hidden_nodes"}, {"score": 0.0035648266669315943, "phrase": "conventional_mlp"}, {"score": 0.0035204268675160257, "phrase": "classification_problems"}, {"score": 0.003376369479008885, "phrase": "smaller_number"}, {"score": 0.0033482708495261864, "phrase": "network_weights"}, {"score": 0.002941389980674657, "phrase": "search_space"}, {"score": 0.0028327783069076883, "phrase": "local_optimums"}, {"score": 0.002762601646452813, "phrase": "convergence_speed"}, {"score": 0.002426717832834137, "phrase": "appropriate_neuron_structure"}, {"score": 0.002327310963701377, "phrase": "proposed_scheme"}, {"score": 0.002298288831092204, "phrase": "real-world_classification_problems"}, {"score": 0.0022696277881969896, "phrase": "iris_data_classification_problem"}, {"score": 0.0021766418372878835, "phrase": "nonmonotonic_activation_functions"}], "paper_keywords": ["Neuron structure", " Nonmonotonic activation function", " Nonlinear input mapping", " Classification", " Multilayer perceptron (MLP)", " Iris data classification"], "paper_abstract": "This paper gives a general insight into how the neuron structure in a multilayer perceptron (MLP) can affect the ability of neurons to deal with classification. Most of the common neuron structures are based on monotonic activation functions and linear input mappings. In comparison, the proposed neuron structure utilizes a nonmonotonic activation function and/or a nonlinear input mapping to increase the power of a neuron. An MLP of these high power neurons usually requires a less number of hidden nodes than conventional MLP for solving classification problems. The fewer number of neurons is equivalent to the smaller number of network weights that must be optimally determined by a learning algorithm. The performance of learning algorithm is usually improved by reducing the number of weights, i.e., the dimension of the search space. This usually helps the learning algorithm to escape local optimums, and also, the convergence speed of the algorithm is increased regardless of which algorithm is used for learning. Several 2-dimensional examples are provided manually to visualize how the number of neurons can be reduced by choosing an appropriate neuron structure. Moreover, to show the efficiency of the proposed scheme in solving real-world classification problems, the Iris data classification problem is solved using an MLP whose neurons are equipped by nonmonotonic activation functions, and the result is compared with two well-known monotonic activation functions.", "paper_title": "A general insight into the effect of neuron structure on classification", "paper_id": "WOS:000298994400006"}