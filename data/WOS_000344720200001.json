{"auto_keywords": [{"score": 0.04747826965952332, "phrase": "local_features"}, {"score": 0.01961790922629078, "phrase": "video_summarization"}, {"score": 0.008247468676730377, "phrase": "local_feature"}, {"score": 0.005172347863838996, "phrase": "importance_score"}, {"score": 0.004852960762651782, "phrase": "lcc"}, {"score": 0.00481495049065317, "phrase": "locality-constrained_coding"}, {"score": 0.0046762829599982245, "phrase": "quick_comprehension"}, {"score": 0.004651501132938394, "phrase": "video_content"}, {"score": 0.004505531506839144, "phrase": "video_frame"}, {"score": 0.004434265855058074, "phrase": "coverage_problem"}, {"score": 0.004329466192376432, "phrase": "individual_local_features"}, {"score": 0.0043060466510345405, "phrase": "anchor_point"}, {"score": 0.004072714718555654, "phrase": "important_local_features"}, {"score": 0.003976425844082854, "phrase": "first_studies"}, {"score": 0.003934361847786133, "phrase": "local_feature_level"}, {"score": 0.003882404598178925, "phrase": "global_feature_level"}, {"score": 0.0036520680903953623, "phrase": "individual_importance_scores"}, {"score": 0.0034536749437877813, "phrase": "weighted_importance"}, {"score": 0.0032747247100407416, "phrase": "raw_local_feature"}, {"score": 0.003129903501390458, "phrase": "hand-crafted_visual_features"}, {"score": 0.003088537001150045, "phrase": "existing_approaches"}, {"score": 0.0030315379838336334, "phrase": "locality-constrained_linear_coding"}, {"score": 0.002951925359532765, "phrase": "sparse_transformed_space"}, {"score": 0.002874397451360957, "phrase": "manifold_geometric_structure"}, {"score": 0.0028515374472279026, "phrase": "high_dimensional_feature_space"}, {"score": 0.0027988999819725303, "phrase": "low_dimensional_transformed_space"}, {"score": 0.0027399283407250497, "phrase": "anchor_points"}, {"score": 0.002536355456684191, "phrase": "importance_scores"}, {"score": 0.0024697148359079137, "phrase": "boi_representation"}, {"score": 0.0023793421855407746, "phrase": "spatial_weighting_template"}, {"score": 0.002354132757230988, "phrase": "perceptual_difference"}, {"score": 0.002341628146530332, "phrase": "spatial_regions"}, {"score": 0.002232027082211918, "phrase": "intra-frame_properties"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Locality-constrained linear coding", " sparse coding", " video summarization"], "paper_abstract": "Video summarization helps users obtain quick comprehension of video content. Recently, some studies have utilized local features to represent each video frame and formulate video summarization as a coverage problem of local features. However, the importance of individual local features has not been exploited. In this paper, we propose a novel Bag-of-Importance (BoI) model for static video summarization by identifying the frames with important local features as keyframes, which is one of the first studies formulating video summarization at local feature level, instead of at global feature level. That is, by representing each frame with local features, a video is characterized with a bag of local features weighted with individual importance scores and the frames with more important local features are more representative, where the representativeness of each frame is the aggregation of the weighted importance of the local features contained in the frame. In addition, we propose to learn a transformation from a raw local feature to a more powerful sparse nonlinear representation for deriving the importance score of each local feature, rather than directly utilize the hand-crafted visual features like most of the existing approaches. Specifically, we first employ locality-constrained linear coding (LCC) to project each local feature into a sparse transformed space. LCC is able to take advantage of the manifold geometric structure of the high dimensional feature space and form the manifold of the low dimensional transformed space with the coordinates of a set of anchor points. Then we calculate the norm of each anchor point as the importance score of each local feature which is projected to the anchor point. Finally, the distribution of the importance scores of all the local features in a video is obtained as the BoI representation of the video. We further differentiate the importance of local features with a spatial weighting template by taking the perceptual difference among spatial regions of a frame into account. As a result, our proposed video summarization approach is able to exploit both the inter-frame and intra-frame properties of feature representations and identify keyframes capturing both the dominant content and discriminative details within a video. Experimental results on three video datasets across various genres demonstrate that the proposed approach clearly outperforms several state-of-the-art methods.", "paper_title": "A Bag-of-Importance Model With Locality-Constrained Coding Based Feature Learning for Video Summarization", "paper_id": "WOS:000344720200001"}