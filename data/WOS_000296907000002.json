{"auto_keywords": [{"score": 0.038499129022680226, "phrase": "bar-yossef"}, {"score": 0.00481495049065317, "phrase": "aggregate_functions"}, {"score": 0.0046986841646790315, "phrase": "search_engines"}, {"score": 0.004630270043271386, "phrase": "corpus_size"}, {"score": 0.004585212379675535, "phrase": "index_freshness"}, {"score": 0.004302791070310513, "phrase": "art_estimators"}, {"score": 0.004077399487259886, "phrase": "broder"}, {"score": 0.0038827100184187805, "phrase": "inaccurate_approximation"}, {"score": 0.0034187896011435245, "phrase": "rejection_sampling"}, {"score": 0.0033524956061690868, "phrase": "new_estimators"}, {"score": 0.003192312493850036, "phrase": "approximate_degrees"}, {"score": 0.0030847461428168614, "phrase": "careful_implementation"}, {"score": 0.0030397596289795143, "phrase": "approximate_importance_sampling_procedure"}, {"score": 0.0030101325901285537, "phrase": "comprehensive_theoretical_and_empirical_analysis"}, {"score": 0.0028944757265206332, "phrase": "essentially_no_bias"}, {"score": 0.0028244513779243107, "phrase": "document_degrees"}, {"score": 0.0027159094186967247, "phrase": "costly_rejection_sampling_approach"}, {"score": 0.002391067964886602, "phrase": "broder_et_al"}, {"score": 0.002265586601922693, "phrase": "generic_method"}, {"score": 0.0022107428653358715, "phrase": "search_engine_estimators"}, {"score": 0.002157223877108711, "phrase": "rao-blackwellizing"}, {"score": 0.0021049977753042253, "phrase": "performance_improvements"}], "paper_keywords": ["Measurement", " Algorithms", " Corpus size estimation", " evaluation", " sampling", " search engines"], "paper_abstract": "We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities [Bar-Yossef and Gurevich 2008b; Broder et al. 2006] are biased due to inaccurate approximation of the so called \"document degrees\". In addition, the estimators in Bar-Yossef and Gurevich [2008b] are quite costly, due to their reliance on rejection sampling. We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated. By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in Bar-Yossef and Gurevich [2008b]. Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy.", "paper_title": "Efficient Search Engine Measurements", "paper_id": "WOS:000296907000002"}