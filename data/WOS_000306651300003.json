{"auto_keywords": [{"score": 0.04773866258910522, "phrase": "feature_space"}, {"score": 0.04460104344129133, "phrase": "support_vectors"}, {"score": 0.00481495049065317, "phrase": "identifying_top-weighted_features"}, {"score": 0.004774937143916685, "phrase": "polynomial_support_vector_machine_models"}, {"score": 0.004735254735115024, "phrase": "polynomial_support_vector_machine"}, {"score": 0.004676345706416371, "phrase": "degree_d"}, {"score": 0.004637478829869904, "phrase": "linear_functions"}, {"score": 0.004392565073466714, "phrase": "actual_representation"}, {"score": 0.004230595706156783, "phrase": "lagrange_multipliers"}, {"score": 0.004143196873246147, "phrase": "human_understanding"}, {"score": 0.0039737570920809215, "phrase": "polynomial_support_vector_machine_model"}, {"score": 0.003875424718379985, "phrase": "largest_absolute_weights"}, {"score": 0.003795335309109018, "phrase": "time_complexity"}, {"score": 0.0031056437850836326, "phrase": "brute_force_approach"}, {"score": 0.0029909851863915283, "phrase": "largest_weights"}, {"score": 0.0027857990883821504, "phrase": "top-weighted_features"}, {"score": 0.0027167830773160203, "phrase": "true_weight_vector"}, {"score": 0.00246765655376112, "phrase": "classification_performances"}, {"score": 0.002346861891137455, "phrase": "variable_selection_methods"}, {"score": 0.0022886952510350416, "phrase": "new_ability"}, {"score": 0.002194929830604162, "phrase": "polynomial_svm_models"}, {"score": 0.002149494537191303, "phrase": "feature_construction"}, {"score": 0.0021315842986705485, "phrase": "dimensionality_reduction"}, {"score": 0.0021049977753042253, "phrase": "variable_selection"}], "paper_keywords": ["Support Vector Machines", " classification", " variable selection"], "paper_abstract": "Polynomial Support Vector Machine models of degree d are linear functions in a feature space of monomials of at most degree d. However, the actual representation is stored in the form of support vectors and Lagrange multipliers that is unsuitable for human understanding. An efficient, heuristic method for searching the feature space of a polynomial Support Vector Machine model for those features with the largest absolute weights is presented. The time complexity of this method is Theta(dms(2) + sdp), where m is the number of variables, d the degree of the kernel, s the number of support vectors, and p the number of features the algorithm is allowed to search. In contrast, the brute force approach of constructing all weights and then selecting the largest weights has complexity Theta(sd((m+d)(d))). The method is shown to be effective in identifying the top-weighted features on several simulated data sets, where the true weight vector is known. Additionally, the method is run on several high-dimensional, real world data sets where the features returned may be used to construct classifiers with classification performances similar to models built with all or subsets of variables returned by variable selection methods. This algorithm provides a new ability to understand, conceptualize, visualize, and communicate polynomial SVM models and has implications for feature construction, dimensionality reduction, and variable selection.", "paper_title": "To feature space and back: Identifying top-weighted features in polynomial Support Vector Machine models", "paper_id": "WOS:000306651300003"}