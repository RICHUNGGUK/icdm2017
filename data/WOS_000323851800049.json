{"auto_keywords": [{"score": 0.05007473913911182, "phrase": "complex_backgrounds"}, {"score": 0.04925795561529137, "phrase": "gaussians"}, {"score": 0.04804794758325386, "phrase": "mog"}, {"score": 0.004415857432588371, "phrase": "background_variations"}, {"score": 0.0042656009036320585, "phrase": "background_subtraction"}, {"score": 0.004014800383343256, "phrase": "model_convergence_speed"}, {"score": 0.003928834302530914, "phrase": "main_difficulty"}, {"score": 0.0037138072936226, "phrase": "effective_learning_strategy"}, {"score": 0.003634262647873338, "phrase": "better_regularization"}, {"score": 0.0036029221645629225, "phrase": "background_adaptation"}, {"score": 0.003571850980647625, "phrase": "mog._first"}, {"score": 0.003332704619499476, "phrase": "background_distributions"}, {"score": 0.0031502008922533894, "phrase": "dynamic_and_static_background"}, {"score": 0.003003579442221871, "phrase": "hypothesis_testing_method"}, {"score": 0.002939202022371694, "phrase": "two-layer_lbp-based_method"}, {"score": 0.0028145455182562807, "phrase": "global_and_static_learning_rates"}, {"score": 0.002754208576828914, "phrase": "adaptive_learning_rates"}, {"score": 0.0027304365438083874, "phrase": "image_pixels"}, {"score": 0.0027068691342344545, "phrase": "distinct_properties"}, {"score": 0.0026033013834547507, "phrase": "proposed_learning_strategy"}, {"score": 0.0025696647013416863, "phrase": "novel_background"}, {"score": 0.002525489173110693, "phrase": "foreground_objects"}, {"score": 0.0025036862991477437, "phrase": "complex_environments"}, {"score": 0.002315719347244451, "phrase": "experimental_results"}, {"score": 0.0021982976170703884, "phrase": "proposed_learning_rate_control_strategy"}, {"score": 0.0021604924928015283, "phrase": "existing_mog_approaches"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Background modeling", " Background subtraction", " Gaussian mixture models", " Learning-rate", " Foreground detection"], "paper_abstract": "Mixture of Gaussians (MoG) is well-known for effectively in sustaining background variations, which has been widely adopted for background subtraction. However, in complex backgrounds, MoG often traps in keeping balance between model convergence speed and its stability. The main difficulty is the selection of learning rates. In this paper, an effective learning strategy is proposed to provide better regularization of background adaptation for MoG. First, the video-data is splitted into the future-data and history-data, then a set of background distributions (MoG) is computed for each case. To distinguish between dynamic and static background, the equality of these two sets is tested by the hypothesis testing method. Next, a two-layer LBP-based method is proposed for foreground classification. Finally, the global and static learning rates are replaced by the adaptive learning rates for image pixels with distinct properties for each frame. By means of the proposed learning strategy, a novel background modeling for detecting foreground objects from complex environments is established. We compare our procedure against the state-of-the-art alternatives, the experimental results show that the performance of learning speed and accuracy obtained by proposed learning rate control strategy is better than existing MoG approaches. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Future-data driven modeling of complex backgrounds using mixture of Gaussians", "paper_id": "WOS:000323851800049"}