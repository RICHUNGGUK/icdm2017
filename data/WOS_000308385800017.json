{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high-dimensional_data"}, {"score": 0.00462773520532301, "phrase": "central_problem"}, {"score": 0.004566954456805558, "phrase": "machine_learning"}, {"score": 0.004506968388996092, "phrase": "pattern_recognition"}, {"score": 0.004418456827500267, "phrase": "large_datasets"}, {"score": 0.004054479957160662, "phrase": "search-based_or_wrapper_techniques"}, {"score": 0.0036714667610391535, "phrase": "prohibitively_long_time"}, {"score": 0.003623199939953831, "phrase": "high-dimensional_datasets"}, {"score": 0.003216285378512588, "phrase": "low-complexity_relevance_and_redundancy_criteria"}, {"score": 0.0029903851294825023, "phrase": "unsupervised_learning"}, {"score": 0.0028173769301594745, "phrase": "computationally_intensive_methods"}, {"score": 0.0027256122357680393, "phrase": "smaller_subsets"}, {"score": 0.002689746491842271, "phrase": "promising_features"}, {"score": 0.002636828502928141, "phrase": "experimental_results"}, {"score": 0.002451527111936163, "phrase": "time_efficiency"}, {"score": 0.0023716501597131024, "phrase": "lower_generalization_error"}], "paper_keywords": ["Feature selection", " Filters", " Dispersion measures", " Similarity measures", " High-dimensional data"], "paper_abstract": "Feature selection is a central problem in machine learning and pattern recognition. On large datasets (in terms of dimension and/or number of instances), using search-based or wrapper techniques can be cornputationally prohibitive. Moreover, many filter methods based on relevance/redundancy assessment also take a prohibitively long time on high-dimensional datasets. In this paper, we propose efficient unsupervised and supervised feature selection/ranking filters for high-dimensional datasets. These methods use low-complexity relevance and redundancy criteria, applicable to supervised, semi-supervised, and unsupervised learning, being able to act as pre-processors for computationally intensive methods to focus their attention on smaller subsets of promising features. The experimental results, with up to 10(5) features, show the time efficiency of our methods, with lower generalization error than state-of-the-art techniques, while being dramatically simpler and faster. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Efficient feature selection filters for high-dimensional data", "paper_id": "WOS:000308385800017"}