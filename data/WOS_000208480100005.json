{"auto_keywords": [{"score": 0.032872094792720755, "phrase": "recognition"}, {"score": 0.00481495049065317, "phrase": "naturalistic_interaction"}, {"score": 0.004769444094319619, "phrase": "affective_and_human-centered_computing"}, {"score": 0.0042354707960660706, "phrase": "multimodal_input"}, {"score": 0.004077508669922702, "phrase": "facial_expressions"}, {"score": 0.004038943596016986, "phrase": "prosody_information"}, {"score": 0.003925414526537783, "phrase": "users'_emotional_state"}, {"score": 0.0038698476651328898, "phrase": "unintrusive_manner"}, {"score": 0.0037789720824759503, "phrase": "best_performing_modality"}, {"score": 0.0036207206487324506, "phrase": "bad_sensing_conditions"}, {"score": 0.0033395980635636644, "phrase": "naturalistic_video_sequences"}, {"score": 0.0032302647179383915, "phrase": "nearly_real_world_situations"}, {"score": 0.0031693980719538317, "phrase": "controlled_recording_conditions"}, {"score": 0.0031393948330209224, "phrase": "audiovisual_material"}, {"score": 0.003036595866130997, "phrase": "recurrent_neural_network"}, {"score": 0.002909342017421165, "phrase": "dynamic_events"}, {"score": 0.002881793492919646, "phrase": "facial_and_prosodic_expressivity"}, {"score": 0.002787406053363446, "phrase": "existing_work"}, {"score": 0.0027218799769229596, "phrase": "user_expressivity"}, {"score": 0.002683304054823427, "phrase": "dimensional_representation"}, {"score": 0.0026202187680903063, "phrase": "discrete_'universal_emotions"}, {"score": 0.0025343769150179764, "phrase": "everyday_human-machine_interaction"}, {"score": 0.0024397017438101726, "phrase": "audiovisual_database"}, {"score": 0.002382329940096419, "phrase": "human-human_discourse"}, {"score": 0.0022933216359142736, "phrase": "subtle_variations"}, {"score": 0.0022393843660697484, "phrase": "emotion_labels"}, {"score": 0.0021049977753042253, "phrase": "recognition_rates"}], "paper_keywords": ["Affective computing", " Emotion recognition", " Recurrent neural network", " Emotion dynamics", " Multimodal analysis"], "paper_abstract": "Affective and human-centered computing have attracted an abundance of attention during the past years, mainly due to the abundance of environments and applications able to exploit and adapt to multimodal input from the users. The combination of facial expressions with prosody information allows us to capture the users' emotional state in an unintrusive manner, relying on the best performing modality in cases where one modality suffers from noise or bad sensing conditions. In this paper, we describe a multi-cue, dynamic approach to detect emotion in naturalistic video sequences, where input is taken from nearly real world situations, contrary to controlled recording conditions of audiovisual material. Recognition is performed via a recurrent neural network, whose short term memory and approximation capabilities cater for modeling dynamic events in facial and prosodic expressivity. This approach also differs from existing work in that it models user expressivity using a dimensional representation, instead of detecting discrete 'universal emotions', which are scarce in everyday human-machine interaction. The algorithm is deployed on an audiovisual database which was recorded simulating human-human discourse and, therefore, contains less extreme expressivity and subtle variations of a number of emotion labels. Results show that in turns lasting more than a few frames, recognition rates rise to 98%.", "paper_title": "Multimodal user's affective state analysis in naturalistic interaction", "paper_id": "WOS:000208480100005"}