{"auto_keywords": [{"score": 0.02461228443759463, "phrase": "lyapunov"}, {"score": 0.00481495049065317, "phrase": "reinforcement_learning_controller_design"}, {"score": 0.004768104290288521, "phrase": "affine_nonlinear_discrete-time_systems"}, {"score": 0.004721711709176679, "phrase": "online_approximators"}, {"score": 0.004585212379675535, "phrase": "reinforcement_learning_state-"}, {"score": 0.004540591176992658, "phrase": "output-feedback-based_adaptive_critic_controller_designs"}, {"score": 0.004260906498835763, "phrase": "general_multi-input"}, {"score": 0.004198839649063847, "phrase": "affine_unknown_nonlinear_discrete-time_systems"}, {"score": 0.004017989546119125, "phrase": "proposed_controller_design"}, {"score": 0.0037888687452790953, "phrase": "optimal_signal"}, {"score": 0.0037336520904376687, "phrase": "critic_network"}, {"score": 0.0036079110889031874, "phrase": "action_network"}, {"score": 0.003503497128638102, "phrase": "cost-to-go_function"}, {"score": 0.0034020946201397057, "phrase": "recursive_equations"}, {"score": 0.0033524956061690868, "phrase": "heuristic_dynamic_programming"}, {"score": 0.003287482882222731, "phrase": "neural_networks"}, {"score": 0.0030101325901285537, "phrase": "radial_basis_functions"}, {"score": 0.002951739421935396, "phrase": "fuzzy_logic"}, {"score": 0.0027969169552385974, "phrase": "output-feedback_counterpart"}, {"score": 0.0027561164046279413, "phrase": "additional_nn"}, {"score": 0.0026372419109713923, "phrase": "unavailable_system_states"}, {"score": 0.002573424607574822, "phrase": "separation_principle"}, {"score": 0.002498873893632726, "phrase": "nn_weight"}, {"score": 0.0024383967028975616, "phrase": "controller_schemes"}, {"score": 0.0023677483882740317, "phrase": "uniform_ultimate_boundedness"}, {"score": 0.002333193766510394, "phrase": "closed-loop_system"}, {"score": 0.0021361800485636823, "phrase": "pendulum_balancing_system"}, {"score": 0.0021049977753042253, "phrase": "two-link_robotic_arm_system"}], "paper_keywords": ["Adaptive critic", " dynamic programming (DP)", " Lyapunov method", " neural networks (NNs)", " online approximators (OLAs)", " online learning", " reinforcement learning"], "paper_abstract": "In this paper, reinforcement learning state- and output-feedback-based adaptive critic controller designs are proposed by using the online approximators (OLAs) for a general multi-input and multioutput affine unknown nonlinear discrete-time systems in the presence of bounded disturbances. The proposed controller design has two entities, an action network that is designed to produce optimal signal and a critic network that evaluates the performance of the action network. The critic estimates the cost-to-go function which is tuned online using recursive equations derived from heuristic dynamic programming. Here, neural networks (NNs) are used both for the action and critic whereas any OLAs, such as radial basis functions, splines, fuzzy logic, etc., can be utilized. For the output-feedback counterpart, an additional NN is designated as the observer to estimate the unavailable system states, and thus, separation principle is not required. The NN weight tuning laws for the controller schemes are also derived while ensuring uniform ultimate boundedness of the closed-loop system using Lyapunov theory. Finally, the effectiveness of the two controllers is tested in simulation on a pendulum balancing system and a two-link robotic arm system.", "paper_title": "Reinforcement Learning Controller Design for Affine Nonlinear Discrete-Time Systems using Online Approximators", "paper_id": "WOS:000302097000008"}