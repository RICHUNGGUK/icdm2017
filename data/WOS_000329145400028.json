{"auto_keywords": [{"score": 0.0495133738546852, "phrase": "depth_map_sequences"}, {"score": 0.045984768987991385, "phrase": "depth_maps"}, {"score": 0.00481495049065317, "phrase": "human_action_recognition"}, {"score": 0.004701825207431857, "phrase": "space-time_occupancy_patterns"}, {"score": 0.0045641317216991205, "phrase": "new_visual_representation"}, {"score": 0.00430067200637091, "phrase": "new_representation"}, {"score": 0.004249825947483216, "phrase": "space_and_time_axes"}, {"score": 0.004149922666541166, "phrase": "multiple_segments"}, {"score": 0.003728529119516347, "phrase": "occupancy_value"}, {"score": 0.0035341028694769036, "phrase": "space-time_points"}, {"score": 0.0033898953358387075, "phrase": "occupancy_values"}, {"score": 0.003290494847464381, "phrase": "high_dimensional_feature_vector"}, {"score": 0.003232254571848865, "phrase": "space-time_occupancy_pattern"}, {"score": 0.0030819219279832224, "phrase": "dimensionality_reduction"}, {"score": 0.0030273620861817055, "phrase": "lower-dimensional_feature_vectors"}, {"score": 0.002852356597632014, "phrase": "spatial_and_temporal_contextual_information"}, {"score": 0.0026556165534143915, "phrase": "intra-action_variations"}, {"score": 0.0024577274522206436, "phrase": "view_invariance"}, {"score": 0.002399850604120431, "phrase": "automatic_segmentation"}, {"score": 0.002371424085814217, "phrase": "time_alignment_method"}, {"score": 0.0023433334920926713, "phrase": "on-line_recognition"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Pattern recognition", " Human action", " Depth map"], "paper_abstract": "We present a new visual representation for 3D action recognition from sequences of depth maps. In this new representation, space and time axes are divided into multiple segments to define a 40 grid for each depth map sequences. Each cell in the grid is associated with an occupancy value which is a function of the number of space-time points falling into this cell. The occupancy values of all the cells form a high dimensional feature vector, called Space-Time Occupancy Pattern (STOP). We then perform dimensionality reduction to obtain lower-dimensional feature vectors. The advantage of STOP is that it preserves spatial and temporal contextual information between space and time cells while being flexible enough to accommodate intra-action variations. Furthermore, we combine depth maps with skeletons in order to obtain view invariance and present an automatic segmentation and time alignment method for on-line recognition of depth sequences. Our visual representation is validated with experiments on a public 3D human action dataset. (C) 2013 Elsevier B.V. All rights reserved,", "paper_title": "On the improvement of human action recognition from depth map sequences using Space-Time Occupancy Patterns", "paper_id": "WOS:000329145400028"}