{"auto_keywords": [{"score": 0.03987218711500845, "phrase": "document_vectors"}, {"score": 0.02524353822395531, "phrase": "training_corpus"}, {"score": 0.00481495049065317, "phrase": "large_text_collections"}, {"score": 0.0046741260449047976, "phrase": "neural_networks"}, {"score": 0.004628101576367193, "phrase": "automatic_text_categorization"}, {"score": 0.004515004397496303, "phrase": "appropriate_surrogates"}, {"score": 0.004404658750724105, "phrase": "text_collection"}, {"score": 0.004109770306454165, "phrase": "leaming_systems"}, {"score": 0.0040491842732233154, "phrase": "unseen_documents"}, {"score": 0.00395017798595193, "phrase": "different_measures"}, {"score": 0.003507504253087326, "phrase": "kohonen's_sofm"}, {"score": 0.003455765987713262, "phrase": "vapniak"}, {"score": 0.0034047882921717913, "phrase": "ne_methods"}, {"score": 0.0032241994368246065, "phrase": "'real-world'_news_stream"}, {"score": 0.003114283314285167, "phrase": "optimal_size"}, {"score": 0.0028768722873689432, "phrase": "best_compromise"}, {"score": 0.0028484940955893134, "phrase": "categorization_accuracy"}, {"score": 0.0026443691099806003, "phrase": "five_different_surrogate_vector_models"}, {"score": 0.002605329434787829, "phrase": "first_two_surrogates"}, {"score": 0.0025541693207036167, "phrase": "tfidf_and_weirdness_measures"}, {"score": 0.002504011301105577, "phrase": "third_surrogate"}, {"score": 0.00240662380079115, "phrase": "high-frequency_words"}, {"score": 0.0023245146355917626, "phrase": "fourth_vector_model"}, {"score": 0.0022675820982741347, "phrase": "standardised_terminology_database"}, {"score": 0.0022120408738490437, "phrase": "fifth_surrogate"}, {"score": 0.002168586864007222, "phrase": "evaluation_purposes"}, {"score": 0.0021049977753042253, "phrase": "random_selection"}], "paper_keywords": ["feature selection", " text categorisation", " information extraction", " self-organizing feature map (SOFM)", " support vector machine (SVM)"], "paper_abstract": "Automatic text categorization requires the construction of appropriate surrogates for documents within a text collection. The surrogates, often called document vectors, are used to train leaming systems for categorising unseen documents. A comparison of different measures (tftdf and weirdness) for creating document vectors is presented together with two different state-of-the-art classifiers: supervised Kohonen's SOFM and unsupervised Vapniak's SVM. ne methods are tested using two 'gold standard' document collections and one data set from a 'real-world' news stream. There appears to be an optimal size both for the of document vectors and for the dimensionality of each vector that gives the best compromise between categorization accuracy and training time. The performance of each of the classifiers was computed for five different surrogate vector models: the first two surrogates were created with tfidf and weirdness measures accordingly, the third surrogate was created purely on the basis of high-frequency words in the training corpus, and the fourth vector model was created from a standardised terminology database. Finally, the fifth surrogate (used for evaluation purposes) was based on a random selection of words from the training corpus.", "paper_title": "Categorization of large text collections: Feature selection for training neural networks", "paper_id": "WOS:000241790900120"}