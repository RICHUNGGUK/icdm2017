{"auto_keywords": [{"score": 0.03201243105298459, "phrase": "policy_vectors"}, {"score": 0.00481495049065317, "phrase": "personal_assistant_agents"}, {"score": 0.004717723626157272, "phrase": "agent_teams"}, {"score": 0.004392565073466714, "phrase": "key_processes"}, {"score": 0.00411064950774695, "phrase": "periodic_decisions"}, {"score": 0.0037307208089736835, "phrase": "uncertain_environment"}, {"score": 0.0035997452281985465, "phrase": "optimal_policy_generation"}, {"score": 0.0032668821950788502, "phrase": "speedup_pomdp_policy_generation"}, {"score": 0.0031042018721479385, "phrase": "personal_assistant_domains"}, {"score": 0.0029951555610569225, "phrase": "policy_computation"}, {"score": 0.0029196116767956273, "phrase": "belief_space_polytope"}, {"score": 0.002831462722290752, "phrase": "progress_structure"}, {"score": 0.002690406375639144, "phrase": "lagrangian_methods"}, {"score": 0.0026359661683151006, "phrase": "bounded_belief_space_support"}, {"score": 0.002479150189700726, "phrase": "bounded_belief_polytope"}, {"score": 0.00235560370536971, "phrase": "fastest_existing_algorithms"}, {"score": 0.002331641523873951, "phrase": "exact_pomdp_policy_generation"}, {"score": 0.002272794656458419, "phrase": "magnitude_speedups"}, {"score": 0.0021049977753042253, "phrase": "human_users"}], "paper_keywords": [""], "paper_abstract": "Agents or agent teams deployed to assist humans often face the challenges of monitoring the state of key processes in their environment (including the state of their human users themselves) and making periodic decisions based on such monitoring. POMDPs appear well suited to enable agents to address these challenges, given the uncertain environment and cost of actions, but optimal policy generation for POMDPs is computationally expensive. This paper introduces two key implementation techniques (one exact and one approximate) to speedup POMDP policy generation that exploit the notion of progress or dynamics in personal assistant domains and the density of policy vectors. Policy computation is restricted to the belief space polytope that remains reachable given the progress structure of a domain. One is based on applying Lagrangian methods to compute a bounded belief space support in polynomial time and other based on approximating policy vectors in the bounded belief polytope. We illustrate this by enhancing two of the fastest existing algorithms for exact POMDP policy generation. The order of magnitude speedups demonstrate the utility of our implementation techniques in facilitating the deployment of POMDPs within agents assisting human users.", "paper_title": "Implementation techniques for solving POMDPs in personal assistant agents", "paper_id": "WOS:000236893200005"}