{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "smart_multimodal_digital_signage"}, {"score": 0.004678940805063634, "phrase": "novel_multimodal_system"}, {"score": 0.004612376827433774, "phrase": "multi-party_human-human_interaction_analysis"}, {"score": 0.004503525064934708, "phrase": "human-machine_interfaces"}, {"score": 0.004355415885815204, "phrase": "simultaneous_processing"}, {"score": 0.004132411680987228, "phrase": "proposed_system"}, {"score": 0.004054169947971483, "phrase": "large_display"}, {"score": 0.003996458454723708, "phrase": "multiple_sensing_devices"}, {"score": 0.0037556843869894566, "phrase": "multiple_users"}, {"score": 0.0034625012596827334, "phrase": "displayed_content"}, {"score": 0.0033806943112103397, "phrase": "particular_devices"}, {"score": 0.0033166379485150507, "phrase": "motion_capture_sensors"}, {"score": 0.00326939082861424, "phrase": "mounted_devices"}, {"score": 0.003222814590544802, "phrase": "acoustic_and_visual_information"}, {"score": 0.0029569247903654477, "phrase": "individual_speech"}, {"score": 0.002928765962664163, "phrase": "gaze_direction"}, {"score": 0.002859533151305368, "phrase": "new_framework"}, {"score": 0.00276534038425712, "phrase": "verbal_and_nonverbal_communication_events"}, {"score": 0.0027129118211250336, "phrase": "audio_signals"}, {"score": 0.0026742419973883134, "phrase": "speaker_diarization"}, {"score": 0.002648768009464535, "phrase": "head_poses"}, {"score": 0.002611010097359509, "phrase": "video_images"}, {"score": 0.0025614999929672, "phrase": "hybrid_dynamical_systems"}, {"score": 0.0024652715040303416, "phrase": "hds_temporal_structure_characteristics"}, {"score": 0.002406968743305622, "phrase": "multimodal_interaction_level_estimation"}, {"score": 0.0022834993174767016, "phrase": "multi-party_communication_experience"}, {"score": 0.0022294857939412073, "phrase": "synthetic_and_real-world_datasets"}, {"score": 0.002166349705365602, "phrase": "poster_presentations"}, {"score": 0.0021049977753042253, "phrase": "proposed_multimodal_system"}], "paper_keywords": ["Human-machine system", " multimodal interaction dynamics", " multiparty interaction", " smart digital signage"], "paper_abstract": "This paper presents a novel multimodal system designed for multi-party human-human interaction analysis. The design of human-machine interfaces for multiple users is challenging because simultaneous processing of actions and reactions have to be consistent. The proposed system consists of a large display equipped with multiple sensing devices: microphone array, HD video cameras, and depth sensors. Multiple users positioned in front of the panel freely interact using voice or gesture while looking at the displayed content, without wearing any particular devices (such as motion capture sensors or head mounted devices). Acoustic and visual information is captured and processed jointly using established and state-of-the-art techniques to obtain individual speech and gaze direction. Furthermore, a new framework is proposed to model A/V multimodal interaction between verbal and nonverbal communication events. Dynamics of audio signals obtained from speaker diarization and head poses extracted from video images are modeled using hybrid dynamical systems (HDS). We show that HDS temporal structure characteristics can be used for multimodal interaction level estimation, which is useful feedback that can help to improve multi-party communication experience. Experimental results using synthetic and real-world datasets of group communication such as poster presentations show the feasibility of the proposed multimodal system.", "paper_title": "Multiparty Interaction Understanding Using Smart Multimodal Digital Signage", "paper_id": "WOS:000342229500006"}