{"auto_keywords": [{"score": 0.04172895263200361, "phrase": "trust_evaluations"}, {"score": 0.027507999683207565, "phrase": "alignment_process"}, {"score": 0.00481495049065317, "phrase": "open_multi-agent_systems_trust_models"}, {"score": 0.0045850648862091085, "phrase": "effective_interactions"}, {"score": 0.004389912080394437, "phrase": "open_systems"}, {"score": 0.004024072318264299, "phrase": "semantic_differences"}, {"score": 0.003916098498242888, "phrase": "different_agents"}, {"score": 0.0037697665240643066, "phrase": "communicated_trust_evaluations"}, {"score": 0.003531474887552889, "phrase": "currently_proposed_solutions"}, {"score": 0.0034554281349858836, "phrase": "common_ontologies"}, {"score": 0.0034180189629740426, "phrase": "ontology_alignment_methods"}, {"score": 0.003344407169522133, "phrase": "additional_problems"}, {"score": 0.003272375502853897, "phrase": "novel_approach"}, {"score": 0.003167217223469026, "phrase": "trust_alignment"}, {"score": 0.002934764205541183, "phrase": "mathematical_framework"}, {"score": 0.00230932021677234, "phrase": "first-order_regression_algorithm"}, {"score": 0.00217496508306238, "phrase": "example_scenario"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Alignment", " Channel theory", " Regression", " Trust"], "paper_abstract": "In open multi-agent systems trust models are an important tool for agents to achieve effective interactions. However, in these kinds of open systems, the agents do not necessarily use the same, or even similar, trust models, leading to semantic differences between trust evaluations in the different agents. Hence, to successfully use communicated trust evaluations, the agents need to align their trust models. We explicate that currently proposed solutions, such as common ontologies or ontology alignment methods, lead to additional problems and propose a novel approach. We show how the trust alignment can be formed by considering the interactions that agents share and describe a mathematical framework to formulate precisely how the interactions support trust evaluations for both agents. We show how this framework can be used in the alignment process and explain how an alignment should be learned. Finally, we demonstrate this alignment process in practice, using a first-order regression algorithm, to learn an alignment and test it in an example scenario. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Engineering trust alignment: Theory, method and experimentation", "paper_id": "WOS:000304230100004"}