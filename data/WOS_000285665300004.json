{"auto_keywords": [{"score": 0.04590026036829434, "phrase": "human_behavior"}, {"score": 0.00481495049065317, "phrase": "intelligibility_predictors"}, {"score": 0.004691273968935458, "phrase": "great_deal"}, {"score": 0.004630626571109149, "phrase": "human_speech_perception"}, {"score": 0.004482400730101349, "phrase": "acoustic_factors"}, {"score": 0.003236802914194731, "phrase": "making_noise_distribution"}, {"score": 0.002477753633057247, "phrase": "automatic_speech_recognition_systems"}, {"score": 0.0023826870912207324, "phrase": "error_patterns"}, {"score": 0.0022468671200629024, "phrase": "testing_and_training_data_sets"}, {"score": 0.002105002791163301, "phrase": "elsevier"}], "paper_keywords": ["Speech perception", " Articulation", " Index", " Speech recognition", " Speech representation"], "paper_abstract": "Intelligibility predictors tell us a great deal about human speech perception, in particular which acoustic factors strongly effect human behavior and which do not A particular intelligibility predictor, the Articulation Index (AI), is interesting because it models human behavior in noise, and its form has implications about representation of speech in the brain Specifically, the Articulation Index implies that a listener pre-consciously estimates the making noise distribution and uses it to classify time/frequency samples as speech or non-speech We classify consonants using representations of speech and noise which are consistent with this hypothesis and determine whether their error rate and error patterns are more or less consistent with human behavior than representations typical of automatic speech recognition systems The new representations resulted in error patterns more similar to humans in cases where the testing and training data sets do not have the same masking noise spectrum (C) 2010 Elsevier B V All rights reserved", "paper_title": "Intelligibility predictors and neural representation of speech", "paper_id": "WOS:000285665300004"}