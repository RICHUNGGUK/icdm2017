{"auto_keywords": [{"score": 0.026374708875538982, "phrase": "svm"}, {"score": 0.011524930581177147, "phrase": "mb"}, {"score": 0.007360975882638268, "phrase": "hmm"}, {"score": 0.00481495049065317, "phrase": "real-time_lip-synch_system"}, {"score": 0.004632039024076267, "phrase": "real_time_lip-synch_system"}, {"score": 0.004494570360108983, "phrase": "incoming_speech_utterance"}, {"score": 0.004398872331144531, "phrase": "real_time_operation"}, {"score": 0.004286708921931472, "phrase": "processing_time"}, {"score": 0.004195418010131832, "phrase": "merge_and_split\"_procedures"}, {"score": 0.0040708542958968605, "phrase": "fine_phoneme_classification"}, {"score": 0.003967021436047945, "phrase": "phoneme_classification"}, {"score": 0.003816197777388722, "phrase": "computational_load"}, {"score": 0.003751011827704279, "phrase": "desired_accuracy"}, {"score": 0.0037028514035425037, "phrase": "coarse-to-fine_phoneme_classification"}, {"score": 0.003592859432554987, "phrase": "feature_extraction"}, {"score": 0.003531474887552889, "phrase": "first_stage"}, {"score": 0.003486123304074186, "phrase": "speech_frame"}, {"score": 0.003382547312072468, "phrase": "lip_opening"}, {"score": 0.003353521531649663, "phrase": "mel_frequency_cepstral_coefficients"}, {"score": 0.003212074324593725, "phrase": "second_stage"}, {"score": 0.0031032109441552287, "phrase": "detailed_lip_shape"}, {"score": 0.00307657476125683, "phrase": "formant_information"}, {"score": 0.0028346293790895024, "phrase": "real-time_lip-synch"}, {"score": 0.002680114113899346, "phrase": "intel_pentium_iv"}, {"score": 0.0025014259919355453, "phrase": "phoneme_merging"}, {"score": 0.0022749744354271834, "phrase": "single_frame"}, {"score": 0.002236055625099805, "phrase": "proposed_method"}, {"score": 0.0021232402128269906, "phrase": "identical_conditions"}], "paper_keywords": ["lip-synch", " support vector machine", " real-time", " viseme", " speech"], "paper_abstract": "In this paper, we present a real time lip-synch system that activates 2-D avatar's lip motion in synch with incoming speech utterance. To achieve the real time operation of the system, the processing time was minimized by \"merge and split\" procedures resulting in coarse-to-fine phoneme classification. At each stage of phoneme classification, the support vector machine (SVM) method was applied to reduce the computational load while maintaining the desired accuracy. The coarse-to-fine phoneme classification, is accomplished via two_stages of feature extraction: in the first stage, each speech frame is acoustically analyzed for three classes of lip opening using Mel Frequency Cepstral Coefficients (MFCC) as a feature; in the second stage, each frame is further refined for detailed lip shape using formant information. The method was implemented in 2-D lip animation and it was demonstrated that the system was effective in accomplishing real-time lip-synch. This approach was tested on a PC using the Microsoft Visual Studio with an Intel Pentium IV 1.4 Giga Hz CPU and 384 MB RAM. It was observed that the methods of phoneme merging and SVM achieved about twice the speed in recognition than the method employing the Hidden Markov Model (HMM). A typical latency time per a single frame observed using the proposed method was in the order of 18.22 milliseconds while an HMM method under identical conditions resulted about 30.67 milliseconds.", "paper_title": "SVM-based phoneme classification and lip shape refinement in real-time lip-synch system", "paper_id": "WOS:000243210800004"}