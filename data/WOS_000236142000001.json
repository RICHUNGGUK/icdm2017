{"auto_keywords": [{"score": 0.03814106342618422, "phrase": "standard_deviation"}, {"score": 0.00481495049065317, "phrase": "non-cooperative_behavior"}, {"score": 0.004345706062641742, "phrase": "reward_collection"}, {"score": 0.004165987300206134, "phrase": "overall_collection_time"}, {"score": 0.0038054151244558123, "phrase": "neighboring_agents"}, {"score": 0.0037371212381934853, "phrase": "normally_distributed_certainty"}, {"score": 0.0034342161742428666, "phrase": "varying_number"}, {"score": 0.0026970382454914437, "phrase": "power_law_relationship"}, {"score": 0.0026646374739979694, "phrase": "optimum_conditions"}, {"score": 0.002448446849997207, "phrase": "feedback_loop"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["agent population", " co-operation", " reward collection", " armed bandit search", " optimum standard deviation", " exploitation and exploration"], "paper_abstract": "We investigate the amount of cooperation between agents in a population during reward collection that is required to minimize the overall collection time. In our computer simulation agents have the option to broadcast the position of a reward to neighboring agents with a normally distributed certainty. We modify the standard deviation of this certainty to investigate its optimum setting for a varying number of agents and rewards. Results reveal that an optimum exists and that (a) the collection time and the number of agents and (b) the collection time and the number of rewards, follow a power law relationship under optimum conditions. We suggest that the standard deviation can be self-tuned via a feedback loop and list some examples from nature were we believe this self-tuning to take place. (c) 2005 IMACS. Published by Elsevier B.V. All rights reserved.", "paper_title": "On the advantages of non-cooperative behavior in agent populations", "paper_id": "WOS:000236142000001"}