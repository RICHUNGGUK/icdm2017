{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "kernel_features"}, {"score": 0.009494209760949475, "phrase": "complex_decision_boundaries"}, {"score": 0.008492838256746649, "phrase": "decision_tree_ensembles"}, {"score": 0.0047385702897315436, "phrase": "classifier_ensemble"}, {"score": 0.004468684611798047, "phrase": "new_examples"}, {"score": 0.004421393652203638, "phrase": "classifiers"}, {"score": 0.00421410521089045, "phrase": "kernel_functions"}, {"score": 0.003727527379253624, "phrase": "representational_power"}, {"score": 0.003687960013746783, "phrase": "individual_classifiers"}, {"score": 0.0032793933580835574, "phrase": "kernel_parameters"}, {"score": 0.003108911488613549, "phrase": "kernel_feature_space"}, {"score": 0.0028391271282216758, "phrase": "existing_ensemble_methods"}, {"score": 0.002794001681064862, "phrase": "kernel_machine_philosophy"}, {"score": 0.00260657976055413, "phrase": "original_features"}, {"score": 0.0024578219177450876, "phrase": "extended_feature_spaces"}, {"score": 0.002431699464104896, "phrase": "experimental_results"}, {"score": 0.0021276199272747404, "phrase": "bagging"}, {"score": 0.0021050060304094956, "phrase": "adaboost"}], "paper_keywords": ["Classifier ensembles", " Decision trees", " Kernel features", " Random subspaces", " Bagging", " AdaBoost.M1", " Random forests"], "paper_abstract": "A classifier ensemble is a set of classifiers whose individual decisions are combined to classify new examples. Classifiers, which can represent complex decision boundaries are accurate. Kernel functions can also represent complex decision boundaries. In this paper, we study the usefulness of kernel features for decision tree ensembles as they can improve the representational power of individual classifiers. We first propose decision tree ensembles based on kernel features and found that the performance of these ensembles is strongly dependent on the kernel parameters; the selected kernel and the dimension of the kernel feature space. To overcome this problem, we present another approach to create ensembles that combines the existing ensemble methods with the kernel machine philosophy. In this approach, kernel features are created and concatenated with the original features. The classifiers of an ensemble are trained on these extended feature spaces. Experimental results suggest that the approach is quite robust to the selection of parameters. Experiments also show that different ensemble methods (Random Subspace, Bagging, Adaboost.M1 and Random Forests) can be improved by using this approach.", "paper_title": "Decision tree ensembles based on kernel features", "paper_id": "WOS:000342426700013"}