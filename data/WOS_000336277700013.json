{"auto_keywords": [{"score": 0.049496721057450455, "phrase": "recommender_systems"}, {"score": 0.004641351004344458, "phrase": "larger_datasets"}, {"score": 0.004446677711302801, "phrase": "high_quality_results"}, {"score": 0.004392565073466714, "phrase": "reasonable_time"}, {"score": 0.004260134699483774, "phrase": "ever_growing_requirements"}, {"score": 0.004208282550386197, "phrase": "industrial_recommender_systems"}, {"score": 0.00410645614623058, "phrase": "parallel_hardware"}, {"score": 0.00405646687267772, "phrase": "distributed_computing"}, {"score": 0.003958299299658418, "phrase": "mapreduce_paradigm"}, {"score": 0.0038389110156920926, "phrase": "massive_parallel_data_processing"}, {"score": 0.0037231102027070724, "phrase": "complex_algorithm_reorganization"}, {"score": 0.0036777702949712457, "phrase": "suboptimal_efficiency"}, {"score": 0.0036329805216306576, "phrase": "mid-computation_values"}, {"score": 0.003459191419478298, "phrase": "hard_disk"}, {"score": 0.003293688299319582, "phrase": "content-based_recommendation_algorithm"}, {"score": 0.0030043508031729277, "phrase": "distributed-memory_environment"}, {"score": 0.002913650880082273, "phrase": "data_parallelism"}, {"score": 0.00257735013358606, "phrase": "complete_calculation_process"}, {"score": 0.002499508839669243, "phrase": "independent_and_equally_sized_jobs"}, {"score": 0.002453934333239467, "phrase": "empirically_validated_performance_model"}, {"score": 0.0023798115577749225, "phrase": "parallel_speedup"}, {"score": 0.002336414425972225, "phrase": "high_efficiencies"}, {"score": 0.0023079225317714815, "phrase": "realistic_hardware_configurations"}, {"score": 0.002210903392596343, "phrase": "efficiency_values"}], "paper_keywords": ["Recommender system", " Distributed", " Parallel", " Speedup"], "paper_abstract": "Burdened by their popularity, recommender systems increasingly take on larger datasets while they are expected to deliver high quality results within reasonable time. To meet these ever growing requirements, industrial recommender systems often turn to parallel hardware and distributed computing. While the MapReduce paradigm is generally accepted for massive parallel data processing, it often entails complex algorithm reorganization and suboptimal efficiency because mid-computation values are typically read from and written to hard disk. This work implements an in-memory, content-based recommendation algorithm and shows how it can be parallelized and efficiently distributed across many homogeneous machines in a distributed-memory environment. By focusing on data parallelism and carefully constructing the definition of work in the context of recommender systems, we are able to partition the complete calculation process into any number of independent and equally sized jobs. An empirically validated performance model is developed to predict parallel speedup and promises high efficiencies for realistic hardware configurations. For the MovieLens 10 M dataset we note efficiency values up to 71 % for a configuration of 200 computing nodes (eight cores per node).", "paper_title": "In-memory, distributed content-based recommender system", "paper_id": "WOS:000336277700013"}