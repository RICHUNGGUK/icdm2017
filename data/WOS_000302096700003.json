{"auto_keywords": [{"score": 0.024709971348945654, "phrase": "markov"}, {"score": 0.00481495049065317, "phrase": "temporal_phases"}, {"score": 0.0047707093928203, "phrase": "facial_actions"}, {"score": 0.0046834372388970405, "phrase": "automatic_analysis"}, {"score": 0.004640398886726948, "phrase": "facial_expressions"}, {"score": 0.004513631630494221, "phrase": "prototypic_expressions"}, {"score": 0.004472146692254812, "phrase": "basic_emotions"}, {"score": 0.004134508282135594, "phrase": "facial_behavior"}, {"score": 0.004077616905584829, "phrase": "facial_muscle_actions"}, {"score": 0.0035335154419206634, "phrase": "proposed_fully_automatic_method"}, {"score": 0.0032215231177304513, "phrase": "temporal_segments"}, {"score": 0.002950608985750058, "phrase": "facial_point_detector"}, {"score": 0.0029136540876145154, "phrase": "gabor-feature"}, {"score": 0.0025802660239538353, "phrase": "factorized_likelihoods"}, {"score": 0.0024636239664210433, "phrase": "tracking_data"}, {"score": 0.0023851187337905412, "phrase": "gentleboost"}, {"score": 0.002363150587804677, "phrase": "support_vector_machines"}, {"score": 0.0022563011263787847, "phrase": "average_au_recognition_rate"}, {"score": 0.0021843879253500894, "phrase": "benchmark_set"}, {"score": 0.002164264600379492, "phrase": "deliberately_displayed_facial_expressions"}, {"score": 0.0021049977753042253, "phrase": "spontaneous_expressions"}], "paper_keywords": ["Facial expression analysis", " GentleBoost", " particle filtering", " spatiotemporal facial behavior analysis", " support vector machine (SVM)"], "paper_abstract": "Past work on automatic analysis of facial expressions has focused mostly on detecting prototypic expressions of basic emotions like happiness and anger. The method proposed here enables the detection of a much larger range of facial behavior by recognizing facial muscle actions [action units (AUs)] that compound expressions. AUs are agnostic, leaving the inference about conveyed intent to higher order decision making (e. g., emotion recognition). The proposed fully automatic method not only allows the recognition of 22 AUs but also explicitly models their temporal characteristics (i.e., sequences of temporal segments: neutral, onset, apex, and offset). To do so, it uses a facial point detector based on Gabor-feature-based boosted classifiers to automatically localize 20 facial fiducial points. These points are tracked through a sequence of images using a method called particle filtering with factorized likelihoods. To encode AUs and their temporal activation models based on the tracking data, it applies a combination of GentleBoost, support vector machines, and hidden Markov models. We attain an average AU recognition rate of 95.3% when tested on a benchmark set of deliberately displayed facial expressions and 72% when tested on spontaneous expressions.", "paper_title": "Fully Automatic Recognition of the Temporal Phases of Facial Actions", "paper_id": "WOS:000302096700003"}