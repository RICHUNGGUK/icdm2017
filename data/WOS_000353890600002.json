{"auto_keywords": [{"score": 0.049561472167574526, "phrase": "graph_embedding"}, {"score": 0.015385260603799224, "phrase": "high_dimensional_data"}, {"score": 0.008048353355760395, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "unified_feature_selection_framework"}, {"score": 0.004535481695403089, "phrase": "powerful_tool"}, {"score": 0.004437910956996051, "phrase": "intrinsic_structures"}, {"score": 0.004272164161788242, "phrase": "data_structure_discovery"}, {"score": 0.004180234323211199, "phrase": "noise_amplification"}, {"score": 0.003980531478122155, "phrase": "small_samples"}, {"score": 0.0037493114715625784, "phrase": "novel_efficient_framework"}, {"score": 0.0036886073959526396, "phrase": "feature_selection"}, {"score": 0.003512308265830794, "phrase": "graph_embedding_methods"}, {"score": 0.0034180189629740426, "phrase": "least_squares_regression_problem"}, {"score": 0.003290237137651949, "phrase": "binary_feature_selector"}, {"score": 0.003167217223469026, "phrase": "feature_cardinality"}, {"score": 0.003115908199991072, "phrase": "least_squares_formulation"}, {"score": 0.0030654278282822027, "phrase": "resultant_integral_programming_problem"}, {"score": 0.002966900048034132, "phrase": "convex_quadratically_constrained_quadratic_program"}, {"score": 0.002934767917534329, "phrase": "qcqp"}, {"score": 0.002719325177422626, "phrase": "accelerated_proximal_gradient"}, {"score": 0.0026033756286033285, "phrase": "apg_optimization"}, {"score": 0.002347378595930709, "phrase": "proposed_framework"}, {"score": 0.002128067018576581, "phrase": "experimental_results"}], "paper_keywords": ["Sparse graph embedding", " sparse principal component analysis", " efficient feature selection", " high dimensional data"], "paper_abstract": "Although graph embedding has been a powerful tool for modeling data intrinsic structures, simply employing all features for data structure discovery may result in noise amplification. This is particularly severe for high dimensional data with small samples. To meet this challenge, this paper proposes a novel efficient framework to perform feature selection for graph embedding, in which a category of graph embedding methods is cast as a least squares regression problem. In this framework, a binary feature selector is introduced to naturally handle the feature cardinality in the least squares formulation. The resultant integral programming problem is then relaxed into a convex Quadratically Constrained Quadratic Program (QCQP) learning problem, which can be efficiently solved via a sequence of accelerated proximal gradient (APG) methods. Since each APG optimization is w.r.t. only a subset of features, the proposed method is fast and memory efficient. The proposed framework is applied to several graph embedding learning problems, including supervised, unsupervised, and semi-supervised graph embedding. Experimental results on several high dimensional data demonstrated that the proposed method outperformed the considered state-of-the-art methods.", "paper_title": "A Unified Feature Selection Framework for Graph Embedding on High Dimensional Data", "paper_id": "WOS:000353890600002"}