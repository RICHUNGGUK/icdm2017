{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cross-calibration"}, {"score": 0.004611369021812453, "phrase": "colour_cameras"}, {"score": 0.004464326432401002, "phrase": "flight_cameras"}, {"score": 0.00441635700580562, "phrase": "depth_information"}, {"score": 0.004252469672259375, "phrase": "photometric_appearance"}, {"score": 0.004139128578531134, "phrase": "ordinary_images"}, {"score": 0.0037554497046697432, "phrase": "coherent_scene_representation"}, {"score": 0.0036553070575015344, "phrase": "individual_cameras"}, {"score": 0.0035965032173588753, "phrase": "different_viewpoints"}, {"score": 0.003210446135114967, "phrase": "geometric_framework"}, {"score": 0.003158776195052182, "phrase": "resulting_multi-view_and_multi-modal_calibration_problem"}, {"score": 0.003057910095857785, "phrase": "three-dimensional_projective_transformations"}, {"score": 0.002928399649340438, "phrase": "parallax-based_representations"}, {"score": 0.0028043588777213533, "phrase": "euclidean_reconstruction"}, {"score": 0.0027592065435732955, "phrase": "new_evaluation_procedure"}, {"score": 0.0026423134393535265, "phrase": "reprojection_error"}, {"score": 0.0025440911575156755, "phrase": "sensor-dependent_components"}, {"score": 0.0025031188149336257, "phrase": "complete_approach"}, {"score": 0.002345707314211983, "phrase": "six_colour_cameras"}, {"score": 0.0022101055924375725, "phrase": "automatic_scene-interpretation_problems"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Camera networks", " Time-of-flight cameras", " Depth cameras", " Camera calibration", " 3D reconstruction", " RGB-D data"], "paper_abstract": "Time-of-flight cameras provide depth information, which is complementary to the photometric appearance of the scene in ordinary images. It is desirable to merge the depth and colour information, in order to obtain a coherent scene representation. However, the individual cameras will have different viewpoints, resolutions and fields of view, which means that they must be mutually calibrated. This paper presents a geometric framework for the resulting multi-view and multi-modal calibration problem. It is shown that three-dimensional projective transformations can be used to align depth and parallax-based representations of the scene, with or without Euclidean reconstruction. A new evaluation procedure is also developed; this allows the reprojection error to be decomposed into calibration and sensor-dependent components. The complete approach is demonstrated on a network of three time-of-flight and six colour cameras. The applications of such a system, to a range of automatic scene-interpretation problems, are discussed. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Cross-calibration of time-of-flight and colour cameras", "paper_id": "WOS:000360592500009"}