{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dictionary_learning"}, {"score": 0.004769958800452922, "phrase": "large_set"}, {"score": 0.004452558960552103, "phrase": "linear_combination"}, {"score": 0.0039038519189781135, "phrase": "previously_unseen_signal"}, {"score": 0.003819187440994939, "phrase": "similar_magnitude"}, {"score": 0.0036553070575015344, "phrase": "fixed_distribution"}, {"score": 0.0035648266669315943, "phrase": "statistical_learning_theory_perspective"}, {"score": 0.0035204268675160257, "phrase": "generalization_bounds"}, {"score": 0.003454858296390957, "phrase": "learned_dictionary"}, {"score": 0.003379898291093486, "phrase": "coefficient_selection"}, {"score": 0.002722468694023035, "phrase": "existing_results"}, {"score": 0.002655017467085696, "phrase": "new_signal"}, {"score": 0.0023127544795063263, "phrase": "high_dimensions"}, {"score": 0.002291089893848235, "phrase": "strong_probabilistic_sense"}, {"score": 0.0021517438495635634, "phrase": "similar_results"}, {"score": 0.0021315842986705485, "phrase": "general_setting"}, {"score": 0.0021049977753042253, "phrase": "weak_smoothness_requirements"}], "paper_keywords": ["dictionary learning", " generalization bound", " sparse representation"], "paper_abstract": "A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classification, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a fixed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefficient selection, as measured by the expected L(2) error in representation when the dictionary is used. For the case of l(1) regularized coefficient selection we provide a generalization bound of the order of O(root npln(m lambda)/m), where n is the dimension, p is the number of elements in the dictionary, l is a bound on the l(1) norm of the coefficient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O(root npln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements.", "paper_title": "The Sample Complexity of Dictionary Learning", "paper_id": "WOS:000298103700007"}