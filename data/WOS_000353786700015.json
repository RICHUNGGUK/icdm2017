{"auto_keywords": [{"score": 0.048636009815037955, "phrase": "msa"}, {"score": 0.00481495049065317, "phrase": "accuracy_methods"}, {"score": 0.004766090327062509, "phrase": "scalable_consistency_library"}, {"score": 0.004717723626157272, "phrase": "multiple_sequence_alignment"}, {"score": 0.004552255360140208, "phrase": "high-throughput_next_generation_sequencing_applications"}, {"score": 0.004027586827290042, "phrase": "current_msa_tools"}, {"score": 0.003769006950727621, "phrase": "accuracy_degradation"}, {"score": 0.0036740168936985314, "phrase": "global_consistency_information"}, {"score": 0.0035997452281985465, "phrase": "t-coffee_msa-tool"}, {"score": 0.003509006038574667, "phrase": "consistency_library"}, {"score": 0.003438058620195309, "phrase": "consistency-based_methods"}, {"score": 0.0033004240095380623, "phrase": "computational_resources"}, {"score": 0.0031845067058084583, "phrase": "consistency_information"}, {"score": 0.0029798924689472014, "phrase": "alternative_method"}, {"score": 0.002860546817196148, "phrase": "unlimited_scalability"}, {"score": 0.0027041907335656782, "phrase": "environment_memory"}, {"score": 0.002622528664187849, "phrase": "memory_limitation"}, {"score": 0.0025174594088331853, "phrase": "better_consistency"}, {"score": 0.0023435920614377306, "phrase": "negative_impact"}, {"score": 0.002284444272376043, "phrase": "second_proposal"}, {"score": 0.0021049977753042253, "phrase": "better_alignment"}], "paper_keywords": ["Large-scale alignments", " Scalability", " Consistency", " Accuracy", " T-Coffee", " Multiple sequence alignment"], "paper_abstract": "Multiple sequence alignment (MSA) is crucial for high-throughput next generation sequencing applications. Large-scale alignments with thousands of sequences are necessary for these applications. However, the quality of the alignment of current MSA tools decreases sharply when the number of sequences grows to several thousand. This accuracy degradation can be mitigated using global consistency information as in the T-Coffee MSA-Tool, which implements a consistency library. However, consistency-based methods do not scale well because of the computational resources required to calculate and store the consistency information, which grows quadratically. In this paper, we propose an alternative method for building the consistency-library. To allow unlimited scalability, consistency information must be discarded to avoid exceeding the environment memory. Our first approach deals with the memory limitation by identifying the most important entries, which provide better consistency. This method is able to achieve scalability, although there is a negative impact on accuracy. The second proposal, aims to reduce this degradation of accuracy, with three different methods presented to attain a better alignment.", "paper_title": "Recovering accuracy methods for scalable consistency library", "paper_id": "WOS:000353786700015"}