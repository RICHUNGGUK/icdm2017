{"auto_keywords": [{"score": 0.046633245390760206, "phrase": "entire_dataset"}, {"score": 0.012540094298868938, "phrase": "dfekm"}, {"score": 0.007765533445393198, "phrase": "original_k-means_algorithm"}, {"score": 0.0073217346497422885, "phrase": "cluster_centres"}, {"score": 0.004815882009165822, "phrase": "clustering"}, {"score": 0.004653220443592979, "phrase": "k-means_clustering"}, {"score": 0.004567294967643222, "phrase": "popular_clustering_algorithms"}, {"score": 0.004345804626592329, "phrase": "large_disk-resident_datasets"}, {"score": 0.0040334640350288, "phrase": "small_number"}, {"score": 0.003861697777410367, "phrase": "new_algorithm"}, {"score": 0.003431332820984854, "phrase": "initial_cluster_centres"}, {"score": 0.0032749212355657215, "phrase": "theoretical_analysis"}, {"score": 0.0030298703815998173, "phrase": "real_and_synthetic_datasets"}, {"score": 0.0028293966058473476, "phrase": "distributed_version"}, {"score": 0.00281184118901173, "phrase": "fekm"}, {"score": 0.0027512482368183596, "phrase": "dfekm."}, {"score": 0.00265038878247831, "phrase": "loosely_coupled_machines"}, {"score": 0.0026175952455594277, "phrase": "previous_work"}, {"score": 0.0023767948946074547, "phrase": "distributed_data"}, {"score": 0.0023039337947957077, "phrase": "sequential_k-means"}, {"score": 0.0022402661832994094, "phrase": "loosely_coupled_configuration"}, {"score": 0.00219196314559129, "phrase": "tightly_coupled_environment"}, {"score": 0.0021049977753042253, "phrase": "significant_load_imbalance"}], "paper_keywords": ["k-means clustering", " out-of-core datasets distributed k-means", " confidence radius", " boundary points"], "paper_abstract": "Clustering has been one of the most widely studied topics in data mining and k-means clustering has been one of the popular clustering algorithms. K-means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k-means, which require only one or a small number of passes on the entire dataset. In this paper, we present a new algorithm, called fast and exact k-means clustering (FEKM), which typically requires only one or a small number of passes on the entire dataset and provably produces the same cluster centres as reported by the original k-means algorithm. The algorithm uses sampling to create initial cluster centres and then takes one or more passes over the entire dataset to adjust these cluster centres. We provide theoretical analysis to show that the cluster centres thus reported are the same as the ones computed by the original k-means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared with k-means. This paper also describes and evaluates a distributed version of FEKM, which we refer to as DFEKM. This algorithm is suitable for analysing data that is distributed across loosely coupled machines. Unlike the previous work in this area, DFEKM provably produces the same results as the original k-means algorithm. Our experimental results show that DFEKM is clearly better than two other possible options for exact clustering on distributed data, which are down loading all data and running sequential k-means or running parallel k-means on a loosely coupled configuration. Moreover, even in a tightly coupled environment, DFEKM can outperform parallel k-means if there is a significant load imbalance.", "paper_title": "Fast and exact out-of-core and distributed k-means clustering", "paper_id": "WOS:000240040900002"}