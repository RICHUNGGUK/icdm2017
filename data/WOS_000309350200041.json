{"auto_keywords": [{"score": 0.031814538916309566, "phrase": "collective_operations"}, {"score": 0.00481495049065317, "phrase": "dynamically_detecting_collective_operations"}, {"score": 0.004732785510739708, "phrase": "steady_increase"}, {"score": 0.0046253989970108985, "phrase": "high-performance_computing_platforms"}, {"score": 0.004392565073466714, "phrase": "large-scale_applications"}, {"score": 0.004123781267600542, "phrase": "transparent_optimization"}, {"score": 0.0040767015025213625, "phrase": "large-scale_communication_patterns"}, {"score": 0.0038713805573219297, "phrase": "group_operation_assembly_language"}, {"score": 0.003511246498068952, "phrase": "device-independent_manner"}, {"score": 0.003431481042678356, "phrase": "fast_schemes"}, {"score": 0.003372844338742438, "phrase": "dataflow_and_synchronization_semantics"}, {"score": 0.0033375405638903866, "phrase": "goal"}, {"score": 0.003094306770612843, "phrase": "known_collective_communication_operation"}, {"score": 0.00290473242917264, "phrase": "detected_patterns"}, {"score": 0.0028715299874549245, "phrase": "highly_optimized_algorithms"}, {"score": 0.00283870598459543, "phrase": "low-level_hardware_calls"}, {"score": 0.002726740692380339, "phrase": "benchmark_results"}, {"score": 0.0026041635822373265, "phrase": "performance_improvement"}, {"score": 0.0024586427394152196, "phrase": "co-array_fortran"}, {"score": 0.0023346234464901978, "phrase": "parallel_languages"}, {"score": 0.002191489192077851, "phrase": "detailed_semantics"}, {"score": 0.002166421738415464, "phrase": "high-level_communication_operations"}, {"score": 0.0021049977753042253, "phrase": "efficient_and_scalable_code"}], "paper_keywords": ["Performance", " Languages", " Collective Communication", " Parallel Compiler Optimization", " Parallel Dataflow"], "paper_abstract": "The steady increase of parallelism in high-performance computing platforms implies that communication will be most important in large-scale applications. In this work, we tackle the problem of transparent optimization of large-scale communication patterns using online compilation techniques. We utilize the Group Operation Assembly Language (GOAL), an abstract parallel dataflow definition language, to specify our transformations in a device-independent manner. We develop fast schemes that analyze dataflow and synchronization semantics in GOAL and detect if parts of the (or the whole) communication pattern express a known collective communication operation. The detection of collective operations allows us to replace the detected patterns with highly optimized algorithms or low-level hardware calls and thus improve performance significantly. Benchmark results suggest that our technique can lead to a performance improvement of orders of magnitude compared with various optimized algorithms written in Co-Array Fortran. Detecting collective operations also improves the programmability of parallel languages in that the user does not have to understand the detailed semantics of high-level communication operations in order to generate efficient and scalable code.", "paper_title": "Communication-Centric Optimizations by Dynamically Detecting Collective Operations", "paper_id": "WOS:000309350200041"}