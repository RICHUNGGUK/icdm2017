{"auto_keywords": [{"score": 0.049040461120392155, "phrase": "k-nearest_neighbor_rule"}, {"score": 0.042326323148367435, "phrase": "nearest_neighbors"}, {"score": 0.03034308955026435, "phrase": "proposed_method"}, {"score": 0.02866084391728884, "phrase": "training_example"}, {"score": 0.00481495049065317, "phrase": "pattern_classification"}, {"score": 0.004541377162789232, "phrase": "simplest_and_most_attractive_pattern_classification_algorithms"}, {"score": 0.004283280680222718, "phrase": "empirical_bayes_classifier"}, {"score": 0.004159763048814389, "phrase": "estimated_a_posteriori_probabilities"}, {"score": 0.003754731210715724, "phrase": "locally_constant_a_posteriori_probability_assumption"}, {"score": 0.003489714006817602, "phrase": "high_dimensional_spaces"}, {"score": 0.0031497178007011666, "phrase": "locally_adaptive_nearest_neighbor_rule"}, {"score": 0.0030143100083252516, "phrase": "euclidean_distance"}, {"score": 0.00276066021951467, "phrase": "effective_influence_size"}, {"score": 0.0026419338604879404, "phrase": "statistical_confidence"}, {"score": 0.0023669270875629205, "phrase": "new_method"}, {"score": 0.0023324855762635616, "phrase": "real-world_benchmark_datasets"}, {"score": 0.0022321322970228308, "phrase": "standard_k-nearest_neighbor_rule"}, {"score": 0.0021049977753042253, "phrase": "experimental_results"}], "paper_keywords": [""], "paper_abstract": "The k-nearest neighbor rule is one of the simplest and most attractive pattern classification algorithms. It can be interpreted as an empirical Bayes classifier based on the estimated a posteriori probabilities from the k nearest neighbors. The performance of the k-nearest neighbor rule relies on the locally constant a posteriori probability assumption. This assumption, however, becomes problematic in high dimensional spaces due to the curse of dimensionality. In this paper we introduce a locally adaptive nearest neighbor rule. Instead of using the Euclidean distance to locate the nearest neighbors, the proposed method takes into account the effective influence size of each training example and the statistical confidence with which the label of each training example can be trusted. We test the new method on real-world benchmark datasets and compare it with the standard k-nearest neighbor rule and the support vector machines. The experimental results confirm the effectiveness of the proposed method.", "paper_title": "A statistical confidence-based adaptive nearest neighbor algorithm for pattern classification", "paper_id": "WOS:000238282100057"}