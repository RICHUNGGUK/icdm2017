{"auto_keywords": [{"score": 0.04559556617638774, "phrase": "strong_edge_features"}, {"score": 0.00481495049065317, "phrase": "image-guided_surgery"}, {"score": 0.004770211811866158, "phrase": "vision-based_tracking"}, {"score": 0.00466016686289196, "phrase": "key_component"}, {"score": 0.004595356579614969, "phrase": "augmented_reality"}, {"score": 0.00453144352814823, "phrase": "surgical_operation"}, {"score": 0.004489327204473076, "phrase": "conventional_tracking_techniques"}, {"score": 0.004447600571127578, "phrase": "computer_vision"}, {"score": 0.004324723240308028, "phrase": "distinctive_textures"}, {"score": 0.004264558080356621, "phrase": "well-lit_environment"}, {"score": 0.004205226397573506, "phrase": "endoscopic_tissue_images"}, {"score": 0.003976007772514762, "phrase": "high_degree"}, {"score": 0.003939033374807842, "phrase": "specular_reflection"}, {"score": 0.0037242701420971062, "phrase": "tissue_surface_profiles"}, {"score": 0.003689628075766217, "phrase": "complex_image_processing_techniques"}, {"score": 0.0033447157263095223, "phrase": "integrated_framework"}, {"score": 0.0032522091171602557, "phrase": "surgical_stereo-cameras"}, {"score": 0.0032219438233946312, "phrase": "real-time_speeds"}, {"score": 0.0031036643805367487, "phrase": "star_feature_detector"}, {"score": 0.0030747771905143273, "phrase": "binary_robust_independent_elementary_features"}, {"score": 0.0029206231857112305, "phrase": "high_frame_rates"}, {"score": 0.002787181295054319, "phrase": "densely-populated_map"}, {"score": 0.002722758109991685, "phrase": "tissue_surface"}, {"score": 0.002622755848331379, "phrase": "popular_feature_algorithms"}, {"score": 0.0025862068041567934, "phrase": "vivo_animal_study_video_sequences"}, {"score": 0.0025028892126605124, "phrase": "proposed_method"}, {"score": 0.002479579797152119, "phrase": "human_partial_nephrectomy_video_sequences"}, {"score": 0.002422249273477746, "phrase": "salient_feature_framework"}, {"score": 0.002388487669097511, "phrase": "region_tracking"}, {"score": 0.0023223663501882917, "phrase": "spatial_correspondence"}, {"score": 0.002289993730291136, "phrase": "tracked_region"}, {"score": 0.002237036807966899, "phrase": "medical_image_registration"}, {"score": 0.0022058510091498666, "phrase": "surrounding_tissue"}, {"score": 0.0021750990136123367, "phrase": "vitro_tissue_studies"}, {"score": 0.002154835714714195, "phrase": "registration_accuracies"}, {"score": 0.0021049977753042253, "phrase": "rigid-body_transformation_method"}], "paper_keywords": ["Feature tracking", " image-guided surgery", " image registration", " salient features", " stereoscopy", " surface reconstruction"], "paper_abstract": "Vision-based tracking of tissue is a key component to enable augmented reality during a surgical operation. Conventional tracking techniques in computer vision rely on identifying strong edge features or distinctive textures in a well-lit environment; however endoscopic tissue images do not have strong edge features, are poorly lit and exhibit a high degree of specular reflection. Therefore, prior work in achieving densely populated 3-D features for describing tissue surface profiles require complex image processing techniques and have been limited in providing stable, long-term tracking or real-time processing. In this paper, we present an integrated framework for accurately tracking tissue in surgical stereo-cameras at real-time speeds. We use a combination of the STAR feature detector and binary robust independent elementary features to acquire salient features that can be persistently tracked at high frame rates. The features are then used to acquire a densely-populated map of the deformations of tissue surface in 3-D. We evaluate the method against popular feature algorithms in in vivo animal study video sequences, and we also apply the proposed method to human partial nephrectomy video sequences. We extend the salient feature framework to support region tracking in order to maintain the spatial correspondence of a tracked region of tissue or a medical image registration to the surrounding tissue. In vitro tissue studies show registration accuracies of 1.3-3.3 mm using a rigid-body transformation method.", "paper_title": "Tissue Tracking and Registration for Image-Guided Surgery", "paper_id": "WOS:000313689400015"}