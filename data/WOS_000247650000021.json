{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "receptive_fields"}, {"score": 0.04970018005584556, "phrase": "camera_calibration"}, {"score": 0.042354965139216966, "phrase": "learning_procedure"}, {"score": 0.032742544884248506, "phrase": "multi-camera_vision_system"}, {"score": 0.00446490613141384, "phrase": "nonlinear_mapping_model"}, {"score": 0.004290969772225093, "phrase": "linear_input-output_models"}, {"score": 0.004189863767755212, "phrase": "local_regions"}, {"score": 0.0039005612858880115, "phrase": "receptive_field"}, {"score": 0.0037935066882208235, "phrase": "associated_linear_model"}, {"score": 0.0036601552758254757, "phrase": "approximation_extent_measurement"}, {"score": 0.0036167509502276294, "phrase": "linear_model"}, {"score": 0.0034895911869254146, "phrase": "fusion_framework"}, {"score": 0.0034345152879289025, "phrase": "linear_models"}, {"score": 0.0033402088717816416, "phrase": "camera_model"}, {"score": 0.0031845067058084583, "phrase": "multiple_cameras"}, {"score": 0.003036040409062437, "phrase": "weighted_regression_fusion"}, {"score": 0.002917604283241353, "phrase": "fusion_strategy"}, {"score": 0.002882980094838904, "phrase": "resultant_calibration_model"}, {"score": 0.0028487656293168795, "phrase": "multi-camera_system"}, {"score": 0.002792638947473095, "phrase": "higher_accuracy"}, {"score": 0.002694375425481759, "phrase": "calibration_model"}, {"score": 0.002478297391121455, "phrase": "variable_sensing_requirements"}, {"score": 0.0024586427394152196, "phrase": "different_stages"}, {"score": 0.002439143581855858, "phrase": "task_fulfillment"}, {"score": 0.002419798694301991, "phrase": "simulation_and_experiment_results"}, {"score": 0.0023532890136269986, "phrase": "proposed_method"}, {"score": 0.002316105585203396, "phrase": "neural_network-based_calibration_method"}, {"score": 0.0022977448723686526, "phrase": "tsai"}, {"score": 0.0022345720876241044, "phrase": "exhibit_advantages"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["camera calibration", " receptive field", " sensor fusion", " uncertainty", " weighted regression"], "paper_abstract": "Camera calibration is to identify a model that infers 3-D space measurements from 2-D image observations. In this paper, the nonlinear mapping model of the camera is approximated by a series of linear input-output models defined on a set of local regions called receptive fields. Camera calibration is thus a learning procedure to evolve the size and shape of every receptive field as well as parameters of the associated linear model. Since the learning procedure can also provide an approximation extent measurement for the linear model on each of the receptive fields, calibration model is consequently obtained from a fusion framework integrated with all linear models weighted by their corresponding approximation measurements. Since each camera model is composed of several receptive fields, it is feasible to unitedly calibrate multiple cameras simultaneously. The 3-D measurements of a multi-camera vision system are produced from a weighted regression fusion on all receptive fields of cameras. Thanks to the fusion strategy, the resultant calibration model of a multi-camera system is expected to have higher accuracy than either of them. Moreover, the calibration model is very efficient to be updated whenever one or more cameras in the multi-camera vision system need to be activated or deactivated to adapt to variable sensing requirements at different stages of task fulfillment. Simulation and experiment results illustrate effectiveness and properties of the proposed method. Comparisons with neural network-based calibration method and Tsai's method are also provided to exhibit advantages of the method. (c) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Camera calibration based on receptive fields", "paper_id": "WOS:000247650000021"}