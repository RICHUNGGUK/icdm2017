{"auto_keywords": [{"score": 0.048445090360224895, "phrase": "rpca"}, {"score": 0.04655141977726234, "phrase": "pattern_recognition"}, {"score": 0.03733650763232972, "phrase": "data_matrix"}, {"score": 0.00481495049065317, "phrase": "intrinsic_data_structure"}, {"score": 0.00478163886605204, "phrase": "corrupted_observations"}, {"score": 0.004683073921949839, "phrase": "robust_principal_component_analysis"}, {"score": 0.004570634647887452, "phrase": "tremendous_interests"}, {"score": 0.004491970050862321, "phrase": "computer_vision"}, {"score": 0.004278807087784626, "phrase": "low-rank_component"}, {"score": 0.004234456398783788, "phrase": "sparse_component"}, {"score": 0.004190563478825626, "phrase": "observed_data_matrix"}, {"score": 0.0040757181942937875, "phrase": "suitable_conditions"}, {"score": 0.00395026016389782, "phrase": "principal_component_pursuit"}, {"score": 0.003802135020791801, "phrase": "nuclear_norm"}, {"score": 0.003685067218455088, "phrase": "existing_methods"}, {"score": 0.003621588555525458, "phrase": "singular_value_decompositions"}, {"score": 0.003485744667868367, "phrase": "high_computational_complexity"}, {"score": 0.0032743220621043023, "phrase": "novel_algorithm"}, {"score": 0.003162463248847839, "phrase": "pcp"}, {"score": 0.002657677633954843, "phrase": "first_algorithm"}, {"score": 0.0026027798552168344, "phrase": "nuclear_norm_minimization_problem"}, {"score": 0.0025225467103906314, "phrase": "data_size"}, {"score": 0.002479043701859468, "phrase": "preliminary_investigation"}, {"score": 0.002427826856342292, "phrase": "potential_extensions"}, {"score": 0.0023043558058824572, "phrase": "real_tasks"}, {"score": 0.002280423827531483, "phrase": "great_advantage"}, {"score": 0.00221781065048839, "phrase": "state-of-the-art_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Robust principal component analysis", " Principal component Pursuit", " l(1) minimization", " Subspace learning", " Incremental learning"], "paper_abstract": "In the past decades, exactly recovering the intrinsic data structure from corrupted observations, which is known as Robust Principal Component Analysis (RPCA), has attracted tremendous interests and found many applications in computer vision and pattern recognition. Recently, this problem has been formulated as recovering a low-rank component and a sparse component from the observed data matrix. It is proved that under some suitable conditions, this problem can be exactly solved by Principal Component Pursuit (PCP), i.e., minimizing a combination of nuclear norm and l(1) norm. Most of the existing methods for solving PCP require Singular Value Decompositions (SVDs) of the data matrix, resulting in a high computational complexity, hence preventing the applications of RPCA to very large scale computer vision problems. In this paper, we propose a novel algorithm, called l(1) filtering, for exactly solving PCP with an O(r(2)(m+n)) complexity, where m x n is the size of data matrix and r is the rank of the matrix to recover, which is supposed to be much smaller than m and n. Moreover, if filtering is highly parallelizable. It is the first algorithm that can exactly solve a nuclear norm minimization problem in linear time (with respect to the data size). As a preliminary investigation, we also discuss the potential extensions of PCP for more complex vision tasks encouraged by l(1) filtering. Experiments on both synthetic data and real tasks testify the great advantage of l(1) filtering in speed over state-of-the-art algorithms and wide applications in computer vision and pattern recognition societies. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Linear time Principal Component Pursuit and its extensions using l(1) filtering", "paper_id": "WOS:000340341400055"}