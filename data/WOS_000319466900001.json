{"auto_keywords": [{"score": 0.04085830846571713, "phrase": "pca"}, {"score": 0.00481495049065317, "phrase": "factor_model"}, {"score": 0.00470862827326028, "phrase": "regularized_pca."}, {"score": 0.004306205029641728, "phrase": "linear_factor_model"}, {"score": 0.004072275820209647, "phrase": "regularized_form"}, {"score": 0.0039822874870125095, "phrase": "principal_component_analysis"}, {"score": 0.003601235776892337, "phrase": "synthetic_and_real_data"}, {"score": 0.0032203158828810244, "phrase": "pre-existing_factor_analysis_approaches"}, {"score": 0.0030451958332949735, "phrase": "theoretical_results"}, {"score": 0.002722930209707764, "phrase": "conventional_approaches"}, {"score": 0.0026330549554033876, "phrase": "important_feature"}, {"score": 0.0022261705957258506, "phrase": "wide_use"}, {"score": 0.002176887956041267, "phrase": "large_part"}], "paper_keywords": ["Principal component analysis", " Factor model", " High-dimensional data", " Covariance matrix estimation", " Regularization"], "paper_abstract": "We consider the problem of learning a linear factor model. We propose a regularized form of principal component analysis (PCA) and demonstrate through experiments with synthetic and real data the superiority of resulting estimates to those produced by pre-existing factor analysis approaches. We also establish theoretical results that explain how our algorithm corrects the biases induced by conventional approaches. An important feature of our algorithm is that its computational requirements are similar to those of PCA, which enjoys wide use in large part due to its efficiency.", "paper_title": "Learning a factor model via regularized PCA", "paper_id": "WOS:000319466900001"}