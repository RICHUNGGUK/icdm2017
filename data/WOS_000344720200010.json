{"auto_keywords": [{"score": 0.044935050925839014, "phrase": "speech_mixtures"}, {"score": 0.02816365002013277, "phrase": "interference_residual"}, {"score": 0.00481495049065317, "phrase": "reverberant_speech_separation_with_visual_voice_activity_detection"}, {"score": 0.0047547526023100665, "phrase": "visual_modality"}, {"score": 0.004597861947580946, "phrase": "audio_modality"}, {"score": 0.004390517953424082, "phrase": "blind_source_separation"}, {"score": 0.004354087948288413, "phrase": "bss"}, {"score": 0.00422781730455246, "phrase": "adverse_environments"}, {"score": 0.004140036888016244, "phrase": "audio-domain_methods"}, {"score": 0.003953256858638655, "phrase": "enhancement_method"}, {"score": 0.003920209964956682, "phrase": "audio-domain_bss"}, {"score": 0.003838791650587681, "phrase": "voice_activity_information"}, {"score": 0.003759057920801288, "phrase": "visual_voice_activity_detection"}, {"score": 0.003619672388007568, "phrase": "human_hearing"}, {"score": 0.003589404014552974, "phrase": "binaural_speech_mixtures"}, {"score": 0.0034418037573506837, "phrase": "off-line_training_stage"}, {"score": 0.003398714712611042, "phrase": "speaker-independent_voice_activity_detector"}, {"score": 0.0033280910751441522, "phrase": "visual_stimuli"}, {"score": 0.003286420941205958, "phrase": "adaboosting_algorithm"}, {"score": 0.00323166859966514, "phrase": "on-line_separation_stage"}, {"score": 0.0031512505937980446, "phrase": "ipd"}, {"score": 0.002971249577042111, "phrase": "probabilistically_each_time-frequency"}, {"score": 0.002885135695263876, "phrase": "audio_mixtures"}, {"score": 0.00284899583294417, "phrase": "source_signals"}, {"score": 0.002789763110395331, "phrase": "detected_voice_activity_cues"}, {"score": 0.0027317585060414253, "phrase": "visual_vad"}, {"score": 0.002459288755782293, "phrase": "energy_ratio_map"}, {"score": 0.002358050109031317, "phrase": "room_impulse_responses"}, {"score": 0.002338306897057454, "phrase": "different_reverberation_times"}, {"score": 0.0023187286045879643, "phrase": "noise_levels"}, {"score": 0.002299313860419974, "phrase": "simulation_results"}, {"score": 0.0022800613046444563, "phrase": "performance_improvement"}, {"score": 0.0022514836269627186, "phrase": "proposed_method"}, {"score": 0.0022326306658609365, "phrase": "target_speech_extraction"}, {"score": 0.0022139352210982398, "phrase": "noisy_and_reverberant_environments"}, {"score": 0.0021678771715091492, "phrase": "signal-to-interference_ratio"}, {"score": 0.002122775261369663, "phrase": "perceptual_evaluation"}, {"score": 0.0021049977753042253, "phrase": "speech_quality"}], "paper_keywords": ["Adaboosting", " binaural", " blind source separation", " interference removal", " visual voice activity detection"], "paper_abstract": "The visual modality, deemed to be complementary to the audio modality, has recently been exploited to improve the performance of blind source separation (BSS) of speech mixtures, especially in adverse environments where the performance of audio-domain methods deteriorates steadily. In this paper, we present an enhancement method to audio-domain BSS with the integration of voice activity information, obtained via a visual voice activity detection (VAD) algorithm. Mimicking aspects of human hearing, binaural speech mixtures are considered in our two-stage system. Firstly, in the off-line training stage, a speaker-independent voice activity detector is formed using the visual stimuli via the adaboosting algorithm. In the on-line separation stage, interaural phase difference (IPD) and interaural level difference (ILD) cues are statistically analyzed to assign probabilistically each time-frequency (TF) point of the audio mixtures to the source signals. Next, the detected voice activity cues (found via the visual VAD) are integrated to reduce the interference residual. Detection of the interference residual takes place gradually, with two layers of boundaries in the correlation and energy ratio map. We have tested our algorithm on speech mixtures generated using room impulse responses at different reverberation times and noise levels. Simulation results show performance improvement of the proposed method for target speech extraction in noisy and reverberant environments, in terms of signal-to-interference ratio (SIR) and perceptual evaluation of speech quality (PESQ).", "paper_title": "Interference Reduction in Reverberant Speech Separation With Visual Voice Activity Detection", "paper_id": "WOS:000344720200010"}