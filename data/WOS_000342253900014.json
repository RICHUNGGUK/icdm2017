{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high-dimensional_class-imbalanced_data_sets"}, {"score": 0.004764542405129454, "phrase": "support_vector_machines"}, {"score": 0.004592214823540993, "phrase": "imbalanced_data_sets"}, {"score": 0.00433386573276633, "phrase": "growing_attention"}, {"score": 0.004199101289890982, "phrase": "feature_selection"}, {"score": 0.004133292222875435, "phrase": "dimensionality_reduction_problem"}, {"score": 0.0040047396607954325, "phrase": "available_features"}, {"score": 0.00392125815948488, "phrase": "good_model"}, {"score": 0.0037594599958631404, "phrase": "class-imbalance_problem"}, {"score": 0.0036810726176482278, "phrase": "class_distribution"}, {"score": 0.003278167066287366, "phrase": "high_dimensionality"}, {"score": 0.0026830601040919166, "phrase": "target_class"}, {"score": 0.002654911400878851, "phrase": "binary_classification"}, {"score": 0.0025858215477025117, "phrase": "backward_elimination_approach"}, {"score": 0.0025452314655691165, "phrase": "successive_holdout_steps"}, {"score": 0.0024529757621396717, "phrase": "balanced_loss_function"}, {"score": 0.0024017638230939514, "phrase": "independent_subset"}, {"score": 0.002326938965568362, "phrase": "six_highly_imbalanced_microarray_data_sets"}, {"score": 0.0022544399432457164, "phrase": "well-known_feature_selection_techniques"}, {"score": 0.0021957487795963666, "phrase": "better_prediction"}, {"score": 0.002172701452347454, "phrase": "consistently_fewer_relevant_features"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Feature selection", " Imbalanced data set", " Dimensionality reduction", " Support Vector Machine", " Data mining"], "paper_abstract": "Feature selection and classification of imbalanced data sets are two of the most interesting machine learning challenges, attracting a growing attention from both, industry and academia. Feature selection addresses the dimensionality reduction problem by determining a subset of available features to build a good model for classification or prediction, while the class-imbalance problem arises when the class distribution is too skewed. Both issues have been independently studied in the literature, and a plethora of methods to address high dimensionality as well as class-imbalance has been proposed. The aim of this work is to simultaneously explore both issues, proposing a family of methods that select those attributes that are relevant for the identification of the target class in binary classification. We propose a backward elimination approach based on successive holdout steps, whose contribution measure is based on a balanced loss function obtained on an independent subset. Our experiments are based on six highly imbalanced microarray data sets, comparing our methods with well-known feature selection techniques, and obtaining a better prediction with consistently fewer relevant features. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines", "paper_id": "WOS:000342253900014"}