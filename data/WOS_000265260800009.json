{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "uncertain_environment"}, {"score": 0.004763895422211728, "phrase": "external_instruction"}, {"score": 0.00458940888106061, "phrase": "fundamental_features"}, {"score": 0.004492573860101133, "phrase": "temporal_difference"}, {"score": 0.004327981760540394, "phrase": "incremental_learning_approach"}, {"score": 0.004103212598803194, "phrase": "eligibility_traces"}, {"score": 0.004038076978795421, "phrase": "important_mechanism"}, {"score": 0.003973971215091993, "phrase": "leaning_ability"}, {"score": 0.0038079353103469865, "phrase": "td_method"}, {"score": 0.003668333531296882, "phrase": "state_space"}, {"score": 0.003261934055562238, "phrase": "td_algorithms"}, {"score": 0.002869613770219469, "phrase": "extensive_set"}, {"score": 0.0028089634624677957, "phrase": "different_parameter_values"}, {"score": 0.002662871928330334, "phrase": "performance_metrics"}, {"score": 0.0024578219177450876, "phrase": "control_algorithm"}, {"score": 0.0023802825369325354, "phrase": "linear_function_approximation_technique"}, {"score": 0.002256435842862802, "phrase": "soccer_agents"}, {"score": 0.002220551141101661, "phrase": "optimal_control_processes"}, {"score": 0.0021049977753042253, "phrase": "optimal_parameter_values"}], "paper_keywords": ["Temporal difference learning", " Agent", " Convergence analysis"], "paper_abstract": "Learning to act in an. uncertain environment without external instruction is considered as one of the fundamental features of intelligence. Temporal difference (TD) learning is an incremental learning approach and has been widely used in various application domains. Utilising eligibility traces is an important mechanism in enhancing leaning ability. For large, stochastic and dynamic systems, however, the TD method suffers,from two problems: the state space grows exponentially with the curse of dimensionality and there is a lack of methodology to analyse the convergence and sensitivity of TD algorithms. Measuring learning performance and analysing sensitivity of parameters are very difficult and expensive, and such performance metrics are obtained only by running an extensive set of experiments with different parameter values. In this paper, convergence is investigated by performance metrics, which is obtained through simulating a game of soccer. Sarsa(lambda) learning control algorithm, in conjunction with a linear function approximation technique known as the coding, is used to help soccer agents learn the optimal control processes. This paper proposes a methodology for finding the optimal parameter values to improve the quality of convergence.", "paper_title": "CONVERGENCE ANALYSIS ON TEMPORAL DIFFERENCE LEARNING", "paper_id": "WOS:000265260800009"}