{"auto_keywords": [{"score": 0.02466578698236496, "phrase": "bgrt"}, {"score": 0.00481495049065317, "phrase": "high_floating-point_errors"}, {"score": 0.004701825207431857, "phrase": "floating-point_error_estimation"}, {"score": 0.004174677062711731, "phrase": "input_settings"}, {"score": 0.004100851549096203, "phrase": "floating_point"}, {"score": 0.003795677450778112, "phrase": "precision_allocation"}, {"score": 0.003576416581189713, "phrase": "current_abstraction-based_precision_analysis_methods"}, {"score": 0.0032709661258673232, "phrase": "highly_pessimistic_error_estimates"}, {"score": 0.0031750418391373035, "phrase": "non-linear_operators"}, {"score": 0.0031374619658664843, "phrase": "complex_input_constraints"}, {"score": 0.0030273620861817055, "phrase": "legal_inputs"}, {"score": 0.002921114512115261, "phrase": "concrete-testing-based_error_estimation_methods"}, {"score": 0.002801849371896573, "phrase": "higher_precision"}, {"score": 0.0027358910467781155, "phrase": "higher_error-inducing_inputs"}, {"score": 0.0027034948760997564, "phrase": "suitable_heuristic_search_guidance"}, {"score": 0.0026241684260044414, "phrase": "higher_errors"}, {"score": 0.0025471636425006155, "phrase": "heuristic_search_algorithm"}, {"score": 0.0025169965861573185, "phrase": "binary_guided_random_testing"}, {"score": 0.0022881443200668886, "phrase": "higher_guaranteed_errors"}, {"score": 0.0021430076159431907, "phrase": "pso"}, {"score": 0.0021049977753042253, "phrase": "better_results"}], "paper_keywords": ["Sequential and parallel programming", " floating-point error estimation methods", " guided search"], "paper_abstract": "Tools for floating-point error estimation are fundamental to program understanding and optimization. In this paper, we focus on tools for determining the input settings to a floating point routine that maximizes its result error. Such tools can help support activities such as precision allocation, performance optimization, and auto-tuning. We benchmark current abstraction-based precision analysis methods, and show that they often do not work at scale, or generate highly pessimistic error estimates, often caused by non-linear operators or complex input constraints that define the set of legal inputs. We show that while concrete-testing-based error estimation methods based on maintaining shadow values at higher precision can search out higher error-inducing inputs, suitable heuristic search guidance is key to finding higher errors. We develop a heuristic search algorithm called Binary Guided Random Testing (BGRT). In 45 of the 48 total benchmarks, including many real-world routines, BGRT returns higher guaranteed errors. We also evaluate BGRT against two other heuristic search methods called ILS and PSO, obtaining better results.", "paper_title": "Efficient Search for Inputs Causing High Floating-point Errors", "paper_id": "WOS:000349142100005"}