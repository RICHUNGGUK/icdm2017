{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "assisted_visualization_design"}, {"score": 0.004628101576367193, "phrase": "perceptual_guidelines"}, {"score": 0.004582528208024817, "phrase": "human_vision"}, {"score": 0.004515004397496303, "phrase": "ai-based_mixed-initiative_search_strategy"}, {"score": 0.004109770306454165, "phrase": "perceptually_salient_visualizations"}, {"score": 0.004069280130346136, "phrase": "large_multidimensional_data_sets"}, {"score": 0.00393066798511457, "phrase": "low-level_human_vision"}, {"score": 0.0037593414153801394, "phrase": "particular_visualization"}, {"score": 0.0036673959568781734, "phrase": "analysis_tasks"}, {"score": 0.0035249212996297332, "phrase": "new_visualizations"}, {"score": 0.0032927926726941032, "phrase": "context"}, {"score": 0.0032563083791198534, "phrase": "domain_expertise"}, {"score": 0.003192406091820197, "phrase": "high-level_understanding"}, {"score": 0.0031453010852936334, "phrase": "data_set"}, {"score": 0.003068327486877065, "phrase": "effective_visualizations"}, {"score": 0.002993231973906559, "phrase": "mixed-initiative_strategy"}, {"score": 0.0028203950416405563, "phrase": "via's_understanding"}, {"score": 0.0027787638791664663, "phrase": "user's_preferences"}, {"score": 0.002724207384182295, "phrase": "historical_weather_conditions"}, {"score": 0.0026839922027139967, "phrase": "via's_search_strategy"}, {"score": 0.002618278365749875, "phrase": "simulated_annealing"}, {"score": 0.002454835845644037, "phrase": "mixed-initiative_interaction"}, {"score": 0.0023947191132328233, "phrase": "intelligent_agents"}, {"score": 0.0023476849595393872, "phrase": "simulated_online_auction"}, {"score": 0.0023130151703068444, "phrase": "via's_perceptual_guidelines"}, {"score": 0.002157857111130435, "phrase": "high-quality_visualizations"}, {"score": 0.0021049977753042253, "phrase": "real-world_data_sets"}], "paper_keywords": ["computer graphics", " perception", " search", " user interfaces", " visualization"], "paper_abstract": "This paper describes the integration of perceptual guidelines from human vision with an AI-based mixed-initiative search strategy. The result is a visualization assistant called ViA, a system that collaborates with its users to identify perceptually salient visualizations for large multidimensional data sets. ViA applies the knowledge of low-level human vision to 1) evaluate the effectiveness of a particular visualization for a given data set and analysis tasks and 2) rapidly direct its search toward new visualizations that are most likely to offer improvements over those seen to date. Context, domain expertise, and a high-level understanding of a data set are critical to identifying effective visualizations. We apply a mixed-initiative strategy that allows ViA and its users to share their different strengths and continually improve ViA's understanding of a user's preferences. We visualize historical weather conditions to compare ViA's search strategy to exhaustive analysis, simulated annealing, and reactive tabu search and to measure the improvement provided by mixed-initiative interaction. We also visualize intelligent agents competing in a simulated online auction to evaluate ViA's perceptual guidelines. Results from each study are positive, suggesting that ViA can construct high-quality visualizations for a range of real-world data sets.", "paper_title": "Visual perception and mixed-initiative interaction for assisted visualization design", "paper_id": "WOS:000252254200013"}