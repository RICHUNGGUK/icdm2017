{"auto_keywords": [{"score": 0.04913548507380238, "phrase": "continuous-valued_attributes"}, {"score": 0.048439279083046086, "phrase": "decision_tree"}, {"score": 0.03267712519184559, "phrase": "integrated_approach"}, {"score": 0.00481495049065317, "phrase": "multi-interval_rules"}, {"score": 0.0045357787854931894, "phrase": "machine_learning"}, {"score": 0.004337028108577593, "phrase": "knowledge_management"}, {"score": 0.004126349202545883, "phrase": "quick_learning"}, {"score": 0.0040854526743211396, "phrase": "easy_creation"}, {"score": 0.0040449598280454645, "phrase": "explicit_knowledge_structures"}, {"score": 0.003945467978172845, "phrase": "congenital_limitation"}, {"score": 0.0038676325043529524, "phrase": "branch_structure"}, {"score": 0.0036250779580069455, "phrase": "nominal_or_discrete-valued_attributes"}, {"score": 0.0034660938095219846, "phrase": "candidate_continuous-valued_attribute"}, {"score": 0.0034146585649631692, "phrase": "finite_manageable_number"}, {"score": 0.00299958349498286, "phrase": "decision_tree_induction"}, {"score": 0.0029403531729828574, "phrase": "hierarchical_clustering_analysis"}, {"score": 0.0028822890425197582, "phrase": "appropriate_intervals"}, {"score": 0.0027834145465170292, "phrase": "proposed_measure"}, {"score": 0.002701361707441567, "phrase": "attribute_closeness"}, {"score": 0.0026745503978347143, "phrase": "discretization_results_similarity"}, {"score": 0.0025571463140118805, "phrase": "multi-interval_discretization"}, {"score": 0.0024693979425773993, "phrase": "processing_difficulty"}, {"score": 0.002420611343093316, "phrase": "decision_tree_induction_learning"}, {"score": 0.0023609780669713288, "phrase": "proposed_approach"}, {"score": 0.002168975681697056, "phrase": "ucl-ml_testing_databases"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Knowledge discovery in database", " Machine learning Knowledge management", " Continuous-valued attribute"], "paper_abstract": "The decision tree induction learning approach in machine learning has been extensively applied to the field of knowledge management in practice. since it provides the advantages of quick learning and easy creation of explicit knowledge structures. However, the congenital limitation of node and branch structure may limit the Success Of decision tree induction learning in dealing with nominal or discrete-valued attributes. The need for discretizing or splitting a candidate continuous-valued attribute into some finite manageable number of intervals therefore is crucial in the application of decision tree induction learning. In this study, all integrated approach was proposed by utilizing the decision tree induction along with the hierarchical clustering analysis which combined the appropriate intervals based oil the use of the proposed measure with considerations of both within attribute closeness and discretization results similarity. This proposed integrated approach facilitates the task of multi-interval discretization to produce more accurate classification rules by improving the processing difficulty Of continuous-valued attributes for decision tree induction learning. Finally, the proposed approach was tested in terms of both the predictive accuracy and the size of the decision tree by using the UCl-ML testing databases. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Recognition of multi-interval rules in dataset with continuous-valued attributes", "paper_id": "WOS:000262178000053"}