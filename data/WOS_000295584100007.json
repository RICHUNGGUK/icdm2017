{"auto_keywords": [{"score": 0.04553149607201767, "phrase": "neural_networks"}, {"score": 0.00481495049065317, "phrase": "feedforward_neural_networks"}, {"score": 0.004773476989344654, "phrase": "feedforward_neural_network"}, {"score": 0.004531997388646422, "phrase": "wide_variety"}, {"score": 0.0043213410601860985, "phrase": "black-box_models"}, {"score": 0.00403221710768334, "phrase": "internal_behavior"}, {"score": 0.003928834302530914, "phrase": "detailed_interpretation"}, {"score": 0.0038781372962808894, "phrase": "neural_network_functional_geometry"}, {"score": 0.003795086335272422, "phrase": "geometrical_interpretation"}, {"score": 0.00374610869916551, "phrase": "new_set"}, {"score": 0.003510507340837656, "phrase": "network_weights"}, {"score": 0.003361702510958392, "phrase": "new_formulation"}, {"score": 0.003261295948815592, "phrase": "newly_defined_variables"}, {"score": 0.0032191849045822415, "phrase": "reformulated_neural_network"}, {"score": 0.003109519872758431, "phrase": "common_feedforward_neural_network"}, {"score": 0.002990592465417758, "phrase": "learning_ability"}, {"score": 0.0029647865602161158, "phrase": "renn"}, {"score": 0.0027781870000242004, "phrase": "derivative-free_optimization_algorithms"}, {"score": 0.0026951616152303373, "phrase": "new_measure"}, {"score": 0.0026033013834547507, "phrase": "developed_geometrical_interpretation"}, {"score": 0.0025145641652082164, "phrase": "generalization_ability"}, {"score": 0.0024288443150616056, "phrase": "proposed_geometrical_interpretation"}, {"score": 0.0023974563359295043, "phrase": "renn_approach"}, {"score": 0.0023562342764987254, "phrase": "new_regularization_measure"}, {"score": 0.002315719347244451, "phrase": "multiple_test_problems"}, {"score": 0.0021604924928015283, "phrase": "common_neural_networks"}, {"score": 0.0021325650133686516, "phrase": "proposed_regularization_measure"}, {"score": 0.0021049977753042253, "phrase": "effective_indicator"}], "paper_keywords": ["Feedforward neural networks", " generalization", " geometrical interpretation", " internal behavior", " measure of regularization", " reformulated neural network", " training"], "paper_abstract": "Feedforward neural network is one of the most commonly used function approximation techniques and has been applied to a wide variety of problems arising from various disciplines. However, neural networks are black-box models having multiple challenges/difficulties associated with training and generalization. This paper initially looks into the internal behavior of neural networks and develops a detailed interpretation of the neural network functional geometry. Based on this geometrical interpretation, a new set of variables describing neural networks is proposed as a more effective and geometrically interpretable alternative to the traditional set of network weights and biases. Then, this paper develops a new formulation for neural networks with respect to the newly defined variables; this reformulated neural network (ReNN) is equivalent to the common feedforward neural network but has a less complex error response surface. To demonstrate the learning ability of ReNN, in this paper, two training methods involving a derivative-based (a variation of backpropagation) and a derivative-free optimization algorithms are employed. Moreover, a new measure of regularization on the basis of the developed geometrical interpretation is proposed to evaluate and improve the generalization ability of neural networks. The value of the proposed geometrical interpretation, the ReNN approach, and the new regularization measure are demonstrated across multiple test problems. Results show that ReNN can be trained more effectively and efficiently compared to the common neural networks and the proposed regularization measure is an effective indicator of how a network would perform in terms of generalization.", "paper_title": "A New Formulation for Feedforward Neural Networks", "paper_id": "WOS:000295584100007"}