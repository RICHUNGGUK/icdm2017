{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "visual_categories"}, {"score": 0.004777917315737992, "phrase": "different_levels"}, {"score": 0.004579244735256393, "phrase": "strong_work"}, {"score": 0.004544016163767266, "phrase": "visual_recognition"}, {"score": 0.004509057381424944, "phrase": "dialogue_interpretation"}, {"score": 0.004474366340935036, "phrase": "multi-modal_learning"}, {"score": 0.004355026964936104, "phrase": "building_blocks"}, {"score": 0.004304856672196293, "phrase": "intelligent_robots"}, {"score": 0.004190019394361629, "phrase": "meaningful_way"}, {"score": 0.004046843083499649, "phrase": "building_systems"}, {"score": 0.00393886140366453, "phrase": "common_architecture"}, {"score": 0.003745881316228299, "phrase": "it's_own_set"}, {"score": 0.003534889186471058, "phrase": "recent_progress"}, {"score": 0.0035076666315744525, "phrase": "visual_category_recognition"}, {"score": 0.0034008496666026585, "phrase": "interactive_systems"}, {"score": 0.0031845067058084613, "phrase": "modern_techniques"}, {"score": 0.0031599739264865554, "phrase": "visual_categorization"}, {"score": 0.003123527559542239, "phrase": "interactive_learning_system"}, {"score": 0.003051887221639784, "phrase": "required_labelled_training_examples"}, {"score": 0.003005039427642664, "phrase": "potentially_erroneous_input"}, {"score": 0.002981885091155911, "phrase": "today's_object_categorization_methods"}, {"score": 0.002846647336160784, "phrase": "unsupervised_methods"}, {"score": 0.0027492483892113, "phrase": "far_more_and_unlabeled_training_data"}, {"score": 0.0026965813247045427, "phrase": "novel_method"}, {"score": 0.002665465323614577, "phrase": "unsupervised_training"}, {"score": 0.0026347074232350503, "phrase": "visual_groupings"}, {"score": 0.0025842289219860795, "phrase": "cross-modal_learning_scheme"}, {"score": 0.0025249264547573943, "phrase": "purely_unsupervised_training"}, {"score": 0.0024669814850296146, "phrase": "unified_and_scale-invariant_object_representation"}, {"score": 0.002382541887825168, "phrase": "unlabeled_information"}, {"score": 0.0023550410480905727, "phrase": "coherent_way"}, {"score": 0.0023368833197343953, "phrase": "first_experiments"}, {"score": 0.0022568866348667547, "phrase": "object_category_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Object categorization", " Cross-modal learning", " Tutor-based learning", " Incremental learning", " Interactive learning", " Unsupervised learning", " Semi-supervised learning"], "paper_abstract": "In recent years we have seen lots of strong work in visual recognition, dialogue interpretation and multi-modal learning that is targeted at provide the building blocks to enable intelligent robots to interact with humans in a meaningful way and even continuously evolve during this process. Building systems that unify those components under a common architecture has turned out to be challenging, as each approach comes with it's own set of assumptions, restrictions, and implications. For example, the impact of recent progress on visual category recognition has been limited from a perspective of interactive systems. Reasons for this are diverse. We identify and address two major challenges in order to integrate modern techniques for visual categorization in an interactive learning system: reducing the number of required labelled training examples and dealing with potentially erroneous input. Today's object categorization methods use either supervised or unsupervised training methods. While supervised methods tend to produce more accurate results, unsupervised methods are highly attractive due to their potential to use far more and unlabeled training data. We proposes a novel method that uses unsupervised training to obtain visual groupings of objects and a cross-modal learning scheme to overcome inherent limitations of purely unsupervised training. The method uses a unified and scale-invariant object representation that allows to handle labeled as well as unlabeled information in a coherent way. First experiments demonstrate the ability of the system to learn object category models from many unlabeled observations and a few dialogue interactions that can be ambiguous or even erroneous. (c) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Tutor-based learning of visual categories using different levels of supervision", "paper_id": "WOS:000277933600006"}