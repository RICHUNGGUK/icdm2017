{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cepstral_domain"}, {"score": 0.04979620480283762, "phrase": "model_combination"}, {"score": 0.04705476550494024, "phrase": "robust_speech_recognition"}, {"score": 0.004660634721704355, "phrase": "effective_cepstral_feature_compensation_scheme"}, {"score": 0.004578547645288202, "phrase": "speech_model"}, {"score": 0.004458107800947839, "phrase": "proposed_scheme"}, {"score": 0.004379572106088617, "phrase": "prior_noisy_speech_database"}, {"score": 0.0043537008514226445, "phrase": "off-line_training"}, {"score": 0.004289686492089576, "phrase": "parallel_model_combination"}, {"score": 0.004251728624599691, "phrase": "noise-corrupted_speech_model"}, {"score": 0.004176813329075736, "phrase": "clean_speech"}, {"score": 0.0040189760459195725, "phrase": "noisy_speech_model"}, {"score": 0.003936461984681817, "phrase": "noise_model"}, {"score": 0.003844224579620687, "phrase": "reduced_computational_expenses"}, {"score": 0.00378767277402891, "phrase": "model_estimation"}, {"score": 0.003633698607193056, "phrase": "time-varying_background_noise"}, {"score": 0.0036015243412109415, "phrase": "novel_interpolation_method"}, {"score": 0.0035802326864717644, "phrase": "multiple_models"}, {"score": 0.0034963129868796033, "phrase": "posterior_probability"}, {"score": 0.003465350958971198, "phrase": "environmental_model"}, {"score": 0.003434662174045939, "phrase": "compensation_procedure_call"}, {"score": 0.003265805876551917, "phrase": "computational_expense"}, {"score": 0.0032272929113113203, "phrase": "multiple-model_method"}, {"score": 0.0031703706626348507, "phrase": "similar_gaussian_components"}, {"score": 0.003132979607342419, "phrase": "acoustically_similar_components"}, {"score": 0.0030868583963299698, "phrase": "environmental_models"}, {"score": 0.0030414140732789186, "phrase": "proposed_sub-optimal_algorithm"}, {"score": 0.003005539334280134, "phrase": "kullback-leibler_similarity_distance"}, {"score": 0.0029789104204803137, "phrase": "combined_hybrid_model"}, {"score": 0.0029263562233195423, "phrase": "selected_gaussian_components"}, {"score": 0.002891834702686575, "phrase": "noisy_speech_model_sharing"}, {"score": 0.0027824197043924527, "phrase": "in-vehicle_environment"}, {"score": 0.0027577621502315594, "phrase": "proposed_feature_compensation_algorithm"}, {"score": 0.0027252240573281163, "phrase": "standard_methods"}, {"score": 0.002669202570863189, "phrase": "cmn"}, {"score": 0.0025988558501741327, "phrase": "experimental_results"}, {"score": 0.0025681878215171976, "phrase": "proposed_feature_compensation_schemes"}, {"score": 0.0025079304766764616, "phrase": "adverse_noisy_environments"}, {"score": 0.002485699358130026, "phrase": "proposed_model_combination-based_feature_compensation_method"}, {"score": 0.0024563633541195124, "phrase": "existing_model-based_feature_compensation_methods"}, {"score": 0.0023987234349331383, "phrase": "proposed_method"}, {"score": 0.002335489878370869, "phrase": "etsi_afe_front-end_method"}, {"score": 0.002314783782656802, "phrase": "multi-model_approach"}, {"score": 0.0022739194433372173, "phrase": "changing_noise_conditions"}, {"score": 0.0022604585778195152, "phrase": "input_speech"}, {"score": 0.0022404162042797262, "phrase": "comparable_performance"}, {"score": 0.002220551141101661, "phrase": "matched_model_condition"}, {"score": 0.002194337509146714, "phrase": "mixture_sharing_method"}, {"score": 0.0021748801206355416, "phrase": "significant_reduction"}, {"score": 0.0021620042653528846, "phrase": "computational_overhead"}, {"score": 0.00213648030003747, "phrase": "recognition_performance"}, {"score": 0.0021175348478776846, "phrase": "reasonable_level"}], "paper_keywords": ["Speech recognition", " Feature compensation", " Model combination", " Multiple models", " Mixture sharing"], "paper_abstract": "In this paper, we present all effective cepstral feature compensation scheme which leverages knowledge of the speech model in order to achieve robust speech recognition. Ill the proposed scheme, the requirement for a prior noisy speech database in off-line training is eliminated by employing parallel model combination for the noise-corrupted speech model. Gaussian mixture models of clean speech and noise are used for the model combination. The adaptation of the noisy speech model is possible only by updating the noise model. This method has the advantage of reduced computational expenses and improved accuracy for model estimation since it is applied in the cepstral domain. In order to cope with time-varying background noise, a novel interpolation method of multiple models is employed. By sequentially calculating the posterior probability of each environmental model, the compensation procedure call be applied oil a frame-by-frame basis. In order to reduce the computational expense due to the multiple-model method, a technique of sharing similar Gaussian components is proposed. Acoustically similar components across all inventory of environmental models are selected by the proposed sub-optimal algorithm which employs the Kullback-Leibler similarity distance. The combined hybrid model, which consists of the selected Gaussian components is used for noisy speech model sharing. The performance is examined using Aurora2 and speech data for an in-vehicle environment. The proposed feature compensation algorithm is compared with standard methods in the field (e.g., CMN, spectral subtraction, RATZ). The experimental results demonstrate that the proposed feature compensation schemes are very effective in realizing robust speech recognition in adverse noisy environments. The proposed model combination-based feature compensation method is superior to existing model-based feature compensation methods. Of particular interest is that the proposed method shows up to an 11.59% relative WER reduction compared to the ETSI AFE front-end method. The multi-model approach is effective at coping with changing noise conditions for input speech, producing comparable performance to the matched model condition. Applying the mixture sharing method brings a significant reduction in computational overhead, while maintaining recognition performance at a reasonable level with near real-time operation. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Feature compensation in the cepstral domain employing model combination", "paper_id": "WOS:000262203800001"}