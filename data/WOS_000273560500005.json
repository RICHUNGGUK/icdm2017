{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "rotation_forest"}, {"score": 0.012227068817389079, "phrase": "ensemble_classifiers"}, {"score": 0.005911611760633525, "phrase": "bagging"}, {"score": 0.004633996867999857, "phrase": "proposed_method"}, {"score": 0.004400329132129292, "phrase": "principal_component_analysis"}, {"score": 0.0043540443503270665, "phrase": "pca"}, {"score": 0.004240116703465252, "phrase": "original_feature_axes"}, {"score": 0.004173244935967375, "phrase": "different_training_sets"}, {"score": 0.0041074234667687875, "phrase": "base_classifiers"}, {"score": 0.003287482882222731, "phrase": "ensemble_membership"}, {"score": 0.0031176485976102688, "phrase": "uci_repository"}, {"score": 0.0030360404090624396, "phrase": "classification_tree"}, {"score": 0.002956562077120703, "phrase": "base_learning_algorithm"}, {"score": 0.0028037753646260937, "phrase": "lower_error"}, {"score": 0.0027448950815394816, "phrase": "adaboost"}, {"score": 0.00267301171407926, "phrase": "bias-variance_analysis"}, {"score": 0.0025348408238837655, "phrase": "prediction_error"}, {"score": 0.0024947959375982614, "phrase": "single_classifier"}, {"score": 0.0023038418282549274, "phrase": "data_sets"}, {"score": 0.002279508335630797, "phrase": "artificial_classification_noise"}, {"score": 0.002231607958228148, "phrase": "new_method"}, {"score": 0.0021731423856364003, "phrase": "noise_and_kappa-error_diagrams"}, {"score": 0.0021049977753042253, "phrase": "diversity-accuracy_patterns"}], "paper_keywords": ["Ensemble classifier", " Rotation Forest", " Bagging", " AdaBoost", " Kappa-error diagram"], "paper_abstract": "Rotation Forest, an effective ensemble classifier generation technique, works by using principal component analysis (PCA) to rotate the original feature axes so that different training sets for learning base classifiers can be formed. This paper presents a variant of Rotation Forest, which can be viewed as a combination of Bagging and Rotation Forest. Bagging is used here to inject more randomness into Rotation Forest in order to increase the diversity among the ensemble membership. The experiments conducted with 33 benchmark classification data sets available from the UCI repository, among which a classification tree is adopted as the base learning algorithm, demonstrate that the proposed method generally produces ensemble classifiers with lower error than Bagging, AdaBoost and Rotation Forest. The bias-variance analysis of error performance shows that the proposed method improves the prediction error of a single classifier by reducing much more variance term than the other considered ensemble procedures. Furthermore, the results computed on the data sets with artificial classification noise indicate that the new method is more robust to noise and kappa-error diagrams are employed to investigate the diversity-accuracy patterns of the ensemble classifiers.", "paper_title": "A variant of Rotation Forest for constructing ensemble classifiers", "paper_id": "WOS:000273560500005"}