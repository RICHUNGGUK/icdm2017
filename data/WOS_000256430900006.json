{"auto_keywords": [{"score": 0.0347939044065656, "phrase": "optimal_q-value_function"}, {"score": 0.00481495049065317, "phrase": "decentralized_pomdps"}, {"score": 0.0047732596509542135, "phrase": "decision-theoretic_planning"}, {"score": 0.0047113960612140335, "phrase": "popular_approach"}, {"score": 0.004670597652149685, "phrase": "sequential_decision_making_problems"}, {"score": 0.00443308765153716, "phrase": "principled_way"}, {"score": 0.004375613460102577, "phrase": "single-agent_frameworks"}, {"score": 0.0041170581247889654, "phrase": "q-value_functions"}, {"score": 0.004063664475797303, "phrase": "optimal_q-value_function_q"}, {"score": 0.003958937342925219, "phrase": "recursive_manner"}, {"score": 0.003924629168434799, "phrase": "dynamic_programming"}, {"score": 0.0038401488612789963, "phrase": "optimal_policy"}, {"score": 0.003660614912817745, "phrase": "similar_q-value_functions"}, {"score": 0.003581797255879607, "phrase": "decentralized_pomdp_models"}, {"score": 0.0032688177078844414, "phrase": "dec-pomdps"}, {"score": 0.0031845067058084583, "phrase": "normative_description"}, {"score": 0.0031431690368229443, "phrase": "q-value_function"}, {"score": 0.0031023662967184216, "phrase": "optimal_pure_joint_policy"}, {"score": 0.0027581067089091434, "phrase": "smallest_problems"}, {"score": 0.0024519545383399773, "phrase": "upper_bound"}, {"score": 0.002357632753637638, "phrase": "previous_approaches"}, {"score": 0.0021607883988873492, "phrase": "experimental_evaluation"}, {"score": 0.0021420294823879292, "phrase": "existing_test_problems"}, {"score": 0.0021049977753042253, "phrase": "new_firefighting_benchmark_problem"}], "paper_keywords": [""], "paper_abstract": "Decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. In single-agent frameworks like MDPs and POMDPs, planning can be carried out by resorting to Q-value functions: an optimal Q-value function Q* is computed in a recursive manner by dynamic programming, and then an optimal policy is extracted from Q*. In this paper we study whether similar Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs), and how policies can be extracted from such value functions. We define two forms of the optimal Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation. This computation, however, is infeasible for all but the smallest problems. Therefore, we analyze various approximate Q-value functions that allow for efficient computation. We describe how they relate, and we prove that they all provide an upper bound to the optimal Q-value function Q*. Finally, unifying some previous approaches for solving Dec-POMDPs, we describe a family of algorithms for extracting policies from such Q-value functions, and perform an experimental evaluation on existing test problems, including a new firefighting benchmark problem.", "paper_title": "Optimal and approximate Q-value functions for decentralized POMDPs", "paper_id": "WOS:000256430900006"}