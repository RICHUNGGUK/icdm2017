{"auto_keywords": [{"score": 0.044658987607950375, "phrase": "unlabeled_test_cases"}, {"score": 0.043453500997964946, "phrase": "training_sample"}, {"score": 0.00481495049065317, "phrase": "semi-supervised_nearest_neighbor_classification"}, {"score": 0.00447115003744675, "phrase": "labeled_observations"}, {"score": 0.0043703658695579085, "phrase": "decision_rule"}, {"score": 0.003944112858235306, "phrase": "sufficient_information"}, {"score": 0.003789763907758991, "phrase": "good_classifier"}, {"score": 0.003683213029483699, "phrase": "statistical_instability"}, {"score": 0.003641433168173135, "phrase": "nonparametric_methods"}, {"score": 0.003419943984098415, "phrase": "nonparametric_classification"}, {"score": 0.0032487508969192293, "phrase": "useful_information"}, {"score": 0.003068537223324389, "phrase": "classification_rule"}, {"score": 0.0029652292779277782, "phrase": "resulting_classifier"}, {"score": 0.002753138528582461, "phrase": "probabilistic_framework"}, {"score": 0.002675654549462983, "phrase": "nearest_neighbor_classification"}, {"score": 0.0026302120935562568, "phrase": "resulting_classifiers"}, {"score": 0.0024281181691350085, "phrase": "supervised_methods"}, {"score": 0.0022933216359142736, "phrase": "benchmark_data_sets"}, {"score": 0.002178400923036365, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Bayesian model averaging", " Cross-validation", " Likelihood function", " Markov Chain Monte Carlo", " Misclassification rate", " Transductive learning"], "paper_abstract": "In supervised classification, we learn from a training set of labeled observations to form a decision rule for classifying all unlabeled test cases. But if the training sample is small, one may fail to extract sufficient information from that sample to develop a good classifier. Because of the statistical instability of nonparametric methods, this problem becomes more evident in the case of nonparametric classification. In such cases, if one can extract useful information also from unlabeled test cases and use that to modify the classification rule, the performance of the resulting classifier can be improved substantially. In this article, we use a probabilistic framework to develop such methods for nearest neighbor classification. The resulting classifiers, called semi-supervised or transductive classifiers, usually perform better than supervised methods, especially when the training sample is small. Some benchmark data sets are analyzed to show the utility of these proposed methods. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A probabilistic approach for semi-supervised nearest neighbor classification", "paper_id": "WOS:000304235500013"}