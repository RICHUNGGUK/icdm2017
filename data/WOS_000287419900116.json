{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "weighted_principal_components"}, {"score": 0.029322338265635416, "phrase": "thresholding_algorithm"}, {"score": 0.02888526278555337, "phrase": "weighted_pc"}, {"score": 0.004765788394931875, "phrase": "feature_selection"}, {"score": 0.004692980391580608, "phrase": "considerable_attention"}, {"score": 0.004504193207111593, "phrase": "informative_features"}, {"score": 0.004390060342250211, "phrase": "statistical_model"}, {"score": 0.0043452175611323335, "phrase": "dimensional_reduction"}, {"score": 0.004127753467337283, "phrase": "principal_component_analysis"}, {"score": 0.003962047977463083, "phrase": "pca"}, {"score": 0.003782655283668953, "phrase": "original_feature"}, {"score": 0.003724810609448597, "phrase": "reduced_dimensions"}, {"score": 0.00368673793612343, "phrase": "linear_combinations"}, {"score": 0.0036303546926971966, "phrase": "large_number"}, {"score": 0.0035932440062595252, "phrase": "original_features"}, {"score": 0.0034308688638878286, "phrase": "important_original_features"}, {"score": 0.003378385493203297, "phrase": "component_dimensions"}, {"score": 0.003209152260950748, "phrase": "loading_plot"}, {"score": 0.0030957429853133147, "phrase": "large_numbers"}, {"score": 0.002895620643297197, "phrase": "unsupervised_feature_selection_method"}, {"score": 0.0026532596852754525, "phrase": "weighted_sum"}, {"score": 0.0026126405272417783, "phrase": "first_k_pcs"}, {"score": 0.0025202582748972122, "phrase": "k_loading_values"}, {"score": 0.0024062532032402533, "phrase": "individual_feature"}, {"score": 0.0022856052476424344, "phrase": "significant_features"}, {"score": 0.0021709933368474223, "phrase": "proposed_unsupervised_feature_selection_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Data mining", " Feature selection", " Principal component analysis", " Unsupervised learning"], "paper_abstract": "Feature selection has received considerable attention in various areas as a way to select informative features and to simplify the statistical model through dimensional reduction. One of the most widely used methods for dimensional reduction includes principal component analysis (PCA). Despite its popularity. PCA suffers from a lack of interpretability of the original feature because the reduced dimensions are linear combinations of a large number of original features. Traditionally, two or three dimensional loading plots provide information to identify important original features in the first few principal component dimensions. However, the interpretation of what constitutes a loading plot is frequently subjective, particularly when large numbers of features are involved. In this study, we propose an unsupervised feature selection method that combines weighted principal components (PCs) with a thresholding algorithm. The weighted PC is obtained by the weighted sum of the first k PCs of interest. Each of the k loading values in the weighted PC reflects the contribution of each individual feature. We also propose a thresholding algorithm that identifies the significant features. Our experimental results with both the simulated and real datasets demonstrated the effectiveness of the proposed unsupervised feature selection method. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Unsupervised feature selection using weighted principal components", "paper_id": "WOS:000287419900116"}