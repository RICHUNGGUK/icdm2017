{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "large_scale"}, {"score": 0.004763565662615503, "phrase": "line_balancing_problem"}, {"score": 0.004662427600500598, "phrase": "reinforcement_learning"}, {"score": 0.004563427013973105, "phrase": "increasing_environmental_concerns"}, {"score": 0.004301834610818624, "phrase": "useful_functional_life"}, {"score": 0.004165497627464024, "phrase": "disassembly_operations"}, {"score": 0.0036422425344358037, "phrase": "disposal_volume"}, {"score": 0.003451766520526601, "phrase": "high_degree"}, {"score": 0.0032712190037495975, "phrase": "product_returns"}, {"score": 0.0031335830806256777, "phrase": "disassembly_line_balancing_problem"}, {"score": 0.003100085797251317, "phrase": "dlbp"}, {"score": 0.003034158379493625, "phrase": "monte-carlo_based_reinforcement_learning_technique"}, {"score": 0.002985631781749091, "phrase": "reinforcement_learning_approach"}, {"score": 0.002890887778279196, "phrase": "underlying_dynamics"}, {"score": 0.0028446460307503343, "phrase": "dlbp."}, {"score": 0.0028142286985235977, "phrase": "research_results"}, {"score": 0.002724909017963099, "phrase": "based_method"}, {"score": 0.0025822818714499795, "phrase": "complex_large_scale_problem"}, {"score": 0.0025273383208209922, "phrase": "reasonable_amount"}, {"score": 0.0025003054002226944, "phrase": "computational_time"}, {"score": 0.0023440589015485077, "phrase": "benchmark_methods"}, {"score": 0.002174041708084857, "phrase": "reinforcement_learning_based_method"}, {"score": 0.0021049977753042253, "phrase": "deterministic_as_well_as_stochastic_environments"}], "paper_keywords": ["Disassembly", " Reinforcement learning", " Heuristics", " Disassembly line balancing", " Cell phone", " PC"], "paper_abstract": "Due to increasing environmental concerns, manufacturers are forced to take back their products at the end of products' useful functional life. Manufacturers explore various options including disassembly operations to recover components and subassemblies for reuse, remanufacture, and recycle to extend the life of materials in use and cut down the disposal volume. However, disassembly operations are problematic due to high degree of uncertainty associated with the quality and configuration of product returns. In this research we address the disassembly line balancing problem (DLBP) using a Monte-Carlo based reinforcement learning technique. This reinforcement learning approach is tailored fit to the underlying dynamics of a DLBP. The research results indicate that the reinforcement learning based method is able to perform effectively, even on a complex large scale problem, within a reasonable amount of computational time. The proposed method performed on par or better than the benchmark methods for solving DLBP reported in the literature. Unlike other methods which are usually limited deterministic environments, the reinforcement learning based method is able to operate in deterministic as well as stochastic environments.", "paper_title": "Solving large scale disassembly line balancing problem with uncertainty using reinforcement learning", "paper_id": "WOS:000339375400002"}