{"auto_keywords": [{"score": 0.03301145866280186, "phrase": "f-b"}, {"score": 0.03151462369793249, "phrase": "faulty_processes"}, {"score": 0.00481495049065317, "phrase": "unreliable_workers"}, {"score": 0.004786315041142917, "phrase": "internet_supercomputing"}, {"score": 0.00472955160788118, "phrase": "powerful_tool"}, {"score": 0.004687419167634463, "phrase": "massive_amounts"}, {"score": 0.004659538620628888, "phrase": "computational_resources"}, {"score": 0.004604271739916253, "phrase": "typical_master-worker_settings"}, {"score": 0.004210050422187886, "phrase": "distributed_system"}, {"score": 0.004160092647083038, "phrase": "master_process"}, {"score": 0.003860961077439364, "phrase": "fallacious_results"}, {"score": 0.0036263171149449997, "phrase": "n_processes"}, {"score": 0.0030313341227908687, "phrase": "previous_works"}, {"score": 0.0028131366865158302, "phrase": "model_f-b"}, {"score": 0.0027221609101329403, "phrase": "f-a"}, {"score": 0.002681783466572198, "phrase": "model_f-a"}, {"score": 0.0026028125439483216, "phrase": "stopping_rule_algorithm"}, {"score": 0.0025872989069543892, "phrase": "dagum_et_al"}, {"score": 0.002323308052177842, "phrase": "randomized_algorithm"}, {"score": 0.0021753930127723386, "phrase": "n_workers"}, {"score": 0.0021559645899673664, "phrase": "models_f-a"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Distributed algorithms", " Fault-tolerance", " Randomized algorithms", " Reliability", " Internet supercomputing"], "paper_abstract": "Internet supercomputing is becoming a powerful tool for harnessing massive amounts of computational resources. However in typical master-worker settings the correctness of the results of the computation crucially relies on the ability of the master to depend on the computation performed by the workers. We consider a distributed system consisting of a master process and a collection of synchronous worker processes that can execute tasks on behalf of the master and that may act nefariously by deliberately returning fallacious results. The master decides on the correctness of the results by assigning the same task to several workers. For such a setting with n processes and t tasks we study the problem of collectively performing the tasks under two different failure models: model-F-a, where some fraction f of workers that are prone to returning arbitrary (e.g., incorrect) results and the probability p of such faulty behavior are not known a priori to the master, and model F-b when these quantities are known to the master. Previous works assume that the number of faulty processes or the probability of a process acting maliciously is known to the master, e.g., as in model F-b. In this paper this assumption is removed in model F-a. First, for model F-a we provide an efficient algorithm-based on the Stopping Rule Algorithm by Dagum et al. (1995)- that can estimate f and p with (is an element of, delta)-approximation, for any 0 < delta < 1 and is an element of > 0. We also provide a randomized algorithm for detecting the faulty processes for model F-a. Finally, we provide algorithms to perform t tasks, with n workers, in models F-a and F-b. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Robust network supercornputing with unreliable workers", "paper_id": "WOS:000346552500007"}