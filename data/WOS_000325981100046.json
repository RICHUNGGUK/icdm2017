{"auto_keywords": [{"score": 0.026712723209610028, "phrase": "catoni"}, {"score": 0.00481495049065317, "phrase": "heavy_tail"}, {"score": 0.004714023408114857, "phrase": "stochastic_multiarmed_bandit_problem"}, {"score": 0.004518442937844318, "phrase": "reward_distributions"}, {"score": 0.004121960535174029, "phrase": "bandit_problem"}, {"score": 0.004035500442527029, "phrase": "weaker_assumption"}, {"score": 0.0030847461428168614, "phrase": "regret_bounds"}, {"score": 0.0029565620771207003, "phrase": "sub-gaussian_reward_distributions"}, {"score": 0.002677744558540711, "phrase": "refined_estimators"}, {"score": 0.002548331088496964, "phrase": "truncated_empirical_mean"}, {"score": 0.002196342933346204, "phrase": "lower_bounds"}, {"score": 0.0021049977753042253, "phrase": "best_achievable_regret"}], "paper_keywords": ["Heavy-tailed distributions", " regret bounds", " robust estimators", " stochastic multi-armed bandit"], "paper_abstract": "The stochastic multiarmed bandit problem is well understood when the reward distributions are sub-Gaussian. In this paper, we examine the bandit problem under the weaker assumption that the distributions have moments of order 1 + epsilon, for some epsilon is an element of (0, 1]. Surprisingly, moments of order 2 (i.e., finite variance) are sufficient to obtain regret bounds of the same order as under sub-Gaussian reward distributions. In order to achieve such regret, we define sampling strategies based on refined estimators of the mean such as the truncated empirical mean, Catoni's M-estimator, and the median-of-means estimator. We also derive matching lower bounds that also show that the best achievable regret deteriorates when epsilon < 1.", "paper_title": "Bandits With Heavy Tail", "paper_id": "WOS:000325981100046"}