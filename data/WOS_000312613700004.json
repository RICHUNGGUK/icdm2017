{"auto_keywords": [{"score": 0.049231721134487566, "phrase": "medical_imaging"}, {"score": 0.04840454988301126, "phrase": "fleiss'_kappa"}, {"score": 0.028646658940364535, "phrase": "observer_agreement"}, {"score": 0.02767528335008882, "phrase": "ci"}, {"score": 0.00481495049065317, "phrase": "inter-observer_agreement"}, {"score": 0.004680283709974884, "phrase": "dummy_run"}, {"score": 0.0044852808671980325, "phrase": "radiation_therapy"}, {"score": 0.004375286710337962, "phrase": "treatment_volume"}, {"score": 0.004267978415344142, "phrase": "uniform_slice_thickness"}, {"score": 0.004104621766670225, "phrase": "arbitrary_number"}, {"score": 0.004032441847334232, "phrase": "single_number"}, {"score": 0.003961526165403768, "phrase": "kappa_index"}, {"score": 0.0039195744361570075, "phrase": "zijdenbos_et_al"}, {"score": 0.0036123093557208492, "phrase": "contoured_region"}, {"score": 0.0034987163424726546, "phrase": "n_indistinguishable_observers"}, {"score": 0.003449379912099073, "phrase": "number_v"}, {"score": 0.0033766722372180003, "phrase": "exactly_i_observers"}, {"score": 0.0032358074601765555, "phrase": "resulting_overall_kappa"}, {"score": 0.0031563574387360465, "phrase": "weighted_sums"}, {"score": 0.0030788521585190802, "phrase": "overall_kappa"}, {"score": 0.003024655649924378, "phrase": "inter-center_variations"}, {"score": 0.0029925953740661754, "phrase": "multicenter_trial"}, {"score": 0.00297141030969672, "phrase": "radiotherapy_planning"}, {"score": 0.0029294877178473017, "phrase": "locally_advanced_lung_cancer"}, {"score": 0.0028984332556216946, "phrase": "contouring_dummy_run"}, {"score": 0.0028474035592052052, "phrase": "quality_assurance_program"}, {"score": 0.002827243837129445, "phrase": "contouring"}, {"score": 0.0027188802510720775, "phrase": "training_program"}, {"score": 0.002426630714074822, "phrase": "average_pairwise_indices"}, {"score": 0.0024094425220967273, "phrase": "overall_kappa_measures"}, {"score": 0.0023502347146791285, "phrase": "full_information"}, {"score": 0.002333586381274878, "phrase": "overlapping_volumes"}, {"score": 0.0021049977753042253, "phrase": "gold_standard"}], "paper_keywords": ["Contour delineation", " Fleiss' kappa", " medical imaging", " observer agreement", " PET-CT"], "paper_abstract": "Background: In medical imaging used for planning of radiation therapy, observers delineate contours of a treatment volume in a series of images of uniform slice thickness. Objective: To summarize agreement in contouring between an arbitrary number of observers by a single number, we generalized the kappa index proposed by Zijdenbos et al. (1994). Methods: Observers characterized voxels by allocating them to one of two categories, inside or outside the contoured region. Fleiss' kappa was used to measure association between n indistinguishable observers. Given the number V, of voxels contoured by exactly i observers (i = 1, ... , n), the resulting overall kappa is representable as a ratio of weighted sums of the V-i. Results: Overall kappa was applied to analyze inter-center variations in a multicenter trial on radiotherapy planning in patients with locally advanced lung cancer. A contouring dummy run was performed within the quality assurance program. Contouring was done twice, once before and once after a training program. Observer agreement was enhanced from 0.59 (with a 95% confidence interval (CI) of 0.51-0.67) to 0.69(95% CI 0.59-0.78). Conclusion: By contrast to average pairwise indices, overall kappa measures observer agreement for more than two observers using the full information about overlapping volumes, while not distinguishing between observers. It is particularly adequate for measuring observer agreement when identification of observers is not possible or desirable and when there is no gold standard.", "paper_title": "Measuring Inter-observer Agreement in Contour Delineation of Medical Imaging in a Dummy Run Using Fleiss' Kappa", "paper_id": "WOS:000312613700004"}