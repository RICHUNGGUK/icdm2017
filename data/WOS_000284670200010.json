{"auto_keywords": [{"score": 0.049757144405741305, "phrase": "ranking_functions"}, {"score": 0.03303841165256633, "phrase": "supervised_algorithms"}, {"score": 0.029549610738318598, "phrase": "test_list"}, {"score": 0.00481495049065317, "phrase": "document_retrieval"}, {"score": 0.004734448316824488, "phrase": "important_component"}, {"score": 0.0047026237903774895, "phrase": "information_retrieval_systems"}, {"score": 0.004336939968686303, "phrase": "labeled_training_data"}, {"score": 0.004235717186207754, "phrase": "reliable_ranking_functions"}, {"score": 0.004193059395895776, "phrase": "learning_methods"}, {"score": 0.004150829420202823, "phrase": "neural_networks"}, {"score": 0.004122911675081472, "phrase": "support_vector_machines"}, {"score": 0.004081385289516914, "phrase": "least_squares"}, {"score": 0.003999578057088828, "phrase": "ranking_problems"}, {"score": 0.003879919492075319, "phrase": "commercial_search_engines"}, {"score": 0.0037133478610481994, "phrase": "supervised_learning_setting"}, {"score": 0.0036511960413423542, "phrase": "relevance_labels"}, {"score": 0.0036022217019257598, "phrase": "human_annotators"}, {"score": 0.003541923233726252, "phrase": "ranking_function"}, {"score": 0.003459191419478298, "phrase": "human_relevance_judgments"}, {"score": 0.0033898129842622536, "phrase": "wide_range"}, {"score": 0.003266201217790818, "phrase": "additional_unlabeled_data"}, {"score": 0.003042555825368091, "phrase": "transductive_setting"}, {"score": 0.0030017207229983385, "phrase": "unlabeled_data"}, {"score": 0.002951444412262927, "phrase": "test_data"}, {"score": 0.0029020077365708966, "phrase": "simple_yet_flexible_transductive_meta-algorithm"}, {"score": 0.002815093247670994, "phrase": "training_procedure"}, {"score": 0.002640038329840458, "phrase": "general_framework"}, {"score": 0.0025436978068459565, "phrase": "unlabeled_test_data"}, {"score": 0.0024842242921857705, "phrase": "test-dependent_feature-set"}, {"score": 0.002385479082527538, "phrase": "domain_adaptation_literature"}, {"score": 0.0023139986108451946, "phrase": "training_data"}, {"score": 0.0021700361584578633, "phrase": "trec_and_ohsumed_tasks"}, {"score": 0.002148136905808445, "phrase": "letor_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Information retrieval", " Ranking algorithms", " Semi-supervised Learning"], "paper_abstract": "Ranking functions are an important component of information retrieval systems. Recently there has been a surge of research in the field of \"learning to rank\", which aims at using labeled training data and machine learning algorithms to construct reliable ranking functions. Machine learning methods such as neural networks, support vector machines, and least squares have been successfully applied to ranking problems, and some are already being deployed in commercial search engines. Despite these successes, most algorithms to date construct ranking functions in a supervised learning setting, which assume that relevance labels are provided by human annotators prior to training the ranking function. Such methods may perform poorly when human relevance judgments are not available for a wide range of queries. In this paper, we examine whether additional unlabeled data, which is easy to obtain, can be used to improve supervised algorithms. In particular, we investigate the transductive setting, where the unlabeled data is equivalent to the test data. We propose a simple yet flexible transductive meta-algorithm: the key idea is to adapt the training procedure to each test list after observing the documents that need to be ranked. We investigate two instantiations of this general framework: The Feature Generation approach is based on discovering more salient features from the unlabeled test data and training a ranker on this test-dependent feature-set. The importance weighting approach is based on ideas in the domain adaptation literature, and works by re-weighting the training data to match the statistics of each test list. We demonstrate that both approaches improve over supervised algorithms on the TREC and OHSUMED tasks from the LETOR dataset. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Semi-supervised ranking for document retrieval", "paper_id": "WOS:000284670200010"}