{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mixed_project_data"}, {"score": 0.03524594274659415, "phrase": "project_data"}, {"score": 0.009617519860552542, "phrase": "mixed_project_predictors"}, {"score": 0.009456398599803476, "phrase": "project_predictors"}, {"score": 0.004774254873524455, "phrase": "defect_predictors"}, {"score": 0.004524348699657099, "phrase": "isolated_projects"}, {"score": 0.004398094237005956, "phrase": "retrospective_analyses"}, {"score": 0.004324031677173549, "phrase": "recent_studies"}, {"score": 0.004156012879026324, "phrase": "defect_prediction_models"}, {"score": 0.004132546937996706, "phrase": "new_projects"}, {"score": 0.0037959868107895053, "phrase": "binary_defect_prediction"}, {"score": 0.003627800727103479, "phrase": "defect_detection_performance"}, {"score": 0.003437697879833853, "phrase": "project_history"}, {"score": 0.003104310486173269, "phrase": "naive_bayes_classifiers"}, {"score": 0.0030005055373644096, "phrase": "first_case"}, {"score": 0.002795228413298423, "phrase": "effect_size"}, {"score": 0.0027170943110836425, "phrase": "second_case"}, {"score": 0.002446470977857123, "phrase": "extra_effort"}, {"score": 0.0023579143175099324, "phrase": "practical_performance_improvement"}, {"score": 0.002305014991156598, "phrase": "project_defect_predictor"}, {"score": 0.0022919762167777427, "phrase": "full_project_history"}, {"score": 0.002246917304058838, "phrase": "limited_project_history"}, {"score": 0.002234206434819338, "phrase": "e.g._early_phases"}, {"score": 0.0022089995300212588, "phrase": "mixed_project_predictions"}, {"score": 0.002141134176057931, "phrase": "project_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Cross project", " Within project", " Mixed project", " Defect prediction", " Fault prediction", " Product metrics"], "paper_abstract": "Context: Defect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects (i.e. within project (WP)) through retrospective analyses. On the other hand, recent studies try to utilize data across projects (i.e. cross project (CP)) for building defect prediction models for new projects. There are no cases where the combination of within and cross (i.e. mixed) project data are used together. Objective: Our goal is to investigate the merits of using mixed project data for binary defect prediction. Specifically, we want to check whether it is feasible, in terms of defect detection performance, to use data from other projects for the cases (i) when there is an existing within project history and (ii) when there are limited within project data. Method: We use data from 73 versions of 41 projects that are publicly available. We simulate the two above-mentioned cases, and compare the performances of naive Bayes classifiers by using within project data vs. mixed project data. Results: For the first case, we find that the performance of mixed project predictors significantly improves over full within project predictors (p-value < 0.001), however the effect size is small (Hedges' g = 0.25). For the second case, we found that mixed project predictors are comparable to full within project predictors, using only 10% of available within project data (p-value = 0.002, g = 0.17). Conclusion: We conclude that the extra effort associated with collecting data from other projects is not feasible in terms of practical performance improvement when there is already an established within project defect predictor using full project history. However, when there is limited project history, e.g. early phases of development, mixed project predictions are justifiable as they perform as good as full within project models. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Empirical evaluation of the effects of mixed project data on learning defect predictors", "paper_id": "WOS:000318584800012"}