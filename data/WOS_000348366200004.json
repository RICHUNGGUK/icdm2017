{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "compact_binary_image_encodings"}, {"score": 0.0047026237903774895, "phrase": "high-dimensional_features"}, {"score": 0.004614643339546038, "phrase": "normal_practice"}, {"score": 0.004506968388996092, "phrase": "large_dimension"}, {"score": 0.00438105507792004, "phrase": "limiting_factor"}, {"score": 0.004278807087784626, "phrase": "data_points"}, {"score": 0.0039114753002009485, "phrase": "novel_approach"}, {"score": 0.0038382392088338784, "phrase": "compact_binary_encoding"}, {"score": 0.0037486121194392564, "phrase": "pairwise_proximity"}, {"score": 0.0037133478610481994, "phrase": "class-label_information"}, {"score": 0.0035755653799043, "phrase": "extra_information"}, {"score": 0.0033624514778823763, "phrase": "original_high-dimensional_features"}, {"score": 0.003299460925313939, "phrase": "final_classification"}, {"score": 0.00326840809777013, "phrase": "retrieval_performance"}, {"score": 0.0030590445140753187, "phrase": "nonparametric_and_parametric_learning_methods"}, {"score": 0.0029594318330247614, "phrase": "embedded_features"}, {"score": 0.002890266318361907, "phrase": "wide_variety"}, {"score": 0.0028630535573013686, "phrase": "computer_vision_tasks"}, {"score": 0.0028093921138602606, "phrase": "image_classification"}, {"score": 0.002782938683493689, "phrase": "content-based_image_retrieval"}, {"score": 0.0026922926476971453, "phrase": "new_compact_descriptor"}, {"score": 0.002507845411496512, "phrase": "visual_descriptor"}, {"score": 0.00247249708860822, "phrase": "original_space"}, {"score": 0.002358220053411644, "phrase": "convex_loss_function"}, {"score": 0.0023249757987087055, "phrase": "regularization_penalty"}, {"score": 0.0021049977753042253, "phrase": "future_flexibility"}], "paper_keywords": ["Hashing", " binary codes", " column generation", " image classification"], "paper_abstract": "The use of high-dimensional features has become a normal practice in many computer vision applications. The large dimension of these features is a limiting factor upon the number of data points, which may be effectively stored and processed, however. We address this problem by developing a novel approach to learning a compact binary encoding, which exploits both pairwise proximity and class-label information on training data set. Exploiting this extra information allows the development of encodings which, although compact, outperform the original high-dimensional features in terms of final classification or retrieval performance. The method is general, in that it is applicable to both nonparametric and parametric learning methods. This generality means that the embedded features are suitable for a wide variety of computer vision tasks, such as image classification and content-based image retrieval. Experimental results demonstrate that the new compact descriptor achieves an accuracy comparable to, and in some cases better than, the visual descriptor in the original space despite being significantly more compact. Moreover, any convex loss function and convex regularization penalty (e.g., l(p) norm with p >= 1) can be incorporated into the framework, which provides future flexibility.", "paper_title": "Large-Margin Learning of Compact Binary Image Encodings", "paper_id": "WOS:000348366200004"}