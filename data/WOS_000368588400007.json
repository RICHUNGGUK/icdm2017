{"auto_keywords": [{"score": 0.04832350902760217, "phrase": "rdf_data"}, {"score": 0.015720037466456477, "phrase": "rdf"}, {"score": 0.014532763348398968, "phrase": "hadoop"}, {"score": 0.013371802288737626, "phrase": "sparql_queries"}, {"score": 0.007768529230172434, "phrase": "data_redundancy"}, {"score": 0.004747997855358171, "phrase": "scalable_sparql_query_processing"}, {"score": 0.004489327204473076, "phrase": "cloud_computing_platforms"}, {"score": 0.004324723240308028, "phrase": "good_choice"}, {"score": 0.004224911726496726, "phrase": "huge_data_sets"}, {"score": 0.004146716743362567, "phrase": "previous_work"}, {"score": 0.0038661089070527424, "phrase": "careful_split"}, {"score": 0.0038301525703886585, "phrase": "hdfs_files"}, {"score": 0.0035376693476114733, "phrase": "system_performance"}, {"score": 0.0034559594248079807, "phrase": "good_partitioning_solution"}, {"score": 0.003329117824699891, "phrase": "cross-node_joins"}, {"score": 0.0031919592786945126, "phrase": "query_evaluation"}, {"score": 0.0030178044717552605, "phrase": "hybrid_architecture"}, {"score": 0.0029069972482610403, "phrase": "computing_tasks"}, {"score": 0.00263505298504827, "phrase": "query_workloads"}, {"score": 0.002562123444678208, "phrase": "novel_algorithm"}, {"score": 0.002479579797152119, "phrase": "approximate_solution"}, {"score": 0.0022686625017583387, "phrase": "good_trade-off"}, {"score": 0.0022475295254231714, "phrase": "query_evaluation_efficiency"}, {"score": 0.0021750990136123367, "phrase": "proposed_approaches"}, {"score": 0.0021247934141234988, "phrase": "extensive_experiments"}, {"score": 0.0021049977753042253, "phrase": "large_rdf_data_sets"}], "paper_keywords": ["RDF data", " data partitioning", " SPARQL query"], "paper_abstract": "The volume of RDF data increases dramatically within recent years, while cloud computing platforms like Hadoop are supposed to be a good choice for processing queries over huge data sets for their wonderful scalability. Previous work on evaluating SPARQL queries with Hadoop mainly focus on reducing the number of joins through careful split of HDFS files and algorithms for generating Map/Reduce jobs. However, the way of partitioning RDF data could also affect system performance. Specifically, a good partitioning solution would greatly reduce or even totally avoid cross-node joins, and significantly cut down the cost in query evaluation. Based on HadoopDB, this work processes SPARQL queries in a hybrid architecture, where Map/Reduce takes charge of the computing tasks, and RDF query engines like RDF-3X store the data and execute join operations. According to the analysis of query workloads, this work proposes a novel algorithm for automatically partitioning RDF data and an approximate solution to physically place the partitions in order to reduce data redundancy. It also discusses how to make a good trade-off between query evaluation efficiency and data redundancy. All of these proposed approaches have been evaluated by extensive experiments over large RDF data sets.", "paper_title": "RDF partitioning for scalable SPARQL query processing", "paper_id": "WOS:000368588400007"}