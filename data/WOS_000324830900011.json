{"auto_keywords": [{"score": 0.03577773358930474, "phrase": "pixel_events"}, {"score": 0.025101750546645506, "phrase": "event-driven_convnet"}, {"score": 0.00481495049065317, "phrase": "frame-driven"}, {"score": 0.0047810714349158165, "phrase": "frame-free_event-driven_vision_systems"}, {"score": 0.004747429622531595, "phrase": "low-rate_rate_coding"}, {"score": 0.004631527916092452, "phrase": "feedforward_convnets"}, {"score": 0.004598933498016774, "phrase": "event-driven_visual_sensors"}, {"score": 0.004470823343432748, "phrase": "different_research_communities"}, {"score": 0.004408106853358092, "phrase": "visual_information"}, {"score": 0.004377077848734575, "phrase": "quite_a_different_way"}, {"score": 0.004346266306175988, "phrase": "conventional_video_systems"}, {"score": 0.004270178957872099, "phrase": "still_images"}, {"score": 0.004165879874212301, "phrase": "event-driven_vision_sensors"}, {"score": 0.0037336520904376687, "phrase": "special_type"}, {"score": 0.0037073529462522403, "phrase": "event-driven_sensor"}, {"score": 0.0036682498811430623, "phrase": "so-called_dynamic_vision_sensor"}, {"score": 0.0035659724303237937, "phrase": "relative_changes"}, {"score": 0.0035159043158639633, "phrase": "\"temporal_contrast"}, {"score": 0.003466536738232506, "phrase": "sensor_output"}, {"score": 0.003417859963947459, "phrase": "continuous_flow"}, {"score": 0.003346119094502001, "phrase": "moving_objects"}, {"score": 0.003241312187060814, "phrase": "microsecond_delays"}, {"score": 0.00290473242917264, "phrase": "input_and_output_event_flows"}, {"score": 0.0025938179480905783, "phrase": "properly_trained_neural_network"}, {"score": 0.0025664295665392203, "phrase": "conventional_frame-driven_representation"}, {"score": 0.0025393296453558203, "phrase": "event-driven_representation"}, {"score": 0.002468450551616404, "phrase": "event-driven_convolutional_neural_networks"}, {"score": 0.002451054825611968, "phrase": "convnet"}, {"score": 0.0023995451127366983, "phrase": "human_silhouettes"}, {"score": 0.0023826206937008257, "phrase": "high_speed_poker_card_symbols"}, {"score": 0.00229164285862567, "phrase": "real_dvs_camera"}, {"score": 0.0022276618873985445, "phrase": "dedicated_event-driven_simulator"}, {"score": 0.0021731423856364003, "phrase": "event-driven_processing_modules"}, {"score": 0.0021049977753042253, "phrase": "individually_manufactured_hardware_modules"}], "paper_keywords": ["Feature extraction", " convolutional neural networks", " object recognition", " spiking neural networks", " event-driven neural networks", " bioinspired vision", " high speed vision"], "paper_abstract": "Event-driven visual sensors have attracted interest from a number of different research communities. They provide visual information in quite a different way from conventional video systems consisting of sequences of still images rendered at a given \"frame rate.\" Event-driven vision sensors take inspiration from biology. Each pixel sends out an event (spike) when it senses something meaningful is happening, without any notion of a frame. A special type of event-driven sensor is the so-called dynamic vision sensor (DVS) where each pixel computes relative changes of light or \"temporal contrast.\" The sensor output consists of a continuous flow of pixel events that represent the moving objects in the scene. Pixel events become available with microsecond delays with respect to \"reality.\" These events can be processed \"as they flow\" by a cascade of event (convolution) processors. As a result, input and output event flows are practically coincident in time, and objects can be recognized as soon as the sensor provides enough meaningful events. In this paper, we present a methodology for mapping from a properly trained neural network in a conventional frame-driven representation to an event-driven representation. The method is illustrated by studying event-driven convolutional neural networks (ConvNet) trained to recognize rotating human silhouettes or high speed poker card symbols. The event-driven ConvNet is fed with recordings obtained from a real DVS camera. The event-driven ConvNet is simulated with a dedicated event-driven simulator and consists of a number of event-driven processing modules, the characteristics of which are obtained from individually manufactured hardware modules.", "paper_title": "Mapping from Frame-Driven to Frame-Free Event-Driven Vision Systems by Low-Rate Rate Coding and Coincidence Processing-Application to Feedforward ConvNets", "paper_id": "WOS:000324830900011"}