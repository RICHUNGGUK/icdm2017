{"auto_keywords": [{"score": 0.030201072870748535, "phrase": "aos_algorithms"}, {"score": 0.00481495049065317, "phrase": "bandit-based_adaptive_operator_selection_mechanisms"}, {"score": 0.004578382275810468, "phrase": "adaptive_operator_selection"}, {"score": 0.004442032546224076, "phrase": "evolutionary_algorithms"}, {"score": 0.0043753813457279404, "phrase": "recent_proposals"}, {"score": 0.004056769841308405, "phrase": "mab_problem"}, {"score": 0.0038573081851680656, "phrase": "fitness_improvement"}, {"score": 0.003780286517897356, "phrase": "corresponding_operator"}, {"score": 0.003558294449586273, "phrase": "aos_problem"}, {"score": 0.003469676666252421, "phrase": "standard_mab_algorithms"}, {"score": 0.0033156708990372047, "phrase": "exploration_trade-off"}, {"score": 0.003216804708106142, "phrase": "original_dynamic_variant"}, {"score": 0.0031684791285366315, "phrase": "standard_mab_upper_confidence_bound_algorithm"}, {"score": 0.003058515297281172, "phrase": "sliding_time_window"}, {"score": 0.0029672941061574375, "phrase": "exploration_terms"}, {"score": 0.002878785759716458, "phrase": "sound_comparisons"}, {"score": 0.002821247621250663, "phrase": "artificial_scenarios"}, {"score": 0.0026420518944303716, "phrase": "smoother_transitions"}, {"score": 0.0026155094974689595, "phrase": "different_reward_settings"}, {"score": 0.0025761937402886954, "phrase": "resulting_original_testbed"}, {"score": 0.002524688179363081, "phrase": "real_evolutionary_algorithm"}, {"score": 0.0024493494346360415, "phrase": "well-known_royal_road_problem"}, {"score": 0.0023405257495466352, "phrase": "thorough_analysis"}, {"score": 0.0021263705039915198, "phrase": "sound_comparison"}], "paper_keywords": ["Parameter control", " Adaptive Operator Selection", " Multi-Armed Bandits"], "paper_abstract": "Several techniques have been proposed to tackle the Adaptive Operator Selection (AOS) issue in Evolutionary Algorithms. Some recent proposals are based on the Multi-armed Bandit (MAB) paradigm: each operator is viewed as one arm of a MAB problem, and the rewards are mainly based on the fitness improvement brought by the corresponding operator to the individual it is applied to. However, the AOS problem is dynamic, whereas standard MAB algorithms are known to optimally solve the exploitation versus exploration trade-off in static settings. An original dynamic variant of the standard MAB Upper Confidence Bound algorithm is proposed here, using a sliding time window to compute both its exploitation and exploration terms. In order to perform sound comparisons between AOS algorithms, artificial scenarios have been proposed in the literature. They are extended here toward smoother transitions between different reward settings. The resulting original testbed also includes a real evolutionary algorithm that is applied to the well-known Royal Road problem. It is used here to perform a thorough analysis of the behavior of AOS algorithms, to assess their sensitivity with respect to their own hyper-parameters, and to propose a sound comparison of their performances.", "paper_title": "Analyzing bandit-based adaptive operator selection mechanisms", "paper_id": "WOS:000291995800003"}