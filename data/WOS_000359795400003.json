{"auto_keywords": [{"score": 0.04908818662852138, "phrase": "auxiliary_variables"}, {"score": 0.04139025270029003, "phrase": "mixture_models"}, {"score": 0.004816066146888616, "phrase": "inference"}, {"score": 0.004764859786310636, "phrase": "gaussian_mixtures"}, {"score": 0.0046176782475198085, "phrase": "lower-dimensional_problem"}, {"score": 0.004545793387330536, "phrase": "higher-dimensional_space"}, {"score": 0.004115697805448384, "phrase": "finite_mixture_models"}, {"score": 0.0038049789989278463, "phrase": "large_literature"}, {"score": 0.003629882559085245, "phrase": "previous_work"}, {"score": 0.0035733198242955634, "phrase": "general_theoretical_justification"}, {"score": 0.0034088473318056537, "phrase": "special_cases"}, {"score": 0.003320756562046789, "phrase": "theoretical_basis"}, {"score": 0.003234934827553253, "phrase": "mixture_multivariate_models"}, {"score": 0.0031845067058084583, "phrase": "corresponding_inference"}, {"score": 0.0031513240597392843, "phrase": "marginal_univariate_mixture_models"}, {"score": 0.0028828348726682965, "phrase": "mixture_memberships"}, {"score": 0.0028378798711397235, "phrase": "information_number"}, {"score": 0.002750056254309311, "phrase": "primary_outcome"}, {"score": 0.0027071661844502992, "phrase": "bivariate_model"}, {"score": 0.00256895975568914, "phrase": "univariate_model"}, {"score": 0.0024377918036784336, "phrase": "mis-specified_models"}, {"score": 0.002218299213266519, "phrase": "causal_inference"}, {"score": 0.0021496083700137305, "phrase": "wiley_periodicals"}, {"score": 0.002127186380459509, "phrase": "inc._statistical_analysis"}, {"score": 0.0021049977753042253, "phrase": "data_mining"}], "paper_keywords": ["bivariate", " EM", " Gaussian", " information matrix", " mixture model", " score function"], "paper_abstract": "Expanding a lower-dimensional problem to a higher-dimensional space and then projecting back is often beneficial. This article rigorously investigates this perspective in the context of finite mixture models, specifically how to improve inference for mixture models by using auxiliary variables. Despite the large literature in mixture models and several empirical examples, there is no previous work that gives general theoretical justification for including auxiliary variables in mixture models, even for special cases. We provide a theoretical basis for comparing inference for mixture multivariate models with the corresponding inference for marginal univariate mixture models. Analytical results for several special cases are established. We show that the probability of correctly allocating mixture memberships and the information number for the means of the primary outcome in a bivariate model with two Gaussian mixtures are generally larger than those in each univariate model. Simulations under a range of scenarios, including mis-specified models, are conducted to examine the improvement. The method is illustrated by two real applications in ecology and causal inference. (C) 2015 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 8: 34-48, 2015", "paper_title": "Improving Inference of Gaussian Mixtures using Auxiliary Variables", "paper_id": "WOS:000359795400003"}