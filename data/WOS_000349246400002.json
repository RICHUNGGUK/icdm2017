{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "non-smooth_optimization"}, {"score": 0.0483597876571922, "phrase": "non-smooth_optimization_problems"}, {"score": 0.032580741245018176, "phrase": "minimax_optimization_problem"}, {"score": 0.004283280680222718, "phrase": "non-smooth_functions"}, {"score": 0.0041294419262414995, "phrase": "efficient_empirical_loss_minimization"}, {"score": 0.003981106369547929, "phrase": "strongly_convex_regularizer"}, {"score": 0.0036731870109004993, "phrase": "simple_yet_efficient_method"}, {"score": 0.00336428077531883, "phrase": "loss_function"}, {"score": 0.0032671775301001513, "phrase": "primal_and_dual_variables"}, {"score": 0.0031497178007011666, "phrase": "non-smooth_optimization_problem"}, {"score": 0.002970476379751357, "phrase": "primal_dual_prox_method"}, {"score": 0.002720505061804933, "phrase": "proximal_step"}, {"score": 0.002528300562452668, "phrase": "standard_subgradient_descent_method"}, {"score": 0.002455266773537278, "phrase": "convergence_rate"}, {"score": 0.002315452777999619, "phrase": "proposed_method"}, {"score": 0.0021996478802839316, "phrase": "machine_learning"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_first_order_methods"}], "paper_keywords": ["Non-smooth optimization", " Primal dual method", " Convergence rate", " Sparsity", " Efficiency"], "paper_abstract": "We study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of assuming that the proximal step can be efficiently solved, significantly faster than a standard subgradient descent method that has an convergence rate. Our empirical studies verify the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods.", "paper_title": "An efficient primal dual prox method for non-smooth optimization", "paper_id": "WOS:000349246400002"}