{"auto_keywords": [{"score": 0.049861401740368134, "phrase": "unlabeled_data"}, {"score": 0.015719716506582538, "phrase": "researcher_homepage_classification"}, {"score": 0.013425602129174508, "phrase": "academic_homepages"}, {"score": 0.013215213310999133, "phrase": "current-day_academic_websites"}, {"score": 0.004653339900995785, "phrase": "specified_set"}, {"score": 0.00460067862022374, "phrase": "key_component"}, {"score": 0.004579780586951551, "phrase": "focused_crawling"}, {"score": 0.0044563619694339045, "phrase": "training_datasets"}, {"score": 0.004405919996432158, "phrase": "irrelevant_pages"}, {"score": 0.004209773542630883, "phrase": "researcher_homepages"}, {"score": 0.004124368285900131, "phrase": "existing_datasets"}, {"score": 0.003976775340098895, "phrase": "labeled_datasets"}, {"score": 0.003922794571908569, "phrase": "new_content"}, {"score": 0.0037395458116642306, "phrase": "academic_websites"}, {"score": 0.0036720021797570787, "phrase": "novel_url-based_features"}, {"score": 0.003613899384402803, "phrase": "content-based_features"}, {"score": 0.0035567126787402026, "phrase": "co-training_framework"}, {"score": 0.00347657813672945, "phrase": "complementary_views"}, {"score": 0.003405996613476789, "phrase": "remarkable_improvements"}, {"score": 0.0033905067962602515, "phrase": "homepage_identification"}, {"score": 0.0033216668696008224, "phrase": "novel_technique"}, {"score": 0.003284026247389735, "phrase": "conforming_pair"}, {"score": 0.0030391035690836136, "phrase": "loss_formulation"}, {"score": 0.0029169007179202164, "phrase": "validation_dataset"}, {"score": 0.002748984715897838, "phrase": "feature_selection"}, {"score": 0.002620424858132324, "phrase": "well-known_technique"}, {"score": 0.002602555367635598, "phrase": "redundant_and_unnecessary_features"}, {"score": 0.0025848074189353397, "phrase": "data_representation"}, {"score": 0.002567184775402783, "phrase": "fh"}, {"score": 0.002532284627202572, "phrase": "hash_functions"}, {"score": 0.0025207582335811207, "phrase": "efficient_encoding"}, {"score": 0.0023864566654035924, "phrase": "hashed_feature_representations"}, {"score": 0.002370178885077625, "phrase": "performance_degradation"}, {"score": 0.0022904365616462146, "phrase": "homepage_classification"}, {"score": 0.0022184273610028477, "phrase": "unlabeled_instances"}, {"score": 0.0021733331199799448, "phrase": "feature_split"}, {"score": 0.002158505888557746, "phrase": "underlying_instances"}, {"score": 0.0021049977753042253, "phrase": "single_view"}], "paper_keywords": ["Algorithms", " Researcher homepage classification", " co-training", " conforming classifiers", " unlabeled data"], "paper_abstract": "A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changing content on the Web? We investigate this question in the context of identifying researcher homepages. We show experimentally that classifiers trained on existing datasets of academic homepages underperform on \"non-homepages\" present on current-day academic websites. As an alternative to obtaining labeled datasets to retrain classifiers for the new content, in this article we ask the following question: \"How can we effectively use the unlabeled data readily available from academic websites to improve researcher homepage classification?\" We design novel URL-based features and use them in conjunction with content-based features for representing homepages. Within the co-training framework, these sets of features can be treated as complementary views enabling us to effectively use unlabeled data and obtain remarkable improvements in homepage identification on the current-day academic websites. We also propose a novel technique for \"learning a conforming pair of classifiers\" that mimics co-training. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We argue that this loss formulation provides insights for understanding co-training and can be used even in the absence of a validation dataset. Our next set of findings pertains to the evaluation of other state-of-the-art techniques for classifying homepages. First, we apply feature selection (FS) and feature hashing (FH) techniques independently and in conjunction with co-training to academic homepages. FS is a well-known technique for removing redundant and unnecessary features from the data representation, whereas FH is a technique that uses hash functions for efficient encoding of features. We show that FS can be effectively combined with co-training to obtain further improvements in identifying homepages. However, using hashed feature representations, a performance degradation is observed possibly due to feature collisions. Finally, we evaluate other semisupervised algorithms for homepage classification. We show that although several algorithms are effective in using information from the unlabeled instances, co-training that explicitly harnesses the feature split in the underlying instances outperforms approaches that combine content and URL features into a single view.", "paper_title": "Improving Researcher Homepage Classification with Unlabeled Data", "paper_id": "WOS:000363658000001"}