{"auto_keywords": [{"score": 0.04243918585673838, "phrase": "probability_distribution"}, {"score": 0.00481495049065317, "phrase": "author-topic_models"}, {"score": 0.004638462976078815, "phrase": "unsupervised_learning_technique"}, {"score": 0.004447600571127578, "phrase": "large_text_collections"}, {"score": 0.004205226397573506, "phrase": "two-stage_stochastic_process"}, {"score": 0.003672427620845944, "phrase": "multi-author_paper"}, {"score": 0.00329813918116714, "phrase": "unsupervised_manner"}, {"score": 0.0032522091171602557, "phrase": "markov_chain"}, {"score": 0.0032370412139943808, "phrase": "monte_carlo_algorithm"}, {"score": 0.0030461580449080553, "phrase": "citeseer_digital_library"}, {"score": 0.002826562521633885, "phrase": "enron_corporation"}, {"score": 0.002622755848331379, "phrase": "specific_topic"}, {"score": 0.0023223663501882896, "phrase": "unusual_papers"}, {"score": 0.002300734266971646, "phrase": "specific_authors"}, {"score": 0.0022475295254231714, "phrase": "perplexity_scores"}, {"score": 0.002226592967032469, "phrase": "test_documents"}, {"score": 0.002185301851004467, "phrase": "document_retrieval"}, {"score": 0.0021347607855562102, "phrase": "systematic_differences"}, {"score": 0.0021049977753042253, "phrase": "proposed_author-topic_model"}], "paper_keywords": ["Algorithms", " Topic models", " Gibbs sampling", " unsupervised learning", " author models", " perplexity"], "paper_abstract": "We propose an unsupervised learning technique for extracting information about authors and topics from large text collections. We model documents as if they were generated by a two-stage stochastic process. An author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words. The probability distribution over topics in a multi-author paper is a mixture of the distributions associated with the authors. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to three large text corpora: 150,000 abstracts from the CiteSeer digital library, 1740 papers from the Neural Information Processing Systems (NIPS) Conferences, and 121,000 emails from the Enron corporation. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, parsing of abstracts by topics and authors, and detection of unusual papers by specific authors. Experiments based on perplexity scores for test documents and precision-recall for document retrieval are used to illustrate systematic differences between the proposed author-topic model and a number of alternatives. Extensions to the model, allowing for example, generalizations of the notion of an author, are also briefly discussed.", "paper_title": "Learning Author-Topic Models from Text Corpora", "paper_id": "WOS:000274028700004"}