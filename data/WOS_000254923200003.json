{"auto_keywords": [{"score": 0.029637584382245584, "phrase": "uda"}, {"score": 0.010612384911619756, "phrase": "high-dimensional_data"}, {"score": 0.009116523636576177, "phrase": "pca"}, {"score": 0.004680624753255133, "phrase": "small_sample_size_problem"}, {"score": 0.004448169552315403, "phrase": "gene_expression_data_analysis"}, {"score": 0.004227209809791869, "phrase": "dimensionality_reduction"}, {"score": 0.00417961150804265, "phrase": "principal_component_analysis"}, {"score": 0.004039995317682942, "phrase": "-group_analysis"}, {"score": 0.0035065523764817143, "phrase": "pattern_recognition_perspective"}, {"score": 0.0032760377953971248, "phrase": "total_scatter_matrix"}, {"score": 0.0032208091960514128, "phrase": "linear_separability"}, {"score": 0.0031665094871183375, "phrase": "ega"}, {"score": 0.0030433302702291116, "phrase": "-class_scatter_matrix"}, {"score": 0.002941558420246247, "phrase": "class_centroids"}, {"score": 0.002827105825163379, "phrase": "automatic_nonparameter_uncorrelated_discriminant_analysis"}, {"score": 0.0027017308406240563, "phrase": "maximum_margin_criterion"}, {"score": 0.0026113524778702624, "phrase": "extracted_features"}, {"score": 0.0024814079299143536, "phrase": "rank-preserving_dimensionality_reduction"}, {"score": 0.0023445770108033288, "phrase": "effective_solution"}, {"score": 0.002305014991156598, "phrase": "small-sample-size_problem"}, {"score": 0.002215274527909304, "phrase": "gene_expression_data_sets"}, {"score": 0.0021049977753042253, "phrase": "classification_accuracy"}], "paper_keywords": ["feature extraction", " feature interpretability", " small sample size problem", " face recognition", " microarray data classification"], "paper_abstract": "High-dimensional data and the small sample size problem occur in many modern pattern classification applications such as face recognition and gene expression data analysis. To deal with such data, one important step is dimensionality reduction. Principal component analysis (PCA) and between-group analysis (BGA) are two commonly used methods, and various extensions of these two methods exist. The principle of these two approaches comes from their best approximation property. From a pattern recognition perspective, we show that PCA, which is based on the total scatter matrix, preserves linear separability, and EGA, which is based on between-class scatter matrix, retains the distance between class centroids. Moreover, we propose an automatic nonparameter uncorrelated discriminant analysis (UDA) algorithm based on the maximum margin criterion (MMC). The extracted features via UDA are statistically uncorrelated. UDA combines rank-preserving dimensionality reduction and constraint discriminant analysis and also serves as an effective solution for the small-sample-size problem. Experiments with face images and gene expression data sets are conducted to evaluate UDA in terms of classification accuracy and robustness.", "paper_title": "Feature extraction and uncorrelated discriminant analysis for high-dimensional data", "paper_id": "WOS:000254923200003"}