{"auto_keywords": [{"score": 0.05007737955022614, "phrase": "extreme_learning_machine"}, {"score": 0.04594191530665986, "phrase": "elm"}, {"score": 0.004579780586951551, "phrase": "new_learning_algorithm"}, {"score": 0.004511485198940203, "phrase": "single-hidden-layer_feedforward_neural_networks"}, {"score": 0.004248294537944227, "phrase": "high_efficiency"}, {"score": 0.004101886428528311, "phrase": "random_determination"}, {"score": 0.004020492138367616, "phrase": "hidden_nodes"}, {"score": 0.00396050388346298, "phrase": "un-optimal_parameters"}, {"score": 0.0038239757564863057, "phrase": "generalization_performance"}, {"score": 0.0036370297611938796, "phrase": "overtraining_problem"}, {"score": 0.0035827424231607784, "phrase": "entire_training_dataset"}, {"score": 0.0034940519373446335, "phrase": "training_error"}, {"score": 0.0033735490732324713, "phrase": "hybrid_model"}, {"score": 0.003257191435048992, "phrase": "elm."}, {"score": 0.0031925019616678217, "phrase": "genetic_algorithms"}, {"score": 0.003051601113971322, "phrase": "candidate_networks"}, {"score": 0.0029463123778313196, "phrase": "specific_ranking_strategy"}, {"score": 0.0027881294891497115, "phrase": "new_network"}, {"score": 0.002665027876903161, "phrase": "empirical_comparisons"}, {"score": 0.0025859868507329634, "phrase": "canonical_elm"}, {"score": 0.0024717881264735477, "phrase": "ee-elm"}, {"score": 0.002447101794397243, "phrase": "en-elm"}, {"score": 0.002422667543394016, "phrase": "bagging"}, {"score": 0.002398492749583162, "phrase": "adaboost"}, {"score": 0.002327310963701377, "phrase": "classification_problems"}, {"score": 0.0021693692831601745, "phrase": "better_generalization_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme learning machine", " Hybrid model", " Genetic algorithm", " Neural network ensemble"], "paper_abstract": "Extreme learning machine (ELM) was proposed as a new learning algorithm to train single-hidden-layer feedforward neural networks (SLFNs). ELM has been proven to perform in high efficiency, however, due to the random determination of parameters for hidden nodes, some un-optimal parameters may be generated to influence the generalization performance and stability. Moreover, ELM may suffer from overtraining problem as the entire training dataset is used to minimize training error. In this paper, a hybrid model is proposed to alleviate such weaknesses of ELM. The model adopts genetic algorithms (GAs) to produce a group of candidate networks first, and according to a specific ranking strategy, some of the networks are selected to ensemble a new network. To verify the performance of our method, empirical comparisons were carried out with the canonical ELM, E-ELM, simple ensemble, EE-ELM, EN-ELM, Bagging and Adaboost to solve both regression and classification problems. The results have shown that our method is able to generate more robust networks with better generalization performance. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Genetic ensemble of extreme learning machine", "paper_id": "WOS:000332132400022"}