{"auto_keywords": [{"score": 0.04967088836874542, "phrase": "sequential_updates"}, {"score": 0.021995548043618647, "phrase": "amcc_algorithms"}, {"score": 0.00481495049065317, "phrase": "distributed_framework_for_data_co-clustering"}, {"score": 0.004613175917207636, "phrase": "powerful_data_mining_tool"}, {"score": 0.004573842508130535, "phrase": "two-dimensional_co-occurrence"}, {"score": 0.004516593648884523, "phrase": "co-clusterd"}, {"score": 0.004419819325202464, "phrase": "-clustering_algorithms"}, {"score": 0.004363401436068803, "phrase": "significant_computational_resources"}, {"score": 0.004216434794603139, "phrase": "large_data_sets"}, {"score": 0.00418046991987597, "phrase": "existing_studies"}, {"score": 0.004127094600862448, "phrase": "strong_empirical_evidence"}, {"score": 0.0037719988989298983, "phrase": "computational_cost"}, {"score": 0.0037078961288626185, "phrase": "resulting_solution"}, {"score": 0.003537155405923517, "phrase": "alternate_minimization_co-clustering"}, {"score": 0.0034033029171804106, "phrase": "em_algorithms"}, {"score": 0.0030969799559421806, "phrase": "distributed_environment"}, {"score": 0.0029797347088361056, "phrase": "convergence_properties"}, {"score": 0.0028302635493019867, "phrase": "new_distributed_framework"}, {"score": 0.0027583557819362034, "phrase": "efficient_implementations"}, {"score": 0.002477837342313456, "phrase": "fnmtf"}, {"score": 0.002254624822004988, "phrase": "empirical_results"}, {"score": 0.0021231452870210965, "phrase": "better_results"}], "paper_keywords": ["Co-Clustering", " concurrent updates", " sequential updates", " cloud computing", " distributed framework"], "paper_abstract": "Co-clustering has emerged to be a powerful data mining tool for two-dimensional co-occurrence and dyadic data. However, co-clustering algorithms often require significant computational resources and have been dismissed as impractical for large data sets. Existing studies have provided strong empirical evidence that expectation-maximization (EM) algorithms (e.g., k-means algorithm) with sequential updates can significantly reduce the computational cost without degrading the resulting solution. Motivated by this observation, we introduce sequential updates for alternate minimization co-clustering (AMCC) algorithms which are variants of EM algorithms, and also show that AMCC algorithms with sequential updates converge. We then propose two approaches to parallelize AMCC algorithms with sequential updates in a distributed environment. Both approaches are proved to maintain the convergence properties of AMCC algorithms. Based on these two approaches, we present a new distributed framework, Co-ClusterD, which supports efficient implementations of AMCC algorithms with sequential updates. We design and implement Co-ClusterD, and show its efficiency through two AMCC algorithms: fast nonnegative matrix tri-factorization (FNMTF) and information theoretic co-clustering (ITCC). We evaluate our framework on both a local cluster of machines and the Amazon EC2 cloud. Empirical results show that AMCC algorithms implemented in Co-ClusterD can achieve a much faster convergence and often obtain better results than their traditional concurrent counterparts.", "paper_title": "Co-ClusterD: A Distributed Framework for Data Co-Clustering with Sequential Updates", "paper_id": "WOS:000364853800007"}