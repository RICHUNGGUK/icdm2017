{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "orthogonal_projection"}, {"score": 0.04774327392954541, "phrase": "theoretical_aspects"}, {"score": 0.03142732938144533, "phrase": "within-class_axes"}, {"score": 0.02869632570078602, "phrase": "class_separability"}, {"score": 0.004549821572786755, "phrase": "dimension_reduction"}, {"score": 0.004474645041899596, "phrase": "supervised_classification"}, {"score": 0.004299228562179176, "phrase": "different_subspaces"}, {"score": 0.003916098498242888, "phrase": "novel_method"}, {"score": 0.0038900709793274484, "phrase": "supervised_dimension_reduction"}, {"score": 0.0038257535874187953, "phrase": "discrimination_purposes"}, {"score": 0.003543273649210823, "phrase": "high_dimensionality"}, {"score": 0.0035197152758635344, "phrase": "high_intercorrelation"}, {"score": 0.0034963129868796033, "phrase": "spectral_variables"}, {"score": 0.003449972561293443, "phrase": "fisher_discriminant_analysis"}, {"score": 0.003370344851760079, "phrase": "lower_dimensional_subspace"}, {"score": 0.003195149953590668, "phrase": "observation_matrix"}, {"score": 0.003173898798475791, "phrase": "variability_sources"}, {"score": 0.0031006232333900055, "phrase": "classification_task"}, {"score": 0.0029296136066184857, "phrase": "good_class_separability"}, {"score": 0.0028715299874549245, "phrase": "between-class_axes"}, {"score": 0.002686085475479125, "phrase": "drop-d_discriminant_axes"}, {"score": 0.0025978919678702793, "phrase": "simplified_interpretability"}, {"score": 0.002572001496778222, "phrase": "main_advantage"}, {"score": 0.0024792523879679836, "phrase": "unnecessary_information"}, {"score": 0.0023739321566540682, "phrase": "model_parameters"}, {"score": 0.0023346234464901978, "phrase": "modeling_techniques"}, {"score": 0.00216923754570397, "phrase": "similar_performances"}, {"score": 0.0021476098263617954, "phrase": "usual_linear_classification_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Discrimination", " Dimension reduction", " Orthogonal projection", " Supervised classification", " Spectral discrimination", " Discriminant analysis"], "paper_abstract": "The objective of this paper is two-fold. First, some theoretical aspects of dimension reduction in the context of supervised classification or discrimination are given. The emphasis is put on the different subspaces that can be defined in this context and what information is contained in each of them. Then, based on these theoretical aspects, we propose a novel method for supervised dimension reduction that is dedicated to discrimination purposes. The method, called Dimension Reduction by Orthogonal Projection for Discrimination (DROP-D) is particularly well suited to the high dimensionality and high intercorrelation of spectral variables. As with Fisher discriminant analysis, DROP-D aims at finding a lower dimensional subspace in which the classes are well separated. To do so, DROP-D cleans the observation matrix of variability sources that do not help with the classification task. For this purpose, the matrix is projected orthogonally to the within-class axes which prevent a good class separability. In cases where some between-class axes are collinear with the within-class axes, DROP-D can preserve these axes in order not to destroy the class separability. DROP-D discriminant axes are orthogonal to one another and thus offer a simplified interpretability. The main advantage of DROP-D is that because it is based on removing unnecessary information, there is no need of a validation set to tune the model parameters. In contrast to modeling techniques, DROP-D thus cannot find class separability when there is none. In terms of results, DROP-D offers similar performances to the usual linear classification methods. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "DROP-D: Dimension reduction by orthogonal projection for discrimination", "paper_id": "WOS:000360595100024"}