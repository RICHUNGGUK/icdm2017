{"auto_keywords": [{"score": 0.025510526242145122, "phrase": "ball"}, {"score": 0.00481495049065317, "phrase": "discrete_versions"}, {"score": 0.0044991925146184025, "phrase": "independent_discrete_random_variables"}, {"score": 0.004362564831572539, "phrase": "shannon's_entropy_power_inequality"}, {"score": 0.004296305122471807, "phrase": "epi"}, {"score": 0.004076352180903171, "phrase": "infinite_divisibility"}, {"score": 0.0039040344230094164, "phrase": "poisson_variables"}, {"score": 0.0037621210719690594, "phrase": "natural_analogues"}, {"score": 0.0034720284451902083, "phrase": "alternative_formulation"}, {"score": 0.003244050414264445, "phrase": "shannon's_epi"}, {"score": 0.0030686679245324837, "phrase": "continuous_random_variables"}, {"score": 0.0029753513692467315, "phrase": "renyi's_operation"}, {"score": 0.0029207251108325006, "phrase": "discrete_random_variables"}, {"score": 0.0028670988845838296, "phrase": "similar_role"}, {"score": 0.002629539710839523, "phrase": "ultra_log-concave_random_variables"}, {"score": 0.002557886563575656, "phrase": "naor"}, {"score": 0.002471973283582411, "phrase": "monotonicity_results"}, {"score": 0.002459471292146297, "phrase": "artstein"}, {"score": 0.002403462226208204, "phrase": "barthe"}, {"score": 0.0022531089868518235, "phrase": "stronger_version"}, {"score": 0.0021311850321045767, "phrase": "strengthened_form"}], "paper_keywords": ["Convolution", " discrete random variables", " entropy", " entropy power inequality (EPI)", " monotonicity", " Poisson distribution", " thinning"], "paper_abstract": "We consider the entropy of sums of independent discrete random variables, in analogy with Shannon's Entropy Power Inequality, where equality holds for normals. In our case, infinite divisibility suggests that equality should hold for Poisson variables. We show that some natural analogues of the EPI do not in fact hold, but propose an alternative formulation which does always hold. The key to many proofs of Shannon's EPI is the behavior of entropy on scaling of continuous random variables. We believe that Renyi's operation of thinning discrete random variables plays a similar role to scaling, and give a sharp bound on how the entropy of ultra log-concave random variables behaves on thinning. In the spirit of the monotonicity results established by Artstein, Ball, Barthe, and Naor, we prove a stronger version of concavity of entropy, which implies a strengthened form of our discrete EPI.", "paper_title": "Monotonicity, Thinning, and Discrete Versions of the Entropy Power Inequality", "paper_id": "WOS:000283449000002"}