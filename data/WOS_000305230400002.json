{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "label_dependence"}, {"score": 0.010433069276475918, "phrase": "multi-label_classification"}, {"score": 0.004344292058151148, "phrase": "class_labels"}, {"score": 0.004285101149043392, "phrase": "simple_binary_relevance_learning"}, {"score": 0.003800271653332246, "phrase": "blanket_explanation"}, {"score": 0.0036469533983843755, "phrase": "underlying_mechanisms"}, {"score": 0.0036220063770882275, "phrase": "true_reasons"}, {"score": 0.0035481809241908044, "phrase": "experimental_studies"}, {"score": 0.0031790442889277864, "phrase": "better_understanding"}, {"score": 0.0031249312057114237, "phrase": "statistical_perspective"}, {"score": 0.0027520299821330125, "phrase": "predictive_performance"}, {"score": 0.00265909739882579, "phrase": "close_connection"}, {"score": 0.002640889905429026, "phrase": "loss_minimization"}, {"score": 0.002440231768165477, "phrase": "concrete_theoretical_results"}, {"score": 0.002374081290112298, "phrase": "hamming"}, {"score": 0.002247049536955399, "phrase": "state-of-the-art_decomposition_algorithms"}, {"score": 0.0022316614434078567, "phrase": "mlc"}, {"score": 0.0021195184368629017, "phrase": "carefully_designed_experiments"}, {"score": 0.0021049977753042253, "phrase": "synthetic_and_benchmark_data"}], "paper_keywords": ["Multi-label classification", " Label dependence", " Loss functions"], "paper_abstract": "Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Comparing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than proposing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the predictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two representative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to reveal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.", "paper_title": "On label dependence and loss minimization in multi-label classification", "paper_id": "WOS:000305230400002"}