{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mahalanobis_distance"}, {"score": 0.049324540543845156, "phrase": "density_kernels"}, {"score": 0.041394261792965556, "phrase": "gaussian_distribution"}, {"score": 0.0037634608487353153, "phrase": "covariance_matrix"}, {"score": 0.003559675269411366, "phrase": "data_point"}, {"score": 0.0031592690527974285, "phrase": "general_unimodal_distributions"}, {"score": 0.0030602966787192745, "phrase": "particular_class"}, {"score": 0.0030119758165837625, "phrase": "mercer_kernel"}, {"score": 0.0028037753646260937, "phrase": "underlying_data_density"}, {"score": 0.0022977342663599042, "phrase": "theoretical_properties"}, {"score": 0.0022434880661725493, "phrase": "proposed_distance"}, {"score": 0.0021049977753042253, "phrase": "artificial_and_real_data_analysis_problems"}], "paper_keywords": ["Mahalanobis distance", " density kernel", " level sets", " outlier detection", " classification"], "paper_abstract": "The Mahalanobis distance ( MD) is a distance widely used in Statistics, Machine Learning and Pattern Recognition. When the data come from a Gaussian distribution, the MD uses the covariance matrix to evaluate the distance between a data point and the distribution mean. In this paper, we generalize the MD for general unimodal distributions, introducing a particular class of Mercer kernel, the density kernel, based on the underlying data density. Density kernels induce distances that generalize the MD and that are useful when data do not fit to the Gaussian distribution. We study the theoretical properties of the proposed distance and show its performance on a variety of artificial and real data analysis problems.", "paper_title": "Generalizing the Mahalanobis distance via density kernels", "paper_id": "WOS:000347782000003"}