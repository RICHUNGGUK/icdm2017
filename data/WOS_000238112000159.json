{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaussian"}, {"score": 0.0046993598473345395, "phrase": "backward_elimination"}, {"score": 0.0046238380150870435, "phrase": "gaussian_processes"}, {"score": 0.004333642054036135, "phrase": "art_performance"}, {"score": 0.003685067218455088, "phrase": "support_vector_machines"}, {"score": 0.002936331371903168, "phrase": "backward_elimination_algorithm"}, {"score": 0.002707444334463544, "phrase": "basis_functions"}, {"score": 0.002599763359352916, "phrase": "stop_criterion"}, {"score": 0.0022830706430588482, "phrase": "reasonable_cost"}, {"score": 0.002246292543825748, "phrase": "extensive_empirical_comparisons"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithm"}], "paper_keywords": [""], "paper_abstract": "Gaussian Processes (GPs) have state of the art performance in regression. In GPs, all the basis functions are required for prediction: hence its test speed is slower than other learning algorithms Such as support vector machines (SVMs), relevance vector machine (RVM), adaptive sparseness (AS), etc, To overcome this limitation, we present a backward elimination algorithm, called GPs-BE that recursively selects the basis functions for GPs until some stop criterion is satisfied. By integrating rank-1 update, GPs-BE can be implemented at a reasonable cost. Extensive empirical comparisons confirm the feasibility and validity of the proposed algorithm.", "paper_title": "Sparse Gaussian processes using backward elimination", "paper_id": "WOS:000238112000159"}