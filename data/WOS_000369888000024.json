{"auto_keywords": [{"score": 0.045840244445644815, "phrase": "svd"}, {"score": 0.00481495049065317, "phrase": "spectral_learning_algorithms"}, {"score": 0.004650896226390719, "phrase": "data-rich_domains"}, {"score": 0.004492406361795448, "phrase": "recent_advances"}, {"score": 0.004440780432053927, "phrase": "large_scale"}, {"score": 0.004289419884432487, "phrase": "spectral_estimation"}, {"score": 0.004240116703465252, "phrase": "hidden_markov_models"}, {"score": 0.004048483412089098, "phrase": "statistical_estimation_algorithms"}, {"score": 0.003777088546146369, "phrase": "real_data_sets"}, {"score": 0.0033644530302903513, "phrase": "embeddings_low_dimensional_real_vectors"}, {"score": 0.0030492472433481764, "phrase": "multi-view_nature"}, {"score": 0.0029794666460666646, "phrase": "i.e._the_left_and_right_context"}, {"score": 0.002795671770303977, "phrase": "strong_theoretical_properties"}, {"score": 0.0026691455928407022, "phrase": "lower_sample_complexity"}, {"score": 0.002418923027024191, "phrase": "optimality_criteria"}, {"score": 0.0022828113789751694, "phrase": "thorough_qualitative_and_quantitative_evaluation"}, {"score": 0.0022565201625076876, "phrase": "eigenwords"}, {"score": 0.0022176487309947266, "phrase": "simple_linear_approaches"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_non-linear_deep_learning_based_methods"}], "paper_keywords": ["spectral learning", " CCA", " word embeddings", " NLP"], "paper_abstract": "Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings low dimensional real vectors (called Eigenwords) that capture the \"meaning\" of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods.", "paper_title": "Eigenwords: Spectral Word Embeddings", "paper_id": "WOS:000369888000024"}