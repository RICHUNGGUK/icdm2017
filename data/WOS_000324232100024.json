{"auto_keywords": [{"score": 0.039853922732353365, "phrase": "high_probability"}, {"score": 0.00481495049065317, "phrase": "low-complexity_models"}, {"score": 0.00462773520532301, "phrase": "unified_analysis"}, {"score": 0.004477270137801732, "phrase": "simple_objects"}, {"score": 0.004418456827500267, "phrase": "random_linear_measurements"}, {"score": 0.004303127794432684, "phrase": "linear_functionals"}, {"score": 0.004249749523307799, "phrase": "gaussian"}, {"score": 0.004081385289516914, "phrase": "s-sparse_vector_in"}, {"score": 0.0037203741725514126, "phrase": "rank_r"}, {"score": 0.003623199939953831, "phrase": "n_matrix"}, {"score": 0.0033245147666408157, "phrase": "sparse_vectors"}, {"score": 0.0031950646670695546, "phrase": "additive_factor"}, {"score": 0.003132236287543072, "phrase": "best_known_nonasymptotic_bounds"}, {"score": 0.0030706395717772436, "phrase": "low-rank_matrices"}, {"score": 0.0029706504420983896, "phrase": "best_known_bounds"}, {"score": 0.0028739078688746374, "phrase": "parallel_analysis"}, {"score": 0.0028360962841331634, "phrase": "block-sparse_vectors"}, {"score": 0.002798780784517325, "phrase": "similarly_tight_bounds"}, {"score": 0.002689746491842271, "phrase": "sparse_and_block-sparse_signals"}, {"score": 0.0024678216843278806, "phrase": "measurement_map"}, {"score": 0.0024192592458651204, "phrase": "random_sign_matrix"}, {"score": 0.002294369814584028, "phrase": "particular_dual_point"}, {"score": 0.0022492129111567824, "phrase": "optimality_conditions"}, {"score": 0.0022049428043454966, "phrase": "respective_convex_programming_problem"}, {"score": 0.0021049977753042253, "phrase": "standard_large_deviation_inequalities"}], "paper_keywords": ["l(1)-norm minimization", " Nuclear-norm minimization", " Block-sparsity", " Duality", " Random matrices"], "paper_abstract": "This note presents a unified analysis of the recovery of simple objects from random linear measurements. When the linear functionals are Gaussian, we show that an s-sparse vector in can be efficiently recovered from 2s log n measurements with high probability and a rank r, n x n matrix can be efficiently recovered from r(6n - 5r) measurements with high probability. For sparse vectors, this is within an additive factor of the best known nonasymptotic bounds. For low-rank matrices, this matches the best known bounds. We present a parallel analysis for block-sparse vectors obtaining similarly tight bounds. In the case of sparse and block-sparse signals, we additionally demonstrate that our bounds are only slightly weakened when the measurement map is a random sign matrix. Our results are based on analyzing a particular dual point which certifies optimality conditions of the respective convex programming problem. Our calculations rely only on standard large deviation inequalities and our analysis is self-contained.", "paper_title": "Simple bounds for recovering low-complexity models", "paper_id": "WOS:000324232100024"}