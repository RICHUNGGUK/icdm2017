{"auto_keywords": [{"score": 0.04343431379804459, "phrase": "arthur"}, {"score": 0.04321661943692001, "phrase": "vassilvitskii"}, {"score": 0.04267840511063453, "phrase": "eighteenth_annual_acm-siam_symposium"}, {"score": 0.04151933694515415, "phrase": "siam"}, {"score": 0.041309053176600954, "phrase": "philadelphia"}, {"score": 0.010342806548883086, "phrase": "proceedings"}, {"score": 0.00481495049065317, "phrase": "k-means_clustering_problem"}, {"score": 0.00472790536079734, "phrase": "k_centers"}, {"score": 0.004666690845682694, "phrase": "objective_function"}, {"score": 0.00449945511326728, "phrase": "euclidean_distance"}, {"score": 0.00444118525702523, "phrase": "closest_center"}, {"score": 0.0038779784043294864, "phrase": "simple_non-uniform_sampling_technique"}, {"score": 0.0036616707006429483, "phrase": "first_point"}, {"score": 0.003595399348262267, "phrase": "p._subsequently"}, {"score": 0.0035027939300676875, "phrase": "next_sample"}, {"score": 0.0033683237474307582, "phrase": "nearest_previously_sampled_point"}, {"score": 0.0032815477724070534, "phrase": "nice_properties"}, {"score": 0.0029638947417413705, "phrase": "k_points"}, {"score": 0.0027911350537669294, "phrase": "nips"}, {"score": 0.0027405761006877747, "phrase": "aggarwal"}, {"score": 0.0026699256851060033, "phrase": "combinatorial_optimization"}, {"score": 0.002587560023409154, "phrase": "berlin"}, {"score": 0.0022707062264635653, "phrase": "k-means_objective_function"}], "paper_keywords": ["k-Means clustering", " PTAS", " Sampling", " k-Median"], "paper_abstract": "Given a set of points , the k-means clustering problem is to find a set of k centers , such that the objective function a (xaP) e(x,C)(2), where e(x,C) denotes the Euclidean distance between x and the closest center in C, is minimized. This is one of the most prominent objective functions that has been studied with respect to clustering. D (2)-sampling (Arthur and Vassilvitskii, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA'07, pp. 1027-1035, SIAM, Philadelphia, 2007) is a simple non-uniform sampling technique for choosing points from a set of points. It works as follows: given a set of points , the first point is chosen uniformly at random from P. Subsequently, a point from P is chosen as the next sample with probability proportional to the square of the distance of this point to the nearest previously sampled point. D (2)-sampling has been shown to have nice properties with respect to the k-means clustering problem. Arthur and Vassilvitskii (Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA'07, pp. 1027-1035, SIAM, Philadelphia, 2007) show that k points chosen as centers from P using D (2)-sampling give an O(logk) approximation in expectation. Ailon et al. (NIPS, pp. 10-18, 2009) and Aggarwal et al. (Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 15-28, Springer, Berlin, 2009) extended results of Arthur and Vassilvitskii (Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA'07, pp. 1027-1035, SIAM, Philadelphia, 2007) to show that O(k) points chosen as centers using D (2)-sampling give an O(1) approximation to the k-means objective function with high probability. In this paper, we further demonstrate the power of D (2)-sampling by giving a simple randomized (1+I mu)-approximation algorithm that uses the D (2)-sampling in its core.", "paper_title": "A Simple D (2)-Sampling Based PTAS for k-Means and Other Clustering Problems", "paper_id": "WOS:000339341900003"}