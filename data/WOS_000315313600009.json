{"auto_keywords": [{"score": 0.04937176974353987, "phrase": "minimum_number"}, {"score": 0.045326042302844864, "phrase": "ensemble_prediction"}, {"score": 0.00481495049065317, "phrase": "parallel_ensemble"}, {"score": 0.0046046429665581555, "phrase": "stable_aggregate_predictions"}, {"score": 0.004536592536813498, "phrase": "majority_voting"}, {"score": 0.0041488110506566825, "phrase": "voting_process"}, {"score": 0.00389427992231721, "phrase": "small_but_non-negligible_fraction"}, {"score": 0.0038224221344037236, "phrase": "large_numbers"}, {"score": 0.0037940502405699765, "phrase": "classifier_queries"}, {"score": 0.0037518852820756323, "phrase": "stable_predictions"}, {"score": 0.0034824716857663114, "phrase": "universal_form"}, {"score": 0.0033801845720347118, "phrase": "ensemble_size"}, {"score": 0.0032083351151314405, "phrase": "infinite_ensemble_prediction"}, {"score": 0.0031726587374218277, "phrase": "average_confidence_level_alpha"}, {"score": 0.0030565692640988585, "phrase": "previous_proposals"}, {"score": 0.0029228387900175554, "phrase": "prediction_error"}, {"score": 0.002722930209707764, "phrase": "generalization_performance"}, {"score": 0.002603758927691463, "phrase": "general_validity"}, {"score": 0.0025272158996086378, "phrase": "statistical_description"}, {"score": 0.00241658945942583, "phrase": "representative_parallel_ensembles"}, {"score": 0.0023107943665108465, "phrase": "proposed_framework"}, {"score": 0.002285075675790214, "phrase": "wide_range"}, {"score": 0.002268088741853586, "phrase": "classification_problems"}, {"score": 0.0022096205841489786, "phrase": "optimal_ensemble_size"}, {"score": 0.0021607035327962246, "phrase": "particular_classification_problem"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Ensemble learning", " Bagging", " Random forest", " Asymptotic ensemble prediction", " Ensemble size"], "paper_abstract": "We propose to determine the size of a parallel ensemble by estimating the minimum number of classifiers that are required to obtain stable aggregate predictions. Assuming that majority voting is used, a statistical description of the convergence of the ensemble prediction to its asymptotic (infinite size) limit is given. The analysis of the voting process shows that for most test instances the ensemble prediction stabilizes after only a few classifiers are polled. By contrast, a small but non-negligible fraction of these instances require large numbers of classifier queries to reach stable predictions. Specifically, the fraction of instances whose stable predictions require more than T classifiers for T >> 1 has a universal form and is proportional to T-1/2. The ensemble size is determined as the minimum number of classifiers that are needed to estimate the infinite ensemble prediction at an average confidence level alpha, close to one. This approach differs from previous proposals, which are based on determining the size for which the prediction error (not the predictions themselves) stabilizes. In particular, it does not require estimates of the generalization performance of the ensemble, which can be unreliable. It has general validity because it is based solely on the statistical description of the convergence of majority voting to its asymptotic limit Extensive experiments using representative parallel ensembles (bagging and random forest) illustrate the application of the proposed framework in a wide range of classification problems. These experiments show that the optimal ensemble size is very sensitive to the particular classification problem considered. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "How large should ensembles of classifiers be?", "paper_id": "WOS:000315313600009"}