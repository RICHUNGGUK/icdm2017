{"auto_keywords": [{"score": 0.040217766178210615, "phrase": "gpu_resources"}, {"score": 0.0052410734412402255, "phrase": "performance_improvement"}, {"score": 0.00488531846270484, "phrase": "default_concurrent_multitasking"}, {"score": 0.00481495049065317, "phrase": "efficient_gpu_spatial-temporal_multitasking"}, {"score": 0.004772818068176176, "phrase": "heterogeneous_computing_nodes"}, {"score": 0.004507760907425495, "phrase": "leading_computing_device"}, {"score": 0.004468304427278713, "phrase": "application_acceleration"}, {"score": 0.00439041997682389, "phrase": "tremendous_computing_potential"}, {"score": 0.0043519860930099795, "phrase": "data-parallel_applications"}, {"score": 0.004128317208406523, "phrase": "gpu-accelerated_applications"}, {"score": 0.0036499505265403377, "phrase": "system_performance"}, {"score": 0.0036179751816682454, "phrase": "prior_techniques"}, {"score": 0.003586278948128509, "phrase": "temporal_multitasking"}, {"score": 0.003226834737003501, "phrase": "unmet_need"}, {"score": 0.003198554193075258, "phrase": "spatial_multitasking"}, {"score": 0.003170602770810742, "phrase": "gpus"}, {"score": 0.002802825862847951, "phrase": "software-hardware_solution"}, {"score": 0.0027782510491938315, "phrase": "efficient_spatial-temporal_multitasking"}, {"score": 0.0027417911324611917, "phrase": "software_based_emulation_framework"}, {"score": 0.002646880721119838, "phrase": "efficient_heuristic"}, {"score": 0.002600661525224505, "phrase": "hardware_leaky-bucket_based_thread-block_interleaving"}, {"score": 0.0024451427672857458, "phrase": "nine_representative_benchmarks"}, {"score": 0.0024236964502593254, "phrase": "cuda_sdk."}, {"score": 0.002288797099841088, "phrase": "sequential_gpu_task_execution"}, {"score": 0.0021330078238529443, "phrase": "hyper-q"}], "paper_keywords": ["GPU", " spatial", " temporal", " multitasking", " resource allocation"], "paper_abstract": "Heterogeneous computing nodes are now pervasive throughout computing, and GPUs have emerged as a leading computing device for application acceleration. GPUs have tremendous computing potential for data-parallel applications, and the emergence of GPUs has led to proliferation of GPU-accelerated applications. This proliferation has also led to systems in which many applications are competing for access to GPU resources, and efficient utilization of the GPU resources is critical to system performance. Prior techniques of temporal multitasking can be employed with GPU resources as well, but not all GPU kernels make full use of the GPU resources. There is, therefore, an unmet need for spatial multitasking in GPUs. Resources used inefficiently by one kernel can be instead assigned to another kernel that can more effectively use the resources. In this paper we propose a software-hardware solution for efficient spatial-temporal multitasking and a software based emulation framework for our system. We pair an efficient heuristic in software with hardware leaky-bucket based thread-block interleaving to implement spatial-temporal multitasking. We demonstrate our techniques on various GPU architecture using nine representative benchmarks from CUDA SDK. Our experiments on Fermi GTX480 demonstrate performance improvement by up to 46% (average 26%) over sequential GPU task execution and 37% (average 18%) over default concurrent multitasking. Compared with the state-of-the-art Kepler K20 using Hyper-Q technology, our technique achieves up to 40% (average 17%) performance improvement over default concurrent multitasking.", "paper_title": "Efficient GPU Spatial-Temporal Multitasking", "paper_id": "WOS:000349769500012"}