{"auto_keywords": [{"score": 0.04771060760127828, "phrase": "cgs"}, {"score": 0.00481495049065317, "phrase": "nu-support_vector_classifier"}, {"score": 0.004742290714667451, "phrase": "conditional_value-at-risk_minimization"}, {"score": 0.004623608221548592, "phrase": "conditional_geometric_score"}, {"score": 0.004485087674770878, "phrase": "gotoh"}, {"score": 0.0044398463723063905, "phrase": "takeda"}, {"score": 0.004395040468458808, "phrase": "binary_linear_classification"}, {"score": 0.004328689762198895, "phrase": "nonlinear_one"}, {"score": 0.003587924960158317, "phrase": "extended_nu-svc"}, {"score": 0.003551694229680741, "phrase": "perez-cruz_et_al"}, {"score": 0.0034277338119963886, "phrase": "convex_case"}, {"score": 0.003308085469148074, "phrase": "scholkopf_et_a."}, {"score": 0.0032746712603464235, "phrase": "cgs_problem"}, {"score": 0.0031926001871260524, "phrase": "risk_measure"}, {"score": 0.0031284221314354095, "phrase": "conditional_value"}, {"score": 0.0028697077879305064, "phrase": "theoretical_aspects"}, {"score": 0.0026863605504566924, "phrase": "generalization_error"}, {"score": 0.002592523232809191, "phrase": "related_quantity"}, {"score": 0.0024516267507109753, "phrase": "small_generalization_error"}, {"score": 0.0023539789234610763, "phrase": "cvar_minimization"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Support vector machines", " Conditional value-at-risk", " Nonconvex quadratic programming", " Extended nu-SVM"], "paper_abstract": "We extend the conditional geometric score (CGS) classifier of Gotoh and Takeda for binary linear classification to a nonlinear one, which we call the beta-support vector classifier (SVC), and investigate the equivalence between the beta-SVC and the (extended) nu-SVC. The CGS classifier has recently been found to be equivalent to the extended nu-SVC of Perez-Cruz et al. and, especially in the convex case, equivalent to the nu-SVC of Scholkopf et A. The CGS problem is to minimize a risk measure known as the conditional value-at-risk (beta-CVaR). In this paper, we discuss theoretical aspects, mainly generalization performance, of the beta-SVC. The formula of a generalization error bound includes the beta-CVaR or a related quantity. It implies that the minimum beta-CVaR leads to a small generalization error bound of the beta-SVC. The viewpoint of CVaR minimization is useful to ensure the validity of not only the beta-SVC but also the (extended) nu-SVC. (c) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Generalization performance of nu-support vector classifier based on conditional value-at-risk minimization", "paper_id": "WOS:000266702300032"}