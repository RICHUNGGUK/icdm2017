{"auto_keywords": [{"score": 0.014701229488814188, "phrase": "ram"}, {"score": 0.012998049648562592, "phrase": "c_columns"}, {"score": 0.012901369064474947, "phrase": "r_rows"}, {"score": 0.012475017139470239, "phrase": "appropriate_rescaling"}, {"score": 0.010386704325494208, "phrase": "high_probability"}, {"score": 0.00848435203396866, "phrase": "additional_error"}, {"score": 0.00481495049065317, "phrase": "compressed_approximate_matrix_decomposition"}, {"score": 0.004760888615472564, "phrase": "data_consist"}, {"score": 0.004516480535797289, "phrase": "random_access_memory"}, {"score": 0.004448972600077131, "phrase": "superlinear_polynomial_time_computations"}, {"score": 0.004341408235880453, "phrase": "x_n_matrix_a"}, {"score": 0.004087538889284099, "phrase": "computed_approximate_decomposition"}, {"score": 0.004049229937161565, "phrase": "provable_bounds"}, {"score": 0.0040264164218713795, "phrase": "error_matrix_a_-_a"}, {"score": 0.0039886780232697245, "phrase": "first_algorithm"}, {"score": 0.0037837619335450486, "phrase": "r_x"}, {"score": 0.003769532926050615, "phrase": "matrix_r"}, {"score": 0.003664500915101391, "phrase": "c_x_r_matrix_u"}, {"score": 0.003596104628864551, "phrase": "matrix_x"}, {"score": 0.0033984546216153, "phrase": "-_a'parallel"}, {"score": 0.003063793462957608, "phrase": "appropriate_choice"}, {"score": 0.002889861520920849, "phrase": "matrix_a"}, {"score": 0.0028145733048266113, "phrase": "external_memory"}, {"score": 0.0026951083937372614, "phrase": "second_algorithm"}, {"score": 0.002654751947108171, "phrase": "matrix_c"}, {"score": 0.0026298358035890456, "phrase": "constant_number"}, {"score": 0.0024898537188826255, "phrase": "best_rank-k_approximation"}, {"score": 0.00239766945318535, "phrase": "low-degree_polynomial"}, {"score": 0.002330772795156782, "phrase": "failure_probability"}, {"score": 0.0023045384769611616, "phrase": "time_linear"}, {"score": 0.002206674151050406, "phrase": "important_use"}, {"score": 0.002198362765957541, "phrase": "matrix_perturbation_theory"}, {"score": 0.0021900826167852894, "phrase": "previous_work"}, {"score": 0.0021777207048336938, "phrase": "matrix_multiplication"}, {"score": 0.0021654284182129504, "phrase": "low-rank_approximations"}, {"score": 0.0021410511613651415, "phrase": "probability_distribution"}, {"score": 0.0021049977753042253, "phrase": "crucial_features"}], "paper_keywords": ["randomized algorithms", " Monte Carlo methods", " massive data sets", " CUR matrix decomposition"], "paper_abstract": "In many applications, the data consist of ( or may be naturally formulated as) an m x n matrix A which may be stored on disk but which is too large to be read into random access memory ( RAM) or to practically perform superlinear polynomial time computations on it. Two algorithms are presented which, when given an m x n matrix A, compute approximations to A which are the product of three smaller matrices, C, U, and R, each of which may be computed rapidly. Let A' = CUR be the computed approximate decomposition; both algorithms have provable bounds for the error matrix A - A'. In the first algorithm, c columns of A and r rows of A are randomly chosen. If the m x c matrix C consists of those c columns of A ( after appropriate rescaling) and the r x n matrix R consists of those r rows of A ( also after appropriate rescaling), then the c x r matrix U may be calculated from C and R. For any matrix X, let parallel to X parallel to(F) and parallel to X parallel to(2) denote its Frobenius norm and its spectral norm, respectively. It is proven that parallel to A - A'parallel to(xi) <= min (D: rank( D) <= k) parallel to A - D parallel to(xi) + poly(k, 1/c)parallel to A parallel to(F) holds in expectation and with high probability for both xi = 2, F and for all k = 1,..., rank(A); thus by appropriate choice of k parallel to A - A'parallel to(2) <=epsilon parallel to A parallel to(F) also holds in expectation and with high probability. This algorithm may be implemented without storing the matrix A in RAM, provided it can make two passes over the matrix stored in external memory and use O( m + n) additional RAM ( assuming that c and r are constants, independent of the size of the input). The second algorithm is similar except that it approximates the matrix C by randomly sampling a constant number of rows of C. Thus, it has additional error but it can be implemented in three passes over the matrix using only constant additional RAM. To achieve an additional error ( beyond the best rank-k approximation) that is at most epsilon parallel to A parallel to(F), both algorithms take time which is a low-degree polynomial in k, 1/epsilon, and 1/delta, where delta > 0 is a failure probability; the. rst takes time linear in max(m, n) and the second takes time independent of m and n. The proofs for the error bounds make important use of matrix perturbation theory and previous work on approximating matrix multiplication and computing low-rank approximations to a matrix. The probability distribution over columns and rows and the rescaling are crucial features of the algorithms and must be chosen judiciously.", "paper_title": "Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition", "paper_id": "WOS:000238324600009"}