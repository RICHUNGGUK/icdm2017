{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "safe_reinforcement_learning"}, {"score": 0.003820144626025562, "phrase": "reasonable_system_performance"}, {"score": 0.003726533059542558, "phrase": "safety_constraints"}, {"score": 0.0031064226956732497, "phrase": "optimality_criterion"}, {"score": 0.0028596699752849682, "phrase": "safety_factor"}, {"score": 0.0026107601878587816, "phrase": "exploration_process"}, {"score": 0.002504880563070155, "phrase": "external_knowledge"}, {"score": 0.0023834640793589435, "phrase": "risk_metric"}, {"score": 0.002286781334850213, "phrase": "proposed_classification"}, {"score": 0.0022122603123901114, "phrase": "existing_literature"}, {"score": 0.0021049977753042253, "phrase": "future_directions"}], "paper_keywords": ["reinforcement learning", " risk sensitivity", " safe exploration", " teacher advice"], "paper_abstract": "Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.", "paper_title": "A Comprehensive Survey on Safe Reinforcement Learning", "paper_id": "WOS:000369887100004"}