{"auto_keywords": [{"score": 0.03618042519491516, "phrase": "k"}, {"score": 0.00481495049065317, "phrase": "probabilistic_k_nearest_neighbour_classification"}, {"score": 0.004763565662615503, "phrase": "probabilistic_k-nearest"}, {"score": 0.004395185177346749, "phrase": "original_k-nearest_neighbour"}, {"score": 0.004301834610818624, "phrase": "classification_algorithm"}, {"score": 0.004077005481951783, "phrase": "feature_vector"}, {"score": 0.0039249110232708615, "phrase": "knn"}, {"score": 0.003843703165128603, "phrase": "pknn"}, {"score": 0.003741371936431574, "phrase": "optimal_number"}, {"score": 0.003378385493203297, "phrase": "decision_making"}, {"score": 0.0032712190037495975, "phrase": "improved_classification"}, {"score": 0.00323625520042892, "phrase": "bayesian_model_averaging"}, {"score": 0.002953711377604328, "phrase": "statistical_model_selection"}, {"score": 0.0027841357057119317, "phrase": "machine_learning_domain"}, {"score": 0.002652639568848117, "phrase": "new_functional_approximation_algorithm"}, {"score": 0.0024339783012484032, "phrase": "monte_carlo_simulations"}, {"score": 0.0023314867226678555, "phrase": "cross_validation"}, {"score": 0.0022941723937662927, "phrase": "bayesian_framework"}, {"score": 0.0022213218042340735, "phrase": "proposed_approaches"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Bayesian inference", " Model averaging", " K-free model order estimation"], "paper_abstract": "Probabilistic K-nearest neighbour (PKNN) classification has been introduced to improve the performance of the original K-nearest neighbour (KNN) classification algorithm by explicitly modelling uncertainty in the classification of each feature vector. However, an issue common to both KNN and PKNN is to select the optimal number of neighbours, K. The contribution of this paper is to incorporate the uncertainty in K into the decision making, and consequently to provide improved classification with Bayesian model averaging. Indeed the problem of assessing the uncertainty in K can be viewed as one of statistical model selection which is one of the most important technical issues in the statistics and machine learning domain. In this paper, we develop a new functional approximation algorithm to reconstruct the density of the model (order) without relying on time consuming Monte Carlo simulations. In addition, the algorithms avoid cross validation by adopting Bayesian framework. The performance of the proposed approaches is evaluated on several real experimental datasets. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Efficient model selection for probabilistic K nearest neighbour classification", "paper_id": "WOS:000346550300062"}