{"auto_keywords": [{"score": 0.0372737859287202, "phrase": "event_primitives"}, {"score": 0.00481495049065317, "phrase": "syntactic_attribute_graph_grammar"}, {"score": 0.004677032188041644, "phrase": "complex_semantic_events"}, {"score": 0.00446820572467064, "phrase": "challenging_task"}, {"score": 0.004431246962597134, "phrase": "high-level_understanding"}, {"score": 0.004233347974930148, "phrase": "attribute_graph_grammar"}, {"score": 0.00396110232316167, "phrase": "semantic_events"}, {"score": 0.0038796562792487, "phrase": "meaningful_\"event_components"}, {"score": 0.0038157022294338454, "phrase": "spatio-temporal_constraints"}, {"score": 0.003768426876815755, "phrase": "event_components"}, {"score": 0.003585089792133518, "phrase": "atomic_event_primitives"}, {"score": 0.0034678302347682694, "phrase": "object-trajectory_table"}, {"score": 0.003424850089214916, "phrase": "mobile_object"}, {"score": 0.0032581736321024373, "phrase": "video_sequence"}, {"score": 0.003191134522157061, "phrase": "temporal_and_spatial_relations"}, {"score": 0.002960979624379786, "phrase": "\"event_parse_graph"}, {"score": 0.002900037689072025, "phrase": "possible_variability"}, {"score": 0.0027020387992490367, "phrase": "syntactic_way"}, {"score": 0.002668524425703961, "phrase": "probability_model"}, {"score": 0.002496659907186784, "phrase": "annotated_event_instances"}, {"score": 0.0024452510143564057, "phrase": "learned_event"}, {"score": 0.0023750478314945303, "phrase": "gibbs_sampling_scheme"}, {"score": 0.002306855521822739, "phrase": "testing_video"}, {"score": 0.002158233639156017, "phrase": "real_indoor_and_outdoor_videos"}, {"score": 0.0021314498639048085, "phrase": "quantitative_recognition_rate"}, {"score": 0.0021049977753042253, "phrase": "public_lhi_dataset"}], "paper_keywords": ["Visual surveillance", " Event representation", " Event recognition", " Attribute graph grammar"], "paper_abstract": "The representation and recognition of complex semantic events (e.g. illegal parking, stealing objects) is a challenging task for high-level understanding of video sequence. To solve this problem, an attribute graph grammar for events modeling is studied in this paper. This grammar models the variability of semantic events by a set of meaningful \"event components\" with the spatio-temporal constraints. The event components are defined manually according to their semantic meaning, and further decomposed into atomic event primitives. These event primitives are learned on a object-trajectory table that describes mobile object attributes (location, velocity, and visibility) in a video sequence. A dictionary of temporal and spatial relations are defined to constrain the event primitives. With this representation, one observed event can be parsed into an \"event Parse graph\", and all possible variability of one event can be modeled into an \"event And-Or graph\", in a syntactic way. The probability model of an \"event And-Or graph\" can be learned on a set of annotated event instances, and given a learned event And-Or graph, a Gibbs sampling scheme is utilized for inference on a testing video. In the experiments, we test events recognition performance of the proposed on both real indoor and outdoor videos and show quantitative recognition rate on the public LHI dataset. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Semantic event representation and recognition using syntactic attribute graph grammar", "paper_id": "WOS:000261857200012"}