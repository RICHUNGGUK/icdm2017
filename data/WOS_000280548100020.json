{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "asynchronous_algorithms"}, {"score": 0.041996689288621714, "phrase": "iterative_algorithms"}, {"score": 0.040179907029343674, "phrase": "loop_tiling"}, {"score": 0.004741845603430361, "phrase": "multicore_chips"}, {"score": 0.004669845451682529, "phrase": "main_building_blocks"}, {"score": 0.004622450915803531, "phrase": "high_performance_computers"}, {"score": 0.004506048850130758, "phrase": "performance_impediment"}, {"score": 0.0044150318724093226, "phrase": "limited_hardware_capacity"}, {"score": 0.004216880725292, "phrase": "off-chip_memory"}, {"score": 0.004068907083697369, "phrase": "large_computing_problems"}, {"score": 0.003926105480202798, "phrase": "large_data"}, {"score": 0.0037307208089736835, "phrase": "loop_blocking"}, {"score": 0.003581412258960711, "phrase": "effective_way"}, {"score": 0.0035269696779744266, "phrase": "data_locality"}, {"score": 0.003403122817578447, "phrase": "memory_bandwidth_pressure"}, {"score": 0.0032336801561165113, "phrase": "single_processor"}, {"score": 0.0031521392193570846, "phrase": "tiled_programs"}, {"score": 0.0031042018721479385, "phrase": "reduced_parallelism"}, {"score": 0.003025915966640355, "phrase": "single_tile"}, {"score": 0.0028026735035370206, "phrase": "asynchronous_model"}, {"score": 0.0027600360731762997, "phrase": "effective_loop"}, {"score": 0.0024918548761791435, "phrase": "communication_cost"}, {"score": 0.002466510117718297, "phrase": "synchronization_overhead"}, {"score": 0.0023798115577749225, "phrase": "carefully_controlled_asynchrony"}, {"score": 0.002272794656458419, "phrase": "parallel_iterative_algorithms"}, {"score": 0.002249672943553135, "phrase": "multicore_processors"}, {"score": 0.0022154296909169826, "phrase": "simultaneously_attained_data_locality"}, {"score": 0.0021928902836049384, "phrase": "loop-level_parallelism"}, {"score": 0.0021484955960643167, "phrase": "supporting_evidence"}], "paper_keywords": ["Algorithms", " Performance", " asynchronous algorithms", " loop tiling", " parallel numerical programs", " data locality", " memory performance"], "paper_abstract": "As multicore chips become the main building blocks for high performance computers, many numerical applications face a performance impediment due to the limited hardware capacity to move data between the CPU and the off-chip memory. This is especially true for large computing problems solved by iterative algorithms because of the large data set typically used. Loop tiling, also known as loop blocking, was shown previously to be an effective way to enhance data locality, and hence to reduce the memory bandwidth pressure, for a class of iterative algorithms executed on a single processor. Unfortunately, the tiled programs suffer from reduced parallelism because only the loop iterations within a single tile can be easily parallelized. In this work, we propose to use the asynchronous model to enable effective loop tiling such that both parallelism and locality can be attained simultaneously. Asynchronous algorithms were previously proposed to reduce the communication cost and synchronization overhead between processors. Our new discovery is that carefully controlled asynchrony and loop tiling can significantly improve the performance of parallel iterative algorithms on multicore processors due to simultaneously attained data locality and loop-level parallelism. We present supporting evidence from experiments with three well-known numerical kernels.", "paper_title": "Improving Parallelism and Locality with Asynchronous Algorithms", "paper_id": "WOS:000280548100020"}