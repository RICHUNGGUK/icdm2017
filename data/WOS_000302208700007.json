{"auto_keywords": [{"score": 0.047032949642620024, "phrase": "annotated_samples"}, {"score": 0.04449264151645753, "phrase": "active_learning"}, {"score": 0.028835291483643638, "phrase": "active_learning_strategies"}, {"score": 0.025651959054697415, "phrase": "random_sampling_method"}, {"score": 0.00481495049065317, "phrase": "assertion_classification"}, {"score": 0.004741167616693272, "phrase": "clinical_text"}, {"score": 0.004704699244406775, "phrase": "supervised_machine_learning_methods"}, {"score": 0.004668510065740584, "phrase": "clinical_natural_language_processing"}, {"score": 0.004632683293980788, "phrase": "nlp"}, {"score": 0.004544016163767266, "phrase": "large_number"}, {"score": 0.004109865377936045, "phrase": "large_pool"}, {"score": 0.004046843083499649, "phrase": "alternative_solution"}, {"score": 0.0039085400187964196, "phrase": "annotation_effort"}, {"score": 0.003804234575698981, "phrase": "predictive_model"}, {"score": 0.003674192201248236, "phrase": "clinical_nlp."}, {"score": 0.003534889186471058, "phrase": "clinical_text_classification_task"}, {"score": 0.0034672240102960644, "phrase": "assertion_status"}, {"score": 0.0034405207528624983, "phrase": "clinical_concepts"}, {"score": 0.0034008496666026585, "phrase": "annotated_corpus"}, {"score": 0.0033616344666486725, "phrase": "assertion_classification_task"}, {"score": 0.0030637124954859657, "phrase": "global_alc_score"}, {"score": 0.00297037471252423, "phrase": "average_learning_curve"}, {"score": 0.002686169280393858, "phrase": "better_classification_models"}, {"score": 0.0026143990748127253, "phrase": "passive_learning_method"}, {"score": 0.0021544587602892466, "phrase": "manual_annotation_effort"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Active learning", " Natural language processing", " Clinical text classification", " Machine learning"], "paper_abstract": "Supervised machine learning methods for clinical natural language processing (NLP) research require a large number of annotated samples, which are very expensive to build because of the involvement of physicians. Active learning, an approach that actively samples from a large pool, provides an alternative solution. Its major goal in classification is to reduce the annotation effort while maintaining the quality of the predictive model. However, few studies have investigated its uses in clinical NLP. This paper reports an application of active learning to a clinical text classification task: to determine the assertion status of clinical concepts. The annotated corpus for the assertion classification task in the 2010 i2b2/VA Clinical NLP Challenge was used in this study. We implemented several existing and newly developed active learning algorithms and assessed their uses. The outcome is reported in the global ALC score, based on the Area under the average Learning Curve of the AUC (Area Under the Curve) score. Results showed that when the same number of annotated samples was used, active learning strategies could generate better classification models (best ALC - 0.7715) than the passive learning method (random sampling) (ALC - 0.7411). Moreover, to achieve the same classification performance, active learning strategies required fewer samples than the random sampling method. For example, to achieve an AUC of 0.79, the random sampling method used 32 samples, while our best active learning algorithm required only 12 samples, a reduction of 62.5% in manual annotation effort. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "Applying active learning to assertion classification of concepts in clinical text", "paper_id": "WOS:000302208700007"}