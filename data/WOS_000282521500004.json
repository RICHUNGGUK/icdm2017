{"auto_keywords": [{"score": 0.004337710093279712, "phrase": "objective_and_reliable_labels"}, {"score": 0.003644714340300824, "phrase": "substantial_amount"}, {"score": 0.0032830808525570903, "phrase": "great_practical_interest"}, {"score": 0.0031984060035955292, "phrase": "conventional_supervised_learning_problems"}, {"score": 0.002906138793910919, "phrase": "probabilistic_approach"}, {"score": 0.002855934865505357, "phrase": "supervised_learning"}, {"score": 0.0027581067089091434, "phrase": "multiple_annotators"}, {"score": 0.0026175952455594277, "phrase": "absolute_gold_standard"}, {"score": 0.0025500402081330394, "phrase": "proposed_algorithm"}, {"score": 0.0024842242921857705, "phrase": "different_experts"}, {"score": 0.0023168823213024856, "phrase": "actual_hidden_labels"}, {"score": 0.0022768346311104735, "phrase": "experimental_results"}, {"score": 0.0021987994275190314, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "commonly_used_majority_voting_baseline"}], "paper_keywords": ["multiple annotators", " multiple experts", " multiple teachers", " crowdsourcing"], "paper_abstract": "For many supervised learning tasks it may be infeasible (or very expensive) to obtain objective and reliable labels. Instead, we can collect subjective (possibly noisy) labels from multiple experts or annotators. In practice, there is a substantial amount of disagreement among the annotators, and hence it is of great practical interest to address conventional supervised learning problems in this scenario. In this paper we describe a probabilistic approach for supervised learning when we have multiple annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.", "paper_title": "Learning From Crowds", "paper_id": "WOS:000282521500004"}