{"auto_keywords": [{"score": 0.04744828772449851, "phrase": "reconstruction_time"}, {"score": 0.03743496955862914, "phrase": "pull-rep"}, {"score": 0.015560954417193747, "phrase": "erasure-coded_storage_clusters"}, {"score": 0.009495014825434958, "phrase": "push-rep"}, {"score": 0.009398519142179504, "phrase": "push-sur"}, {"score": 0.004692980391580608, "phrase": "key_design_goal"}, {"score": 0.004390060342250211, "phrase": "high_reliability"}, {"score": 0.004322967486451877, "phrase": "vulnerability_window_size"}, {"score": 0.004064653055162238, "phrase": "pull-type_transmission"}, {"score": 0.003982011128052839, "phrase": "rebuilding_node"}, {"score": 0.0038217146888983576, "phrase": "read_requests"}, {"score": 0.003782655283668953, "phrase": "surviving_nodes"}, {"score": 0.003724810609448597, "phrase": "surviving_blocks"}, {"score": 0.0036303546926971966, "phrase": "transmission_bottleneck"}, {"score": 0.0035932440062595252, "phrase": "replacement_nodes"}, {"score": 0.0034841647157189985, "phrase": "extra_overhead"}, {"score": 0.0034308688638878286, "phrase": "non-contiguous_disk_access"}, {"score": 0.0033957904376127187, "phrase": "pull-sur"}, {"score": 0.00332670230152448, "phrase": "push-type_transmissions"}, {"score": 0.003209152260950748, "phrase": "reconstruction_procedure"}, {"score": 0.003143849342926373, "phrase": "multiple_tasks"}, {"score": 0.0030327408069772293, "phrase": "pipelining_manner"}, {"score": 0.002494466951433885, "phrase": "reconstruction_process"}, {"score": 0.0023816257768353344, "phrase": "large-scale_storage_clusters"}, {"score": 0.0023211517663846346, "phrase": "proof-of-concept_prototype"}, {"score": 0.0021821915946356168, "phrase": "experimental_results"}, {"score": 0.0021377414429251647, "phrase": "push-based_reconstruction_schemes"}, {"score": 0.0021049977753042253, "phrase": "pull-based_counterparts"}], "paper_keywords": ["Erasure-coded storage cluster", " reconstruction", " PULL-type transmission", " PUSH-type transmission", " TCP Incast"], "paper_abstract": "A key design goal of erasure-coded storage clusters is to minimize reconstruction time, which in turn leads to high reliability by reducing vulnerability window size. PULL-Rep and PULL-Sur are two existing reconstruction schemes based on PULL-type transmission, where a rebuilding node initiates reconstruction by sending a set of read requests to surviving nodes to retrieve surviving blocks. To eliminate the transmission bottleneck of replacement nodes in PULL-Rep and mitigate the extra overhead caused by non-contiguous disk access in PULL-Sur, we incorporate PUSH-type transmissions to node reconstruction, where the reconstruction procedure is divided into multiple tasks accomplished by surviving nodes in a pipelining manner. We also propose two PUSH-based reconstruction schemes (i.e., PUSH-Rep and PUSH-Sur), which can not only exploit the I/O parallelism of PULL-Sur, but also maintain sequential I/O accesses inherited from PULL-Rep. We build four reconstruction-time models to study the reconstruction process and estimate the reconstruction time of the four schemes in large-scale storage clusters. We implement a proof-of-concept prototype where the four reconstruction schemes are deployed and quantitatively evaluated. Experimental results show that the PUSH-based reconstruction schemes outperform the PULL-based counterparts. In a real-world (9,6) RS-coded storage cluster, PUSH-Rep speeds up the reconstruction time by a factor of 5.76 compared with PULL-Rep; PUSH-Sur accelerates the reconstruction by a factor of 1.85 relative to PULL-Sur.", "paper_title": "PUSH: A Pipelined Reconstruction I/O for Erasure-Coded Storage Clusters", "paper_id": "WOS:000349769300018"}