{"auto_keywords": [{"score": 0.04551169820631223, "phrase": "target_domain"}, {"score": 0.041609355713619414, "phrase": "sparse_graph"}, {"score": 0.015616468673903016, "phrase": "domain_adaptation_learning"}, {"score": 0.00481495049065317, "phrase": "regularization_label_propagation"}, {"score": 0.004719664411221952, "phrase": "domain_adaptation"}, {"score": 0.004626255242038005, "phrase": "surprising_performance"}, {"score": 0.004580243025274209, "phrase": "labeled_samples"}, {"score": 0.004400705158735729, "phrase": "robust_classifier"}, {"score": 0.0042281749832498, "phrase": "even_no_labeled_samples"}, {"score": 0.004116913661033223, "phrase": "classical_graph-based_transductive_ssl_diagram"}, {"score": 0.003929177231678094, "phrase": "kernel_sparse_representation"}, {"score": 0.0038642157757169315, "phrase": "optimal_reproduced_kernel_hilbert_space"}, {"score": 0.0037750632698768704, "phrase": "inter-domain_distribution_discrepancy"}, {"score": 0.003687960013746783, "phrase": "sparsity_regularization_label_propagation"}, {"score": 0.003531474887552889, "phrase": "labeled_data"}, {"score": 0.0034614999671904964, "phrase": "unlabeled_one"}, {"score": 0.0032706520949775065, "phrase": "optimal_rkhs"}, {"score": 0.003173898798475791, "phrase": "data_distributions"}, {"score": 0.0030595112110658675, "phrase": "best_kernel_sparse_reconstructed_coefficients"}, {"score": 0.002929634095211462, "phrase": "rkhs"}, {"score": 0.0027312956497456374, "phrase": "unlabeled_points"}, {"score": 0.002546368390189036, "phrase": "data_point"}, {"score": 0.002397833600728684, "phrase": "proposed_sparsity_regularization_framework"}, {"score": 0.0023268399760599336, "phrase": "slpdal"}, {"score": 0.0023113506171584157, "phrase": "out-of-sample_data"}, {"score": 0.002213147136067999, "phrase": "toy_datasets"}, {"score": 0.0021620042653528846, "phrase": "visual_video"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Domain adaptation learning", " Sparse representation", " Label propagation", " Maximum mean discrepancy", " Multiple kernel learning"], "paper_abstract": "Recently, domain adaptation learning (DAL) has shown surprising performance by utilizing labeled samples from the source (or auxiliary) domain to learn a robust classifier for the target domain of the interest which has a few or even no labeled samples. In this paper, by incorporating classical graph-based transductive SSL diagram, a novel DAL method is proposed based on a sparse graph constructed via kernel sparse representation of data in an optimal reproduced kernel Hilbert space (RKHS) recovered by minimizing inter-domain distribution discrepancy. Our method, named as Sparsity regularization Label Propagation for Domain Adaptation Learning (SLPDAL), can propagate the labels of the labeled data from both domains to the unlabeled one in the target domain using their sparsely reconstructed objects with sufficient smoothness by using three steps: (1) an optimal RKHS is first recovered so as to minimize the data distributions of two domains; (2) it then computes the best kernel sparse reconstructed coefficients for each data point in both domains by using l(1)-norm minimization in the RKHS, thus constructing a sparse graph; and (3) the labels of the labeled data from both domains is finally propagated to the unlabeled points in the target domain over the sparse graph based on our proposed sparsity regularization framework, in which it is assumed that the label of each data point can be sparsely reconstructed by those of other data points from both domains. Furthermore, based on the proposed sparsity regularization framework, an easy way is derived to extend SLPDAL to out-of-sample data. Promising experimental results have been obtained on both a serial of toy datasets and several real-world datasets such as face, visual video and text. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Sparsity regularization label propagation for domain adaptation learning", "paper_id": "WOS:000337661800021"}