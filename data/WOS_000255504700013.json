{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "linear_discriminant_functions"}, {"score": 0.004733625412289433, "phrase": "statistical_learning_theory"}, {"score": 0.0046141941030386525, "phrase": "formal_criterion"}, {"score": 0.004060566667226606, "phrase": "empirical_fit"}, {"score": 0.003697100573612401, "phrase": "structural_risk-minimization_principle"}, {"score": 0.0027194778713860715, "phrase": "popular_and_powerful_learning_mechanism"}, {"score": 0.0021049977753042253, "phrase": "useful_criterion"}], "paper_keywords": ["linear discriminant functions", " structural risk-minimization principle", " the minimum description length principle", " VC-dimension", " margin", " risk bound", " empirical risk", " genetic algorithm", " support vector machines"], "paper_abstract": "Statistical learning theory provides a formal criterion for learning a concept from examples. This theory addresses directly the trade-off in empirical fit and generalization. In practice, this leads to the structural risk-minimization principle where one minimizes a bound on the overall risk functional. For learning linear discriminant functions, this bound is impacted by the minimum of two terms-the dimension and the inverse of the margin. A popular and powerful learning mechanism, support vector machines, focuses on maximizing the margin. We compare this to methods that focus on minimizing the dimensionality, which, coincidentally, fulfills another useful criterion-the minimum description length principle.", "paper_title": "Risk minimization and minimum description for linear discriminant functions", "paper_id": "WOS:000255504700013"}