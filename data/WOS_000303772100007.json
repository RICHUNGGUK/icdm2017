{"auto_keywords": [{"score": 0.03856348556738522, "phrase": "lasso"}, {"score": 0.00481495049065317, "phrase": "structured_sparsity"}, {"score": 0.0044870272550540415, "phrase": "data_dependent_generalization"}, {"score": 0.004309725894724034, "phrase": "large_class"}, {"score": 0.004223708778943785, "phrase": "regularized_algorithms"}, {"score": 0.0040978781692373005, "phrase": "structured_sparsity_constraints"}, {"score": 0.003780286517897356, "phrase": "standard_squared-norm_regularization"}, {"score": 0.003216804708106142, "phrase": "overlapping_groups"}, {"score": 0.0026823700218343506, "phrase": "novel_feature"}, {"score": 0.002400373755068185, "phrase": "infinite_dimensional_setting"}, {"score": 0.0022365262201954643, "phrase": "separable_hilbert_space"}, {"score": 0.0021917966722180132, "phrase": "multiple_kernel"}, {"score": 0.0021049977753042253, "phrase": "countable_number"}], "paper_keywords": ["empirical processes", " Rademacher average", " sparse estimation"], "paper_abstract": "We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels.", "paper_title": "Structured Sparsity and Generalization", "paper_id": "WOS:000303772100007"}