{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multiagent_rewards"}, {"score": 0.0047820521205253035, "phrase": "dynamic_and_stochastic_domains"}, {"score": 0.004636756166068091, "phrase": "agent_reward_structures"}, {"score": 0.004557943646825813, "phrase": "successful_design"}, {"score": 0.004526793300190607, "phrase": "multiagent_learning_algorithms"}, {"score": 0.0044804647007342865, "phrase": "final_system_performance"}, {"score": 0.004434608130268514, "phrase": "best_indicator"}, {"score": 0.004212240534862585, "phrase": "reward_properties"}, {"score": 0.0041548413176193235, "phrase": "good_system_behavior"}, {"score": 0.003987275549870311, "phrase": "strong_signal"}, {"score": 0.00396000980403964, "phrase": "noise_ratios"}, {"score": 0.0037742798298349145, "phrase": "simple_table_backup_schemes"}, {"score": 0.0036220063770882275, "phrase": "reward_structure"}, {"score": 0.00349979888845939, "phrase": "chosen_learning_algorithm"}, {"score": 0.0033933295645859015, "phrase": "new_reward_evaluation_method"}, {"score": 0.0031572878619497624, "phrase": "learning_problem"}, {"score": 0.0030402538087882015, "phrase": "learning_algorithm"}, {"score": 0.0029782504696742283, "phrase": "problem_domain"}, {"score": 0.002947723206640026, "phrase": "agents'_reward_structure"}, {"score": 0.0028975361811534265, "phrase": "reward_property_visualization_method"}, {"score": 0.0028580007147015734, "phrase": "effective_reward"}, {"score": 0.0028287025225754337, "phrase": "extensive_simulations"}, {"score": 0.002705165729333657, "phrase": "dynamic_multi-rover_learning_domain"}, {"score": 0.00265909739882579, "phrase": "continuous_state_spaces"}, {"score": 0.002631832840403278, "phrase": "noisy_actions"}, {"score": 0.002587010112064415, "phrase": "agents'_movement_decisions"}, {"score": 0.002440231768165477, "phrase": "reward_efficiency_visualization_method"}, {"score": 0.002398664348595521, "phrase": "magnitude_speedup"}, {"score": 0.002374063735432571, "phrase": "good_rewards"}, {"score": 0.002325615071949044, "phrase": "full_simulation"}, {"score": 0.0022240004131756095, "phrase": "new_rewards"}, {"score": 0.002193634795186667, "phrase": "observational_limitations"}, {"score": 0.0021195184368629017, "phrase": "best_properties"}, {"score": 0.0021049977753042253, "phrase": "traditional_rewards"}], "paper_keywords": ["multiagent learning", " visualization", " reinforcement learning", " reward analysis"], "paper_abstract": "The ability to analyze the effectiveness of agent reward structures is critical to the successful design of multiagent learning algorithms. Though final system performance is the best indicator of the suitability of a given reward structure, it is often preferable to analyze the reward properties that lead to good system behavior (i.e., properties promoting coordination among the agents and providing agents with strong signal to noise ratios). This step is particularly helpful in continuous, dynamic, stochastic domains ill-suited to simple table backup schemes commonly used in TD(lambda)/Q-learning where the effectiveness of the reward structure is difficult to distinguish from the effectiveness of the chosen learning algorithm. In this paper, we present a new reward evaluation method that provides a visualization of the tradeoff between the level of coordination among the agents and the difficulty of the learning problem each agent faces. This method is independent of the learning algorithm and is only a function of the problem domain and the agents' reward structure. We use this reward property visualization method to determine an effective reward without performing extensive simulations. We then test this method in both a static and a dynamic multi-rover learning domain where the agents have continuous state spaces and take noisy actions (e.g., the agents' movement decisions are not always carried out properly). Our results show that in the more difficult dynamic domain, the reward efficiency visualization method provides a two order of magnitude speedup in selecting good rewards, compared to running a full simulation. In addition, this method facilitates the design and analysis of new rewards tailored to the observational limitations of the domain, providing rewards that combine the best properties of traditional rewards.", "paper_title": "Analyzing and visualizing multiagent rewards in dynamic and stochastic domains", "paper_id": "WOS:000259125500006"}