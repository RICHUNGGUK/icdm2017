{"auto_keywords": [{"score": 0.04377815815707316, "phrase": "ltbqe"}, {"score": 0.00481495049065317, "phrase": "diverse_web"}, {"score": 0.00476323161560059, "phrase": "traditional_approaches"}, {"score": 0.004648876767247476, "phrase": "centralised_warehouses"}, {"score": 0.004611369021812453, "phrase": "remote_data"}, {"score": 0.004561826854638056, "phrase": "linked_data_principles"}, {"score": 0.004525018246376883, "phrase": "answering_queries"}, {"score": 0.004404445045826858, "phrase": "remote_data_sources"}, {"score": 0.004263972370406983, "phrase": "sparql_queries"}, {"score": 0.004206767038617782, "phrase": "link-traversal_based_query_execution"}, {"score": 0.004139128578531134, "phrase": "linked_data"}, {"score": 0.004116823963855969, "phrase": "up-to-date_results"}, {"score": 0.00394264312173188, "phrase": "incomplete_dereferenceable_knowledge"}, {"score": 0.003910810973842123, "phrase": "remote_documents"}, {"score": 0.00386876598761639, "phrase": "response_times"}, {"score": 0.003806541299696042, "phrase": "query_answers"}, {"score": 0.0035100601949451028, "phrase": "diverse_sources"}, {"score": 0.0034723086467062774, "phrase": "lightweight_reasoning_extensions"}, {"score": 0.003434961725919714, "phrase": "additional_answers"}, {"score": 0.002984373546099842, "phrase": "link-traversal_query_techniques"}, {"score": 0.002928399649340438, "phrase": "large_crawl"}, {"score": 0.002796782643988021, "phrase": "raw_data"}, {"score": 0.0027741761880779535, "phrase": "dereferenceable_documents"}, {"score": 0.0026638482306228575, "phrase": "query_answering"}, {"score": 0.0025648274981472426, "phrase": "real-world_settings"}, {"score": 0.002530359917257626, "phrase": "dbpedia_sparql_benchmark_frameworks"}, {"score": 0.0024963543716522087, "phrase": "novel_benchmark"}, {"score": 0.0024628046987344846, "phrase": "random_walks"}, {"score": 0.0024495111090152857, "phrase": "diverse_data"}, {"score": 0.002416589459425828, "phrase": "link-traversal_query_approaches"}, {"score": 0.0023841092222805253, "phrase": "uncontrolled_environments"}, {"score": 0.0023712394008737958, "phrase": "simple_queries"}, {"score": 0.002326738402957945, "phrase": "unfeasible_number"}, {"score": 0.002222102813520549, "phrase": "slower_execution"}, {"score": 0.0021164256448611063, "phrase": "performance_issues"}, {"score": 0.0021049977753042253, "phrase": "complex_queries"}], "paper_keywords": ["Linked data", " SPARQL", " RDFS", " OWL", " Semantic Web", " RDF", " Web of Data", " live querying", " reasoning"], "paper_abstract": "Traditional approaches for querying the Web of Data often involve centralised warehouses that replicate remote data. Conversely, Linked Data principles allow for answering queries live over the Web by dereferencing URIs to traverse remote data sources at runtime. A number of authors have looked at answering SPARQL queries in such a manner; these link-traversal based query execution (LTBQE) approaches for Linked Data offer up-to-date results and decentralised (i.e., client-side) execution, but must operate over incomplete dereferenceable knowledge available in remote documents, thus affecting response times and \"recall\" for query answers. In this paper, we study the recall and effectiveness of LTBQE, in practice, for the Web of Data. Furthermore, to integrate data from diverse sources, we propose lightweight reasoning extensions to help find additional answers. From the state-of-the-art which (1) considers only dereferenceable information and (2) follows rdfs: seeAlso links, we propose extensions to consider (3) owl: sameAs links and reasoning, and (4) lightweight RDFS reasoning. We then estimate the recall of link-traversal query techniques in practice: we analyse a large crawl of the Web of Data (the BTC'11 dataset), looking at the ratio of raw data contained in dereferenceable documents vs. the corpus as a whole and determining how much more raw data our extensions make available for query answering. We then stress-test LTBQE (and our extensions) in real-world settings using the FedBench and DBpedia SPARQL Benchmark frameworks, and propose a novel benchmark called QWalk based on random walks through diverse data. We show that link-traversal query approaches often work well in uncontrolled environments for simple queries, but need to retrieve an unfeasible number of sources for more complex queries. We also show that our reasoning extensions increase recall at the cost of slower execution, often increasing the rate at which results return; conversely, we show that reasoning aggravates performance issues for complex queries.", "paper_title": "Link traversal querying for a diverse Web of Data", "paper_id": "WOS:000364155700004"}