{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "least_square_regression_with_coefficient_regularization"}, {"score": 0.0046046429665581555, "phrase": "gradient_descent."}, {"score": 0.004211069233133584, "phrase": "stochastic_gradient_descent_algorithm"}, {"score": 0.0039380388343479384, "phrase": "least_square_regression"}, {"score": 0.0037658881407605445, "phrase": "coefficient_regularization"}, {"score": 0.0035216195857272403, "phrase": "explicit_expression"}, {"score": 0.003149094460617212, "phrase": "sampling_operator"}, {"score": 0.0030113280682863234, "phrase": "empirical_integral_operator"}, {"score": 0.0027535633837592597, "phrase": "learning_rates"}, {"score": 0.002354302193867031, "phrase": "suitable_choices"}, {"score": 0.0022013916728990564, "phrase": "step_sizes"}, {"score": 0.0021049977753042253, "phrase": "regularization_parameters"}], "paper_keywords": ["Stochastic gradient descent", " coefficient regularization", " integral operator", " learning rate"], "paper_abstract": "We propose a stochastic gradient descent algorithm for the least square regression with coefficient regularization. An explicit expression of the solution via sampling operator and empirical integral operator is derived. Learning rates are given in terms of the suitable choices of the step sizes and regularization parameters.", "paper_title": "LEAST SQUARE REGRESSION WITH COEFFICIENT REGULARIZATION BY GRADIENT DESCENT", "paper_id": "WOS:000300776800005"}