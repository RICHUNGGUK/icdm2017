{"auto_keywords": [{"score": 0.046487871371234384, "phrase": "different_channels"}, {"score": 0.0046570462031865385, "phrase": "affective_computing"}, {"score": 0.00450429688285697, "phrase": "affective_information"}, {"score": 0.0044052452943364314, "phrase": "facial_sensing_system"}, {"score": 0.0043404186974790706, "phrase": "emotional_classification_mechanism"}, {"score": 0.004213601332868765, "phrase": "robust_manner"}, {"score": 0.003912469968653304, "phrase": "associated_weight"}, {"score": 0.0038691879515317267, "phrase": "facial_expression"}, {"score": 0.0037981086150309885, "phrase": "six_ekman's_universal_emotional_categories"}, {"score": 0.003385555209074879, "phrase": "statistical_evaluation_strategies"}, {"score": 0.0033110234463504125, "phrase": "classification_accuracy_ratios"}, {"score": 0.00328654473734697, "phrase": "confusion_matrices"}, {"score": 0.0032501644753165555, "phrase": "categorical_facial_sensing_system"}, {"score": 0.0030288982511487835, "phrase": "multimodal_human_affect_recognition"}, {"score": 0.002995361424576051, "phrase": "novel_fusion_methodology"}, {"score": 0.0029185415814209055, "phrase": "affective_modules"}, {"score": 0.00265987582861194, "phrase": "continuous_emotional_path"}, {"score": 0.002630414143385786, "phrase": "user's_affective_progress"}, {"score": 0.002582032847722892, "phrase": "kalman_filtering_technique"}, {"score": 0.0025064622665903645, "phrase": "temporal_consistency"}, {"score": 0.002379484250348599, "phrase": "eventual_temporal_changes"}, {"score": 0.0023270490574896804, "phrase": "different_inputs'_quality"}, {"score": 0.0022757667100089243, "phrase": "multimodal_fusion_methodology"}, {"score": 0.0022338938462817867, "phrase": "dynamic_affective_information"}, {"score": 0.0021049977753042253, "phrase": "instant_messaging_tool"}], "paper_keywords": ["Affective Computing", " Kansei (sense/emotion) engineering", " Human factors", " Facial expression analysis", " Multimodal fusion"], "paper_abstract": "This paper deals with two main research focuses on Affective Computing: facial emotion recognition and multimodal fusion of affective information coming from different channels. The facial sensing system developed implements an emotional classification mechanism that combines, in a novel and robust manner, the five most commonly used classifiers in the field of affect sensing, obtaining at the output an associated weight of the facial expression to each of the six Ekman's universal emotional categories plus the neutral. The system is able to analyze any subject, male or female, of any age, and ethnicity and has been validated by means of statistical evaluation strategies, such as cross-validation, classification accuracy ratios and confusion matrices. The categorical facial sensing system has been subsequently expanded to a continuous 2D affective space which has made it also possible to face the problem of multimodal human affect recognition. A novel fusion methodology able to fuse any number of affective modules, with very different time-scales and output labels, is proposed. It relies on the 2D Whissell affective space and is able to output a continuous emotional path characterizing the user's affective progress over time. A Kalman filtering technique controls this path in real-time to ensure temporal consistency and robustness to the system. Moreover, the methodology is adaptive to eventual temporal changes in the reliability of the different inputs' quality. The potential of the multimodal fusion methodology is demonstrated by fusing dynamic affective information extracted from different channels (video, typed-in text and emoticons) of an Instant Messaging tool.", "paper_title": "Emotional facial sensing and multimodal fusion in a continuous 2D affective space", "paper_id": "WOS:000209331500005"}