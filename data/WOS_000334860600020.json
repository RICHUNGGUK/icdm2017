{"auto_keywords": [{"score": 0.04574703079843426, "phrase": "online_learning"}, {"score": 0.010612387000973441, "phrase": "sparse_online_learning"}, {"score": 0.004720043307726269, "phrase": "compact_predictive_model"}, {"score": 0.004650086856905758, "phrase": "online_setting"}, {"score": 0.0045357787854931894, "phrase": "great_deal"}, {"score": 0.004337028108577593, "phrase": "sparsity-inducing_regularization"}, {"score": 0.004209369306023422, "phrase": "smaller_memory_space"}, {"score": 0.0041469500929423595, "phrase": "previous_learning_frameworks"}, {"score": 0.004065156038847781, "phrase": "learning_algorithms"}, {"score": 0.003500812175550438, "phrase": "small_range"}, {"score": 0.0032164136388931805, "phrase": "pre-processing_step"}, {"score": 0.0029698211157405618, "phrase": "new_regularization_framework"}, {"score": 0.00286795254228461, "phrase": "regularization_terms"}, {"score": 0.0027834145465170292, "phrase": "state-of-the-art_regularization_approach"}, {"score": 0.0026480044868438875, "phrase": "loss_function"}, {"score": 0.0026086775334428617, "phrase": "regularization_term"}, {"score": 0.00256993314099902, "phrase": "resulting_algorithms"}, {"score": 0.002457110101089137, "phrase": "feature's_truncations"}, {"score": 0.0023028105026714533, "phrase": "theoretical_properties"}, {"score": 0.0022460727898764216, "phrase": "computational_complexity"}, {"score": 0.002223770196464436, "phrase": "upper_bound"}, {"score": 0.002126111716875144, "phrase": "previous_methods"}], "paper_keywords": ["online learning", " supervised learning", " sparsity-inducing regularization", " feature selection", " sentiment analysis"], "paper_abstract": "Learning a compact predictive model in an online setting has recently gained a great deal of attention. The combination of online learning with sparsity-inducing regularization enables faster learning with a smaller memory space than the previous learning frameworks. Many optimization methods and learning algorithms have been developed on the basis of online learning with L1-regularization. L1-regularization tends to truncate some types of parameters, such as those that rarely occur or have a small range of values, unless they are emphasized in advance. However, the inclusion of a pre-processing step would make it very difficult to preserve the advantages of online learning. We propose a new regularization framework for sparse online learning. We focus on regularization terms, and we enhance the state-of-the-art regularization approach by integrating information on all previous subgradients of the loss function into a regularization term. The resulting algorithms enable online learning to adjust the intensity of each feature's truncations without pre-processing and eventually eliminate the bias of L1-regularization. We show theoretical properties of our framework, the computational complexity and upper bound of regret. Experiments demonstrated that our algorithms outperformed previous methods in many classification tasks.", "paper_title": "Feature-aware regularization for sparse online learning", "paper_id": "WOS:000334860600020"}