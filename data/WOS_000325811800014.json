{"auto_keywords": [{"score": 0.030868998874318338, "phrase": "intermediate_representation"}, {"score": 0.015719716506582538, "phrase": "multimedia_event_detection"}, {"score": 0.0047559432344272, "phrase": "classifier-specific_intermediate_representation"}, {"score": 0.004717055892073801, "phrase": "multimedia"}, {"score": 0.004640093664122174, "phrase": "med"}, {"score": 0.004564401713056939, "phrase": "important_role"}, {"score": 0.004471530397729526, "phrase": "video_indexing"}, {"score": 0.004398589723893764, "phrase": "current_event_detection"}, {"score": 0.004273782634394231, "phrase": "news_event_detection"}, {"score": 0.004238775572746022, "phrase": "abnormality_detection"}, {"score": 0.004204054047427536, "phrase": "surveillance_videos"}, {"score": 0.004018087343963485, "phrase": "longer_video_sequence"}, {"score": 0.0038561471948364723, "phrase": "intermediate_concept_classifiers"}, {"score": 0.003824547895167011, "phrase": "concept_lexica"}, {"score": 0.0034935290662899488, "phrase": "particular_video_analysis_task"}, {"score": 0.003422371752150812, "phrase": "robust_semantic_concept_classifiers"}, {"score": 0.00338037259057507, "phrase": "large_number"}, {"score": 0.003352658916650986, "phrase": "positive_training_examples"}, {"score": 0.003257432325797907, "phrase": "human_annotation"}, {"score": 0.003074991770768925, "phrase": "external_concepts"}, {"score": 0.002890810475014827, "phrase": "video_features"}, {"score": 0.002831894638104635, "phrase": "classifier_inference"}, {"score": 0.0028086649378423357, "phrase": "latent_intermediate_representation"}, {"score": 0.0027741761880779535, "phrase": "joint_framework"}, {"score": 0.002740109776120878, "phrase": "joint_optimization"}, {"score": 0.002461810159687652, "phrase": "classifier_dependent_intermediate_representation"}, {"score": 0.0024017007954518065, "phrase": "task_semantics"}, {"score": 0.002323826502178581, "phrase": "specific_classifier"}, {"score": 0.0022577559526646904, "phrase": "discriminative_semantic_analysis_framework"}, {"score": 0.002220846338732873, "phrase": "tightly_coupled_intermediate_representation"}, {"score": 0.002157697367447098, "phrase": "real-world_videos"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Intermediate representation", " multimedia event detection", " p-norm"], "paper_abstract": "Multimedia event detection (MED) plays an important role in many applications such as video indexing and retrieval. Current event detection works mainly focus on sports and news event detection or abnormality detection in surveillance videos. Differently, our research aims to detect more complicated and generic events within a longer video sequence. In the past, researchers have proposed using intermediate concept classifiers with concept lexica to help understand the videos. Yet it is difficult to judge how many and what concepts would be sufficient for the particular video analysis task. Additionally, obtaining robust semantic concept classifiers requires a large number of positive training examples, which in turn has high human annotation cost. In this paper, we propose an approach that exploits the external concepts-based videos and event-based videos simultaneously to learn an intermediate representation from video features. Our algorithm integrates the classifier inference and latent intermediate representation into a joint framework. The joint optimization of the intermediate representation and the classifier makes them mutually beneficial and reciprocal. Effectively, the intermediate representation and the classifier are tightly correlated. The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier. Thus we have created a discriminative semantic analysis framework based on a tightly coupled intermediate representation. Extensive experiments on multimedia event detection using real-world videos demonstrate the effectiveness of the proposed approach.", "paper_title": "Multimedia Event Detection Using A Classifier-Specific Intermediate Representation", "paper_id": "WOS:000325811800014"}