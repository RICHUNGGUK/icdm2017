{"auto_keywords": [{"score": 0.02932751831347652, "phrase": "baum-welch"}, {"score": 0.011850408411438924, "phrase": "unsupervised_methods"}, {"score": 0.00481495049065317, "phrase": "short_message_service_dialect"}, {"score": 0.004752954376588157, "phrase": "textual_data"}, {"score": 0.004542158659382197, "phrase": "phonetic_spellings"}, {"score": 0.004483659121004398, "phrase": "standard_transliteration"}, {"score": 0.004411588377842043, "phrase": "considerable_problems"}, {"score": 0.004284755353695709, "phrase": "instant_messenger"}, {"score": 0.004257067039134535, "phrase": "short_message_service_data"}, {"score": 0.00418862323286423, "phrase": "off-the-shelf_text_mining_methods"}, {"score": 0.004081385289516914, "phrase": "supervised_methods"}, {"score": 0.0040027552011993005, "phrase": "labeled_corrections"}, {"score": 0.003938383970047866, "phrase": "human_generated_labels"}, {"score": 0.003515758094739175, "phrase": "singular_concern"}, {"score": 0.00342568771304873, "phrase": "cost_effective_results"}, {"score": 0.003370565320453525, "phrase": "expensive_human_intervention"}, {"score": 0.0032418515965728775, "phrase": "generative_model_based_unsupervised_technique"}, {"score": 0.0031793439438669176, "phrase": "-standard_words"}, {"score": 0.0031281730014429666, "phrase": "hidden_markov_model"}, {"score": 0.0030678504924907166, "phrase": "\"subsequencized\"_representation"}, {"score": 0.002922069655177295, "phrase": "weighted_subsequences"}, {"score": 0.002893753089970741, "phrase": "approximate_maximum_likelihood_inference_algorithm"}, {"score": 0.0028379381771661116, "phrase": "training_phase"}, {"score": 0.002659527314613792, "phrase": "principled_transformation"}, {"score": 0.0026423134393535265, "phrase": "maximum_likelihood_based_\"central_clustering\"_cost_function"}, {"score": 0.002599763359352916, "phrase": "\"pairwise_similarity\"_based_clustering"}, {"score": 0.0025085435314540837, "phrase": "\"subsequence_kernel\"_based_methods"}, {"score": 0.002312959811505281, "phrase": "hmm"}, {"score": 0.0022390081666133627, "phrase": "loglikelihood_function"}, {"score": 0.002160419006055386, "phrase": "pairwise_distance"}, {"score": 0.0021464286727045623, "phrase": "anecdotal_evidence"}, {"score": 0.0021049977753042253, "phrase": "public_and_proprietary_data"}], "paper_keywords": ["Noisy text", " Unsupervised learning", " Clustering"], "paper_abstract": "Noise in textual data such as those introduced by multilinguality, misspellings, abbreviations, deletions, phonetic spellings, non-standard transliteration, etc. pose considerable problems for text-mining. Such corruptions are very common in instant messenger and short message service data and they adversely affect off-the-shelf text mining methods. Most techniques address this problem by supervised methods by making use of hand labeled corrections. But they require human generated labels and corrections that are very expensive and time consuming to obtain because of multilinguality and complexity of the corruptions. While we do not champion unsupervised methods over supervised when quality of results is the singular concern, we demonstrate that unsupervised methods can provide cost effective results without the need for expensive human intervention that is necessary to generate a parallel labeled corpora. A generative model based unsupervised technique is presented that maps non-standard words to their corresponding conventional frequent form. A hidden Markov model (HMM) over a \"subsequencized\" representation of words is used, where a word is represented as a bag of weighted subsequences. The approximate maximum likelihood inference algorithm used is such that the training phase involves clustering over vectors and not the customary and expensive dynamic programming (Baum-Welch algorithm) over sequences that is necessary for HMMs. A principled transformation of maximum likelihood based \"central clustering\" cost function of Baum-Welch into a \"pairwise similarity\" based clustering is proposed. This transformation makes it possible to apply \"subsequence kernel\" based methods that model delete and insert corruptions well. The novelty of this approach lies in that the expensive (Baum-Welch) iterations required for HMM, can be avoided through an approximation of the loglikelihood function and by establishing a connection between the loglikelihood and a pairwise distance. Anecdotal evidence of efficacy is provided on public and proprietary data.", "paper_title": "Language independent unsupervised learning of short message service dialect", "paper_id": "WOS:000270432400005"}