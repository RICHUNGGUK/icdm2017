{"auto_keywords": [{"score": 0.048163992431376056, "phrase": "pca"}, {"score": 0.00481495049065317, "phrase": "high_dimensional_kernel_principal_component_analysis"}, {"score": 0.004717723626157272, "phrase": "small_sample_high-dimensional_principal_component_analysis"}, {"score": 0.003807684498435389, "phrase": "simple_leave-one-out_variance_renormalization_scheme"}, {"score": 0.0030726480920918097, "phrase": "computationally_less_intensive_approximate_leave-one-out_estimator"}, {"score": 0.002690406375639144, "phrase": "kernel_principal_component_analysis"}, {"score": 0.002479150189700726, "phrase": "non-parametric_renormalization_scheme"}, {"score": 0.002284444272376043, "phrase": "kpca."}, {"score": 0.0021049977753042253, "phrase": "simplified_approximate_expression"}], "paper_keywords": ["PCA", " kernel PCA", " generalizability", " variance renormalization"], "paper_abstract": "Small sample high-dimensional principal component analysis (PCA) suffers from variance inflation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inflation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efficiently restore generalizability in kPCA. As for PCA our analysis also suggests a simplified approximate expression.", "paper_title": "A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis", "paper_id": "WOS:000293757200009"}