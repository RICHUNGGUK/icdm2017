{"auto_keywords": [{"score": 0.04116482427643901, "phrase": "design_matrix_x"}, {"score": 0.026758392565224182, "phrase": "high_probability"}, {"score": 0.00481495049065317, "phrase": "restricted_eigenvalue_properties_for_correlated_gaussian_designs"}, {"score": 0.004630150894006861, "phrase": "basis_pursuit"}, {"score": 0.0044524122425287005, "phrase": "sparse_regression"}, {"score": 0.004413846562278824, "phrase": "high_dimensions"}, {"score": 0.0038737214010638745, "phrase": "restricted_nullspace_property"}, {"score": 0.0037741838859699805, "phrase": "lasso"}, {"score": 0.0035507457285983268, "phrase": "p-dimensional_regression_problem"}, {"score": 0.003507278277712241, "phrase": "gaussian"}, {"score": 0.003384695931421288, "phrase": "restricted_eigenvalue_condition"}, {"score": 0.003340768341189764, "phrase": "key_issue"}, {"score": 0.0031984060035955292, "phrase": "desirable_properties"}, {"score": 0.0030888830649190282, "phrase": "numerous_results"}, {"score": 0.00303553181618577, "phrase": "restricted_isometry_property"}, {"score": 0.0025723630851105304, "phrase": "restricted_nullspace"}, {"score": 0.0025500402081330394, "phrase": "eigenvalue_conditions"}, {"score": 0.0024950747439601863, "phrase": "quite_general_classes"}, {"score": 0.002473420909291381, "phrase": "gaussian_matrices"}, {"score": 0.002218054404179068, "phrase": "attractive_theoretical_guarantees"}, {"score": 0.0021049977753042253, "phrase": "completely_independent_or_unitary_designs"}], "paper_keywords": ["Lasso", " basis pursuit", " random matrix theory", " Gaussian comparison inequality", " concentration of measure"], "paper_abstract": "Methods based on l(1)-relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisfies the restricted nullspace property, and (2) the squared l(2)-error of a Lasso estimate decays at the minimax optimal rate klogp/n, where k is the sparsity of the p-dimensional regression problem with additive Gaussian noise, whenever the design satisfies a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisfies these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisfied when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on l(1)-relaxations to a much broader class of problems than the case of completely independent or unitary designs.", "paper_title": "Restricted Eigenvalue Properties for Correlated Gaussian Designs", "paper_id": "WOS:000282523300008"}