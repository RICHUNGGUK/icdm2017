{"auto_keywords": [{"score": 0.047244318311945976, "phrase": "feature_selection"}, {"score": 0.00481495049065317, "phrase": "growing_interest"}, {"score": 0.0044692705329574, "phrase": "different_learning_problems"}, {"score": 0.0037984549828326106, "phrase": "joint_convex_optimization_formulation"}, {"score": 0.0037219886365287085, "phrase": "ranking_errors"}, {"score": 0.0035494788628319903, "phrase": "optimization_formulation"}, {"score": 0.0034780071331554003, "phrase": "flexible_framework"}, {"score": 0.003272083495778595, "phrase": "similarity_measures"}, {"score": 0.0030992666061123533, "phrase": "optimization_problem"}, {"score": 0.0029959082125682918, "phrase": "nesterov's_approach"}, {"score": 0.002915701586760994, "phrase": "accelerated_gradient_algorithm"}, {"score": 0.0028569549040495163, "phrase": "fast_convergence_rate"}, {"score": 0.0026335379469691997, "phrase": "proposed_optimization_problem"}, {"score": 0.0025804616931593897, "phrase": "rademacher_complexities"}, {"score": 0.0025456716198597627, "phrase": "extensive_experimental_evaluations"}, {"score": 0.0024607296642187846, "phrase": "public_letor_benchmark_datasets"}, {"score": 0.0023465400668244386, "phrase": "proposed_method"}], "paper_keywords": ["Accelerated gradient algorithm", " feature selection", " generalization bound", " learning to rank"], "paper_abstract": "In recent years, there has been growing interest in learning to rank. The introduction of feature selection into different learning problems has been proven effective. These facts motivate us to investigate the problem of feature selection for learning to rank. We propose a joint convex optimization formulation which minimizes ranking errors while simultaneously conducting feature selection. This optimization formulation provides a flexible framework in which we can easily incorporate various importance measures and similarity measures of the features. To solve this optimization problem, we use the Nesterov's approach to derive an accelerated gradient algorithm with a fast convergence rate O(1/T-2). We further develop a generalization bound for the proposed optimization problem using the Rademacher complexities. Extensive experimental evaluations are conducted on the public LETOR benchmark datasets. The results demonstrate that the proposed method shows: 1) significant ranking performance gain compared to several feature selection baselines for ranking, and 2) very competitive performance compared to several state-of-the-art learning-to-rank algorithms.", "paper_title": "FSMRank: Feature Selection Algorithm for Learning to Rank", "paper_id": "WOS:000317786000008"}