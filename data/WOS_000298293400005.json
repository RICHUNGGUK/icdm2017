{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "risky_actions"}, {"score": 0.004724365740062236, "phrase": "reinforcement_learning_agent"}, {"score": 0.00459165923324494, "phrase": "frequent_damage"}, {"score": 0.0032302647179383915, "phrase": "standard_q-learning_algorithm"}, {"score": 0.0028006988338192375, "phrase": "additional_mechanism"}, {"score": 0.0025103699913760057, "phrase": "worst_results"}, {"score": 0.0024281181691350085, "phrase": "daring_factor"}, {"score": 0.0022608057001451414, "phrase": "autonomous_agent"}, {"score": 0.0022181655497912796, "phrase": "virtual_environment"}, {"score": 0.0021049977753042253, "phrase": "different_daring_degrees"}], "paper_keywords": ["autonomous agent", " decision making system", " fear", " reinforcement learning", " risky actions"], "paper_abstract": "When a reinforcement learning agent executes actions that can cause frequent damage to itself, it can learn, by using Q-learning, that these actions must not be executed again. However, there are other actions that do not cause damage frequently but only once in a while, for example, risky actions such as parachuting. These actions may imply punishment to the agent and, depending on its personality, it would be better to avoid them. Nevertheless, using the standard Q-learning algorithm, the agent is not able to learn to avoid them, because the result of these actions can be positive on average. In this article, an additional mechanism of Q-learning, inspired by the emotion of fear, is introduced in order to deal with those risky actions by considering the worst results. Moreover, there is a daring factor for adjusting the consideration of the risk. This mechanism is implemented on an autonomous agent living in a virtual environment. The results present the performance of the agent with different daring degrees.", "paper_title": "Learning to Avoid Risky Actions", "paper_id": "WOS:000298293400005"}