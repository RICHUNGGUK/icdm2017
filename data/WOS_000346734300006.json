{"auto_keywords": [{"score": 0.03889717436970545, "phrase": "hji_equation"}, {"score": 0.00481495049065317, "phrase": "h-infinity_control_design"}, {"score": 0.0047395356621991935, "phrase": "h-infinity_control_design_problem"}, {"score": 0.004568108391981922, "phrase": "unknown_internal_system_model"}, {"score": 0.004402854153653114, "phrase": "nonlinear_h-infinity_control_problem"}, {"score": 0.004133292222875435, "phrase": "nonlinear_partial_differential_equation"}, {"score": 0.0038597866217852353, "phrase": "model-based_approaches"}, {"score": 0.003623352647973122, "phrase": "accurate_system_model"}, {"score": 0.003330406613112555, "phrase": "off-policy_reinforcement"}, {"score": 0.0031262984171100856, "phrase": "real_system_data"}, {"score": 0.0030772505522663612, "phrase": "mathematical_system_model"}, {"score": 0.0029346624184926305, "phrase": "off-policy_rl_method"}, {"score": 0.002798662730881037, "phrase": "arbitrary_policies"}, {"score": 0.002740253546178098, "phrase": "evaluating_policy"}, {"score": 0.002613239644974825, "phrase": "practical_systems"}, {"score": 0.0025722202862084186, "phrase": "implementation_purpose"}, {"score": 0.0025318431634446426, "phrase": "neural_network"}, {"score": 0.002440071551819324, "phrase": "least-square_nn_weight_update_algorithm"}, {"score": 0.002339246282204109, "phrase": "weighted_residuals"}, {"score": 0.0022783523466965187, "phrase": "developed_nn-based_off-policy_rl_method"}], "paper_keywords": ["H-infinity control design", " Hamilton-Jacobi-Isaacs equation", " neural network", " off-policy learning", " reinforcement learning"], "paper_abstract": "The H-infinity control design problem is considered for nonlinear systems with unknown internal system model. It is known that the nonlinear H-infinity control problem can be transformed into solving the so-called Hamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial differential equation that is generally impossible to be solved analytically. Even worse, model-based approaches cannot be used for approximately solving HJI equation, when the accurate system model is unavailable or costly to obtain in practice. To overcome these difficulties, an off-policy reinforcement leaning (RL) method is introduced to learn the solution of HJI equation from real system data instead of mathematical system model, and its convergence is proved. In the off-policy RL method, the system data can be generated with arbitrary policies rather than the evaluating policy, which is extremely important and promising for practical systems. For implementation purpose, a neural network (NN)-based actor-critic structure is employed and a least-square NN weight update algorithm is derived based on the method of weighted residuals. Finally, the developed NN-based off-policy RL method is tested on a linear F16 aircraft plant, and further applied to a rotational/translational actuator system.", "paper_title": "Off-Policy Reinforcement Learning for H-infinity Control Design", "paper_id": "WOS:000346734300006"}