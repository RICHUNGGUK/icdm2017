{"auto_keywords": [{"score": 0.03784144720028532, "phrase": "bb"}, {"score": 0.015582524873720659, "phrase": "bayesian_binning"}, {"score": 0.014657331577938288, "phrase": "individual_actions"}, {"score": 0.009877424395378304, "phrase": "segmentation_points"}, {"score": 0.00481495049065317, "phrase": "emulating_human_observers"}, {"score": 0.004688989566904871, "phrase": "action_streams"}, {"score": 0.0046477357199085035, "phrase": "natural_body_movements"}, {"score": 0.004526129302830441, "phrase": "temporal_sequences"}, {"score": 0.0044272138900038095, "phrase": "visual_action_analysis"}, {"score": 0.00436890075128947, "phrase": "human_visual_system"}, {"score": 0.004292337783332826, "phrase": "temporal_segmentation"}, {"score": 0.004235793609579307, "phrase": "action_stream"}, {"score": 0.004052616113326177, "phrase": "hierarchical_models"}, {"score": 0.004016938499717623, "phrase": "action_synthesis"}, {"score": 0.0039815737219022675, "phrase": "computer_animation"}, {"score": 0.0037925389501636975, "phrase": "unsupervised_manner"}, {"score": 0.003709595902020469, "phrase": "unsupervised_segmentation_algorithm"}, {"score": 0.003502305158321511, "phrase": "human_segmentations"}, {"score": 0.003456132182564149, "phrase": "psychophysical_data"}, {"score": 0.003335948601271999, "phrase": "observation_model"}, {"score": 0.003191560305761507, "phrase": "exact_bayesian_method"}, {"score": 0.003107935254560724, "phrase": "automatic_determination"}, {"score": 0.0028699415886017468, "phrase": "martial_arts"}, {"score": 0.0026267609106294817, "phrase": "motion_capture_data"}, {"score": 0.0026036031415940563, "phrase": "human_segmentation"}, {"score": 0.002535344521369057, "phrase": "interactive_adjustment_paradigm"}, {"score": 0.002404136142279618, "phrase": "relevant_frames"}, {"score": 0.002351485871160907, "phrase": "good_agreement"}, {"score": 0.0023307492812556204, "phrase": "automatically_generated_segmentations"}, {"score": 0.0023101951336207955, "phrase": "human_performance"}, {"score": 0.0022696277881969896, "phrase": "transition_points"}, {"score": 0.002123730137917387, "phrase": "differential_invariants"}, {"score": 0.0021049977753042253, "phrase": "human_movements"}], "paper_keywords": ["Algorithms", " Human Factors", " Theory", " Motion capture", " action segmentation", " unsupervised learning", " Bayesian methods"], "paper_abstract": "Natural body movements arise in the form of temporal sequences of individual actions. During visual action analysis, the human visual system must accomplish a temporal segmentation of the action stream into individual actions. Such temporal segmentation is also essential to build hierarchical models for action synthesis in computer animation. Ideally, such segmentations should be computed automatically in an unsupervised manner. We present an unsupervised segmentation algorithm that is based on Bayesian Binning (BB) and compare it to human segmentations derived from psychophysical data. BB has the advantage that the observation model can be easily exchanged. Moreover, being an exact Bayesian method, BB allows for the automatic determination of the number and positions of segmentation points. We applied this method to motion capture sequences from martial arts and compared the results to segmentations provided by humans from movies that showed characters that were animated with the motion capture data. Human segmentation was then assessed by an interactive adjustment paradigm, where participants had to indicate segmentation points by selection of the relevant frames. Results show a good agreement between automatically generated segmentations and human performance when the trajectory segments between the transition points were modeled by polynomials of at least third order. This result is consistent with theories about differential invariants of human movements.", "paper_title": "Emulating Human Observers with Bayesian Binning: Segmentation of Action Streams", "paper_id": "WOS:000294815800001"}