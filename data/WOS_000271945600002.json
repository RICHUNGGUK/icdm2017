{"auto_keywords": [{"score": 0.03032744492498588, "phrase": "batu_et_al"}, {"score": 0.025451360000654986, "phrase": "data-stream_model"}, {"score": 0.004822331688916576, "phrase": "entropy"}, {"score": 0.004673079747693193, "phrase": "machine_learning_problems"}, {"score": 0.004434745755769205, "phrase": "arbitrary_points"}, {"score": 0.004385270183517169, "phrase": "high-dimensional_space"}, {"score": 0.004192803336288522, "phrase": "high-dimensional_simplex"}, {"score": 0.0041151131903594445, "phrase": "natural_measures"}, {"score": 0.004023775199194536, "phrase": "information-theoretic_measures"}, {"score": 0.003934478738336462, "phrase": "hellinger"}, {"score": 0.003691969158027291, "phrase": "frequency_moments"}, {"score": 0.00366443843576107, "phrase": "efficient_estimation"}, {"score": 0.003583067515485425, "phrase": "key_component"}, {"score": 0.003154836091281154, "phrase": "sublinear-time_algorithms"}, {"score": 0.0027569106820061707, "phrase": "information-theoretic_sense"}, {"score": 0.002675516297446243, "phrase": "optimal_algorithms"}, {"score": 0.00244543519230361, "phrase": "sublinear-space_algorithms"}, {"score": 0.002260367486652148, "phrase": "aforementioned_oracle_models"}, {"score": 0.002177218756653181, "phrase": "feigenbaum_et_al"}, {"score": 0.002152874480615438, "phrase": "important_additional_component"}, {"score": 0.0021049977753042253, "phrase": "data_streams"}], "paper_keywords": ["Data streams", " entropy", " information divergences", " property testing"], "paper_abstract": "In many data mining and machine learning problems, the data items that need to be clustered or classified are not arbitrary points in a high-dimensional space, but are distributions, that is, points on a high-dimensional simplex. For distributions, natural measures are not l(p) distances, but information-theoretic measures such as the Kullback-Leibler and Hellinger divergences. Similarly, quantities such as the entropy of a distribution are more natural than frequency moments. Efficient estimation of these quantities is a key component in algorithms for manipulating distributions. Since the datasets involved are typically massive, these algorithms need to have only sublinear complexity in order to be feasible in practice. We present a range of sublinear-time algorithms in various oracle models in which the algorithm accesses the data via an oracle that supports various queries. In particular, we answer a question posed by Batu et al. on testing whether two distributions are close in an information-theoretic sense given independent samples. We then present optimal algorithms for estimating various information-divergences and entropy with a more powerful oracle called the combined oracle that was also considered by Batu et al. Finally, we consider sublinear-space algorithms for these quantities in the data-stream model. In the course of doing so, we explore the relationship between the aforementioned oracle models and the data-stream model. This continues work initiated by Feigenbaum et al. An important additional component to the study is considering data streams that are ordered randomly rather than just those which are ordered adversarially.", "paper_title": "Sublinear Estimation of Entropy and Information Distances", "paper_id": "WOS:000271945600002"}