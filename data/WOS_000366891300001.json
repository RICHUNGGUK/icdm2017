{"auto_keywords": [{"score": 0.0483770649744895, "phrase": "biped_robot"}, {"score": 0.0048149540663384284, "phrase": "adjust"}, {"score": 0.004728624975677976, "phrase": "gait_patterns"}, {"score": 0.004116012194714212, "phrase": "observed_walking_pattern"}, {"score": 0.0036700484729006136, "phrase": "defective_walking_pattern"}, {"score": 0.003232956879952914, "phrase": "humanoid_robot"}, {"score": 0.003117890043128058, "phrase": "learning_process"}, {"score": 0.0030069062905274976, "phrase": "action_space"}, {"score": 0.0027298319171606498, "phrase": "refined_pattern"}, {"score": 0.002318799760022913, "phrase": "actual_biped_robot"}, {"score": 0.002169607777109063, "phrase": "successful_walking_policies"}, {"score": 0.0021049977753042253, "phrase": "learning_system"}], "paper_keywords": ["Biped robot", " gait patterns", " Q-learning", " reinforcement learning", " walking balance"], "paper_abstract": "In this paper, a reinforced learning method for biped walking is proposed, where the robot learns to appropriately modulate an observed walking pattern. The biped robot was equipped with two Q-learning mechanisms. First, the robot learns a policy to adjust a defective walking pattern, gait-by-gait, into a more stable one. To avoid the complexity of adjusting too many joints of a humanoid robot and to speed up the learning process, the dimensionality of the action space was reduced. In turn, the other learning mechanism trained the robot to walk in a refined pattern, allowing it to walk faster without the loss of other required criteria, such as walking straight. This approach was implemented with both a simulated robot model and an actual biped robot. The results from the simulations and experiments show that successful walking policies were obtained. The learning system works quickly enough so that the robot was able to continually adapt to the terrain as it walked.", "paper_title": "Learning to Adjust and Refine Gait Patterns for a Biped Robot", "paper_id": "WOS:000366891300001"}