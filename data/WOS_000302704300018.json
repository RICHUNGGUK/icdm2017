{"auto_keywords": [{"score": 0.03954662417587033, "phrase": "hidden_nodes"}, {"score": 0.037208707277005586, "phrase": "new_approach"}, {"score": 0.00481495049065317, "phrase": "universal_approximation_of_extreme_learning_machine"}, {"score": 0.004754600386388821, "phrase": "adaptive_growth_of_hidden_nodes"}, {"score": 0.0044079880067019765, "phrase": "generalized_single-hidden-layer_feedforward_networks"}, {"score": 0.0038124354878360032, "phrase": "adaptive_growth"}, {"score": 0.003467940281748612, "phrase": "automated_design"}, {"score": 0.0031148723927633955, "phrase": "new_hidden_nodes"}, {"score": 0.0027976491158911514, "phrase": "adaptive_way"}, {"score": 0.0026935568995690947, "phrase": "existing_networks"}, {"score": 0.002609770322855842, "phrase": "newly_generated_networks"}, {"score": 0.0025285834281312705, "phrase": "better_generalization_performance"}, {"score": 0.0024038923350585962, "phrase": "lebesgue_p-integrable_hidden_activation_functions"}, {"score": 0.0023438664524018634, "phrase": "lebesgue_p-integrable_function"}, {"score": 0.0022423971916457684, "phrase": "simulation_results"}], "paper_keywords": ["Extreme learning machine (ELM)", " feedforward neural networks", " growing algorithm", " incremental learning", " universal approximation"], "paper_abstract": "Extreme learning machines (ELMs) have been proposed for generalized single-hidden-layer feedforward networks which need not be neuron-like and perform well in both regression and classification applications. In this brief, we propose an ELM with adaptive growth of hidden nodes (AG-ELM), which provides a new approach for the automated design of networks. Different from other incremental ELMs (I-ELMs) whose existing hidden nodes are frozen when the new hidden nodes are added one by one, in AG-ELM the number of hidden nodes is determined in an adaptive way in the sense that the existing networks may be replaced by newly generated networks which have fewer hidden nodes and better generalization performance. We then prove that such an AG-ELM using Lebesgue p-integrable hidden activation functions can approximate any Lebesgue p-integrable function on a compact input set. Simulation results demonstrate and verify that this new approach can achieve a more compact network architecture than the I-ELM.", "paper_title": "Universal Approximation of Extreme Learning Machine with Adaptive Growth of Hidden Nodes", "paper_id": "WOS:000302704300018"}