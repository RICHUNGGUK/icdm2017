{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "learning_linear_predictors"}, {"score": 0.0043258453436258405, "phrase": "tight_sample_complexity"}, {"score": 0.004068907083697369, "phrase": "bounded-norm_linear_predictors"}, {"score": 0.003769006950727621, "phrase": "squared_loss"}, {"score": 0.003438058620195309, "phrase": "agnostic_pac-style_setting"}, {"score": 0.0029951555610569225, "phrase": "data_distribution"}, {"score": 0.002690406375639144, "phrase": "existing_results"}, {"score": 0.0022382002476510573, "phrase": "specific_parameter_settings"}], "paper_keywords": ["sample complexity", " squared loss", " linear predictors", " distribution-free learning"], "paper_abstract": "We provide a tight sample complexity bound for learning bounded-norm linear predictors with respect to the squared loss. Our focus is on an agnostic PAC-style setting, where no assumptions are made on the data distribution beyond boundedness. This contrasts with existing results in the literature, which rely on other distributional assumptions, refer to specific parameter settings, or use other performance measures.", "paper_title": "The Sample Complexity of Learning Linear Predictors with the Squared Loss", "paper_id": "WOS:000369888000037"}