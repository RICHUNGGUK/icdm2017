{"auto_keywords": [{"score": 0.029585393486621675, "phrase": "manual_effort"}, {"score": 0.00481495049065317, "phrase": "active_visual_category_learning"}, {"score": 0.004678785623021003, "phrase": "active_learning_framework"}, {"score": 0.004443276312972159, "phrase": "information_gain"}, {"score": 0.004342430078306461, "phrase": "candidate_image_annotation"}, {"score": 0.004243862930012258, "phrase": "unlabeled_and_partially_labeled_images"}, {"score": 0.0040767015025213625, "phrase": "object_recognition_system"}, {"score": 0.003961330917648707, "phrase": "multi-label_multiple-instance_approach"}, {"score": 0.0038271712887391015, "phrase": "multiple_objects"}, {"score": 0.0034911335714333507, "phrase": "strong_and_weak_labels"}, {"score": 0.0034118234596993836, "phrase": "annotation_cost"}, {"score": 0.003296212583539166, "phrase": "image's_complexity"}, {"score": 0.0031481164109237636, "phrase": "active_selection"}, {"score": 0.0029722900280316216, "phrase": "unlabeled_image"}, {"score": 0.002822434555587263, "phrase": "optimal_use"}, {"score": 0.002649472341607234, "phrase": "multiple_levels"}, {"score": 0.0025449518236394103, "phrase": "accurate_prediction"}, {"score": 0.002321234686549485, "phrase": "lower_total_expenditure"}, {"score": 0.002294686545883412, "phrase": "annotation_effort"}, {"score": 0.0022424956516376073, "phrase": "small_initial_pool"}, {"score": 0.0022168460543913787, "phrase": "labeled_data"}, {"score": 0.0021789194955896124, "phrase": "proposed_method"}, {"score": 0.002129356014992349, "phrase": "category_models"}, {"score": 0.0021049977753042253, "phrase": "minimal_manual_intervention"}], "paper_keywords": ["Visual category learning", " Active learning", " Multi-label", " Multiple-instance learning", " Cost prediction", " Cost sensitive learning", " Object recognition"], "paper_abstract": "We present an active learning framework that predicts the tradeoff between the effort and information gain associated with a candidate image annotation, thereby ranking unlabeled and partially labeled images according to their expected \"net worth\" to an object recognition system. We develop a multi-label multiple-instance approach that accommodates realistic images containing multiple objects and allows the category-learner to strategically choose what annotations it receives from a mixture of strong and weak labels. Since the annotation cost can vary depending on an image's complexity, we show how to improve the active selection by directly predicting the time required to segment an unlabeled image. Our approach accounts for the fact that the optimal use of manual effort may call for a combination of labels at multiple levels of granularity, as well as accurate prediction of manual effort. As a result, it is possible to learn more accurate category models with a lower total expenditure of annotation effort. Given a small initial pool of labeled data, the proposed method actively improves the category models with minimal manual intervention.", "paper_title": "Cost-Sensitive Active Visual Category Learning", "paper_id": "WOS:000286118400002"}