{"auto_keywords": [{"score": 0.0369267499271085, "phrase": "bayesian"}, {"score": 0.005762510802069811, "phrase": "abc"}, {"score": 0.00481495049065317, "phrase": "adaptive_abc"}, {"score": 0.0046018264570209765, "phrase": "hidden_gibbs_random_fields"}, {"score": 0.004498812433096501, "phrase": "different_dependency_structures"}, {"score": 0.004448169552315403, "phrase": "hidden_markov_random_field"}, {"score": 0.003949508242904001, "phrase": "approximate_bayesian_computation"}, {"score": 0.0037959868107895053, "phrase": "model_choice_method"}, {"score": 0.0037320255482878365, "phrase": "bayesian_paradigm"}, {"score": 0.0035666632046674153, "phrase": "grelaud_et_al"}, {"score": 0.003332210257373994, "phrase": "sufficient_statistics"}, {"score": 0.0032946562900625187, "phrase": "directly_observed_gibbs_random_fields"}, {"score": 0.0032026066791743866, "phrase": "random_field"}, {"score": 0.002975099633631871, "phrase": "geometric_summary_statistics"}, {"score": 0.0029249294679606656, "phrase": "general_approach"}, {"score": 0.002859345731792013, "phrase": "intuitive_statistics"}, {"score": 0.002795228413298423, "phrase": "clustering_analysis"}, {"score": 0.0026864540069792275, "phrase": "observed_colors"}, {"score": 0.002656158309155626, "phrase": "plausible_latent_graphs"}, {"score": 0.0025819015229390663, "phrase": "abc_model_choice"}, {"score": 0.0024534188907003726, "phrase": "local_error_rate"}, {"score": 0.0023848164282879885, "phrase": "independent_interest"}, {"score": 0.002279011030331704, "phrase": "abc_algorithm"}, {"score": 0.002190280748985751, "phrase": "summary_statistics"}, {"score": 0.0021049977753042253, "phrase": "model_selection"}], "paper_keywords": ["Approximate Bayesian computation", " Model choice", " Hidden Gibbs random fields", " Summary statistics", " Misclassification rate", " k-nearest neighbors"], "paper_abstract": "Selecting between different dependency structures of hidden Markov random field can be very challenging, due to the intractable normalizing constant in the likelihood. We answer this question with approximate Bayesian computation (ABC) which provides a model choice method in the Bayesian paradigm. This comes after the work of Grelaud et al. (Bayesian Anal, 4(2):317-336, 2009) who exhibited sufficient statistics on directly observed Gibbs random fields. But when the random field is latent, the sufficiency falls and we complement the set with geometric summary statistics. The general approach to construct these intuitive statistics relies on a clustering analysis of the sites based on the observed colors and plausible latent graphs. The efficiency of ABC model choice based on these statistics is evaluated via a local error rate which may be of independent interest. As a byproduct we derived an ABC algorithm that adapts the dimension of the summary statistics to the dataset without distorting the model selection.", "paper_title": "Adaptive ABC model choice and geometric summary statistics for hidden Gibbs random fields", "paper_id": "WOS:000349028500019"}