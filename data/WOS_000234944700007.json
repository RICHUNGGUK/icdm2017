{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "error_exponents"}, {"score": 0.006713214610244314, "phrase": "awgn_channel"}, {"score": 0.0047555023078686386, "phrase": "module_lattice_additive_noise_channels"}, {"score": 0.00469678464293492, "phrase": "modulo_lattice_additive_noise"}, {"score": 0.004413846562278824, "phrase": "structured_binning_codes"}, {"score": 0.004359328949187473, "phrase": "costa's_dirty-paper_channel"}, {"score": 0.0041478818670853115, "phrase": "additive_white_gaussian_noise"}, {"score": 0.003640183972131238, "phrase": "mlan_channel"}, {"score": 0.0035507457285983268, "phrase": "proper_choice"}, {"score": 0.0034851070574149993, "phrase": "shaping_lattice"}, {"score": 0.0034206776096452015, "phrase": "scaling_parameter"}, {"score": 0.0027857115172243226, "phrase": "sphere-packing_and_straight-line_regions"}, {"score": 0.0025691622090542304, "phrase": "channel_capacity"}, {"score": 0.002474961375646351, "phrase": "dirty_paper"}, {"score": 0.0023694065215394593, "phrase": "clean_paper"}, {"score": 0.0022967713922705, "phrase": "lattice_encoding"}, {"score": 0.0021446993452098597, "phrase": "optimal_codes"}, {"score": 0.0021049977753042253, "phrase": "maximum-likelihood_decoding"}], "paper_keywords": ["additive white Gaussian noise (AWGN) channel", " Costa's dirty-paper channel", " error exponents", " lattice decoding", " modulo lattice additive noise (MLAN) channel", " nested lattice codes"], "paper_abstract": "Modulo lattice additive noise (MLAN) channels appear in the analysis of structured binning codes for Costa's dirty-paper channel and of nested lattice codes for the additive white Gaussian noise (AWGN) channel. In this paper, we derive a new lower bound on the error exponents of the MLAN channel. With a proper choice of the shaping lattice and the scaling parameter, the new lower bound coincides with the random-coding lower bound on the error exponents of the AWGN channel at the same signal-to-noise ratio (SNR) in the sphere-packing and straight-line regions. This result implies that, at least for rates close to channel capacity, 1) writing on dirty paper is as reliable as writing on clean paper; and 2) lattice encoding and decoding suffer no loss of error exponents relative to the optimal codes (with maximum-likelihood decoding) for the AWGN channel.", "paper_title": "On error exponents of module lattice additive noise channels", "paper_id": "WOS:000234944700007"}