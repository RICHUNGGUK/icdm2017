{"auto_keywords": [{"score": 0.04263334351221679, "phrase": "jacobs"}, {"score": 0.00481495049065317, "phrase": "experts_model"}, {"score": 0.004749824776508639, "phrase": "cooperative_coevolution"}, {"score": 0.00460125407969429, "phrase": "generalization_performance"}, {"score": 0.004437115547184535, "phrase": "single_network"}, {"score": 0.004278807087784626, "phrase": "largely_open_question"}, {"score": 0.004144908086646161, "phrase": "suitable_combination"}, {"score": 0.0037506567448470163, "phrase": "neural_networks"}, {"score": 0.0036497740708342093, "phrase": "gate_network"}, {"score": 0.003600351201298387, "phrase": "tight_coupling_mechanism"}, {"score": 0.0034092396400930446, "phrase": "individual_neural_networks"}, {"score": 0.0033326252993345685, "phrase": "different_regions"}, {"score": 0.003287482882222731, "phrase": "input_space"}, {"score": 0.003170060683860338, "phrase": "\"good\"_combination_weights"}, {"score": 0.0029610472910087176, "phrase": "dynamic_weights"}, {"score": 0.002778385486855157, "phrase": "cooperative_coevolutionary"}, {"score": 0.0026913130726432645, "phrase": "basic_me_model"}, {"score": 0.0026676287652743712, "phrase": "ccme"}, {"score": 0.0026548344763838213, "phrase": "cc_layer"}, {"score": 0.0026307897899760383, "phrase": "better_exploration"}, {"score": 0.0025951294037626174, "phrase": "weight_space"}, {"score": 0.0023802126605244438, "phrase": "original_me"}, {"score": 0.002326669201839705, "phrase": "classification_problems"}, {"score": 0.0022537212268979507, "phrase": "novel_mechanism"}, {"score": 0.002213065910356615, "phrase": "modular_structures"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["neuro-ensemble", " artificial neural networks", " mixture of experts", " cooperative coevolution"], "paper_abstract": "Combining several suitable neural networks can enhance the generalization performance of the group when compared to a single network alone. However, it remains a largely open question, how best to build a suitable combination of individuals. Jacobs and his colleagues proposed the mixture of experts (ME) model, in which a set of neural networks are trained together with a gate network. This tight coupling mechanism enables the system to (i) encourage diversity between the individual neural networks by specializing them in different regions of the input space and (ii) allow for a \"good\" combination weights of the ensemble members to emerge by training the gate, which computes the dynamic weights together with the classifiers. In this paper, we have wrapped a cooperative coevolutionary (CC) algorithm around the basic ME model. This CC layer allows better exploration of the weight space, and hence, an ensemble with better performance. The results show that CCME is better on average than the original ME on a number of classification problems. We have also introduced a novel mechanism for visualizing the modular structures that emerged from the model. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "A novel mixture of experts model based on cooperative coevolution", "paper_id": "WOS:000242602300017"}