{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cost-sensitive_regression"}, {"score": 0.00469445816161498, "phrase": "predictive_models"}, {"score": 0.004600228893803583, "phrase": "full_use"}, {"score": 0.004530794611704416, "phrase": "available_contextual_information"}, {"score": 0.0044398358177815305, "phrase": "operating_context"}, {"score": 0.004073113268289532, "phrase": "global_reframing_solutions"}, {"score": 0.003871653343842317, "phrase": "estimated_outputs"}, {"score": 0.0038131738052230254, "phrase": "new_cost_context"}, {"score": 0.0037555742428544096, "phrase": "possible_solutions"}, {"score": 0.0036801209922064817, "phrase": "alternative_approach"}, {"score": 0.00351582806482809, "phrase": "comprehensive_way"}, {"score": 0.0034277338119963886, "phrase": "knowledge_discovery"}, {"score": 0.003393115177189967, "phrase": "data_mining_literature"}, {"score": 0.0031284221314354095, "phrase": "estimated_output"}, {"score": 0.002672750634553146, "phrase": "normal_conditional_probability_density"}, {"score": 0.002618995204424126, "phrase": "conditional_mean"}, {"score": 0.0025663181430010686, "phrase": "regression_technique"}, {"score": 0.0025146979283050923, "phrase": "lightweight_\"enrichment\"_methods"}, {"score": 0.002426842369975213, "phrase": "conditional_variance"}, {"score": 0.002226017732566121, "phrase": "cost-sensitive_problems"}, {"score": 0.002181227522730908, "phrase": "optimal_predictions"}, {"score": 0.0021373366163721518, "phrase": "asymmetric_loss_scenarios"}, {"score": 0.0021049977753042253, "phrase": "rejection_rules"}], "paper_keywords": ["Cost-sensitive regression", " conditional density estimation", " reframing", " reliability estimation in regression", " asymmetric loss", " calibration"], "paper_abstract": "Common-day applications of predictive models usually involve the full use of the available contextual information. When the operating context changes, one may fine-tune the by-default (incontextual) prediction or may even abstain from predicting a value (a reject). Global reframing solutions, where the same function is applied to adapt the estimated outputs to a new cost context, are possible solutions here. An alternative approach, which has not been studied in a comprehensive way for regression in the knowledge discovery and data mining literature, is the use of a local (e. g., probabilistic) reframing approach, where decisions are made according to the estimated output and a reliability, confidence, or probability estimation. In this article, we advocate for a simple two-parameter (mean and variance) approach, working with a normal conditional probability density. Given the conditional mean produced by any regression technique, we develop lightweight \"enrichment\" methods that produce good estimates of the conditional variance, which are used by the probabilistic (local) reframing methods. We apply these methods to some very common families of cost-sensitive problems, such as optimal predictions in (auction) bids, asymmetric loss scenarios, and rejection rules.", "paper_title": "Probabilistic Reframing for Cost-Sensitive Regression", "paper_id": "WOS:000344352800001"}