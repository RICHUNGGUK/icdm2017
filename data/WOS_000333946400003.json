{"auto_keywords": [{"score": 0.04671359185365177, "phrase": "square_algorithm"}, {"score": 0.04579236087036461, "phrase": "online_learning"}, {"score": 0.00481495049065317, "phrase": "kernel_regularized_least_mean_square_algorithms"}, {"score": 0.004594951819987416, "phrase": "novel_type"}, {"score": 0.004430784015995566, "phrase": "regularized_structural_risk"}, {"score": 0.00422826362023938, "phrase": "continuous_growing"}, {"score": 0.004184526032558792, "phrase": "kernel_functions"}, {"score": 0.004119762981892093, "phrase": "new_dictionary_selection_method"}, {"score": 0.004034962424068988, "phrase": "cumulative_coherence_measure"}, {"score": 0.003911010373894104, "phrase": "sparsification_procedure"}, {"score": 0.0037516215763875225, "phrase": "diagonally_dominant_gram_matrix"}, {"score": 0.0035614563596982306, "phrase": "kernel_weight"}, {"score": 0.0033633514001658086, "phrase": "reproducing_kernel_hilbert_space"}, {"score": 0.0032769720645483102, "phrase": "minimized_updating_structural_risk"}, {"score": 0.0029994894380575604, "phrase": "krlms_algorithm"}, {"score": 0.0027311809666295565, "phrase": "computational_complexity"}, {"score": 0.002702887319160532, "phrase": "theoretical_analysis"}, {"score": 0.002619748321217153, "phrase": "variable_learning_rates"}, {"score": 0.0025524176761139413, "phrase": "training_process"}, {"score": 0.0024868131939261716, "phrase": "weight_convergence"}, {"score": 0.002385326947545194, "phrase": "bounded_measurement_error"}, {"score": 0.0022407927161620855, "phrase": "proposed_algorithm"}, {"score": 0.002171836484200008, "phrase": "existing_kernel_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Kernel method", " Dictionary", " Cumulative coherence", " Diagonally dominant", " Weight convergence"], "paper_abstract": "In this paper, we propose a novel type of kernel least mean square algorithm with regularized structural risk for online learning. In order to curb the continuous growing of kernel functions, a new dictionary selection method based on the cumulative coherence measure is applied to perform the sparsification procedure, which can obtain a dictionary with diagonally dominant Gram matrix under certain conditions. On the updating of the kernel weight, the linear least mean square algorithm is generalized into the reproducing kernel Hilbert space (RKHS) with minimized updating structural risk and it results in a kernel regularized least mean square (KRLMS) algorithm. A simplified version of the KRLMS algorithm is also presented by applying only partial updating information to train the algorithm at each iteration, which reduces the computational complexity. Theoretical analysis of their convergence issues is examined and variable learning rates are adopted in the training process which can guarantee the weight convergence of the algorithm in terms of a bounded measurement error. Several experiments are carried out to prove the effectiveness of the proposed algorithm for online learning compared to some existing kernel algorithms. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Online learning with kernel regularized least mean square algorithms", "paper_id": "WOS:000333946400003"}