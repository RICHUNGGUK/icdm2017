{"auto_keywords": [{"score": 0.026778966405330708, "phrase": "santa_clara"}, {"score": 0.026584697332684374, "phrase": "ca"}, {"score": 0.026366651003901516, "phrase": "usa"}, {"score": 0.00481495049065317, "phrase": "cpu-gpu_systems"}, {"score": 0.004719816332271661, "phrase": "graphics_processing_units"}, {"score": 0.004626553103375699, "phrase": "high-performance_computing"}, {"score": 0.004571478018964363, "phrase": "growing_momentum"}, {"score": 0.004463278123628969, "phrase": "gpu-programming_platforms"}, {"score": 0.0044277810648955624, "phrase": "compute_unified_device_architecture"}, {"score": 0.004153739859461064, "phrase": "high-performance_parallel_applications"}, {"score": 0.00407161650109722, "phrase": "runtime_systems"}, {"score": 0.0038655854071187115, "phrase": "modern_cpu-gpu_systems"}, {"score": 0.0038043055953253047, "phrase": "parallel_kernels"}, {"score": 0.0030781126453776723, "phrase": "available_devices"}, {"score": 0.00287583918696114, "phrase": "single_high-level_abstraction"}, {"score": 0.002818909613625567, "phrase": "heterogeneous_cpu-gpu_systems"}, {"score": 0.0026336693577870096, "phrase": "gpu."}, {"score": 0.0026022112631368223, "phrase": "parallel_skeletons"}, {"score": 0.002560907803820395, "phrase": "intel_threading_building_blocks"}, {"score": 0.0025405019456703325, "phrase": "intel_corporation"}, {"score": 0.0024506642262508735, "phrase": "nvidia_cuda"}, {"score": 0.0024311346712789553, "phrase": "nvidia_corporation"}, {"score": 0.0022895278869126848, "phrase": "parallel_applications"}, {"score": 0.0022352169832099153, "phrase": "average_performance"}, {"score": 0.0021734768728417977, "phrase": "cpu-only_and_gpu-only_parallel_applications"}], "paper_keywords": ["stencil pattern", " parallel skeletons", " task partitioning", " CPU-GPU systems", " TBB", " CUDA"], "paper_abstract": "The use of Graphics Processing Units (GPUs) for high-performance computing has gained growing momentum in recent years. Unfortunately, GPU-programming platforms like Compute Unified Device Architecture (CUDA) are complex, user unfriendly, and increase the complexity of developing high-performance parallel applications. In addition, runtime systems that execute those applications often fail to fully utilize the parallelism of modern CPU-GPU systems. Typically, parallel kernels run entirely on the most powerful device available, leaving other devices idle. These observations sparked research in two directions: (1) high-level approaches to software development for GPUs, which strike a balance between performance and ease of programming; and (2) task partitioning to fully utilize the available devices. In this paper, we propose a framework, called PSkel, that provides a single high-level abstraction for stencil programming on heterogeneous CPU-GPU systems, while allowing the programmer to partition and assign data and computation to both CPU and GPU. Our current implementation uses parallel skeletons to transparently leverage Intel Threading Building Blocks (Intel Corporation, Santa Clara, CA, USA) and NVIDIA CUDA (Nvidia Corporation, Santa Clara, CA, USA). In our experiments, we observed that parallel applications with task partitioning can improve average performance by up to 76% and 28% compared with CPU-only and GPU-only parallel applications, respectively. Copyright (c) 2015John Wiley & Sons, Ltd.", "paper_title": "PSkel: A stencil programming framework for CPU-GPU systems", "paper_id": "WOS:000363766600022"}