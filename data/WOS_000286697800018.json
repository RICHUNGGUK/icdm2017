{"auto_keywords": [{"score": 0.049031285206733555, "phrase": "nearest_neighbor_classification"}, {"score": 0.00481495049065317, "phrase": "sample_weight_learning"}, {"score": 0.004606479650820579, "phrase": "simple_and_yet_effective_technique"}, {"score": 0.004185108816866677, "phrase": "distance_function"}, {"score": 0.0036105923401756126, "phrase": "distance_structure"}, {"score": 0.0032081215771180664, "phrase": "sample_weights"}, {"score": 0.0030464123943266673, "phrase": "gradient_descent_algorithm"}, {"score": 0.0029796206196589115, "phrase": "margin_based_classification_loss"}, {"score": 0.002551231163230566, "phrase": "hypothesis_margin"}, {"score": 0.002422552850701705, "phrase": "proposed_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Nearest neighbor", " Sample weighting", " Loss function", " Large margin"], "paper_abstract": "The nearest neighbor classification is a simple and yet effective technique for pattern recognition. Performance of this technique depends significantly on the distance function used to compute similarity between examples. Some techniques were developed to learn weights of features for changing the distance structure of samples in nearest neighbor classification. In this paper, we propose an approach to learning sample weights for enlarging margin by using a gradient descent algorithm to minimize margin based classification loss. Experimental analysis shows that the distances trained in this way reduce the loss of the margin and enlarge the hypothesis margin on several datasets. Moreover, the proposed approach consistently outperforms nearest neighbor classification and some other state-of-the-art methods. (c) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Large-margin nearest neighbor classifiers via sample weight learning", "paper_id": "WOS:000286697800018"}