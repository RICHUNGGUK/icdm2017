{"auto_keywords": [{"score": 0.03510429867928044, "phrase": "user's_labeling_information"}, {"score": 0.00481495049065317, "phrase": "sparse_transfer_learning_for_interactive_video_search_reranking"}, {"score": 0.0047588556219929756, "phrase": "visual_reranking"}, {"score": 0.0045143706249204905, "phrase": "text-based_video_search"}, {"score": 0.00440976345264104, "phrase": "existing_reranking_algorithms"}, {"score": 0.004307569753106432, "phrase": "limited_improvement"}, {"score": 0.004207734323806386, "phrase": "well-known_semantic_gap"}, {"score": 0.0041586840496756474, "phrase": "low-level_visual_features"}, {"score": 0.0041102032028337366, "phrase": "high-level_semantic_concepts"}, {"score": 0.0039218440805789965, "phrase": "interactive_video_search"}, {"score": 0.0038085117064624208, "phrase": "semantic_gap"}, {"score": 0.003742084495435749, "phrase": "user's_labeling_effort"}, {"score": 0.003633927813203731, "phrase": "novel_dimension_reduction_tool"}, {"score": 0.0032889759660990323, "phrase": "stl"}, {"score": 0.0031938648476635225, "phrase": "interactive_video_search_reranking"}, {"score": 0.0030118058266380503, "phrase": "pair-wise_discriminative_information"}, {"score": 0.0029592333031063156, "phrase": "separate_labeled_query_relevant_samples"}, {"score": 0.002890557197908352, "phrase": "irrelevant_ones"}, {"score": 0.0027741761880779535, "phrase": "sparse_representation"}, {"score": 0.0026781477395106993, "phrase": "user's_intention"}, {"score": 0.002615977783510631, "phrase": "elastic_net_penalty"}, {"score": 0.002452333524620638, "phrase": "unlabeled_samples"}, {"score": 0.0023953929817357882, "phrase": "data_distribution_knowledge"}, {"score": 0.002339771435910601, "phrase": "extensive_experiments"}, {"score": 0.002272053079580848, "phrase": "benchmark_datasets"}, {"score": 0.002206290313627494, "phrase": "popular_dimension_reduction_algorithms"}, {"score": 0.0021550502873107654, "phrase": "superior_performance"}, {"score": 0.0021049977753042253, "phrase": "proposed_stl-based_interactive_video_search_reranking"}], "paper_keywords": ["Algorithm", " Experimentation", " Performance", " Interactive video search reranking", " dimension reduction", " transfer learning", " sparsity"], "paper_abstract": "Visual reranking is effective to improve the performance of the text-based video search. However, existing reranking algorithms can only achieve limited improvement because of the well-known semantic gap between low-level visual features and high-level semantic concepts. In this article, we adopt interactive video search reranking to bridge the semantic gap by introducing user's labeling effort. We propose a novel dimension reduction tool, termed sparse transfer learning (STL), to effectively and efficiently encode user's labeling information. STL is particularly designed for interactive video search reranking. Technically, it (a) considers the pair-wise discriminative information to maximally separate labeled query relevant samples from labeled query irrelevant ones, (b) achieves a sparse representation for the subspace to encodes user's intention by applying the elastic net penalty, and (c) propagates user's labeling information from labeled samples to unlabeled samples by using the data distribution knowledge. We conducted extensive experiments on the TRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular dimension reduction algorithms. We report superior performance by using the proposed STL-based interactive video search reranking.", "paper_title": "Sparse Transfer Learning for Interactive Video Search Reranking", "paper_id": "WOS:000307311700003"}