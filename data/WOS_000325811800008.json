{"auto_keywords": [{"score": 0.04959024199151092, "phrase": "fusion"}, {"score": 0.00481495049065317, "phrase": "multimodal_saliency"}, {"score": 0.004717125879761534, "phrase": "movie_summarization"}, {"score": 0.004574085781766281, "phrase": "textual_attention"}, {"score": 0.004527371799277529, "phrase": "multimodal_streams"}, {"score": 0.004481132747216135, "phrase": "sensory_information"}, {"score": 0.004278807087784626, "phrase": "signal-level_feature_extraction_and_higher_level_cognitive_processes"}, {"score": 0.0039010428828989826, "phrase": "saliency_models"}, {"score": 0.0036678472439508484, "phrase": "auditory_saliency"}, {"score": 0.0035382855372646164, "phrase": "multifrequency_waveform_modulations"}, {"score": 0.0034663083890589235, "phrase": "nonlinear_operators"}, {"score": 0.0034308688638878286, "phrase": "energy_tracking"}, {"score": 0.0033957904376127187, "phrase": "visual_saliency"}, {"score": 0.0033096502887394233, "phrase": "spatiotemporal_attention_model"}, {"score": 0.003143849342926373, "phrase": "textual_or_linguistic_saliency"}, {"score": 0.002955782731675529, "phrase": "subtitles_information"}, {"score": 0.0028659990472580154, "phrase": "individual_saliency_streams"}, {"score": 0.0028076592725295646, "phrase": "modality-depended_cues"}, {"score": 0.0027223626424366207, "phrase": "multimodal_saliency_curve"}, {"score": 0.00266693888510356, "phrase": "time-varying_perceptual_importance"}, {"score": 0.0026261108615242557, "phrase": "composite_video_stream"}, {"score": 0.002585906251354904, "phrase": "prevailing_sensory_events"}, {"score": 0.002546315584783112, "phrase": "multimodal_saliency_representation"}, {"score": 0.002357249810661667, "phrase": "movie_database"}, {"score": 0.0023331227492950422, "phrase": "multimodal_saliency_annotations"}, {"score": 0.002309242063033298, "phrase": "comparative_results"}, {"score": 0.0022390533383717715, "phrase": "produced_summaries"}, {"score": 0.002193447487477984, "phrase": "low-level_features"}, {"score": 0.0021709933368474223, "phrase": "content-independent_fusion"}, {"score": 0.0021049977753042253, "phrase": "subjectively_high_aesthetic_and_informative_quality"}], "paper_keywords": ["Attention", " audio saliency", " fusion", " movie summarization", " multimodal saliency", " multistream processing", " text saliency", " video summarization", " visual saliency"], "paper_abstract": "Multimodal streams of sensory information are naturally parsed and integrated by humans using signal-level feature extraction and higher level cognitive processes. Detection of attention-invoking audiovisual segments is formulated in this work on the basis of saliency models for the audio, visual, and textual information conveyed in a video stream. Aural or auditory saliency is assessed by cues that quantify multifrequency waveform modulations, extracted through nonlinear operators and energy tracking. Visual saliency is measured through a spatiotemporal attention model driven by intensity, color, and orientation. Textual or linguistic saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions. The individual saliency streams, obtained from modality-depended cues, are integrated in a multimodal saliency curve, modeling the time-varying perceptual importance of the composite video stream and signifying prevailing sensory events. The multimodal saliency representation forms the basis of a generic, bottom-up video summarization algorithm. Different fusion schemes are evaluated on a movie database of multimodal saliency annotations with comparative results provided across modalities. The produced summaries, based on low-level features and content-independent fusion and selection, are of subjectively high aesthetic and informative quality.", "paper_title": "Multimodal Saliency and Fusion for Movie Summarization Based on Aural, Visual, and Textual Attention", "paper_id": "WOS:000325811800008"}