{"auto_keywords": [{"score": 0.03912269620023402, "phrase": "kernel_methods"}, {"score": 0.00481495049065317, "phrase": "tensorial_data_analysis"}, {"score": 0.004774530554776608, "phrase": "tensor-based_techniques"}, {"score": 0.004577440795549049, "phrase": "carefully_chosen_representations"}, {"score": 0.004444317282171449, "phrase": "desirable_feature"}, {"score": 0.0043150485532080065, "phrase": "training_patterns"}, {"score": 0.004102096151528319, "phrase": "biosignal_processing"}, {"score": 0.003949280009382125, "phrase": "tensor-based_models"}, {"score": 0.003818210967854446, "phrase": "limited_discriminative_power"}, {"score": 0.0037543106253910313, "phrase": "different_track"}, {"score": 0.0036759314172445934, "phrase": "flexible_nonlinear_models"}, {"score": 0.0034944094552943, "phrase": "naive_application"}, {"score": 0.003407026489727146, "phrase": "structural_properties"}, {"score": 0.003144426503333397, "phrase": "non-parametric_tensor-based_models"}, {"score": 0.003027178161479457, "phrase": "discriminative_power"}, {"score": 0.0030017207229983385, "phrase": "supervised_tensor-based_models"}, {"score": 0.002939007076062123, "phrase": "structural_information"}, {"score": 0.002805597755796506, "phrase": "feature_space"}, {"score": 0.002770273247998903, "phrase": "multilinear_functionals"}, {"score": 0.00266693888510356, "phrase": "infinite_dimensional_analogue"}, {"score": 0.0025458479274370832, "phrase": "input_patterns"}, {"score": 0.0024405312482831646, "phrase": "algebraic_structure"}, {"score": 0.002419995316129757, "phrase": "data_tensors"}, {"score": 0.0023296961971785357, "phrase": "mlsvd"}, {"score": 0.0022906498741695094, "phrase": "interesting_invariance_property"}, {"score": 0.0021590588269751816, "phrase": "svm-like_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multilinear algebra", " Reproducing kernel Hilbert spaces", " Tensorial kernels", " Subspace angles"], "paper_abstract": "Tensor-based techniques for learning allow one to exploit the structure of carefully chosen representations of data. This is a desirable feature in particular when the number of training patterns is small which is often the case in areas such as biosignal processing and chemometrics. However, the class of tensor-based models is somewhat restricted and might suffer from limited discriminative power. On a different track, kernel methods lead to flexible nonlinear models that have been proven successful in many different contexts. Nonetheless, a naive application of kernel methods does not exploit structural properties possessed by the given tensorial representations. The goal of this work is to go beyond this limitation by introducing non-parametric tensor-based models. The proposed framework aims at improving the discriminative power of supervised tensor-based models while still exploiting the structural information embodied in the data. We begin by introducing a feature space formed by multilinear functionals. The latter can be considered as the infinite dimensional analogue of tensors. Successively we show how to implicitly map input patterns in such a feature space by means of kernels that exploit the algebraic structure of data tensors. The proposed tensorial kernel links to the MLSVD and features an interesting invariance property; the approach leads to convex optimization and fits into the same primal-dual framework underlying SVM-like algorithms. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "A kernel-based framework to tensorial data analysis", "paper_id": "WOS:000295105700010"}