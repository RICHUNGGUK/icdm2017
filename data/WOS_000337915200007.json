{"auto_keywords": [{"score": 0.04635903482246181, "phrase": "regression_coefficients"}, {"score": 0.038533818161098556, "phrase": "mcem"}, {"score": 0.015541483645644942, "phrase": "bayesian_lasso"}, {"score": 0.014679788740446821, "phrase": "marginal_posterior_mode"}, {"score": 0.00481495049065317, "phrase": "carlo_em"}, {"score": 0.004651271219032404, "phrase": "popular_technique"}, {"score": 0.004615656893404919, "phrase": "simultaneous_estimation"}, {"score": 0.004580314008293946, "phrase": "variable_selection"}, {"score": 0.004307099985737249, "phrase": "non-bayesian_lasso"}, {"score": 0.0042250952759280225, "phrase": "independent_laplace_priors"}, {"score": 0.0041287394089001405, "phrase": "statistical_inferences"}, {"score": 0.004081385289516914, "phrase": "bayesian_approach"}, {"score": 0.0040190868373730015, "phrase": "growing_body"}, {"score": 0.0039274105882136775, "phrase": "current_approaches"}, {"score": 0.0038230844545071303, "phrase": "fully_bayesian_analysis"}, {"score": 0.0037937873032347, "phrase": "markov_chain_monte_carlo"}, {"score": 0.003692997523960715, "phrase": "monte_carlo_expectation_maximization"}, {"score": 0.0035948757703015287, "phrase": "mcmc_algorithm"}, {"score": 0.0034993519231560637, "phrase": "mcmc-based_bayesian_method"}, {"score": 0.0034194902384248006, "phrase": "slow_convergence"}, {"score": 0.0032526334220034326, "phrase": "generalized_linear"}, {"score": 0.003240139593712436, "phrase": "mixed_models"}, {"score": 0.003215295260684715, "phrase": "correlated_binary_data"}, {"score": 0.003011641380351574, "phrase": "non-iterative_sampling_approach"}, {"score": 0.002977061391551628, "phrase": "inverse_bayes_formula"}, {"score": 0.002954229816271602, "phrase": "ibf"}, {"score": 0.002853620962601042, "phrase": "hierarchical_model"}, {"score": 0.0027992118597278087, "phrase": "mcem._motivated"}, {"score": 0.0027247727563746694, "phrase": "ibf_sampler"}, {"score": 0.0025034338848015187, "phrase": "importance_sampling"}, {"score": 0.002465161736207254, "phrase": "full_conditional_distribution"}, {"score": 0.0024181413029825205, "phrase": "simulation_experiments"}, {"score": 0.002381169982803105, "phrase": "computational_time"}, {"score": 0.00230003335593664, "phrase": "expectation_maximization_algorithm"}, {"score": 0.0021961259795489833, "phrase": "prediction_accuracy"}, {"score": 0.0021625417261202603, "phrase": "variable_selection_accuracy"}, {"score": 0.0021049977753042253, "phrase": "sample_size"}], "paper_keywords": ["Bayesian analysis", " lasso", " Monte Carlo EM algorithm", " inverse Bayes formulae", " importance sampling"], "paper_abstract": "The lasso is a popular technique of simultaneous estimation and variable selection in many research areas. The marginal posterior mode of the regression coefficients is equivalent to estimates given by the non-Bayesian lasso when the regression coefficients have independent Laplace priors. Because of its flexibility of statistical inferences, the Bayesian approach is attracting a growing body of research in recent years. Current approaches are primarily to either do a fully Bayesian analysis using Markov chain Monte Carlo (MCMC) algorithm or use Monte Carlo expectation maximization (MCEM) methods with an MCMC algorithm in each E-step. However, MCMC-based Bayesian method has much computational burden and slow convergence. Tan et al. [An efficient MCEM algorithm for fitting generalized linear mixed models for correlated binary data. J Stat Comput Simul. 2007; 77: 929-943] proposed a non-iterative sampling approach, the inverse Bayes formula (IBF) sampler, for computing posteriors of a hierarchical model in the structure of MCEM. Motivated by their paper, we develop this IBF sampler in the structure of MCEM to give the marginal posterior mode of the regression coefficients for the Bayesian lasso, by adjusting the weights of importance sampling, when the full conditional distribution is not explicit. Simulation experiments show that the computational time is much reduced with our method based on the expectation maximization algorithm and our algorithms and our methods behave comparably with other Bayesian lasso methods not only in prediction accuracy but also in variable selection accuracy and even better especially when the sample size is relatively large.", "paper_title": "An efficient Monte Carlo EM algorithm for Bayesian lasso", "paper_id": "WOS:000337915200007"}