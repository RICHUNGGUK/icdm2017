{"auto_keywords": [{"score": 0.04966447721457639, "phrase": "maximum_entropy"}, {"score": 0.04438340620704104, "phrase": "joint_distribution"}, {"score": 0.00481495049065317, "phrase": "selectivity_estimation"}, {"score": 0.004731928080086235, "phrase": "cost-based_query_optimizers"}, {"score": 0.004590052784169268, "phrase": "conjunctive_predicates"}, {"score": 0.004530552828603269, "phrase": "alternative_query_execution_plans"}, {"score": 0.004356620736507642, "phrase": "multivariate_statistics"}, {"score": 0.00420760449108585, "phrase": "attribute_values"}, {"score": 0.0038401488612789963, "phrase": "resulting_use"}, {"score": 0.003806866174028117, "phrase": "partial_distribution_information"}, {"score": 0.0035974243349266894, "phrase": "current_optimizers"}, {"score": 0.0035662378173267647, "phrase": "cumbersome_ad_hoc_methods"}, {"score": 0.0034291991359158827, "phrase": "consistent_manner"}, {"score": 0.0033553473725594003, "phrase": "valuable_information"}, {"score": 0.0032404677650217407, "phrase": "query_plans"}, {"score": 0.0031845067058084583, "phrase": "least_information"}, {"score": 0.0031023662967184216, "phrase": "poor_results"}, {"score": 0.0029961221584602405, "phrase": "novel_method"}, {"score": 0.0029701328654852246, "phrase": "consistent_selectivity_estimation"}, {"score": 0.0027943943921568456, "phrase": "available_information"}, {"score": 0.0027461155463816772, "phrase": "bias_problem"}, {"score": 0.002675252546913683, "phrase": "detailed_knowledge"}, {"score": 0.0026062133750675894, "phrase": "standard_uniformity_and_independence_assumptions"}, {"score": 0.0024306740161601625, "phrase": "optimizer's_cardinality_estimates"}, {"score": 0.002347378595930709, "phrase": "better_plan_quality"}, {"score": 0.002257070536352418, "phrase": "almost_all_queries"}, {"score": 0.002132711053003407, "phrase": "overall_time"}, {"score": 0.0021049977753042253, "phrase": "query_optimization"}], "paper_keywords": [""], "paper_abstract": "Cost-based query optimizers need to estimate the selectivity of conjunctive predicates when comparing alternative query execution plans. To this end, advanced optimizers use multivariate statistics to improve information about the joint distribution of attribute values in a table. The joint distribution for all columns is almost always too large to store completely, and the resulting use of partial distribution information raises the possibility that multiple, non-equivalent selectivity estimates may be available for a given predicate. Current optimizers use cumbersome ad hoc methods to ensure that selectivities are estimated in a consistent manner. These methods ignore valuable information and tend to bias the optimizer toward query plans for which the least information is available, often yielding poor results. In this paper we present a novel method for consistent selectivity estimation based on the principle of maximum entropy (ME). Our method exploits all available information and avoids the bias problem. In the absence of detailed knowledge, the ME approach reduces to standard uniformity and independence assumptions. Experiments with our prototype implementation in DB2 UDB show that use of the ME approach can improve the optimizer's cardinality estimates by orders of magnitude, resulting in better plan quality and significantly reduced query execution times. For almost all queries, these improvements are obtained while adding only tens of milliseconds to the overall time required for query optimization.", "paper_title": "Consistent selectivity estimation via maximum entropy", "paper_id": "WOS:000241580900004"}