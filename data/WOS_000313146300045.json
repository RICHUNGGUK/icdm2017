{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high-dimensional_discriminant_analysis"}, {"score": 0.004723322467153553, "phrase": "different_variance-covariance_matrices"}, {"score": 0.0043738432231020885, "phrase": "two-class_classification_problem"}, {"score": 0.00429057323623293, "phrase": "high-dimensional_data"}, {"score": 0.003643607569483598, "phrase": "good_performance"}, {"score": 0.003034916887102753, "phrase": "reasonable_sufficient_condition"}, {"score": 0.002864628676257225, "phrase": "misclassification_rate"}, {"score": 0.0027564303939865476, "phrase": "worst_value"}], "paper_keywords": ["discriminant analysis", " high-dimensional data", " misclassification rate", " d-asymptotics"], "paper_abstract": "In this paper we consider the two-class classification problem with high-dimensional data. It is important to find a class of distributions such that we cannot expect good performance in classification for any classifier. In this paper, when two population variance-covariance matrices are different, we give a reasonable sufficient condition for distributions such that the misclassification rate converges to the worst value as the dimension of data tends to infinity for any classifier. Our results can give guidelines to decide whether or not an experiment is worth performing in many fields such as bioinformatics.", "paper_title": "On d-Asymptotics for High-Dimensional Discriminant Analysis with Different Variance-Covariance Matrices", "paper_id": "WOS:000313146300045"}