{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "floating_point_results"}, {"score": 0.004774530554776604, "phrase": "multiple_runs"}, {"score": 0.0042250123967577284, "phrase": "dynamic_scheduling"}, {"score": 0.004189523950071494, "phrase": "parallel_computing_resources"}, {"score": 0.00413684709497912, "phrase": "floating_point"}, {"score": 0.003965975628742746, "phrase": "simple_reduction_operations"}, {"score": 0.0036759314172445934, "phrase": "floating_point_summation"}, {"score": 0.003479692058963528, "phrase": "rump's_algorithm"}, {"score": 0.0034504423179340738, "phrase": "error-free_vector_transformation"}, {"score": 0.003131178497790961, "phrase": "highly_accurate_results"}, {"score": 0.003091767402370757, "phrase": "absolute_error"}, {"score": 0.002805597755796506, "phrase": "small_constant_amount"}, {"score": 0.0027819985260847577, "phrase": "extra_memory_usage"}, {"score": 0.002758597251689795, "phrase": "higher_accuracies"}, {"score": 0.002655697256976997, "phrase": "error-free_transformations"}, {"score": 0.002524428047147766, "phrase": "nearest_rounding_mode"}, {"score": 0.002461241017147324, "phrase": "proposed_algorithms"}, {"score": 0.0023003498123420237, "phrase": "minimum_number"}, {"score": 0.002195866991284673, "phrase": "six_double_precision_floating_point_numbers"}, {"score": 0.0021049977753042253, "phrase": "massively_parallel_environments"}], "paper_keywords": ["Reproducibility", " summation", " floating-point", " rounding error", " parallel computing", " numerical analysis"], "paper_abstract": "Reproducibility, i.e. getting bitwise identical floating point results from multiple runs of the same program, is a property that many users depend on either for debugging or correctness checking in many codes [10]. However, the combination of dynamic scheduling of parallel computing resources, and floating point nonassociativity, makes attaining reproducibility a challenge even for simple reduction operations like computing the sum of a vector of numbers in parallel. We propose a technique for floating point summation that is reproducible independent of the order of summation. Our technique uses Rump's algorithm for error-free vector transformation [7], and is much more efficient than using (possibly very) high precision arithmetic. Our algorithm reproducibly computes highly accurate results with an absolute error bound of n . 2(-28) . macheps . max(i) vertical bar v(i)vertical bar at a cost of 7n FLOPs and a small constant amount of extra memory usage. Higher accuracies are also possible by increasing the number of error-free transformations. As long as all operations are performed in to-nearest rounding mode, results computed by the proposed algorithms are reproducible for any run on any platform. In particular, our algorithm requires the minimum number of reductions, i.e. one reduction of an array of six double precision floating point numbers per sum, and hence is well suited for massively parallel environments.", "paper_title": "Parallel Reproducible Summation", "paper_id": "WOS:000355989900020"}