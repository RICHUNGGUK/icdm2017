{"auto_keywords": [{"score": 0.04643842478255815, "phrase": "g.-b._huang"}, {"score": 0.04610688847088977, "phrase": "l._chen"}, {"score": 0.0457776054564914, "phrase": "c.-k._siew"}, {"score": 0.045468292901310936, "phrase": "universal"}, {"score": 0.044964166360440594, "phrase": "incremental_constructive_feedforward_networks"}, {"score": 0.0446426624209999, "phrase": "random_hidden_nodes"}, {"score": 0.044323350618466935, "phrase": "ieee_trans"}, {"score": 0.04199704127064953, "phrase": "hidden_nodes"}, {"score": 0.00481495049065317, "phrase": "based_incremental_extreme_learning_machine"}, {"score": 0.00474348728309194, "phrase": "incremental_algorithm"}, {"score": 0.004673079747693193, "phrase": "incremental_extreme_learning_machine"}, {"score": 0.004518442937844318, "phrase": "huang_et"}, {"score": 0.003919763262805793, "phrase": "output_weights"}, {"score": 0.003890540531028228, "phrase": "huang_et_al"}, {"score": 0.0034385353123700885, "phrase": "additive_or_rbf_hidden_nodes"}, {"score": 0.003275197814021, "phrase": "universal_approximator"}, {"score": 0.0030275252142439213, "phrase": "network_output"}, {"score": 0.002949189427671915, "phrase": "network_complexity"}, {"score": 0.0027569106820061707, "phrase": "enhanced_method"}, {"score": 0.0026357223464279983, "phrase": "learning_step"}, {"score": 0.0025388011374541546, "phrase": "hidden_node"}, {"score": 0.00250103550267459, "phrase": "largest_residual_error_decreasing"}, {"score": 0.00244543519230361, "phrase": "existing_network"}, {"score": 0.002418099158357658, "phrase": "output_weight"}, {"score": 0.00216907363015155, "phrase": "widespread_type"}, {"score": 0.002152874480615438, "phrase": "piecewise_continuous_hidden_nodes"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Incremental extreme learning machine", " Convergence rate", " Random hidden nodes", " Random search"], "paper_abstract": "Recently an incremental algorithm referred to as incremental extreme learning machine (I-ELM) was proposed by Huang et at. [G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Trans. Neural Networks 17(4) (2006) 879-892], which randomly generates hidden nodes and then analytically determines the output weights. Huang et al. [G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Trans. Neural Networks 17(4) (2006) 879-892] have proved in theory that although additive or RBF hidden nodes are generated randomly the network constructed by I-ELM can work as a universal approximator. During our recent study, it is found that some of the hidden nodes in such networks may play a very minor role in the network output and thus may eventually increase the network complexity. In order to avoid this issue and to obtain a more compact network architecture, this paper proposes an enhanced method for I-ELM (referred to as EI-ELM). At each learning step, several hidden nodes are randomly generated and among them the hidden node leading to the largest residual error decreasing will be added to the existing network and the output weight of the network will be calculated in a same simple way as in the original I-ELM. Generally speaking, the proposed enhanced I-ELM works for the widespread type of piecewise continuous hidden nodes. (C) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Enhanced random search based incremental extreme learning machine", "paper_id": "WOS:000260066100047"}