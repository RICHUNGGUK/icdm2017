{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "nearest_neighbor_estimators"}, {"score": 0.004399905285945236, "phrase": "r-d_x_r-valued_random_vector"}, {"score": 0.004081385289516914, "phrase": "regression_function"}, {"score": 0.003701401653850141, "phrase": "data_set"}, {"score": 0.003485304124598714, "phrase": "convergence_rate"}, {"score": 0.0033315236647127734, "phrase": "k_nearest_neighbor_estimators"}, {"score": 0.002976019715894587, "phrase": "minimax_rate"}, {"score": 0.0028661330049356186, "phrase": "k_nearest_neighbor_estimator"}, {"score": 0.0021693692831601745, "phrase": "error_criterion"}], "paper_keywords": ["regression", " nonparametric estimation", " nearest neighbor", " rate of convergence"], "paper_abstract": "Let (X, Y) be a R-d x R-valued random vector. In regression analysis one wants to estimate the regression function m(x) := E(Y vertical bar X = x) from a data set. In this paper we consider the convergence rate of the error for the k nearest neighbor estimators in case that m is (p, C)-smooth. It is known that the minimax rate is unachievable by any k nearest neighbor estimator for p > 1.5 and d = 1. We generalize this result to any d >= 1. Throughout this paper, we assume that the data is independent and identically distributed and as an error criterion we use the expected L-2 error.", "paper_title": "The Lower Bound for the Nearest Neighbor Estimators with (p, C)-Smooth Regression Functions", "paper_id": "WOS:000296673400022"}