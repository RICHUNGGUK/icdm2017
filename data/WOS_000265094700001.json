{"auto_keywords": [{"score": 0.029492249680776805, "phrase": "dnc"}, {"score": 0.01320488508378576, "phrase": "wilcoxon"}, {"score": 0.004690025569165577, "phrase": "multilayer_perceptron."}, {"score": 0.00450020161335541, "phrase": "multilayer_perceptron"}, {"score": 0.004399905285945236, "phrase": "state_space_search"}, {"score": 0.004066076430818046, "phrase": "five_variants"}, {"score": 0.003382017342396959, "phrase": "computational_complexity"}, {"score": 0.003021141590548526, "phrase": "ten-fold_cv_t"}, {"score": 0.002920561105784389, "phrase": "multiple_comparisons"}, {"score": 0.0028021527740710508, "phrase": "dynamic_node_creation"}, {"score": 0.0024748913336907923, "phrase": "simpler_architectures"}, {"score": 0.0024105326885334962, "phrase": "single_node_additions"}, {"score": 0.00223570488212345, "phrase": "multiple_operators"}, {"score": 0.0021209097150420896, "phrase": "simpler_networks"}, {"score": 0.0021049977753042253, "phrase": "lower_or_comparable_error_rates"}], "paper_keywords": ["Model selection", " cross-validation", " statistical testing", " growing/pruning methods"], "paper_abstract": "We define the problem of optimizing the architecture of a multilayer perceptron (MLP) as a state space search and propose the MOST (Multiple Operators using Statistical Tests) framework that incrementally modifies the structure and checks for improvement using cross-validation. We consider five variants that implement forward/backward search, using single/multiple operators and searching depth-first/breadth-first. On 44 classification and 30 regression datasets, we exhaustively search for the optimal and evaluate the goodness based on: (1) Order, the accuracy with respect to the optimal and (2) Rank, the computational complexity. We check for the effect of two resampling methods (5 x 2, ten-fold cv), four statistical tests (5 x 2 cv t, ten-fold cv t, Wilcoxon, sign) and two corrections for multiple comparisons (Bonferroni, Holm). We also compare with Dynamic Node Creation (DNC) and Cascade Correlation (CC). Our results show that: (1) On most datasets, networks with few hidden units are optimal, (2) forward searching finds simpler architectures, (3) variants using single node additions (deletions) generally stop early and get stuck in simple (complex) networks, (4) choosing the best of multiple operators finds networks closer to the optimal, (5) MOST variants generally find simpler networks having lower or comparable error rates than DNC and CC.", "paper_title": "AN INCREMENTAL FRAMEWORK BASED ON CROSS-VALIDATION FOR ESTIMATING THE ARCHITECTURE OF A MULTILAYER PERCEPTRON", "paper_id": "WOS:000265094700001"}