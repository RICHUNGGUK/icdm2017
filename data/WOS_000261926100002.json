{"auto_keywords": [{"score": 0.03085914642755347, "phrase": "neural_network"}, {"score": 0.008211538341628395, "phrase": "motion_analysis"}, {"score": 0.00481495049065317, "phrase": "affective_factors"}, {"score": 0.004766388573192505, "phrase": "selective_motion_analysis"}, {"score": 0.00469445816161498, "phrase": "dynamic_environment"}, {"score": 0.004600228893803583, "phrase": "new_integrated_saliency_map"}, {"score": 0.004553822674247401, "phrase": "selective_motion_analysis_models"}, {"score": 0.0044398358177815305, "phrase": "biological_visual_attention_mechanism"}, {"score": 0.00437281181878921, "phrase": "proposed_models"}, {"score": 0.004177724522109952, "phrase": "final_attention_area"}, {"score": 0.004011603024990615, "phrase": "closer_area"}, {"score": 0.00395101799538683, "phrase": "human_binocular_vision"}, {"score": 0.0038520615699124123, "phrase": "single_eye_alignment_hypothesis"}, {"score": 0.0036245239349157236, "phrase": "input_scene"}, {"score": 0.0035337158050484474, "phrase": "proposed_saliency_map_model"}, {"score": 0.0034803228231621687, "phrase": "affective_computing_process"}, {"score": 0.003410380718649286, "phrase": "unwanted_area"}, {"score": 0.003308085469148074, "phrase": "desired_area"}, {"score": 0.0032251796659158696, "phrase": "human_preference"}, {"score": 0.0031603487697867538, "phrase": "subsequent_visual_search_processes"}, {"score": 0.0029735479904284675, "phrase": "symmetry_feature"}, {"score": 0.0028697077879305064, "phrase": "independent_component_analysis"}, {"score": 0.002713788077665689, "phrase": "object_preferable_attention_model"}, {"score": 0.002618995204424126, "phrase": "selective_motion_analysis_model"}, {"score": 0.0025663181430010686, "phrase": "proposed_saliency_map"}, {"score": 0.0023183698048498797, "phrase": "planar_motion"}, {"score": 0.002283298119790413, "phrase": "optical_flow"}, {"score": 0.002248755794042486, "phrase": "selected_area"}, {"score": 0.002181227522730908, "phrase": "proposed_model"}, {"score": 0.002148226022654619, "phrase": "plausible_scan_paths"}, {"score": 0.002126502291103601, "phrase": "selective_motion_analysis_results"}, {"score": 0.0021049977753042253, "phrase": "natural_input_scenes"}], "paper_keywords": ["Integrated saliency map", " Stereo saliency map", " Affective attention", " Bottom-up attention", " Selective motion analysis"], "paper_abstract": "We propose new integrated saliency map and selective motion analysis models partly inspired by a biological visual attention mechanism. The proposed models consider not only binocular stereopsis to identify a final attention area so that the system focuses on the closer area as in human binocular vision, based on the single eye alignment hypothesis, but also both the static and dynamic features of an input scene. Moreover, the proposed saliency map model includes an affective computing process that skips an unwanted area and pays attention to a desired area, which reflects the human preference and refusal in subsequent visual search processes. In addition, we show the effectiveness of considering the symmetry feature determined by a neural network and an independent component analysis (ICA) filter which are helpful to construct an object preferable attention model. Also, we propose a selective motion analysis model by integrating the proposed saliency map with a neural network for motion analysis. The neural network for motion analysis responds selectively to rotation, expansion, contraction and planar motion of the optical flow in a selected area. Experiments show that the proposed model can generate plausible scan paths and selective motion analysis results for natural input scenes. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Stereo saliency map considering affective factors and selective motion analysis in a dynamic environment", "paper_id": "WOS:000261926100002"}