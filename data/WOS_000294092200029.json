{"auto_keywords": [{"score": 0.04749848892978978, "phrase": "adaboost"}, {"score": 0.006691381306107592, "phrase": "competitive_results"}, {"score": 0.005428048862424806, "phrase": "training_data"}, {"score": 0.005156293565154829, "phrase": "weighted_knn_algorithm"}, {"score": 0.00481495049065317, "phrase": "weighted_knn."}, {"score": 0.004777034835192062, "phrase": "realistic_model"}, {"score": 0.00459186790405058, "phrase": "noisy_data"}, {"score": 0.004466519142879166, "phrase": "effective_method"}, {"score": 0.004361792415184563, "phrase": "base_classifiers"}, {"score": 0.0042650820522753775, "phrase": "new_boosting_algorithm"}, {"score": 0.004225950334493496, "phrase": "previous_studies"}, {"score": 0.0040300463421312225, "phrase": "noisy_domains"}, {"score": 0.003919973783578995, "phrase": "knn_rule"}, {"score": 0.0038431888954930083, "phrase": "oldest_and_simplest_methods"}, {"score": 0.0035507457285983268, "phrase": "prior_knowledge"}, {"score": 0.0032934951320964276, "phrase": "edited_adaboost"}, {"score": 0.0032675209435107273, "phrase": "weighted_knn"}, {"score": 0.003153149935158228, "phrase": "knn"}, {"score": 0.0029362173680378624, "phrase": "statistical_regularity"}, {"score": 0.002788865808981643, "phrase": "feature_space"}, {"score": 0.002596937660779953, "phrase": "classification_accuracy"}, {"score": 0.0025259082947443343, "phrase": "data_sets"}, {"score": 0.0023896108055582615, "phrase": "ten_different_uci_data_sets"}, {"score": 0.0023150468182777813, "phrase": "considerably_better_classification_accuracy"}, {"score": 0.0022250978492127163, "phrase": "artificially_controlled_noise"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["AdaBoost", " Overfitting", " kNN rules", " Adaptability", " Feature space"], "paper_abstract": "Any realistic model of learning from samples must address the issue of noisy data. AdaBoost is known as an effective method for improving the performance of base classifiers both theoretically and empirically. However, previous studies have shown that AdaBoost is prone to overfitting, especially in noisy domains. On the other hand, the kNN rule is one of the oldest and simplest methods for pattern classification. Nevertheless, it often yields competitive results, and in certain domains, when cleverly combined with prior knowledge, it has significantly advanced the state-of-the-art. In this paper, an edited AdaBoost by weighted kNN (EAdaBoost) is designed where AdaBoost and kNN naturally complement each other. First, AdaBoost is run on the training data to capitalize on some statistical regularity in the data. Then, a weighted kNN algorithm is run on the feature space composed of classifiers produced by AdaBoost to achieve competitive results. AdaBoost is then used to enhance the classification accuracy and avoid overfitting by editing the data sets using the weighted kNN algorithm for improving the quality of training data. Experiments performed on ten different UCI data sets show that the new Boosting algorithm almost always achieves considerably better classification accuracy than AdaBoost. Furthermore, experiments on data with artificially controlled noise indicate that the new Boosting algorithm is robust to noise. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Edited AdaBoost by weighted kNN", "paper_id": "WOS:000294092200029"}