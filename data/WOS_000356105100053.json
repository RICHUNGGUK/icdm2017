{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "binary_nearest_neighbor_tree"}, {"score": 0.012982248350383212, "phrase": "classification_accuracy"}, {"score": 0.011072294306924316, "phrase": "proposed_algorithm"}, {"score": 0.004362564831572539, "phrase": "prohibitive_computational_and_storage_requirements"}, {"score": 0.004291393968030603, "phrase": "large_datasets"}, {"score": 0.004238775572746022, "phrase": "reasonable_way"}, {"score": 0.004084744133085742, "phrase": "small_representative_subset"}, {"score": 0.004034649282040889, "phrase": "original_dataset"}, {"score": 0.003808845109556467, "phrase": "boundary_patterns"}, {"score": 0.0037312893857519803, "phrase": "better_classification_accuracy"}, {"score": 0.0035808664384880213, "phrase": "new_algorithm"}, {"score": 0.0035369291417564606, "phrase": "binary_tree_technique"}, {"score": 0.0034935290662899488, "phrase": "reduction_operations"}, {"score": 0.003422371752150812, "phrase": "key_issues"}, {"score": 0.0033115124244647736, "phrase": "binary_nearest_neighbor_search_tree"}, {"score": 0.003204232566468624, "phrase": "high_classification_accuracy_patterns"}, {"score": 0.0030623745850494578, "phrase": "knn"}, {"score": 0.0029631248341951064, "phrase": "random_pattern"}, {"score": 0.002878930338561009, "phrase": "node_locations"}, {"score": 0.002740109776120878, "phrase": "different_kinds"}, {"score": 0.002607965563121546, "phrase": "class_boundary_regions"}, {"score": 0.0025547994452342266, "phrase": "interior_regions"}, {"score": 0.0025130460190930554, "phrase": "internal_patterns"}, {"score": 0.00245168871716047, "phrase": "experimental_results"}, {"score": 0.0022670785913397637, "phrase": "traditional_knn_algorithm"}, {"score": 0.0021845387990972543, "phrase": "simple_and_fast_hybrid_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Supervised classification", " Binary nearest neighbor tree", " K-nearest neighbor rule", " Prototype selection", " Prototype generation"], "paper_abstract": "The K-nearest neighbor (KNN) rule is one of the most useful supervised classification methods, and is widely used in many pattern classification applications due to its simplicity. However, it faces prohibitive computational and storage requirements when dealing with large datasets. A reasonable way of alleviating this problem is to extract a small representative subset from the original dataset without reducing the classification accuracy. This means the most internal patterns are removed and the boundary patterns that can contribute to better classification accuracy are retained. To achieve this purpose, a new algorithm based on binary tree technique and some reduction operations is presented. The key issues of the proposed algorithm are how to build binary nearest neighbor search tree and design reduction strategies to keep the high classification accuracy patterns. In particular, firstly, we utilize several tree control rules and KNN rule to build a binary nearest neighbor tree of each random pattern. Secondly, according to the node locations in each binary nearest neighbor tree and the strategies of selection and replacement, different kinds of patterns as prototypes are obtained, which are close to class boundary regions or locate in the interior regions, and some internal patterns are generated. Finally, experimental results show that the proposed algorithm effectively reduces the number of prototypes while maintaining the same level of classification accuracy as the traditional KNN algorithm and other prototype algorithms. Moreover, it is a simple and fast hybrid algorithm for prototype reduction. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A new fast reduction technique based on binary nearest neighbor tree", "paper_id": "WOS:000356105100053"}