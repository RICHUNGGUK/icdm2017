{"auto_keywords": [{"score": 0.048970045726729895, "phrase": "naive_bayes"}, {"score": 0.0045143706249204905, "phrase": "five_main_categories"}, {"score": 0.0035082436540296406, "phrase": "structure_extension"}, {"score": 0.0034068225125064586, "phrase": "random_bayes_model"}, {"score": 0.002555247330102505, "phrase": "maximal_conditional_mutual_information"}, {"score": 0.0024379730432321656, "phrase": "bayesian_network_classifiers"}, {"score": 0.0023953929817357882, "phrase": "experimental_results"}, {"score": 0.002353554841514325, "phrase": "large_number"}, {"score": 0.002326068563512387, "phrase": "uci_data_sets"}, {"score": 0.0022192891418394514, "phrase": "class_probability_estimation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Naive Bayes", " One-dependence estimators", " Random selection", " Classification", " Class probability estimation", " Ranking"], "paper_abstract": "Many approaches attempt to improve naive Bayes and have been broadly divided into five main categories: (1) structure extension; (2) attribute weighting; (3) attribute selection; (4) instance weighting; (5) instance selection, also called local learning. In this paper, we work on the approach of structure extension and single out a random Bayes model by augmenting the structure of naive Bayes. We called it random one-dependence estimators, simply RODE. In RODE, each attribute has at most one parent from other attributes and this parent is randomly selected from log(2)m (where m is the number of attributes) attributes with the maximal conditional mutual information. Our work conducts the randomness into Bayesian network classifiers. The experimental results on a large number of UCI data sets validate its effectiveness in terms of classification, class probability estimation, and ranking. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Random one-dependence estimators", "paper_id": "WOS:000286560600015"}