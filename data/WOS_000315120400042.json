{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "changing_world"}, {"score": 0.004762893197005439, "phrase": "restless_multiarmed_bandit"}, {"score": 0.0047113960612140335, "phrase": "unknown_dynamics"}, {"score": 0.0045850648862091085, "phrase": "restless_multiarmed_bandit_problem"}, {"score": 0.004024072318264299, "phrase": "arm_transits"}, {"score": 0.003937459891461576, "phrase": "unknown_markovian_rule"}, {"score": 0.003728966994353039, "phrase": "arbitrary_unknown_random_process"}, {"score": 0.003531474887552889, "phrase": "arm_selection_policy"}, {"score": 0.003362660648932247, "phrase": "reward_loss"}, {"score": 0.003032228032253871, "phrase": "best_arm"}, {"score": 0.0029029754274913803, "phrase": "interleaving_exploration_and_exploitation_epoch_structure"}, {"score": 0.0028096544129939277, "phrase": "logarithmic_order"}, {"score": 0.002675252546913683, "phrase": "decentralized_setting"}, {"score": 0.002646267286675984, "phrase": "multiple_distributed_players"}, {"score": 0.002575167114383286, "phrase": "information_exchange"}, {"score": 0.0024923577071656014, "phrase": "endogenous_restless_model"}, {"score": 0.002412204754253618, "phrase": "decentralized_extension"}, {"score": 0.0023730978395564116, "phrase": "proposed_policy"}, {"score": 0.0023346234464901978, "phrase": "logarithmic_regret_order"}, {"score": 0.002284290601837099, "phrase": "centralized_setting"}, {"score": 0.002210814150677161, "phrase": "adaptive_learning"}, {"score": 0.0021631447601689444, "phrase": "communication_networks"}, {"score": 0.0021049977753042253, "phrase": "financial_investment"}], "paper_keywords": ["Distributed learning", " online learning", " regret", " restless multiarmed bandit (RMAB)"], "paper_abstract": "We consider the restless multiarmed bandit problem with unknown dynamics in which a player chooses one out of arms to play at each time. The reward state of each arm transits according to an unknown Markovian rule when it is played and evolves according to an arbitrary unknown random process when it is passive. The performance of an arm selection policy is measured by regret, defined as the reward loss with respect to the case where the player knows which arm is the most rewarding and always plays the best arm. We construct a policy with an interleaving exploration and exploitation epoch structure that achieves a regret with logarithmic order. We further extend the problem to a decentralized setting where multiple distributed players share the arms without information exchange. Under both an exogenous restless model and an endogenous restless model, we show that a decentralized extension of the proposed policy preserves the logarithmic regret order as in the centralized setting. The results apply to adaptive learning in various dynamic systems and communication networks, as well as financial investment.", "paper_title": "Learning in a Changing World: Restless Multiarmed Bandit With Unknown Dynamics", "paper_id": "WOS:000315120400042"}