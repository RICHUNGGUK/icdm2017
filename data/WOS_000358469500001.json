{"auto_keywords": [{"score": 0.046411162769564376, "phrase": "computational_demands"}, {"score": 0.00481495049065317, "phrase": "heterogeneous_lattice_boltzmann_simulations"}, {"score": 0.004772593750252861, "phrase": "cpu-gpu_clusters"}, {"score": 0.0047306078475683865, "phrase": "computational_fluid_dynamic_simulations"}, {"score": 0.004526129302830441, "phrase": "parallel_simulations"}, {"score": 0.0044863018460183784, "phrase": "modern_supercomputers"}, {"score": 0.004407690586271672, "phrase": "complex_simulation_tasks"}, {"score": 0.004217110862974562, "phrase": "high_performance"}, {"score": 0.004106725110985659, "phrase": "high_floating_point_performance"}, {"score": 0.0040347380310668994, "phrase": "processor_chip_bandwidth"}, {"score": 0.003946519058822334, "phrase": "gpu_clusters"}, {"score": 0.003894512634265439, "phrase": "daily_business"}, {"score": 0.0038431888954930083, "phrase": "large_community"}, {"score": 0.0038093479435377764, "phrase": "usable_software_frameworks"}, {"score": 0.00354909279546064, "phrase": "maintainable_software_designs"}, {"score": 0.0032629585038738856, "phrase": "software_design_concepts"}, {"score": 0.003121719509312159, "phrase": "multi-physics_simulations"}, {"score": 0.003066945490483524, "phrase": "lattice_boltzmann_method"}, {"score": 0.002934166052954463, "phrase": "hybrid_parallelization_approach"}, {"score": 0.00289546142998427, "phrase": "heterogeneous_simulations"}, {"score": 0.0027700855441353165, "phrase": "first_time"}, {"score": 0.0027578496677536373, "phrase": "weak_and_strong_scaling_performance_results"}, {"score": 0.0025578967205807843, "phrase": "new_communication_model"}, {"score": 0.002535344521369057, "phrase": "parallel_efficiency"}, {"score": 0.002436289099444074, "phrase": "detailed_and_structured_performance_analysis"}, {"score": 0.0023724065150232897, "phrase": "walberla_framework"}, {"score": 0.002351485871160907, "phrase": "production_runs"}, {"score": 0.0023307492812556204, "phrase": "large_gpu_clusters"}, {"score": 0.0022297712188622293, "phrase": "strong_scaling_experiments"}, {"score": 0.0021809311689473493, "phrase": "porous_medium"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Lattice Boltzmann method", " CUDA", " Heterogeneous computations", " Performance modeling"], "paper_abstract": "Computational fluid dynamic simulations are in general very compute intensive. Only by parallel simulations on modern supercomputers the computational demands of complex simulation tasks can be satisfied. Facing these computational demands GPUs offer high performance, as they provide the high floating point performance and memory to processor chip bandwidth. To successfully utilize GPU clusters for the daily business of a large community, usable software frameworks must be established on these clusters. The development of such software frameworks is only feasible with maintainable software designs that consider performance as a design objective right from the start. For this work we extend the software design concepts to achieve more efficient and highly scalable multi-GPU parallelization within our software framework waLBerla for multi-physics simulations centered around the lattice Boltzmann method. Our software designs now also support a pure-MPI and a hybrid parallelization approach capable of heterogeneous simulations using CPUs and GPUs in parallel. For the first time weak and strong scaling performance results obtained on the Tsubame 2.0 cluster for more than 1000 GPUs are presented using waLBerla. With the help of a new communication model the parallel efficiency of our implementation is investigated and analyzed in a detailed and structured performance analysis. The suitability of the waLBerla framework for production runs on large GPU clusters is demonstrated. As one possible application we show results of strong scaling experiments for flows through a porous medium. (C) 2015 Published by Elsevier B.V.", "paper_title": "Performance modeling and analysis of heterogeneous lattice Boltzmann simulations on CPU-GPU clusters", "paper_id": "WOS:000358469500001"}