{"auto_keywords": [{"score": 0.04961049691122095, "phrase": "coordinate_descent"}, {"score": 0.015719716506582538, "phrase": "majorization_minimization"}, {"score": 0.01526751765740687, "phrase": "generalized_linear_models"}, {"score": 0.01106583584242049, "phrase": "mmcd"}, {"score": 0.00462919213721188, "phrase": "recent_studies"}, {"score": 0.004561386908745206, "phrase": "theoretical_attractiveness"}, {"score": 0.004450568197899267, "phrase": "concave_penalties"}, {"score": 0.004406994912939125, "phrase": "variable_selection"}, {"score": 0.004321118442104989, "phrase": "smoothly_clipped_absolute_deviation"}, {"score": 0.004278807087784626, "phrase": "minimax_concave_penalties"}, {"score": 0.004154332348800334, "phrase": "concave_penalized_solutions"}, {"score": 0.0041136473765874815, "phrase": "high-dimensional_models"}, {"score": 0.003993958025223994, "phrase": "difficult_task"}, {"score": 0.0036914757355166966, "phrase": "penalized_solutions"}, {"score": 0.003548905904860611, "phrase": "existing_algorithms"}, {"score": 0.003496868378204657, "phrase": "local_quadratic_or_local_linear_approximation"}, {"score": 0.003296212583539166, "phrase": "negative_log-likelihood"}, {"score": 0.003247868216333148, "phrase": "quadratic_loss"}, {"score": 0.002899965889623787, "phrase": "scaling_factor"}, {"score": 0.0026277995163948263, "phrase": "theoretical_convergence_property"}, {"score": 0.0025892330566358503, "phrase": "mmcd."}, {"score": 0.0025014259919355453, "phrase": "penalized_logistic_regression_model"}, {"score": 0.0024647158865152476, "phrase": "scad"}, {"score": 0.002440543964955628, "phrase": "mcp"}, {"score": 0.002404706573379854, "phrase": "simulation_studies"}, {"score": 0.0022443390559699974, "phrase": "penalized_logistic_regression"}, {"score": 0.0022223176391038785, "phrase": "high-dimensional_settings"}, {"score": 0.0021049977753042253, "phrase": "sample_size"}], "paper_keywords": ["Logistic regression", " p >> n models", " Smoothly clipped absolute deviation penalty", " Minimax concave penalty", " Variable selection"], "paper_abstract": "Recent studies have demonstrated theoretical attractiveness of a class of concave penalties in variable selection, including the smoothly clipped absolute deviation and minimax concave penalties. The computation of the concave penalized solutions in high-dimensional models, however, is a difficult task. We propose a majorization minimization by coordinate descent (MMCD) algorithm for computing the concave penalized solutions in generalized linear models. In contrast to the existing algorithms that use local quadratic or local linear approximation to the penalty function, the MMCD seeks to majorize the negative log-likelihood by a quadratic loss, but does not use any approximation to the penalty. This strategy makes it possible to avoid the computation of a scaling factor in each update of the solutions, which improves the efficiency of coordinate descent. Under certain regularity conditions, we establish theoretical convergence property of the MMCD. We implement this algorithm for a penalized logistic regression model using the SCAD and MCP penalties. Simulation studies and a data example demonstrate that the MMCD works sufficiently fast for the penalized logistic regression in high-dimensional settings where the number of covariates is much larger than the sample size.", "paper_title": "Majorization minimization by coordinate descent for concave penalized generalized linear models", "paper_id": "WOS:000339380000013"}