{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "mutual_information_estimation"}, {"score": 0.004525629293824217, "phrase": "new_feature_selection_method"}, {"score": 0.0043875257506180865, "phrase": "mutual_information-based_criterion"}, {"score": 0.003956784357357341, "phrase": "backward_selection_framework"}, {"score": 0.0030554300179552415, "phrase": "existing_mutual_information-based_methods"}, {"score": 0.0026161698049580804, "phrase": "proposed_method"}, {"score": 0.0024842242921857705, "phrase": "important_features"}, {"score": 0.0024333239726872604, "phrase": "data_sets"}, {"score": 0.0021940118003312397, "phrase": "almost_all_cases"}, {"score": 0.0021049977753042253, "phrase": "benchmark_methods"}], "paper_keywords": ["Feature ranking", " feature selection", " mutual information", " random permutation (RP)"], "paper_abstract": "This paper proposes a new feature selection method using a mutual information-based criterion that measures the importance of a feature in a backward selection framework. It considers the dependency among many features and uses either one of two well-known probability density function estimation methods when computing the criterion. The proposed approach is compared with existing mutual information-based methods and another sophisticated filter method on many artificial and real-world problems. The numerical results show that the proposed method can effectively identify the important features in data sets having dependency among many features and is superior, in almost all cases, to the benchmark methods.", "paper_title": "An Effective Feature Selection Method via Mutual Information Estimation", "paper_id": "WOS:000311353700004"}