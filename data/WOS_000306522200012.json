{"auto_keywords": [{"score": 0.04668184499136066, "phrase": "machine_learning"}, {"score": 0.03475588358113425, "phrase": "mtl"}, {"score": 0.00481495049065317, "phrase": "svm-based_multitask_learning"}, {"score": 0.00461302961851157, "phrase": "traditional_inductive_learning"}, {"score": 0.004529093447865156, "phrase": "active_research_area"}, {"score": 0.004031699976413366, "phrase": "group_information"}, {"score": 0.003815481506657645, "phrase": "vapnik"}, {"score": 0.0037459885494311217, "phrase": "general_approach"}, {"score": 0.0035018466576215517, "phrase": "cherkassky"}, {"score": 0.003116913644889395, "phrase": "svm-based_formulation"}, {"score": 0.0028084079003212973, "phrase": "large_quadratic_programming_optimization_problem"}, {"score": 0.002673957615079041, "phrase": "sample_size_n."}, {"score": 0.0025459275963373496, "phrase": "computationally_efficient_algorithms"}, {"score": 0.0024389277933858054, "phrase": "platt's_sequential_minimal_optimization"}, {"score": 0.0023079225317714815, "phrase": "empirical_results"}, {"score": 0.002197379785587054, "phrase": "proposed_generalized_smo"}, {"score": 0.0021049977753042253, "phrase": "general-purpose_optimization_routines"}], "paper_keywords": ["Classification", " learning with structured data", " multitask learning", " quadratic optimization", " sequential minimal optimization", " support vector machine (SVM)", " SVM"], "paper_abstract": "Exploiting additional information to improve traditional inductive learning is an active research area in machine learning. In many supervised-learning applications, training data can be naturally separated into several groups, and incorporating this group information into learning may improve generalization. Recently, Vapnik proposed a general approach to formalizing such problems, known as \"learning with structured data\" and its support vector machine (SVM) based optimization formulation called SVM+. Liang and Cherkassky showed the connection between SVM+ and multitask learning (MTL) approaches in machine learning, and proposed an SVM-based formulation for MTL called SVM+ MTL for classification. Training the SVM+ MTL classifier requires the solution of a large quadratic programming optimization problem which scales as O(n(3)) with sample size n. So there is a need to develop computationally efficient algorithms for implementing SVM+ MTL. This brief generalizes Platt's sequential minimal optimization (SMO) algorithm to the SVM+ MTL setting. Empirical results show that, for typical SVM+ MTL problems, the proposed generalized SMO achieves over 100 times speed-up, in comparison with general-purpose optimization routines.", "paper_title": "Generalized SMO Algorithm for SVM-Based Multitask Learning", "paper_id": "WOS:000306522200012"}