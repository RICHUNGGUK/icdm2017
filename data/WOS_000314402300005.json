{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "bridging_structure_and_feature"}, {"score": 0.0044192837023190445, "phrase": "object_recognition"}, {"score": 0.004184526032558792, "phrase": "essential_problem"}, {"score": 0.004119762981892093, "phrase": "pattern_recognition"}, {"score": 0.003389704997952315, "phrase": "attributed_graphs"}, {"score": 0.0033372020049592726, "phrase": "structural_as_well_as_feature-based_information"}, {"score": 0.0031597433845085092, "phrase": "pure_structural_description"}, {"score": 0.002619748321217153, "phrase": "weighted_combinations"}], "paper_keywords": ["Graph edit distance", " graph kernel", " multiple instance learning"], "paper_abstract": "Structures and features are opposite approaches in building representations for object recognition. Bridging the two is an essential problem in pattern recognition as the two opposite types of information are fundamentally different. As dissimilarities can be computed for both the dissimilarity representation can be used to combine the two. Attributed graphs contain structural as well as feature-based information. Neglecting the attributes yields a pure structural description. Isolating the features and neglecting the structure represents objects by a bag of features. In this paper we will show that weighted combinations of dissimilarities may perform better than these two extremes, indicating that these two types of information are essentially different and strengthen each other. In addition we present two more advanced integrations than weighted combining and show that these may improve the classification performances even further.", "paper_title": "BRIDGING STRUCTURE AND FEATURE REPRESENTATIONS IN GRAPH MATCHING", "paper_id": "WOS:000314402300005"}