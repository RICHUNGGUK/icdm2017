{"auto_keywords": [{"score": 0.04138579382509754, "phrase": "vocal_features"}, {"score": 0.014786499977154593, "phrase": "affective_states"}, {"score": 0.010880016476799979, "phrase": "nine_affective-state_groups"}, {"score": 0.008656707982822634, "phrase": "co-occurring_affective_states"}, {"score": 0.00481495049065317, "phrase": "complex_information"}, {"score": 0.004772593750252861, "phrase": "inference_of_co-occurring_affective_states"}, {"score": 0.004689182807921819, "phrase": "speech"}, {"score": 0.004586531354697466, "phrase": "classification_algorithm"}, {"score": 0.004446823283475876, "phrase": "mental_states"}, {"score": 0.0039815737219022675, "phrase": "different_sets"}, {"score": 0.0038431888954930083, "phrase": "speech_rate"}, {"score": 0.003775803846431323, "phrase": "nonverbal_expressions"}, {"score": 0.003742554019950264, "phrase": "different_affective_states"}, {"score": 0.0036445446531633368, "phrase": "inference_system"}, {"score": 0.0035965032173588753, "phrase": "large_set"}, {"score": 0.003350740737336966, "phrase": "independent_pairwise_comparisons"}, {"score": 0.003121719509312159, "phrase": "different_pairs"}, {"score": 0.0030942116765996426, "phrase": "affective-state_groups"}, {"score": 0.003066945490483524, "phrase": "average_classification_accuracy"}, {"score": 0.0029212075318449616, "phrase": "comparison_results"}, {"score": 0.0028572658971252616, "phrase": "single_ranked_list"}, {"score": 0.002661883310629088, "phrase": "inferred_combination"}, {"score": 0.0025578967205807843, "phrase": "inference_accuracy"}, {"score": 0.0025241429061867633, "phrase": "combined_machine"}, {"score": 0.002289821830549781, "phrase": "inferred_combinations"}, {"score": 0.0022595974985992664, "phrase": "lexical_definitions"}, {"score": 0.002200337772403983, "phrase": "analyzed_sentences"}, {"score": 0.0021712920081029194, "phrase": "distinguishing_capabilities"}, {"score": 0.0021049977753042253, "phrase": "human_performance"}], "paper_keywords": ["Affective computing", " human perception", " cognition", " affective states", " emotions", " speech", " machine learning", " intelligent systems", " multiclass", " multilabel"], "paper_abstract": "We present a classification algorithm for inferring affective states (emotions, mental states, attitudes, and the like) from their nonverbal expressions in speech. It is based on the observations that affective states can occur simultaneously and different sets of vocal features, such as intonation and speech rate, distinguish between nonverbal expressions of different affective states. The input to the inference system was a large set of vocal features and metrics that were extracted from each utterance. The classification algorithm conducted independent pairwise comparisons between nine affective-state groups. The classifier used various subsets of metrics of the vocal features and various classification algorithms for different pairs of affective-state groups. Average classification accuracy of the 36 pairwise machines was 75 percent, using 10-fold cross validation. The comparison results were consolidated into a single ranked list of the nine affective-state groups. This list was the output of the system and represented the inferred combination of co-occurring affective states for the analyzed utterance. The inference accuracy of the combined machine was 83 percent. The system automatically characterized over 500 affective state concepts from the Mind Reading database. The inference of co-occurring affective states was validated by comparing the inferred combinations to the lexical definitions of the labels of the analyzed sentences. The distinguishing capabilities of the system were comparable to human performance.", "paper_title": "Classification of Complex Information: Inference of Co-Occurring Affective States from Their Expressions in Speech", "paper_id": "WOS:000277649100010"}