{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sliding_mode_trajectories"}, {"score": 0.004762893197005439, "phrase": "multi-objective_neural_networks"}, {"score": 0.004389912080394437, "phrase": "constrained_set"}, {"score": 0.004068087482412997, "phrase": "neural_networks"}, {"score": 0.004046020290832207, "phrase": "supervised_learning"}, {"score": 0.003811010743703935, "phrase": "neural_network"}, {"score": 0.0037087324991876727, "phrase": "dynamic_system"}, {"score": 0.0033810134162565843, "phrase": "learning_trajectory"}, {"score": 0.0033262524445013303, "phrase": "resulting_state_space"}, {"score": 0.0031500213134140953, "phrase": "mode_dynamics"}, {"score": 0.002966900048034132, "phrase": "arbitrary_learning_trajectories"}, {"score": 0.002855934865505357, "phrase": "sliding_mode_gains"}, {"score": 0.0027943943921568456, "phrase": "formal_proofs"}, {"score": 0.0027641218567092665, "phrase": "convergence_conditions"}, {"score": 0.0024653491388223546, "phrase": "final_state"}, {"score": 0.0024253827365474734, "phrase": "pareto_set"}, {"score": 0.0023219374444738723, "phrase": "different_trajectories"}, {"score": 0.00217496508306238, "phrase": "additional_objective_function"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Neural networks", " Multi-objective learning", " Sliding mode"], "paper_abstract": "The Pareto-optimality concept is used in this paper in order to represent a constrained set of solutions that are able to trade-off the two main objective functions involved in neural networks supervised learning: data-set error and network complexity. The neural network is described as a dynamic system having error and complexity as its state variables and learning is presented as a process of controlling a learning trajectory in the resulting state space. In order to control the trajectories, sliding mode dynamics is imposed to the network. It is shown that arbitrary learning trajectories can be achieved by maintaining the sliding mode gains within their convergence intervals. Formal proofs of convergence conditions are therefore presented. The concept of trajectory learning presented in this paper goes further beyond the selection of a final state in the Pareto set, since it can be reached through different trajectories and states in the trajectory can be assessed individually against an additional objective function. (c) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Convergence analysis of sliding mode trajectories in multi-objective neural networks learning", "paper_id": "WOS:000307430900003"}