{"auto_keywords": [{"score": 0.03892622593568118, "phrase": "simulated_rl_robot"}, {"score": 0.00481495049065317, "phrase": "human_teaching_behavior"}, {"score": 0.004703149944841539, "phrase": "reinforcement_learning"}, {"score": 0.004666573243232647, "phrase": "rl"}, {"score": 0.00454027551702508, "phrase": "interactive_supervisory_input"}, {"score": 0.004487241576747549, "phrase": "human_teacher"}, {"score": 0.0044002186469797476, "phrase": "robot_and_software_agents"}, {"score": 0.0043148760739240575, "phrase": "human_input"}, {"score": 0.004181743040532619, "phrase": "reward_signal"}, {"score": 0.0038969619124903884, "phrase": "human-given_reward"}, {"score": 0.003821342577980031, "phrase": "traditional_rl_reward_signal"}, {"score": 0.0037471850931318942, "phrase": "experimental_platform"}, {"score": 0.0036172962914449826, "phrase": "real-time_human_teaching_behavior"}, {"score": 0.00351938145050947, "phrase": "untrained_subjects"}, {"score": 0.003424107910734022, "phrase": "new_task"}, {"score": 0.0032539388572032563, "phrase": "reinforcement_learning_agent"}, {"score": 0.0031534367609709233, "phrase": "reward_channel"}, {"score": 0.0030440661915657175, "phrase": "future-directed_guidance"}, {"score": 0.0029500270981001058, "phrase": "positive_bias"}, {"score": 0.0028365413717574544, "phrase": "motivational_channel"}, {"score": 0.0026954938318408464, "phrase": "mental_model"}, {"score": 0.0026639507068734907, "phrase": "robotic_learner"}, {"score": 0.0025917743904900576, "phrase": "specific_modifications"}, {"score": 0.0024150092189446424, "phrase": "significant_improvements"}, {"score": 0.0021551702945512494, "phrase": "robot's_learning_behavior"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["human-robot interaction", " reinforcement learning", " user studies"], "paper_abstract": "While Reinforcement Learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a Reinforcement Learning agent: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback, possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. Given this, we made specific modifications to the simulated RL robot, and analyzed and evaluated its learning behavior in four follow-up experiments with human trainers. We report significant improvements on several learning measures. This work demonstrates the importance of understanding the human-teacher/robot-learner partnership in order to design algorithms that support how people want to teach and simultaneously improve the robot's learning behavior. (c) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "paper_id": "WOS:000253604800004"}