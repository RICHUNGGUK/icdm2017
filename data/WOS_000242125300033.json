{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "self-organizing_maps"}, {"score": 0.004664983066310498, "phrase": "actor-critic_model"}, {"score": 0.004567596860086148, "phrase": "basal_ganglia"}, {"score": 0.0045037997020733415, "phrase": "reward-seeking_task"}, {"score": 0.0044408896450445125, "phrase": "continuous_environment"}, {"score": 0.004227515355865597, "phrase": "rat's_basal_ganglia"}, {"score": 0.004183137595671171, "phrase": "task_complexity"}, {"score": 0.003968112038249246, "phrase": "particular_subset"}, {"score": 0.0038444185258456245, "phrase": "classical_method"}, {"score": 0.0036467403626789666, "phrase": "expert's_performance"}, {"score": 0.003608437115371213, "phrase": "strong_limitations"}, {"score": 0.0035082436540296406, "phrase": "continuous_state_space"}, {"score": 0.0034713900276078786, "phrase": "ad_hoc_method"}, {"score": 0.0033513085425415545, "phrase": "present_work"}, {"score": 0.0031901183059524804, "phrase": "autonomously_the_experts'_responsibility_space"}, {"score": 0.003079736435077217, "phrase": "classical_kohonen_maps"}, {"score": 0.0030259817489390302, "phrase": "task_decompositions"}, {"score": 0.0026844452074327265, "phrase": "neural_gas"}, {"score": 0.00247548600804924, "phrase": "good_performances"}, {"score": 0.0023646398663605493, "phrase": "best_kohonen_maps"}, {"score": 0.002188219483646592, "phrase": "current_behavior"}, {"score": 0.002134888329742086, "phrase": "task_decomposition"}, {"score": 0.0021049977753042253, "phrase": "reinforcement_learning_process"}], "paper_keywords": [""], "paper_abstract": "In a reward-seeking task performed in a continuous environment, our previous work compared several Actor-Critic (AC) architectures implementing dopamine-like reinforcement learning mechanisms in the rat's basal ganglia. The task complexity imposes the coordination of several AC submodules, each module being an expert trained in a particular subset of the task. We showed that the classical method where the choice of the expert to train at a given time depends on each expert's performance suffered from strong limitations. We rather proposed to cluster the continuous state space by an ad hoc method that lacked autonomy and generalization abilities. In the present work we have combined the mixture of experts with self-organizing maps in order to cluster autonomously the experts' responsibility space. On the one hand, we find that classical Kohonen maps give very variable results: some task decompositions provide very good and stable reinforcement learning performances, whereas some others are unadapted to the task. Moreover, they require the number of experts to be set a priori. On the other hand, algorithms like Growing Neural Gas or Growing When Required have the property to choose autonomously and incrementally the number of experts to train. They lead to good performances, even if they are still weaker than our hand-tuned task decomposition and than the best Kohonen maps that we got. We finally discuss on propositions about what information to add to these algorithms, such as knowledge of current behavior, in order to make the task decomposition appropriate to the reinforcement learning process.", "paper_title": "Combining self-organizing maps with mixtures of experts: Application to an actor-critic model of reinforcement learning in the basal ganglia", "paper_id": "WOS:000242125300033"}