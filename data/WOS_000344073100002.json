{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "chalearn"}, {"score": 0.004460309246062286, "phrase": "chalearn_gesture_challenges"}, {"score": 0.0035997452281985465, "phrase": "arm_gestures"}, {"score": 0.0035269696779744266, "phrase": "rgb-d_camera"}, {"score": 0.0033004240095380623, "phrase": "small_gesture_vocabulary"}, {"score": 0.0031360786664165093, "phrase": "continuous_sequences"}, {"score": 0.003010496595090984, "phrase": "man-made_annotations"}, {"score": 0.002934566917795928, "phrase": "individual_gestures"}, {"score": 0.0024414225109100672, "phrase": "user's_horizontal_position"}, {"score": 0.0022382002476510573, "phrase": "sample_code"}, {"score": 0.0021928902836049384, "phrase": "new_solutions"}, {"score": 0.0021375377706211686, "phrase": "datacollection_software"}, {"score": 0.0021049977753042253, "phrase": "gesture_vocabularies"}], "paper_keywords": ["Computer vision", " Gesture recognition", " Sign language recognition", " RGBD cameras", " Kinect", " Dataset", " Challenge", " Machine learning", " Transfer learning", " One-shot-learning"], "paper_abstract": "This paper describes the data used in the ChaLearn gesture challenges that took place in 2011/2012, whose results were discussed at the CVPR 2012 and ICPR 2012 conferences. The task can be described as: user-dependent, small vocabulary, fixed camera, one-shot-learning. The data include 54,000 hand and arm gestures recorded with an RGB-D camera. The data are organized into batches of 100 gestures pertaining to a small gesture vocabulary of 8-12 gestures, recorded by the same user. Short continuous sequences of 1-5 randomly selected gestures are recorded. We provide man-made annotations (temporal segmentation into individual gestures, alignment of RGB and depth images, and body part location) and a library of function to preprocess and automatically annotate data. We also provide a subset of batches in which the user's horizontal position is randomly shifted or scaled. We report on the results of the challenge and distribute sample code to facilitate developing new solutions. The data, datacollection software and the gesture vocabularies are downloadable from http://gesture.chalearn.org. We set up a forum for researchers working on these data http://groups.google.com/group/gesturechallenge.", "paper_title": "The ChaLearn gesture dataset (CGD 2011)", "paper_id": "WOS:000344073100002"}