{"auto_keywords": [{"score": 0.03492580197010662, "phrase": "random_sampling"}, {"score": 0.01598274583106002, "phrase": "learning_curves"}, {"score": 0.01374665293158732, "phrase": "alc."}, {"score": 0.010559850352838105, "phrase": "named_entity_recognition"}, {"score": 0.009143367130281375, "phrase": "active_learning"}, {"score": 0.008765077044168783, "phrase": "annotation_cost"}, {"score": 0.008298469332901511, "phrase": "clinical_ner_task"}, {"score": 0.00481495049065317, "phrase": "active_learning_methods"}, {"score": 0.004767118498560508, "phrase": "clinical_text"}, {"score": 0.004696262282646294, "phrase": "ner"}, {"score": 0.004557660361421094, "phrase": "fundamental_tasks"}, {"score": 0.00452365245546062, "phrase": "clinical_natural_language_processing"}, {"score": 0.0044563926211018704, "phrase": "machine_learning"}, {"score": 0.004335658280718254, "phrase": "large_amounts"}, {"score": 0.004314059105213589, "phrase": "annotated_samples"}, {"score": 0.004186695596916661, "phrase": "domain_experts"}, {"score": 0.00403274446211977, "phrase": "supervised_ml"}, {"score": 0.003913653531528535, "phrase": "ml-based_models"}, {"score": 0.0037885886880740993, "phrase": "existing_and_new_al_methods"}, {"score": 0.003713612964777365, "phrase": "medical_problems"}, {"score": 0.0036675057058936263, "phrase": "lab_tests"}, {"score": 0.003640115580601024, "phrase": "clinical_notes"}, {"score": 0.003585945274168956, "phrase": "annotated_ner_corpus"}, {"score": 0.0034800026627449182, "phrase": "al_experiments"}, {"score": 0.003436785843635843, "phrase": "existing_and_novel_algorithms"}, {"score": 0.003244776720584036, "phrase": "ner_model"}, {"score": 0.003220533467925735, "phrase": "estimated_annotation_cost"}, {"score": 0.003133176133652981, "phrase": "training_set"}, {"score": 0.003086526466500671, "phrase": "different_active_learning"}, {"score": 0.0030634619419916214, "phrase": "passive_learning_methods"}, {"score": 0.0030178471028791424, "phrase": "learning_curve"}, {"score": 0.003002797158162766, "phrase": "alc"}, {"score": 0.002863462719650537, "phrase": "uncertainty_sampling_algorithms"}, {"score": 0.002710167675474143, "phrase": "best_method"}, {"score": 0.002689907959061299, "phrase": "uncertainty_sampling"}, {"score": 0.0025458804204949136, "phrase": "uncertainty_sampling_methods"}, {"score": 0.0024276992529591148, "phrase": "best_uncertainty_based_method"}, {"score": 0.0023736447757404487, "phrase": "best_diversity"}, {"score": 0.002314991387596956, "phrase": "simulated_setting"}, {"score": 0.002303435385569474, "phrase": "al_methods"}, {"score": 0.002196483866453684, "phrase": "actual_benefit"}, {"score": 0.0021746070312182995, "phrase": "clinical_ner"}, {"score": 0.002136844445460011, "phrase": "real-time_setting"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Active learning", " Machine learning", " Clinical natural language processing", " Clinical named entity recognition"], "paper_abstract": "Objectives: Named entity recognition (NER), a sequential labeling task, is one of the fundamental tasks for building clinical natural language processing (NLP) systems. Machine learning (ML) based approaches can achieve good performance, but they often require large amounts of annotated samples, which are expensive to build due to the requirement of domain experts in annotation. Active learning (AL), a sample selection approach integrated with supervised ML, aims to minimize the annotation cost while maximizing the performance of ML-based models. In this study, our goal was to develop and evaluate both existing and new AL methods for a clinical NER task to identify concepts of medical problems, treatments, and lab tests from the clinical notes. Methods: Using the annotated NER corpus from the 2010 i2b2/VA NLP challenge that contained 349 clinical documents with 20,423 unique sentences, we simulated AL experiments using a number of existing and novel algorithms in three different categories including uncertainty-based, diversity-based, and baseline sampling strategies. They were compared with the passive learning that uses random sampling. Learning curves that plot performance of the NER model against the estimated annotation cost (based on number of sentences or words in the training set) were generated to evaluate different active learning and the passive learning methods and the area under the learning curve (ALC) score was computed. Results: Based on the learning curves of F-measure vs. number of sentences, uncertainty sampling algorithms outperformed all other methods in ALC. Most diversity-based methods also performed better than random sampling in ALC. To achieve an F-measure of 0.80, the best method based on uncertainty sampling could save 66% annotations in sentences, as compared to random sampling. For the learning curves of F-measure vs. number of words, uncertainty sampling methods again outperformed all other methods in ALC. To achieve 0.80 in F-measure, in comparison to random sampling, the best uncertainty based method saved 42% annotations in words. But the best diversity based method reduced only 7% annotation effort. Conclusion: In the simulated setting, AL methods, particularly uncertainty-sampling based approaches, seemed to significantly save annotation cost for the clinical NER task. The actual benefit of active learning in clinical NER should be further evaluated in a real-time setting. (C) 2015 Elsevier Inc. All rights reserved.", "paper_title": "A study of active learning methods for named entity recognition in clinical text", "paper_id": "WOS:000366791000002"}