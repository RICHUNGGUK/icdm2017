{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "wordnet"}, {"score": 0.03793623403989233, "phrase": "training_set"}, {"score": 0.0362089809584225, "phrase": "visual_terms"}, {"score": 0.004779406424192843, "phrase": "hierarchical_ensembles"}, {"score": 0.004744123493964429, "phrase": "automatic_image_annotation"}, {"score": 0.004622662375029505, "phrase": "image_contents"}, {"score": 0.004571559854487074, "phrase": "pre-defined_set"}, {"score": 0.0044052452943364314, "phrase": "image_high-level_semantics"}, {"score": 0.0043083624695326745, "phrase": "semantic_image_retrieval"}, {"score": 0.004229249483672165, "phrase": "serious_problem"}, {"score": 0.004136221184894985, "phrase": "unsatisfactory_annotation_performance"}, {"score": 0.00407533728096378, "phrase": "semantic_gap"}, {"score": 0.00403026082093352, "phrase": "visual_content"}, {"score": 0.0038548668648948044, "phrase": "new_approach"}, {"score": 0.003784049616066444, "phrase": "lexical_semantics"}, {"score": 0.0037421829222825964, "phrase": "image_annotation_process"}, {"score": 0.00328654473734697, "phrase": "associated_keywords"}, {"score": 0.0032023568549958362, "phrase": "k-means"}, {"score": 0.0031551009669284545, "phrase": "semantic_constraints"}, {"score": 0.003074197032561992, "phrase": "statistical_correlation"}, {"score": 0.0029732094730218488, "phrase": "two-level_hierarchical_ensemble_model"}, {"score": 0.0028862231180242053, "phrase": "co-occurrence_language_model"}, {"score": 0.0027810502577708105, "phrase": "unlabeled_image"}, {"score": 0.0026996706283276407, "phrase": "posterior_probability"}, {"score": 0.0026401983633087267, "phrase": "visual_term"}, {"score": 0.0026109540617641593, "phrase": "first-level_classifier_ensemble"}, {"score": 0.0025724636254035337, "phrase": "second-level_language_model"}, {"score": 0.0025157865918985682, "phrase": "annotation_quality"}, {"score": 0.002497172413754263, "phrase": "word_co-occurrence_statistics"}, {"score": 0.0024603551979225195, "phrase": "annotated_keywords"}, {"score": 0.002344398099179438, "phrase": "medium-sized_image_collection"}, {"score": 0.0023270490574896804, "phrase": "corel_stock_photo"}, {"score": 0.0022927343007881846, "phrase": "experimental_results"}, {"score": 0.002258924405138231, "phrase": "annotation_performance"}, {"score": 0.0022091400304857043, "phrase": "traditional_annotation_methods"}, {"score": 0.002176560171729638, "phrase": "average_precision"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": [""], "paper_abstract": "Automatic image annotation concerns a process of automatically labeling image contents with a pre-defined set of keywords, which are regarded as descriptors of image high-level semantics, so as to enable semantic image retrieval via keywords. A serious problem in this task is the unsatisfactory annotation performance due to the semantic gap between the visual content and keywords. Targeting at this problem, we present a new approach that tries to incorporate lexical semantics into the image annotation process. In the phase of training, given a training set of images labeled with keywords, a basic visual vocabulary consisting of visual terms, extracted from the image to represent its content, and the associated keywords is generated at first, using K-means clustering combined with semantic constraints obtained from WordNet, then the statistical correlation between visual terms and keywords is modeled by a two-level hierarchical ensemble model composed of probabilistic SVM classifiers and a co-occurrence language model. In the phase of annotation, given an unlabeled image, the most likely associated keywords are predicted by the posterior probability of each keyword given each visual term at the first-level classifier ensemble, then the second-level language model is used to refine the annotation quality by word co-occurrence statistics derived from the annotated keywords in the training set of images. We carried out experiments on a medium-sized image collection from Corel Stock Photo CDs. The experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 7% in average precision, showing the feasibility and effectiveness of the proposed approach.", "paper_title": "Automatic image annotation based on WordNet and hierarchical ensembles", "paper_id": "WOS:000236109700044"}