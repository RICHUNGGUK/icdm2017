{"auto_keywords": [{"score": 0.04008926140547966, "phrase": "target_language"}, {"score": 0.012811375745116083, "phrase": "source_language"}, {"score": 0.00481495049065317, "phrase": "multi-kernel_svms"}, {"score": 0.0047145329604366395, "phrase": "fine_grained_opinion_analysis"}, {"score": 0.004635701867952543, "phrase": "annotated_corpus"}, {"score": 0.004577440795549049, "phrase": "high_quality_analysis"}, {"score": 0.004481954307639453, "phrase": "insufficient_resources"}, {"score": 0.004278807087784626, "phrase": "lingual_resources"}, {"score": 0.004189523950071494, "phrase": "resource_poor_languages"}, {"score": 0.004084829853870028, "phrase": "novel_approach"}, {"score": 0.004033464035028803, "phrase": "lingual_opinion_holder_extraction"}, {"score": 0.003982741546775355, "phrase": "finely_annotated_opinion_corpus"}, {"score": 0.0038668462001710314, "phrase": "supplementary_training_samples"}, {"score": 0.003738502710504415, "phrase": "opinion_corpus"}, {"score": 0.003660452313567116, "phrase": "fine_grained_annotations"}, {"score": 0.0034944094552943, "phrase": "training_samples"}, {"score": 0.003378385493203297, "phrase": "multi-kernel_support_vector_machines"}, {"score": 0.003266201217790818, "phrase": "opinion_holders"}, {"score": 0.0031577303835881964, "phrase": "tree_kernel"}, {"score": 0.003117986133774198, "phrase": "syntactic_features"}, {"score": 0.0030787405733989615, "phrase": "polynomial_kernel"}, {"score": 0.0030399874838260886, "phrase": "semantic_features"}, {"score": 0.002889778121878728, "phrase": "pivot_function"}, {"score": 0.002853396755102163, "phrase": "word_pair_similarity"}, {"score": 0.002770273247998903, "phrase": "low_quality_translated_samples"}, {"score": 0.002735392281320413, "phrase": "transfer_learning_algorithm"}, {"score": 0.0026782279715171866, "phrase": "high_quality_translated_samples"}, {"score": 0.0026222551419772867, "phrase": "multi-kernel_classifiers"}, {"score": 0.002535115422089855, "phrase": "mpqa"}, {"score": 0.0025032321772556782, "phrase": "english"}, {"score": 0.0023996317685956213, "phrase": "chinese_opinion_analysis"}, {"score": 0.0023100907305651872, "phrase": "opinion_holder_extraction_performance"}], "paper_keywords": ["Opinion holder extraction", " Cross lingual", " Multi-kernel SVMs", " Transfer learning"], "paper_abstract": "Fine grained opinion analysis has much higher demand for annotated corpus which makes high quality analysis difficult when there are insufficient resources. In this paper we explore the use of cross lingual resources for opinion mining for resource poor languages. This paper presents a novel approach for cross lingual opinion holder extraction through leveraging finely annotated opinion corpus selectively from a source language as the supplementary training samples for the target language. Firstly, the opinion corpus in the source language with fine grained annotations are translated and projected to the target language to generate the training samples. Then, a classifier based on multi-kernel Support Vector Machines (SVMs) is developed to identify opinion holders in the target language, which uses a tree kernel based on syntactic features and a polynomial kernel based on semantic features, respectively. The two kernels are further improved by incorporating a pivot function based on word pair similarity. To reduce the noise of low quality translated samples, a Transfer learning algorithm is applied to select high quality translated samples iteratively for training the multi-kernel classifiers on the target language. Evaluations on transferring MPQA, an English opinion corpus (as the source language), to Chinese opinion analysis (as the target language) show that the opinion holder extraction performance on NTCIR-7 MOAT dataset is improved, which is higher than the Conditional Random Fields (CRFs) based approach and most reported systems in NTCIR-7 MOAT evaluation.", "paper_title": "Cross lingual opinion holder extraction based on multi-kernel SVMs and transfer learning", "paper_id": "WOS:000349608400008"}