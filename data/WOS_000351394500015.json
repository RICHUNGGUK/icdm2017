{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multi-modal_retrieval"}, {"score": 0.047032949642620024, "phrase": "multimedia_documents"}, {"score": 0.004777917315737992, "phrase": "content_and_semantics_similarities"}, {"score": 0.004650519511749445, "phrase": "ongoing_development"}, {"score": 0.004544016163767266, "phrase": "large_number"}, {"score": 0.004355026964936104, "phrase": "daily_life"}, {"score": 0.00406250784433007, "phrase": "important_issue"}, {"score": 0.0037749456215035856, "phrase": "single_information_source"}, {"score": 0.0035761182400032487, "phrase": "semantic_level"}, {"score": 0.0034538466642399976, "phrase": "novel_probabilistic_model"}, {"score": 0.0034140249882206085, "phrase": "ccss"}, {"score": 0.0033357416981328577, "phrase": "low-level_content_and_high-level_semantics_similarities"}, {"score": 0.0032601603775768793, "phrase": "markov"}, {"score": 0.0031845067058084613, "phrase": "heterogeneous_similarity_measures"}, {"score": 0.0031599739264865554, "phrase": "different_unimedia_types"}, {"score": 0.003123527559542239, "phrase": "ranked_list"}, {"score": 0.0030166837276747816, "phrase": "optimal_path"}, {"score": 0.0029589086332593674, "phrase": "content_similarity"}, {"score": 0.002913483929932102, "phrase": "internal_structure"}, {"score": 0.002846647336160784, "phrase": "semantics_similarity"}, {"score": 0.002802941205762953, "phrase": "semantic_correlation"}, {"score": 0.0027813397260607487, "phrase": "different_modalities"}, {"score": 0.0026143990748127253, "phrase": "multi-class_logistic_regression"}, {"score": 0.0025942468561537682, "phrase": "random_forests"}, {"score": 0.0025347150832077175, "phrase": "original_features"}, {"score": 0.002476546030310661, "phrase": "semantic_space"}, {"score": 0.002429090371567499, "phrase": "query-by-example_scenario"}, {"score": 0.002373339527491525, "phrase": "wikipedia_dataset_show"}, {"score": 0.0022136307563123256, "phrase": "cross-modal_retrieval"}, {"score": 0.002171202123281881, "phrase": "proposed_multi-modal_method"}, {"score": 0.0021213575697367148, "phrase": "previous_systems"}, {"score": 0.0021049977753042253, "phrase": "image_retrieval_task"}], "paper_keywords": ["Cross-modal retrieval", " Multi-modal retrieval", " Content similarity", " Semantics similarity", " Probabilistic model"], "paper_abstract": "With the ongoing development of the internet, a large number of multimedia documents containing images and texts have appeared in the daily life of people. Therefore, how to effectively and efficiently conduct cross-modal and multi-modal retrieval is being an important issue. Although some methods have been proposed to deal with the issue, their retrieval processes are confined to a single information source of multimedia documents, such as the representations of images and texts at a semantic level. In this paper, we propose a novel probabilistic model, namely CCSS, which not only combines low-level content and high-level semantics similarities through a first-order Markov chain, but also provides heterogeneous similarity measures for different unimedia types. The ranked list for a query is obtained by highlighting an optimal path across the chain. Content similarity focuses on the internal structure of each modality, while semantics similarity focuses on the semantic correlation between different modalities. Both of them are significant and their combination can be complementary to each other. Multi-class logistic regression and random forests are used to map the original features of each unimedia into a semantic space. According to the query-by-example scenario, the experiments on the Wikipedia dataset show that the performance of our model significantly outperforms those of state-of-the-art approaches for cross-modal retrieval. Additionally, the proposed multi-modal method is also shown to outperform previous systems on image retrieval task.", "paper_title": "Improving cross-modal and multi-modal retrieval combining content and semantics similarities with probabilistic model", "paper_id": "WOS:000351394500015"}