{"auto_keywords": [{"score": 0.043007234959413346, "phrase": "rl_problems"}, {"score": 0.039233071846805946, "phrase": "proposed_algorithm"}, {"score": 0.004724020632103649, "phrase": "reinforcement_learning"}, {"score": 0.004664465192979084, "phrase": "rl"}, {"score": 0.00454725669814484, "phrase": "extensive_studies"}, {"score": 0.004349337717774096, "phrase": "continuous_state_ri_problems"}, {"score": 0.004055511503825102, "phrase": "continuous_action_spaces"}, {"score": 0.0032253065687854357, "phrase": "gsom"}, {"score": 0.0031643005824589917, "phrase": "better_performance"}, {"score": 0.0031242709509664837, "phrase": "topology_preservation"}, {"score": 0.0030457198309616694, "phrase": "non-stationary_distribution_approximation"}, {"score": 0.0029691399875682416, "phrase": "som."}, {"score": 0.002931569771634119, "phrase": "novel_algorithm"}, {"score": 0.002750721367040336, "phrase": "best_representation"}, {"score": 0.0026986685585382347, "phrase": "state_space"}, {"score": 0.0025160740422447837, "phrase": "highly_rewarded_regions"}, {"score": 0.002468450551616404, "phrase": "action_space"}, {"score": 0.0024372021710656585, "phrase": "experimental_results"}, {"score": 0.0024063484128685367, "phrase": "delayed_reward"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Growing self-organizing maps", " Continuous state/action spaces"], "paper_abstract": "This paper proposes an algorithm to deal with continuous state/action space in the reinforcement learning (RL) problem. Extensive studies have been done to solve the continuous state RI problems, but more research should be carried out for RL problems with continuous action spaces. Due to non-stationary, very large size, and continuous nature of RL problems, the proposed algorithm uses two growing self-organizing maps (GSOM) to elegantly approximate the state/action space through addition and deletion of neurons. It has been demonstrated that GSOM has a better performance in topology preservation, quantization error reduction, and non-stationary distribution approximation than the standard SOM. The novel algorithm proposed in this paper attempts to simultaneously find the best representation for the state space, accurate estimation of Q-values, and appropriate representation for highly rewarded regions in the action space. Experimental results on delayed reward, non-stationary, and large-scale problems demonstrate very satisfactory performance of the proposed algorithm. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Continuous state/action reinforcement learning: A growing self-organizing map approach", "paper_id": "WOS:000288412700001"}