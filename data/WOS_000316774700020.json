{"auto_keywords": [{"score": 0.03610310423475189, "phrase": "undesired_transitions"}, {"score": 0.0311896592498224, "phrase": "usap_module"}, {"score": 0.00481495049065317, "phrase": "multi-agent_reinforcement"}, {"score": 0.004745260932479212, "phrase": "linked_multi-component_robotic_system_control"}, {"score": 0.004454691230204595, "phrase": "multi-component_robotic_systems"}, {"score": 0.0043477114072437316, "phrase": "multi-agent_reinforcement_learning"}, {"score": 0.004202223763361474, "phrase": "linked_mcrs"}, {"score": 0.004121275290411653, "phrase": "over-constrained_environments"}, {"score": 0.004061584718427419, "phrase": "great_difficulties"}, {"score": 0.0040222701316738295, "phrase": "efficient_learning"}, {"score": 0.003925633992809873, "phrase": "multi-agent_reinforcement_algorithms"}, {"score": 0.003757480190200645, "phrase": "hybrid_learning_algorithm"}, {"score": 0.003685067218455088, "phrase": "modified_q-learning_algorithm"}, {"score": 0.003359647377874098, "phrase": "physical_constraints"}, {"score": 0.003310951184365485, "phrase": "usap_module's_output"}, {"score": 0.00323134923196813, "phrase": "q-learning_algorithm"}, {"score": 0.003107935254560724, "phrase": "learning_efficiency"}, {"score": 0.0030628762739398855, "phrase": "hybrid_approach"}, {"score": 0.002989220655735981, "phrase": "multi-agent_case"}, {"score": 0.002917331114260319, "phrase": "distributed_round-robin_q-learning"}, {"score": 0.002659527314613792, "phrase": "computational_experiments"}, {"score": 0.002608218347502229, "phrase": "classical_multi-agent_taxi_scheduling_task"}, {"score": 0.0024842242921857705, "phrase": "considerable_learning_gain"}, {"score": 0.002366110899738696, "phrase": "state-of-the-art_distributed_q-learning_approach"}, {"score": 0.0022536005664642294, "phrase": "hose_transportation_task"}, {"score": 0.002199363362666037, "phrase": "significant_improvement"}, {"score": 0.0021674482571326283, "phrase": "convergence_speed"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Hose transportation", " Multi-robot systems", " Multi-agent reinforcement learning", " Distributed Q-Learning"], "paper_abstract": "The paper deals with the problem of learning the control of Multi-Component Robotic Systems (MCRSs) applying Multi-Agent Reinforcement Learning (MARL) algorithms. Modeling Linked MCRS usually leads to over-constrained environments, posing great difficulties for efficient learning with conventional single and multi-agent reinforcement algorithms. In this paper, we propose a hybrid learning algorithm composed of a modified Q-Learning algorithm embedding an Undesired State-Action Prediction (USAP) module trained by a supervised learning approach which learns a model predicting undesired transitions to states breaking physical constraints. The USAP module's output is used by the Q-Learning algorithm to prevent these undesired transitions, therefore boosting learning efficiency. This hybrid approach is extended to the multi-agent case embedding the USAP module in Distributed Round-Robin Q-Learning (D-RR-QL), which requires very little communications among agents. We present results of computational experiments conducted in the classical multi-agent taxi scheduling task and a hose transportation task. Results show a considerable learning gain both in time and accuracy, compared to the state-of-the-art Distributed Q-Learning approach in the deterministic taxi scheduling task. In the hose transportation task, USAP module introduces a significant improvement in learning convergence speed. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Undesired state-action prediction in multi-agent reinforcement learning for linked multi-component robotic system control", "paper_id": "WOS:000316774700020"}