{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "object_manipulation_activities"}, {"score": 0.00470862827326028, "phrase": "visual_cues"}, {"score": 0.004403480614319231, "phrase": "human_activities"}, {"score": 0.004118026552723285, "phrase": "models_high-level_tasks"}, {"score": 0.00400459716272594, "phrase": "realistic_settings"}, {"score": 0.0038725824361608243, "phrase": "computer_vision"}, {"score": 0.003808209866576388, "phrase": "sporadic_occlusion"}, {"score": 0.003724034990158436, "phrase": "non-frontal_poses"}, {"score": 0.003405472159599442, "phrase": "depth_data"}, {"score": 0.003293142761521425, "phrase": "simple_and_fast_techniques"}, {"score": 0.0029945352738604742, "phrase": "manipulating_hand"}, {"score": 0.002847540262784593, "phrase": "temporal_information"}, {"score": 0.0027382040767844093, "phrase": "sporadically_detected_objects"}, {"score": 0.002677616534005393, "phrase": "careful_inclusion"}, {"score": 0.0026478260192958924, "phrase": "depth_cues"}, {"score": 0.0025461386232786356, "phrase": "challenging_dataset"}, {"score": 0.0023941575109692336, "phrase": "entire_framework"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Activity recognition", " Action recognition", " Joint object and action recognition", " HMM", " Depth camera", " Temporal action recognition", " Temporal smoothing", " Boost"], "paper_abstract": "We propose a framework, consisting of several algorithms to recognize human activities that involve manipulating objects. Our proposed algorithm identifies objects being manipulated and models high-level tasks being performed accordingly. Realistic settings for such tasks pose several problems for computer vision, including sporadic occlusion by subjects, non-frontal poses, and objects with few local features. We show how size and segmentation information derived from depth data can address these challenges using simple and fast techniques. In particular, we show how to robustly and without super-vision find the manipulating hand, properly detect/recognize objects and properly use the temporal information to fill in the gaps between sporadically detected objects, all through careful inclusion of depth cues. We evaluate our approach on a challenging dataset of 12 kitchen tasks that involve 24 objects performed by 2 subjects. The entire framework yields 82%/84% precision (74%/83%recall) for task/object recognition. Our techniques outperform the state-of-the-art significantly in activity/object recognition. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Recognizing object manipulation activities using depth and visual cues", "paper_id": "WOS:000333072500012"}