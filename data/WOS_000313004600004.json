{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "topology-based_multi-agent_systems"}, {"score": 0.00474283953918251, "phrase": "tmas"}, {"score": 0.004365022891478405, "phrase": "topological_constraints"}, {"score": 0.004299621153829837, "phrase": "tmas_system"}, {"score": 0.004156012879026324, "phrase": "different_state_space"}, {"score": 0.004002044115117644, "phrase": "traditional_approaches"}, {"score": 0.003971938976270902, "phrase": "multi-agent_cooperative_learning"}, {"score": 0.00378167947598949, "phrase": "network_topology"}, {"score": 0.0036553070575015344, "phrase": "cooperative_learning_strategy"}, {"score": 0.0036005006349430586, "phrase": "autonomous_agents"}, {"score": 0.0035331426502479687, "phrase": "binary_tree_formation"}, {"score": 0.003325921766364862, "phrase": "state_space"}, {"score": 0.0033008858616145205, "phrase": "individual_agents"}, {"score": 0.003263683763829895, "phrase": "policy_sharing"}, {"score": 0.0031784960695034645, "phrase": "multi-agent_systems"}, {"score": 0.0031426778800597694, "phrase": "btf"}, {"score": 0.00307221807606505, "phrase": "higher_level"}, {"score": 0.0029920129598858545, "phrase": "general_form"}, {"score": 0.002763709443278514, "phrase": "temporal_difference-fusion_architecture"}, {"score": 0.0026511422582578027, "phrase": "comparative_experiments"}, {"score": 0.0026113524778702624, "phrase": "generic_network_routing_problem"}, {"score": 0.002562451837774567, "phrase": "typical_tmas_domain"}, {"score": 0.002514464610980629, "phrase": "td-falcon_btf_teams"}, {"score": 0.002458061764297103, "phrase": "td-falcon_teams"}, {"score": 0.0024120247307686084, "phrase": "n-ary_tree_formation"}, {"score": 0.0023225152126545067, "phrase": "table_lookup_mechanism"}, {"score": 0.00227040825063645, "phrase": "classical_linear_programming_algorithm"}, {"score": 0.0022110892229209407, "phrase": "td-falcon_btf"}, {"score": 0.002137088643982437, "phrase": "network_complexity"}, {"score": 0.002120982646893519, "phrase": "traffic_volume"}, {"score": 0.0021049977753042253, "phrase": "tmas_domains"}], "paper_keywords": ["Topology-based multi-agent systems", " Cooperative learning", " Reinforcement learning", " Binary tree formation", " Policy sharing"], "paper_abstract": "Topology-based multi-agent systems (TMAS), wherein agents interact with one another according to their spatial relationship in a network, are well suited for problems with topological constraints. In a TMAS system, however, each agent may have a different state space, which can be rather large. Consequently, traditional approaches to multi-agent cooperative learning may not be able to scale up with the complexity of the network topology. In this paper, we propose a cooperative learning strategy, under which autonomous agents are assembled in a binary tree formation (BTF). By constraining the interaction between agents, we effectively unify the state space of individual agents and enable policy sharing across agents. Our complexity analysis indicates that multi-agent systems with the BTF have a much smaller state space and a higher level of flexibility, compared with the general form of n-ary (n > 2) tree formation. We have applied the proposed cooperative learning strategy to a class of reinforcement learning agents known as temporal difference-fusion architecture for learning and cognition (TD-FALCON). Comparative experiments based on a generic network routing problem, which is a typical TMAS domain, show that the TD-FALCON BTF teams outperform alternative methods, including TD-FALCON teams in single agent and n-ary tree formation, a Q-learning method based on the table lookup mechanism, as well as a classical linear programming algorithm. Our study further shows that TD-FALCON BTF can adapt and function well under various scales of network complexity and traffic volume in TMAS domains.", "paper_title": "Cooperative reinforcement learning in topology-based multi-agent systems", "paper_id": "WOS:000313004600004"}