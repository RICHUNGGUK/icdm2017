{"auto_keywords": [{"score": 0.048427710852443524, "phrase": "image_search"}, {"score": 0.0472596937865907, "phrase": "exemplar_images"}, {"score": 0.00481495049065317, "phrase": "interactive_image_search"}, {"score": 0.004784331242964228, "phrase": "relative_attribute_feedback"}, {"score": 0.00470862827326028, "phrase": "novel_mode"}, {"score": 0.004251582719171643, "phrase": "image_results"}, {"score": 0.004197650347643135, "phrase": "\"black_shoes"}, {"score": 0.004052823656906696, "phrase": "shoe_images"}, {"score": 0.0038510053729959074, "phrase": "ranking_functions"}, {"score": 0.0037658881407605445, "phrase": "relative_strength"}, {"score": 0.0037299855191553, "phrase": "nameable_attribute"}, {"score": 0.003601235776892337, "phrase": "query_time"}, {"score": 0.003367611042980526, "phrase": "comparative_statements"}, {"score": 0.0032721691785599833, "phrase": "multi-dimensional_attribute_space"}, {"score": 0.0026584273498059295, "phrase": "irrelevant_portions"}, {"score": 0.0026330549554033876, "phrase": "visual_feature_space"}, {"score": 0.0026079240853929555, "phrase": "semantic_language"}, {"score": 0.002376994899030371, "phrase": "traditional_binary_relevance_feedback"}, {"score": 0.002346786090713685, "phrase": "search_speed"}, {"score": 0.0022875127203850027, "phrase": "ordinal_nature"}, {"score": 0.0022729292542096077, "phrase": "relative_attributes"}, {"score": 0.002173409712988074, "phrase": "reference_images"}, {"score": 0.0021049977753042253, "phrase": "conventional_passive_and_active_methods"}], "paper_keywords": ["Content-based image search", " Interactive image search", " Active selection", " Relative attributes"], "paper_abstract": "We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query \"black shoes\", the user might state, \"Show me shoe images like these, but sportier.\" Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently \"whittle away\" irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient-both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.", "paper_title": "WhittleSearch: Interactive Image Search with Relative Attribute Feedback", "paper_id": "WOS:000362285700006"}