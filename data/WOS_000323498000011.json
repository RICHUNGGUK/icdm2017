{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "path_planning"}, {"score": 0.004737579882204798, "phrase": "mobile_robot"}, {"score": 0.00441635700580562, "phrase": "presumed_knowledge"}, {"score": 0.00427550604912774, "phrase": "current_state"}, {"score": 0.0033888405252810927, "phrase": "proposed_algorithm"}, {"score": 0.003334309053314909, "phrase": "insignificantly_small_time_complexity"}, {"score": 0.003057910095857785, "phrase": "best_possible_action"}, {"score": 0.002944284514509205, "phrase": "significant_storage"}, {"score": 0.002865710140187219, "phrase": "simulated_maze"}, {"score": 0.0028348690177346448, "phrase": "real_platforms"}, {"score": 0.002714779217561487, "phrase": "proposed_q-learning"}, {"score": 0.0026423134393535243, "phrase": "path-planning_application"}, {"score": 0.0025031188149336257, "phrase": "extended_q-learning"}, {"score": 0.0021863046959469863, "phrase": "energy_consumption"}, {"score": 0.0021049977753042253, "phrase": "robotics_literature"}], "paper_keywords": ["Agent", " mobile robots", " path planning", " Q-learning", " reinforcement learning"], "paper_abstract": "This paper provides a new deterministic Q-learning with a presumed knowledge about the distance from the current state to both the next state and the goal. This knowledge is efficiently used to update the entries in the Q-table once only by utilizing four derived properties of the Q-learning, instead of repeatedly updating them like the classical Q-learning. Naturally, the proposed algorithm has an insignificantly small time complexity in comparison to its classical counterpart. Furthermore, the proposed algorithm stores the Q-value for the best possible action at a state and thus saves significant storage. Experiments undertaken on simulated maze and real platforms confirm that the Q-table obtained by the proposed Q-learning when used for the path-planning application of mobile robots outperforms both the classical and the extended Q-learning with respect to three metrics: traversal time, number of states traversed, and 90 degrees turns required. The reduction in 90 degrees turnings minimizes the energy consumption and thus has importance in the robotics literature.", "paper_title": "A Deterministic Improved Q-Learning for Path Planning of a Mobile Robot", "paper_id": "WOS:000323498000011"}