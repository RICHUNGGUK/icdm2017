{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multi-agent_systems"}, {"score": 0.004515252649226697, "phrase": "original_reinforcement_learning"}, {"score": 0.004044064541716598, "phrase": "realistic_setting"}, {"score": 0.003970439654470374, "phrase": "situated_agents"}, {"score": 0.0038981499110532307, "phrase": "local_perception"}, {"score": 0.0036218682585355895, "phrase": "coordinated_system"}, {"score": 0.0035233696341819437, "phrase": "crucial_importance"}, {"score": 0.00327356326590461, "phrase": "simple_reactive_agents"}, {"score": 0.0031845067058084583, "phrase": "decentralized_way"}, {"score": 0.003126481516783286, "phrase": "independent_learners"}, {"score": 0.002851859621184338, "phrase": "rl"}, {"score": 0.0026011706232569316, "phrase": "incremental_learning_algorithm"}, {"score": 0.0024389277933858054, "phrase": "progressively_more_complex_tasks"}, {"score": 0.002329258721870127, "phrase": "general_framework"}, {"score": 0.002286781334850213, "phrase": "computer_experiments"}, {"score": 0.0021049977753042253, "phrase": "global_goal"}], "paper_keywords": ["reinforcement learning", " multi-agent systems", " partially observable Markov decision processes", " shaping", " policy-gradient"], "paper_abstract": "An original reinforcement learning (RL) methodology is proposed for the design of multi-agent systems. In the realistic setting of situated agents with local perception, the task of automatically building a coordinated system is of crucial importance. To that end, we design simple reactive agents in a decentralized way as independent learners. But to cope with the difficulties inherent to RL used in that framework, we have developed an incremental learning algorithm where agents face a sequence of progressively more complex tasks. We illustrate this general framework by computer experiments where agents have to coordinate to reach a global goal.", "paper_title": "Shaping multi-agent systems with gradient reinforcement learning", "paper_id": "WOS:000248894000003"}