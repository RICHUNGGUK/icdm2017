{"auto_keywords": [{"score": 0.04955228514169987, "phrase": "non-markovian_rewards"}, {"score": 0.007775411473818948, "phrase": "decision-theoretic_planning"}, {"score": 0.004814951563358001, "phrase": "decision-theoretic"}, {"score": 0.004709567162738894, "phrase": "decision_process"}, {"score": 0.0045915634864557185, "phrase": "nmrdpp"}, {"score": 0.004505638402009237, "phrase": "current_state"}, {"score": 0.004123781267600542, "phrase": "execution_sequences"}, {"score": 0.004003798546806401, "phrase": "nmrdps"}, {"score": 0.003916098498242888, "phrase": "commonly_adopted_fully_markovian_decision_process"}, {"score": 0.0038873210159255456, "phrase": "mdp"}, {"score": 0.0035708152831417104, "phrase": "solution_methods"}, {"score": 0.003390886284555397, "phrase": "compact_specification"}, {"score": 0.003353521531649663, "phrase": "non-markovian_reward_function"}, {"score": 0.0032438719265030194, "phrase": "nmrdp"}, {"score": 0.0032081215771180664, "phrase": "equivalent_mdp"}, {"score": 0.0031494088980556934, "phrase": "efficient_mdp_solution_methods"}, {"score": 0.0030464123943266673, "phrase": "software_platform"}, {"score": 0.002860938351029155, "phrase": "nmrdpp_implements"}, {"score": 0.0028189599390180536, "phrase": "single_interface"}, {"score": 0.0026966744704447275, "phrase": "new_approaches"}, {"score": 0.0026084451971464867, "phrase": "dynamic_programming"}, {"score": 0.0025892330566358503, "phrase": "heuristic_search"}, {"score": 0.0025606791601860543, "phrase": "structured_methods"}, {"score": 0.0023958326775913165, "phrase": "nmrdpp's_treatment"}, {"score": 0.002317423416027688, "phrase": "domain-specific_search_control_knowledge"}, {"score": 0.0022250584824198218, "phrase": "special_case"}, {"score": 0.002192389791199368, "phrase": "first_international_probabilistic_planning_competition"}], "paper_keywords": [""], "paper_abstract": "A decision process in which rewards depend on history rather than merely on the current state is called a decision process with non-Markovian rewards (NMRDP). In decision-theoretic planning, where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states, NMRDPs form a more natural model than the commonly adopted fully Markovian decision process (MDP) model. While the more tractable solution methods developed for MDPs do not directly apply in the presence of non-Markovian rewards, a number of solution methods for NMRDPs have been proposed in the literature. These all exploit a compact specification of the non-Markovian reward function in temporal logic, to automatically translate the NMRDP into an equivalent MDP which is solved using efficient MDP solution methods. This paper presents NMRDPP(Non-Markovian Reward Decision Process Planner), a software platform for the development and experimentation of methods for decision-theoretic planning with non-Markovian rewards. The current version of NMRDPP implements, under a single interface, a family of methods based on existing as well as new approaches which we describe in detail. These include dynamic programming, heuristic search, and structured methods. Using NMRDPP, we compare the methods and identify certain problem features that affect their performance. NMRDPP's treatment of non-Markovian rewards is inspired by the treatment of domain-specific search control knowledge in the TLPlan planner, which it incorporates as a special case. In the First International Probabilistic Planning Competition, NMRDPP was able to compete and perform well in both the domain-independent and hand-coded tracks, using search control knowledge in the latter.", "paper_title": "Decision-theoretic planning with non-Markovian rewards", "paper_id": "WOS:000235084000001"}