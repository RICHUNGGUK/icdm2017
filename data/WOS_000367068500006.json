{"auto_keywords": [{"score": 0.04043812428061782, "phrase": "energy_function"}, {"score": 0.03980713684810354, "phrase": "non-negative_orthant"}, {"score": 0.015719716506582538, "phrase": "recurrent_neural_networks"}, {"score": 0.013232489800110598, "phrase": "casl"}, {"score": 0.012880631330203644, "phrase": "optimization_problem"}, {"score": 0.011879539873049173, "phrase": "minimum_points"}, {"score": 0.009993749661939009, "phrase": "stable_attractors"}, {"score": 0.0033810134162565843, "phrase": "lotka-volterra_recurrent_neural_networks"}, {"score": 0.0031845067058084583, "phrase": "sufficient_conditions"}, {"score": 0.002999386721353559, "phrase": "network_model"}, {"score": 0.0025611773887867255, "phrase": "proposed_lv_rnns"}, {"score": 0.002347378595930709, "phrase": "synthetic_and_real_data"}, {"score": 0.0021049977753042253, "phrase": "multiple_manifolds"}], "paper_keywords": ["Closed affine subspace learning (CASL)", " Non-negative matrix factorization (NMF)", " Sparse representation", " Multiple manifolds clustering and embedding", " Lotka-Volterra recurrent neural networks (LV RNNs)"], "paper_abstract": "In this paper, the problem that we are interested is constructing l(1)-graphs for subspace learning via recurrent neural networks. We propose a closed affine subspace learning (CASL) method to do so. The problem of CASL is formulated as an optimization problem described by an energy function in the non-negative orthant. The sufficient conditions for keeping the minimum points of the energy function in the non-negative orthant are given. A model of Lotka-Volterra recurrent neural networks is constructed to solve the optimization problem in terms of these sufficient conditions. It shows that the set of stable attractors of the network model just equals the set of minimum points of the energy function in the non-negative orthant. Based on these equivalences, the problem of CASL can be solved by running the proposed LV RNNs to obtain the stable attractors. The l(1)-graphs then can be constructed. Experiments on some synthetic and real data show that the l(1)-graphs constructed in this way are more effective, especially when processing the data sampled from multiple manifolds.", "paper_title": "Constructing L-1-graphs for subspace learning via recurrent neural networks", "paper_id": "WOS:000367068500006"}