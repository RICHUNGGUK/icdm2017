{"auto_keywords": [{"score": 0.04360563439799413, "phrase": "error_estimation"}, {"score": 0.031481782621283184, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "support_vector_machines"}, {"score": 0.004700468701207975, "phrase": "maximal_discrepancy"}, {"score": 0.0044795698035664695, "phrase": "powerful_statistical_method"}, {"score": 0.004234884779773412, "phrase": "model_selection"}, {"score": 0.004101088357088207, "phrase": "classification_problems"}, {"score": 0.0037847308929559163, "phrase": "small_sample_problems"}, {"score": 0.0035208552733545463, "phrase": "separate_validation"}, {"score": 0.0033552055241931346, "phrase": "md_method"}, {"score": 0.0032753168662834516, "phrase": "bounded_loss_function"}, {"score": 0.0027667539384678814, "phrase": "non-convex_optimization_problem"}, {"score": 0.0025944417620244924, "phrase": "new_approach"}, {"score": 0.0024922258873137093, "phrase": "md_technique"}, {"score": 0.002209060366991921, "phrase": "original_svm_framework"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Maximal discrepancy (MD)", " Error estimation", " Support vector machine (SVM)"], "paper_abstract": "The Maximal Discrepancy (MD) is a powerful statistical method, which has been proposed for model selection and error estimation in classification problems. This approach is particularly attractive when dealing with small sample problems, since it avoids the use of a separate validation set. Unfortunately, the MD method requires a bounded loss function, which is usually avoided by most learning algorithms, including the Support Vector Machine (SVM), because it gives rise to a non-convex optimization problem. We derive in this work a new approach for rigorously applying the MD technique to the error estimation of the SVM and, at the same time, preserving the original SVM framework. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Maximal Discrepancy for Support Vector Machines", "paper_id": "WOS:000290078600015"}