{"auto_keywords": [{"score": 0.03379621455202772, "phrase": "soft_pointwise_constraints"}, {"score": 0.031071491193311245, "phrase": "optimal_solution"}, {"score": 0.004746065479362881, "phrase": "learning_paradigm"}, {"score": 0.004545240514015278, "phrase": "classical_framework"}, {"score": 0.004311241567238011, "phrase": "hard_pointwise_constraints"}, {"score": 0.004148631410346188, "phrase": "finite_set"}, {"score": 0.0038415095105632157, "phrase": "coherent_decisions"}, {"score": 0.003750260412839128, "phrase": "different_views"}, {"score": 0.003643607569483598, "phrase": "classical_examples"}, {"score": 0.003608732492631878, "phrase": "supervised_learning"}, {"score": 0.003293570937654716, "phrase": "suitable_loss_function"}, {"score": 0.003169222811082848, "phrase": "constrained_variational_calculus"}, {"score": 0.0030790429005600898, "phrase": "representer_theorem"}, {"score": 0.002977061391551631, "phrase": "functional_structure"}, {"score": 0.0028923335491735564, "phrase": "proposed_learning_paradigm"}, {"score": 0.002677964894127131, "phrase": "support_constraints"}, {"score": 0.0025892330566358503, "phrase": "support_vectors"}, {"score": 0.0025034338848015166, "phrase": "novel_learning_paradigm"}, {"score": 0.0024088451312725924, "phrase": "general_theory"}, {"score": 0.0022302306304023602, "phrase": "hard_linear_pointwise_constraints"}, {"score": 0.002166711569953301, "phrase": "supervised_examples"}, {"score": 0.0021049977753042253, "phrase": "closed-form_optimal_solutions"}], "paper_keywords": ["Constrained variational calculus", " hard and soft constraints", " parsimony principle", " representer theorems", " support constraint machines (SCMs)", " support constraints"], "paper_abstract": "A learning paradigm is proposed and investigated, in which the classical framework of learning from examples is enhanced by the introduction of hard pointwise constraints, i.e., constraints imposed on a finite set of examples that cannot be violated. Such constraints arise, e.g., when requiring coherent decisions of classifiers acting on different views of the same pattern. The classical examples of supervised learning, which can be violated at the cost of some penalization (quantified by the choice of a suitable loss function) play the role of soft pointwise constraints. Constrained variational calculus is exploited to derive a representer theorem that provides a description of the functional structure of the optimal solution to the proposed learning paradigm. It is shown that such an optimal solution can be represented in terms of a set of support constraints, which generalize the concept of support vectors and open the doors to a novel learning paradigm, called support constraint machines. The general theory is applied to derive the representation of the optimal solution to the problem of learning from hard linear pointwise constraints combined with soft pointwise constraints induced by supervised examples. In some cases, closed-form optimal solutions are obtained.", "paper_title": "Learning With Mixed Hard/Soft Pointwise Constraints", "paper_id": "WOS:000360437300014"}