{"auto_keywords": [{"score": 0.040567250179834126, "phrase": "hdfs"}, {"score": 0.03171262586141619, "phrase": "storage_space"}, {"score": 0.015719654160018, "phrase": "dynamic_deduplication_decision"}, {"score": 0.009064448229373445, "phrase": "storage_utilization"}, {"score": 0.00890934878187007, "phrase": "data_center"}, {"score": 0.004732215871738028, "phrase": "hadoop_distributed_file_system"}, {"score": 0.004240116703465252, "phrase": "big_data"}, {"score": 0.004119313626110943, "phrase": "multiform_data"}, {"score": 0.004071957673937727, "phrase": "real_time"}, {"score": 0.004001938457469328, "phrase": "heavy_challenge"}, {"score": 0.003955955607095711, "phrase": "hadoop"}, {"score": 0.003910441483033426, "phrase": "file_system"}, {"score": 0.0036272258853286433, "phrase": "distributed_data_center"}, {"score": 0.00344321906220063, "phrase": "data_reliability"}, {"score": 0.0032496585171039797, "phrase": "extra_storage_space"}, {"score": 0.0031026497891826726, "phrase": "deduplication_technique"}, {"score": 0.002548331088496964, "phrase": "proper_deduplication_strategy"}, {"score": 0.002432971714608409, "phrase": "limited_storage_devices"}, {"score": 0.002377260665147444, "phrase": "useless_duplicates"}, {"score": 0.0022828113789751694, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "hdfs_system"}], "paper_keywords": [""], "paper_abstract": "Data are generated and updated tremendously fast by users through any devices in anytime and anywhere in big data. Coping with these multiform data in real time is a heavy challenge. Hadoop distributed file system (HDFS) is designed to deal with data for building a distributed data center. HDFS uses the data duplicates to increase data reliability. However, data duplicates need a lot of extra storage space and funding in infrastructure. Using the deduplication technique can improve utilization of the storage space effectively. In this paper, we propose a dynamic deduplication decision to improve the storage utilization of a data center which uses HDFS as its file system. Our proposed system can formulate a proper deduplication strategy to sufficiently utilize the storage space under the limited storage devices. Our deduplication strategy deletes useless duplicates to increase the storage space. The experimental results show that our method can efficiently improve the storage utilization of a data center using the HDFS system.", "paper_title": "Dynamic Deduplication Decision in a Hadoop Distributed File System", "paper_id": "WOS:000335680200001"}