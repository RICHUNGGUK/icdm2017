{"auto_keywords": [{"score": 0.031091440814077425, "phrase": "poland"}, {"score": 0.024281017503623673, "phrase": "bo"}, {"score": 0.00481495049065317, "phrase": "reduced_representations"}, {"score": 0.0047761092701293474, "phrase": "feature_selection_methods"}, {"score": 0.004642604455537883, "phrase": "small_set"}, {"score": 0.004605147081797856, "phrase": "informative_features"}, {"score": 0.004549524321051407, "phrase": "good_classification_results"}, {"score": 0.004094639048362357, "phrase": "backward_search"}, {"score": 0.004012500850311087, "phrase": "floating_search"}, {"score": 0.003916098498242888, "phrase": "forward_as_well_as_backward_selection"}, {"score": 0.0037301606780397456, "phrase": "final_classifier"}, {"score": 0.0036701570847323447, "phrase": "high-dimensional_spaces"}, {"score": 0.003640516896245813, "phrase": "small_sample_size_problems"}, {"score": 0.0032762198196613084, "phrase": "new_strategy"}, {"score": 0.003223494720348768, "phrase": "pairwise_evaluation"}, {"score": 0.003198931819683113, "phrase": "jonassen"}, {"score": 0.0029602552900111407, "phrase": "computer_recognition_systems"}, {"score": 0.002696479050724968, "phrase": "two-dimensional_spaces"}, {"score": 0.0026316112429083235, "phrase": "small_sample_size_problem"}, {"score": 0.0023298892270500983, "phrase": "traditional_procedures"}, {"score": 0.0022830706430588482, "phrase": "artificial_and_real-world_examples"}, {"score": 0.0021049977753042253, "phrase": "pairwise_selection"}], "paper_keywords": ["feature selection", " prototype selection", " pairwise feature evaluation", " pattern classification"], "paper_abstract": "Feature selection methods are often used to determine a small set of informative features that guarantee good classification results. Such procedures usually consist of two components: a separability criterion and a selection strategy. The most basic choices for the latter are individual ranking, forward search and backward search. Many intermediate methods such as floating search are also available. The forward as well as backward selection may cause lossy evaluation of the criterion and/or overtraining of the final classifier in case of high-dimensional spaces and small sample size problems. Backward selection may also become computationally prohibitive. Individual ranking, on the other hand, suffers as it neglects dependencies between features. A new strategy based on a pairwise evaluation has recently been proposed by Bo and Jonassen (Genome Biol 3, 2002) and Pekalska et al. (International Conference on Computer Recognition Systems, Poland, pp 271-278, 2005). Since it considers interactions between features, but always restricted to two-dimensional spaces, it may circumvent the small sample size problem. In this paper, we evaluate this idea in a more general framework for the selection of features as well as prototypes. Our finding is that such a pairwise selection may improve over traditional procedures and we present some artificial and real-world examples to support this claim. Additionally, we have also discovered that the set of problems for which the pairwise selection may be effective is small.", "paper_title": "Pairwise feature evaluation for constructing reduced representations", "paper_id": "WOS:000243908800005"}