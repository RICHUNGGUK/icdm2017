{"auto_keywords": [{"score": 0.035648822730098034, "phrase": "chunkfarm"}, {"score": 0.009612105607093973, "phrase": "de-duplication_backup"}, {"score": 0.00481495049065317, "phrase": "hash_join"}, {"score": 0.004706453355638515, "phrase": "high_space_efficiency"}, {"score": 0.004626680470601208, "phrase": "enterprise_de-duplication_backup"}, {"score": 0.0044967055264893184, "phrase": "high_scalability"}, {"score": 0.0043703658695579085, "phrase": "large-scale_distributed_environments"}, {"score": 0.0038332390893263844, "phrase": "duplicate_chunks"}, {"score": 0.003789763907758991, "phrase": "existing_inline_de-duplication_approaches"}, {"score": 0.0036207206487324506, "phrase": "disk_bottleneck"}, {"score": 0.0034789831290103967, "phrase": "poor_duplicate_locality_workload"}, {"score": 0.0026003455833291124, "phrase": "high_write_throughput"}, {"score": 0.0025416235470290286, "phrase": "workload_locality"}, {"score": 0.002442025065383455, "phrase": "fingerprint_lookup"}, {"score": 0.0021659920594094407, "phrase": "distributed_implementation"}, {"score": 0.0021049977753042253, "phrase": "large-scale_and_distributed_storage_systems"}], "paper_keywords": ["Backup system", " De-duplication", " Post-processing", " Fingerprint lookup", " Scalability"], "paper_abstract": "Apart from high space efficiency, other demanding requirements for enterprise de-duplication backup are high performance, high scalability, and availability for large-scale distributed environments. The main challenge is reducing the significant disk input/output (I/O) overhead as a result of constantly accessing the disk to identify duplicate chunks. Existing inline de-duplication approaches mainly rely on duplicate locality to avoid disk bottleneck, thus suffering from degradation under poor duplicate locality workload. This paper presents Chunkfarm, a post-processing de-duplication backup system designed to improve capacity, throughput, and scalability for de-duplication. Chunkfarm performs de-duplication backup using the hash join algorithm, which turns the notoriously random and small disk I/Os of fingerprint lookups and updates into large sequential disk I/Os, hence achieving high write throughput not influenced by workload locality. More importantly, by decentralizing fingerprint lookup and update, Chunkfarm supports a cluster of servers to perform de-duplication backup in parallel; it hence is conducive to distributed implementation and thus applicable to large-scale and distributed storage systems.", "paper_title": "Scalable high performance de-duplication backup via hash join", "paper_id": "WOS:000277763000001"}