{"auto_keywords": [{"score": 0.049480569959645185, "phrase": "hadoop_distributed_file_system"}, {"score": 0.03262661215568323, "phrase": "core_affinity"}, {"score": 0.031194902663221332, "phrase": "dynamic_core_affinity"}, {"score": 0.00481495049065317, "phrase": "high-performance_file"}, {"score": 0.004695303774351139, "phrase": "mapreduce_programming_model"}, {"score": 0.004597861947580946, "phrase": "data_nodes"}, {"score": 0.004390517953424082, "phrase": "big-data_processing"}, {"score": 0.00422781730455246, "phrase": "different_resource_requirements"}, {"score": 0.004140036888016244, "phrase": "computation_tasks"}, {"score": 0.00403709306806495, "phrase": "multi-core_processors"}, {"score": 0.003903789851015775, "phrase": "high-performance_data"}, {"score": 0.003759057920801288, "phrase": "continuously_increasing_volume"}, {"score": 0.0036809741830924796, "phrase": "distributed_file_systems"}, {"score": 0.003650195070580192, "phrase": "database_servers"}, {"score": 0.0035296217979800463, "phrase": "performance_characteristics"}, {"score": 0.0034418037573506837, "phrase": "upstream_data"}, {"score": 0.002971249577042111, "phrase": "novel_approach"}, {"score": 0.002921732652917206, "phrase": "high-throughput_file_upload"}, {"score": 0.0028609920171902186, "phrase": "dynamic_changes"}, {"score": 0.0028251537271620996, "phrase": "processor_load"}, {"score": 0.002630364569659529, "phrase": "service_threads"}, {"score": 0.0025221012517852885, "phrase": "data_locality"}, {"score": 0.002358050109031317, "phrase": "measurement_results"}, {"score": 0.002289667393538803, "phrase": "file_upload_throughput"}, {"score": 0.002270495425498003, "phrase": "end_applications"}, {"score": 0.002195407751741358, "phrase": "hdfs"}, {"score": 0.002158780936704905, "phrase": "better_scalability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Affinity", " Big-data", " Multi-core", " Hadoop Distributed File System", " Process scheduling"], "paper_abstract": "The MapReduce programming model, in which the data nodes perform both the data storing and the computation, was introduced for big-data processing. Thus, we need to understand the different resource requirements of data storing and computation tasks and schedule these efficiently over multi-core processors. In particular, the provision of high-performance data storing has become more critical because of the continuously increasing volume of data uploaded to distributed file systems and database servers. However, the analysis of the performance characteristics of the processes that store upstream data is very intricate, because both network and disk inputs/outputs (I/O) are heavily involved in their operations. In this paper, we analyze the impact of core affinity on both network and disk I/O performance and propose a novel approach for dynamic core affinity for high-throughput file upload. We consider the dynamic changes in the processor load and the intensiveness of the file upload at run-time, and accordingly decide the core affinity for service threads, with the objective of maximizing the parallelism, data locality, and resource efficiency. We apply the dynamic core affinity to Hadoop Distributed File System (HDFS). Measurement results show that our implementation can improve the file upload throughput of end applications by more than 30% as compared with the default HDFS, and provide better scalability. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Dynamic core affinity for high-performance file upload on Hadoop Distributed File System", "paper_id": "WOS:000347018800012"}