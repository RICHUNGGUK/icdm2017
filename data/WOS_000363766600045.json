{"auto_keywords": [{"score": 0.03003425983162416, "phrase": "arm"}, {"score": 0.01565297017620053, "phrase": "scientific_workloads"}, {"score": 0.014885023507811327, "phrase": "exascale_computation"}, {"score": 0.013744356114081322, "phrase": "arm_socs"}, {"score": 0.012689974201410246, "phrase": "performance_levels"}, {"score": 0.00481495049065317, "phrase": "arm_hpc_clusters"}, {"score": 0.004764220976404241, "phrase": "power_consumption"}, {"score": 0.0047440784262330945, "phrase": "modern_high-performance_computing"}, {"score": 0.004634799977448855, "phrase": "commodity_servers"}, {"score": 0.004585959752491973, "phrase": "major_hurdles"}, {"score": 0.004480307124735185, "phrase": "hpc_community"}, {"score": 0.004294381488413906, "phrase": "large-scale_hpc_systems"}, {"score": 0.004186534737631288, "phrase": "hpc_systems"}, {"score": 0.004046923261950549, "phrase": "hpc_platforms"}, {"score": 0.003953639783762917, "phrase": "major_shortcomings"}, {"score": 0.003936910716320182, "phrase": "current_arm-hpc_evaluations"}, {"score": 0.0038953958723049287, "phrase": "detailed_insights"}, {"score": 0.0038624981944755813, "phrase": "distributed_multicore_systems"}, {"score": 0.003813669883040509, "phrase": "large-scale_applications"}, {"score": 0.003789487937906101, "phrase": "hpc."}, {"score": 0.0037257428913496596, "phrase": "comprehensive_evaluation"}, {"score": 0.0036864465771943933, "phrase": "major_aspects"}, {"score": 0.0036553326013023684, "phrase": "hpc"}, {"score": 0.003632124527478033, "phrase": "arm-based_socs"}, {"score": 0.0035634522350563107, "phrase": "unconventional_cluster"}, {"score": 0.003503497128638099, "phrase": "weiser"}, {"score": 0.0034812740368914455, "phrase": "single-node_benchmarks"}, {"score": 0.0034518616109647307, "phrase": "sysbench"}, {"score": 0.0034300396450687672, "phrase": "parsec"}, {"score": 0.0034082066583526463, "phrase": "multi-node_scientific_benchmarks"}, {"score": 0.003372248390322932, "phrase": "linpack"}, {"score": 0.003336729762904818, "phrase": "nasa"}, {"score": 0.003204841443605308, "phrase": "performance_limitations"}, {"score": 0.003150900886669123, "phrase": "experimental_results"}, {"score": 0.0030651710672229926, "phrase": "memory_bandwidth"}, {"score": 0.0029754456141964365, "phrase": "compiler_optimizations"}, {"score": 0.0029565620771207003, "phrase": "server-based_benchmarking"}, {"score": 0.002912963877144772, "phrase": "memory_intensive_benchmarks"}, {"score": 0.002900625406565972, "phrase": "database_transactions"}, {"score": 0.002857849698346116, "phrase": "multithreaded_query_processing"}, {"score": 0.002785978280998868, "phrase": "power_ratios"}, {"score": 0.002768293851511292, "phrase": "single_core"}, {"score": 0.0026986685585382347, "phrase": "double_precision_floating_point"}, {"score": 0.0026873095145644554, "phrase": "java"}, {"score": 0.0026196433868693925, "phrase": "cpu-bound_benchmarks"}, {"score": 0.002570064617803693, "phrase": "computation-oriented_applications"}, {"score": 0.002548331088496964, "phrase": "better_scalability"}, {"score": 0.002526780881314497, "phrase": "shared_memory_benchmarks"}, {"score": 0.0024736973328071026, "phrase": "mpj-express_runtime"}, {"score": 0.0024579902691324795, "phrase": "comparative_analysis"}, {"score": 0.00241658945942583, "phrase": "similar_results"}, {"score": 0.0024063484128685367, "phrase": "network_bandwidth"}, {"score": 0.002205692205167157, "phrase": "energy_efficiency"}, {"score": 0.002196342933346204, "phrase": "arm-based_clusters"}, {"score": 0.002136529094879374, "phrase": "energy-efficient_hpc_clusters"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["energy-efficiency", " arm evaluation", " multicore cluster", " large-scale scientific application"], "paper_abstract": "The power consumption of modern high-performance computing (HPC) systems that are built using power hungry commodity servers is one of the major hurdles for achieving Exascale computation. Several efforts have been made by the HPC community to encourage the use of low-powered system-on-chip (SoC) embedded processors in large-scale HPC systems. These initiatives have successfully demonstrated the use of ARM SoCs in HPC systems, but there is still a need to analyze the viability of these systems for HPC platforms before a case can be made for Exascale computation. The major shortcomings of current ARM-HPC evaluations include a lack of detailed insights about performance levels on distributed multicore systems and performance levels for benchmarking in large-scale applications running on HPC. In this paper, we present a comprehensive evaluation of results that covers major aspects of server and HPC benchmarking for ARM-based SoCs. For the experiments, we built an unconventional cluster of ARM Cortex-A9s that is referred to as Weiser and ran single-node benchmarks (STREAM, Sysbench, and PARSEC) and multi-node scientific benchmarks (High-performance Linpack (HPL), NASA Advanced Supercomputing (NAS) Parallel Benchmark, and Gadget-2) in order to provide a baseline for performance limitations of the system. Based on the experimental results, we claim that the performance of ARM SoCs depends heavily on the memory bandwidth, network latency, application class, workload type, and support for compiler optimizations. During server-based benchmarking, we observed that when performing memory intensive benchmarks for database transactions, x86 performed 12% better for multithreaded query processing. However, ARM performed four times better for performance to power ratios for a single core and 2.6 times better on four cores. We noticed that emulated double precision floating point in Java resulted in three to four times slower performance as compared with the performance in C for CPU-bound benchmarks. Even though Intel x86 performed slightly better in computation-oriented applications, ARM showed better scalability in I/O bound applications for shared memory benchmarks. We incorporated the support for ARM in the MPJ-Express runtime and performed comparative analysis of two widely used message passing libraries. We obtained similar results for network bandwidth, large-scale application scaling, floating-point performance, and energy-efficiency for clusters in message passing evaluations (NBP and Gadget 2 with MPJ-Express and MPICH). Our findings can be used to evaluate the energy efficiency of ARM-based clusters for server workloads and scientific workloads and to provide a guideline for building energy-efficient HPC clusters. Copyright (c) 2015 John Wiley & Sons, Ltd.", "paper_title": "Evaluating ARM HPC clusters for scientific workloads", "paper_id": "WOS:000363766600045"}