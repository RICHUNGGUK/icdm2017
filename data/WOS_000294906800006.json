{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "neural_network"}, {"score": 0.04102458095058994, "phrase": "disparity_energy_neurons"}, {"score": 0.00471071771178029, "phrase": "internally_defined_reward"}, {"score": 0.004583579470330216, "phrase": "autonomous_development"}, {"score": 0.004533685795068016, "phrase": "binocular_vergence_control"}, {"score": 0.004459857205489101, "phrase": "active_robotic_vision_system"}, {"score": 0.00441130431691048, "phrase": "attention-gated_reinforcement"}, {"score": 0.004245476558619167, "phrase": "control_policy"}, {"score": 0.00380511089035825, "phrase": "vergence_commands"}, {"score": 0.0036419834462626125, "phrase": "reward_signal"}, {"score": 0.003524227590204673, "phrase": "internal_representation"}, {"score": 0.003466780226380516, "phrase": "visual_input"}, {"score": 0.0034102660898135155, "phrase": "total_activation"}, {"score": 0.003228434077467506, "phrase": "previous_work"}, {"score": 0.00319324425647735, "phrase": "q_learning"}, {"score": 0.0029573932605287947, "phrase": "input_state_space"}, {"score": 0.0027539714188091866, "phrase": "larger_diversity"}, {"score": 0.002592784882557155, "phrase": "possible_actions"}, {"score": 0.002522662955545598, "phrase": "network_learning"}, {"score": 0.0024679299547216956, "phrase": "natural_images"}, {"score": 0.0024276590297034064, "phrase": "real_objects"}, {"score": 0.002388043660169452, "phrase": "cluttered_environment"}, {"score": 0.002163535133532765, "phrase": "squared_errors"}, {"score": 0.0021049977753042253, "phrase": "closed_loop_frequency_response"}], "paper_keywords": ["Attention-gated reinforcement learning", " autonomous vergence control", " disparity energy neuron"], "paper_abstract": "We describe the autonomous development of binocular vergence control in an active robotic vision system through attention-gated reinforcement learning (AGREL). The control policy is implemented by a neural network, which maps the outputs from a population of disparity energy neurons to a set of vergence commands. The network learns to maximize a reward signal that is based on an internal representation of the visual input: the total activation in the population of disparity energy neurons. This system extends previous work using Q learning by increasing the complexity of the policy in two ways. First, the input state space is continuous, rather than discrete, and is based upon a larger diversity of neurons. Second, we increase the number of possible actions. We evaluate the network learning and performance on natural images and with real objects in a cluttered environment. The policies learned by the network outperform policies by Q learning in two ways: the mean squared errors are smaller and the closed loop frequency response has larger bandwidth.", "paper_title": "Improved Binocular Vergence Control via a Neural Network That Maximizes an Internally Defined Reward", "paper_id": "WOS:000294906800006"}