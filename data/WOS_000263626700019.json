{"auto_keywords": [{"score": 0.029717155326300093, "phrase": "cap"}, {"score": 0.00481495049065317, "phrase": "ensemble_methods"}, {"score": 0.00456159659731398, "phrase": "better_classification"}, {"score": 0.004288261454002681, "phrase": "minimal_number"}, {"score": 0.004094018760244776, "phrase": "best_performance"}, {"score": 0.0038189638340666936, "phrase": "large_storage_requirements"}, {"score": 0.003674192201248236, "phrase": "classification_performance"}, {"score": 0.003534889186471058, "phrase": "ensemble_pruning"}, {"score": 0.0033486631403337555, "phrase": "ensemble_members"}, {"score": 0.003123527559542239, "phrase": "original_ensemble"}, {"score": 0.0026551731045904854, "phrase": "individual_members"}, {"score": 0.0024957862318504753, "phrase": "individual_predictive_ability"}, {"score": 0.0021049977753042253, "phrase": "low_inter-agreement"}], "paper_keywords": [""], "paper_abstract": "Ensemble methods combine several individual pattern classifiers in order to achieve better classification. The challenge is to choose the minimal number of classifiers that achieve the best performance. An ensemble that contains too many members might incur large storage requirements and even reduce the classification performance. The goal of ensemble pruning is to identify a subset of ensemble members that performs at least as good as the original ensemble and discard any other members. In this paper, we introduce the Collective-Agreement-based Pruning (CAP) method. Rather than ranking individual members, CAP ranks subsets by considering the individual predictive ability of each member along with the degree of redundancy among them. Subsets whose members highly agree with the class while having low inter-agreement are preferred. (C) 2008 Elsevier B.V. All rights reserved,", "paper_title": "Collective-agreement-based pruning of ensembles", "paper_id": "WOS:000263626700019"}