{"auto_keywords": [{"score": 0.013938375469171102, "phrase": "objective_functions"}, {"score": 0.012536419197840785, "phrase": "additive_input_noise"}, {"score": 0.011058204463700363, "phrase": "multinode_fault"}, {"score": 0.00889612706943984, "phrase": "objective_function"}, {"score": 0.004454691230204595, "phrase": "tolerant_neural_network"}, {"score": 0.0036493838225192883, "phrase": "multiplicative_node_noise"}, {"score": 0.0035616743422949766, "phrase": "multiweight_fault"}, {"score": 0.0031231012822640672, "phrase": "gladyshev_theorem"}, {"score": 0.002989220655735981, "phrase": "six_online_algorithms"}, {"score": 0.0026209522797963447, "phrase": "tikhonov_regularizer_approach"}, {"score": 0.00247215300812495, "phrase": "simple_mean_square_training_error"}, {"score": 0.002331781764406665, "phrase": "fault_tolerance"}, {"score": 0.002297949552105756, "phrase": "rbf_network"}, {"score": 0.0022536005664642294, "phrase": "injective_additive_input_noise"}, {"score": 0.0021049977753042253, "phrase": "specialized_regularization_term"}], "paper_keywords": ["Convergence", " gladyshev theorem", " fault tolerance", " objective functions", " RBF Networks"], "paper_abstract": "In the last two decades, many online fault/noise injection algorithms have been developed to attain a fault tolerant neural network. However, not much theoretical works related to their convergence and objective functions have been reported. This paper studies six common fault/noise-injection-based online learning algorithms for radial basis function (RBF) networks, namely 1) injecting additive input noise, 2) injecting additive/multiplicative weight noise, 3) injecting multiplicative node noise, 4) injecting multiweight fault (random disconnection of weights), 5) injecting multinode fault during training, and 6) weight decay with injecting multinode fault. Based on the Gladyshev theorem, we show that the convergence of these six online algorithms is almost sure. Moreover, their true objective functions being minimized are derived. For injecting additive input noise during training, the objective function is identical to that of the Tikhonov regularizer approach. For injecting additive/multiplicative weight noise during training, the objective function is the simple mean square training error. Thus, injecting additive/multiplicative weight noise during training cannot improve the fault tolerance of an RBF network. Similar to injective additive input noise, the objective functions of other fault/noise-injection-based online algorithms contain a mean square error term and a specialized regularization term.", "paper_title": "Convergence and Objective Functions of Some Fault/Noise-Injection-Based Online Learning Algorithms for RBF Networks", "paper_id": "WOS:000278537600007"}