{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "dependence_maximization"}, {"score": 0.04586846484671967, "phrase": "feature_selection"}, {"score": 0.00406001734835979, "phrase": "selected_features"}, {"score": 0.003752550907120244, "phrase": "estimation_problem"}, {"score": 0.0035605741849764187, "phrase": "hilbert-schmidt_independence_criterion"}, {"score": 0.003247868216333148, "phrase": "good_features"}, {"score": 0.002737960968372391, "phrase": "greedy_procedure"}, {"score": 0.0024007585788965655, "phrase": "existing_feature_selectors"}, {"score": 0.0023384629381463054, "phrase": "special_cases"}, {"score": 0.0021049977753042253, "phrase": "artificial_and_real-world_data"}], "paper_keywords": ["kernel methods", " feature selection", " independence measure", " Hilbert-Schmidt independence criterion", " Hilbert space embedding of distribution"], "paper_abstract": "We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice.", "paper_title": "Feature Selection via Dependence Maximization", "paper_id": "WOS:000305456600005"}