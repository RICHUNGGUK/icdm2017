{"auto_keywords": [{"score": 0.04564158107788786, "phrase": "best_approximation"}, {"score": 0.00481495049065317, "phrase": "radial_basis_function_networks"}, {"score": 0.00475924837336259, "phrase": "radial_basis_function"}, {"score": 0.004722527203446182, "phrase": "rbf"}, {"score": 0.004490212943845175, "phrase": "regression_problem"}, {"score": 0.004421063493736839, "phrase": "learning_paradigm"}, {"score": 0.004219912413995784, "phrase": "observed_data"}, {"score": 0.0035989687070420977, "phrase": "reduced_component"}, {"score": 0.00354349559754076, "phrase": "learning_process"}, {"score": 0.0033821367649595254, "phrase": "strange_behaviors"}, {"score": 0.002963640433319524, "phrase": "singular_regions"}, {"score": 0.002884111441153185, "phrase": "detailed_analysis"}, {"score": 0.002850682396853918, "phrase": "dynamical_behaviors"}, {"score": 0.002742017872865778, "phrase": "rbf_networks"}, {"score": 0.0026892447164623247, "phrase": "averaged_learning_equation"}, {"score": 0.0025867180355950816, "phrase": "batch_mode"}, {"score": 0.0024977819584875573, "phrase": "overlap_singularity"}, {"score": 0.0024308684879598436, "phrase": "hessian"}, {"score": 0.0023746766301040974, "phrase": "stability_analysis"}, {"score": 0.0023289568500915207, "phrase": "analytical_dynamic_vector_fields"}, {"score": 0.002231441081806766, "phrase": "real_trajectories"}, {"score": 0.0021970000365512227, "phrase": "numeric_method"}], "paper_keywords": ["Neural networks", " RBF networks", " Singularity", " Dynamics of learning", " Plateau phenomenon", " Gradient descent learning", " Stability analysis"], "paper_abstract": "The radial basis function (RBF) networks are one of the most widely used models for function approximation in the regression problem. In the learning paradigm, the best approximation is recursively or iteratively searched for based on observed data (teacher signals). One encounters difficulties in such a process when two component basis functions become identical, or when the magnitude of one component becomes null. In this case, the number of the components reduces by one, and then the reduced component recovers as the learning process proceeds further, provided such a component is necessary for the best approximation. Strange behaviors, especially the plateau phenomena, have been observed in dynamics of learning when such reduction occurs. There exist singularities in the space of parameters, and the above reduction takes place at the singular regions. This paper focuses on a detailed analysis of the dynamical behaviors of learning near the overlap and elimination singularities in RBF networks, based on the averaged learning equation that is applicable to both on-line and batch mode learning. We analyze the stability on the overlap singularity by solving the eigenvalues of the Hessian explicitly. Based on the stability analysis, we plot the analytical dynamic vector fields near the singularity, which are then compared to those real trajectories obtained by a numeric method. We also confirm the existence of the plateaus in both batch and on-line learning by simulation. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Dynamics of learning near singularities in radial basis function networks", "paper_id": "WOS:000260550300012"}