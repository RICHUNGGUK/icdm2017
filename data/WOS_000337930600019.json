{"auto_keywords": [{"score": 0.04978789345170093, "phrase": "continuous_pose_estimation"}, {"score": 0.00481495049065317, "phrase": "object_detection"}, {"score": 0.004698938469311569, "phrase": "multiview_model"}, {"score": 0.004670372812069178, "phrase": "object_categories"}, {"score": 0.004599712603988837, "phrase": "virtually_any_type"}, {"score": 0.0045717473871437425, "phrase": "image_features"}, {"score": 0.004434439786037454, "phrase": "unified_manner"}, {"score": 0.004327555970055785, "phrase": "novel_scenes"}, {"score": 0.004096353910862841, "phrase": "discrete_viewpoints"}, {"score": 0.0038187460155038383, "phrase": "optical_flow"}, {"score": 0.0037955107949094124, "phrase": "training_examples"}, {"score": 0.0036814304184738774, "phrase": "arbitrary_test_image"}, {"score": 0.0036367606686493227, "phrase": "chosen_viewpoints"}, {"score": 0.003527434895463077, "phrase": "common_method"}, {"score": 0.0034528614466646625, "phrase": "low-level_image_features"}, {"score": 0.0033490453422160677, "phrase": "coarse-scale_gradients"}, {"score": 0.0032582736131450316, "phrase": "probabilistic_templates"}, {"score": 0.0030934538358409216, "phrase": "probabilistic_techniques"}, {"score": 0.003065243071851165, "phrase": "kernel_density_estimation"}, {"score": 0.003046578530654012, "phrase": "monte_carlo_integration"}, {"score": 0.0030280272943359813, "phrase": "importance_sampling"}, {"score": 0.002982140436039972, "phrase": "extensive_evaluation"}, {"score": 0.0029549417860846284, "phrase": "wide_variety"}, {"score": 0.0029369469007332963, "phrase": "benchmark_datasets"}, {"score": 0.0028748209350107743, "phrase": "\"ethz_shape\"_dataset"}, {"score": 0.002704445718144686, "phrase": "remarkable_performance"}, {"score": 0.0025912115192027485, "phrase": "savarese"}, {"score": 0.0025519272424492164, "phrase": "detection_rates"}, {"score": 0.0024979252889176756, "phrase": "pose_estimation"}, {"score": 0.002371476222721094, "phrase": "\"rotating_cars\"_dataset"}, {"score": 0.0023071389598419076, "phrase": "particular_capabilities"}, {"score": 0.0022860825534337255, "phrase": "novel_dataset"}, {"score": 0.0022721515777319044, "phrase": "non-textured_objects"}, {"score": 0.00214396012113301, "phrase": "coarse_scale_intensity_gradients"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Appearance-based object recognition", " Object detection", " Pose estimation", " Hough voting", " Edges and shape models", " Viewpoint synthesis"], "paper_abstract": "This paper presents a multiview model of object categories, generally applicable to virtually any type of image features, and methods to efficiently perform, in a unified manner, detection, localization and continuous pose estimation in novel scenes. We represent appearance as distributions of low-level, fine-grained image features. Multiview models encode the appearance of objects at discrete viewpoints, and, in addition, how these viewpoints deform into one another as the viewpoint continuously varies (as detected from optical flow between training examples). Using a measure of similarity between an arbitrary test image and such a model at chosen viewpoints, we perform all tasks mentioned above with a common method. We leverage the simplicity of low-level image features, such as points extracted along edges, or coarse-scale gradients extracted densely over the images, by building probabilistic templates, i.e. distributions of features, learned from one or several training examples. We efficiently handle these distributions with probabilistic techniques such as kernel density estimation, Monte Carlo integration and importance sampling. We provide an extensive evaluation on a wide variety of benchmark datasets. We demonstrate performance on the \"ETHZ Shape\" dataset, with single (hand-drawn) and multiple training examples, well above baseline methods, on par with a number of more task-specific methods. We obtain remarkable performance on the recognition of more complex objects, notably the cars of the \"3D Object\" dataset of Savarese et al. with detection rates of 92.5% and an accuracy in pose estimation of 91%. We perform better than the state-of-the-art on continuous pose estimation with the \"rotating cars\" dataset of Ozuysal et al. We also demonstrate particular capabilities with a novel dataset featuring non-textured objects of undistinctive shapes, the pose of which can only be determined from shading, captured here by coarse scale intensity gradients. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Multiview feature distributions for object detection and continuous pose estimation", "paper_id": "WOS:000337930600019"}