{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "hidden_markov_models"}, {"score": 0.004518442937844318, "phrase": "wide_range"}, {"score": 0.004470823343432748, "phrase": "sequence_modeling_problems"}, {"score": 0.004377077848734575, "phrase": "classification_context"}, {"score": 0.004262643556463569, "phrase": "simplest_approaches"}, {"score": 0.004151188559520802, "phrase": "single_hmm"}, {"score": 0.004042635935882651, "phrase": "test_sequence"}, {"score": 0.003874918042970017, "phrase": "hmm"}, {"score": 0.0033937776228549557, "phrase": "improper_models"}, {"score": 0.0032355864829403413, "phrase": "prior_knowledge"}, {"score": 0.0032014433546265694, "phrase": "poor_estimates"}, {"score": 0.003150900886669123, "phrase": "insufficient_training_data"}, {"score": 0.0029253546782349875, "phrase": "descriptive_strengths"}, {"score": 0.002863921785266393, "phrase": "discriminative_classifiers"}, {"score": 0.0027594936602962075, "phrase": "feature-based_classifiers"}, {"score": 0.0027159094186967247, "phrase": "hmm-induced_vector_space"}, {"score": 0.0026307897899760383, "phrase": "individual_hidden_markov_models"}, {"score": 0.002255431275670053, "phrase": "dynamic_kernels"}, {"score": 0.0021731423856364003, "phrase": "fisher_kernel_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Hidden Markov models", " Discriminative classification", " Dimensionality reduction", " Hybrid models", " Generative embeddings"], "paper_abstract": "Hidden Markov models (HMMs) have been successfully applied to a wide range of sequence modeling problems. In the classification context, one of the simplest approaches is to train a single HMM per class. A test sequence is then assigned to the class whose HMM yields the maximum a posterior (MAP) probability. This generative scenario works well when the models are correctly estimated. However, the results can become poor when improper models are employed, due to the lack of prior knowledge, poor estimates, violated assumptions or insufficient training data. To improve the results in these cases we propose to combine the descriptive strengths of HMMs with discriminative classifiers. This is achieved by training feature-based classifiers in an HMM-induced vector space defined by specific components of individual hidden Markov models. We introduce four major ways of building Such vector spaces and study which trained combiners are useful in which context. Moreover, we motivate and discuss the merit of our method in comparison to dynamic kernels, in particular, to the Fisher Kernel approach. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Component-based discriminative classification for hidden Markov models", "paper_id": "WOS:000268453800027"}