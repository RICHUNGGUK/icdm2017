{"auto_keywords": [{"score": 0.046166196857162826, "phrase": "depth_information"}, {"score": 0.012944489662743775, "phrase": "active_contour"}, {"score": 0.008084094196027515, "phrase": "color_segmentation"}, {"score": 0.00481495049065317, "phrase": "stereoscopic_video_objects"}, {"score": 0.00436890075128947, "phrase": "stereoscopic_pairs"}, {"score": 0.003939980614608309, "phrase": "erroneous_disparity_estimation_and_occlusion_issues"}, {"score": 0.0034570879043633114, "phrase": "object_contours"}, {"score": 0.003373984423727724, "phrase": "compensated_disparity_field"}, {"score": 0.003333182329983738, "phrase": "depth_map"}, {"score": 0.003194209418929854, "phrase": "modified_version"}, {"score": 0.00316518932828897, "phrase": "segmentation_algorithm"}, {"score": 0.003070340620983981, "phrase": "first_\"constrained_fusion_of"}, {"score": 0.0030239846974875998, "phrase": "cfcs"}, {"score": 0.0029783256772952073, "phrase": "color_segments_map"}, {"score": 0.0028715299874549245, "phrase": "stereoscopic_channels"}, {"score": 0.002828166765606611, "phrase": "video_objects"}, {"score": 0.0027854565462509095, "phrase": "color_segments"}, {"score": 0.0027601397818618617, "phrase": "depth_similarity_criteria"}, {"score": 0.002735052488172277, "phrase": "second_method"}, {"score": 0.0027019561451518746, "phrase": "depth_segments_map"}, {"score": 0.0025892330566358503, "phrase": "depth_segment"}, {"score": 0.002526938673434697, "phrase": "video_object's_boundary"}, {"score": 0.0024736587129641652, "phrase": "fitness_function"}, {"score": 0.0024511688853030168, "phrase": "different_color_areas"}, {"score": 0.002406799309495983, "phrase": "depth_segments'_boundaries"}, {"score": 0.0023849158959156026, "phrase": "acceleration_purposes"}, {"score": 0.0023133947578541977, "phrase": "\"attractive_edge\"_point"}, {"score": 0.0021966859554037174, "phrase": "real_life"}, {"score": 0.0021900066569764862, "phrase": "stereoscopic_sequences"}, {"score": 0.002163491500079535, "phrase": "extensive_comparisons"}, {"score": 0.0021049977753042253, "phrase": "promising_performance"}], "paper_keywords": ["Unsupervised video object segmentation", " Disparity field", " Depthmap", " M-RSST", " Active contour", " Attractive edge point", " Greedy approach", " Performance evaluation"], "paper_abstract": "In this paper two efficient unsupervised video object segmentation approaches are proposed and thoroughly compared. Both methods are based on the exploitation of depth information, estimated from stereoscopic pairs. Depth is a more efficient semantic descriptor of visual content, since usually an object is located on one depth plane. However, depth information fails to accurately represent the contours of an object mainly due to erroneous disparity estimation and occlusion issues. For this reason, the first approach projects color segments onto depth information in order to address the limitations of both depth and color segmentation; color segmentation usually over-partitions an object into several regions, while depth fails to precisely represent object contours. Depth information is produced through an occlusion compensated disparity field and then a depth map is generated. On the contrary, color segmentation is accomplished by incorporating a modified version of the Multiresolution Recursive Shortest Spanning Tree segmentation algorithm (M-RSST). Next considering the first \"Constrained Fusion of Color Segments\" (CFCS) approach, a color segments map is created, by applying the M-RSST to one of the stereoscopic channels. In this case video objects are extracted by fusing color segments according to depth similarity criteria. The second method also utilizes the depth segments map. In particular an active contour is automatically initialized onto the boundary of each depth segment, which is usually different from a video object's boundary. Initialization is accomplished by a fitness function that considers different color areas and preserves the shapes of depth segments' boundaries. For acceleration purposes each point of the active contour is associated to an \"attractive edge\" point and a greedy approach is incorporated so that the active contour converges to its final position. Several experiments on real life stereoscopic sequences are performed and extensive comparisons in terms of speed and accuracy indicate the promising performance of both methods.", "paper_title": "Unsupervised Segmentation of Stereoscopic Video Objects: Constrained Segmentation Fusion Versus Greedy Active Contours", "paper_id": "WOS:000358336200002"}