{"auto_keywords": [{"score": 0.04541485776191829, "phrase": "projective_depths"}, {"score": 0.010612387000973441, "phrase": "projective_depth"}, {"score": 0.009801213569375388, "phrase": "view-invariant_action_recognition"}, {"score": 0.004270665568018583, "phrase": "camera_internal_parameters"}, {"score": 0.004066898031169739, "phrase": "similar_motion"}, {"score": 0.003995226376780718, "phrase": "varying_viewpoints"}, {"score": 0.003907403442285109, "phrase": "human_body"}, {"score": 0.003737485125097945, "phrase": "body_posture"}, {"score": 0.003329368827624161, "phrase": "different_ways"}, {"score": 0.0031006232333900055, "phrase": "action_recognition"}, {"score": 0.003005539334280134, "phrase": "body-point_triplets"}, {"score": 0.0027989800906333784, "phrase": "mirror_symmetry"}, {"score": 0.0027373807320805084, "phrase": "different_techniques"}, {"score": 0.0025266078302900036, "phrase": "kinect_dataset"}, {"score": 0.0024930878032164757, "phrase": "ixmas"}, {"score": 0.002427372728532664, "phrase": "semi-synthetic_video_data"}, {"score": 0.0022705467477443417, "phrase": "dynamic_timeline_maps"}, {"score": 0.00221068461235127, "phrase": "camera_parameters"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["View invariance", " Action recognition", " Projective depth"], "paper_abstract": "In this paper, we investigate the concept of projective depth, demonstrate its application and significance in view-invariant action recognition. We show that projective depths are invariant to camera internal parameters and orientation, and hence can be used to identify similar motion of body-points from varying viewpoints. By representing the human body as a set of points, we decompose a body posture into a set of projective depths. The similarity between two actions is, therefore, measured by the motion of projective depths. We exhaustively investigate the different ways of extracting planes, which can be used to estimate the projective depths for use in action recognition including (i) ground plane, (ii) body-point triplets, (iii) planes in time, and (iv) planes extracted from mirror symmetry. We analyze these different techniques and analyze their efficacy in view-invariant action recognition. Experiments are performed on three categories of data including the CMU MoCap dataset, Kinect dataset, and IXMAS dataset. Results evaluated over semi-synthetic video data and real data confirm that our method can recognize actions, even when they have dynamic timeline maps, and the viewpoints and camera parameters are unknown and totally different. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "View invariant action recognition using projective depth", "paper_id": "WOS:000335488600004"}