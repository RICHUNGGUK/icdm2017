{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "centralized"}, {"score": 0.004755353864362941, "phrase": "cooperative_receding_horizon_control"}, {"score": 0.004716031208088975, "phrase": "autonomous_vehicle_missions"}, {"score": 0.004580933283492972, "phrase": "multiple_vehicles"}, {"score": 0.004449688143230257, "phrase": "multiple_target_points"}, {"score": 0.0042864307215634756, "phrase": "team_objective"}, {"score": 0.004198323646273642, "phrase": "total_reward"}, {"score": 0.0040949726088047225, "phrase": "complicating_factors"}, {"score": 0.003977594755967813, "phrase": "target_points"}, {"score": 0.0038157022294338454, "phrase": "vehicle_capabilities"}, {"score": 0.003424850089214916, "phrase": "optimization_problems"}, {"score": 0.0033824008297657494, "phrase": "planning_horizon"}, {"score": 0.0032990689963715157, "phrase": "shorter_action_horizon"}, {"score": 0.0032581736321024373, "phrase": "key_property"}, {"score": 0.002816814445002924, "phrase": "discrete_point_assignments"}, {"score": 0.0027818804382539444, "phrase": "proposed_scheme"}, {"score": 0.0027020387992490367, "phrase": "cooperative_behavior"}, {"score": 0.0026354246446771324, "phrase": "distributed_cooperative_controller"}, {"score": 0.0025385621498128243, "phrase": "perfect_information"}, {"score": 0.0025070703395635133, "phrase": "entire_team"}, {"score": 0.002404885278896803, "phrase": "centralized_case"}, {"score": 0.0023261372216194383, "phrase": "real-time_constraints"}, {"score": 0.0022877332844249065, "phrase": "simulation-based_comparisons"}, {"score": 0.002259346051668556, "phrase": "centralized_algorithm"}, {"score": 0.002231310272815607, "phrase": "distributed_version"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["cooperative control", " receding horizon", " potential field", " optimization", " distributed computation"], "paper_abstract": "We consider a setting where multiple vehicles form a team cooperating to visit multiple target points and collect rewards associated with them. The team objective is to maximize the total reward accumulated over a given time interval. Complicating factors include uncertainties regarding the locations of target points and the effectiveness of collecting rewards, differences among vehicle capabilities, and the fact that rewards are time-varying. We present a Receding Horizon (RH) control scheme which dynamically determines vehicle trajectories by solving a sequence of optimization problems over a planning horizon and executing them over a shorter action horizon. A key property of this scheme is that the trajectories it generates are stationary, in the sense that they ultimately guide vehicles to target points, even though the controller is not designed to perform any discrete point assignments. The proposed scheme is centralized and it induces a cooperative behavior. We subsequently develop a distributed cooperative controller which does not require a vehicle to maintain perfect information on the entire team and whose computational cost is scalable and significantly lower than the centralized case, making it attractive for applications with real-time constraints. We include simulation-based comparisons between the centralized algorithm and the distributed version, which illustrate the effectiveness of the latter. (c) 2005 Elsevier Ltd. All rights reserved.", "paper_title": "Centralized and distributed cooperative Receding Horizon control of autonomous vehicle missions", "paper_id": "WOS:000237816000015"}