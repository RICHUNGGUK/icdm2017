{"auto_keywords": [{"score": 0.036870309326606863, "phrase": "clsa"}, {"score": 0.00481495049065317, "phrase": "unsupervised_cross-lingual_speaker_adaptation"}, {"score": 0.0047707093928203, "phrase": "hmm-based_speech_synthesis"}, {"score": 0.0047268728636719725, "phrase": "kld-based_transform_mapping"}, {"score": 0.004640398886726948, "phrase": "emime_project"}, {"score": 0.004534517530868195, "phrase": "mobile_device"}, {"score": 0.004472146692254812, "phrase": "personalized_speech"}, {"score": 0.004390312150344245, "phrase": "speech_translation"}, {"score": 0.004309968592440261, "phrase": "user's_spoken_input"}, {"score": 0.004153647187175639, "phrase": "spoken_output"}, {"score": 0.003947884118204803, "phrase": "user's_voice"}, {"score": 0.003822263046602146, "phrase": "single_architecture"}, {"score": 0.003735090905046665, "phrase": "hmm"}, {"score": 0.0036835897973591203, "phrase": "tts"}, {"score": 0.0036496814103475174, "phrase": "word-based_large-vocabulary_continuous_speech_recognition"}, {"score": 0.0034528025626422154, "phrase": "hmm-based_tts."}, {"score": 0.0033428820402140683, "phrase": "state-level_transform"}, {"score": 0.003281643474564305, "phrase": "minimum_kullback-leibler_divergence"}, {"score": 0.0032215231177304513, "phrase": "hmm_states"}, {"score": 0.0031479142819948007, "phrase": "output_languages"}, {"score": 0.0030759821501544224, "phrase": "unsupervised_cross-lingual_speaker_adaptation_system"}, {"score": 0.0030196181347744372, "phrase": "end-to-end_speech-to-speech_translation_systems"}, {"score": 0.0029508175028748512, "phrase": "english"}, {"score": 0.00292345026265849, "phrase": "finnish"}, {"score": 0.0028965390702177145, "phrase": "mandarin"}, {"score": 0.0028566712825583832, "phrase": "japanese"}, {"score": 0.0026899455459275575, "phrase": "english-to-japanese_adaptation"}, {"score": 0.0025096359514694523, "phrase": "target_speaker"}, {"score": 0.0024865237976623286, "phrase": "average_voices"}, {"score": 0.002429668083484833, "phrase": "supervised_and_unsupervised_cross-lingual_speaker_adaptation"}, {"score": 0.002363150587804677, "phrase": "kld_state-mapping"}, {"score": 0.002225196379204285, "phrase": "detrimental_effect"}, {"score": 0.002164264600379492, "phrase": "synthetic_speech"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["HMM-based speech synthesis", " Unsupervised speaker adaptation", " Cross-lingual speaker adaptation", " Speech-to-speech translation"], "paper_abstract": "In the EMIME project, we developed a mobile device that performs personalized speech-to-speech translation such that a user's spoken input in one language is used to produce spoken output in another language, while continuing to sound like the user's voice. We integrated two techniques into a single architecture: unsupervised adaptation for HMM-based TTS using word-based large-vocabulary continuous speech recognition, and cross-lingual speaker adaptation (CLSA) for HMM-based TTS. The CLSA is based on a state-level transform mapping learned using minimum Kullback-Leibler divergence between pairs of HMM states in the input and output languages. Thus, an unsupervised cross-lingual speaker adaptation system was developed. End-to-end speech-to-speech translation systems for four languages (English, Finnish, Mandarin, and Japanese) were constructed within this framework. In this paper, the English-to-Japanese adaptation is evaluated. Listening tests demonstrate that adapted voices sound more similar to a target speaker than average voices and that differences between supervised and unsupervised cross-lingual speaker adaptation are small. Calculating the KLD state-mapping on only the first 10 mel-cepstral coefficients leads to huge savings in computational costs, without any detrimental effect on the quality of the synthetic speech. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Analysis of unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis using KLD-based transform mapping", "paper_id": "WOS:000303908400002"}