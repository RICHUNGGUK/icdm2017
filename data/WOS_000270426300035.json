{"auto_keywords": [{"score": 0.0447839370319534, "phrase": "differential_inclusion"}, {"score": 0.00481495049065317, "phrase": "solving_a"}, {"score": 0.004686963264680882, "phrase": "saddle_point"}, {"score": 0.004133055661437163, "phrase": "wider_class"}, {"score": 0.004059438255358923, "phrase": "saddle_point_problems"}, {"score": 0.003677280504320636, "phrase": "mixed_linear_equality_constraints"}, {"score": 0.0032423089921154503, "phrase": "suitable_assumption"}, {"score": 0.0031559919178954644, "phrase": "feasible_region"}, {"score": 0.0030171910207671205, "phrase": "global_existence"}, {"score": 0.0026362590493392785, "phrase": "convergence_results"}, {"score": 0.002366361492754389, "phrase": "proposed_differential_inclusion"}, {"score": 0.0021049977753042253, "phrase": "proposed_equation"}], "paper_keywords": ["Convergence in finite time", " Differential inclusion", " Nonsmooth saddle point problems", " Generalized gradient"], "paper_abstract": "In this paper, we propose a differential inclusion for solving a wider class of saddle point problems (not necessarily smooth). The nonsmoothness and the mixed linear equality constraints are the two significant characters of the problem considered in this paper. Under a suitable assumption on the feasible region, we prove the global existence and uniqueness of the solution to the differential inclusion. Moreover, we get some convergence results about the solution to the differential inclusion and the exactness of the proposed differential inclusion. Furthermore, one illustrative example further demonstrates the effectiveness and characteristics of the proposed equation modeled by a differential inclusion.", "paper_title": "SOLVING A CLASS OF SADDLE POINT PROBLEMS BY NEURAL NETWORKS", "paper_id": "WOS:000270426300035"}