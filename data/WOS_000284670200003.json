{"auto_keywords": [{"score": 0.040028642553390774, "phrase": "rdf_form"}, {"score": 0.00481495049065317, "phrase": "conversational_companion"}, {"score": 0.004639822587124109, "phrase": "initial_prototype"}, {"score": 0.004588531238691602, "phrase": "companions_project"}, {"score": 0.0043083624695326745, "phrase": "novel_approaches"}, {"score": 0.004167001961564886, "phrase": "information_extraction"}, {"score": 0.004015345955907906, "phrase": "incoming_dialogue_utterances"}, {"score": 0.003970930401776528, "phrase": "asr_phase"}, {"score": 0.0037283301619292636, "phrase": "new_facts"}, {"score": 0.00370077772049505, "phrase": "existing_ones"}, {"score": 0.0036059320136308808, "phrase": "dialogue_manager"}, {"score": 0.003474624627033273, "phrase": "stored_knowledge"}, {"score": 0.003410768840300913, "phrase": "real_time"}, {"score": 0.0031786037839399055, "phrase": "network_virtual_machine"}, {"score": 0.0031434146012468307, "phrase": "mixed_initiative"}, {"score": 0.003120171161064748, "phrase": "dialogue_control"}, {"score": 0.002984264956378651, "phrase": "corpus_evidence"}, {"score": 0.002951220859491642, "phrase": "prototype_platform"}, {"score": 0.002770745509774383, "phrase": "emotion_detection"}, {"score": 0.0027097117582440687, "phrase": "lexical_content"}, {"score": 0.00265987582861194, "phrase": "extended_forms"}, {"score": 0.0026401983633087267, "phrase": "machine_learning"}, {"score": 0.0026012779346272848, "phrase": "preliminary_studies"}, {"score": 0.0024512358145634358, "phrase": "open_dialogue_systems"}, {"score": 0.002379484250348599, "phrase": "speech_signal"}, {"score": 0.002309828105129336, "phrase": "learned_dm"}, {"score": 0.0022757667100089243, "phrase": "higher_level"}, {"score": 0.002250549946875429, "phrase": "dm_virtual_machine"}, {"score": 0.0022009497295481678, "phrase": "sc's_responses"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Dialogue", " Human-computer interaction", " Dialogue management", " ASR and emotion"], "paper_abstract": "This paper describes an initial prototype of the Companions project (www.companions-project.org): the Senior Companion (SC), designed to be a platform to display novel approaches to: (1) The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an ASR phase. (2) The conversion of the input to RDF form to allow the generation of new facts from existing ones, under the control of a Dialogue Manager (DM), that also has access to stored knowledge and knowledge accessed in real time from the web, all in RDF form. (3) A DM expressed as a stack and network virtual machine that models mixed initiative in dialogue control. (4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning. We describe preliminary studies and results for these, in particular a novel approach to enabling reinforcement learning for open dialogue systems through the detection of emotion in the speech signal and its deployment as a form of a learned DM, at a higher level than the DM virtual machine and able to direct the SC's responses to a more emotionally appropriate part of its repertoire. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "A prototype for a conversational companion for reminiscing about images", "paper_id": "WOS:000284670200003"}