{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "differential_entropy"}, {"score": 0.004103212598803194, "phrase": "unknown_one-dimensional_distribution"}, {"score": 0.0029394074636168435, "phrase": "unknown_distribution"}, {"score": 0.0027865506182608263, "phrase": "previous_distribution-free_bounds"}, {"score": 0.0026771333996253783, "phrase": "cumulative_distribution_function"}, {"score": 0.002572001496778222, "phrase": "random_variable"}], "paper_keywords": ["Convex optimization", " differential entropy", " entropy", " entropy bound", " string-tightening algorithm"], "paper_abstract": "A novel probabilistic upper bound on the entropy of an unknown one-dimensional distribution, given the support of the distribution and a sample from that distribution, is presented. No knowledge beyond the support of the unknown distribution is required. Previous distribution-free bounds on the cumulative distribution function of a random variable given a sample of that variable are used to construct the bound. A simple, fast, and intuitive algorithm for computing the entropy bound from a sample is provided.", "paper_title": "A Probabilistic Upper Bound on Differential Entropy", "paper_id": "WOS:000260426400032"}