{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "emotional_state"}, {"score": 0.0047026237903774895, "phrase": "conversational_computer_game"}, {"score": 0.004647442352794694, "phrase": "automatic_recognition"}, {"score": 0.004611013257082199, "phrase": "user's_communicative_style"}, {"score": 0.004556901872357751, "phrase": "spoken_dialog_system_framework"}, {"score": 0.004485735417293941, "phrase": "affective_aspects"}, {"score": 0.004415675459182871, "phrase": "increased_attention"}, {"score": 0.004329630571352367, "phrase": "dialog_systems"}, {"score": 0.0038624981944755813, "phrase": "richer_and_more_natural_interaction"}, {"score": 0.0035141291835319682, "phrase": "child's_speech_communication_cues"}, {"score": 0.003459191419478298, "phrase": "spontaneous_dialog_interactions"}, {"score": 0.003432044343307207, "phrase": "computer_characters"}, {"score": 0.003097865379563127, "phrase": "wizard-of-oz_dialog_corpus"}, {"score": 0.0029547698888028697, "phrase": "voice_activated_computer_game"}, {"score": 0.002931569771634119, "phrase": "three-way_classification_experiments"}, {"score": 0.0028630535573013686, "phrase": "pairwise_classification"}, {"score": 0.0027307746993761035, "phrase": "experimental_results"}, {"score": 0.0026986685585382347, "phrase": "lexical_information"}, {"score": 0.002656445226355143, "phrase": "acoustic_and_contextual_cues"}, {"score": 0.0025841336988141235, "phrase": "context_and_acoustic_features"}, {"score": 0.0025436978068459565, "phrase": "frustration_detection"}, {"score": 0.0023694065215394593, "phrase": "classification_performance"}, {"score": 0.0022688141624966967, "phrase": "\"politeness\"_detection_task"}], "paper_keywords": ["Emotion recognition", " Spoken dialog systems", " Children speech", " Spontaneous speech", " Natural emotions", " Child-computer interaction", " Feature extraction"], "paper_abstract": "The automatic recognition of user's communicative style within a spoken dialog system framework, including the affective aspects, has received increased attention in the past few years. For dialog systems, it is important to know not only what was said but also how something was communicated, so that the system can engage the user in a richer and more natural interaction. This paper addresses the problem of automatically detecting \"frustration\", \"politeness\", and \"neutral\" attitudes from a child's speech communication cues, elicited in spontaneous dialog interactions with computer characters. Several information sources such as acoustic, lexical, and contextual features, as well as, their combinations are used for this purpose. The study is based on a Wizard-of-Oz dialog corpus of 103 children, 7-14 years of age, playing a voice activated computer game. Three-way classification experiments, as well as, pairwise classification between polite vs. others and frustrated vs. others were performed. Experimental results show that lexical information has more discriminative power than acoustic and contextual cues for detection of politeness, whereas context and acoustic features perform best for frustration detection. Furthermore, the fusion of acoustic, lexical and contextual information provided significantly better classification results. Results also showed that classification performance varies with age and gender. Specifically, for the \"politeness\" detection task, higher classification accuracy was achieved for females and 10-11 years-olds, compared to males and other age groups, respectively. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Detecting emotional state of a child in a conversational computer game", "paper_id": "WOS:000282563500003"}