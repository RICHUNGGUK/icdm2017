{"auto_keywords": [{"score": 0.03367926914121131, "phrase": "finite_number"}, {"score": 0.02661104179666677, "phrase": "self-learning_classes"}, {"score": 0.00481495049065317, "phrase": "memory-limited_non-u-shaped_learning"}, {"score": 0.004732542343057138, "phrase": "empirical_cognitive_science"}, {"score": 0.004697655699119423, "phrase": "human_learning"}, {"score": 0.004663025022394783, "phrase": "semantic_or_behavioral_u-shape"}, {"score": 0.004405830164485517, "phrase": "formal_framework"}, {"score": 0.004384144362375153, "phrase": "inductive_inference"}, {"score": 0.004330393665579603, "phrase": "positive_data"}, {"score": 0.00430907756010007, "phrase": "previous_results"}, {"score": 0.00418335722826862, "phrase": "explanatory_learning"}, {"score": 0.004121873594853149, "phrase": "behaviorally_correct_and_non-trivial_vacillatory_learning"}, {"score": 0.00405127920820053, "phrase": "semantic_and_syntactic_u-shapes"}, {"score": 0.003981889041223034, "phrase": "open_questions"}, {"score": 0.003952513974962462, "phrase": "prior_literature"}, {"score": 0.0039040344230094164, "phrase": "new_results"}, {"score": 0.0038848088569472857, "phrase": "syntactic_u-shapes"}, {"score": 0.003846640194623036, "phrase": "cognitive_science"}, {"score": 0.003780741771838863, "phrase": "previously_noticed_pattern"}, {"score": 0.0037435917371120278, "phrase": "parameterized_learning_criteria"}, {"score": 0.003643293770051351, "phrase": "full_learning_power"}, {"score": 0.0035108250491231365, "phrase": "bounded_memory_state"}, {"score": 0.0034935664739087153, "phrase": "bms"}, {"score": 0.0034167419580838953, "phrase": "explicitly-bounded_state_memory"}, {"score": 0.0031727293410170583, "phrase": "memory_states"}, {"score": 0.003064880297836129, "phrase": "u-shapes"}, {"score": 0.0029900896231825685, "phrase": "open_question"}, {"score": 0.0029460917964452015, "phrase": "second_setting"}, {"score": 0.0029171191200905587, "phrase": "memoryless_feedback"}, {"score": 0.002902745108494467, "phrase": "mlf"}, {"score": 0.002831894638104635, "phrase": "bounded_number"}, {"score": 0.002668822707001489, "phrase": "class_learnable"}, {"score": 0.0026425695441776847, "phrase": "single_feedback_query"}, {"score": 0.002527581719069596, "phrase": "feedback_queries"}, {"score": 0.002364391125079659, "phrase": "inclusion_results"}, {"score": 0.002323826502178581, "phrase": "main_part"}, {"score": 0.0022503253892242406, "phrase": "learning_criteria"}, {"score": 0.002239225269052105, "phrase": "complexity-bounded_learners"}, {"score": 0.00217914399056098, "phrase": "u-shaped_learning"}, {"score": 0.0021470531678441478, "phrase": "wide_range"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Learning from positive data", " U-shaped learning"], "paper_abstract": "In empirical cognitive science, for human learning, a semantic or behavioral U-shape occurs when a learner first learns, then unlearns, and, finally, relearns, some target concept. Within the formal framework of Inductive Inference, for learning from positive data, previous results have shown, for example, that such U-shapes are unnecessary for explanatory learning, but are necessary for behaviorally correct and non-trivial vacillatory learning. Herein we also distinguish between semantic and syntactic U-shapes. We answer a number of open questions in the prior literature as well as provide new results regarding syntactic U-shapes. Importantly for cognitive science, we see more of a previously noticed pattern that, for parameterized learning criteria, beyond very few initial parameter values, U-shapes are necessary for full learning power. We analyze the necessity of U-shapes in two memory-limited settings. The first setting is Bounded Memory State (BMS) learning, where a learner has an explicitly-bounded state memory, and otherwise only knows its current datum. We show that there are classes learnable with three (or more) memory states that are not learnable non-U-shapedly with any finite number of memory states. This result is surprising, since, for learning with one or two memory states, U-shapes are known to be unnecessary. This solves an open question from the literature. The second setting is that of Memoryless Feedback (MLF) learning, where a learner may ask a bounded number of questions about what data has been seen so far, and otherwise only knows its current datum. We show that there is a class learnable memorylessly with a single feedback query such that this class is not learnable non-U-shapedly memorylessly with any finite number of feedback queries. We employ self-learning classes together with the Operator Recursion Theorem for many of our results, but we also introduce two new techniques for obtaining results. The first is for transferring inclusion results from one setting to another. The main part of the second is the Hybrid Operator Recursion Theorem, which enables us to separate some learning criteria featuring complexity-bounded learners, employing self-learning classes. Both techniques are not specific to U-shaped learning, but applicable for a wide range of settings. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Memory-limited non-U-shaped learning with solved open problems", "paper_id": "WOS:000315075300007"}