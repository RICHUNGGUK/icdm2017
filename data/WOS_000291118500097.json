{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "chinese_text_categorization"}, {"score": 0.004749646583168342, "phrase": "automatic_chinese_text_classification"}, {"score": 0.004653339900995785, "phrase": "important_and_a_well-known_technology"}, {"score": 0.004375928080972614, "phrase": "first_step"}, {"score": 0.004287166497171827, "phrase": "chinese_text_categorization_problems"}, {"score": 0.004143196873246147, "phrase": "chinese_words"}, {"score": 0.0040040425282442125, "phrase": "non-segmented_sentences"}, {"score": 0.0038960785110831162, "phrase": "previous_literatures"}, {"score": 0.0037910145286595386, "phrase": "chinese_word"}, {"score": 0.0036386875640595944, "phrase": "different_sources"}, {"score": 0.003173636240475185, "phrase": "incorrect_word"}, {"score": 0.003152006152091526, "phrase": "boundary_information"}, {"score": 0.002963830787280652, "phrase": "n-gram-based_language_model"}, {"score": 0.002767856819656175, "phrase": "chinese_word_tokenizer"}, {"score": 0.002497862210672172, "phrase": "novel_smoothing_approach"}, {"score": 0.002447101794397243, "phrase": "logistic_regression"}, {"score": 0.0023486473525617816, "phrase": "experimental_result"}, {"score": 0.0022696277881969896, "phrase": "traditional_methods"}, {"score": 0.0021932609696277937, "phrase": "micro-average_f-measure"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Text classification", " N-gram-based classification", " Feature selection", " Word segmentation", " Logistic regression"], "paper_abstract": "Automatic Chinese text classification is an important and a well-known technology in the field of machine learning. The first step for solving Chinese text categorization problems is to tokenize the Chinese words from a sequence of non-segmented sentences. However, previous literatures often employ a Chinese word tokenizer that was trained with different sources and then perform the conventional text classification approaches. However, these taggers are not perfect and often provide incorrect word boundary information. In this paper, we propose an N-gram-based language model which takes word relations into account for Chinese text categorization without Chinese word tokenizer. To prevent from out-of-vocabulary, we also propose a novel smoothing approach based on logistic regression to improve accuracy. The experimental result shows that our approach outperforms traditional methods at least 11% on micro-average F-measure. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "A logistic regression-based smoothing method for Chinese text categorization", "paper_id": "WOS:000291118500097"}