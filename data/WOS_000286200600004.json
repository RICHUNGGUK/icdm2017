{"auto_keywords": [{"score": 0.04850460722985758, "phrase": "gpu"}, {"score": 0.02295212839703617, "phrase": "bp"}, {"score": 0.00481495049065317, "phrase": "multiple_feed-forward"}, {"score": 0.0041522059428827345, "phrase": "general-purpose_computations"}, {"score": 0.0040735909648053105, "phrase": "result_graphics_hardware"}, {"score": 0.003939565240409143, "phrase": "unprecedented_performance"}, {"score": 0.0038834787946979863, "phrase": "relatively_low_cost"}, {"score": 0.0037556843869894566, "phrase": "ideal_candidate"}, {"score": 0.0036845490951857617, "phrase": "wide_variety"}, {"score": 0.003632080001489739, "phrase": "parallel_tasks"}, {"score": 0.003529364798514874, "phrase": "machine_learning"}, {"score": 0.00326939082861424, "phrase": "useful_application"}, {"score": 0.0031466527290980686, "phrase": "neural_networks"}, {"score": 0.0029996699429582835, "phrase": "long_training_times"}, {"score": 0.0029569247903654477, "phrase": "learning_process"}, {"score": 0.002845883666777154, "phrase": "gpu_parallel_implementation"}, {"score": 0.002739001004969201, "phrase": "multiple_back-propagation"}, {"score": 0.0026235360404084137, "phrase": "gpu_kernels"}, {"score": 0.002500927169609655, "phrase": "well-known_benchmarks"}, {"score": 0.0024771000236931836, "phrase": "faster_training_times"}, {"score": 0.0023613185213421173, "phrase": "traditional_hardware"}, {"score": 0.00230546852838578, "phrase": "floating-point_throughput"}, {"score": 0.0022188370388602813, "phrase": "preliminary_gpu_based_autonomous_training_system"}, {"score": 0.0021049977753042253, "phrase": "high-quality_nns-based_solutions"}], "paper_keywords": ["Neural networks", " GPU computing", " autonomous training system"], "paper_abstract": "The Graphics Processing Unit (GPU) originally designed for rendering graphics and which is difficult to program for other tasks, has since evolved into a device suitable for general-purpose computations. As a result graphics hardware has become progressively more attractive yielding unprecedented performance at a relatively low cost. Thus, it is the ideal candidate to accelerate a wide variety of data parallel tasks in many fields such as in Machine Learning (ML). As problems become more and more demanding, parallel implementations of learning algorithms are crucial for a useful application. In particular, the implementation of Neural Networks (NNs) in GPUs can significantly reduce the long training times during the learning process. In this paper we present a GPU parallel implementation of the Back-Propagation (BP) and Multiple Back-Propagation (MBP) algorithms, and describe the GPU kernels needed for this task. The results obtained on well-known benchmarks show faster training times and improved performances as compared to the implementation in traditional hardware, due to maximized floating-point throughput and memory bandwidth. Moreover, a preliminary GPU based Autonomous Training System (ATS) is developed which aims at automatically finding high-quality NNs-based solutions for a given problem.", "paper_title": "AN EVALUATION OF MULTIPLE FEED-FORWARD NETWORKS ON GPUs", "paper_id": "WOS:000286200600004"}