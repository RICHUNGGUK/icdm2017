{"auto_keywords": [{"score": 0.035374146279425235, "phrase": "good_performance"}, {"score": 0.029292645274264516, "phrase": "hypothesis-generating_function"}, {"score": 0.00481495049065317, "phrase": "general_reinforcement_learning"}, {"score": 0.004687815426832587, "phrase": "top-down_theoretical_study"}, {"score": 0.004656557209439822, "phrase": "general_reinforcement_learning_agents"}, {"score": 0.004579316323166273, "phrase": "rational_agents"}, {"score": 0.004548778268252678, "phrase": "unlimited_resources"}, {"score": 0.004340614094598847, "phrase": "limited_number"}, {"score": 0.004141936429360251, "phrase": "agent_designer"}, {"score": 0.0038350096441245153, "phrase": "systematic_explorative_behavior"}, {"score": 0.0036349635956605675, "phrase": "finite-error_bounds"}, {"score": 0.0030847461428168614, "phrase": "fully_general_settings"}, {"score": 0.0028179057379595124, "phrase": "rationality_axioms"}, {"score": 0.002789715649746005, "phrase": "decision_function"}, {"score": 0.0026797398345997114, "phrase": "number_errors"}, {"score": 0.0026175952455594277, "phrase": "optimistic_decision_function"}, {"score": 0.0024643161985886085, "phrase": "natural_situations"}, {"score": 0.002431489080631481, "phrase": "bayesian_flavor"}, {"score": 0.0024152394732063908, "phrase": "deterministic_or_stochastic_environments"}, {"score": 0.0023513172266105982, "phrase": "sufficiently_good_bounds"}, {"score": 0.0023044856709624494, "phrase": "practical_success"}, {"score": 0.0021049977753042253, "phrase": "fully_general_reinforcement_learning_environments"}], "paper_keywords": ["reinforcement learning", " rationality", " optimism", " optimality", " error bounds"], "paper_abstract": "In this article,1 we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis-generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments.", "paper_title": "Rationality, Optimism and Guarantees in General Reinforcement Learning", "paper_id": "WOS:000369887100002"}