{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "target_information"}, {"score": 0.04966229511350135, "phrase": "multiple_views"}, {"score": 0.032723156143854966, "phrase": "target_appearance"}, {"score": 0.004731492618963217, "phrase": "robust_visual_tracking"}, {"score": 0.004450568197899267, "phrase": "single_target"}, {"score": 0.004392565073466714, "phrase": "calibrated_multi-camera_surveillance_system"}, {"score": 0.0042415434489566995, "phrase": "first_frame"}, {"score": 0.00407781620412982, "phrase": "online_multiple_instance_learning"}, {"score": 0.004042295865725559, "phrase": "omil"}, {"score": 0.003920384090110374, "phrase": "promising_tracking_results"}, {"score": 0.003769006950727621, "phrase": "real_surveillance_system"}, {"score": 0.003671338229632309, "phrase": "target_orientation"}, {"score": 0.0032054891061528896, "phrase": "camera_node"}, {"score": 0.0031636612155559267, "phrase": "efficient_omil_algorithm"}, {"score": 0.0030281251520610604, "phrase": "omil-based_classifier"}, {"score": 0.002949598537993663, "phrase": "co-training_strategy"}, {"score": 0.0028731024405707277, "phrase": "representative_set"}, {"score": 0.0028480459059007468, "phrase": "training_bags"}, {"score": 0.00270221723573854, "phrase": "unique_weight"}, {"score": 0.0026091594819333654, "phrase": "current_view"}, {"score": 0.0024218895273910943, "phrase": "target_motion"}, {"score": 0.0023902621773166963, "phrase": "camera's_image_plane"}, {"score": 0.0023384629381463054, "phrase": "modified_particle_filter"}, {"score": 0.002297831085501592, "phrase": "corresponding_object"}, {"score": 0.0021801135036476136, "phrase": "experimental_results"}, {"score": 0.0021422271362195734, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "human_tracking"}], "paper_keywords": [""], "paper_abstract": "In this study, the authors address the problem of tracking a single target in a calibrated multi-camera surveillance system with information on its location in the first frame of each view. Recently, tracking with online multiple instance learning (OMIL) has been shown to give promising tracking results. However, it may fail in a real surveillance system because of problems arising from target orientation, scale or illumination changes. In this study, the authors show that fusing target information from multiple views can avoid these problems and lead to a more robust tracker. At each camera node, an efficient OMIL algorithm is used to model target appearance. To update the OMIL-based classifier in one view, a co-training strategy is applied to generate a representative set of training bags from all views. Bags extracted from each view hold a unique weight depending on similarity of target appearance between the current view and the view which contains the classifier that needs to be updated. In addition, target motion on a camera's image plane is modelled by a modified particle filter guided by the corresponding object two-dimensional (2D) location and fused 3D location. Experimental results demonstrate that the proposed algorithm is robust for human tracking in challenging scenes.", "paper_title": "Fusing target information from multiple views for robust visual tracking", "paper_id": "WOS:000333611900002"}