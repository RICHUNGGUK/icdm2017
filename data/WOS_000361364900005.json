{"auto_keywords": [{"score": 0.040410708564970775, "phrase": "pythia"}, {"score": 0.00481495049065317, "phrase": "scalable_data_quality"}, {"score": 0.004783852487938866, "phrase": "big_data"}, {"score": 0.004646366831056171, "phrase": "small_estimation_errors"}, {"score": 0.004616352597792864, "phrase": "large-scale_data_environments"}, {"score": 0.004571692719970733, "phrase": "notoriously_resource-demanding_task"}, {"score": 0.0038750439121298503, "phrase": "novel_framework"}, {"score": 0.0037635782112965662, "phrase": "distributed_data_nodes"}, {"score": 0.0036081880905161606, "phrase": "original_dataset"}, {"score": 0.0035616743422949766, "phrase": "mv_imputation"}, {"score": 0.0034817084307034955, "phrase": "specific_machine"}, {"score": 0.003459191419478298, "phrase": "statistical_learning"}, {"score": 0.003294875957676514, "phrase": "missing_value_substitution_algorithm"}, {"score": 0.0031690434364310435, "phrase": "particular_subset"}, {"score": 0.0029126002350975634, "phrase": "different_cohorts"}, {"score": 0.0027741761880779535, "phrase": "single_machine"}, {"score": 0.0027472888482450776, "phrase": "godzilla"}, {"score": 0.0026942913145235614, "phrase": "entire_massive_dataset"}, {"score": 0.0026681761480139067, "phrase": "imputation_requests"}, {"score": 0.0025167026017217926, "phrase": "pythia_framework"}, {"score": 0.002420516633871837, "phrase": "signature_construction_algorithms"}, {"score": 0.002320450423665291, "phrase": "k-nearest"}, {"score": 0.0021464286727045623, "phrase": "adaptive_vector_quantization"}, {"score": 0.0021325287436338228, "phrase": "competitive_learning"}, {"score": 0.0021049977753042253, "phrase": "comprehensive_experiments"}], "paper_keywords": ["adaptive vector quantization", " missing values", " adaptive resonance theory", " imputation", " big data", " scalability", " self-organizing maps"], "paper_abstract": "Solving the missing-value (MV) problem with small estimation errors in large-scale data environments is a notoriously resource-demanding task. The most widely used MV imputation approaches are computationally expensive because they explicitly depend on the volume and the dimension of the data. Moreover, as datasets and their user community continuously grow, the problem can only be exacerbated. In an attempt to deal with such a problem, in our previous work, we introduced a novel framework coined Pythia, which employs a number of distributed data nodes (cohorts), each of which contains a partition of the original dataset. To perform MV imputation, the Pythia, based on specific machine and statistical learning structures (signatures), selects the most appropriate subset of cohorts to perform locally a missing value substitution algorithm (MVA). This selection relies on the principle that particular subset of cohorts maintains the most relevant partition of the dataset. In addition to this, as Pythia uses only part of the dataset for imputation and accesses different cohorts in parallel, it improves efficiency, scalability, and accuracy compared to a single machine (coined Godzilla), which uses the entire massive dataset to compute imputation requests. Although this article is an extension of our previous work, we particularly investigate the robustness of the Pythia framework and show that the Pythia is independent from any MVA and signature construction algorithms. In order to facilitate our research, we considered two well-known MVAs (namely K-nearest neighbor and expectation-maximization imputation algorithms), as well as two machine and neural computational learning signature construction algorithms based on adaptive vector quantization and competitive learning. We prove comprehensive experiments to assess the performance of the Pythia against Godzilla and showcase the benefits stemmed from this framework.", "paper_title": "Scalable Data Quality for Big Data: The Pythia Framework for Handling Missing Values", "paper_id": "WOS:000361364900005"}