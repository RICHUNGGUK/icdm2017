{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "brain-inspired_computing_part"}, {"score": 0.0040027552011993005, "phrase": "notable_implementations"}, {"score": 0.003200045185608395, "phrase": "\"two_heads"}, {"score": 0.0027651847105654363, "phrase": "u-boost"}, {"score": 0.002608218347502229, "phrase": "geometrical_structure"}, {"score": 0.0025331002386232014, "phrase": "statistical_properties"}, {"score": 0.0022101055924375725, "phrase": "simulation_studies"}, {"score": 0.0021049977753042253, "phrase": "discussed_behaviors"}], "paper_keywords": ["boosting", " classification problem", " large-scale learning machine", " statistical learning theory"], "paper_abstract": "In this article, several boosting methods are discussed, which are notable implementations of the ensemble learning. Starting from the firstly introduced \"boosting by filter\" which is an embodiment of the proverb \"Two heads are better than one\", more advanced versions of boosting methods \"AdaBoost\" and \"U-Boost\" are introduced. A geometrical structure and some statistical properties such as consistency and robustness of boosting algorithms are discussed, and then simulation studies are presented for confirming discussed behaviors of algorithms.", "paper_title": "Tutorial series on brain-inspired computing Part 6: Geometrical structure of boosting algorithm", "paper_id": "WOS:000242924800006"}