{"auto_keywords": [{"score": 0.03191444842303613, "phrase": "appearance_descriptors"}, {"score": 0.00481495049065317, "phrase": "effective_affective_computing"}, {"score": 0.004668240033896263, "phrase": "facial_expression_recognition_systems"}, {"score": 0.004582358363660175, "phrase": "real_world"}, {"score": 0.004415284957189386, "phrase": "previously_unseen_person's_face"}, {"score": 0.00432064072724161, "phrase": "realistic_environments"}, {"score": 0.0042806998378295425, "phrase": "highly_plausible_solution"}, {"score": 0.004228016636379321, "phrase": "\"dense\"_form"}, {"score": 0.00411182818527434, "phrase": "high_accuracy"}, {"score": 0.003949592017305331, "phrase": "dense_alignment"}, {"score": 0.003829147385604486, "phrase": "generic_sense"}, {"score": 0.0037703042917511635, "phrase": "poor_reliability"}, {"score": 0.003644001325962353, "phrase": "\"coarse\"_form"}, {"score": 0.0035219144770122315, "phrase": "biologically_inspired_appearance"}, {"score": 0.003446353878654566, "phrase": "oriented_gradients"}, {"score": 0.0034250633972597254, "phrase": "gabor_magnitudes"}, {"score": 0.003341204247243636, "phrase": "dense_alignment_algorithms"}, {"score": 0.0033000452351286077, "phrase": "high_reliability"}, {"score": 0.0032593915869553714, "phrase": "unseen_subjects"}, {"score": 0.003209275810748739, "phrase": "local_models"}, {"score": 0.0030634906292640947, "phrase": "illumination_variation"}, {"score": 0.0028350413546400703, "phrase": "perfect_alignment"}, {"score": 0.0027741761880779535, "phrase": "real_benefit"}, {"score": 0.002739982973943104, "phrase": "different_appearance-based_representations"}, {"score": 0.0027146141712666694, "phrase": "consistent_illumination_conditions"}, {"score": 0.0025277455170687746, "phrase": "alignment_error"}, {"score": 0.002450554073692113, "phrase": "dense_alignment-subject-dependent_active_appearance_models"}, {"score": 0.0023683568569815606, "phrase": "action-unit_detection"}, {"score": 0.0022397550560279158, "phrase": "pain"}, {"score": 0.0022052711621931144, "phrase": "gemep-fera"}, {"score": 0.0021049977753042253, "phrase": "subject-independent_task"}], "paper_keywords": ["Active appearance models", " automatic facial expression recognition", " biologically-inspired appearance descriptors", " constrained local models"], "paper_abstract": "For facial expression recognition systems to be applicable in the real world, they need to be able to detect and track a previously unseen person's face and its facial movements accurately in realistic environments. A highly plausible solution involves performing a \"dense\" form of alignment, where 60-70 fiducial facial points are tracked with high accuracy. The problem is that, in practice, this type of dense alignment had so far been impossible to achieve in a generic sense, mainly due to poor reliability and robustness. Instead, many expression detection methods have opted for a \"coarse\" form of face alignment, followed by an application of a biologically inspired appearance descriptor such as the histogram of oriented gradients or Gabor magnitudes. Encouragingly, recent advances to a number of dense alignment algorithms have demonstrated both high reliability and accuracy for unseen subjects [e.g., constrained local models (CLMs)]. This begs the question: Aside from countering against illumination variation, what do these appearance descriptors do that standard pixel representations do not? In this paper, we show that, when close to perfect alignment is obtained, there is no real benefit in employing these different appearance-based representations (under consistent illumination conditions). In fact, when misalignment does occur, we show that these appearance descriptors do work well by encoding robustness to alignment error. For this work, we compared two popular methods for dense alignment-subject-dependent active appearance models versus subject-independent CLMs-on the task of action-unit detection. These comparisons were conducted through a battery of experiments across various publicly available data sets (i.e., CK+, Pain, M3, and GEMEP-FERA). We also report our performance in the recent 2011 Facial Expression Recognition and Analysis Challenge for the subject-independent task.", "paper_title": "In the Pursuit of Effective Affective Computing: The Relationship Between Features and Registration", "paper_id": "WOS:000308995000004"}