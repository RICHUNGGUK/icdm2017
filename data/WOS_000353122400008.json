{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "semi-supervised_nearest_mean_classification"}, {"score": 0.00471228691115201, "phrase": "constrained_log-likelihood"}, {"score": 0.004546000131069232, "phrase": "semi-supervised_nearest_mean_classifier"}, {"score": 0.004354149483556514, "phrase": "first_author"}, {"score": 0.00374399358035391, "phrase": "important_suggestion"}, {"score": 0.003611751858608048, "phrase": "error_rates"}, {"score": 0.003038983069081333, "phrase": "classification_error"}, {"score": 0.002995554770117301, "phrase": "mixed_results"}, {"score": 0.0025752890978323873, "phrase": "test_set"}], "paper_keywords": ["Constrained estimation", " inherent loss", " log-likelihood", " nearest centroid", " nearest mean classifier", " semi-supervised learning", " surrogate loss"], "paper_abstract": "We cast a semi-supervised nearest mean classifier, previously introduced by the first author, in a more principled log-likelihood formulation that is subject to constraints. This, in turn, leads us to make the important suggestion to not only investigate error rates of semi-supervised learners but also consider the risk they originally aim to optimize. We demonstrate empirically that in terms of classification error, mixed results are obtained when comparing supervised to semi-supervised nearest mean classification, while in terms of log-likelihood on the test set, the semi-supervised method consistently outperforms its supervised counterpart. Comparisons to self-learning, a standard approach in semi-supervised learning, are included to further clarify the way, in which our constrained nearest mean classifier improves over regular, supervised nearest mean classification.", "paper_title": "Semi-Supervised Nearest Mean Classification Through a Constrained Log-Likelihood", "paper_id": "WOS:000353122400008"}