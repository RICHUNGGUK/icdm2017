{"auto_keywords": [{"score": 0.026163501402362226, "phrase": "error_rate"}, {"score": 0.00481495049065317, "phrase": "appearance_based_sign_language_recognition"}, {"score": 0.0044968982227349625, "phrase": "automatic_sign_language_recognition"}, {"score": 0.004359328949187473, "phrase": "american_sign_language"}, {"score": 0.0041478818670853115, "phrase": "appearance-_based_features"}, {"score": 0.003946650265907885, "phrase": "standard_cameras"}, {"score": 0.0038737214010638745, "phrase": "special_data_acquisition_tools"}, {"score": 0.003640183972131238, "phrase": "complex_preprocessing"}, {"score": 0.0035728978842974246, "phrase": "video_signal"}, {"score": 0.0034851070574149993, "phrase": "intermediate_segmentation_step"}, {"score": 0.003234425353582135, "phrase": "word_recognition"}, {"score": 0.0031549249117721946, "phrase": "publicly_available_set"}, {"score": 0.0031159081999910694, "phrase": "video_streams"}, {"score": 0.0029645930408561086, "phrase": "large_variability"}, {"score": 0.00265038878247831, "phrase": "distinct_pronunciations"}, {"score": 0.0025691622090542304, "phrase": "different_clustering_approaches"}, {"score": 0.0025373711401047772, "phrase": "automatic_clustering"}, {"score": 0.00231111848128734, "phrase": "model_global_image_transformations"}, {"score": 0.0022683432490549064, "phrase": "tangent_distance"}, {"score": 0.0021987994275190314, "phrase": "gaussian_emission_densities"}, {"score": 0.0021580985506699105, "phrase": "bidden_markov_model_classifier"}, {"score": 0.0021049977753042253, "phrase": "euclidean_distance"}], "paper_keywords": [""], "paper_abstract": "In this paper, we present a system for automatic sign language recognition of segmented words in American Sign Language (ASL). The system uses appearance- based features extracted directly from the frames captured by standard cameras without any special data acquisition tools. This means that we do not rely on complex preprocessing of the video signal or on an intermediate segmentation step that may produce errors. We introduce a database for ASL word recognition extracted from a publicly available set of video streams. One important property of this database is the large variability of the utterances for each word. To cope with this variability, we propose to model distinct pronunciations of each word using different clustering approaches. Automatic clustering of pronunciations improves the error rate of the system from 28.4% to 23.2%. To model global image transformations, the tangent distance is used within the Gaussian emission densities of the bidden Markov model classifier instead of the Euclidean distance. This approach can further reduce the error rate to 21.5%.", "paper_title": "Pronunciation clustering and modeling of variability for appearance based sign language recognition", "paper_id": "WOS:000237042600008"}