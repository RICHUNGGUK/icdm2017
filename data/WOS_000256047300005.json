{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "spatiotemporal_volumes"}, {"score": 0.004690305164360866, "phrase": "novel_framework"}, {"score": 0.00462919213721188, "phrase": "video_sequences"}, {"score": 0.004568871736067739, "phrase": "spatiotemporal_segmentation"}, {"score": 0.004431149592844886, "phrase": "appearance_features"}, {"score": 0.004392565073466714, "phrase": "region_correspondence"}, {"score": 0.004278807087784626, "phrase": "interest_point_trajectories"}, {"score": 0.004223032986655664, "phrase": "video_volumes"}, {"score": 0.004186253035932442, "phrase": "point_trajectories"}, {"score": 0.00406001734835979, "phrase": "sift_operator"}, {"score": 0.003954837422946628, "phrase": "motion_segments"}, {"score": 0.0038692638652494697, "phrase": "spatial_properties"}, {"score": 0.003818807671883534, "phrase": "temporal_correspondence"}, {"score": 0.003769006950727621, "phrase": "estimated_motion_segments"}, {"score": 0.003393206299636482, "phrase": "consistently_tracked_motion_segments"}, {"score": 0.0031636612155559267, "phrase": "sift_descriptors"}, {"score": 0.002860546817196148, "phrase": "bipartite_graph"}, {"score": 0.0027140798043053986, "phrase": "edge_weights"}, {"score": 0.002690406375639144, "phrase": "maximum_matching"}, {"score": 0.0026321198895843173, "phrase": "volume_correspondences"}, {"score": 0.002552628669352939, "phrase": "volume_matching_scores"}, {"score": 0.0024864020775253767, "phrase": "final_video_matching_score"}, {"score": 0.002443206010337041, "phrase": "video_retrieval"}, {"score": 0.0023384629381463054, "phrase": "different_sources"}, {"score": 0.0023180581939456686, "phrase": "bbc_motion_gallery"}, {"score": 0.002297831085501592, "phrase": "promising_results"}, {"score": 0.0022382002476510573, "phrase": "qualitative_and_quantitative_analysis"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["video retrieval", " video matching", " spatiotemporal volumes", " motion segmentation"], "paper_abstract": "This paper presents a novel framework for matching video sequences using the spatiotemporal segmentation of videos. Instead of using appearance features for region correspondence across frames, we use interest point trajectories to generate video volumes. Point trajectories, which are generated using the SIFT operator, are clustered to form motion segments by analyzing their motion and spatial properties. The temporal correspondence between the estimated motion segments is then established based on most common SIFT correspondences. A two pass correspondence algorithm is used to handle splitting and merging regions. Spatiotemporal volumes are extracted using the consistently tracked motion segments. Next, a set of features including color, texture, motion, and SIFT descriptors are extracted to represent a volume. We employ an Earth Mover's Distance (EMD) based approach for the comparison of volume features. Given two videos, a bipartite graph is constructed by modeling the volumes as vertices and their similarities as edge weights. Maximum matching of this graph produces volume correspondences between the videos, and these volume matching scores are used to compute the final video matching score. Experiments for video retrieval were performed on a variety of videos obtained from different sources including BBC Motion Gallery and promising results were achieved. We present qualitative and quantitative analysis of retrieval along with a comparison with two baseline methods. (C) 2007 Elsevier Inc. All rights reserved.", "paper_title": "Content based video matching using spatiotemporal volumes", "paper_id": "WOS:000256047300005"}