{"auto_keywords": [{"score": 0.035240068277609664, "phrase": "aixi"}, {"score": 0.004771200988143729, "phrase": "hutter"}, {"score": 0.004621148492564008, "phrase": "incomputable_aixi"}, {"score": 0.0040295196927787056, "phrase": "aixi."}, {"score": 0.0038849148160362257, "phrase": "fully_autonomous_agent"}, {"score": 0.0037454814403672697, "phrase": "artificial_general_intelligence"}, {"score": 0.003711415292828451, "phrase": "agi"}, {"score": 0.003545625043276134, "phrase": "incomplete_definition"}, {"score": 0.00349734405943853, "phrase": "generally_intelligent_agent"}, {"score": 0.003265591844292821, "phrase": "suboptimal_behavior"}, {"score": 0.0031627430133087616, "phrase": "intrinsic_difficulty"}, {"score": 0.0031340312197647487, "phrase": "rl"}, {"score": 0.003021392767766593, "phrase": "new_model"}, {"score": 0.0028210902750714075, "phrase": "solomonoff_induction"}, {"score": 0.0027322025124213566, "phrase": "completely_autonomous_agent"}, {"score": 0.002527788827701828, "phrase": "arbitrary_rewards"}, {"score": 0.0024258208033731154, "phrase": "optimal_way"}, {"score": 0.002370955769074803, "phrase": "strong_asymptotic_optimality"}, {"score": 0.0023173287402646577, "phrase": "horizon_functions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Universal artificial intelligence", " AIXI", " Solomonoff induction", " Artificial general intelligence"], "paper_abstract": "Reinforcement learning (RL) agents like Hutter's universal, Pareto optimal, incomputable AIXI heavily rely on the definition of the rewards, which are necessarily given by some \"teacher\" to define the tasks to solve. Therefore, as is, AIXI. cannot be said to be a fully autonomous agent. From the point of view of artificial general intelligence (AGI), this can be argued to be an incomplete definition of a generally intelligent agent. Furthermore, it has recently been shown that AIXI can converge to a suboptimal behavior in certain situations, hence showing the intrinsic difficulty of RL, with its non-obvious pitfalls. We propose a new model of intelligence, the knowledge-seeking agent (KSA), halfway between Solomonoff induction and AIXI, that defines a completely autonomous agent that does not require a teacher. The goal of this agent is not to maximize arbitrary rewards, but to entirely explore its world in an optimal way. A proof of strong asymptotic optimality for a class of horizon functions shows that this agent behaves according to expectation. Some implications of such an unusual agent are proposed. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Universal knowledge-seeking agents", "paper_id": "WOS:000330823600008"}