{"auto_keywords": [{"score": 0.04842456838190479, "phrase": "voice_qualities"}, {"score": 0.00481495049065317, "phrase": "fuzzy-input_fuzzy-output_support_vector_machines"}, {"score": 0.004681413843223496, "phrase": "dynamic_use"}, {"score": 0.004616034337089482, "phrase": "spoken_language"}, {"score": 0.004567596860086148, "phrase": "useful_information"}, {"score": 0.004519665344659923, "phrase": "speakers_attitude"}, {"score": 0.004456534664975427, "phrase": "affective_states"}, {"score": 0.004124688058533477, "phrase": "speech_signals"}, {"score": 0.0040385352882334235, "phrase": "consistent_labeling"}, {"score": 0.003912655174107315, "phrase": "perceived_voice_quality"}, {"score": 0.003698442219929622, "phrase": "current_study"}, {"score": 0.003447035529485425, "phrase": "breathy_dimension"}, {"score": 0.0032697235411641695, "phrase": "fuzzy-input_fuzzy-output_support_vector_machine"}, {"score": 0.003101503914610488, "phrase": "voice_quality_recordings"}, {"score": 0.0030047427137652218, "phrase": "thorough_analysis"}, {"score": 0.002983652306921617, "phrase": "standard_crisp_approaches"}, {"score": 0.0029522931151878505, "phrase": "promising_results"}, {"score": 0.002890557197908352, "phrase": "standard_support_vector_machines"}, {"score": 0.0028601736602156033, "phrase": "sole_difference"}, {"score": 0.002790511479398411, "phrase": "fuzzy_label_information"}, {"score": 0.002415170557932468, "phrase": "cross_corpus_experiment"}, {"score": 0.0023563212280243682, "phrase": "entirely_different_recording_conditions"}, {"score": 0.0023233376117560295, "phrase": "frame-wise_analysis"}, {"score": 0.002258745901862197, "phrase": "full_sentences"}, {"score": 0.0022036996784968793, "phrase": "fuzzy_measures"}, {"score": 0.0021499920551456956, "phrase": "human_annotators"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Voice quality", " Fuzzy-input fuzzy-output support vector machines", " Fuzzy classification", " LF-model", " Cross corpus analysis"], "paper_abstract": "The dynamic use of voice qualities in spoken language can reveal useful information on a speakers attitude, mood and affective states. This information may be very desirable for a range of, both input and output, speech technology applications. However, voice quality annotation of speech signals may frequently produce far from consistent labeling. Groups of annotators may disagree on the perceived voice quality, but whom should one trust or is the truth somewhere in between? The current study looks first to describe a voice quality feature set that is suitable for differentiating voice qualities on a tense to breathy dimension. Further, the study looks to include these features as inputs to a fuzzy-input fuzzy-output support vector machine ((FSVM)-S-2) algorithm, which is in turn capable of softly categorizing voice quality recordings. The (FSVM)-S-2 is compared in a thorough analysis to standard crisp approaches and shows promising results, while outperforming for example standard support vector machines with the sole difference being that the (FSVM)-S-2 approach receives fuzzy label information during training. Overall, it is possible to achieve accuracies of around 90% for both speaker dependent (cross validation) and speaker independent (leave one speaker out validation) experiments. Additionally, the approach using (FSVM)-S-2 performs at an accuracy of 82% for a cross corpus experiment (i.e. training and testing on entirely different recording conditions) in a frame-wise analysis and of around 97% after temporally integrating over full sentences. Furthermore, the output of fuzzy measures gave performances close to that of human annotators. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Investigating fuzzy-input fuzzy-output support vector machines for robust voice quality classification", "paper_id": "WOS:000311524800015"}