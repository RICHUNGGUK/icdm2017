{"auto_keywords": [{"score": 0.03881565070988674, "phrase": "cenknn"}, {"score": 0.008049480583948958, "phrase": "irrelevant_or_noisy_term_features"}, {"score": 0.005151101774274881, "phrase": "centroid"}, {"score": 0.00481495049065317, "phrase": "big_challenge"}, {"score": 0.004778594263962987, "phrase": "text_classification"}, {"score": 0.004653485665080485, "phrase": "large-scale_and_high-dimensional_text_corpus"}, {"score": 0.004480394224548146, "phrase": "large_number"}, {"score": 0.00421668332357505, "phrase": "varying_degrees"}, {"score": 0.003983512612770236, "phrase": "k-nearest-neighbor"}, {"score": 0.00373474182086791, "phrase": "effective_flat_classifier"}, {"score": 0.0033582976109650608, "phrase": "-d_tree_structure"}, {"score": 0.003320277992107974, "phrase": "nearest_neighbors"}, {"score": 0.003245520963944678, "phrase": "strong_representation_power"}, {"score": 0.0032209766681045365, "phrase": "class_centroids"}, {"score": 0.0031365196064043494, "phrase": "existing_knn_text_classifiers"}, {"score": 0.0030658872249549893, "phrase": "imbalanced_class_distributions"}, {"score": 0.002985484575498061, "phrase": "projected_low-dimensional_data"}, {"score": 0.0029182434479165884, "phrase": "expensive_computation_time"}, {"score": 0.002896167001212006, "phrase": "knn._cenknn"}, {"score": 0.0027151161550531206, "phrase": "complex_data"}, {"score": 0.002526108843593707, "phrase": "chinese"}, {"score": 0.002488001663771768, "phrase": "synthetic_corpora"}, {"score": 0.002413531395475291, "phrase": "significantly_lower-dimensional_space"}, {"score": 0.002305974875400655, "phrase": "existing_scalable_classifiers"}, {"score": 0.002262583629115082, "phrase": "rocchio"}, {"score": 0.0021865218861454256, "phrase": "well-known_classifier"}, {"score": 0.0021291312714904957, "phrase": "highly_imbalanced_corpora"}, {"score": 0.0021049977753042253, "phrase": "small_number"}], "paper_keywords": ["Text classification", " KNN", " Centroid", " Dimension reduction", " Imbalanced classification"], "paper_abstract": "A big challenge in text classification is to perform classification on a large-scale and high-dimensional text corpus in the presence of imbalanced class distributions and a large number of irrelevant or noisy term features. A number of techniques have been proposed to handle this challenge with varying degrees of success. In this paper, by combining the strengths of two widely used text classification techniques, K-Nearest-Neighbor (KNN) and centroid based (Centroid) classifiers, we propose a scalable and effective flat classifier, called CenKNN, to cope with this challenge. CenKNN projects high-dimensional (often hundreds of thousands) documents into a low-dimensional (normally a few dozen) space spanned by class centroids, and then uses the -d tree structure to find nearest neighbors efficiently. Due to the strong representation power of class centroids, CenKNN overcomes two issues related to existing KNN text classifiers, i.e., sensitivity to imbalanced class distributions and irrelevant or noisy term features. By working on projected low-dimensional data, CenKNN substantially reduces the expensive computation time in KNN. CenKNN also works better than Centroid since it uses all the class centroids to define similarity and works well on complex data, i.e., non-linearly separable data and data with local patterns within each class. A series of experiments on both English and Chinese, benchmark and synthetic corpora demonstrates that although CenKNN works on a significantly lower-dimensional space, it performs substantially better than KNN and its five variants, and existing scalable classifiers, including Centroid and Rocchio. CenKNN is also empirically preferable to another well-known classifier, support vector machines, on highly imbalanced corpora with a small number of classes.", "paper_title": "CenKNN: a scalable and effective text classifier", "paper_id": "WOS:000352642800002"}