{"auto_keywords": [{"score": 0.036313720386738475, "phrase": "inverted_indexes"}, {"score": 0.01295967240368833, "phrase": "online_retrieval"}, {"score": 0.009827565833465536, "phrase": "initial_set"}, {"score": 0.009471071047246354, "phrase": "semantic_attributes"}, {"score": 0.00481495049065317, "phrase": "image_retrieval"}, {"score": 0.004748371393578422, "phrase": "content-based_image_retrieval"}, {"score": 0.0046394362688524475, "phrase": "fast_access"}, {"score": 0.004596561972944185, "phrase": "database_images"}, {"score": 0.004408456735066782, "phrase": "indexing_multiple_clues"}, {"score": 0.004367707617503722, "phrase": "image_contents"}, {"score": 0.00432733352307636, "phrase": "retrieval_algorithms_search"}, {"score": 0.00428733103049275, "phrase": "relevant_images"}, {"score": 0.004247696749118451, "phrase": "different_perspectives"}, {"score": 0.00411182818527434, "phrase": "satisfactory_user_experiences"}, {"score": 0.00399881984083004, "phrase": "diverse_image_features"}, {"score": 0.00383508177668219, "phrase": "retrieval_efficiency"}, {"score": 0.003678023470672862, "phrase": "large-scale_image_retrieval"}, {"score": 0.003593555152077465, "phrase": "semantic-aware_co-indexing_algorithm"}, {"score": 0.0032593915869553714, "phrase": "low-level_image_contents"}, {"score": 0.003140401552000472, "phrase": "large-scale_object_recognition"}, {"score": 0.0030825404960282713, "phrase": "image_semantic_meanings"}, {"score": 0.002942495422585405, "phrase": "local_features"}, {"score": 0.0028350413546400703, "phrase": "isolated_images"}, {"score": 0.002795762692061496, "phrase": "semantically_similar_images"}, {"score": 0.0026440008776012665, "phrase": "discriminative_capability"}, {"score": 0.002500456462028735, "phrase": "small_computation_overhead"}, {"score": 0.002267708463966758, "phrase": "existing_image_retrieval_methods"}, {"score": 0.0022155569675462333, "phrase": "retrieval_results"}, {"score": 0.002195033004022173, "phrase": "extensive_experiments"}, {"score": 0.0021545524633190985, "phrase": "recent_retrieval_methods"}, {"score": 0.0021246816854197732, "phrase": "competitive_performance"}], "paper_keywords": ["Large-scale Image Retrieval", " Inverted Indexing", " Vocabulary Trees", " Semantic Attributes", " Deep CNN"], "paper_abstract": "In content-based image retrieval, inverted indexes allow fast access to database images and summarize all knowledge about the database. Indexing multiple clues of image contents allows retrieval algorithms search for relevant images from different perspectives, which is appealing to deliver satisfactory user experiences. However, when incorporating diverse image features during online retrieval, it is challenging to ensure retrieval efficiency and scalability. In this paper, for large-scale image retrieval, we propose a semantic-aware co-indexing algorithm to jointly embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. Specifically, for an initial set of inverted indexes of local features, we utilize semantic attributes to filter out isolated images and insert semantically similar images to this initial set. Encoding these two distinct and complementary cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online retrieval, because only local features but no semantic attributes are employed for the query. Hence, this co-indexing is different from existing image retrieval methods fusing multiple features or retrieval results. Extensive experiments and comparisons with recent retrieval methods manifest the competitive performance of our method.", "paper_title": "Semantic-Aware Co-Indexing for Image Retrieval", "paper_id": "WOS:000364831700017"}