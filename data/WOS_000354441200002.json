{"auto_keywords": [{"score": 0.046069464194864115, "phrase": "space-time_volume"}, {"score": 0.00481495049065317, "phrase": "action_recognition"}, {"score": 0.004550669123303854, "phrase": "video_data"}, {"score": 0.004481132747216135, "phrase": "sparse_self-similar_manifold"}, {"score": 0.00421340730888561, "phrase": "linear_rank_decomposition"}, {"score": 0.004106612344123407, "phrase": "recurrence_plot_theory"}, {"score": 0.003961613484271729, "phrase": "joint_self-similarity_volume"}, {"score": 0.003782655283668953, "phrase": "sparse_action_manifold"}, {"score": 0.0035565113234273926, "phrase": "compact_low-dimensional_descriptors"}, {"score": 0.0033957904376127187, "phrase": "video_sequence"}, {"score": 0.003292685392413911, "phrase": "descriptor_vectors"}, {"score": 0.002895620643297197, "phrase": "video_frame_rates"}, {"score": 0.0027789349951704177, "phrase": "proposed_method"}, {"score": 0.0025992391074270097, "phrase": "different_low-level_features"}, {"score": 0.002456272788591051, "phrase": "oriented_gradients"}, {"score": 0.0022973933270056743, "phrase": "explicit_tracking"}, {"score": 0.002193447487477984, "phrase": "five_public_data_sets"}, {"score": 0.0021377414429251647, "phrase": "promising_results"}], "paper_keywords": ["Action recognition", " self-similarity", " tensor approximation"], "paper_abstract": "We propose that the dynamics of an action in video data forms a sparse self-similar manifold in the space-time volume, which can be fully characterized by a linear rank decomposition. Inspired by the recurrence plot theory, we introduce the concept of Joint Self-Similarity Volume (Joint-SSV) to model this sparse action manifold, and hence propose a new optimized rank-1 tensor approximation of the Joint-SSV to obtain compact low-dimensional descriptors that very accurately characterize an action in a video sequence. We show that these descriptor vectors make it possible to recognize actions without explicitly aligning the videos in time in order to compensate for speed of execution or differences in video frame rates. Moreover, we show that the proposed method is generic, in the sense that it can be applied using different low-level features, such as silhouettes, tracked points, histogram of oriented gradients, and so forth. Therefore, our method does not necessarily require explicit tracking of features in the space-time volume. Our experimental results on five public data sets demonstrate that our method produces promising results and outperforms many baseline methods.", "paper_title": "Exploring Sparseness and Self-Similarity for Action Recognition", "paper_id": "WOS:000354441200002"}