{"auto_keywords": [{"score": 0.03633929124256928, "phrase": "wide_variety"}, {"score": 0.00481495049065317, "phrase": "multirobot_coordination_for_space_exploration"}, {"score": 0.004720043307726269, "phrase": "artificially_intelligent_planetary_rovers"}, {"score": 0.004604023526673226, "phrase": "space_exploration"}, {"score": 0.004513255056769647, "phrase": "reduced_cost"}, {"score": 0.004230382915464492, "phrase": "multiple_autonomous_devices"}, {"score": 0.0039651694106206245, "phrase": "best_results"}, {"score": 0.003753738096719609, "phrase": "simple_task"}, {"score": 0.003661382948704217, "phrase": "large_distances"}, {"score": 0.0036250779580069455, "phrase": "harsh_environments"}, {"score": 0.0033306180744375616, "phrase": "potential_teammates"}, {"score": 0.0032975819773787985, "phrase": "uncertain_and_unsafe_environments"}, {"score": 0.0028394925080958205, "phrase": "coordinated_reinforcement_learning"}, {"score": 0.0025066304722422463, "phrase": "high_overall_system_return"}, {"score": 0.0022799458141165587, "phrase": "state-of-the-art_reward-shaping_techniques"}, {"score": 0.002168975681697056, "phrase": "complex_performance_indicators"}, {"score": 0.002136747803494932, "phrase": "accessible_form"}, {"score": 0.0021049977753042253, "phrase": "key_future_research_directions"}], "paper_keywords": [""], "paper_abstract": "Teams of artificially intelligent planetary rovers have tremendous potential for space exploration, allowing for reduced cost, increased flexibility, and increased reliability. However, having these multiple autonomous devices acting simultaneously leads to a problem of coordination: to achieve the best results, they should work together. This is not a simple task. Due to the large distances and harsh environments, a rover must be able to perform a wide variety of tasks with a wide variety of potential teammates in uncertain and unsafe environments. Directly coding all the necessary rules that can reliably handle all of this coordination and uncertainty is problematic. Instead, this article examines tackling this problem through the use of coordinated reinforcement learning: rather than being programmed what to do, the rovers iteratively learn through trial and error to take take actions that lead to high overall system return. To allow for coordination, yet allow each agent to learn and act independently, we employ state-of-the-art reward-shaping techniques. This article uses visualization techniques to break down complex performance indicators into an accessible form and identifies key future research directions.", "paper_title": "Multirobot Coordination for Space Exploration", "paper_id": "WOS:000349277200006"}