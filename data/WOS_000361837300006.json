{"auto_keywords": [{"score": 0.0317638439001368, "phrase": "ccm"}, {"score": 0.015719526011421067, "phrase": "cluster_cache_monitor"}, {"score": 0.012484559192884822, "phrase": "home_nodes"}, {"score": 0.011103935725612671, "phrase": "long-distance_accesses"}, {"score": 0.004748556604045108, "phrase": "proximity_data"}, {"score": 0.004586531354697466, "phrase": "working_sets"}, {"score": 0.0045547927862194856, "phrase": "parallel_workloads"}, {"score": 0.004384098916012568, "phrase": "better_use"}, {"score": 0.004338661611644755, "phrase": "total_available_cache_capacity"}, {"score": 0.004147123642915346, "phrase": "longer_average_distance"}, {"score": 0.00395026016389782, "phrase": "main_causes"}, {"score": 0.0036341965226247028, "phrase": "high_probability"}, {"score": 0.0035965032173588753, "phrase": "target_data"}, {"score": 0.0034615969985956866, "phrase": "neighbor_node"}, {"score": 0.0031187606191012012, "phrase": "aforementioned_property"}, {"score": 0.0029705677977833857, "phrase": "hardware_structure"}, {"score": 0.002742196991950893, "phrase": "coherence_protocol"}, {"score": 0.0025138404303805813, "phrase": "parsec"}, {"score": 0.0024109902338204873, "phrase": "execution_time"}, {"score": 0.0022883734727569298, "phrase": "directory_storage_area"}, {"score": 0.0022567398334877847, "phrase": "standard_multi-core"}, {"score": 0.002164432930854161, "phrase": "recent_mechanisms"}, {"score": 0.00213451831208554, "phrase": "asr"}, {"score": 0.002119716311831437, "phrase": "dcc"}, {"score": 0.0021049977753042253, "phrase": "rnuca."}], "paper_keywords": [""], "paper_abstract": "As the number of cores and the working sets of parallel workloads increase, shared L2 caches exhibit fewer misses than private L2 caches by making a better use of the total available cache capacity, but they also induce higher overall L1 miss latencies because of the longer average distance between two nodes, and the potential congestions at certain nodes. One of the main causes of the long L1 miss latencies are accesses to home nodes of the directory. However, we have observed that there is a high probability that the target data of an L1 miss resides in the L1 cache of a neighbor node. In such cases, these long-distance accesses to the home nodes can be potentially avoided. We organize the multi-core into clusters of nodes, and in order to leverage the aforementioned property, we introduce the Cluster Cache Monitor (CCM). The CCM is a hardware structure in charge of detecting whether an L1 miss can be served by one of the cluster L1 caches, and two cluster-related states are added in the coherence protocol in order to avoid long-distance accesses to home nodes upon hits in the cluster L1 caches. We evaluate this approach on a 64-node multi-core using SPLASH-2 and PARSEC benchmarks, and we find that the CCM can reduce the execution time by 15 % and reduce the energy by 14 %, while saving 28 % of the directory storage area compared to a standard multi-core with a shared L2. We also show that the CCM outperforms recent mechanisms, such as ASR, DCC and RNUCA.", "paper_title": "Cluster Cache Monitor: Leveraging the Proximity Data in CMP", "paper_id": "WOS:000361837300006"}