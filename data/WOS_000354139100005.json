{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "adversarial_label_contamination"}, {"score": 0.004766683200410046, "phrase": "machine_learning_algorithms"}, {"score": 0.004624751696958501, "phrase": "security-related_tasks"}, {"score": 0.004509694461067052, "phrase": "malware_detection"}, {"score": 0.004397487039229447, "phrase": "deliberate_attacks"}, {"score": 0.00418134346679498, "phrase": "adaptive_attackers"}, {"score": 0.0040978781692373005, "phrase": "specific_vulnerabilities"}, {"score": 0.004036369952624852, "phrase": "machine_learning_techniques"}, {"score": 0.003975781274672803, "phrase": "system_security"}, {"score": 0.003876806615011266, "phrase": "adversarial_data_manipulation"}, {"score": 0.0036676174658755683, "phrase": "adversarial_settings"}, {"score": 0.003469676666252421, "phrase": "support_vector_machines"}, {"score": 0.003152531962718477, "phrase": "svm's_classification_error"}, {"score": 0.003012560489169148, "phrase": "training_data"}, {"score": 0.0029374939071038146, "phrase": "corresponding_optimal_attack_strategy"}, {"score": 0.002821247621250663, "phrase": "heuristic_approaches"}, {"score": 0.002764856305713066, "phrase": "computational_complexity"}, {"score": 0.002682370021834353, "phrase": "extensive_experimental_analysis"}, {"score": 0.0026023381988430666, "phrase": "considered_attacks"}, {"score": 0.0025761937402886954, "phrase": "linear_and_non-linear_svms"}, {"score": 0.002524688179363081, "phrase": "synthetic_and_real-world_datasets"}, {"score": 0.002400373755068185, "phrase": "useful_insights"}, {"score": 0.0023287359930308864, "phrase": "also_novel_techniques"}, {"score": 0.0022706700510423954, "phrase": "related_research_areas"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Support vector machines", " Adversarial learning", " Label noise", " Label flip attacks"], "paper_abstract": "Machine learning algorithms are increasingly being applied in security-related tasks such as spam and malware detection, although their security properties against deliberate attacks have not yet been widely understood. Intelligent and adaptive attackers may indeed exploit specific vulnerabilities exposed by machine learning techniques to violate system security. Being robust to adversarial data manipulation is thus an important, additional requirement for machine learning algorithms to successfully operate in adversarial settings. In this work, we evaluate the security of Support Vector Machines (SVMs) to wellcrafted, adversarial label noise attacks. In particular, we consider an attacker that aims to maximize the SVM's classification error by flipping a number of labels in the training data. We formalize a corresponding optimal attack strategy, and solve it by means of heuristic approaches to keep the computational complexity tractable. We report an extensive experimental analysis on the effectiveness of the considered attacks against linear and non-linear SVMs, both on synthetic and real-world datasets. We finally argue that our approach can also provide useful insights for developing more secure SVM learning algorithms, and also novel techniques in a number of related research areas, such as semi-supervised and active learning. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Support vector machines under adversarial label contamination", "paper_id": "WOS:000354139100005"}