{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "online_learning"}, {"score": 0.004655924035639928, "phrase": "non-identical_distributions"}, {"score": 0.004209539728590977, "phrase": "identical_distribution"}, {"score": 0.0038833278847237858, "phrase": "different_setting"}, {"score": 0.003704797078215222, "phrase": "non-identical_sequence"}, {"score": 0.0033946556965877873, "phrase": "different_distribution"}, {"score": 0.003216804708106142, "phrase": "fully_online_learning_algorithm"}, {"score": 0.003131393466870228, "phrase": "general_convex_loss_function"}, {"score": 0.0030688220103201836, "phrase": "reproducing_kernel_hilbert_space"}, {"score": 0.0029672941061574375, "phrase": "error_analysis"}, {"score": 0.0027741761880779535, "phrase": "marginal_distributions"}, {"score": 0.002628747284714371, "phrase": "holder_space"}, {"score": 0.002541741549046619, "phrase": "least_square_or_insensitive_loss"}, {"score": 0.002282166507956957, "phrase": "hinge_loss"}, {"score": 0.002251637552201872, "phrase": "support_vector_machine_q-norm_loss"}, {"score": 0.0021049977753042253, "phrase": "excess_misclassification_error"}], "paper_keywords": ["sampling with non-identical distributions", " online learning", " classification with a general convex loss", " regression with insensitive loss and least square loss", " reproducing kernel Hilbert space"], "paper_abstract": "Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a Holder space. For regression with least square or insensitive loss, learning rates are given in both the RKHS norm and the L-2 norm. For classification with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassification error.", "paper_title": "Online Learning with Samples Drawn from Non-identical Distributions", "paper_id": "WOS:000273877300005"}