{"auto_keywords": [{"score": 0.04628969214633414, "phrase": "variable_selection"}, {"score": 0.008529480331647661, "phrase": "bilinear_regression_models"}, {"score": 0.006209199809472886, "phrase": "cross_model_validation"}, {"score": 0.004815832404439633, "phrase": "cross"}, {"score": 0.004662427600500598, "phrase": "regression_models"}, {"score": 0.004514714250357749, "phrase": "optimisation_steps"}, {"score": 0.004371660146981191, "phrase": "parameter_estimation"}, {"score": 0.0043095378396174125, "phrase": "overly_optimistic_models"}, {"score": 0.0041283957965834875, "phrase": "reported_work"}, {"score": 0.0032712190037495975, "phrase": "extra_layer"}, {"score": 0.0031788080316930687, "phrase": "available_samples"}, {"score": 0.003055977395076106, "phrase": "residual_error"}, {"score": 0.0029802879302375986, "phrase": "challenging_questions"}, {"score": 0.0028651061556017304, "phrase": "full_work-flow"}, {"score": 0.0027941308786390033, "phrase": "complete_framework"}, {"score": 0.0026008598632891837, "phrase": "separate_stage"}, {"score": 0.0024383449794706477, "phrase": "gene_expression_data"}, {"score": 0.0024036292057373803, "phrase": "low_signal-to-noise_ratio"}, {"score": 0.0022133711375320244, "phrase": "cross_model"}, {"score": 0.002158505888557746, "phrase": "associated_error_estimates"}, {"score": 0.0021354096283081317, "phrase": "matlab_toolbox"}, {"score": 0.0021201492891323587, "phrase": "mathworks_inc"}, {"score": 0.002105024261911892, "phrase": "usa"}], "paper_keywords": ["cross model validation", " partial least squares regression", " PLSR", " variable selection", " backward elimination", " microarray data"], "paper_abstract": "Whenever regression models are optimised, it is important that all optimisation steps are properly validated. Variable selection is one example of parameter estimation that will give overly optimistic models if not included in the validation. There are many examples of reported work where the validation is performed posterior to variable selection, and many have correctly noted that these models are optimistically biased. However, if the availability of samples is limited, separation of the data into a training and validation set may decrease the quality of both the calibration model and the validation. Cross model validation is designed to validate the optimisation by including the variable selection in an extra layer of cross-validation. This means that all available samples are utilised both in the training and for estimating the residual error of the model. Cross model validation poses challenging questions both conceptually and algorithmically, and a presentation of the full work-flow is needed. We present a complete framework including optimisation, validation and calibration of bilinear regression models with variable selection. Several issues are addressed that are important for each separate stage of the analysis, and suggestions for improvements are proposed. The method is validated on a gene expression data set with a low signal-to-noise ratio and a small number of samples. It is shown that many replicates are needed to model these data properly, and that cross model validated variable selection improves both the final calibration model and the associated error estimates. A Matlab toolbox (Mathworks Inc, USA) is available from www.specmod.org. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Cross model validation and optimisation of bilinear regression models", "paper_id": "WOS:000257825400001"}