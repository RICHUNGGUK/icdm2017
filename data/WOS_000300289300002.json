{"auto_keywords": [{"score": 0.04641494111758786, "phrase": "counterexample_trace"}, {"score": 0.004664352260912174, "phrase": "model_checker"}, {"score": 0.0039005612858880115, "phrase": "non-trivial_task"}, {"score": 0.0034757552381557604, "phrase": "halpern"}, {"score": 0.0034482261029360256, "phrase": "pearl"}, {"score": 0.0031342307827666675, "phrase": "red_dots"}, {"score": 0.003036040409062437, "phrase": "visual_explanation"}, {"score": 0.0028944757265206332, "phrase": "exact_set"}, {"score": 0.002814956065949522, "phrase": "polynomial-time_algorithm"}, {"score": 0.002468450551616404, "phrase": "ibm_formal_verification_platform"}, {"score": 0.0024101837218440834, "phrase": "visual_explanations"}, {"score": 0.002381566880282456, "phrase": "integral_part"}, {"score": 0.002181813794658713, "phrase": "light-weight_external_layer"}, {"score": 0.0021559025985688255, "phrase": "model_checking_tool"}, {"score": 0.0021049977753042253, "phrase": "simulation_traces"}], "paper_keywords": ["Formal verification", " Hardware verification", " Explanation of counterexamples", " Causality"], "paper_abstract": "When a model does not satisfy a given specification, a counterexample is produced by the model checker to demonstrate the failure. A user must then examine the counterexample trace, in order to visually identify the failure that it demonstrates. If the trace is long, or the specification is complex, finding the failure in the trace becomes a non-trivial task. In this paper, we address the problem of analyzing a counterexample trace and highlighting the failure that it demonstrates. Using the notion of causality introduced by Halpern and Pearl, we formally define a set of causes for the failure of the specification on the given counterexample trace. These causes are marked as red dots and presented to the user as a visual explanation of the failure. We study the complexity of computing the exact set of causes, and provide a polynomial-time algorithm that approximates it. We then analyze the output of the algorithm and compare it to the one expected by the definition. The algorithm is implemented as a feature in the IBM formal verification platform RuleBase PE, where the visual explanations are an integral part of every counterexample trace. Our approach is independent of the tool that produced the counterexample, and can be applied as a light-weight external layer to any model checking tool, or used to explain simulation traces.", "paper_title": "Explaining counterexamples using causality", "paper_id": "WOS:000300289300002"}