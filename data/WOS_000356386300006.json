{"auto_keywords": [{"score": 0.04971245563968758, "phrase": "equilibrium_transfer"}, {"score": 0.011865668511780473, "phrase": "equilibrium-based_marl"}, {"score": 0.007740489563401452, "phrase": "multiagent_reinforcement_learning"}, {"score": 0.007478109570476077, "phrase": "marl"}, {"score": 0.004791284806874058, "phrase": "transfer_loss"}, {"score": 0.004723322467153553, "phrase": "important_approach"}, {"score": 0.004527804152066942, "phrase": "equilibrium_solution_concepts"}, {"score": 0.004493130934067195, "phrase": "game_theory"}, {"score": 0.004390689303780405, "phrase": "equilibrium_strategies"}, {"score": 0.004160612392068268, "phrase": "large_number"}, {"score": 0.0041287394089001405, "phrase": "computationally_expensive_equilibrium_computations"}, {"score": 0.004065721092116729, "phrase": "computing_nash_equilibria"}, {"score": 0.004034575930655251, "phrase": "ppad"}, {"score": 0.0038973170902076707, "phrase": "first_time"}, {"score": 0.0037792227071149985, "phrase": "learning_process"}, {"score": 0.0037072310568610723, "phrase": "one-shot_games"}, {"score": 0.003650622777534568, "phrase": "state's_successive_visits"}, {"score": 0.003459191419478298, "phrase": "successive_visits"}, {"score": 0.0034326732528567826, "phrase": "similar_equilibria"}, {"score": 0.00322769360059123, "phrase": "equilibrium-based_marl."}, {"score": 0.0031178024467857215, "phrase": "previously_computed_equilibria"}, {"score": 0.003046621804854539, "phrase": "small_incentive"}, {"score": 0.002954228277691572, "phrase": "transfer_condition"}, {"score": 0.0029203055813347874, "phrase": "novel_framework"}, {"score": 0.0027247727563746694, "phrase": "equilibrium-based_marl_algorithms"}, {"score": 0.002662541169311738, "phrase": "equilibrium_policy"}, {"score": 0.0026218430041123164, "phrase": "experimental_results"}, {"score": 0.002601727192945944, "phrase": "widely_used_benchmarks"}, {"score": 0.002427473263013356, "phrase": "proposed_framework"}, {"score": 0.0022474806085983536, "phrase": "higher_average_rewards"}], "paper_keywords": ["Equilibrium", " equilibrium transfer", " game theory", " multiagent reinforcement learning (MARL)"], "paper_abstract": "An important approach in multiagent reinforcement learning (MARL) is equilibrium-based MARL, which adopts equilibrium solution concepts in game theory and requires agents to play equilibrium strategies at each state. However, most existing equilibrium-based MARL algorithms cannot scale due to a large number of computationally expensive equilibrium computations (e.g., computing Nash equilibria is PPAD-hard) during learning. For the first time, this paper finds that during the learning process of equilibrium-based MARL, the one-shot games corresponding to each state's successive visits often have the same or similar equilibria (for some states more than 90% of games corresponding to successive visits have similar equilibria). Inspired by this observation, this paper proposes to use equilibrium transfer to accelerate equilibrium-based MARL. The key idea of equilibrium transfer is to reuse previously computed equilibria when each agent has a small incentive to deviate. By introducing transfer loss and transfer condition, a novel framework called equilibrium transfer-based MARL is proposed. We prove that although equilibrium transfer brings transfer loss, equilibrium-based MARL algorithms can still converge to an equilibrium policy under certain assumptions. Experimental results in widely used benchmarks (e.g., grid world game, soccer game, and wall game) show that the proposed framework: 1) not only significantly accelerates equilibrium-based MARL (up to 96.7% reduction in learning time), but also achieves higher average rewards than algorithms without equilibrium transfer and 2) scales significantly better than algorithms without equilibrium transfer when the state/action space grows and the number of agents increases.", "paper_title": "Accelerating Multiagent Reinforcement Learning by Equilibrium Transfer", "paper_id": "WOS:000356386300006"}