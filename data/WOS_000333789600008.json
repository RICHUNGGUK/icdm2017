{"auto_keywords": [{"score": 0.04196815273226382, "phrase": "paired_objects"}, {"score": 0.01097634946574912, "phrase": "human_actions"}, {"score": 0.00481495049065317, "phrase": "novel_object_object"}, {"score": 0.004634799977448855, "phrase": "intelligent_robots"}, {"score": 0.004518442937844318, "phrase": "interactive_functionalities"}, {"score": 0.004404994123834589, "phrase": "human_demonstrations"}, {"score": 0.004349337717774093, "phrase": "everyday_environments"}, {"score": 0.004186534737631288, "phrase": "single_object"}, {"score": 0.004055511503825102, "phrase": "interactive_motions"}, {"score": 0.003928572641266562, "phrase": "human_object_object_way"}, {"score": 0.00385431710959668, "phrase": "innate_interaction"}, {"score": 0.003593812067062062, "phrase": "labeled_training_dataset"}, {"score": 0.003459191419478298, "phrase": "relative_motions"}, {"score": 0.003287482882222731, "phrase": "object_labels"}, {"score": 0.0032253056091816495, "phrase": "learned_knowledge"}, {"score": 0.0031242709509664837, "phrase": "bayesian_network"}, {"score": 0.002912963877144772, "phrase": "recognition_reliability"}, {"score": 0.002750721367040336, "phrase": "proper_manipulation_motion"}, {"score": 0.002468450551616404, "phrase": "image-based_visual_servoing_approach"}, {"score": 0.0024063484128685367, "phrase": "learned_motion_features"}, {"score": 0.002286781334850213, "phrase": "control_goals"}, {"score": 0.0021870332034407817, "phrase": "manipulation_tasks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action recognition", " Robot learning", " Learn from demonstration", " Object classification", " Graphical model"], "paper_abstract": "This paper presents a novel object object affordance learning approach that enables intelligent robots to learn the interactive functionalities of objects from human demonstrations in everyday environments. Instead of considering a single object, we model the interactive motions between paired objects in a human object object way. The innate interaction-affordance knowledge of the paired objects are learned from a labeled training dataset that contains a set of relative motions of the paired objects, human actions, and object labels. The learned knowledge is represented with a Bayesian Network, and the network can be used to improve the recognition reliability of both objects and human actions and to generate proper manipulation motion for a robot if a pair of objects is recognized. This paper also presents an image-based visual servoing approach that uses the learned motion features of the affordance in interaction as the control goals to control a robot to perform manipulation tasks. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Object-object interaction affordance learning", "paper_id": "WOS:000333789600008"}