{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "relational_domains"}, {"score": 0.004769444094319619, "phrase": "model-based_reinforcement_learning"}, {"score": 0.004701985831087475, "phrase": "fundamental_problem"}, {"score": 0.004657542074154025, "phrase": "reinforcement_learning"}, {"score": 0.004215397034321004, "phrase": "large_stochastic_relational_domains"}, {"score": 0.004155742403177802, "phrase": "relational_extensions"}, {"score": 0.004000741805312148, "phrase": "r-max_algorithms"}, {"score": 0.003906804492585634, "phrase": "exponentially_large_state_spaces"}, {"score": 0.0037610536973798113, "phrase": "learned_model"}, {"score": 0.0036727235025069828, "phrase": "propositional_setting"}, {"score": 0.0034526191487098093, "phrase": "relational_setting"}, {"score": 0.0034037221513153566, "phrase": "well-known_context"}, {"score": 0.0032149396890830575, "phrase": "relational_count_functions"}, {"score": 0.003154360892955196, "phrase": "classical_notion"}, {"score": 0.002965229277927781, "phrase": "exploration_efficiency"}, {"score": 0.002909342017421165, "phrase": "count_functions"}, {"score": 0.0028006988338192375, "phrase": "relational_kwik_learner"}, {"score": 0.0027610089418453614, "phrase": "near-optimal_planner"}, {"score": 0.002696101728474703, "phrase": "concrete_exploration_algorithm"}, {"score": 0.0026452734009203764, "phrase": "practically_efficient_probabilistic_rule_learner"}, {"score": 0.0024051153560799335, "phrase": "learned_relational_rules"}, {"score": 0.002186712881521657, "phrase": "international_planning_competition"}, {"score": 0.0021049977753042253, "phrase": "existing_propositional_and_factored_exploration_techniques"}], "paper_keywords": ["reinforcement learning", " statistical relational learning", " exploration", " relational transition models", " robotics"], "paper_abstract": "A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E-3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.", "paper_title": "Exploration in Relational Domains for Model-based Reinforcement Learning", "paper_id": "WOS:000314529000009"}