{"auto_keywords": [{"score": 0.03807729444122008, "phrase": "base_classifiers"}, {"score": 0.016082293835151147, "phrase": "ensemble_creation"}, {"score": 0.005205990707518576, "phrase": "cluster_number"}, {"score": 0.00481495049065317, "phrase": "classifier_ensemble_framework"}, {"score": 0.004776608455951671, "phrase": "classifier_selection"}, {"score": 0.004751216062917875, "phrase": "decision_tree"}, {"score": 0.004638602678192715, "phrase": "machine_learning"}, {"score": 0.004601658429231648, "phrase": "data_mining"}, {"score": 0.004577191791065994, "phrase": "classification_problem"}, {"score": 0.004528646311967683, "phrase": "general_classifier"}, {"score": 0.004468684611798047, "phrase": "pattern_recognition_communities"}, {"score": 0.004374385639298755, "phrase": "problem's_dataset"}, {"score": 0.004070515037356991, "phrase": "specific_problems"}, {"score": 0.004005896380356443, "phrase": "strong_solution"}, {"score": 0.0038900709793274484, "phrase": "specific_problem"}, {"score": 0.0038487848937121204, "phrase": "ensemble_learning"}, {"score": 0.003818107035361389, "phrase": "good_way"}, {"score": 0.003777581820724243, "phrase": "near-optimal_classifying_system"}, {"score": 0.003678133730034786, "phrase": "classifier_ensemble"}, {"score": 0.0036390887584943723, "phrase": "suitable_ensemble"}, {"score": 0.0034407781868385423, "phrase": "successful_ensemble"}, {"score": 0.0032272929113113203, "phrase": "ensemble_classifiers"}, {"score": 0.003002865798215281, "phrase": "novel_method"}, {"score": 0.002816474286556831, "phrase": "classifiers_technique"}, {"score": 0.0027058852678238632, "phrase": "base_classifier"}, {"score": 0.002662871928330334, "phrase": "decision_tree_classier"}, {"score": 0.002641621667026613, "phrase": "multilayer_perceptron_classifier"}, {"score": 0.0025514745448798385, "phrase": "clustering_algorithm"}, {"score": 0.002531111001325683, "phrase": "csbc"}, {"score": 0.0025109095718850376, "phrase": "final_ensemble"}, {"score": 0.0024512651776530023, "phrase": "weighted_majority_vote_method"}, {"score": 0.002418742283474052, "phrase": "aggregator_function"}, {"score": 0.002317533977770783, "phrase": "csbc_method"}, {"score": 0.002256435842862802, "phrase": "good_approximate_value"}, {"score": 0.002185235873435714, "phrase": "large_number"}, {"score": 0.0021735890994119757, "phrase": "real_datasets"}, {"score": 0.0021620042653528846, "phrase": "uci_repository"}, {"score": 0.0021390191069313945, "phrase": "definite_result"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Decision tree", " Classifier ensembles", " Clustering", " Bagging", " Learning", " Ada Boosting"], "paper_abstract": "One of the most important tasks in pattern, machine learning, and data mining is classification problem. Introducing a general classifier is a challenge for pattern recognition communities, which enables one to learn each problem's dataset. Many classifiers have been proposed to learn any problem thus far. However, many of them have their own positive and negative aspects. So they are good only for specific problems. But there is no strong solution to recognize which classifier is better or good for a specific problem. Fortunately, ensemble learning provides a good way to have a near-optimal classifying system for any problem. One of the most challenging problems in classifier ensemble is introducing a suitable ensemble of base classifiers. Every ensemble needs diversity. It means that if a group of classifiers is to be a successful ensemble, they must be diverse enough to cover their errors. Therefore, during ensemble creation, a mechanism is needed to ensure that the ensemble classifiers are diverse. Sometimes this mechanism can select/remove a subset of base classifiers with respect to maintaining the diversity of the ensemble. This paper proposes a novel method, named the Classifier Selection Based on Clustering (CSBS), for ensemble creation. To insure diversity in ensemble classifiers, this method uses the clustering of classifiers technique. Bagging is used to produce base classifiers. During ensemble creation, every type of base classifier is the same as a decision tree classier or a multilayer perceptron classifier. After producing a number of base classifiers, CSBC partitions them by using a clustering algorithm. Then CSBC produces a final ensemble by selecting one classifier from each cluster. Weighted majority vote method is used as an aggregator function. In this paper we investigate the influence of cluster number on the performance of the CSBC method; we also probe how we can select a good approximate value for cluster number in any dataset. We base our study on a large number of real datasets of UCI repository to reach a definite result. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Proposing a classifier ensemble framework based on classifier selection and decision tree", "paper_id": "WOS:000346453800004"}