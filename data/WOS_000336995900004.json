{"auto_keywords": [{"score": 0.042599325952589626, "phrase": "training_data"}, {"score": 0.015043882960475326, "phrase": "relevant_samples"}, {"score": 0.01180806248211387, "phrase": "unlabeled_data"}, {"score": 0.009059657829322448, "phrase": "ap"}, {"score": 0.00481495049065317, "phrase": "video_semantic_annotation"}, {"score": 0.004751216062917875, "phrase": "video_annotation"}, {"score": 0.004646852519870358, "phrase": "relevancy_degrees"}, {"score": 0.004251728624599691, "phrase": "typical_relevant_samples"}, {"score": 0.004195418010131832, "phrase": "non-typical_ones"}, {"score": 0.004121490798711703, "phrase": "generally_the_labels"}, {"score": 0.0038556354709511818, "phrase": "typical_or_non-typical_training_samples"}, {"score": 0.003737485125097945, "phrase": "learning_process"}, {"score": 0.0036553070575015344, "phrase": "learned_scores"}, {"score": 0.0032852337992126564, "phrase": "real-valued_typicality_scores"}, {"score": 0.0032272929113113203, "phrase": "typicality_scores"}, {"score": 0.0031006232333900055, "phrase": "manifold_ranking"}, {"score": 0.0029789104204803137, "phrase": "novel_criterion"}, {"score": 0.002952516738708181, "phrase": "average_typicality_precision"}, {"score": 0.0026652435674846095, "phrase": "video_typicality_ranking_algorithms"}, {"score": 0.0025042115651986332, "phrase": "annotation_rank_list"}, {"score": 0.00241658945942583, "phrase": "typicality_order"}, {"score": 0.0022806798371116698, "phrase": "evaluation_strategy"}, {"score": 0.0022705467477443417, "phrase": "atp._experiments"}, {"score": 0.0022304616068189575, "phrase": "trecvid_data"}, {"score": 0.0021813467128927347, "phrase": "typicality_ranking_scheme"}, {"score": 0.0021333110194180997, "phrase": "human_perception"}, {"score": 0.0021143936116203594, "phrase": "normal_accuracy"}, {"score": 0.0021049977753042253, "phrase": "based_ranking_schemes"}], "paper_keywords": ["Video Annotation", " Typicality Ranking", " Average Typicality Precision"], "paper_abstract": "In video annotation, the typicalities or relevancy degrees of relevant samples to a certain concept are generally different. Thus we argue that it is more reasonable to rank typical relevant samples higher than non-typical ones. However, generally the labels of the training data only differentiate relevant of irrelevant; that is to say, typical or non-typical training samples have the same contribution to the learning process. Therefore, the learned scores of the unlabeled data cannot well measure the typicality. Accordingly, three preprocessing approaches are proposed to relax the labels of the training data to real-valued typicality scores. Then the typicality scores of the training data are propagated to unlabeled data using manifold ranking. Meanwhile, we propose to use a novel criterion, Average Typicality Precision (ATP), to replace the frequently used one, Average Precision (AP), for evaluating the performance of video typicality ranking algorithms. Though AP cares the number of relevant samples at the top of the annotation rank list, it actually does not care the typicality order of these samples, while which was taken into consideration of the evaluation strategy ATP. Experiments conducted on the TRECVID data set demonstrate that this typicality ranking scheme is more consistent with human perception than normal accuracy based ranking schemes.", "paper_title": "Typicality ranking: beyond accuracy for video semantic annotation", "paper_id": "WOS:000336995900004"}