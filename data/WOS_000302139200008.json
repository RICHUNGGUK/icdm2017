{"auto_keywords": [{"score": 0.04385127860272084, "phrase": "proposed_estimator"}, {"score": 0.011211987679562501, "phrase": "finite_entropy"}, {"score": 0.00481495049065317, "phrase": "turing's_perspective"}, {"score": 0.004732215871738028, "phrase": "new_nonparametric_estimator"}, {"score": 0.004677847043645765, "phrase": "shannon's_entropy"}, {"score": 0.004597457370802855, "phrase": "countable_alphabet"}, {"score": 0.004071957673937727, "phrase": "turing's_formula"}, {"score": 0.003978866233891427, "phrase": "distributional_characteristics"}, {"score": 0.0037336520904376687, "phrase": "size-n_sample"}, {"score": 0.0036694287631595995, "phrase": "fundamental_switch"}, {"score": 0.0035648266669315943, "phrase": "substantial_gain"}, {"score": 0.003523822585025555, "phrase": "estimation_accuracy"}, {"score": 0.003306559295382181, "phrase": "uniform_variance"}, {"score": 0.003287482882222731, "phrase": "upper_bound"}, {"score": 0.0031937337757356526, "phrase": "entire_class"}, {"score": 0.002795671770303977, "phrase": "wide_range"}, {"score": 0.0024049552650867935, "phrase": "convergence_rates"}, {"score": 0.0022434880661725493, "phrase": "finite_alphabet"}], "paper_keywords": [""], "paper_abstract": "A new nonparametric estimator of Shannon's entropy on a countable alphabet is proposed and analyzed against the well-known plug-in estimator. The proposed estimator is developed based on Turing's formula, which recovers distributional characteristics on the subset of the alphabet not covered by a size-n sample. The fundamental switch in perspective brings about substantial gain in estimation accuracy for every distribution with finite entropy. In general, a uniform variance upper bound is established for the entire class of distributions with finite entropy that decays at a rate of O(ln(n)/n) compared to O([ln(n)](2)/n) for the plug-in. In a wide range of subclasses, the variance of the proposed estimator converges at a rate of O(1/n), and this rate of convergence carries over to the convergence rates in mean squared errors in many subclasses. Specifically, for any finite alphabet, the proposed estimator has a bias decaying exponentially in n. Several new bias-adjusted estimators are also discussed.", "paper_title": "Entropy Estimation in Turing's Perspective", "paper_id": "WOS:000302139200008"}