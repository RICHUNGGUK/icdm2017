{"auto_keywords": [{"score": 0.044702849001308755, "phrase": "dyna-qpc_algorithm"}, {"score": 0.026201042156588066, "phrase": "experimental_scenario"}, {"score": 0.00481495049065317, "phrase": "dyna_architecture"}, {"score": 0.004634799977448855, "phrase": "rapid_learning_algorithm"}, {"score": 0.00459568664065473, "phrase": "dyna-qpc."}, {"score": 0.004556901872357751, "phrase": "proposed_algorithm"}, {"score": 0.004518442937844318, "phrase": "considerably_less_training_time"}, {"score": 0.0044424917428102445, "phrase": "table-based_dyna-q_algorithm"}, {"score": 0.004330941610682453, "phrase": "real-world_control_tasks"}, {"score": 0.004186534737631288, "phrase": "existing_learning_techniques"}, {"score": 0.004151191644338367, "phrase": "cmac"}, {"score": 0.003978866233891427, "phrase": "practical_experiment"}, {"score": 0.00378145978325955, "phrase": "learning_time"}, {"score": 0.0036553070575015344, "phrase": "discrete_state-space"}, {"score": 0.003578600071301459, "phrase": "robot_learning_agent"}, {"score": 0.0034886660821793576, "phrase": "policy_learning"}, {"score": 0.0033295966773779174, "phrase": "system_environment"}, {"score": 0.003287482882222731, "phrase": "prioritized_sweeping_technique"}, {"score": 0.0031777570808834213, "phrase": "previously_influential_state-action_pairs"}, {"score": 0.003071682293903262, "phrase": "planning_function"}, {"score": 0.0030071857666598193, "phrase": "background_task"}, {"score": 0.0029691377785333872, "phrase": "learning_policy"}, {"score": 0.002931569771634119, "phrase": "previous_experience"}, {"score": 0.0028822153400215973, "phrase": "approximation_model"}, {"score": 0.0028457440091791252, "phrase": "background_tasks"}, {"score": 0.0027978304423349246, "phrase": "idle_time"}, {"score": 0.0027390682208834013, "phrase": "additional_loading"}, {"score": 0.002704403349815203, "phrase": "system_processor"}, {"score": 0.0025919830205975215, "phrase": "real_and_virtual_modes"}, {"score": 0.002526780881314497, "phrase": "rapid_policy_learning"}, {"score": 0.002380934815435267, "phrase": "simulated_scenario"}, {"score": 0.002205692205167157, "phrase": "new_dyna-qpc_agent"}, {"score": 0.0021049977753042253, "phrase": "proposed_learning_agent"}], "paper_keywords": ["reinforcement learning", " Q-learning", " CMAC", " prioritized sweeping", " dyna agent"], "paper_abstract": "In this paper, we present a rapid learning algorithm called Dyna-QPC. The proposed algorithm requires considerably less training time than Q-learning and Table-based Dyna-Q algorithm, making it applicable to real-world control tasks. The Dyna-QPC algorithm is a combination of existing learning techniques: CMAC, Q-learning, and prioritized sweeping. In a practical experiment, the Dyna-QPC algorithm is implemented with the goal of minimizing the learning time required for a robot to navigate a discrete state-space containing obstacles. The robot learning agent uses Q-learning for policy learning and a CMAC-Model as an approximator of the system environment. The prioritized sweeping technique is used to manage a queue of previously influential state-action pairs used in a planning function. The planning function is implemented as a background task updating the learning policy based on previous experience stored by the approximation model. As background tasks run during CPU idle time, there is no additional loading on the system processor. The Dyna-QPC agent switches seamlessly between real and virtual modes with the objective of achieving rapid policy learning. A simulated and an experimental scenario have been designed and implemented. The simulated scenario is used to test the speed and efficiency of the three learning algorithms, while the experimental scenario evaluates the new Dyna-QPC agent. Results from both simulated and experimental scenarios demonstrate the superior performance of the proposed learning agent.", "paper_title": "A Fast Learning Agent Based on the Dyna Architecture", "paper_id": "WOS:000345497200008"}