{"auto_keywords": [{"score": 0.03306951750255661, "phrase": "audio_signal"}, {"score": 0.00481495049065317, "phrase": "moving-sounding_objects"}, {"score": 0.004628223381645729, "phrase": "novel_method"}, {"score": 0.004527619017250708, "phrase": "audio-visual_dynamics"}, {"score": 0.004294962474996283, "phrase": "dominant_source"}, {"score": 0.004146510711518748, "phrase": "two-step_spatiotemporal_segmentation_mechanism"}, {"score": 0.00395069818358908, "phrase": "visual_features"}, {"score": 0.003698442219929622, "phrase": "appearance_cues"}, {"score": 0.0036499505265403377, "phrase": "quickshift_algorithm"}, {"score": 0.003508326606628292, "phrase": "k-means"}, {"score": 0.0034018288705284427, "phrase": "spatiotemporal_video_segmentation"}, {"score": 0.003298631258267805, "phrase": "motion_features"}, {"score": 0.003255364511414329, "phrase": "individual_segments"}, {"score": 0.003212663446866797, "phrase": "mel-frequency_cepstral_coefficients"}, {"score": 0.0031845232869294826, "phrase": "mfcc"}, {"score": 0.002981026404927824, "phrase": "proposed_framework"}, {"score": 0.0029161225023394363, "phrase": "non-trivial_correlation"}, {"score": 0.0028778584739756786, "phrase": "audio_features"}, {"score": 0.002705808393126695, "phrase": "canonical_correlation_analysis"}, {"score": 0.002682119851703578, "phrase": "cca"}, {"score": 0.002600661525224505, "phrase": "moving_objects"}, {"score": 0.002434396049547248, "phrase": "object_identification"}, {"score": 0.0023090525096074264, "phrase": "audio-video_synchronization"}, {"score": 0.002238930800075706, "phrase": "interactive_segmentation"}, {"score": 0.0021049977753042253, "phrase": "significant_increase"}], "paper_keywords": ["Audio-visual analysis", " audio-visual synchronization", " canonical correlation analysis", " video segmentation"], "paper_abstract": "In this paper, we propose a novel method that exploits correlation between audio-visual dynamics of a video to segment and localize objects that are the dominant source of audio. Our approach consists of a two-step spatiotemporal segmentation mechanism that relies on velocity and acceleration of moving objects as visual features. Each frame of the video is segmented into regions based on motion and appearance cues using the QuickShift algorithm, which are then clustered over time using K-means, so as to obtain a spatiotemporal video segmentation. The video is represented by motion features computed over individual segments. The Mel-Frequency Cepstral Coefficients (MFCC) of the audio signal, and their first order derivatives are exploited to represent audio. The proposed framework assumes there is a non-trivial correlation between these audio features and the velocity and acceleration of the moving and sounding objects. The canonical correlation analysis (CCA) is utilized to identify the moving objects which are most correlated to the audio signal. In addition to moving-sounding object identification, the same framework is also exploited to solve the problem of audio-video synchronization, and is used to aid interactive segmentation. We evaluate the performance of our proposed method on challenging videos. Our experiments demonstrate significant increase in performance over the state-of-the-art both qualitatively and quantitatively, and validate the feasibility and superiority of our approach.", "paper_title": "Multimodal Analysis for Identification and Segmentation of Moving-Sounding Objects", "paper_id": "WOS:000313875500013"}