{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "batch_learning"}, {"score": 0.04967710401179305, "phrase": "logged_bandit_feedback"}, {"score": 0.04927872927155838, "phrase": "counterfactual_risk_minimization"}, {"score": 0.004635701867952543, "phrase": "learning_principle"}, {"score": 0.004577440795549049, "phrase": "efficient_algorithm"}, {"score": 0.004351595642056769, "phrase": "online_systems"}, {"score": 0.0037543106253910313, "phrase": "bandit_feedback"}, {"score": 0.0036450381529042103, "phrase": "presented_ads"}, {"score": 0.0035389348527719404, "phrase": "counterfactual_nature"}, {"score": 0.0034944094552943, "phrase": "learning_problem"}, {"score": 0.003349984456443527, "phrase": "propensity_scoring"}, {"score": 0.003131178497790961, "phrase": "propensity-weighted_empirical_risk_estimator"}, {"score": 0.00305285084207725, "phrase": "structural_risk_minimization_principle"}, {"score": 0.003027178161479457, "phrase": "wapnik"}, {"score": 0.0030017207229983385, "phrase": "tscherwonenkis"}, {"score": 0.0029266219967426224, "phrase": "constructive_bounds"}, {"score": 0.0027586198323713606, "phrase": "crm"}, {"score": 0.0026222551419772867, "phrase": "policy_optimizer"}, {"score": 0.002600194073758015, "phrase": "exponential_models"}, {"score": 0.002556625752785268, "phrase": "stochastic_linear_rules"}, {"score": 0.002535115422089855, "phrase": "structured_output_prediction"}, {"score": 0.002450864311898833, "phrase": "poem_objective"}, {"score": 0.002419995316129757, "phrase": "efficient_stochastic_gradient_optimization"}, {"score": 0.002242758902593467, "phrase": "real-world_information_retrieval_problem"}, {"score": 0.0021682027434117095, "phrase": "crm_objective"}, {"score": 0.0021228663458966813, "phrase": "improved_robustness"}, {"score": 0.0021049977753042253, "phrase": "generalization_performance"}], "paper_keywords": ["empirical risk minimization", " bandit feedback", " importance sampling", " propensity score matching", " structured prediction"], "paper_abstract": "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method-called Policy Optimizer for Exponential Models (POEM)-for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi-label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state-of-the-art.", "paper_title": "Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization", "paper_id": "WOS:000369887300003"}