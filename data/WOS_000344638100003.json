{"auto_keywords": [{"score": 0.04284631580526269, "phrase": "optimal_rates"}, {"score": 0.038173305178979114, "phrase": "regression_function"}, {"score": 0.00481495049065317, "phrase": "adaptive_minimax_regression_estimation"}, {"score": 0.004698086429418318, "phrase": "m-n_predictors"}, {"score": 0.0046407174174471644, "phrase": "random_design_regression"}, {"score": 0.004472760779811399, "phrase": "best_performance"}, {"score": 0.004154788336557036, "phrase": "linear_coe_ffi_cients"}, {"score": 0.003919109076818856, "phrase": "full_range"}, {"score": 0.003674086624699676, "phrase": "best_linear_coe_ffi_cients"}, {"score": 0.0035848128208966403, "phrase": "model_misspeci_fi_cation"}, {"score": 0.0034337536554942174, "phrase": "striking_feature"}, {"score": 0.0033917709810816093, "phrase": "speci_fi_c_relationship"}, {"score": 0.0032991709808491147, "phrase": "upper_rates"}, {"score": 0.0032388418727065144, "phrase": "basically_arbitrary_correlations"}, {"score": 0.003140727213993457, "phrase": "true_regression_function"}, {"score": 0.003017594652966029, "phrase": "sparse_representation"}, {"score": 0.0029171727082441244, "phrase": "best_extent"}, {"score": 0.0027770234187618815, "phrase": "sparse_approximation_result"}, {"score": 0.0026763361486623195, "phrase": "minimax_rate_optimal_aggregation"}, {"score": 0.0025713631281616763, "phrase": "prescribed_accuracy"}, {"score": 0.002532091374790013, "phrase": "best_linear_combination"}, {"score": 0.002417829700753522, "phrase": "ff_ers"}, {"score": 0.0023808972257735018, "phrase": "minimax_rate"}, {"score": 0.0023229723740009604, "phrase": "e_ff_ective_model_size"}, {"score": 0.002287485527155396, "phrase": "sparsity_index"}, {"score": 0.0021641529087429797, "phrase": "easily_interpretable_way"}, {"score": 0.0021376596688185605, "phrase": "classical_model_selection_theory"}, {"score": 0.0021049977753042253, "phrase": "large_number"}], "paper_keywords": [""], "paper_abstract": "Given a dictionary of M-n predictors, in a random design regression setting with n observations, we construct estimators that target the best performance among all the linear combinations of the predictors under a sparse l(q) - norm (0 <= q <= 1) constraint on the linear coe ffi cients. Besides identifying the optimal rates of convergence, our universal aggregation strategies by model mixing achieve the optimal rates simultaneously over the full range of 0 <= q <= 1 for any M n and without knowledge of the l(q)-norm of the best linear coe ffi cients to represent the regression function. To allow model misspeci fi cation, our upper bound results are obtained in a framework of aggregation of estimates. A striking feature is that no speci fi c relationship among the predictors is needed to achieve the upper rates of convergence (hence permitting basically arbitrary correlations between the predictors). Therefore, whatever the true regression function (assumed to be uniformly bounded), our estimators automatically exploit any sparse representation of the regression function (if any), to the best extent possible within the l(q)-constrained linear combinations for any 0 <= q <= 1. A sparse approximation result in the l(q)-hulls turns out to be crucial to adaptively achieve minimax rate optimal aggregation. It precisely characterizes the number of terms needed to achieve a prescribed accuracy of approximation to the best linear combination in an l(q)-hull for 0 <= q <= 1. It o ff ers the insight that the minimax rate of l(q)-aggregation is basically determined by an e ff ective model size, which is a sparsity index that depends on q, M-n, n, and the l(q)-norm bound in an easily interpretable way based on a classical model selection theory that deals with a large number of models.", "paper_title": "Adaptive Minimax Regression Estimation over Sparse l(q)-Hulls", "paper_id": "WOS:000344638100003"}