{"auto_keywords": [{"score": 0.039578375939719655, "phrase": "physical_games"}, {"score": 0.00481495049065317, "phrase": "cognitive_modeling"}, {"score": 0.004685575787741084, "phrase": "entertainment_preferences"}, {"score": 0.004437115547184535, "phrase": "subject's_desires"}, {"score": 0.004357245559948974, "phrase": "important_topic"}, {"score": 0.004317848984770719, "phrase": "machine_learning_research"}, {"score": 0.004201774694043468, "phrase": "comparative_study"}, {"score": 0.003996983218174335, "phrase": "case_study"}, {"score": 0.00385431710959668, "phrase": "expressed_entertainment_preferences"}, {"score": 0.0032725712584012953, "phrase": "gaussian_processes"}, {"score": 0.003170060683860338, "phrase": "custom-designed_approaches"}, {"score": 0.002988101395789399, "phrase": "preference_learning_techniques"}, {"score": 0.0029076695661972114, "phrase": "selection_methods"}, {"score": 0.0028422946420821075, "phrase": "effective_preference_models"}, {"score": 0.0028037753646260937, "phrase": "suitable_individual_playing_features"}, {"score": 0.0027657766592943844, "phrase": "underlying_preference_model"}, {"score": 0.0027159094186967247, "phrase": "children_preferences"}, {"score": 0.0026188490199688013, "phrase": "cross-validation_accuracy"}, {"score": 0.0025833500717948343, "phrase": "reported_entertainment"}, {"score": 0.002548331088496964, "phrase": "main_set"}, {"score": 0.002525248554951453, "phrase": "game_survey_experimentation"}, {"score": 0.002423931101340771, "phrase": "expressed_preferences"}, {"score": 0.0023586493263143553, "phrase": "previously_unseen_data"}, {"score": 0.0022537212268979507, "phrase": "second_physical_activity_control_experiment"}, {"score": 0.002133937537670314, "phrase": "sequential_forward_selection"}, {"score": 0.0021049977753042253, "phrase": "investigated_complex_case_study"}], "paper_keywords": ["Augmented-reality games", " Bayesian learning (BL)", " entertainment modeling", " large-margin classifiers", " neuroevolution", " preference learning"], "paper_abstract": "Learning from preferences, which provide means for expressing a subject's desires, constitutes an important topic in machine learning research. This paper presents a comparative study of four alternative instance preference learning algorithms (both linear and nonlinear). The case study investigated is to learn to predict the expressed entertainment preferences of children when playing physical games built on their personalized playing features (entertainment modeling). Two of the approaches are derived from the literature-the large-margin algorithm (LMA) and preference learning with Gaussian processes-while the remaining two are custom-designed approaches for the problem under investigation: meta-LMA and neuroevolution. Preference learning techniques are combined with feature set selection methods permitting the construction of effective preference models, given suitable individual playing features. The underlying preference model that best reflects children preferences is obtained through neuroevolution: 82.22% of cross-validation accuracy in predicting reported entertainment in the main set of game survey experimentation. The model is able to correctly match expressed preferences in 66.66% of cases on previously unseen data (p-value = 0.0136) of a second physical activity control experiment. Results indicate the benefit of the use of neuroevolution and sequential forward selection for the investigated complex case study of cognitive modeling in physical games.", "paper_title": "Preference Learning for Cognitive Modeling: A Case Study on Entertainment Preferences", "paper_id": "WOS:000270947600003"}