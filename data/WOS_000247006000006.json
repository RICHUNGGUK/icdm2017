{"auto_keywords": [{"score": 0.038840076428792135, "phrase": "temporal_domain"}, {"score": 0.00481495049065317, "phrase": "novel_framework"}, {"score": 0.004484908028307205, "phrase": "continuous_exploration"}, {"score": 0.004398872331144531, "phrase": "dynamic_scene"}, {"score": 0.003941478054422721, "phrase": "unsynchronized_video_inputs"}, {"score": 0.003865826699096508, "phrase": "added_freedom"}, {"score": 0.0036006068766833103, "phrase": "smooth_slow_motion"}, {"score": 0.0035543706594585076, "phrase": "temporal_integration"}, {"score": 0.0034191818674096453, "phrase": "novel_views"}, {"score": 0.003163985877966426, "phrase": "two-stage_rendering_algorithm"}, {"score": 0.0029658905122290536, "phrase": "globally_synchronized_images"}, {"score": 0.0029089095588612007, "phrase": "robust_spatial-temporal_image_registration_algorithm"}, {"score": 0.0028530201960995896, "phrase": "edge-preserving_image_morphing"}, {"score": 0.0027444334639134217, "phrase": "software-synchronized_images"}, {"score": 0.0026916956768363158, "phrase": "spatial_domain"}, {"score": 0.002622947550220266, "phrase": "final_view"}, {"score": 0.00244278788608393, "phrase": "subframe_temporal_offsets"}, {"score": 0.002411383506019635, "phrase": "input_video_sequences"}, {"score": 0.002380381895973937, "phrase": "experimental_results"}, {"score": 0.0023497779149212737, "phrase": "unsynchronized_videos"}, {"score": 0.0022897425998475362, "phrase": "time_stamps"}, {"score": 0.0021742244283295986, "phrase": "photorealistic_quality"}, {"score": 0.0021049977753042253, "phrase": "real_scenes"}], "paper_keywords": ["image-based rendering", " space-time light field", " epipolar constraint", " image morphing"], "paper_abstract": "In this paper, we propose a novel framework called space-time light field rendering, which allows continuous exploration of a dynamic scene in both space and time. Compared to existing light field capture/rendering systems, it offers the capability of using unsynchronized video inputs and the added freedom of controlling the visualization in the temporal domain, such as smooth slow motion and temporal integration. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate these software-synchronized images in the spatial domain to synthesize the final view. In addition, we introduce a very accurate and robust algorithm to estimate subframe temporal offsets among input video sequences. Experimental results from unsynchronized videos with or without time stamps show that our approach is capable of maintaining photorealistic quality from a variety of real scenes.", "paper_title": "Space-time light field rendering", "paper_id": "WOS:000247006000006"}