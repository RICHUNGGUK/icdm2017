{"auto_keywords": [{"score": 0.01423306817212826, "phrase": "finite_number"}, {"score": 0.004596960835906736, "phrase": "human_actions"}, {"score": 0.004288261454002681, "phrase": "training_phase"}, {"score": 0.004190019394361629, "phrase": "satisfactory_performance"}, {"score": 0.00415777266387528, "phrase": "arbitrary_testing_views"}, {"score": 0.0037314328720368453, "phrase": "different_views"}, {"score": 0.003674192201248236, "phrase": "proposed_features"}, {"score": 0.0036178264212372497, "phrase": "volumetric_information"}, {"score": 0.003374655868348244, "phrase": "corresponding_test_image_planes"}, {"score": 0.0033228699494959172, "phrase": "action_recognition"}, {"score": 0.003284550970221379, "phrase": "camera_viewpoint"}, {"score": 0.002993439939273828, "phrase": "simultaneous_gradient_variation"}, {"score": 0.002879872305061576, "phrase": "important_spatial_corner_points"}, {"score": 0.0027813397260607487, "phrase": "arbitrary_viewpoints"}, {"score": 0.002707033618304716, "phrase": "generalization_capability"}, {"score": 0.0026347074232350503, "phrase": "projected_features"}, {"score": 0.0025942468561537682, "phrase": "motion_history_images"}, {"score": 0.00257424957319189, "phrase": "non-motion_history_images"}, {"score": 0.0025347150832077175, "phrase": "moving_and_non-moving_parts"}, {"score": 0.0024479627964740748, "phrase": "feature_dimension"}, {"score": 0.002419708657850547, "phrase": "final_features"}, {"score": 0.002382541887825168, "phrase": "support_vector_data_description_method"}, {"score": 0.002292097733181405, "phrase": "ixmas_dataset"}, {"score": 0.002265638487492082, "phrase": "five_views"}, {"score": 0.0022136307563123256, "phrase": "new_snu_dataset"}, {"score": 0.002171202123281881, "phrase": "generalization_performance"}, {"score": 0.0021544587602892466, "phrase": "arbitrary_view_videos"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action recognition", " View invariant", " 4D space-time interest points"], "paper_abstract": "In this paper, we propose a method to recognize human actions independently of viewpoints by developing 4D space-time features which can generalize the information from a finite number of views in training phase so as to show a satisfactory performance in arbitrary testing views. This 4D space-time interest points (4D-STIPs, [x, y, z, t.]) are extracted using 3D space volumes reconstructed from images of a finite number of different views. Since the proposed features are constructed using volumetric information, the features for arbitrary 2D space viewpoint in testing can be generated by projecting the 3D space volumes and 4D-STIPs on corresponding test image planes. This enables action recognition in any camera viewpoint even after training with images from only a finite number of views. We also propose the variant of 3D space-time interest points, which take into account the simultaneous gradient variation in all 3 dimensions to focus on the motion of important spatial corner points. 3D space volumes and 4D-STIPs can be projected to arbitrary viewpoints for training each action to get generalization capability of the classifier. With these projected features, we construct motion history images and non-motion history images which encode the moving and non-moving parts of an action respectively. After reducing the feature dimension, the final features are learned by support vector data description method. In experiments, we train the models using IXMAS dataset constructed from five views and test them with a new SNU dataset made for evaluating the generalization performance for arbitrary view videos. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "View invariant action recognition using generalized 4D features", "paper_id": "WOS:000343852400006"}