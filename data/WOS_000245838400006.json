{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "distributed_video_coding"}, {"score": 0.004773040022691547, "phrase": "static_scenes"}, {"score": 0.004608997772926119, "phrase": "side_information_extraction"}, {"score": 0.004568871736067739, "phrase": "distributed_coding"}, {"score": 0.004297560964892607, "phrase": "targeted_applications"}, {"score": 0.004095692768056685, "phrase": "hazardous_environments"}, {"score": 0.003852371777780562, "phrase": "structure-from-motion_paradigm"}, {"score": 0.0037198532511087566, "phrase": "video_content"}, {"score": 0.003623453648863547, "phrase": "scene_geometry"}, {"score": 0.003545024951402821, "phrase": "block_matching"}, {"score": 0.0035141291835319682, "phrase": "epipolar_lines"}, {"score": 0.0032908057622663732, "phrase": "robust_algorithm"}, {"score": 0.0032621183241997777, "phrase": "sub-pel_matching"}, {"score": 0.0031498398446368025, "phrase": "semi-dense_correspondences"}, {"score": 0.002962543869356265, "phrase": "side_information"}, {"score": 0.0028480459059007468, "phrase": "linear_motion"}, {"score": 0.002823207272381655, "phrase": "key_frames"}, {"score": 0.0026436755508085223, "phrase": "camera_parameters"}, {"score": 0.0026091594819333654, "phrase": "wz_frames"}, {"score": 0.0025750928944698673, "phrase": "first_technique"}, {"score": 0.0023798115577749225, "phrase": "significant_rd_performance_gains"}, {"score": 0.0023487324083283205, "phrase": "second_technique"}, {"score": 0.0023079225317714815, "phrase": "rd_performances"}, {"score": 0.0022678201262721323, "phrase": "limited_tracking"}, {"score": 0.0021328586481696157, "phrase": "key_frame_frequency"}, {"score": 0.0021049977753042253, "phrase": "video_motion"}], "paper_keywords": ["distributed source coding (DSC)", " distributed video coding (DVC)", " image-based rendering (IBR)", " motion-adaptive key frames", " point tracking", " structure-from-motion (SfM)"], "paper_abstract": "This paper addresses the problem of side information extraction for distributed coding of videos captured by a camera moving in a 3-D static environment. Examples of targeted applications are augmented reality, remote-controlled robots operating in hazardous environments, or remote exploration by drones. It explores the benefits of the structure-from-motion paradigm for distributed coding of this type of video content. Two interpolation methods constrained by the scene geometry, based either on block matching along epipolar lines or on 3-D mesh fitting, are first developed. These techniques are based on a robust algorithm for sub-pel matching of feature points, which leads to semi-dense correspondences between key frames. However, their rate-distortion (RD) performances are limited by misalignments between the side information and the actual Wyner-Ziv (WZ) frames due to the assumption of linear motion between key frames. To cope with this problem, two feature point tracking techniques are introduced, which recover the camera parameters of the WZ frames. A first technique, in which the frames remain encoded separately, performs tracking at the decoder and leads to significant RD performance gains. A second technique further improves the RD performances by allowing a limited tracking at the encoder. As an additional benefit, statistics on tracks allow the encoder to adapt the key frame frequency to the video motion content.", "paper_title": "3-D model-based frame interpolation for distributed video coding of static scenes", "paper_id": "WOS:000245838400006"}