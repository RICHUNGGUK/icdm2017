{"auto_keywords": [{"score": 0.02786845340522553, "phrase": "hypre"}, {"score": 0.004719664411221952, "phrase": "modern_computer_architectures"}, {"score": 0.004565007074039171, "phrase": "increasing_numbers"}, {"score": 0.0045346863560033175, "phrase": "lower_power_cores"}, {"score": 0.004474645041899596, "phrase": "decreasing_memory_to_core_ratio"}, {"score": 0.004400705158735729, "phrase": "strong_evolutionary_pressure"}, {"score": 0.004144452614362445, "phrase": "data_movement"}, {"score": 0.003981928669805484, "phrase": "domain_decomposition_methods"}, {"score": 0.003663443192307343, "phrase": "intra-node_parallelisation"}, {"score": 0.0035669891404642015, "phrase": "increased_algorithmic_freedom"}, {"score": 0.003531474887552889, "phrase": "memory_requirements"}, {"score": 0.0034730657554172405, "phrase": "reduced_number"}, {"score": 0.0032706520949775065, "phrase": "hybrid_openmp_mpi_version"}, {"score": 0.0032380786944838204, "phrase": "unstructured_finite_element_code_fluidity"}, {"score": 0.0032058286587989234, "phrase": "matrix_assembly_kernels"}, {"score": 0.003173898798475791, "phrase": "openmp_parallel_algorithm"}, {"score": 0.0031109870010753663, "phrase": "independent_sets"}, {"score": 0.0030088845918380468, "phrase": "race_conditions"}, {"score": 0.0029296136066184857, "phrase": "mpi_overheads"}, {"score": 0.002900426826591769, "phrase": "mpi_process"}, {"score": 0.002842920225545139, "phrase": "global_matrix"}, {"score": 0.002795867549039675, "phrase": "openmp_threaded_fork"}, {"score": 0.0027404283412788997, "phrase": "resulting_sparse_linear_systems"}, {"score": 0.0025892330566358503, "phrase": "algebraic_multigrid_preconditioner_boomeramg"}, {"score": 0.0025125868638809284, "phrase": "unstructured_finite_element_codes"}, {"score": 0.002430076037416881, "phrase": "particular_attention"}, {"score": 0.002397833600728684, "phrase": "ccnuma_architectures"}, {"score": 0.002381872778203302, "phrase": "data_locality"}, {"score": 0.0023346234464901978, "phrase": "good_intra-node_scaling_characteristics"}, {"score": 0.002213147136067999, "phrase": "mixed-mode_application"}, {"score": 0.00216923754570397, "phrase": "better_parallel_performance"}, {"score": 0.0021476098263617954, "phrase": "pure_mpi_version"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Fluidity-ICOM", " OpenMP", " MPI", " FEM", " Matrix assembly", " Sparse linear solver", " HYPRE", " PETSc", " SpMV"], "paper_abstract": "The trend of all modern computer architectures, and the path to exascale, is towards increasing numbers of lower power cores, with a decreasing memory to core ratio. This imposes a strong evolutionary pressure on algorithms and software to efficiently utilise all levels of parallelism available on a given platform while minimising data movement. Unstructured finite elements codes have long been effectively parallelised using domain decomposition methods, implemented using libraries such as the Message Passing Interface (MPI). However, there are many optimisation opportunities when threading is used for intra-node parallelisation for the latest multi-core/many-core platforms. The benefits include increased algorithmic freedom, reduced memory requirements, cache sharing, reduced number of partitions, less MPI communication and I/O overhead. In this paper, we report progress in implementing a hybrid OpenMP MPI version of the unstructured finite element code Fluidity. For matrix assembly kernels, the OpenMP parallel algorithm uses graph colouring to identify independent sets of elements that can be assembled concurrently with no race conditions. In this phase there are no MPI overheads as each MPI process only assembles its own local part of the global matrix. We use an OpenMP threaded fork of PETSc to solve the resulting sparse linear systems of equations. We experiment with a range of preconditioners, including HYPRE which provides the algebraic multigrid preconditioner BoomerAMG where the smoother is also threaded. Since unstructured finite element codes are well known to be memory latency bound, particular attention is paid to ccNUMA architectures where data locality is particularly important to achieve good intra-node scaling characteristics. We also demonstrate that utilising non-blocking algorithms and libraries are critical to mixed-mode application so that it can achieve better parallel performance than the pure MPI version. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Developing a scalable hybrid MPI/OpenMP unstructured finite element model", "paper_id": "WOS:000350535100024"}