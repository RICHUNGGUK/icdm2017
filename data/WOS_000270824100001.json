{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "training_deep_neural_networks"}, {"score": 0.004778088905211055, "phrase": "deep_multi-layer_neural_networks"}, {"score": 0.004545240514015278, "phrase": "highly_non-linear"}, {"score": 0.00451043437607004, "phrase": "highly-varying_functions"}, {"score": 0.0041446453895708, "phrase": "random_initialization"}, {"score": 0.004034572092608487, "phrase": "poor_solutions"}, {"score": 0.004003660777805554, "phrase": "hinton_et_al"}, {"score": 0.0039274105882136775, "phrase": "greedy_layer-wise_unsupervised_learning_procedure"}, {"score": 0.0038230844545071303, "phrase": "restricted_boltzmann_machines"}, {"score": 0.0036788184374592706, "phrase": "deep_belief_network"}, {"score": 0.0035263833746562788, "phrase": "hidden_causal_variables"}, {"score": 0.0033932753847910366, "phrase": "greedy_layer-wise_procedure"}, {"score": 0.0033030907500120397, "phrase": "autoassociator_networks"}, {"score": 0.003000070450638434, "phrase": "greedy_layer-wise_unsupervised_training_strategy"}, {"score": 0.002864628676257225, "phrase": "good_local_minimum"}, {"score": 0.0027247727563746694, "phrase": "better_generalization"}, {"score": 0.0026934777251033776, "phrase": "internal_distributed_representations"}, {"score": 0.002662541169311738, "phrase": "high-level_abstractions"}, {"score": 0.0024556852466581527, "phrase": "deep_neural_networks"}, {"score": 0.0024368411484675823, "phrase": "practical_aspects"}, {"score": 0.0022474806085983536, "phrase": "simple_variants"}, {"score": 0.002221655243524476, "phrase": "training_algorithms"}, {"score": 0.0021625417261202603, "phrase": "different_rbm_input_unit_distributions"}, {"score": 0.0021049977753042253, "phrase": "gradient_estimators"}], "paper_keywords": ["artificial neural networks", " deep belief networks", " restricted Boltzmann machines", " autoassociators", " unsupervised learning"], "paper_abstract": "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.", "paper_title": "Exploring Strategies for Training Deep Neural Networks", "paper_id": "WOS:000270824100001"}