{"auto_keywords": [{"score": 0.04920822493417558, "phrase": "hinge_loss"}, {"score": 0.040537135041717604, "phrase": "sample_average"}, {"score": 0.00481495049065317, "phrase": "reject_option"}, {"score": 0.004529093447865156, "phrase": "binary_classification"}, {"score": 0.004312622975366676, "phrase": "particular_cost"}, {"score": 0.003982617095636925, "phrase": "conventional_classification_problem"}, {"score": 0.0037231102027070724, "phrase": "difficult_optimization_problem"}, {"score": 0.003293688299319582, "phrase": "support_vector_machines"}, {"score": 0.0030601162942933665, "phrase": "surrogate_loss"}, {"score": 0.002791239714439304, "phrase": "expected_surrogate_loss"}, {"score": 0.0023652572260178637, "phrase": "fast_rates"}, {"score": 0.002293806844411685, "phrase": "conditional_probability"}], "paper_keywords": ["Bayes classifiers", " classification", " convex surrogate loss", " empirical risk minimization", " hinge loss", " large margin classifiers", " margin condition", " reject option", " support vector machines"], "paper_abstract": "We consider the problem of binary classification where the classifier can, for a particular cost, choose not to classify an observation. Just as in the conventional classification problem, minimization of the sample average of the cost is a difficult optimization problem. As an alternative, we propose the optimization of a certain convex loss function phi, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efficiently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss-the phi-risk-also minimizes the risk. We also study the rate at which the phi-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1 vertical bar X) is unlikely to be close to certain critical values.", "paper_title": "Classification with a Reject Option using a Hinge Loss", "paper_id": "WOS:000262636800007"}