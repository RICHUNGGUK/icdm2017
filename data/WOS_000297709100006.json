{"auto_keywords": [{"score": 0.0493450354116846, "phrase": "hierarchical_q-learning"}, {"score": 0.04032376502196102, "phrase": "mdp"}, {"score": 0.00481495049065317, "phrase": "hybrid_mdp"}, {"score": 0.004645057834614705, "phrase": "widely_used_reinforcement_learning_method"}, {"score": 0.004322967486451877, "phrase": "computational_complexity"}, {"score": 0.0041703614123283165, "phrase": "state-action_space"}, {"score": 0.004002513373645485, "phrase": "integrated_hierarchical_q-learning_framework"}, {"score": 0.0038810583785936505, "phrase": "hybrid_markov_decision_process"}, {"score": 0.003782655283668953, "phrase": "temporal_abstraction"}, {"score": 0.0037057255600764475, "phrase": "simple_mdp."}, {"score": 0.0036678472439508484, "phrase": "learning_process"}, {"score": 0.003574830654211291, "phrase": "multiple_levels"}, {"score": 0.0030017207229983385, "phrase": "hierarchical_control_architecture"}, {"score": 0.0027223626424366207, "phrase": "proposed_hierarchical_q-learning"}, {"score": 0.002559444836360178, "phrase": "upper_level_learning_process"}, {"score": 0.002468938912850425, "phrase": "effective_integral_learning_and_control_scheme"}, {"score": 0.0023331227492950422, "phrase": "puzzle_problem"}, {"score": 0.0022973933270056743, "phrase": "gridworld_environment"}, {"score": 0.002262209824105154, "phrase": "navigation_control_problem"}, {"score": 0.0022275639370573264, "phrase": "mobile_robot"}, {"score": 0.002193447487477984, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["reinforcement learning", " hierarchical Q-learning", " hybrid MDP", " temporal abstraction"], "paper_abstract": "As a widely used reinforcement learning method, Q-learning is bedeviled by the curse of dimensionality: The computational complexity grows dramatically with the size of state-action space. To combat this difficulty, an integrated hierarchical Q-learning framework is proposed based on the hybrid Markov decision process (MDP) using temporal abstraction instead of the simple MDP. The learning process is naturally organized into multiple levels of learning, e.g., quantitative (lower) level and qualitative (upper) level, which are modeled as MDP and semi-MDP (SMDP), respectively. This hierarchical control architecture constitutes a hybrid MDP as the model of hierarchical Q-learning, which bridges the two levels of learning. The proposed hierarchical Q-learning can scale up very well and speed up learning with the upper level learning process. Hence this approach is an effective integral learning and control scheme for complex problems. Several experiments are carried out using a puzzle problem in a gridworld environment and a navigation control problem for a mobile robot. The experimental results demonstrate the effectiveness and efficiency of the proposed approach.", "paper_title": "Hybrid MDP based integrated hierarchical Q-learning", "paper_id": "WOS:000297709100006"}