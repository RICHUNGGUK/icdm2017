{"auto_keywords": [{"score": 0.0486868426803837, "phrase": "ser"}, {"score": 0.00481495049065317, "phrase": "amplitude_modulation_parameters"}, {"score": 0.004754447398742591, "phrase": "combined_feature_selection_procedure"}, {"score": 0.0047145329604366395, "phrase": "speech_emotion_recognition"}, {"score": 0.004596779715291598, "phrase": "challenging_framework"}, {"score": 0.004539005582217309, "phrase": "human_machine_interaction_systems"}, {"score": 0.004406994912939125, "phrase": "categorical_model"}, {"score": 0.004333283735537634, "phrase": "low_performance"}, {"score": 0.004154332348800334, "phrase": "distinct_and_independent_affective_states"}, {"score": 0.004067635935640407, "phrase": "recently_investigated_assumption"}, {"score": 0.004016485454498076, "phrase": "dimensional_circumplex_model"}, {"score": 0.003949280009382125, "phrase": "ser_systems"}, {"score": 0.0037543106253910313, "phrase": "continuous_scale"}, {"score": 0.0037070855410818986, "phrase": "two-dimensional_domain"}, {"score": 0.0035240305920962796, "phrase": "pls_regression_model"}, {"score": 0.0034504423179340738, "phrase": "specific_features"}, {"score": 0.0034359094594153304, "phrase": "selection_procedures"}, {"score": 0.0033641551050434663, "phrase": "italian_speech_corpus"}, {"score": 0.003349984456443527, "phrase": "emovo"}, {"score": 0.003117986133774198, "phrase": "new_speech_features"}, {"score": 0.0030657684627749364, "phrase": "speech_amplitude_modulation"}, {"score": 0.0030017207229983385, "phrase": "slowly-varying_articulatory_motion"}, {"score": 0.0029020077365708966, "phrase": "pitch_contour"}, {"score": 0.0028174721236762317, "phrase": "regression_model"}, {"score": 0.0027819985260847577, "phrase": "average_value"}, {"score": 0.002482126086922683, "phrase": "female_model"}, {"score": 0.0023395610923736595, "phrase": "minimum_value"}, {"score": 0.002242758902593467, "phrase": "male_model"}, {"score": 0.0022051664509486206, "phrase": "seven_primary_emotions"}, {"score": 0.0021682027434117095, "phrase": "neutral_state"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speech emotion recognition (SER)", " Circumplex model of emotions", " Partial least square (PLS) regression", " Pearson correlation coefficient", " Pitch contour characterization", " Audio signal modulation"], "paper_abstract": "Speech emotion recognition (SER) is a challenging framework in demanding human machine interaction systems. Standard approaches based on the categorical model of emotions reach low performance, probably due to the modelization of emotions as distinct and independent affective states. Starting from the recently investigated assumption on the dimensional circumplex model of emotions, SER systems are structured as the prediction of valence and arousal on a continuous scale in a two-dimensional domain. In this study, we propose the use of a PLS regression model, optimized according to specific features selection procedures and trained on the Italian speech corpus EMOVO, suggesting a way to automatically label the corpus in terms of arousal and valence. New speech features related to the speech amplitude modulation, caused by the slowly-varying articulatory motion, and standard features extracted from the pitch contour, have been included in the regression model. An average value for the coefficient of determination R-2 of 0.72 (maximum value of 0.95 for fear and minimum of 0.60 for sadness) is obtained for the female model and a value for R-2 of 0.81 (maximum value of 0.89 for anger and minimum value of 0.71 for joy) is obtained for the male model, over the seven primary emotions (including the neutral state). (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Speech emotion recognition using amplitude modulation parameters and a combined feature selection procedure", "paper_id": "WOS:000336696800007"}