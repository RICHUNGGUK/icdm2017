{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "pad_model"}, {"score": 0.010183011258847685, "phrase": "pad_descriptors"}, {"score": 0.004736563452994885, "phrase": "expressive_talking_avatar"}, {"score": 0.004558564684763264, "phrase": "expressive_head"}, {"score": 0.004508942069742167, "phrase": "facial_gestures"}, {"score": 0.004041359131062721, "phrase": "semantic_expressivity"}, {"score": 0.00380511089035825, "phrase": "text_semantics"}, {"score": 0.0037636590459094762, "phrase": "visual_motion"}, {"score": 0.003466780226380516, "phrase": "correlation_analysis"}, {"score": 0.003429001377800255, "phrase": "pad_annotations"}, {"score": 0.0033916328188225843, "phrase": "motion_patterns"}, {"score": 0.0032819448594643853, "phrase": "facial_motion"}, {"score": 0.0031411751440831165, "phrase": "explicit_mapping"}, {"score": 0.0030730639262332698, "phrase": "facial_animation_parameters"}, {"score": 0.0030395624703152212, "phrase": "linear_regression"}, {"score": 0.0030064251340251196, "phrase": "neural_networks"}, {"score": 0.0029736479837154843, "phrase": "head_motion"}, {"score": 0.0029412271277166873, "phrase": "facial_expression"}, {"score": 0.002877438939246423, "phrase": "pad-driven_talking_avatar"}, {"score": 0.0027843339377323878, "phrase": "visual-speech_system"}, {"score": 0.002709045632654716, "phrase": "expressive_head_motions"}, {"score": 0.002664850766529523, "phrase": "prosodic_word_level"}, {"score": 0.002522662955545598, "phrase": "lexical_appraisal"}, {"score": 0.0024276590297034064, "phrase": "sentence_level"}, {"score": 0.002349073225500705, "phrase": "emotional_information"}, {"score": 0.002285527382381719, "phrase": "pad_reverse_evaluation"}, {"score": 0.002260591903283384, "phrase": "comparative_perceptual_experiments"}, {"score": 0.00221153232497724, "phrase": "head_and_facial_gestures"}, {"score": 0.0021049977753042253, "phrase": "visual_expressivity"}], "paper_keywords": ["Text-to-visual-speech", " Head motion", " Facial expression", " Talking avatar"], "paper_abstract": "This paper proposes to synthesize expressive head and facial gestures on talking avatar using the three dimensional pleasure-displeasure, arousal-nonarousal and dominance-submissiveness (PAD) descriptors of semantic expressivity. The PAD model is adopted to bridge the gap between text semantics and visual motion features with three dimensions of pleasure-displeasure, arousal-nonarousal, and dominance-submissiveness. Based on the correlation analysis between PAD annotations and motion patterns derived from the head and facial motion database, we propose to build an explicit mapping from PAD descriptors to facial animation parameters with linear regression and neural networks for head motion and facial expression respectively. A PAD-driven talking avatar in text-to-visual-speech system is implemented by generating expressive head motions at the prosodic word level based on the (P, A) descriptors of lexical appraisal, and facial expressions at the sentence level according to the PAD descriptors of emotional information. A series of PAD reverse evaluation and comparative perceptual experiments shows that the head and facial gestures synthesized based on PAD model can significantly enhance the visual expressivity of talking avatar.", "paper_title": "Head and facial gestures synthesis using PAD model for an expressive talking avatar", "paper_id": "WOS:000342418700022"}