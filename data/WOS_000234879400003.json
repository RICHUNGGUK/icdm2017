{"auto_keywords": [{"score": 0.03048205616961731, "phrase": "batch_mode"}, {"score": 0.008798395285644167, "phrase": "cga"}, {"score": 0.00481495049065317, "phrase": "perceptron_training"}, {"score": 0.004703671193348433, "phrase": "adaptive_algorithm"}, {"score": 0.004488734023485018, "phrase": "conjugate_gradients"}, {"score": 0.004283576270012874, "phrase": "linear_discriminant_functions"}, {"score": 0.004217286731331736, "phrase": "pattern_classification"}, {"score": 0.0038404642251010797, "phrase": "consistent_and_inconsistent_cases"}, {"score": 0.0037516215763875225, "phrase": "finite_number"}, {"score": 0.0033372020049592726, "phrase": "adaptive_versions"}, {"score": 0.0032599633778910516, "phrase": "ho-kashyap_procedure"}, {"score": 0.0032094634756436595, "phrase": "ahk"}, {"score": 0.003015143226706584, "phrase": "batch_version"}, {"score": 0.002854761308379149, "phrase": "ahk."}, {"score": 0.0027240799841720957, "phrase": "proposed_adaptive_conjugate_gradient_algorithm"}, {"score": 0.002619748321217153, "phrase": "vastly_superior_performance"}, {"score": 0.002499798127196099, "phrase": "training_cycles"}, {"score": 0.002422890839801611, "phrase": "classification_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["perceptron", " conjugate-gradient", " linear separability", " linear inequalities", " training"], "paper_abstract": "An adaptive algorithm for function minimization based on conjugate gradients for the problem of finding linear discriminant functions in pattern classification is developed. The algorithm converges to a solution in both consistent and inconsistent cases in a finite number of steps on several datasets. We have applied our algorithm and compared its performance with the adaptive versions of the Ho-Kashyap procedure (AHK). We have also compared the batch version of the algorithm with the batch mode AHK. The results show that the proposed adaptive conjugate gradient algorithm (CGA) gives vastly superior performance in terms of both the number of training cycles required and the classification rate. Also, the batch mode CGA performs much better than the batch mode AHK. (c) 2005 Elsevier B.V. All rights reserved.", "paper_title": "Adaptive conjugate gradient algorithm for perceptron training", "paper_id": "WOS:000234879400003"}