{"auto_keywords": [{"score": 0.03192280240290009, "phrase": "pixel_prototypes"}, {"score": 0.004823530554614883, "phrase": "matrix"}, {"score": 0.004692980391580608, "phrase": "unsupervised_human_action_categorization"}, {"score": 0.004645057834614705, "phrase": "human_action"}, {"score": 0.004550669123303854, "phrase": "basic_unit"}, {"score": 0.004367581653370531, "phrase": "low-level_visual_features"}, {"score": 0.004322967486451877, "phrase": "high-level_semantics"}, {"score": 0.004278807087784626, "phrase": "human_action_recognition"}, {"score": 0.00421340730888561, "phrase": "great_significance"}, {"score": 0.004106612344123407, "phrase": "human-computer_interaction"}, {"score": 0.004064653055162238, "phrase": "intelligent_video_surveillance"}, {"score": 0.004023120753104693, "phrase": "video_retrieval"}, {"score": 0.003802135020791801, "phrase": "novel_unsupervised_approach"}, {"score": 0.0037632749713937637, "phrase": "mining_categories"}, {"score": 0.003724810609448597, "phrase": "action_video_sequences"}, {"score": 0.0035382855372646164, "phrase": "video_data_structurization"}, {"score": 0.0034485432592912917, "phrase": "unsupervised_categorization"}, {"score": 0.0033957904376127187, "phrase": "action_representation"}, {"score": 0.003343841875107018, "phrase": "novel_view"}, {"score": 0.0033096502887394233, "phrase": "video_decomposition"}, {"score": 0.003176333519010176, "phrase": "spatially_distributed_dynamic_pixel_time_series"}, {"score": 0.002940626369102927, "phrase": "pixel_time_series"}, {"score": 0.0028659990472580154, "phrase": "video_sequences"}, {"score": 0.0028076592725295646, "phrase": "two-dimensional_action_matrices"}, {"score": 0.0027505037762786087, "phrase": "learning_model"}, {"score": 0.0026126405272417783, "phrase": "multi-action_tensor"}, {"score": 0.002546315584783112, "phrase": "joint_matrix_factorization_method"}, {"score": 0.002456272788591051, "phrase": "pixel_signatures"}, {"score": 0.002393907896783117, "phrase": "action_classes"}, {"score": 0.0021709933368474223, "phrase": "public_and_popular_weizmann"}, {"score": 0.0021377414429251647, "phrase": "kth_datasets"}, {"score": 0.0021049977753042253, "phrase": "promising_results"}], "paper_keywords": ["Action categorization", " joint matrix factorization", " tensor representation", " video analysis"], "paper_abstract": "Human action, as the basic unit of most human-relevant video content, bridges the gap between low-level visual features and high-level semantics. Human action recognition is of great significance in the applications of human-computer interaction, intelligent video surveillance, video retrieval and search. In this paper, we propose a novel unsupervised approach to mining categories from action video sequences, which consists of two modules: action representation for video data structurization and learning model for unsupervised categorization. In action representation, a novel view of video decomposition is presented. Videos are regarded as spatially distributed dynamic pixel time series, and these dynamic pixels are first quantized into pixel prototypes. After replacing the pixel time series with their corresponding prototype labels, the video sequences are compressed into two-dimensional action matrices. In the learning model, we put these matrices together to form an multi-action tensor, and propose the joint matrix factorization method to simultaneously cluster the pixel prototypes into pixel signatures, and matrices into action classes with the consideration of the duality between pixel clustering and action clustering. The approach is tested on public and popular Weizmann, and KTH datasets, and promising results are achieved.", "paper_title": "A Matrix-Based Approach to Unsupervised Human Action Categorization", "paper_id": "WOS:000302701100010"}