{"auto_keywords": [{"score": 0.04037188415354747, "phrase": "fair_sharing"}, {"score": 0.00481495049065317, "phrase": "disengaged_scheduling"}, {"score": 0.00467424463996457, "phrase": "fast_computational_accelerators"}, {"score": 0.004634799977448855, "phrase": "today's_operating_systems"}, {"score": 0.004461359547152617, "phrase": "simple_devices"}, {"score": 0.004404994123834589, "phrase": "bounded_and_predictable_response_times"}, {"score": 0.004294381488413906, "phrase": "increasing_share"}, {"score": 0.004204320052179726, "phrase": "modern_machines"}, {"score": 0.0039119492240482, "phrase": "operating_system"}, {"score": 0.0036864465771943933, "phrase": "accelerator_scheduling"}, {"score": 0.0036553070575015344, "phrase": "resource_management"}, {"score": 0.003593812067062062, "phrase": "safe_scheduling"}, {"score": 0.003548368289666932, "phrase": "particular_challenge"}, {"score": 0.0035183910023550246, "phrase": "fast_accelerators"}, {"score": 0.0032184697893776052, "phrase": "disengaged_scheduling_strategy"}, {"score": 0.0030457198309616694, "phrase": "infrequent_basis"}, {"score": 0.0029691377785333872, "phrase": "accelerator_cycles"}, {"score": 0.002833689453299307, "phrase": "next_time_interval"}, {"score": 0.002603012037901889, "phrase": "latest_nvidia_gpus"}, {"score": 0.0024947959375982614, "phrase": "overuse_control"}, {"score": 0.0024423826953133844, "phrase": "disengaged_fair_queueing"}, {"score": 0.002380934815435267, "phrase": "resource_idleness"}, {"score": 0.0022724316174866526, "phrase": "gpu"}, {"score": 0.0022339789476489877, "phrase": "uncooperative_or_adversarial_applications"}, {"score": 0.002215081186105616, "phrase": "disengaged_fair_queueing_incurs_a"}, {"score": 0.0021229583685841405, "phrase": "direct_device_access"}], "paper_keywords": ["Operating system protection", " scheduling", " fairness", " hardware accelerators", " GPUs"], "paper_abstract": "Today's operating systems treat GPUs and other computational accelerators as if they were simple devices, with bounded and predictable response times. With accelerators assuming an increasing share of the workload on modern machines, this strategy is already problematic, and likely to become untenable soon. If the operating system is to enforce fair sharing of the machine, it must assume responsibility for accelerator scheduling and resource management. Fair, safe scheduling is a particular challenge on fast accelerators, which allow applications to avoid kernel-crossing overhead by interacting directly with the device. We propose a disengaged scheduling strategy in which the kernel intercedes between applications and the accelerator on an infrequent basis, to monitor their use of accelerator cycles and to determine which applications should be granted access over the next time interval. Our strategy assumes a well defined, narrow interface exported by the accelerator. We build upon such an interface, systematically inferred for the latest Nvidia GPUs. We construct several example schedulers, including Disengaged Timeslice with overuse control that guarantees fairness and Disengaged Fair Queueing that is effective in limiting resource idleness, but probabilistic. Both schedulers ensure fair sharing of the GPU, even among uncooperative or adversarial applications; Disengaged Fair Queueing incurs a 4% overhead on average (max 18%) compared to direct device access across our evaluation scenarios.", "paper_title": "Disengaged Scheduling for Fair, Protected Access to Fast Computational Accelerators", "paper_id": "WOS:000360535000021"}