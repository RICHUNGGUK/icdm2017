{"auto_keywords": [{"score": 0.03045717985280817, "phrase": "mapreduce_programs"}, {"score": 0.011930478877693659, "phrase": "mapreduce"}, {"score": 0.00481495049065317, "phrase": "hpc_analytics_applications"}, {"score": 0.004786519708868553, "phrase": "access_patterns"}, {"score": 0.004758255999331728, "phrase": "data_restructuring"}, {"score": 0.0043537008514226445, "phrase": "data-intensive_computing"}, {"score": 0.004327981760540394, "phrase": "computational-intensive_hpc_facilities"}, {"score": 0.004276996526659816, "phrase": "data_analytics"}, {"score": 0.004127601533040351, "phrase": "hpc_storage_system"}, {"score": 0.004091072003277219, "phrase": "data-intensive_one"}, {"score": 0.003971616493886099, "phrase": "data_semantics"}, {"score": 0.003948145581074458, "phrase": "hpc_storage"}, {"score": 0.0039248128268720645, "phrase": "data-intensive_system"}, {"score": 0.0036770435321756903, "phrase": "existing_data-intensive_tools"}, {"score": 0.003334309053314909, "phrase": "multiple_mapreduce_phases"}, {"score": 0.003314591878914152, "phrase": "significant_overhead"}, {"score": 0.0031703706626348507, "phrase": "mapreduce_phase"}, {"score": 0.0031422859551159506, "phrase": "distributed_read_and_write_operation"}, {"score": 0.0030144682722892833, "phrase": "mapreduce-based_framework"}, {"score": 0.0029966577204556593, "phrase": "hpc"}, {"score": 0.002952516738708181, "phrase": "multiple_scans"}, {"score": 0.002824005136794786, "phrase": "data-centric_scheduler"}, {"score": 0.0027659570272100835, "phrase": "hpc_analytics"}, {"score": 0.0027252240573281163, "phrase": "data_locality"}, {"score": 0.002685089327921282, "phrase": "additional_expressiveness"}, {"score": 0.0026612920049803473, "phrase": "mapreduce_language"}, {"score": 0.002637705034367748, "phrase": "application_scientists"}, {"score": 0.00260657976055413, "phrase": "logical_semantics"}, {"score": 0.0025004981545243992, "phrase": "multiple_data"}, {"score": 0.0023633857274905977, "phrase": "data-intensive_file_system"}, {"score": 0.002328567395792052, "phrase": "mapreduce_with_access_patterns"}, {"score": 0.0021049977753042253, "phrase": "prevalent_hpc_analysis_application"}], "paper_keywords": ["HPC analytics framework", " data-intensive systems", " MapReduce", " scheduling"], "paper_abstract": "Current High Performance Computing (HPC) applications have seen an explosive growth in the size of data in recent years. Many application scientists have initiated efforts to integrate data-intensive computing into computational-intensive HPC facilities, particularly for data analytics. We have observed several scientific applications which must migrate their data from an HPC storage system to a data-intensive one for analytics. There is a gap between the data semantics of HPC storage and data-intensive system, hence, once migrated, the data must be further refined and reorganized. This reorganization must be performed before existing data-intensive tools such as MapReduce can be used to analyze data. This reorganization requires at least two complete scans through the data set and then at least one MapReduce program to prepare the data before analyzing it. Running multiple MapReduce phases causes significant overhead for the application, in the form of excessive I/O operations. That is for every MapReduce phase, a distributed read and write operation on the file system must be performed. Our contribution is to develop a MapReduce-based framework for HPC analytics to eliminate the multiple scans and also reduce the number of data preprocessing MapReduce programs. We also implement a data-centric scheduler to further improve the performance of HPC analytics MapReduce programs by maintaining the data locality. We have added additional expressiveness to the MapReduce language to allow application scientists to specify the logical semantics of their data such that 1) the data can be analyzed without running multiple data preprocessing MapReduce programs, and 2) the data can be simultaneously reorganized as it is migrated to the data-intensive file system. Using our augmented Map-Reduce system, MapReduce with Access Patterns (MRAP), we have demonstrated up to 33 percent throughput improvement in one real application, and up to 70 percent in an I/O kernel of another application. Our results for scheduling show up to 49 percent improvement for an I/O kernel of a prevalent HPC analysis application.", "paper_title": "Supporting HPC Analytics Applications with Access Patterns Using Data Restructuring and Data-Centric Scheduling Techniques in MapReduce", "paper_id": "WOS:000312837400014"}