{"auto_keywords": [{"score": 0.04966877349162058, "phrase": "error_probability"}, {"score": 0.02322730111336099, "phrase": "tight_upper_bound"}, {"score": 0.010432328426602518, "phrase": "binary_gaussian_classification_problem"}, {"score": 0.0075541927006702895, "phrase": "proposed_technique"}, {"score": 0.0045464538515249085, "phrase": "different_class_covariance_matrices"}, {"score": 0.004379977556546879, "phrase": "closed-form_expression"}, {"score": 0.0038381762329455556, "phrase": "highly_dimensional_situations"}, {"score": 0.0037617986040878342, "phrase": "high_degree"}, {"score": 0.003644824523316739, "phrase": "new_technique"}, {"score": 0.0035213462189102713, "phrase": "well-known_binary_gaussian_classification_problem"}, {"score": 0.0034512514926836667, "phrase": "basic_idea"}, {"score": 0.003372844338742438, "phrase": "optimal_bayes_decision_boundary"}, {"score": 0.0033056958344288524, "phrase": "easy-to-calculate_upper_bound"}, {"score": 0.0032028587883593702, "phrase": "decision_boundaries"}, {"score": 0.0031481164109237636, "phrase": "elliptic_cylinders"}, {"score": 0.003094306770612843, "phrase": "new_decision_boundaries"}, {"score": 0.0029130926778916143, "phrase": "upper_bound"}, {"score": 0.0028550709503536494, "phrase": "often_used_bounds"}, {"score": 0.0028224468763183653, "phrase": "chernoff"}, {"score": 0.002734590059580852, "phrase": "computation_time"}, {"score": 0.0026342827320931937, "phrase": "monte-carlo_simulation_technique"}, {"score": 0.0025966876163886882, "phrase": "real_world_classification_problems"}, {"score": 0.0025596276616952516, "phrase": "uci_repository"}, {"score": 0.0025449518236394103, "phrase": "h._chernoff"}, {"score": 0.002508628385630638, "phrase": "asymptotic_efficiency"}, {"score": 0.0024306239471863036, "phrase": "ann"}, {"score": 0.0022424956516376073, "phrase": "analytical_error_probability"}, {"score": 0.0022232308790410124, "phrase": "quadratic_discriminant_analysis"}, {"score": 0.002147809092768899, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["binary classification", " Bayesian decision rule", " decision boundary", " error probability", " Monte-Carlo simulations", " multivariate normal distribution", " quadratic surfaces"], "paper_abstract": "It is well known that the error probability, of the binary Gaussian classification problem with different class covariance matrices, cannot be generally evaluated exactly because of the lack of closed-form expression. This fact pointed out the need to find a tight upper bound for the error probability. This issue has been for more than 50 years ago and is still of interest. All derived upper-bounds are not free of flaws. They might be loose, computationally inefficient particularly in highly dimensional situations, or excessively time consuming if high degree of accuracy is desired. In this paper, a new technique is developed to estimate a tight upper bound for the error probability of the well-known binary Gaussian classification problem with different covariance matrices. The basic idea of the proposed technique is to replace the optimal Bayes decision boundary with suboptimal boundaries which provide an easy-to-calculate upper bound for the error probability. In particular, three types of decision boundaries are investigated: planes, elliptic cylinders, and cones. The new decision boundaries are selected in such a way as to provide the tightest possible upper bound. The proposed technique is found to provide an upper bound, tighter than many of the often used bounds such as the Chernoff bound and the Bayesian-distance bound. In addition, the computation time of the proposed bound is much less than that required by the Monte-Carlo simulation technique. When applied to real world classification problems, obtained from the UCI repository [H. Chernoff, A measure for asymptotic efficiency of a hypothesis based on a sum of observations, Ann. Math. Statist. 23 (1952) 493-507.], the proposed bound was found to provide a tight bound for the analytical error probability of the quadratic discriminant analysis (QDA) classifier and a good approximation to its empirical error probability. Crown Copyright (C) 2007 Published by Elsevier Ltd. All rights reserved.", "paper_title": "Toward a tight upper bound for the error probability of the binary Gaussian classification problem", "paper_id": "WOS:000254106900022"}