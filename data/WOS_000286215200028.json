{"auto_keywords": [{"score": 0.03078406879787557, "phrase": "gpu"}, {"score": 0.009853813651318877, "phrase": "ssa"}, {"score": 0.00481495049065317, "phrase": "reaction-diffusion_simulations"}, {"score": 0.004712726613162057, "phrase": "general-purpose_graphics"}, {"score": 0.004371660146981191, "phrase": "massively_parallel_stochastic_simulation_algorithm"}, {"score": 0.004143196873246147, "phrase": "reaction-diffusion_systems"}, {"score": 0.004011867299107537, "phrase": "graphics_processing_units"}, {"score": 0.0037213333316218522, "phrase": "designated_chips"}, {"score": 0.003526735797659508, "phrase": "high_number"}, {"score": 0.003451766520526601, "phrase": "floating_point_operations"}, {"score": 0.0030017207229983385, "phrase": "scientific_high-performance_computations"}, {"score": 0.0027841357057119317, "phrase": "high-_level_programming_interface"}, {"score": 0.0026384167183586015, "phrase": "general-purpose_graphics_processing_units"}, {"score": 0.0024209250169088575, "phrase": "gpgpu_architecture"}, {"score": 0.0023189818169615135, "phrase": "performance_gain"}, {"score": 0.0021507795710951384, "phrase": "fastest_existing_implementations"}, {"score": 0.0021049977753042253, "phrase": "conventional_hardware"}], "paper_keywords": [""], "paper_abstract": "We present a massively parallel stochastic simulation algorithm (SSA) for reaction-diffusion systems implemented on Graphics Processing Units (GPUs). These are designated chips optimized to process a high number of floating point operations in parallel, rendering them well-suited for a range of scientific high-performance computations. Newer GPU generations provide a high- level programming interface which turns them into General-Purpose Graphics Processing Units (GPGPUs). Our SSA exploits GPGPU architecture to achieve a performance gain of two orders of magnitude over the fastest existing implementations on conventional hardware.", "paper_title": "Accelerating reaction-diffusion simulations with general-purpose graphics processing units", "paper_id": "WOS:000286215200028"}