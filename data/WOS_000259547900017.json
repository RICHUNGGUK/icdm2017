{"auto_keywords": [{"score": 0.03422688836844331, "phrase": "naive-bayes"}, {"score": 0.029714380798947043, "phrase": "missing_data"}, {"score": 0.015719713042815744, "phrase": "classification_error"}, {"score": 0.015640623781226066, "phrase": "discrete_data"}, {"score": 0.011146230458404505, "phrase": "classification_accuracy"}, {"score": 0.010922573995432632, "phrase": "k-nearest"}, {"score": 0.004766090327062509, "phrase": "numerous_industrial_and_research_databases"}, {"score": 0.004403784208967898, "phrase": "data_analysis_methods"}, {"score": 0.004336894811284795, "phrase": "complete_data"}, {"score": 0.004303830426772906, "phrase": "common_way"}, {"score": 0.004142235944700292, "phrase": "missing_values"}, {"score": 0.0040481944502103505, "phrase": "different_imputation_methods"}, {"score": 0.003926105480202798, "phrase": "imputed_data"}, {"score": 0.0037786394889250887, "phrase": "missing_data_imputation"}, {"score": 0.003759398875210585, "phrase": "five_single_imputation_methods"}, {"score": 0.003535985831484511, "phrase": "recently_proposed_imputation_framework"}, {"score": 0.0033857878144822906, "phrase": "six_popular_classifiers"}, {"score": 0.0032668821950788533, "phrase": "polynomial_and_rbf_kernels"}, {"score": 0.0031926502105078494, "phrase": "experimental_study"}, {"score": 0.0031440987202675332, "phrase": "tested_methods"}, {"score": 0.002987514296908759, "phrase": "universally_best_imputation_method"}, {"score": 0.0029196116767956273, "phrase": "best_results"}, {"score": 0.0028973211828356767, "phrase": "ripper_classifier"}, {"score": 0.002867864266547742, "phrase": "high_amount"}, {"score": 0.0027883885425383534, "phrase": "polytomous_regression_imputation"}, {"score": 0.00272499944221247, "phrase": "vector_machine_classifier"}, {"score": 0.002711109302199029, "phrase": "polynomial_kernel"}, {"score": 0.0026630475331025955, "phrase": "imputation_framework"}, {"score": 0.0026091594819333654, "phrase": "support_vector_machine"}, {"score": 0.0025958582759972315, "phrase": "rbf_kernel"}, {"score": 0.002498231552489415, "phrase": "varying_amounts"}, {"score": 0.002422773998743726, "phrase": "imputation_methods"}, {"score": 0.002392008392892289, "phrase": "mean_imputation"}, {"score": 0.0021985035933343405, "phrase": "accurate_classification"}, {"score": 0.0021049984022934944, "phrase": "ripper"}], "paper_keywords": ["missing values", " classification", " imputation of missing values", " single imputation", " multiple imputations"], "paper_abstract": "Numerous industrial and research databases include missing values. It is not uncommon to encounter databases that have LIP to a half of the entries missing, making it very difficult to mine them using data analysis methods that can work only with complete data. A common way of dealing with this problem is to impute (fill-in) the missing values. This paper evaluates how the choice of different imputation methods affects the performance of classifiers that are subsequently used with the imputed data. The experiments here focus on discrete data. This paper studies the effect of missing data imputation using five single imputation methods (a mean method, a Hot deck method, a Naive-Bayes method, and the latter two methods with a recently proposed imputation framework) and one multiple imputation method (a polytomous regression based method) on classification accuracy for six popular classifiers (RIPPER, C4.5, K-nearest-neighbor, support vector machine with polynomial and RBF kernels, and Naive-Bayes) on 15 datasets. This experimental study shows that imputation with the tested methods on average improves classification accuracy when compared to classification without imputation. Although the results show that there is no universally best imputation method, Naive-Bayes imputation is shown to give the best results for the RIPPER classifier for datasets with high amount (i.e., 40% and 50%) of missing data, polytomous regression imputation is shown to be the best for support vector machine classifier with polynomial kernel, and the application of the imputation framework is shown to be superior for the support vector machine with RBF kernel and K-nearest-neighbor. The analysis of the quality of the imputation with respect to varying amounts of missing data (i.e., between 5% and 50%) shows that all imputation methods, except for the mean imputation, improve classification error for data with more than 10% of missing data. Finally, some classifiers such as C4.5 and Naive-Bayes were found to be missing data resistant, i.e., they can Produce accurate classification in the presence of missing data, while other classifiers such as K-nearest-neighbor, SVMs and RIPPER benefit from the imputation. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Impact of imputation of missing values on classification error for discrete data", "paper_id": "WOS:000259547900017"}