{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sparse_representation"}, {"score": 0.01560937278433192, "phrase": "random_projection"}, {"score": 0.014495829283994387, "phrase": "regression-based_methods"}, {"score": 0.004548253531664032, "phrase": "visual_surveillance"}, {"score": 0.004515967518891287, "phrase": "public_resource"}, {"score": 0.004342430078306461, "phrase": "considerable_importance"}, {"score": 0.004160681347769684, "phrase": "crowded_scenes"}, {"score": 0.004087207102190335, "phrase": "principal_drawback"}, {"score": 0.003986509098918949, "phrase": "optimal_set"}, {"score": 0.0038060092376105414, "phrase": "crowd_density"}, {"score": 0.0037387746981660717, "phrase": "recent_success"}, {"score": 0.0036207206487324506, "phrase": "robust_and_scalable_people_counting_method"}, {"score": 0.0035063811070537233, "phrase": "hidden_structure"}, {"score": 0.003481465013724763, "phrase": "semantic_information"}, {"score": 0.0034567253581991226, "phrase": "visual_data"}, {"score": 0.003128216758835217, "phrase": "dimensionality_reduction_method"}, {"score": 0.0030402088074201793, "phrase": "sparse_representation_framework"}, {"score": 0.003018595266561849, "phrase": "new_insight"}, {"score": 0.002954669496511141, "phrase": "classification_problem"}, {"score": 0.0029024303666003153, "phrase": "feature_extraction"}, {"score": 0.0027121844541913367, "phrase": "pre-trained_deep_convolutional_neural_network"}, {"score": 0.002516350372379076, "phrase": "semi-supervised_elastic_net"}, {"score": 0.002480680078868508, "phrase": "unlabelled_data"}, {"score": 0.002445514186654732, "phrase": "user-labelled_image_frames"}, {"score": 0.0023513512089705325, "phrase": "extensive_evaluations"}, {"score": 0.002326304140277375, "phrase": "crowd_analysis_benchmark_datasets"}, {"score": 0.002220806891955454, "phrase": "state-of-the-art_regression-based_people"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["People counting", " Sparse representation", " Fast l(1)-minimization", " Random projection", " Convolutional neural network", " Semi-supervised learning"], "paper_abstract": "Estimating the number of people present in an image has many practical applications including visual surveillance and public resource management. Recently, regression-based methods for people counting have gained considerable importance, principally due to the capability of these methods to handle crowded scenes. However, the principal drawback of regression-based methods is to find an optimal set of features and a model, which is usually dependent on the crowd density. Encouraged by the recent success of sparse representation, here, we develop a robust and scalable people counting method. Sparse representation allows us to capture the hidden structure and semantic information in visual data and leads to faster processing algorithms. In order to reduce the complexity of solving l(1)-minimization problem, which resides at the heart of the sparse representation, a dimensionality reduction method based on random projection is employed. The sparse representation framework provides new insight that if sparsity in the classification problem is properly harnessed, feature extraction is no longer critical. So, in addition to several hand-crafted features, we exploit the features obtained from pre-trained deep Convolutional neural network and show these features perform competitively. Further, to render the proposed method user friendly, we employ a semi-supervised elastic net to automatically annotate unlabelled data with only a handful of user-labelled image frames. Our semi-supervised method exploits temporal continuity in videos. We use extensive evaluations on the crowd analysis benchmark datasets to demonstrate the effectiveness of our approach as well as its superiority over the state-of-the-art regression-based people counting methods, in terms of accuracy and time. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Robust people counting using sparse representation and random projection", "paper_id": "WOS:000357246100008"}