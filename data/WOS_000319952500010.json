{"auto_keywords": [{"score": 0.049348918639928865, "phrase": "dimensionality_reduction"}, {"score": 0.00481495049065317, "phrase": "kullback-leibler_divergences"}, {"score": 0.004778088905211055, "phrase": "cost_functions"}, {"score": 0.004687159050154687, "phrase": "similarity_preservation"}, {"score": 0.004651271219032404, "phrase": "stochastic_neighbor"}, {"score": 0.004599174213031215, "phrase": "sne"}, {"score": 0.0043738432231020885, "phrase": "normalized_softmax_similarities"}, {"score": 0.004323690117778359, "phrase": "pairwise_distances"}, {"score": 0.004176640648520877, "phrase": "low-dimensional_embedding_space"}, {"score": 0.004081385289516914, "phrase": "high-dimensional_data_space"}, {"score": 0.0037072310568610723, "phrase": "previous_work"}, {"score": 0.0035399770257915466, "phrase": "shift_invariance"}, {"score": 0.0034993519231560637, "phrase": "appropriately_normalized_softmax_similarities"}, {"score": 0.0034194902384248006, "phrase": "norm_concentration"}, {"score": 0.00334144503360907, "phrase": "complementary_aspect"}, {"score": 0.0031178024467857215, "phrase": "low-dimensional_spaces"}, {"score": 0.002954228277691572, "phrase": "single_kullback-leibler_divergence"}, {"score": 0.0029090845459992737, "phrase": "weighted_mixture"}, {"score": 0.002842655426951218, "phrase": "neighborhood_retrieval"}, {"score": 0.002703869368554536, "phrase": "different_mixture"}, {"score": 0.002683125911877974, "phrase": "kl_divergences"}, {"score": 0.002631959004685823, "phrase": "scaled_version"}, {"score": 0.002601727192945944, "phrase": "generalized_jensen-shannon_divergence"}, {"score": 0.002474674704809152, "phrase": "small_k-ary_neighborhoods"}, {"score": 0.0022388390432214415, "phrase": "future_improvements"}, {"score": 0.002221655243524476, "phrase": "similarity-based_dr"}, {"score": 0.0021792693001152896, "phrase": "better_definitions"}, {"score": 0.002154226045704907, "phrase": "cost_function"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Dimensionality reduction", " Manifold learning", " Stochastic neighbor embedding", " Divergence"], "paper_abstract": "Stochastic neighbor embedding (SNE) and its variants are methods of dimensionality reduction (DR) that involve normalized softmax similarities derived from pairwise distances. These methods try to reproduce in the low-dimensional embedding space the similarities observed in the high-dimensional data space. Their outstanding experimental results, compared to previous state-of-the-art methods, originate from their capability to foil the curse of dimensionality. Previous work has shown that this immunity stems partly from a property of shift invariance that allows appropriately normalized softmax similarities to mitigate the phenomenon of norm concentration. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between similarities computed in the high- and low-dimensional spaces. Stochastic neighbor embedding and its variant t-SNE rely on a single Kullback-Leibler divergence, whereas a weighted mixture of two dual KL divergences is used in neighborhood retrieval and visualization (NeRV). We propose in this paper a different mixture of KL divergences, which is a scaled version of the generalized Jensen-Shannon divergence. We show experimentally that this divergence produces embeddings that better preserve small K-ary neighborhoods, as compared to both the single KL divergence used in SNE and t-SNE and the mixture used in NeRV. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in dimensionality reduction based on similarity preservation", "paper_id": "WOS:000319952500010"}