{"auto_keywords": [{"score": 0.024763674562134574, "phrase": "youtube"}, {"score": 0.005536200109108782, "phrase": "daisy"}, {"score": 0.004478918834265115, "phrase": "kth"}, {"score": 0.004408729661992651, "phrase": "novel_spatial-temporal_descriptor"}, {"score": 0.0040366412779632085, "phrase": "recent_image"}, {"score": 0.003992385826251404, "phrase": "local_descriptor"}, {"score": 0.003459191419478298, "phrase": "additional_temporal_domain"}, {"score": 0.0028675712105299496, "phrase": "bag-of-words_framework"}, {"score": 0.002594072393698645, "phrase": "weizmann"}, {"score": 0.002511803988580901, "phrase": "public_action"}, {"score": 0.0022492129111567824, "phrase": "ut-interaction"}, {"score": 0.0021520139248317333, "phrase": "promising_results"}], "paper_keywords": ["Action recognition", " 3D DAISY descriptor"], "paper_abstract": "In this paper we propose a novel spatial-temporal descriptor for action recognition. We extend a recent image local descriptor, DAISY, to three dimensions to deal with the information in the additional temporal domain in videos. The new 3D DAISY descriptor is both functionally discriminative and computationally efficient. We use the bag-of-words framework and non-linear SVM for classification. The experiments on public action datasets, KTH, WEIZMANN, YouTube, and UT-Interaction, demonstrate the promising results of our method.", "paper_title": "Action recognition using 3D DAISY descriptor", "paper_id": "WOS:000330314100012"}