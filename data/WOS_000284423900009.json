{"auto_keywords": [{"score": 0.04484219594356789, "phrase": "cuda"}, {"score": 0.00481495049065317, "phrase": "graphics_processing_units"}, {"score": 0.004661868629458321, "phrase": "competitive_accelerator"}, {"score": 0.004555499634179394, "phrase": "graphics_domain"}, {"score": 0.004390312150344245, "phrase": "gpu_programmability"}, {"score": 0.004211594861013822, "phrase": "simple_c-like_interface"}, {"score": 0.003947884118204803, "phrase": "average_programmers"}, {"score": 0.0037177622939493084, "phrase": "packaging_gpu_code"}, {"score": 0.003683565150278339, "phrase": "separate_functions"}, {"score": 0.0036328561748082138, "phrase": "explicitly_managing_data_transfer"}, {"score": 0.0035498822293046884, "phrase": "gpu_memories"}, {"score": 0.003389557201018519, "phrase": "gpu_memory"}, {"score": 0.003358368814209542, "phrase": "practical_experience"}, {"score": 0.0032364494905485677, "phrase": "significant_code_changes"}, {"score": 0.003061793572858598, "phrase": "optimized_program"}, {"score": 0.002843450389745497, "phrase": "cuda_programming"}, {"score": 0.00274017282519193, "phrase": "tedious_tasks"}, {"score": 0.002702415623060517, "phrase": "simpler_manner"}, {"score": 0.002640636493247601, "phrase": "sequential_code"}, {"score": 0.0025683581446365165, "phrase": "porting_process"}, {"score": 0.0024636239664210433, "phrase": "hicuda_directives"}, {"score": 0.0023522423291484212, "phrase": "prototype_compiler"}, {"score": 0.0023091093387071593, "phrase": "hicuda_program"}, {"score": 0.0022772782536385717, "phrase": "cuda_program"}, {"score": 0.002204697943520258, "phrase": "real-world_applications"}, {"score": 0.002174303033371867, "phrase": "multiple_procedures"}, {"score": 0.0021443262599352996, "phrase": "dynamically_allocated_arrays"}, {"score": 0.0021049977753042253, "phrase": "nine_cuda_benchmarks"}], "paper_keywords": ["CUDA", " GPGPU", " data-parallel programming", " directive-based language", " source-to-source compiler"], "paper_abstract": "Graphics Processing Units (GPUs) have become a competitive accelerator for applications outside the graphics domain, mainly driven by the improvements in GPU programmability. Although the Compute Unified Device Architecture ( CUDA) is a simple C-like interface for programming NVIDIA GPUs, porting applications to CUDA remains a challenge to average programmers. In particular, CUDA places on the programmer the burden of packaging GPU code in separate functions, of explicitly managing data transfer between the host and GPU memories, and of manually optimizing the utilization of the GPU memory. Practical experience shows that the programmer needs to make significant code changes, often tedious and error-prone, before getting an optimized program. We have designed hiCUDA, a high-level directive-based language for CUDA programming. It allows programmers to perform these tedious tasks in a simpler manner and directly to the sequential code, thus speeding up the porting process. In this paper, we describe the hiCUDA directives as well as the design and implementation of a prototype compiler that translates a hiCUDA program to a CUDA program. Our compiler is able to support real-world applications that span multiple procedures and use dynamically allocated arrays. Experiments using nine CUDA benchmarks show that the simplicity hiCUDA provides comes at no expense to performance.", "paper_title": "hiCUDA: High-Level GPGPU Programming", "paper_id": "WOS:000284423900009"}