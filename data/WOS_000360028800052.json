{"auto_keywords": [{"score": 0.04929460594219635, "phrase": "elm"}, {"score": 0.04518630955013626, "phrase": "mapreduce_framework"}, {"score": 0.012347313782337485, "phrase": "elastic_elm"}, {"score": 0.009037068891604333, "phrase": "matrix_multiplications"}, {"score": 0.00481495049065317, "phrase": "big_data_classification"}, {"score": 0.004775335634705251, "phrase": "extreme_learning_machine"}, {"score": 0.00446987606516801, "phrase": "good_generalization_performance"}, {"score": 0.004396600680348434, "phrase": "distributed_elm"}, {"score": 0.004031082737449308, "phrase": "challenging_task"}, {"score": 0.0038678251731669865, "phrase": "elastic_extreme_learning_machine"}, {"score": 0.003473529187092258, "phrase": "updated_large-scale_training_dataset"}, {"score": 0.0028596699752849682, "phrase": "intermediate_matrix_multiplications"}, {"score": 0.0028243822059448266, "phrase": "updated_training_data_subset"}, {"score": 0.002709862658377861, "phrase": "old_matrix_multiplications"}, {"score": 0.002676418520290942, "phrase": "intermediate_ones"}, {"score": 0.002621590601574503, "phrase": "corresponding_new_output_weight_vector"}, {"score": 0.0025678829712650437, "phrase": "centralized_computing"}, {"score": 0.0024637379406427878, "phrase": "efficient_learning"}, {"score": 0.0024434201175883674, "phrase": "rapidly_updated_massive_training_dataset"}, {"score": 0.002344311033783139, "phrase": "extensive_experiments"}, {"score": 0.0023249757987087055, "phrase": "synthetic_data"}, {"score": 0.0021940118003312397, "phrase": "massive_rapidly_updated_training_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme learning machine", " Big data classification", " Incremental learning", " Decremental learning", " Correctional learning"], "paper_abstract": "Extreme Learning Machine (ELM) and its variants have been widely used for many applications due to its fast convergence and good generalization performance. Though the distributed ELM* based on MapReduce framework can handle very large scale training dataset in big data applications, how to cope with its rapidly updating is still a challenging task. Therefore, in this paper, a novel Elastic Extreme Learning Machine based on MapReduce framework, named Elastic ELM ((ELM)-L-2), is proposed to cover the shortage of ELM* whose learning ability is weak to the updated large-scale training dataset. Firstly, after analyzing the property of ELM* adequately, it can be found out that its most computation-expensive part, matrix multiplication, can be incrementally, decrementally and correctionally calculated. Next, the Elastic ELM based on MapReduce framework is developed, which first calculates the intermediate matrix multiplications of the updated training data subset, and then update the matrix multiplications by modifying the old matrix multiplications with the intermediate ones. Then, the corresponding new output weight vector can be obtained with centralized computing using the update the matrix multiplications. Therefore, the efficient learning of rapidly updated massive training dataset can be realized effectively. Finally, we conduct extensive experiments on synthetic data to verify the effectiveness and efficiency of our proposed (ELM)-L-2 in learning massive rapidly updated training dataset with various experimental settings. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Elastic extreme learning machine for big data classification", "paper_id": "WOS:000360028800052"}