{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "weighted_sums"}, {"score": 0.008632666292884901, "phrase": "winnow"}, {"score": 0.007572742020469788, "phrase": "original_algorithm"}, {"score": 0.004681226806682058, "phrase": "dnf_learning"}, {"score": 0.00461575768501871, "phrase": "markov_chain_monte_carlo_method"}, {"score": 0.004424768776701965, "phrase": "multiplicative_weight_update_algorithms"}, {"score": 0.004123781267600542, "phrase": "extensive_simulation"}, {"score": 0.004066076430818046, "phrase": "markov_chain"}, {"score": 0.0039716833701918365, "phrase": "accurate_estimates"}, {"score": 0.003825173769489887, "phrase": "optimized_version"}, {"score": 0.0037188360359702182, "phrase": "exactly_the_same_classifications"}, {"score": 0.0034494494982242187, "phrase": "original_metropolis_sampler"}, {"score": 0.003306559295382181, "phrase": "good_samples"}, {"score": 0.0032602525602322832, "phrase": "least_amount"}, {"score": 0.003125175084962886, "phrase": "weighted_sum_estimates"}, {"score": 0.0030525578912808647, "phrase": "winnow's_prediction_accuracy"}, {"score": 0.0029260602766930065, "phrase": "metropolized_gibbs"}, {"score": 0.0027395968397212053, "phrase": "prediction_errors"}, {"score": 0.002626034130205569, "phrase": "mcmc_techniques"}, {"score": 0.0025290375256888883, "phrase": "data_sets"}, {"score": 0.0023346234464901978, "phrase": "brute_force_winnow"}, {"score": 0.0022273034991008326, "phrase": "generalization_accuracy"}], "paper_keywords": ["DNF", " Winnow", " Markov chain Monte Carlo"], "paper_abstract": "A Markov chain Monte Carlo method has previously been introduced to estimate weighted sums in multiplicative weight update algorithms when the number of inputs is exponential. However, the original algorithm still required extensive simulation of the Markov chain in order to get accurate estimates of the weighted sums. We propose an optimized version of the original algorithm that produces exactly the same classifications while often using fewer Markov chain simulations. We also apply three other sampling techniques and empirically compare them with the original Metropolis sampler to determine how effective each is in drawing good samples in the least amount of time, in terms of accuracy of weighted sum estimates and in terms of Winnow's prediction accuracy. We found that two other samplers (Gibbs and Metropolized Gibbs) were slightly better than Metropolis in their estimates of the weighted sums. For prediction errors, there is little difference between any pair of MCMC techniques we tested. Also, on the data sets we tested, we discovered that all approximations of Winnow have no disadvantage when compared to brute force Winnow (where weighted sums are exactly computed), so generalization accuracy is not compromised by our approximation. This is true even when very small sample sizes and mixing times are used.", "paper_title": "Improved MCMC sampling methods for estimating weighted sums in Winnow with application to DNF learning", "paper_id": "WOS:000259571400001"}