{"auto_keywords": [{"score": 0.032889140980823496, "phrase": "proposed_approach"}, {"score": 0.00481495049065317, "phrase": "reinforcement_learning"}, {"score": 0.00418892838079239, "phrase": "reinforcement_learning_perspective"}, {"score": 0.00399881984083004, "phrase": "new_pruning_approach"}, {"score": 0.003852939975815287, "phrase": "q-learning_algorithm"}, {"score": 0.003644001325962353, "phrase": "optimal_policy"}, {"score": 0.003199345207727474, "phrase": "extensive_experimental_comparisons"}, {"score": 0.003054009753601139, "phrase": "state-of-the-art_pruning"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Ensemble selection"], "paper_abstract": "This paper studies the problem of pruning an ensemble of classifiers from a reinforcement learning perspective. It contributes a new pruning approach that uses the Q-learning algorithm in order to approximate an optimal policy of choosing whether to include or exclude each classifier from the ensemble. Extensive experimental comparisons of the proposed approach against state-of-the-art pruning and combination methods show very promising results. Additionally, we present an extension that allows the improvement of the solutions returned by the proposed approach over time, which is very useful in certain performance-critical domains. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Pruning an ensemble of classifiers via reinforcement learning", "paper_id": "WOS:000264993200055"}