{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "natural_scenes"}, {"score": 0.004735519962945449, "phrase": "dynamic_feature_combination."}, {"score": 0.004555217356295278, "phrase": "human's_visual_attention"}, {"score": 0.004406121974246201, "phrase": "research_areas"}, {"score": 0.004285594361623147, "phrase": "computer_vision"}, {"score": 0.004191540451296395, "phrase": "object_recognition"}, {"score": 0.004145287175409325, "phrase": "scene_understanding"}, {"score": 0.00392153537973826, "phrase": "visual_attention_model"}, {"score": 0.0037931018103829427, "phrase": "dynamic_feature_combination_strategy"}, {"score": 0.0035486713647627246, "phrase": "feature_extraction"}, {"score": 0.003509487090932835, "phrase": "dynamic_feature_combination"}, {"score": 0.0034707339811048403, "phrase": "salient_objects_detection"}, {"score": 0.0033757067564871494, "phrase": "saliency_features"}, {"score": 0.0033015555065209865, "phrase": "information_entropy"}, {"score": 0.003265090934183249, "phrase": "salient_boundary"}, {"score": 0.003175676186323053, "phrase": "original_colored_image"}, {"score": 0.002987463078971627, "phrase": "feature_maps"}, {"score": 0.002921815042028607, "phrase": "dynamic_combination_strategy"}, {"score": 0.002794802975530612, "phrase": "feature_map"}, {"score": 0.0026881926384093088, "phrase": "dynamic_weighting"}, {"score": 0.0026584843838039166, "phrase": "individual_feature_maps"}, {"score": 0.0026000466400112974, "phrase": "salient_objects"}, {"score": 0.0025287979714720423, "phrase": "integrated_saliency_map"}, {"score": 0.002486987042061596, "phrase": "computational_method"}, {"score": 0.0024054226746197706, "phrase": "location_shift"}, {"score": 0.0023656467839555458, "phrase": "real_human_visual_attention"}, {"score": 0.0022007302738063566, "phrase": "saliency_detection"}, {"score": 0.0021049977753042253, "phrase": "real_human_visual_attention_mechanism"}], "paper_keywords": ["Visual attention", " feature combination", " saliency", " salient object"], "paper_abstract": "In recent years, many research works indicate that human's visual attention is very helpful in some research areas that are related to computer vision, such as object recognition, scene understanding and object-based image/video retrieval or annotation. This paper presents a visual attention model for natural scenes based on a dynamic feature combination strategy. The model can be divided into three parts, which are feature extraction, dynamic feature combination and salient objects detection. First, the saliency features of color, information entropy and salient boundary are extracted from an original colored image. After that, two different evaluation measurements are proposed for two different categories of feature maps defined in this dynamic combination strategy, which measures the contribution of each feature map to saliency and carries out a dynamic weighting of individual feature maps. Finally, salient objects are located from an integrated saliency map and a computational method is given to simulate the location shift of the real human visual attention. Experimental results show that this model is effective and robust for saliency detection in natural scenes, also similar to the real human visual attention mechanism.", "paper_title": "VISUAL ATTENTION MODEL FOR NATURAL SCENES BASED ON DYNAMIC FEATURE COMBINATION", "paper_id": "WOS:000289955900003"}