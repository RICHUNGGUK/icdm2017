{"auto_keywords": [{"score": 0.04747135426005355, "phrase": "gpu"}, {"score": 0.008713562486709145, "phrase": "gpu-aware_mpi"}, {"score": 0.007827818880286145, "phrase": "device_memory"}, {"score": 0.006508429194729267, "phrase": "programming_productivity"}, {"score": 0.004773692073488661, "phrase": "rdma-enabled_clusters"}, {"score": 0.004592340708764929, "phrase": "high-performance_and_scalable_applications"}, {"score": 0.004417848254181138, "phrase": "key_challenge"}, {"score": 0.004361163586439822, "phrase": "separate_host_memory"}, {"score": 0.004195418010131832, "phrase": "multiple_programming_models"}, {"score": 0.004123800572419314, "phrase": "cuda"}, {"score": 0.004088456260689466, "phrase": "mpi"}, {"score": 0.003967021436047945, "phrase": "different_memory_spaces"}, {"score": 0.003799796252156605, "phrase": "non-contiguous_data"}, {"score": 0.003686935223424015, "phrase": "real-world_applications"}, {"score": 0.003546722347907148, "phrase": "application_performance"}, {"score": 0.003426556090707852, "phrase": "data_communication"}, {"score": 0.003267925215811654, "phrase": "separate_memory_spaces"}, {"score": 0.003212074324593725, "phrase": "explicit_cpu-gpu_data_movement"}, {"score": 0.0030898642396380662, "phrase": "mpi_datatypes"}, {"score": 0.0025892330566358503, "phrase": "data_movement"}, {"score": 0.0025122381773548742, "phrase": "rdma"}, {"score": 0.0023855210798264205, "phrase": "open-source_mpi_library"}, {"score": 0.0023346234464901978, "phrase": "production_application"}, {"score": 0.0021601997088758957, "phrase": "application-level_performance"}, {"score": 0.0021049977753042253, "phrase": "oakley_supercomputer"}], "paper_keywords": ["GPU", " MPI", " CUDA", " RDMA", " InfiniBand", " Lattice Boltzmann method"], "paper_abstract": "Designing high-performance and scalable applications on GPU clusters requires tackling several challenges. The key challenge is the separate host memory and device memory, which requires programmers to use multiple programming models, such as CUDA and MPI, to operate on data in different memory spaces. This challenge becomes more difficult to tackle when non-contiguous data in multidimensional structures is used by real-world applications. These challenges limit the programming productivity and the application performance. We propose the GPU-Aware MPI to support data communication from GPU to GPU using standard MPI. It unifies the separate memory spaces, and avoids explicit CPU-GPU data movement and CPU/GPU buffer management. It supports all MPI datatypes on device memory with two algorithms: a GPU datatype vectorization algorithm and a vector based GPU kernel data pack and unpack algorithm. A pipeline is designed to overlap the non-contiguous data packing and unpacking on GPUs, the data movement on the PCIe, and the RDMA data transfer on the network. We incorporate our design with the open-source MPI library MVAPICH2 and optimize a production application: the multiphase 3D LBM. Besides the increase of programming productivity, we observe up to 19.9 percent improvement in application-level performance on 64 GPUs of the Oakley supercomputer.", "paper_title": "GPU-Aware MPI on RDMA-Enabled Clusters: Design, Implementation and Evaluation", "paper_id": "WOS:000342179600011"}