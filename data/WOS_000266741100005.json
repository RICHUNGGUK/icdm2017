{"auto_keywords": [{"score": 0.04797130800606528, "phrase": "osa"}, {"score": 0.04115941959158914, "phrase": "optimal_policies"}, {"score": 0.026588622262565205, "phrase": "gittins_indices"}, {"score": 0.00481495049065317, "phrase": "constrained_multi-armed_bandit_formulation"}, {"score": 0.004717485406306594, "phrase": "instantaneous_spectrum_opportunities"}, {"score": 0.00467905143301368, "phrase": "fundamental_challenges"}, {"score": 0.004640929126263433, "phrase": "opportunistic_spectrum_access"}, {"score": 0.004491506786670695, "phrase": "bursty_traffic"}, {"score": 0.0044549056501745074, "phrase": "primary_users"}, {"score": 0.0044005599565517875, "phrase": "limited_spectrum"}, {"score": 0.004329124434198367, "phrase": "secondary_users"}, {"score": 0.004021689117050817, "phrase": "sequential_decision_framework"}, {"score": 0.003782174375003052, "phrase": "partially_observed_markov_decision_process"}, {"score": 0.00335867620389808, "phrase": "stationary_osa_policies"}, {"score": 0.003056682083784002, "phrase": "priori_unknown_statistical_spectrum_knowledge"}, {"score": 0.0029581155916843663, "phrase": "channel_evolution"}, {"score": 0.0028744731986533076, "phrase": "multi-armed_bandit"}, {"score": 0.002759058732719621, "phrase": "optimal_policy"}, {"score": 0.0027030963394733916, "phrase": "well-known_gittins_index_rule"}, {"score": 0.002626645763725465, "phrase": "largest_gittins_index"}, {"score": 0.0025523518748506347, "phrase": "closed-form_formulas"}, {"score": 0.002480154140902474, "phrase": "tunable_approximation"}, {"score": 0.0024099937116733227, "phrase": "reinforcement_learning_algorithm"}, {"score": 0.002294294558114909, "phrase": "markovian_channel_parameters"}, {"score": 0.002131054594847684, "phrase": "extensive_experiments"}], "paper_keywords": ["Multi-armed bandit (MAB) problem", " opportunistic spectrum access (OSA)", " partially observed Markov decision process (POMDP)", " reinforcement learning (RL)"], "paper_abstract": "Tracking and exploiting instantaneous spectrum opportunities are fundamental challenges in opportunistic spectrum access (OSA) in presence of the bursty traffic of primary users and the limited spectrum sensing capability of secondary users. In order to take advantage of the history of spectrum sensing and access decisions, a sequential decision framework is widely used to design optimal policies. However, many existing schemes, based on a partially observed Markov decision process (POMDP) framework, reveal that optimal policies are non-stationary in nature which renders them difficult to calculate and implement. Therefore, this work pursues stationary OSA policies, which are thereby efficient yet low-complexity, while still incorporating many practical factors, such as spectrum sensing errors and a priori unknown statistical spectrum knowledge. First, with an approximation on channel evolution, OSA is formulated in a multi-armed bandit (MAB) framework. As a result, the optimal policy is specified by the well-known Gittins index rule, where the channel with the largest Gittins index is always selected. Then, closed-form formulas are derived for the Gittins indices with tunable approximation, and the design of a reinforcement learning algorithm is presented for calculating the Gittins indices, depending on whether the Markovian channel parameters are available a priori or not. Finally, the superiority of the scheme is presented via extensive experiments compared to other existing schemes in terms of the quality of policies and optimality.", "paper_title": "Opportunistic Spectrum Access Based on a Constrained Multi-Armed Bandit Formulation", "paper_id": "WOS:000266741100005"}