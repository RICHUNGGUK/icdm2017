{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "memory-intensive_computations"}, {"score": 0.0047864177420244795, "phrase": "multi-core_processors"}, {"score": 0.004758053266234474, "phrase": "explicit_memory_hierarchy"}, {"score": 0.004729856080441582, "phrase": "limited_bandwidth"}, {"score": 0.004701825207431857, "phrase": "off-chip_main_memory"}, {"score": 0.004632469280786256, "phrase": "performance_bottleneck"}, {"score": 0.004605012887484129, "phrase": "chip_multiprocessors"}, {"score": 0.004456872931482395, "phrase": "increasing_number"}, {"score": 0.00430067200637091, "phrase": "computational_work"}, {"score": 0.004275173711009189, "phrase": "memory_transfer"}, {"score": 0.004064427818174677, "phrase": "chip_pipelining"}, {"score": 0.003992544149307519, "phrase": "partial_results"}, {"score": 0.0038640303886081444, "phrase": "high-bandwidth_internal_network"}, {"score": 0.0037619540348087493, "phrase": "main_memory_accesses"}, {"score": 0.0035446342985010434, "phrase": "limited_amount"}, {"score": 0.003502695291158573, "phrase": "chip_memory"}, {"score": 0.0034612507694547013, "phrase": "forwarded_data"}, {"score": 0.003290494847464381, "phrase": "load_balancing"}, {"score": 0.0032709661258673206, "phrase": "buffer_memory_consumption"}, {"score": 0.003241889436209603, "phrase": "communication_load"}, {"score": 0.003213070385558666, "phrase": "on-chip_network"}, {"score": 0.0031845067058084583, "phrase": "larger_buffer_size"}, {"score": 0.0030183628643645436, "phrase": "parallel_mergesort"}, {"score": 0.002991524908432975, "phrase": "representative_memory-intensive_application"}, {"score": 0.002921114512115261, "phrase": "global_merging_phase"}, {"score": 0.0028693932802982417, "phrase": "overall_sorting_time"}, {"score": 0.002852356597632014, "phrase": "larger_data_sets"}, {"score": 0.002801849371896573, "phrase": "technical_issues"}, {"score": 0.002768674345336861, "phrase": "on-chip_pipelining_technique"}, {"score": 0.0027115578750853673, "phrase": "optimized_mapping"}, {"score": 0.0026954557881801546, "phrase": "merge_trees"}, {"score": 0.0026714812866037584, "phrase": "multiprocessor_cores"}, {"score": 0.0024946054043656113, "phrase": "pipelined_parallel_mergesort"}, {"score": 0.0024358622406092344, "phrase": "exemplary_target"}, {"score": 0.002378499067102358, "phrase": "buffer_sizes"}, {"score": 0.00236437009958525, "phrase": "mapping_optimizations"}, {"score": 0.0022610379771517966, "phrase": "realistic_problem_sizes"}, {"score": 0.0021622120805979937, "phrase": "merge_phase"}, {"score": 0.0021493651517912036, "phrase": "cellsort"}, {"score": 0.0021049977753042253, "phrase": "fastest_merge_sort_implementation"}], "paper_keywords": ["parallel merge sort", " on-chip pipelining", " multicore computing", " task mapping", " streaming computations"], "paper_abstract": "Limited bandwidth to off-chip main memory tends to be a performance bottleneck in chip multiprocessors, and this will become even more problematic with an increasing number of cores. Especially for streaming computations where the ratio between computational work and memory transfer is low, transforming the program into more memory-efficient code is an important program optimization. On-chip pipelining reorganizes the computation so that partial results of subtasks are forwarded immediately between the cores over the high-bandwidth internal network, in order to reduce the volume of main memory accesses, and thereby improves the throughput for memory-intensive computations. At the same time, throughput is also constrained by the limited amount of on-chip memory available for buffering forwarded data. By optimizing the mapping of tasks to cores, balancing a trade-off between load balancing, buffer memory consumption, and communication load on the on-chip network, a larger buffer size can be applied, resulting in less DMA communication and scheduling overhead. In this article, we consider parallel mergesort as a representative memory-intensive application in detail, and focus on the global merging phase, which is dominating the overall sorting time for larger data sets. We work out the technical issues of applying the on-chip pipelining technique, and present several algorithms for optimized mapping of merge trees to the multiprocessor cores. We also demonstrate how some of these algorithms can be used for mapping of other streaming task graphs. We describe an implementation of pipelined parallel mergesort for the Cell Broadband Engine, which serves as an exemplary target. We evaluate experimentally the influence of buffer sizes and mapping optimizations, and show that optimized on-chip pipelining indeed speeds up, for realistic problem sizes, merging times by up to 70% on QS20 and 143% on PS3 compared to the merge phase of CellSort, which was by now the fastest merge sort implementation on Cell.", "paper_title": "Optimized On-Chip-Pipelining for Memory-Intensive Computations on Multi-Core Processors with Explicit Memory Hierarchy", "paper_id": "WOS:000310430600006"}