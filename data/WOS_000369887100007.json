{"auto_keywords": [{"score": 0.03879994748535876, "phrase": "svm"}, {"score": 0.007017936948449211, "phrase": "imbalanced_data"}, {"score": 0.005970531806531438, "phrase": "dwd"}, {"score": 0.00481495049065317, "phrase": "important_topic"}, {"score": 0.00477513738640734, "phrase": "statistics_and_machine_learning"}, {"score": 0.004486800020569973, "phrase": "support_vector_machine"}, {"score": 0.0039446780026489905, "phrase": "unified_family"}, {"score": 0.0039120325834029355, "phrase": "classification_machines"}, {"score": 0.003863568356785813, "phrase": "flexible_assortment_machine"}, {"score": 0.003660374693346876, "phrase": "special_cases"}, {"score": 0.0036150168165327286, "phrase": "flame_family"}, {"score": 0.003453444640541076, "phrase": "dwd."}, {"score": 0.003285380677401785, "phrase": "high-dimensional_setting"}, {"score": 0.0030611534228319717, "phrase": "larger_sample_size"}, {"score": 0.002960979624379786, "phrase": "decision_boundary"}, {"score": 0.0029242629306714773, "phrase": "minority_class"}, {"score": 0.00284034646798833, "phrase": "imbalanced_data_issue"}, {"score": 0.0027818804382539444, "phrase": "high-dimensional_data_sets"}, {"score": 0.002735972929840289, "phrase": "undesired_data-piling_phenomenon"}, {"score": 0.002602734354498382, "phrase": "high_dimensional_setting"}, {"score": 0.002517524071220657, "phrase": "imbalanced_ratio"}, {"score": 0.002496659907186784, "phrase": "sample_sizes"}, {"score": 0.002424984362388813, "phrase": "intrinsic_connection"}, {"score": 0.002249961956629804, "phrase": "high-dimensional_data"}, {"score": 0.0022036216160494925, "phrase": "flame_classifiers"}, {"score": 0.0021492686054379755, "phrase": "real_data_applications"}, {"score": 0.0021049977753042253, "phrase": "theoretical_findings"}], "paper_keywords": ["classification", " Fisher consistency", " high-dimensional low-sample size asymptotics", " imbalanced data", " support vector machine"], "paper_abstract": "Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large-margin classification methods, Support Vector Machine (SVM) and Distance Weighted Discrimination (DWD), under two contexts: the high-dimensional, low-sample size data and the imbalanced data. A unified family of classification machines, the FLexible Assortment MachinE (FLAME) is proposed, within which DWD and SVM are special cases. The FLAME family helps to identify the similarities and differences between SVM and DWD. It is well known that many classifiers overfit the data in the high-dimensional setting; and others are sensitive to the imbalanced data, that is, the class with a larger sample size overly influences the classifier and pushes the decision boundary towards the minority class. SVM is resistant to the imbalanced data issue, but it overfits high-dimensional data sets by showing the undesired data-piling phenomenon. The DWD method was proposed to improve SVM in the high dimensional setting, but its decision boundary is sensitive to the imbalanced ratio of sample sizes. Our FLAME family helps to understand an intrinsic connection between SVM and DWD, and provides a trade-off between sensitivity to the imbalanced data and overfitting the high-dimensional data. Several asymptotic properties of the FLAME classifiers are studied. Simulations and real data applications are investigated to illustrate theoretical findings.", "paper_title": "Flexible High-Dimensional Classification Machines and Their Asymptotic Properties", "paper_id": "WOS:000369887100007"}