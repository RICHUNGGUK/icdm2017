{"auto_keywords": [{"score": 0.02246846748295755, "phrase": "elsevier"}, {"score": 0.00481495049065317, "phrase": "feature_extraction_budgets"}, {"score": 0.004568871736067739, "phrase": "sparse_linear_models"}, {"score": 0.00450933377534525, "phrase": "multi-label_prediction_tasks"}, {"score": 0.003928969293303814, "phrase": "feature_values"}, {"score": 0.003752550907120244, "phrase": "greedy_multi-label"}, {"score": 0.003468287913322255, "phrase": "greedy_forward_selection_search"}, {"score": 0.003400640983719424, "phrase": "cross-validation_based_selection_criterion"}, {"score": 0.0030414140732789186, "phrase": "highly_efficient_algorithm"}, {"score": 0.0029238768315330305, "phrase": "linear_time"}, {"score": 0.00270221723573854, "phrase": "matrix_update_formulas"}, {"score": 0.0026321198895843173, "phrase": "feature_addition"}, {"score": 0.0025977542995083624, "phrase": "cross-validation_computations"}, {"score": 0.00241658945942583, "phrase": "sparse_accurate_predictors"}, {"score": 0.0023694065215394593, "phrase": "wide_range"}, {"score": 0.0023384629381463054, "phrase": "benchmark_problems"}, {"score": 0.0022628564592874147, "phrase": "multi-task_lasso_baseline_method"}], "paper_keywords": ["Feature selection", " Greedy forward selection", " Multi-label learning", " Regularized least-squares"], "paper_abstract": "We consider the problem of learning sparse linear models for multi-label prediction tasks under a hard constraint on the number of features. Such budget constraints are important in domains where the acquisition of the feature values is costly. We propose a greedy multi-label regularized least-squares algorithm that solves this problem by combining greedy forward selection search with a cross-validation based selection criterion in order to choose, which features to include in the model. We present a highly efficient algorithm for implementing this procedure with linear time and space complexities. This is achieved through the use of matrix update formulas for speeding up feature addition and cross-validation computations. Experimentally, we demonstrate that the approach allows finding sparse accurate predictors on a wide range of benchmark problems, typically outperforming the multi-task lasso baseline method when the budget is small. (C) 2013 Elsevier B. V. All rights reserved.", "paper_title": "Multi-label learning under feature extraction budgets", "paper_id": "WOS:000333105600008"}