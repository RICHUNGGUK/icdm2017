{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaze_gestures"}, {"score": 0.004756521061849989, "phrase": "input_mechanism"}, {"score": 0.004660702714929615, "phrase": "small_handheld_devices"}, {"score": 0.004493045859210086, "phrase": "touch_sensitive_surfaces"}, {"score": 0.004366808573435364, "phrase": "growing_interest"}, {"score": 0.004244102948402086, "phrase": "human-computer_interaction"}, {"score": 0.003817655499320344, "phrase": "predefined_movement_patterns"}, {"score": 0.003771282384646587, "phrase": "\"gaze_gestures"}, {"score": 0.0036652492107503956, "phrase": "hci_purposes"}, {"score": 0.0036355032885145894, "phrase": "desktop_computers"}, {"score": 0.003490343299449249, "phrase": "video-based_eye-tracking_system"}, {"score": 0.00332375606483146, "phrase": "effective_input_paradigm"}, {"score": 0.0032700071110068323, "phrase": "handheld_electronic_devices"}, {"score": 0.002941147710077221, "phrase": "potential_users"}, {"score": 0.002881793492919646, "phrase": "needleman-wunsch_algorithm"}, {"score": 0.0028351712509358997, "phrase": "intentional_gaze_gestures"}, {"score": 0.0028121429464519733, "phrase": "otherwise_typical_gaze_activity"}, {"score": 0.002777949746964955, "phrase": "standard_interaction"}, {"score": 0.0027441711627551064, "phrase": "small_smartphone_screen"}, {"score": 0.002699769374493867, "phrase": "reliable_gaze-smartphone_interaction"}, {"score": 0.0026560841114619147, "phrase": "accuracy_rates"}, {"score": 0.0022287497627555895, "phrase": "low-cost_eye-tracking_equipment"}, {"score": 0.0021659920594094407, "phrase": "new_hci_modality"}, {"score": 0.0021049977753042253, "phrase": "small-screen_handheld_devices"}], "paper_keywords": [""], "paper_abstract": "The emergence of small handheld devices such as tablets and smartphones, often with touch sensitive surfaces as their only input modality, has spurred a growing interest in the subject of gestures for human-computer interaction (HCI). It has been proven before that eye movements can be consciously controlled by humans to the extent of performing sequences of predefined movement patterns, or \"gaze gestures\" that can be used for HCI purposes in desktop computers. Gaze gestures can be tracked noninvasively using a video-based eye-tracking system. We propose here that gaze gestures can also be an effective input paradigm to interact with handheld electronic devices. We show through a pilot user study how gaze gestures can be used to interact with a smartphone, how they are easily assimilated by potential users, and how the Needleman-Wunsch algorithm can effectively discriminate intentional gaze gestures from otherwise typical gaze activity performed during standard interaction with a small smartphone screen. Hence, reliable gaze-smartphone interaction is possible with accuracy rates, depending on the modality of gaze gestures being used (with or without dwell), higher than 80 to 90%, negligible false positive rates, and completion speeds lower than 1 to 1.5 s per gesture. These encouraging results and the low-cost eye-tracking equipment used suggest the possibilities of this new HCI modality for the field of interaction with small-screen handheld devices.", "paper_title": "Controlling a Smartphone Using Gaze Gestures as the Input Mechanism", "paper_id": "WOS:000342504600002"}