{"auto_keywords": [{"score": 0.04649794337781344, "phrase": "human_infants"}, {"score": 0.04300106597094418, "phrase": "verbal_descriptions"}, {"score": 0.00481495049065317, "phrase": "perception-action_association"}, {"score": 0.004667833113864304, "phrase": "language_acquisition"}, {"score": 0.004613825044585923, "phrase": "artificial_system"}, {"score": 0.0042859292226559535, "phrase": "manipulation_tasks"}, {"score": 0.003935147683802319, "phrase": "affordance_network"}, {"score": 0.0038296485293910026, "phrase": "robot_actions"}, {"score": 0.003800026771283179, "phrase": "robot_perceptions"}, {"score": 0.0037414662674652805, "phrase": "perceived_effects"}, {"score": 0.0035989687070420977, "phrase": "affordance_model"}, {"score": 0.003557283483525861, "phrase": "spoken_words"}, {"score": 0.0034484600692786097, "phrase": "verbal_symbols"}, {"score": 0.003129317057177867, "phrase": "temporal_co-occurrence"}, {"score": 0.003069112841132911, "phrase": "speech_utterances"}, {"score": 0.003033546013625887, "phrase": "involved_objects"}, {"score": 0.002884111441153185, "phrase": "able_form_useful_word-to-meaning_associations"}, {"score": 0.00278497898719426, "phrase": "learning_process"}, {"score": 0.002720785798036066, "phrase": "recognition_errors"}, {"score": 0.0026892447164623247, "phrase": "word-to-meaning_associations"}, {"score": 0.0026374845514245547, "phrase": "robot's_own_understanding"}, {"score": 0.0023746766301040974, "phrase": "speech_recognition_task"}, {"score": 0.002319918823705175, "phrase": "encouraging_results"}, {"score": 0.002222780643552078, "phrase": "language_descriptors"}, {"score": 0.0021049977753042253, "phrase": "challenging_process"}], "paper_keywords": ["Affordances", " automatic speech recognition", " Bayesian networks", " cognitive robotics", " grasping", " humanoid robots", " language", " unsupervised learning"], "paper_abstract": "We address the problem of bootstrapping language acquisition for an artificial system similarly to what is observed in experiments with human infants. Our method works by associating meanings to words in manipulation tasks, as a robot interacts with objects and listens to verbal descriptions of the interactions. The model is based on an affordance network, i.e., a mapping between robot actions, robot perceptions, and the perceived effects of these actions upon objects. We extend the affordance model to incorporate spoken words, which allows us to ground the verbal symbols to the execution of actions and the perception of the environment. The model takes verbal descriptions of a task as the input and uses temporal co-occurrence to create links between speech utterances and the involved objects, actions, and effects. We show that the robot is able form useful word-to-meaning associations, even without considering grammatical structure in the learning process and in the presence of recognition errors. These word-to-meaning associations are embedded in the robot's own understanding of its actions. Thus, they can be directly used to instruct the robot to perform tasks and also allow to incorporate context in the speech recognition task. We believe that the encouraging results with our approach may afford robots with a capacity to acquire language descriptors in their operation's environment as well as to shed some light as to how this challenging process develops with human infants.", "paper_title": "Language Bootstrapping: Learning Word Meanings From Perception-Action Association", "paper_id": "WOS:000304163200007"}