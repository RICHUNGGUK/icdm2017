{"auto_keywords": [{"score": 0.049666636555871356, "phrase": "multicore_processors"}, {"score": 0.00481495049065317, "phrase": "recent_trends"}, {"score": 0.0047323590206695305, "phrase": "graphical_processing_units"}, {"score": 0.004551647527068771, "phrase": "thread-level_parallelism"}, {"score": 0.004228839109569368, "phrase": "nvidia_gpus"}, {"score": 0.004120435930514554, "phrase": "intel_xeon"}, {"score": 0.004084920064588169, "phrase": "amd_opteron"}, {"score": 0.004049776495781106, "phrase": "ibm"}, {"score": 0.004014800383343256, "phrase": "cell_broadband_engine"}, {"score": 0.003962998498942108, "phrase": "case_study"}, {"score": 0.0038447018470115146, "phrase": "biological_spiking_neural_network"}, {"score": 0.0037138111590259005, "phrase": "izhikevich"}, {"score": 0.003634262647873338, "phrase": "hodgkin-huxley_neuron_models"}, {"score": 0.0035564156572789473, "phrase": "varying_requirements"}, {"score": 0.0034204548167213545, "phrase": "performance_analysis"}, {"score": 0.0033762956164519286, "phrase": "hardware_platforms"}, {"score": 0.0031230216871739776, "phrase": "available_optimization_techniques"}, {"score": 0.0030960762496815768, "phrase": "execution_configuration"}, {"score": 0.0030560920414081645, "phrase": "fitness_performance_model"}, {"score": 0.002802373595203129, "phrase": "snn_implementation_results"}, {"score": 0.002766171878646582, "phrase": "roofline_model"}, {"score": 0.0024928553725344933, "phrase": "significant_speedups"}, {"score": 0.0023767561522363367, "phrase": "maximum_speedup"}, {"score": 0.002236762786982225, "phrase": "proper_match"}, {"score": 0.0021982976170703884, "phrase": "algorithm_complexity"}, {"score": 0.002169882514603539, "phrase": "best_performance"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["performance", " multicore", " GPU", " optimization", " Fitness model"], "paper_abstract": "Recent trends involving multicore processors and graphical processing units (GPUs) focus on exploiting task- and thread-level parallelism. In this paper, we have analyzed various aspects of the performance of these architectures including NVIDIA GPUs, and multicore processors such as Intel Xeon, AMD Opteron, IBM's Cell Broadband Engine. The case study used in this paper is a biological spiking neural network (SNN), implemented with the Izhikevich, Wilson, Morris-Lecar, and Hodgkin-Huxley neuron models. The four SNN models have varying requirements for communication and computation making them useful for performance analysis of the hardware platforms. We report and analyze the variation of performance with network (problem size) scaling, available optimization techniques and execution configuration. A Fitness performance model, that predicts the suitability of the architecture for accelerating an application, is proposed and verified with the SNN implementation results. The Roofline model, another existing performance model, has also been utilized to determine the hardware bottleneck(s) and attainable peak performance of the architectures. Significant speedups for the four SNN neuron models utilizing these architectures are reported; the maximum speedup of 574x was observed in our GPU implementation. Our results and analysis show that a proper match of architecture with algorithm complexity provides the best performance. Copyright (C) 2010 John Wiley & Sons, Ltd.", "paper_title": "Performance, optimization, and fitness: Connecting applications to architectures", "paper_id": "WOS:000293155900003"}