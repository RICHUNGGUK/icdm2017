{"auto_keywords": [{"score": 0.038535167700333696, "phrase": "optimal_number"}, {"score": 0.00481495049065317, "phrase": "unsupervised_gaussian_mixture_models"}, {"score": 0.004283576270012874, "phrase": "gaussian_mixture_model"}, {"score": 0.0038404642251010797, "phrase": "careful_initialization"}, {"score": 0.0026402908398883832, "phrase": "shannon_entropy"}, {"score": 0.0024803459795996116, "phrase": "classical_em_algorithm"}, {"score": 0.002171836484200008, "phrase": "probability_density_estimation"}, {"score": 0.0021381565332182773, "phrase": "pattern_recognition"}, {"score": 0.0021049977753042253, "phrase": "color_image_segmentation"}], "paper_keywords": [""], "paper_abstract": "In this paper we address the problem of estimating the parameters of a Gaussian mixture model. Although the EM (Expectation-Maximization) algorithm yields the maximum-likelihood solution it requires a careful initialization of the parameters and the optimal number of kernels in the mixture may be unknown beforehand. We propose a criterion based on the entropy of the pdf (probability density function) associated to each kernel to measure the quality of a given mixture model. Two different methods for estimating Shannon entropy are proposed and a modification of the classical EM algorithm to find the optimal number of kernels in the mixture is presented. We test our algorithm in probability density estimation, pattern recognition and color image segmentation.", "paper_title": "Two entropy-based methods for learning unsupervised Gaussian mixture models", "paper_id": "WOS:000240075100071"}