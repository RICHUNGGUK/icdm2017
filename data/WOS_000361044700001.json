{"auto_keywords": [{"score": 0.027974447194407184, "phrase": "lower_bound"}, {"score": 0.00481495049065317, "phrase": "scp_algorithms"}, {"score": 0.004781915513030716, "phrase": "linguistic_corpus_design"}, {"score": 0.004732785510739708, "phrase": "critical_concern"}, {"score": 0.004684157900507474, "phrase": "rich_annotated_corpora"}, {"score": 0.004636027593561925, "phrase": "different_domains"}, {"score": 0.004525629293824217, "phrase": "speech_technologies"}, {"score": 0.004479159820421669, "phrase": "asr"}, {"score": 0.004283019592801715, "phrase": "huge_amount"}, {"score": 0.004253618550301311, "phrase": "speech_data"}, {"score": 0.004209893377338457, "phrase": "data-driven_models"}, {"score": 0.004152288616186981, "phrase": "synthetic_speech"}, {"score": 0.0034472883424839346, "phrase": "linguistic_text_content"}, {"score": 0.0034000828197314264, "phrase": "sufficient_level"}, {"score": 0.0033767221949228834, "phrase": "linguistic_richness"}, {"score": 0.0030554300179552415, "phrase": "large_text_corpora"}, {"score": 0.00303464457899255, "phrase": "english"}, {"score": 0.003013672381510214, "phrase": "french"}, {"score": 0.002982557900944478, "phrase": "phonological_information"}, {"score": 0.0029214769181390653, "phrase": "first_considered_algorithm"}, {"score": 0.0028914057633212045, "phrase": "standard_greedy_solution"}, {"score": 0.0028127163745564777, "phrase": "second_algorithm"}, {"score": 0.0027837615811247963, "phrase": "lagrangian_relaxation"}, {"score": 0.0027551040319267446, "phrase": "latter_approach"}, {"score": 0.0026616868981501006, "phrase": "covering_solution"}, {"score": 0.002518746501941657, "phrase": "reduced_corpus"}, {"score": 0.0024333239726872604, "phrase": "suboptimal_algorithm"}, {"score": 0.002408265305704217, "phrase": "greedy_algorithm"}, {"score": 0.0023917027133236813, "phrase": "good_results"}, {"score": 0.002209225188316266, "phrase": "scp"}, {"score": 0.0021049977753042253, "phrase": "covering_feature"}], "paper_keywords": [""], "paper_abstract": "Linguistic corpus design is a critical concern for building rich annotated corpora useful in different domains of applications. For example, speech technologies such as ASR (Automatic Speech Recognition) or TTS (Text-to-Speech) need a huge amount of speech data to train data-driven models or to produce synthetic speech. Collecting data is always related to costs (recording speech, verifying annotations, etc.), and as a rule of thumb, the more data you gather, the more costly your application will be. Within this context, we present in this article solutions to reduce the amount of linguistic text content while maintaining a sufficient level of linguistic richness required by a model or an application. This problem can be formalized as a Set Covering Problem (SCP) and we evaluate two algorithmic heuristics applied to design large text corpora in English and French for covering phonological information or POS labels. The first considered algorithm is a standard greedy solution with an agglomerative/spitting strategy and we propose a second algorithm based on Lagrangian relaxation. The latter approach provides a lower bound to the cost of each covering solution. This lower bound can be used as a metric to evaluate the quality of a reduced corpus whatever the algorithm applied. Experiments show that a suboptimal algorithm like a greedy algorithm achieves good results; the cost of its solutions is not so far from the lower bound (about 4.35% for 3-phoneme coverings). Usually, constraints in SCP are binary; we proposed here a generalization where the constraints on each covering feature can be multi-valued.", "paper_title": "Large Linguistic Corpus Reduction with SCP Algorithms", "paper_id": "WOS:000361044700001"}