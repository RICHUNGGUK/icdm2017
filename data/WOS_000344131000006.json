{"auto_keywords": [{"score": 0.047667240890459914, "phrase": "pose_estimation"}, {"score": 0.00481495049065317, "phrase": "uncertain_point_cloud_data"}, {"score": 0.0044369056710423065, "phrase": "even_a_small_error"}, {"score": 0.004286708921931472, "phrase": "planned_grasp"}, {"score": 0.0041773934798743405, "phrase": "robust_grasp_planning"}, {"score": 0.004123781267600542, "phrase": "object_geometry"}, {"score": 0.004088421020408183, "phrase": "tactile_sensor_feedback"}, {"score": 0.004001335931796133, "phrase": "pose_range_estimation"}, {"score": 0.003967021436047945, "phrase": "specific_uncertainties"}, {"score": 0.003751011827704279, "phrase": "grasp_planning_method"}, {"score": 0.003623949225126944, "phrase": "visually-estimated_object"}, {"score": 0.003531474887552889, "phrase": "known_shape"}, {"score": 0.0033391018638565715, "phrase": "-_possibly_sparse_-_point_cloud"}, {"score": 0.00306334226516593, "phrase": "particular_viewpoint"}, {"score": 0.002934098412396701, "phrase": "heterogeneous_textures"}, {"score": 0.0028963961037320805, "phrase": "object_surface"}, {"score": 0.0028468767420392945, "phrase": "stereo-vision_algorithms"}, {"score": 0.002810292047426252, "phrase": "robust_feature-point_matching"}, {"score": 0.0026456667695073043, "phrase": "unavoidable_ambiguities"}, {"score": 0.0026116610153271943, "phrase": "proposed_grasp_planner"}, {"score": 0.002555950807368337, "phrase": "particle_filter"}, {"score": 0.0025122373359090454, "phrase": "object_probability_distribution"}, {"score": 0.0024799423362011582, "phrase": "discrete_set"}, {"score": 0.002255431275670053, "phrase": "robust_grasps"}, {"score": 0.0021789194955896124, "phrase": "humanoid_robot_icub"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Robotic grasping", " Multi-fingered hand", " Inverse kinematics"], "paper_abstract": "Robotic grasping is very sensitive to how accurate is the pose estimation of the object to grasp. Even a small error in the estimated pose may cause the planned grasp to fail. Several methods for robust grasp planning exploit the object geometry or tactile sensor feedback. However, object pose range estimation introduces specific uncertainties that can also be exploited to choose more robust grasps. We present a grasp planning method that explicitly considers the uncertainties on the visually-estimated object pose. We assume a known shape (e.g. primitive shape or triangle mesh), observed as a - possibly sparse - point cloud. The measured points are usually not uniformly distributed over the surface as the object is seen from a particular viewpoint; additionally this non-uniformity can be the result of heterogeneous textures over the object surface, when using stereo-vision algorithms based on robust feature-point matching. Consequently the pose estimation may be more accurate in some directions and contain unavoidable ambiguities. The proposed grasp planner is based on a particle filter to estimate the object probability distribution as a discrete set. We show that, for grasping, some ambiguities are less unfavorable so the distribution can be used to select robust grasps. Some experiments are presented with the humanoid robot iCub and its stereo cameras. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Grasping objects localized from uncertain point cloud data", "paper_id": "WOS:000344131000006"}