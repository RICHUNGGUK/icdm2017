{"auto_keywords": [{"score": 0.04075292987843501, "phrase": "word-position_matrices"}, {"score": 0.004823530554614887, "phrase": "matrix"}, {"score": 0.004601045179623499, "phrase": "natural_language"}, {"score": 0.004488384083300084, "phrase": "machine_learning_methods"}, {"score": 0.0044514440242754815, "phrase": "natural_language_inputs"}, {"score": 0.0043066860455264, "phrase": "input_text"}, {"score": 0.003997891221012094, "phrase": "word-position_matrix_representation"}, {"score": 0.0039323225960857956, "phrase": "linear_feature_transformations"}, {"score": 0.003835972725191194, "phrase": "kernel_functions"}, {"score": 0.0036052633308076933, "phrase": "word_similarities"}, {"score": 0.003360464999544279, "phrase": "elegant_way"}, {"score": 0.003264547032549398, "phrase": "positional_similarities"}, {"score": 0.0032109670662222416, "phrase": "previously_proposed_techniques"}, {"score": 0.0031582637043009562, "phrase": "latent_semantic_analysis"}, {"score": 0.0029805015010906013, "phrase": "novel_ways"}, {"score": 0.002847858807513713, "phrase": "efficient_algorithms"}, {"score": 0.0028243822059448266, "phrase": "computing_kernel_functions"}, {"score": 0.0026543514413663893, "phrase": "highly_efficient_method"}, {"score": 0.002546708483171714, "phrase": "natural_language_disambiguation_tasks"}, {"score": 0.0024535580495414783, "phrase": "single_word"}, {"score": 0.0021669202726173928, "phrase": "context-sensitive_spelling_error_correction"}, {"score": 0.0021402861983777235, "phrase": "news"}, {"score": 0.0021049977753042253, "phrase": "model_problem"}], "paper_keywords": ["Kernel methods", " Feature transformations", " Natural language processing", " Natural language disambiguation"], "paper_abstract": "In the application of machine learning methods with natural language inputs, the words and their positions in the input text are some of the most important features. In this article, we introduce a framework based on a word-position matrix representation of text, linear feature transformations of the word-position matrices, and kernel functions constructed from the transformations. We consider two categories of transformations, one based on word similarities and the second on their positions, which can be applied simultaneously in the framework in an elegant way. We show how word and positional similarities obtained by applying previously proposed techniques, such as latent semantic analysis, can be incorporated as transformations in the framework. We also introduce novel ways to determine word and positional similarities. We further present efficient algorithms for computing kernel functions incorporating the transformations on the word-position matrices, and, more importantly, introduce a highly efficient method for prediction. The framework is particularly suitable to natural language disambiguation tasks where the aim is to select for a single word a particular property from a set of candidates based on the context of the word. We demonstrate the applicability of the framework to this type of tasks using context-sensitive spelling error correction on the Reuters News corpus as a model problem.", "paper_title": "Matrix representations, linear transformations, and kernels for disambiguation in natural language", "paper_id": "WOS:000263062800002"}