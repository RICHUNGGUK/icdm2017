{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "distant_microphone_speech_recognition_systems"}, {"score": 0.0047283116134926645, "phrase": "human-like_robustness"}, {"score": 0.004664352260912174, "phrase": "distant_goal"}, {"score": 0.00460125407969429, "phrase": "key_difficulty"}, {"score": 0.004497973025639345, "phrase": "everyday_listening_conditions"}, {"score": 0.004417012437860011, "phrase": "speech_signal"}, {"score": 0.004278807087784626, "phrase": "noise_background"}, {"score": 0.0042209023469249205, "phrase": "multiple_competing_sound_sources"}, {"score": 0.0041074234667687875, "phrase": "recent_speech_recognition_evaluation"}, {"score": 0.003942877273973905, "phrase": "multiple_communities"}, {"score": 0.00385431710959668, "phrase": "novel_approaches"}, {"score": 0.003584025547693833, "phrase": "audio_backgrounds"}, {"score": 0.003503497128638099, "phrase": "busy_domestic_environment"}, {"score": 0.0033630629374009607, "phrase": "essential_difficulties"}, {"score": 0.0033175095965171674, "phrase": "multisource_environment_problem"}, {"score": 0.0031271133933467575, "phrase": "wide_audience"}, {"score": 0.0030707512751243214, "phrase": "previous_asr_evaluations"}, {"score": 0.0030429511467945525, "phrase": "particular_novelty"}, {"score": 0.0028422946420821075, "phrase": "continuous_audio_background"}, {"score": 0.0027910516355928983, "phrase": "pre-segmented_utterances"}, {"score": 0.0027159094186967247, "phrase": "background_modelling_techniques"}, {"score": 0.0026188490199688013, "phrase": "thirteen_submissions"}, {"score": 0.002548331088496964, "phrase": "challenge_problem"}, {"score": 0.0021632743048552536, "phrase": "future_such_evaluations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Speech recognition", " Source separation", " Noise robustness"], "paper_abstract": "Distant microphone speech recognition systems that operate with human-like robustness remain a distant goal. The key difficulty is that operating in everyday listening conditions entails processing a speech signal that is reverberantly mixed into a noise background composed of multiple competing sound sources. This paper describes a recent speech recognition evaluation that was designed to bring together researchers from multiple communities in order to foster novel approaches to this problem. The task was to identify keywords from sentences reverberantly mixed into audio backgrounds binaurally recorded in a busy domestic environment. The challenge was designed to model the essential difficulties of the multisource environment problem while remaining on a scale that would make it accessible to a wide audience. Compared to previous ASR evaluations a particular novelty of the task is that the utterances to be recognised were provided in a continuous audio background rather than as pre-segmented utterances thus allowing a range of background modelling techniques to be employed. The challenge attracted thirteen submissions. This paper describes the challenge problem, provides an overview of the systems that were entered and provides a comparison alongside both a baseline recognition system and human performance. The paper discusses insights gained from the challenge and lessons learnt for the design of future such evaluations. (c) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "The PASCAL CHiME speech separation and recognition challenge", "paper_id": "WOS:000314741500002"}