{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "linear_algebra_libraries"}, {"score": 0.033946973968416505, "phrase": "blocked_algorithms"}, {"score": 0.0046593017130868, "phrase": "future_many-core_processors"}, {"score": 0.004123781267600542, "phrase": "data_locality"}, {"score": 0.004066076430818046, "phrase": "better_load_balance"}, {"score": 0.003990385737207714, "phrase": "careful_attention"}, {"score": 0.003934540154108188, "phrase": "critical_path"}, {"score": 0.00375395061615019, "phrase": "existing_serial_libraries"}, {"score": 0.003649664279047831, "phrase": "flame"}, {"score": 0.003531474887552889, "phrase": "smpss_tools"}, {"score": 0.003401148484767927, "phrase": "run-time_system"}, {"score": 0.0033377940531761985, "phrase": "lapack_case"}, {"score": 0.0031845067058084583, "phrase": "simple_blas-level_operations"}, {"score": 0.003081400693055518, "phrase": "finer_grain"}, {"score": 0.003038237581227068, "phrase": "better_performance"}, {"score": 0.002765490759189057, "phrase": "block_data_layout"}, {"score": 0.002688532127821979, "phrase": "deeper_rewrite"}, {"score": 0.002663362811454526, "phrase": "lapack"}, {"score": 0.0025409639091476363, "phrase": "storage_pattern"}, {"score": 0.0024586427394152196, "phrase": "flame_routines"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["linear algebra libraries", " programmability", " high performance", " dynamic scheduling", " multi-core processors"], "paper_abstract": "The promise of future many-core processors, with hundreds of threads running concurrently, has led the developers of linear algebra libraries to rethink their design in order to extract more parallelism, further exploit data locality, attain better load balance, and pay careful attention to the critical path of computation. In this paper we describe how existing serial libraries such as (C)LAPACK and FLAME can be easily parallelized using the SMPSs tools, consisting of a few OpenMP-like pragmas and a run-time system. In the LAPACK case, this usually requires the development of blocked algorithms for simple BLAS-level operations, which expose concurrency at a finer grain. For better performance, our experimental results indicate that column-major order, as employed by this library, needs to be abandoned in benefit of a block data layout. This will require a deeper rewrite of LAPACK or, alternatively, a dynamic conversion of the storage pattern at run-time. The parallelization of FLAME routines using SMPSs is simpler as this library includes blocked algorithms (or algorithms-by-blocks in the FLAME argot) for most operations and storage-by-blocks (or block data layout) is already in place. Copyright (C) 2009 John Wiley & Sons, Ltd.", "paper_title": "Parallelizing dense and banded linear algebra libraries using SMPSs", "paper_id": "WOS:000272417100008"}