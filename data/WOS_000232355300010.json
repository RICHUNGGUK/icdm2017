{"auto_keywords": [{"score": 0.015236319355575356, "phrase": "feature_selection"}, {"score": 0.010572798575593415, "phrase": "text_categorization"}, {"score": 0.009622179265912804, "phrase": "koller"}, {"score": 0.00481495049065317, "phrase": "machine_learning-based_text_categorization"}, {"score": 0.004568108391981922, "phrase": "high_dimensionality"}, {"score": 0.004496542293425754, "phrase": "feature_space"}, {"score": 0.0038801697284045864, "phrase": "bayesian_network_classifiers"}, {"score": 0.0037005154613244363, "phrase": "new_information_gain"}, {"score": 0.0036617315529835497, "phrase": "divergence-based_feature_selection_method"}, {"score": 0.0035853745478488254, "phrase": "learning-based_text_categorization"}, {"score": 0.003312901888725435, "phrase": "information_gain"}, {"score": 0.0032609360162842767, "phrase": "appropriate_features"}, {"score": 0.0031929098985796814, "phrase": "empirical_results"}, {"score": 0.0029038817737006405, "phrase": "sahami's_method"}, {"score": 0.00284358652405718, "phrase": "d."}, {"score": 0.0026830601040919166, "phrase": "optimal_feature_selection"}, {"score": 0.0025722202862084186, "phrase": "international_conference"}, {"score": 0.0025452314655691165, "phrase": "machine_learning"}, {"score": 0.0024529757621396717, "phrase": "greedy_feature_selection_methods"}, {"score": 0.0024144659350277954, "phrase": "conventional_information_gain"}, {"score": 0.002207363758847308, "phrase": "conventional_machine_learning_algorithms"}, {"score": 0.00218419478369074, "phrase": "support_vector_machines"}, {"score": 0.0021049977753042253, "phrase": "best_classification_accuracy"}], "paper_keywords": ["text categorization", " feature selection", " information gain and divergence-based feature selection"], "paper_abstract": "Most previous works of feature selection emphasized only the reduction of high dimensionality of the feature space. But in cases where many features are highly redundant with each other, we must utilize other means, for example, more complex dependence models such as Bayesian network classifiers. In this paper, we introduce a new information gain and divergence-based feature selection method for statistical machine learning-based text categorization without relying on more complex dependence models. Our feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization. Empirical results are given on a number of dataset, showing that our feature selection method is more effective than Koller and Sahami's method [Koller, D., & Sahami, M. (1996). Toward optimal feature selection. In Proceedings of ICML-96, 13th international conference on machine learning], which is one of greedy feature selection methods, and conventional information gain which is commonly used in feature selection for text categorization. Moreover, our feature selection method sometimes produces more improvements of conventional machine learning algorithms over support vector machines which are known to give the best classification accuracy. (c) 2004 Elsevier Ltd. All rights reserved.", "paper_title": "Information gain and divergence-based feature selection for machine learning-based text categorization", "paper_id": "WOS:000232355300010"}