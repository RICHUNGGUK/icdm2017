{"auto_keywords": [{"score": 0.046030203640502004, "phrase": "multimodal_data"}, {"score": 0.015380848847530482, "phrase": "latent_joint_representation"}, {"score": 0.007229986196940622, "phrase": "cross-media_retrieval"}, {"score": 0.00481495049065317, "phrase": "cross-modal_learning"}, {"score": 0.004658426838467567, "phrase": "cross-modal_ranking"}, {"score": 0.0045820747375409435, "phrase": "research_topic"}, {"score": 0.004312622975366676, "phrase": "joint_representation"}, {"score": 0.004172360444377281, "phrase": "ranking_function"}, {"score": 0.0032198357970567595, "phrase": "image_query"}, {"score": 0.0031669873302916, "phrase": "text_document"}, {"score": 0.003097865379563127, "phrase": "conditional_random_field"}, {"score": 0.0030638704739988595, "phrase": "structural_learning"}, {"score": 0.0030135741897058844, "phrase": "listwise_ranking_manner"}, {"score": 0.002931569771634119, "phrase": "approach_cross-modal_learning"}, {"score": 0.0024569466759289055, "phrase": "hidden-topic-driven_discriminative_ranking_function"}, {"score": 0.0022741894748181243, "phrase": "proposed_approach"}, {"score": 0.0022368274529152342, "phrase": "good_performance"}, {"score": 0.0021049977753042253, "phrase": "discriminative_representation"}], "paper_keywords": ["Cross-modal ranking", " latent joint representation", " learning to rank"], "paper_abstract": "Cross-modal ranking is a research topic that is imperative to many applications involving multimodal data. Discovering a joint representation for multimodal data and learning a ranking function are essential in order to boost the cross-media retrieval (i.e., image-query-text or text-query-image). In this paper, we propose an approach to discover the latent joint representation of pairs of multimodal data (e.g., pairs of an image query and a text document) via a conditional random field and structural learning in a listwise ranking manner. We call this approach cross-modal learning to rank via latent joint representation ((CMLR)-R-2). In (CMLR)-R-2, the correlations between multimodal data are captured in terms of their sharing hidden variables (e.g., topics), and a hidden-topic-driven discriminative ranking function is learned in a listwise ranking manner. The experiments show that the proposed approach achieves a good performance in cross-media retrieval and meanwhile has the capability to learn the discriminative representation of multimodal data.", "paper_title": "Cross-Modal Learning to Rank via Latent Joint Representation", "paper_id": "WOS:000351405700005"}