{"auto_keywords": [{"score": 0.04900761970662832, "phrase": "stochastic_systems"}, {"score": 0.00481495049065317, "phrase": "decentralized_control"}, {"score": 0.004299621153829837, "phrase": "decentralized_control_policies"}, {"score": 0.004109212945115815, "phrase": "finite_state_and_action_spaces"}, {"score": 0.003927203683176278, "phrase": "optimal_decentralized_policies"}, {"score": 0.0030606302942851027, "phrase": "meaningful_suboptimal"}, {"score": 0.00302612773666719, "phrase": "decentralized'_control_policies"}, {"score": 0.0026411384812121503, "phrase": "optimal_q-functions"}, {"score": 0.0024673738215365104, "phrase": "performance_loss"}, {"score": 0.0023579143175099324, "phrase": "decentralized_policies"}, {"score": 0.0022278779300543548, "phrase": "approximate_q-function"}, {"score": 0.0021049977753042253, "phrase": "approximation_error"}], "paper_keywords": [""], "paper_abstract": "We consider the problem of computing decentralized control policies for stochastic systems with finite state and action spaces. Synthesis of optimal decentralized policies for such problems is known to be NP-hard [1]. Here we focus on methods for efficiently computing meaningful suboptimal decentralized' control policies. The algorithms we present here are based on approximation of optimal Q-functions. We show that the performance loss associated with choosing decentralized policies with respect to an approximate Q-function is related to the approximation error.", "paper_title": "An approximate dynamic programming approach to decentralized control of stochastic systems", "paper_id": "WOS:000236901900013"}