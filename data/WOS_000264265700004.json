{"auto_keywords": [{"score": 0.049708716733180815, "phrase": "locally-weighted_regression"}, {"score": 0.03507772760615425, "phrase": "projection_directions"}, {"score": 0.03296780395043166, "phrase": "pca"}, {"score": 0.024824672782657146, "phrase": "pls"}, {"score": 0.00481495049065317, "phrase": "non-parametric_regression"}, {"score": 0.004724980378009193, "phrase": "computationally-efficient_technique"}, {"score": 0.004689462599037994, "phrase": "non-linear_regression"}, {"score": 0.0046018264570209765, "phrase": "high-dimensional_data"}, {"score": 0.004251210982670317, "phrase": "local_linear_dimensionality_reduction"}, {"score": 0.004124754315882517, "phrase": "promising_solution"}, {"score": 0.004002044115117644, "phrase": "linear_dimensionality-reduction_methods"}, {"score": 0.0039124037252916055, "phrase": "non-parametric_locally-linear_regression"}, {"score": 0.00378167947598949, "phrase": "incremental_learning"}, {"score": 0.0037390789999636764, "phrase": "considered_methods"}, {"score": 0.003546513036085932, "phrase": "input_data"}, {"score": 0.0034539685987677376, "phrase": "joint_input-output_data_distribution"}, {"score": 0.0033008858616145205, "phrase": "output_data"}, {"score": 0.003239114833599948, "phrase": "principal_component_regression"}, {"score": 0.003142668996712078, "phrase": "principal_component_analysis"}, {"score": 0.003083849594367653, "phrase": "joint_input"}, {"score": 0.0030606302942851027, "phrase": "output_space"}, {"score": 0.0030375852871252934, "phrase": "factor_analysis"}, {"score": 0.0029249294679606656, "phrase": "reduced_rank_regression"}, {"score": 0.002902919658146248, "phrase": "rrr"}, {"score": 0.002870173628745123, "phrase": "partial_least_squares"}, {"score": 0.0027846824619356583, "phrase": "tested_methods"}, {"score": 0.0027017308406240563, "phrase": "robust_performance"}, {"score": 0.0026611838252683507, "phrase": "non-optimal_number"}, {"score": 0.0024120247307686084, "phrase": "correct_estimate"}, {"score": 0.0023848164282879885, "phrase": "true_intrinsic_dimensionality"}, {"score": 0.002129020447793464, "phrase": "building_block"}, {"score": 0.0021049977753042253, "phrase": "locally-weighted_regressor"}], "paper_keywords": ["Correlation", " Dimensionality reduction", " Factor analysis", " Incremental learning", " Kernel function", " Locally-weighted regression", " Partial least squares", " Principal component analysis", " Principal component regression", " Reduced-rank regression"], "paper_abstract": "Locally-weighted regression is a computationally-efficient technique for non-linear regression. However, for high-dimensional data, this technique becomes numerically brittle and computationally too expensive if many local models need to be maintained simultaneously. Thus, local linear dimensionality reduction combined with locally-weighted regression seems to be a promising solution. In this context, we review linear dimensionality-reduction methods, compare their performance on non-parametric locally-linear regression, and discuss their ability to extend to incremental learning. The considered methods belong to the following three groups: (1) reducing dimensionality only on the input data, (2) modeling the joint input-output data distribution, and (3) optimizing the correlation between projection directions and output data. Group 1 contains principal component regression (PCR); group 2 contains principal component analysis (PCA) in joint input and output space, factor analysis, and probabilistic PCA; and group 3 contains reduced rank regression (RRR) and partial least squares (PLS) regression. Among the tested methods, only group 3 managed to achieve robust performance even for a non-optimal number of components (factors or projection directions). In contrast, group 1 and 2 failed for fewer components since these methods rely on the correct estimate of the true intrinsic dimensionality. In group 3, PLS is the only method for which a computationally-efficient incremental implementation exists. Thus, PLS appears to be ideally suited as a building block for a locally-weighted regressor in which projection directions are incrementally added on the fly.", "paper_title": "Local Dimensionality Reduction for Non-Parametric Regression", "paper_id": "WOS:000264265700004"}