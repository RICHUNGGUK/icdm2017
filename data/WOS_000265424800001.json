{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gesture_recognition"}, {"score": 0.00474827840031692, "phrase": "image_sequence"}, {"score": 0.00464998919563789, "phrase": "core_problem"}, {"score": 0.004367108655796158, "phrase": "perfect_segmentation"}, {"score": 0.004232160987948483, "phrase": "low-_and_mid-level_processes"}, {"score": 0.004002426645789667, "phrase": "uncertain_information"}, {"score": 0.003946960862020283, "phrase": "intermediate_grouping_module"}, {"score": 0.003905864553426306, "phrase": "multiple_candidates"}, {"score": 0.0038249462699142733, "phrase": "low-level_image_primitives"}, {"score": 0.0037719307016238998, "phrase": "constant_color_region_patches"}, {"score": 0.003480993634130542, "phrase": "low-level_cues"}, {"score": 0.003444732262403694, "phrase": "region_shape"}, {"score": 0.003314965171168532, "phrase": "underlying_object_parts"}, {"score": 0.0031678720736808574, "phrase": "frame-wise_group_hypotheses"}, {"score": 0.0030378759123133644, "phrase": "minimization_problem"}, {"score": 0.00292339061443511, "phrase": "non-statistical_matching"}, {"score": 0.0028828348726682965, "phrase": "sample-based_modeling"}, {"score": 0.002832928222306059, "phrase": "statistical_matching"}, {"score": 0.002803398730609738, "phrase": "hmm_models"}, {"score": 0.0027071661844502992, "phrase": "matching_score"}, {"score": 0.002651015392318126, "phrase": "best_group"}, {"score": 0.0026233771102489416, "phrase": "image_frame"}, {"score": 0.0026051114364288126, "phrase": "i.e._recognition"}, {"score": 0.002586972610351776, "phrase": "final_segmentation"}, {"score": 0.0023540798554571724, "phrase": "hard_task"}, {"score": 0.002289180040240783, "phrase": "sign_language_recognition"}, {"score": 0.002218299213266519, "phrase": "ground_truth_hand_groups"}, {"score": 0.0021049977753042253, "phrase": "sports_video_dataset"}], "paper_keywords": ["Grouping and segmentation", " Gesture recognition", " American Sign Language recognition", " Hidden Markov Models"], "paper_abstract": "Matching an image sequence to a model is a core problem in gesture or sign recognition. In this paper, we consider such a matching problem, without requiring a perfect segmentation of the scene. Instead of requiring that low- and mid-level processes produce near-perfect segmentation, we take into account that such processes can only produce uncertain information and use an intermediate grouping module to generate Multiple candidates. From the set of low-level image primitives, such as constant color region patches found in each image, a ranked set of salient, overlapping, groups of these primitives are formed, based on low-level cues such as region shape, proximity, or color. These groups corresponds to underlying object parts of interest, such as the hands. The sequence of these frame-wise group hypotheses are then matched to a model by casting it into a minimization problem. We show the coupling of these hypotheses with both non-statistical matching (match to sample-based modeling of signs) and statistical matching (match to HMM models) are possible. Our algorithm not only produces a matching score, but also selects the best group in each image frame, i.e. recognition and final segmentation of the scene are coupled. In addition, there is no need for tracking of features across sequences, which is known to be a hard task. We demonstrate Our method using data from sign language recognition and gesture recognition, we compare our results with the ground truth hand groups, and achieved less than 5% performance loss for both two models. We also tested our algorithm on a sports video dataset that has moving background. (C) 2008 Elsevier Inc. All rights reserved.", "paper_title": "Coupled grouping and matching for sign and gesture recognition", "paper_id": "WOS:000265424800001"}