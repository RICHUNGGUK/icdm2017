{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "proactive_leader_election"}, {"score": 0.004746263721902201, "phrase": "memory_systems"}, {"score": 0.004562362497952548, "phrase": "fault-tolerant_proactive_leader_election"}, {"score": 0.0042920078348374375, "phrase": "leader_election_algorithm"}, {"score": 0.0036247638406450735, "phrase": "explicit_events"}, {"score": 0.003373181367365139, "phrase": "individual_nodes"}, {"score": 0.003028067500715357, "phrase": "maximum_drift"}, {"score": 0.002995554770117301, "phrase": "different_nodes"}, {"score": 0.002952745241052452, "phrase": "different_epochs"}, {"score": 0.0027675275077806744, "phrase": "counter_values"}, {"score": 0.002493184318829227, "phrase": "highest_counter"}, {"score": 0.0022058958906038466, "phrase": "clustered_shared_disk_systems"}, {"score": 0.002174346780014853, "phrase": "primary_network_partition"}, {"score": 0.0021278653331014614, "phrase": "state_machine_approach"}], "paper_keywords": [""], "paper_abstract": "In this paper, we give an algorithm for fault-tolerant proactive leader election in asynchronous shared memory systems, and later its formal verification. Roughly speaking, a leader election algorithm is proactive if it can tolerate failure of nodes even after a leader is elected, and (stable) leader election happens periodically. This is needed in systems where a leader is required after every failure to ensure the availability of the system and there might be no explicit events such as messages in the (shared memory) system. Previous algorithms like DiskPaxos[1] are not proactive. In our model, individual nodes can fail and reincarnate at any point in time. Each node has a counter which is incremented every period, which is same across all the nodes (modulo a maximum drift). Different nodes can be in different epochs at the same time. Our algorithm ensures that per epoch there can be at most one leader. So if the counter values of some set of nodes match, then there can be at most one leader among them. If the nodes satisfy certain timeliness constraints, then the leader for the epoch with highest counter also becomes the leader for the next epoch(stable property). Our algorithm uses shared memory proportional to the number of processes, the best possible. We also show how our protocol can be used in clustered shared disk systems to select a primary network partition. We have used the state machine approach to represent our protocol in Isabelle HOL[3] logic system and have proved the safety property of the protocol.", "paper_title": "Proactive leader election in asynchronous shared memory systems", "paper_id": "WOS:000241648200029"}