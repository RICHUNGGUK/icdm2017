{"auto_keywords": [{"score": 0.02246846748295755, "phrase": "hmm"}, {"score": 0.00481495049065317, "phrase": "audio-visual_affective_expression_recognition"}, {"score": 0.004639712384096822, "phrase": "computer_processing_power"}, {"score": 0.004590820632326585, "phrase": "emerging_algorithms"}, {"score": 0.004518442937844318, "phrase": "new_ways"}, {"score": 0.004447201233380302, "phrase": "human-computer_interaction"}, {"score": 0.0043080553835735825, "phrase": "audio-visual_fusion"}, {"score": 0.004217708394188058, "phrase": "affect_recognition"}, {"score": 0.004151188559520802, "phrase": "psychological_and_engineering_perspectives"}, {"score": 0.004064117897000019, "phrase": "existing_approaches"}, {"score": 0.004021267064832957, "phrase": "automatic_human_affect_analysis"}, {"score": 0.0038953958723049287, "phrase": "computer_system"}, {"score": 0.0037935066882208235, "phrase": "face_images"}, {"score": 0.0037336520904376687, "phrase": "speech_signals"}, {"score": 0.0035408501923902477, "phrase": "computing_algorithm"}, {"score": 0.003466536738232506, "phrase": "audio_and_visual_sensors"}, {"score": 0.0033579708256295847, "phrase": "user's_affective_state"}, {"score": 0.003304965389155549, "phrase": "computer_decision_making"}, {"score": 0.0031342307827666675, "phrase": "coupled_audio_and_visual_streams"}, {"score": 0.0026168641536413978, "phrase": "mfhmm"}, {"score": 0.0025348408238837655, "phrase": "optimal_connection"}, {"score": 0.002508073621696602, "phrase": "multiple_streams"}, {"score": 0.0024553821147953463, "phrase": "maximum_entropy_principle"}, {"score": 0.0023532890136269986, "phrase": "person-independent_experimental_results"}, {"score": 0.0022434880661725493, "phrase": "mfhmm_approach"}, {"score": 0.002219790619936024, "phrase": "face-only_hmm"}], "paper_keywords": ["affective computing", " affect recognition", " emotion recognition", " human-computer interaction", " human computing", " multimodal fusion"], "paper_abstract": "Advances in computer processing power and emerging algorithms are allowing new ways of envisioning Human-Computer Interaction. Although the benefit of audio-visual fusion is expected for affect recognition from the psychological and engineering perspectives, most of existing approaches to automatic human affect analysis are unimodal: information processed by computer system is limited to either face images or the speech signals. This paper focuses on the development of a computing algorithm that uses both audio and visual sensors to detect and track a user's affective state to aid computer decision making. Using our Multistream Fused Hidden Markov Model (MFHMM), we analyzed coupled audio and visual streams to detect four cognitive states (interest, boredom, frustration and puzzlement) and seven prototypical emotions (neural, happiness, sadness, anger, disgust, fear and surprise). The MFHMM allows the building of an optimal connection among multiple streams according to the maximum entropy principle and the maximum mutual information criterion. Person-independent experimental results from 20 subjects in 660 sequences show that the MFHMM approach outperforms face-only HMM, pitch-only HMM, energy-only HMM, and independent HMM fusion, tinder clean and varying audio channel noise condition.", "paper_title": "Audio-visual affective expression recognition through multistream fused HMM", "paper_id": "WOS:000258767200002"}