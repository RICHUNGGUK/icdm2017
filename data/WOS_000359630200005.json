{"auto_keywords": [{"score": 0.04775741297060809, "phrase": "high_performance"}, {"score": 0.025098904315951662, "phrase": "geometric_mean"}, {"score": 0.00481495049065317, "phrase": "graphics_processing_unit_performance"}, {"score": 0.004728780808284175, "phrase": "multiprocessors_locality"}, {"score": 0.004623224002010226, "phrase": "new_platform"}, {"score": 0.0045404698551856125, "phrase": "general-purpose_computing"}, {"score": 0.004419431539962814, "phrase": "gpu"}, {"score": 0.004262273304740847, "phrase": "faster_improvement"}, {"score": 0.004223941453664225, "phrase": "peak_processing_speed"}, {"score": 0.00418595288276353, "phrase": "low_latency"}, {"score": 0.00391167729353583, "phrase": "small_private_data_cache"}, {"score": 0.0038590090616766434, "phrase": "single_instruction_multiple_thread"}, {"score": 0.0036060781902674207, "phrase": "global_memory"}, {"score": 0.003525493807130663, "phrase": "public_memory"}, {"score": 0.0034937647771945803, "phrase": "long_time"}, {"score": 0.0034467039994594065, "phrase": "large_amount"}, {"score": 0.0033393377529146893, "phrase": "memory_bandwidth"}, {"score": 0.003220709449062307, "phrase": "parallel_processing"}, {"score": 0.0031203609829058587, "phrase": "last_level_cache"}, {"score": 0.0030094882299407256, "phrase": "slow_off-chip_memory_harm_power"}, {"score": 0.0028376361457661415, "phrase": "light_overhead_mechanism"}, {"score": 0.0027993885490232677, "phrase": "off-chip_memory_requests"}, {"score": 0.0027367816003722252, "phrase": "miss_events"}, {"score": 0.002712131567923321, "phrase": "on-chip_caches"}, {"score": 0.002639502113350897, "phrase": "cluster-based_architecture"}, {"score": 0.002568812627436125, "phrase": "memory_requests"}, {"score": 0.0025456716198597627, "phrase": "simt_cores"}, {"score": 0.002488724764295623, "phrase": "missed_requests"}, {"score": 0.002466303439899648, "phrase": "adjacent_cores"}, {"score": 0.002389403849342894, "phrase": "proposed_architecture"}, {"score": 0.0022733662000757318, "phrase": "evaluated_benchmarks"}, {"score": 0.002232579371411546, "phrase": "maximum_gain"}, {"score": 0.0021434588322608653, "phrase": "total_energy_consumption_overhead"}, {"score": 0.0021049977753042253, "phrase": "evaluated_applications"}], "paper_keywords": [""], "paper_abstract": "Owing to a new platform for high performance and general-purpose computing, graphics processing unit (GPU) is one of the most promising candidates for faster improvement in peak processing speed, low latency and high performance. As GPUs employ multithreading to hide latency, there is a small private data cache in each single instruction multiple thread (SIMT) core. Hence, these cores communicate in many applications through the global memory. Access to this public memory takes long time and consumes large amount of power. Moreover, the memory bandwidth is limited which is quite challenging in parallel processing. The missed memory requests in last level cache that are followed by accesses to the slow off-chip memory harm power and performance significantly. In this research, the authors introduce a light overhead mechanism to reduce off-chip memory requests which are triggering by miss events in on-chip caches. The authors propose a cluster-based architecture to capture the similarity of memory requests between SIMT cores and provide data for missed requests by adjacent cores. Simulation results reveal that the proposed architecture enhances the geometric mean of instructions per cycle by 6.3% for evaluated benchmarks, whereas the maximum gain is 22%. Furthermore, the geometric mean of total energy consumption overhead is 4.8% for evaluated applications.", "paper_title": "Cluster-based approach for improving graphics processing unit performance by inter streaming multiprocessors locality", "paper_id": "WOS:000359630200005"}