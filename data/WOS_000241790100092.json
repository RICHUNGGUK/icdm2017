{"auto_keywords": [{"score": 0.0250125371583897, "phrase": "flake"}, {"score": 0.012574642420185793, "phrase": "lawrence"}, {"score": 0.00481495049065317, "phrase": "support_vector_regression"}, {"score": 0.003384262486955773, "phrase": "standard_quadratic_programming_problems"}, {"score": 0.00303319936111073, "phrase": "training_samples"}, {"score": 0.0027854565462509095, "phrase": "set_selection"}, {"score": 0.0026209522797963447, "phrase": "experimental_results"}, {"score": 0.0024963543716522087, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "fastest_conventional_smo."}], "paper_keywords": [""], "paper_abstract": "A novel sequential minimal optimization (SMO) algorithm for support vector regression is proposed. This algorithm is based on Flake and Lawrence's SMO in which convex optimization problems with iota variables are solved instead of standard quadratic programming problems with 2 iota variables where iota is the number of training samples, but the strategy for working set selection is quite different. Experimental results show that the proposed algorithm is much faster than Flake and Lawrence's SMO and comparable to the fastest conventional SMO.", "paper_title": "A novel sequential minimal optimization algorithm for support vector regression", "paper_id": "WOS:000241790100092"}