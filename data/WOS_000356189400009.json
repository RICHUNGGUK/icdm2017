{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "data_compression"}, {"score": 0.03783260282574376, "phrase": "end-to-end_data_transfer"}, {"score": 0.004775531918452571, "phrase": "wide-area_networks"}, {"score": 0.00471700544744796, "phrase": "recent_emergence"}, {"score": 0.004678384952779725, "phrase": "ultra_high-speed_networks"}, {"score": 0.004545675105321216, "phrase": "numerous_challenges"}, {"score": 0.004416713079266374, "phrase": "efficient_protocols"}, {"score": 0.004291393968030603, "phrase": "end-to-end_data_transfers"}, {"score": 0.004152502093643003, "phrase": "overall_transfer_performance"}, {"score": 0.004034649282040889, "phrase": "additional_computation"}, {"score": 0.003985166330358716, "phrase": "data_streams"}, {"score": 0.0038403151693333017, "phrase": "achievable_bandwidth"}, {"score": 0.0038089685486220526, "phrase": "tcp"}, {"score": 0.0032174507570710835, "phrase": "maximum_data_transfer_throughput"}, {"score": 0.0031910685060552485, "phrase": "parallel_data_flows"}, {"score": 0.0031132086055691214, "phrase": "variable_data_flow"}, {"score": 0.0031004173389196387, "phrase": "gridffp_xio"}, {"score": 0.0030497740719977835, "phrase": "data_transfer"}, {"score": 0.00292674497467906, "phrase": "data_transfers"}, {"score": 0.002651291854234327, "phrase": "wide-area_cluster-to-cluster_testbed"}, {"score": 0.0023624435546097658, "phrase": "larger_size_files"}, {"score": 0.002295277579105179, "phrase": "cluster-to-cluster_testbed_show"}, {"score": 0.0021576973674470997, "phrase": "normal_cluster_data_transfer"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Disk-to-disk data transfer", " System modeling", " Optimization", " High-speed networks"], "paper_abstract": "The recent emergence of ultra high-speed networks up to 100 Gb/s has posed numerous challenges and has led to many investigations on efficient protocols to saturate 100 Gb/s links. However, end-to-end data transfers involve many components, not only protocols, affecting overall transfer performance. These components include disk I/O subsystem, additional computation associated with data streams, and network adapters. For example, achievable bandwidth by TCP may not be implementable if disk I/O or CPU becomes a bottleneck in end-to-end data transfer. In this paper, we first model all the system components involved in end-to-end data transfer as a graph. We then formulate the problem whose goal is to achieve maximum data transfer throughput using parallel data flows. We also propose a variable data flow GridFFP XIO stack to improve data transfer with data compression. Our contributions lie in how to optimize data transfers considering all the system components involved rather than in accurately modeling all the system components involved. Our proposed formulations and solutions are evaluated through experiments on the ESnet 100G testbed and a wide-area cluster-to-cluster testbed. The experimental results on the ESnet 100G testbed show that our approach is several times faster than Globus Online-8 x faster for datasets with many 10 MB files and 3-4 x faster for other datasets of larger size files. The experimental results on the cluster-to-cluster testbed show that our variable data flow approach is up to 4 x faster than a normal cluster data transfer. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Cluster-to-cluster data transfer with data compression over wide-area networks", "paper_id": "WOS:000356189400009"}