{"auto_keywords": [{"score": 0.03414229704911109, "phrase": "kp"}, {"score": 0.00481495049065317, "phrase": "kernel_matrix_learning"}, {"score": 0.004778258911940497, "phrase": "kernel_propagation"}, {"score": 0.004687742829233727, "phrase": "semisupervised_kernel_matrix_learning"}, {"score": 0.004511799033574768, "phrase": "kernel_matrix"}, {"score": 0.004359078118356132, "phrase": "class_label"}, {"score": 0.0043258453436258405, "phrase": "pairwise_constraint"}, {"score": 0.004227653047394939, "phrase": "extensive_research"}, {"score": 0.0038271712887391015, "phrase": "ss-kml"}, {"score": 0.003613556291453534, "phrase": "pcps_scalability"}, {"score": 0.0034911335714333507, "phrase": "novel_algorithm"}, {"score": 0.003308862957060602, "phrase": "comprehensive_performance"}, {"score": 0.0032836104153151973, "phrase": "ss-kml."}, {"score": 0.0032585499655442404, "phrase": "main_idea"}, {"score": 0.003160200215473349, "phrase": "small-sized_sub-kernel_matrix"}, {"score": 0.0029951555610569225, "phrase": "larger-sized_full-kernel_matrix"}, {"score": 0.0028170314398963704, "phrase": "supervised_sample"}, {"score": 0.0027215202678634517, "phrase": "full_sample_set_x"}, {"score": 0.0026596472456182707, "phrase": "seed-kernel_matrix"}, {"score": 0.0025892330566358503, "phrase": "small-scale_sdp_problem"}, {"score": 0.0025206783724309252, "phrase": "learnt_seed-kernel_matrix"}, {"score": 0.0024918548761791435, "phrase": "full-kernel_matrix"}, {"score": 0.0023435925848395768, "phrase": "kml"}, {"score": 0.0023079225317714815, "phrase": "batch-style_extension"}, {"score": 0.002255431275670053, "phrase": "online-style_extension"}], "paper_keywords": ["Kernel propagation", " out-of-sample extension", " pairwise constraint", " seed-kernel matrix learning", " semidefinite programming"], "paper_abstract": "The goal of semisupervised kernel matrix learning (SS-KML) is to learn a kernel matrix on all the given samples on which just a little supervised information, such as class label or pairwise constraint, is provided. Despite extensive research, the performance of SS-KML still leaves some space for improvement in terms of effectiveness and efficiency. For example, a recent pairwise constraints propagation (PCP) algorithm has formulated SS-KML into a semidefinite programming (SDP) problem, but its computation is very expensive, which undoubtedly restricts PCPs scalability in practice. In this paper, a novel algorithm, called kernel propagation (KP), is proposed to improve the comprehensive performance in SS-KML. The main idea of KP is first to learn a small-sized sub-kernel matrix (named seed-kernel matrix) and then propagate it into a larger-sized full-kernel matrix. Specifically, the implementation of KP consists of three stages: 1) separate the supervised sample (sub) set X(l) from the full sample set X; 2) learn a seed-kernel matrix on X(l) through solving a small-scale SDP problem; and 3) propagate the learnt seed-kernel matrix into a full-kernel matrix on X. Furthermore, following the idea in KP, we naturally develop two conveniently realizable out-of-sample extensions for KML: one is batch-style extension, and the other is online-style extension. The experiments demonstrate that KP is encouraging in both effectiveness and efficiency compared with three state-of-the-art algorithms and its related out-of-sample extensions are promising too.", "paper_title": "Semisupervised Kernel Matrix Learning by Kernel Propagation", "paper_id": "WOS:000283944100012"}