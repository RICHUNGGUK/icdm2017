{"auto_keywords": [{"score": 0.049450454172374314, "phrase": "vector_machine"}, {"score": 0.02322730111336098, "phrase": "least_squares"}, {"score": 0.004689321062844586, "phrase": "function_estimation"}, {"score": 0.004054479957160662, "phrase": "large_sample_data"}, {"score": 0.003845500924733243, "phrase": "kernel_hilbert_space"}, {"score": 0.0029903851294825023, "phrase": "small_equations"}, {"score": 0.002912222095383313, "phrase": "numerical_example"}, {"score": 0.002671990515821213, "phrase": "nonlinear_models"}, {"score": 0.002636828502928141, "phrase": "large_data_set"}, {"score": 0.0025509294021449254, "phrase": "common_least_squares"}, {"score": 0.0024192592458651204, "phrase": "sparse_solution"}, {"score": 0.002279217896185919, "phrase": "computing_speed"}, {"score": 0.0021903801614787423, "phrase": "final_result"}, {"score": 0.0021049977753042253, "phrase": "small-scale_equations"}], "paper_keywords": [""], "paper_abstract": "Sparse least squares support vector machine (SLS-SVM) for regression is proposed to solve the problem of regression for large sample data. The samples are mapped into Reproducing Kernel Hilbert Space (RKHS) and span a subspace there. Then we could find the basis of the subspace. The basis can represent all the samples linearly. So we can get the least squares support vector machine by solving a small equations set. A numerical example is used to illustrate that this approach can be used to fit nonlinear models for large data set. Being compared with common least squares support vector machine, this method can find sparse solution without any pruning or surgeon, and the computing speed is much faster because the final result is found by solving a small-scale equations set.", "paper_title": "Sparse least squares support vector machine for function estimation", "paper_id": "WOS:000238112000149"}