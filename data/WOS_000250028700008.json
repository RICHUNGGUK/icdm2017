{"auto_keywords": [{"score": 0.03271521283089865, "phrase": "eigenvalue_distribution"}, {"score": 0.024730964177972208, "phrase": "optimal_parameters"}, {"score": 0.004750440867604888, "phrase": "small_sample_size_problem"}, {"score": 0.004381057030851691, "phrase": "dcv"}, {"score": 0.004221450123456042, "phrase": "small_sample_size"}, {"score": 0.003738502710504415, "phrase": "rda's_regularization_parameter"}, {"score": 0.003541923233726252, "phrase": "close_relationships"}, {"score": 0.0034243271052114234, "phrase": "efficient_algorithms"}, {"score": 0.0033330581836898032, "phrase": "principal_component_analysis"}, {"score": 0.0031791333962016507, "phrase": "face_recognition"}, {"score": 0.0030220695120265974, "phrase": "formulated_eigenvalue_distribution"}, {"score": 0.0029614320534324104, "phrase": "wmmc's_projection_vectors"}, {"score": 0.0028824650336365465, "phrase": "underlying_methodology"}, {"score": 0.0024674880645446312, "phrase": "mean_standard_variance"}, {"score": 0.0023774292521631093, "phrase": "competitive_classification_performance"}, {"score": 0.0022370795358896784, "phrase": "lower_classification_accuracy"}, {"score": 0.0021336600569395693, "phrase": "elsevier_ltd"}, {"score": 0.0021049977753042253, "phrase": "pattern_recognition_society"}], "paper_keywords": ["regularized discriminant analysis (RDA)", " discriminant common vectors (DCV)", " maximal margin criterion (MMC)", " small sample size (SSS)", " eigenvalue distribution"], "paper_abstract": "In this paper, we make a study on three linear discriminant analysis (LDA) based methods: regularized discriminant analysis (RDA), discriminant common vectors (DCV) and maximal margin criterion (MMC) in the small sample size (SSS) problem. Our contributions are that: (1) we reveal that DCV obtains the same projection subspace as both RDA and wMMC (weighted MMC, a general form of MMQ when RDA's regularization parameter tends to zero and wMMC's weight parameter approaches to +infinity, which builds on close relationships among these three LDA based methods; (2) we offer efficient algorithms to perform RDA and wMMC in the principal component analysis transformed space, which makes them feasible and efficient to applications such as face recognition; (3) we formulate the eigenvalue distribution of wMMC. On one hand, the formulated eigenvalue distribution can guide practitioners in choosing wMMC's projection vectors, and on the other hand, the underlying methodology can be employed in analyzing the eigenvalue distribution of matrices such as AA(T) - BB(T), where the rows of A and B are far larger than their columns; and (4) we compare their classification performance on several benchmarks to get that, when the mean standard variance (MSV) criterion is small, DCV can obtain competitive classification performance to both RDA and wMMC under optimal parameters, but when MSV is large, DCV generally yields lower classification accuracy than RDA and wMMC under optimal parameters. (C) 2007 Published by Elsevier Ltd on behalf of Pattern Recognition Society.", "paper_title": "A study on three linear discriminant analysis based methods in small sample size problem", "paper_id": "WOS:000250028700008"}