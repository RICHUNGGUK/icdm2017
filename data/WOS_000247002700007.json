{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "feature_selection"}, {"score": 0.004733625412289433, "phrase": "pattern_recognition"}, {"score": 0.004653667500876187, "phrase": "polynomial_time"}, {"score": 0.004347057700479576, "phrase": "minimal_feature"}, {"score": 0.0034828730516958807, "phrase": "latter_problem"}, {"score": 0.0033660587983313536, "phrase": "recent_applications"}, {"score": 0.0032531496610470377, "phrase": "particularly_gene_expression_analysis"}, {"score": 0.0029871209444889716, "phrase": "data_distributions"}, {"score": 0.002413153346716919, "phrase": "distribution_classes"}, {"score": 0.0021412594173592513, "phrase": "wide_range"}, {"score": 0.0021049977753042253, "phrase": "machine_learning_tasks"}], "paper_keywords": ["learning theory", " relevance", " classification", " Markov blanket", " bioinformatics"], "paper_abstract": "We analyze two different feature selection problems: finding a minimal feature set optimal for classification ( MINIMAL-OPTIMAL) vs. finding all features relevant to the target variable ( ALL-RELEVANT). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL-RELEVANT is much harder than MINIMAL-OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks.", "paper_title": "Consistent feature selection for pattern recognition in polynomial time", "paper_id": "WOS:000247002700007"}