{"auto_keywords": [{"score": 0.0456315961504987, "phrase": "low-level_acoustic_features"}, {"score": 0.02748288969079361, "phrase": "fmri-measured_features"}, {"score": 0.00481495049065317, "phrase": "audio_content"}, {"score": 0.004779085953614184, "phrase": "functional_brain_response"}, {"score": 0.00474348728309194, "phrase": "effective_analysis"}, {"score": 0.004518442937844318, "phrase": "significant_attention"}, {"score": 0.004451360793577698, "phrase": "traditional_methods"}, {"score": 0.004320156581220516, "phrase": "digital_audio_stream"}, {"score": 0.004130535674260505, "phrase": "well-known_semantic_gap"}, {"score": 0.003978866233891427, "phrase": "novel_framework"}, {"score": 0.003719705944877787, "phrase": "functional_magnetic_resonance_imaging"}, {"score": 0.0035965032173588753, "phrase": "brain's_functional_response"}, {"score": 0.0034385353123700885, "phrase": "brain_networks"}, {"score": 0.003275197814021, "phrase": "audio_stimuli"}, {"score": 0.0031312981393997355, "phrase": "working_memory_systems"}, {"score": 0.0030617295196636173, "phrase": "new_approach"}, {"score": 0.0030388841099720843, "phrase": "dense_individualized_and_common_connectivity-based_cortical_landmarks"}, {"score": 0.0029381648092259064, "phrase": "functional_connectivity_matrix"}, {"score": 0.0028728747019026485, "phrase": "fmri_signals"}, {"score": 0.0028514344213899177, "phrase": "different_rois"}, {"score": 0.002788066194685293, "phrase": "brain's_comprehension"}, {"score": 0.00276725708403607, "phrase": "audio_semantics"}, {"score": 0.0026956374041441126, "phrase": "improved_twin_gaussian_process"}, {"score": 0.002675516297446243, "phrase": "itgp"}, {"score": 0.0025388011374541546, "phrase": "fmri_scanning"}, {"score": 0.00250103550267459, "phrase": "multi-view_learning_algorithms"}, {"score": 0.0024546155360991567, "phrase": "acoustic_features"}, {"score": 0.002320449502337321, "phrase": "experimental_results"}, {"score": 0.0022434880661725493, "phrase": "existing_methods"}, {"score": 0.002185394402074883, "phrase": "functional_brain_responses"}, {"score": 0.00216907363015155, "phrase": "fmri_data"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Audio analysis", " fMRI", " fMRI-measured feature", " Multi-view learning"], "paper_abstract": "Effective analysis of music/speech data such as clustering, retrieval, and classification has received significant attention in recent years. Traditional methods mainly rely on the low-level acoustic features derived from digital audio stream, and the accuracy of these methods is limited by the well-known semantic gap. To alleviate this problem, we propose a novel framework for music/speech clustering, retrieval, and classification by integrating the low-level acoustic features derived from audio content with the functional magnetic resonance imaging (fMRI) measured features that represent the brain's functional response when subjects are listening to the music/speech excerpts. First, the brain networks and regions of interest (ROIs) involved in the comprehension of audio stimuli, such as the auditory, emotion, attention, and working memory systems, are located by a new approach named dense individualized and common connectivity-based cortical landmarks (DICC-COLs). Then the functional connectivity matrix measuring the similarity between the fMRI signals of different ROIs is adopted to represent the brain's comprehension of audio semantics. Afterwards, we propose an improved twin Gaussian process (ITGP) model based on self-training to predict the fMRI-measured features of testing data without fMRI scanning. Finally, multi-view learning algorithms are proposed to integrate acoustic features with fMRI-measured features for music/speech clustering, retrieval, and classification, respectively. The experimental results demonstrate the superiority of our proposed work in comparison with existing methods and suggest the advantage of integrating functional brain responses via fMRI data for music/speech analysis. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Analysis of music/speech via integration of audio content and functional brain response", "paper_id": "WOS:000347862200015"}