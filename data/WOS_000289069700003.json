{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "memory_bottleneck"}, {"score": 0.004736045157669388, "phrase": "limiting_factor"}, {"score": 0.004658426838467567, "phrase": "explosive_demands"}, {"score": 0.0045443671106012405, "phrase": "modern_embedded_system_design"}, {"score": 0.004506968388996092, "phrase": "selected_computation_kernels"}, {"score": 0.004378469469879815, "phrase": "nest_loops"}, {"score": 0.004132312946154097, "phrase": "loop_tiling"}, {"score": 0.004098291340525772, "phrase": "loop_pipelining"}, {"score": 0.004031082737449308, "phrase": "memory_bandwidth"}, {"score": 0.0039323225960857956, "phrase": "optimal_throughput"}, {"score": 0.0038678251731669865, "phrase": "available_parallelism"}, {"score": 0.0037419747525425586, "phrase": "automatic_memory"}, {"score": 0.003590383760209319, "phrase": "energy_consumption"}, {"score": 0.003560807940070886, "phrase": "pipelined_loop_kernels"}, {"score": 0.003502382624110703, "phrase": "platform_requirements"}, {"score": 0.0034024277222012597, "phrase": "general_array_access"}, {"score": 0.0033743949483348626, "phrase": "affine_array_references"}, {"score": 0.003264547032549398, "phrase": "first_step"}, {"score": 0.0032376465741070274, "phrase": "cycle_accurate_scheduling_information"}, {"score": 0.0031845067058084583, "phrase": "hard_constraints"}, {"score": 0.0031582637043009562, "phrase": "memory_bandwidth_requirements"}, {"score": 0.0031193028729331667, "phrase": "synchronized_hardware_designs"}, {"score": 0.0030808211830129304, "phrase": "ilp_formulation"}, {"score": 0.0029194625087935345, "phrase": "small_designs"}, {"score": 0.0028596699752849682, "phrase": "heuristic_algorithm"}, {"score": 0.0027437235598539904, "phrase": "large_scale_problems"}, {"score": 0.0027211030647461324, "phrase": "experimental_results"}, {"score": 0.0026324658253309673, "phrase": "real-world_designs"}, {"score": 0.0026107601878587816, "phrase": "moderate_area_increase"}, {"score": 0.0024842242921857705, "phrase": "higher_throughput"}, {"score": 0.0024637379406427878, "phrase": "optimized_designs"}, {"score": 0.0023834640793589435, "phrase": "memory_banks"}, {"score": 0.002344311033783139, "phrase": "dynamic_power_consumption"}, {"score": 0.002315367926547472, "phrase": "final_design"}, {"score": 0.0022679195103377124, "phrase": "previous_approaches"}, {"score": 0.0022122603123901114, "phrase": "memory_access_frequencies"}, {"score": 0.002194011800331242, "phrase": "polynomial_time"}], "paper_keywords": ["Algorithm", " Design", " Experimentation", " Behavioral synthesis", " memory partition"], "paper_abstract": "Memory bottleneck has become a limiting factor in satisfying the explosive demands on performance and cost in modern embedded system design. Selected computation kernels for acceleration are usually captured by nest loops, which are optimized by state-of-the-art techniques like loop tiling and loop pipelining. However, memory bandwidth bottlenecks prevent designs from reaching optimal throughput with respect to available parallelism. In this paper we present an automatic memory partitioning technique which can efficiently improve throughput and reduce energy consumption of pipelined loop kernels for given throughput constraints and platform requirements. Also, our proposed algorithm can handle general array access beyond affine array references. Our partition scheme consists of two steps. The first step considers cycle accurate scheduling information to meet the hard constraints on memory bandwidth requirements specifically for synchronized hardware designs. An ILP formulation is proposed to solve the memory partitioning and scheduling problem optimally for small designs, followed by a heuristic algorithm which is more scalable and equally effective for solving large scale problems. Experimental results show an average 6x throughput improvement on a set of real-world designs with moderate area increase (about 45% on average), given that less resource sharing opportunities exist with higher throughput in optimized designs. The second step further partitions the memory banks for reducing the dynamic power consumption of the final design. In contrast to previous approaches, our technique can statically compute memory access frequencies in polynomial time with little or no profiling. Experimental results show about 30% power reduction on the same set of benchmarks.", "paper_title": "Automatic Memory Partitioning and Scheduling for Throughput and Power Optimization", "paper_id": "WOS:000289069700003"}