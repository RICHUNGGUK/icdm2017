{"auto_keywords": [{"score": 0.045923793841804825, "phrase": "irrelevant_features"}, {"score": 0.00481495049065317, "phrase": "high-dimensional_data_analysis"}, {"score": 0.0046018264570209765, "phrase": "data_classification"}, {"score": 0.004448169552315403, "phrase": "huge_number"}, {"score": 0.004275347843870176, "phrase": "new_feature-selection_algorithm"}, {"score": 0.004156012879026324, "phrase": "prior_work"}, {"score": 0.004039995317682942, "phrase": "algorithm_implementation"}, {"score": 0.00399449659126893, "phrase": "computational_complexity"}, {"score": 0.003927203683176278, "phrase": "solution_accuracy"}, {"score": 0.0038610400240820307, "phrase": "key_idea"}, {"score": 0.0037532257703888315, "phrase": "arbitrarily_complex_nonlinear_problem"}, {"score": 0.003648411055689428, "phrase": "locally_linear_ones"}, {"score": 0.0036073064072936626, "phrase": "local_learning"}, {"score": 0.0035065523764817143, "phrase": "feature_relevance"}, {"score": 0.0034279721355412285, "phrase": "large_margin_framework"}, {"score": 0.003370190834214415, "phrase": "proposed_algorithm"}, {"score": 0.0032946562900625187, "phrase": "well-established_machine_learning"}, {"score": 0.003078028357912228, "phrase": "underlying_data_distribution"}, {"score": 0.002843180211561427, "phrase": "personal_computer"}, {"score": 0.002671263324230699, "phrase": "growing_number"}, {"score": 0.0026113524778702624, "phrase": "theoretical_analyses"}, {"score": 0.0025673005153462707, "phrase": "algorithm's_sample_complexity"}, {"score": 0.0024673738215365104, "phrase": "logarithmical_sample_complexity"}, {"score": 0.002202742267484358, "phrase": "feature-selection_problem"}], "paper_keywords": ["Feature selection", " local learning", " logistical regression", " l(1) regularization", " sample complexity"], "paper_abstract": "This paper considers feature selection for data classification in the presence of a huge number of irrelevant features. We propose a new feature-selection algorithm that addresses several major issues with prior work, including problems with algorithm implementation, computational complexity, and solution accuracy. The key idea is to decompose an arbitrarily complex nonlinear problem into a set of locally linear ones through local learning, and then learn feature relevance globally within the large margin framework. The proposed algorithm is based on well-established machine learning and numerical analysis techniques, without making any assumptions about the underlying data distribution. It is capable of processing many thousands of features within minutes on a personal computer while maintaining a very high accuracy that is nearly insensitive to a growing number of irrelevant features. Theoretical analyses of the algorithm's sample complexity suggest that the algorithm has a logarithmical sample complexity with respect to the number of features. Experiments on 11 synthetic and real-world data sets demonstrate the viability of our formulation of the feature-selection problem for supervised learning and the effectiveness of our algorithm.", "paper_title": "Local-Learning-Based Feature Selection for High-Dimensional Data Analysis", "paper_id": "WOS:000279969000006"}