{"auto_keywords": [{"score": 0.045458312844898874, "phrase": "probability_distributions"}, {"score": 0.00481495049065317, "phrase": "algebraic_geometric_comparison"}, {"score": 0.004500891637057374, "phrase": "novel_algebraic_algorithmic_framework"}, {"score": 0.0040334640350288, "phrase": "covariance_matrix"}, {"score": 0.003738502710504415, "phrase": "unsupervised_learning_problem"}, {"score": 0.0033218213825698417, "phrase": "objective_function"}, {"score": 0.0032387402145603412, "phrase": "estimated_cumulants"}, {"score": 0.0029020077365708966, "phrase": "polynomial_ring"}, {"score": 0.00266693888510356, "phrase": "lower_computational_cost"}, {"score": 0.002600194073758015, "phrase": "higher_accuracy"}, {"score": 0.0024926348205500715, "phrase": "algebraic_viewpoint"}, {"score": 0.0022906498741695094, "phrase": "algebraic_geometry"}, {"score": 0.0021590588269751816, "phrase": "compact_proof"}, {"score": 0.0021049977753042253, "phrase": "identifiability_criterion"}], "paper_keywords": ["computational algebraic geometry", " approximate algebra", " unsupervised Learning"], "paper_abstract": "We propose a novel algebraic algorithmic framework for dealing with probability distributions represented by their cumulants such as the mean and covariance matrix. As an example, we consider the unsupervised learning problem of finding the subspace on which several probability distributions agree. Instead of minimizing an objective function involving the estimated cumulants, we show that by treating the cumulants as elements of the polynomial ring we can directly solve the problem, at a lower computational cost and with higher accuracy. Moreover, the algebraic viewpoint on probability distributions allows us to invoke the theory of algebraic geometry, which we demonstrate in a compact proof for an identifiability criterion.", "paper_title": "Algebraic Geometric Comparison of Probability Distributions", "paper_id": "WOS:000303772100015"}