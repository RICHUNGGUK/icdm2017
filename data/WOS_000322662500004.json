{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "menu_selection"}, {"score": 0.046458381465691374, "phrase": "user_interfaces"}, {"score": 0.03924652090419656, "phrase": "shared_input_multimodal_interface"}, {"score": 0.03494422442607806, "phrase": "shared_input_multimodal_menu_system"}, {"score": 0.004555499634179394, "phrase": "visual_modalities"}, {"score": 0.004390312150344245, "phrase": "today's_mobile_devices"}, {"score": 0.004172874316207784, "phrase": "visual_modality"}, {"score": 0.0041154571996523505, "phrase": "primary_output_channel"}, {"score": 0.004058826900076531, "phrase": "audio_output"}, {"score": 0.004002972723329302, "phrase": "secondary_role"}, {"score": 0.0038756055612167942, "phrase": "increased_need"}, {"score": 0.003839962121679795, "phrase": "shared_input_multimodal_user_interfaces"}, {"score": 0.003616108223034427, "phrase": "specific_output_modality"}, {"score": 0.0035010071030752883, "phrase": "preferred_method"}, {"score": 0.0034368818187904744, "phrase": "different_scenarios"}, {"score": 0.0032514447965530323, "phrase": "single-task_desktop"}, {"score": 0.003177154457204778, "phrase": "dynamic_dual-task_setting"}, {"score": 0.002950608985750058, "phrase": "simulated_vehicle"}, {"score": 0.0028042741141125712, "phrase": "target_item"}, {"score": 0.00274017282519193, "phrase": "visual_feedback"}, {"score": 0.0025683581446365165, "phrase": "dual-task_driving"}, {"score": 0.0025329623864528317, "phrase": "visual_output"}, {"score": 0.00249805321224923, "phrase": "significant_source"}, {"score": 0.0024750474634399797, "phrase": "visual_distraction"}, {"score": 0.002363150587804677, "phrase": "auditory_output"}, {"score": 0.002154272413276506, "phrase": "multiple_feedback_modalities"}, {"score": 0.0021049977753042253, "phrase": "better_overall_experience"}], "paper_keywords": ["earPod", " eyes-free", " shared-input multimodal interfaces"], "paper_abstract": "Audio and visual modalities are two common output channels in the user interfaces embedded in today's mobile devices. However, these user interfaces are typically centered on the visual modality as the primary output channel, with audio output serving a secondary role. This paper argues for an increased need for shared input multimodal user interfaces for mobile devices. A shared input multimodal interface can be operated independently using a specific output modality, leaving users to choose the preferred method of interaction in different scenarios. We evaluate the value of a shared input multimodal menu system both in a single-task desktop setting and in a dynamic dual-task setting, in which the user was required to interact with the shared input multimodal menu system while driving a simulated vehicle. Results indicate that users were faster at locating a target item in the menu when visual feedback was provided in the single-task desktop setting, but in the dual-task driving setting, visual output presented a significant source of visual distraction that interfered with driving performance. In contrast, auditory output mitigated some of the risk associated with menu selection while driving. A shared input multimodal interface allows users to take advantage of multiple feedback modalities properly, providing a better overall experience.", "paper_title": "Shared Input Multimodal Mobile Interfaces: Interaction Modality Effects on Menu Selection in Single-Task and Dual-Task Environments(dagger)", "paper_id": "WOS:000322662500004"}