{"auto_keywords": [{"score": 0.0451854857850558, "phrase": "possible_data"}, {"score": 0.00481495049065317, "phrase": "learning_paradigm"}, {"score": 0.004356535752468849, "phrase": "finite_set_d"}, {"score": 0.004260719592945006, "phrase": "target_language_l"}, {"score": 0.0035135084960705816, "phrase": "classification_task"}, {"score": 0.003323330859027428, "phrase": "desirable_constraints"}, {"score": 0.0031668306413359794, "phrase": "mind_changes"}, {"score": 0.0031086137705026483, "phrase": "special_attention"}, {"score": 0.002822652835367827, "phrase": "current_data_sigma"}, {"score": 0.002791393223133408, "phrase": "language_l"}, {"score": 0.002760478838718213, "phrase": "also_consistency"}, {"score": 0.002740059168271987, "phrase": "larger_sets"}, {"score": 0.002562929776267296, "phrase": "classical_paradigms"}, {"score": 0.002543967677678384, "phrase": "inductive_inference"}, {"score": 0.002335707508067455, "phrase": "fairly_comprehensive_set"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Classification of predicates", " Multiclassification", " Learning in the limit", " Inductive inference"], "paper_abstract": "We define and study a learning paradigm that sits between identification in the limit and classification. More precisely, we expect a learner to determine in the limit which members of a finite set D of possible data belong to a target language L, where D is arbitrary. So as D becomes larger and larger, the task becomes closer and closer to identifying L. But as D is always finite and L can be infinite, it can still be expected that Ex- and BC-learning are often more difficult than performing this classification task. The paper supports this intuition and makes it precise, taking into account desirable constraints on how the learner behaves, such as bounding the number of mind changes and being conservative. Special attention is given to various forms of consistency. In particular, we might not only require consistency between the members of D to classify, the current data sigma and a language L, but also consistency between larger sets of possible data to classify (supersets of D) and the same sigma and L: whereas in the classical paradigms of inductive inference or classification, only the available data can grow, here both the available data and the set of possible data to classify can grow. We provide a fairly comprehensive set of results, many of which are optimal, that demonstrate the fruitfulness of the approach and the richness of the paradigm. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Learning and classifying", "paper_id": "WOS:000318393100007"}