{"auto_keywords": [{"score": 0.04028252665987454, "phrase": "overall_quality"}, {"score": 0.015719716506582538, "phrase": "peer_review"}, {"score": 0.015632873521717713, "phrase": "computer_science"}, {"score": 0.0137991497417455, "phrase": "peer_review_processes"}, {"score": 0.004539913872457199, "phrase": "peer_reviews"}, {"score": 0.004439301692276909, "phrase": "different_review_processes"}, {"score": 0.0042446888127531945, "phrase": "theoretical_model"}, {"score": 0.004150592040012897, "phrase": "appropriate_metrics"}, {"score": 0.0040928394196673955, "phrase": "main_characteristics"}, {"score": 0.003935354824241103, "phrase": "known_metrics"}, {"score": 0.0038805854107176116, "phrase": "new_ones"}, {"score": 0.0035080343502078394, "phrase": "reviewers'_decision_making_process"}, {"score": 0.0033074024842843575, "phrase": "proposed_model"}, {"score": 0.003288901736901966, "phrase": "analysis_framework"}, {"score": 0.0032613438411286046, "phrase": "large_reviews"}, {"score": 0.003224957676162907, "phrase": "ten_different_conferences"}, {"score": 0.0029646555801624563, "phrase": "analysed_peer_review_processes"}, {"score": 0.0029233560054443483, "phrase": "interesting_results"}, {"score": 0.0028107438825394, "phrase": "peer_review_outcome"}, {"score": 0.002756070639666095, "phrase": "accepted_contributions"}, {"score": 0.0026873320229798664, "phrase": "assessment_scale"}, {"score": 0.002562123444678208, "phrase": "rating_bias"}, {"score": 0.002415459080723135, "phrase": "statistical_approaches"}, {"score": 0.002388487669097511, "phrase": "process_parameters"}, {"score": 0.002270786672851054, "phrase": "overall_effort"}, {"score": 0.0021049977753042253, "phrase": "current_editorial_management_systems"}], "paper_keywords": ["Peer review", " Quality metrics", " Reliability", " Fairness", " Validity", " Efficiency"], "paper_abstract": "In this paper we focus on the analysis of peer reviews and reviewers behaviour in a number of different review processes. More specifically, we report on the development, definition and rationale of a theoretical model for peer review processes to support the identification of appropriate metrics to assess the processes main characteristics in order to render peer review more transparent and understandable. Together with known metrics and techniques we introduce new ones to assess the overall quality (i.e. ,reliability, fairness, validity) and efficiency of peer review processes e.g. the robustness of the process, the degree of agreement/disagreement among reviewers, or positive/negative bias in the reviewers' decision making process. We also check the ability of peer review to assess the impact of papers in subsequent years. We apply the proposed model and analysis framework to a large reviews data set from ten different conferences in computer science for a total of ca. 9,000 reviews on ca. 2,800 submitted contributions. We discuss the implications of the results and their potential use toward improving the analysed peer review processes. A number of interesting results were found, in particular: (1) a low correlation between peer review outcome and impact in time of the accepted contributions; (2) the influence of the assessment scale on the way how reviewers gave marks; (3) the effect and impact of rating bias, i.e. reviewers who constantly give lower/higher marks w.r.t. all other reviewers; (4) the effectiveness of statistical approaches to optimize some process parameters (e.g. ,number of papers per reviewer) to improve the process overall quality while maintaining the overall effort under control. Based on the lessons learned, we suggest ways to improve the overall quality of peer-review through procedures that can be easily implemented in current editorial management systems.", "paper_title": "On peer review in computer science: analysis of its effectiveness and suggestions for improvement", "paper_id": "WOS:000325475300010"}