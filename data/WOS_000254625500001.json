{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "factor_analysis"}, {"score": 0.004664352260912174, "phrase": "maximum_likelihood"}, {"score": 0.004346266306175988, "phrase": "conditional_maximization"}, {"score": 0.00422516469733364, "phrase": "quadratic_and_monotone_convergence"}, {"score": 0.004121960535174029, "phrase": "cm_log-likelihood"}, {"score": 0.004092955097819235, "phrase": "cml"}, {"score": 0.004021267064832957, "phrase": "main_contribution"}, {"score": 0.003923023699005215, "phrase": "closed_form_expression"}, {"score": 0.0036682498811430623, "phrase": "numerical_optimization_methods"}, {"score": 0.003591272267478512, "phrase": "new_ecme_algorithm"}, {"score": 0.0035535529853574365, "phrase": "liu"}, {"score": 0.0032184697893776052, "phrase": "simple_iteration_algorithm"}, {"score": 0.00316206336164649, "phrase": "r._soc"}, {"score": 0.0026401165487877, "phrase": "ecme"}, {"score": 0.0026217401134122167, "phrase": "em"}, {"score": 0.0024510413243251906, "phrase": "cpu_time"}, {"score": 0.0023325586335885104, "phrase": "well_known_heywood_case"}, {"score": 0.002212408012860986, "phrase": "cm"}, {"score": 0.0021049977753042253, "phrase": "ecme."}], "paper_keywords": ["CM", " ECME", " EM", " factor analysis", " maximum likelihood estimation"], "paper_abstract": "To obtain maximum likelihood (ML) estimation in factor analysis (FA), we propose in this paper a novel and fast conditional maximization (CM) algorithm, which has quadratic and monotone convergence, consisting of a sequence of CM log-likelihood (CML) steps. The main contribution of this algorithm is that the closed form expression for the parameter to be updated in each step can be obtained explicitly, without resorting to any numerical optimization methods. In addition, a new ECME algorithm similar to Liu's (Biometrika 81, 633-648, 1994) one is obtained as a by-product, which turns out to be very close to the simple iteration algorithm proposed by Lawley (Proc. R. Soc. Edinb. 60, 64-82, 1940) but our algorithm is guaranteed to increase log-likelihood at every iteration and hence to converge. Both algorithms inherit the simplicity and stability of EM but their convergence behaviors are much different as revealed in our extensive simulations: (1) In most situations, ECME and EM perform similarly; (2) CM outperforms EM and ECME substantially in all situations, no matter assessed by the CPU time or the number of iterations. Especially for the case close to the well known Heywood case, it accelerates EM by factors of around 100 or more. Also, CM is much more insensitive to the choice of starting values than EM and ECME.", "paper_title": "ML estimation for factor analysis: EM or non-EM?", "paper_id": "WOS:000254625500001"}