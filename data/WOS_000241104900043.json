{"auto_keywords": [{"score": 0.004616859702338596, "phrase": "human_interpretability"}, {"score": 0.004426882270402403, "phrase": "real_world_problems"}, {"score": 0.004334832612029898, "phrase": "blind_faith"}, {"score": 0.0040133278382101885, "phrase": "novel_approach"}, {"score": 0.003848088904418221, "phrase": "account_interpretability"}, {"score": 0.003275093899046286, "phrase": "two-dimensional_scatter_plot"}, {"score": 0.0029274600162609654, "phrase": "classification_algorithm"}, {"score": 0.0027483472371835865, "phrase": "classification_accuracy"}, {"score": 0.002562123444678208, "phrase": "real_data"}, {"score": 0.002371783323468861, "phrase": "wide_range"}, {"score": 0.0023387234705715154, "phrase": "canonical_data_sets"}, {"score": 0.0021347607855562102, "phrase": "additional_interpretability"}], "paper_keywords": [""], "paper_abstract": "Many classification algorithms suffer from a lack of human interpretability. Using such classifiers to solve real world problems often requires blind faith in the given model. In this paper we present a novel approach to classification that takes into account interpretability and visualization of the results. We attempt to efficiently discover the most relevant snapshot of the data, in the form of a two-dimensional scatter plot with easily understandable axes. We then use this plot as the basis for a classification algorithm. Furthermore, we investigate the trade-off between classification accuracy and interpretability by comparing the performance of our classifier on real data with that of several traditional classifiers. Upon evaluating our algorithm on a wide range of canonical data sets we find that, in most cases, it is possible to obtain additional interpretability with little or no loss in classification accuracy.", "paper_title": "Autonomous visualization", "paper_id": "WOS:000241104900043"}