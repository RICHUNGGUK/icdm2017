{"auto_keywords": [{"score": 0.04908067956118528, "phrase": "data-parallel_architectures"}, {"score": 0.015719716506582538, "phrase": "memory_access_patterns"}, {"score": 0.011636073471880469, "phrase": "memory_subsystem"}, {"score": 0.004754447398742591, "phrase": "memory_performance"}, {"score": 0.0046161999598320486, "phrase": "general-purpose_computation"}, {"score": 0.004351595642056769, "phrase": "parallel_computing"}, {"score": 0.004067635935640407, "phrase": "low-cost_supercomputing"}, {"score": 0.004016485454498076, "phrase": "attractive_power_budgets"}, {"score": 0.003932654396285872, "phrase": "numerous_benefits"}, {"score": 0.003722761107761564, "phrase": "wider_adoption"}, {"score": 0.003599182629885611, "phrase": "heterogeneous_and_distributed_nature"}, {"score": 0.0032387402145603412, "phrase": "execution_units"}, {"score": 0.00305285084207725, "phrase": "memory_efficiency"}, {"score": 0.0028654728453890426, "phrase": "loop_bodies"}, {"score": 0.0027937732927852646, "phrase": "data_transformation"}, {"score": 0.002758597251689795, "phrase": "vector-based_architectures"}, {"score": 0.002700972230602559, "phrase": "amd"}, {"score": 0.0026112013707473938, "phrase": "scalar-based_architectures"}, {"score": 0.0024097920845706795, "phrase": "wide_range"}, {"score": 0.002389514188569084, "phrase": "benchmark_suites"}, {"score": 0.002349467661235186, "phrase": "benchmark_kernels"}, {"score": 0.0023003498123420237, "phrase": "consistent_and_significant_performance_improvements"}, {"score": 0.002195950685807041, "phrase": "gpu"}], "paper_keywords": ["General-purpose computation on GPUs (GPGPUs)", " GPU computing", " memory optimization", " memory access pattern", " vectorization", " memory selection", " memory coalescing", " data parallelism", " data-parallel architectures"], "paper_abstract": "The introduction of General-Purpose computation on GPUs (GPGPUs) has changed the landscape for the future of parallel computing. At the core of this phenomenon are massively multithreaded, data-parallel architectures possessing impressive acceleration ratings, offering low-cost supercomputing together with attractive power budgets. Even given the numerous benefits provided by GPGPUs, there remain a number of barriers that delay wider adoption of these architectures. One major issue is the heterogeneous and distributed nature of the memory subsystem commonly found on data-parallel architectures. Application acceleration is highly dependent on being able to utilize the memory subsystem effectively so that all execution units remain busy. In this paper, we present techniques for enhancing the memory efficiency of applications on data-parallel architectures, based on the analysis and characterization of memory access patterns in loop bodies; we target vectorization via data transformation to benefit vector-based architectures (e. g., AMD GPUs) and algorithmic memory selection for scalar-based architectures (e. g., NVIDIA GPUs). We demonstrate the effectiveness of our proposed methods with kernels from a wide range of benchmark suites. For the benchmark kernels studied, we achieve consistent and significant performance improvements (up to 11.4 x and 13.5 x over baseline GPU implementations on each platform, respectively) by applying our proposed methodology.", "paper_title": "Exploiting Memory Access Patterns to Improve Memory Performance in Data-Parallel Architectures", "paper_id": "WOS:000284423900011"}