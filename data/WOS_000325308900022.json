{"auto_keywords": [{"score": 0.030558920626333336, "phrase": "hbf_neural_network"}, {"score": 0.015719716506582538, "phrase": "sequential_learning_algorithm"}, {"score": 0.013508003732321746, "phrase": "gaussian_type"}, {"score": 0.013400506514235641, "phrase": "activation_function"}, {"score": 0.012671298703416222, "phrase": "complex_nonlinear_problems"}, {"score": 0.012077541701703698, "phrase": "neural_network"}, {"score": 0.011511286226550317, "phrase": "hbf"}, {"score": 0.009728445148021547, "phrase": "hbf_neuron"}, {"score": 0.004776297980616768, "phrase": "hyper_basis_function_neural_network"}, {"score": 0.004496080495247288, "phrase": "rbf_neurons"}, {"score": 0.00421520166010848, "phrase": "conventional_rbf_neuron"}, {"score": 0.004064958518350867, "phrase": "single_width"}, {"score": 0.003935892934907777, "phrase": "neuron_performance"}, {"score": 0.003765067028482197, "phrase": "single_scale"}, {"score": 0.0036602262729063775, "phrase": "similar_but_yet_different_activation"}, {"score": 0.003487222504700868, "phrase": "different_scaling"}, {"score": 0.003459191419478298, "phrase": "input_dimensions"}, {"score": 0.003417565253539315, "phrase": "better_generalization_property"}, {"score": 0.003335804608791029, "phrase": "engineering_practice"}, {"score": 0.003152531962718477, "phrase": "mahalanobis-like_distance"}, {"score": 0.003114584448820957, "phrase": "distance_metrics"}, {"score": 0.0030895394261534776, "phrase": "input_training_sample"}, {"score": 0.0030646951765987414, "phrase": "prototype_vector"}, {"score": 0.0030034705676019115, "phrase": "rbf"}, {"score": 0.002838387600282891, "phrase": "hbf_neurons"}, {"score": 0.0027816546451972725, "phrase": "input_and_output_sets"}, {"score": 0.0027260525536580912, "phrase": "good_generalization_property"}, {"score": 0.002682370021834353, "phrase": "recent_research_results"}, {"score": 0.00266079124891013, "phrase": "hbf_neural_network_performance"}, {"score": 0.0023762535034673017, "phrase": "neuron's_significance"}, {"score": 0.002263799931018574, "phrase": "extensive_experimental_study"}, {"score": 0.0022006705963260433, "phrase": "developed_learning_algorithm"}, {"score": 0.002174155795026218, "phrase": "lower_prediction_error"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feedforward neural networks", " Hyper basis function", " Sequential learning", " Neuron's significance", " Extended Kalman Filter"], "paper_abstract": "Radial basis function (RBF) neural network is constructed of certain number of RBF neurons, and these networks are among the most used neural networks for modeling of various nonlinear problems in engineering. Conventional RBF neuron is usually based on Gaussian type of activation function with single width for each activation function. This feature restricts neuron performance for modeling the complex nonlinear problems. To accommodate limitation of a single scale, this paper presents neural network with similar but yet different activation function hyper basis function (HBF). The HBF allows different scaling of input dimensions to provide better generalization property when dealing with complex nonlinear problems in engineering practice. The HBF is based on generalization of Gaussian type of neuron that applies Mahalanobis-like distance as a distance metrics between input training sample and prototype vector. Compared to the RBF, the HBF neuron has more parameters to optimize, but HBF neural network needs less number of HBF neurons to memorize relationship between input and output sets in order to achieve good generalization property. However, recent research results of HBF neural network performance have shown that optimal way of constructing this type of neural network is needed; this paper addresses this issue and modifies sequential learning algorithm for HBF neural network that exploits the concept of neuron's significance and allows growing and pruning of HBF neuron during learning process. Extensive experimental study shows that HBF neural network, trained with developed learning algorithm, achieves lower prediction error and more compact neural network. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "A growing and pruning sequential learning algorithm of hyper basis function neural network for function approximation", "paper_id": "WOS:000325308900022"}