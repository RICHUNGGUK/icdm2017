{"auto_keywords": [{"score": 0.047543433871394314, "phrase": "observed_data"}, {"score": 0.04151111040011756, "phrase": "partition_function"}, {"score": 0.02589975523192009, "phrase": "unnormalized_models"}, {"score": 0.00481495049065317, "phrase": "unnormalized_statistical_models"}, {"score": 0.004728499903204582, "phrase": "natural_image_statistics"}, {"score": 0.004397875849247165, "phrase": "finite_number"}, {"score": 0.004180234323211199, "phrase": "model_probability_density_function"}, {"score": 0.0035507457285983268, "phrase": "closed_form"}, {"score": 0.003525074461418428, "phrase": "gibbs_distributions"}, {"score": 0.0034995881411519925, "phrase": "markov_and_multi-layer_networks"}, {"score": 0.003424225677958491, "phrase": "analytical_normalization"}, {"score": 0.003362660648932247, "phrase": "maximum_likelihood_estimation"}, {"score": 0.0032546105164772995, "phrase": "numerical_approximations"}, {"score": 0.0031272381208680613, "phrase": "new_objective_function"}, {"score": 0.003059869469624301, "phrase": "normalized_and_unnormalized_models"}, {"score": 0.0030267296900520217, "phrase": "basic_idea"}, {"score": 0.0029830992938695007, "phrase": "nonlinear_logistic_regression"}, {"score": 0.002897710777957041, "phrase": "artificially_generated_noise"}, {"score": 0.0028249975318149468, "phrase": "normalizing_partition_function"}, {"score": 0.002704554429386671, "phrase": "new_estimation_method"}, {"score": 0.0025892330566358503, "phrase": "large_noise_sample_sizes"}, {"score": 0.0025611773887867255, "phrase": "new_estimator"}, {"score": 0.002487835869510301, "phrase": "maximum_likelihood_estimator"}, {"score": 0.002355920629285463, "phrase": "statistical_and_computational_performance"}, {"score": 0.0023051296931856265, "phrase": "new_method"}, {"score": 0.0022801453935251503, "phrase": "competitive_trade-off"}, {"score": 0.002182881082103045, "phrase": "real_data"}, {"score": 0.002151388538895855, "phrase": "novel_two-layer_models"}, {"score": 0.0021049977753042253, "phrase": "spline_nonlinearities"}], "paper_keywords": ["unnormalized models", " partition function", " computation", " estimation", " natural image statistics"], "paper_abstract": "We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.", "paper_title": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics", "paper_id": "WOS:000303046000004"}