{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "parallel_digital_vlsi_architecture"}, {"score": 0.004762202897268383, "phrase": "integrated_support_vector_machine_training"}, {"score": 0.004506968388996092, "phrase": "combined_support_vector_machine"}, {"score": 0.004265354774450696, "phrase": "first_time"}, {"score": 0.004218602458288531, "phrase": "cascade_svm"}, {"score": 0.0038838500580958744, "phrase": "hardware-based_svm_training"}, {"score": 0.00379914165379475, "phrase": "efficient_parallel_vlsi_architecture"}, {"score": 0.00373682045166829, "phrase": "presented_architecture"}, {"score": 0.0036958398682001015, "phrase": "excellent_scalability"}, {"score": 0.003615217162972726, "phrase": "training_workload"}, {"score": 0.0034975571855450343, "phrase": "multiple_svm_processing_units"}, {"score": 0.003459191419478298, "phrase": "minimal_communication_overhead"}, {"score": 0.003421245054271364, "phrase": "hardware-friendly_implementation"}, {"score": 0.0033651019362620866, "phrase": "cascade_algorithm"}, {"score": 0.00327356326590461, "phrase": "low_hardware_overhead"}, {"score": 0.0031669873302916, "phrase": "data_sets"}, {"score": 0.003132236287543072, "phrase": "variable_size"}, {"score": 0.0030638704739988595, "phrase": "proposed_parallel_cascade_architecture"}, {"score": 0.0030135741897058844, "phrase": "multilayer_system_bus_and_multiple_distributed_memories"}, {"score": 0.00282048824450047, "phrase": "proposed_architecture"}, {"score": 0.002669042572650594, "phrase": "hybrid_use"}, {"score": 0.002639740960006001, "phrase": "hardware_parallel_processing"}, {"score": 0.0026107601878587816, "phrase": "temporal_reuse"}, {"score": 0.0025820967615278073, "phrase": "processing_resources"}, {"score": 0.0025257081555100556, "phrase": "good_tradeoffs"}, {"score": 0.002470547930651519, "phrase": "silicon_overhead"}, {"score": 0.0024434201175883674, "phrase": "power_dissipation"}, {"score": 0.002163930783583212, "phrase": "software_svm_algorithm"}, {"score": 0.002105013449780447, "phrase": "cpu."}], "paper_keywords": ["Digital integrated circuits", " multicore processing", " parallel architectures", " support vector machines", " system buses"], "paper_abstract": "This paper presents a parallel digital VLSI architecture for combined support vector machine (SVM) training and classification. For the first time, cascade SVM, a powerful training algorithm, is leveraged to significantly improve the scalability of hardware-based SVM training and develop an efficient parallel VLSI architecture. The presented architecture achieves excellent scalability by spreading the training workload of a given data set over multiple SVM processing units with minimal communication overhead. Hardware-friendly implementation of the cascade algorithm is employed to achieve low hardware overhead and allow for training over data sets of variable size. In the proposed parallel cascade architecture, a multilayer system bus and multiple distributed memories are used to fully exploit parallelism. In addition, the proposed architecture is rather flexible and can be tailored to realize hybrid use of hardware parallel processing and temporal reuse of processing resources, leading to good tradeoffs between throughput, silicon overhead and power dissipation. Several parallel cascade SVM processors have been designed with a commercial 90-nm CMOS technology, which provide up to a 561x training time speedup and a significant estimated 21 859x energy reduction compared with the software SVM algorithm running on a 45-nm commercial general-purpose CPU.", "paper_title": "A Parallel Digital VLSI Architecture for Integrated Support Vector Machine Training and Classification", "paper_id": "WOS:000358511300009"}