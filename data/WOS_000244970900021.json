{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "nonstationary_environment"}, {"score": 0.0045262653692203815, "phrase": "neural_networks"}, {"score": 0.004351595642056769, "phrase": "bayesian_learning"}, {"score": 0.0041839974419222575, "phrase": "gibbs"}, {"score": 0.004090577229368418, "phrase": "rosenblatt_potential"}, {"score": 0.003954837422946628, "phrase": "online_scheme"}, {"score": 0.003717528595128948, "phrase": "kullback-leibler_divergence"}, {"score": 0.00357395617690842, "phrase": "true_posterior_distribution"}, {"score": 0.003378385493203297, "phrase": "learning_algorithm"}, {"score": 0.00319348234663792, "phrase": "gaussian_distribution"}, {"score": 0.0031223774204093713, "phrase": "spherical_covariance_matrix"}, {"score": 0.0030017207229983385, "phrase": "particular_case"}, {"score": 0.002951444412262927, "phrase": "linearly_separable_rules"}, {"score": 0.0028373742057129126, "phrase": "fixed_rule"}, {"score": 0.002758597251689795, "phrase": "asymptotic_generalization_error"}, {"score": 0.0025638362414466278, "phrase": "full_covariance_matrix_approximations"}, {"score": 0.0023166075278110254, "phrase": "good_performance"}, {"score": 0.0021049977753042253, "phrase": "extra_information"}], "paper_keywords": ["online gradient methods", " pattern classification", " time-varying environment"], "paper_abstract": "In this letter, we study online learning in neural networks (NNs) obtained by approximating Bayesian learning. The approach is applied to Gibbs learning with the Rosenblatt potential in a nonstationary environment. The online scheme is obtained by the minimization (maximization) of the Kullback-Leibler divergence (cross entropy) between the true posterior distribution and the parameterized one. The complexity of the learning algorithm is further decreased by projecting the posterior onto a Gaussian distribution and imposing a spherical covariance matrix. We study in detail the particular case of learning linearly separable rules. In the case of a fixed rule, we observe an asymptotic generalization error e(g) proportional to alpha(-1) for both the spherical and the full covariance matrix approximations. However, in the case of drifting rule, only the full covariance matrix algorithm shows a good performance. This good performance is indeed a surprise since the algorithm is obtained by projecting without the benefit of the extra information on drifting.", "paper_title": "The Rosenblatt Bayesian algorithm learning in a nonstationary environment", "paper_id": "WOS:000244970900021"}