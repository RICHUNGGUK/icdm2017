{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "b-spline_transformations"}, {"score": 0.00458488025289647, "phrase": "novel_framework"}, {"score": 0.004501453956464956, "phrase": "penalization_techniques"}, {"score": 0.004446677711302801, "phrase": "partial_least_squares"}, {"score": 0.003982617095636925, "phrase": "roughness_penalty"}, {"score": 0.003910106508089187, "phrase": "high-imensional_regression_problems"}, {"score": 0.0037921662574137535, "phrase": "scalar_response"}, {"score": 0.0036107898615445797, "phrase": "additive_model"}, {"score": 0.0033961782274326948, "phrase": "generous_number"}, {"score": 0.0033548062793973144, "phrase": "b-spline_basis_functions"}, {"score": 0.003097865379563127, "phrase": "penalized_version"}, {"score": 0.0030043508031729277, "phrase": "additional_model_flexibility"}, {"score": 0.002931569771634119, "phrase": "sparsity_penalty"}, {"score": 0.0027403605798830984, "phrase": "unified_algorithm"}, {"score": 0.0027069560455629917, "phrase": "penalized_partial_least_squares"}, {"score": 0.002499508839669243, "phrase": "kernel_trick"}, {"score": 0.0023944552329463035, "phrase": "close_connection"}, {"score": 0.0023652572260178637, "phrase": "penalized_pls"}, {"score": 0.002336414425972225, "phrase": "preconditioned_linear_systems"}, {"score": 0.002157302200451323, "phrase": "functional_data"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_regression_models"}], "paper_keywords": ["NIR spectroscopy", " Additive model", " Dimensionality reduction", " Nonlinear regression", " Conjugate gradient", " Krylov spaces"], "paper_abstract": "We propose a novel framework that combines penalization techniques with Partial Least Squares (PLS). We focus on two important applications. (1) We combine PLS with a roughness penalty to estimate high-imensional regression problems with functional predictors and scalar response. (2) Starting with an additive model, we expand each variable in terms of a generous number of B-spline basis functions. To prevent overfitting, we estimate the model by applying a penalized version of PLS. We gain additional model flexibility by incorporating a sparsity penalty. Both applications can be formulated in terms of a unified algorithm called Penalized Partial Least Squares, which can be computed virtually as fast as PLS using the kernel trick. Furthermore, we prove a close connection of penalized PLS to preconditioned linear systems. In experiments. we show the benefits of our method to noisy functional data and to sparse nonlinear regression models. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Penalized Partial Least Squares with applications to B-spline transformations and functional data", "paper_id": "WOS:000263443400007"}