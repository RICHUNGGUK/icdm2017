{"auto_keywords": [{"score": 0.044340662163821154, "phrase": "human_assessors"}, {"score": 0.008797318586371113, "phrase": "correlation_coefficient"}, {"score": 0.00481495049065317, "phrase": "retrieval_systems"}, {"score": 0.004774324145784758, "phrase": "pseudo_relevance_judgments"}, {"score": 0.0046544807197808095, "phrase": "system-based_approach"}, {"score": 0.004518442937844318, "phrase": "large_test_collections"}, {"score": 0.004276216827737399, "phrase": "relevance_judgment"}, {"score": 0.004116139569480336, "phrase": "large_amount"}, {"score": 0.0039620308501590795, "phrase": "possible_errors"}, {"score": 0.003624429617337833, "phrase": "exponential_variation"}, {"score": 0.003578600071301459, "phrase": "ranking_methods"}, {"score": 0.0035183910023550246, "phrase": "reliable_set"}, {"score": 0.0034886660821793576, "phrase": "relevance_judgments"}, {"score": 0.00337224813954299, "phrase": "human_efforts"}, {"score": 0.00327356326590461, "phrase": "large_amounts"}, {"score": 0.0031777570808834213, "phrase": "human_disagreement_errors"}, {"score": 0.0031375577552605533, "phrase": "judgment_process"}, {"score": 0.0028822153400215973, "phrase": "document_rankings"}, {"score": 0.002833689453299307, "phrase": "alternate_methods"}, {"score": 0.0027390682208834013, "phrase": "proposed_method"}, {"score": 0.002658863179011018, "phrase": "ranked_systems"}, {"score": 0.0026363807171515255, "phrase": "mean_average_precision_scores"}, {"score": 0.0024842242921857705, "phrase": "proposed_document_ranking_method"}, {"score": 0.0024527767211457046, "phrase": "pool_depth"}, {"score": 0.002391067964886602, "phrase": "reliable_alternative"}, {"score": 0.002360796860462937, "phrase": "human_effort"}, {"score": 0.0023408288327263316, "phrase": "disagreement_errors"}, {"score": 0.0023013968622710847, "phrase": "trec-like_relevance_judgments"}, {"score": 0.002215081186105616, "phrase": "study_show_improvement"}, {"score": 0.0021593396036472777, "phrase": "alternate_relevance_judgment"}, {"score": 0.0021049977753042253, "phrase": "information_retrieval_evaluation"}], "paper_keywords": ["Information retrieval", " TREC", " Batch evaluation", " Large-scale experimentation", " Relevance judgments", " Retrieval evaluation"], "paper_abstract": "Purpose - In a system-based approach, replicating the web would require large test collections, and judging the relevancy of all documents per topic in creating relevance judgment through human assessors is infeasible. Due to the large amount of documents that requires judgment, there are possible errors introduced by human assessors because of disagreements. The paper aims to discuss these issues. Design/methodology/approach - This study explores exponential variation and document ranking methods that generate a reliable set of relevance judgments (pseudo relevance judgments) to reduce human efforts. These methods overcome problems with large amounts of documents for judgment while avoiding human disagreement errors during the judgment process. This study utilizes two key factors: number of occurrences of each document per topic from all the system runs; and document rankings to generate the alternate methods. Findings - The effectiveness of the proposed method is evaluated using the correlation coefficient of ranked systems using mean average precision scores between the original Text REtrieval Conference (TREC) relevance judgments and pseudo relevance judgments. The results suggest that the proposed document ranking method with a pool depth of 100 could be a reliable alternative to reduce human effort and disagreement errors involved in generating TREC-like relevance judgments. Originality/value - Simple methods proposed in this study show improvement in the correlation coefficient in generating alternate relevance judgment without human assessors while contributing to information retrieval evaluation.", "paper_title": "Ranking retrieval systems using pseudo relevance judgments", "paper_id": "WOS:000366468500005"}