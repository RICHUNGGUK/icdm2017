{"auto_keywords": [{"score": 0.027239775275477923, "phrase": "graphical_interpretation"}, {"score": 0.00481495049065317, "phrase": "cost-sensitive_naive_bayes_classifiers"}, {"score": 0.0038925903520043623, "phrase": "classification_decision"}, {"score": 0.0038614646772981848, "phrase": "bayesian_decision_theory"}, {"score": 0.0038152404172111815, "phrase": "fundamental_statistical_approach"}, {"score": 0.003739421738326676, "phrase": "pattern_classification"}, {"score": 0.0033822643082593285, "phrase": "loss_function"}, {"score": 0.003083793024782014, "phrase": "formal_justification"}, {"score": 0.0030468495296763617, "phrase": "decision_function"}, {"score": 0.003010347277067459, "phrase": "bayesian_decision_framework"}, {"score": 0.002915125480162171, "phrase": "bayesian_risk"}, {"score": 0.0028456853685842293, "phrase": "empirical_decision_function"}, {"score": 0.0028115867500986638, "phrase": "domingos"}, {"score": 0.002789080722621009, "phrase": "pazzani"}, {"score": 0.002722634472231842, "phrase": "new_decision_function"}, {"score": 0.002615381301695394, "phrase": "cartesian_plane"}, {"score": 0.0025326211341475903, "phrase": "different_approaches"}, {"score": 0.0024922258873137093, "phrase": "best_decision"}, {"score": 0.002452654790195864, "phrase": "nb"}, {"score": 0.0024151509669988846, "phrase": "gaussian"}, {"score": 0.002394203141882866, "phrase": "bernoulli"}, {"score": 0.0023748640492376816, "phrase": "multinomial"}, {"score": 0.002347243035814208, "phrase": "poisson"}, {"score": 0.0023182554418193927, "phrase": "different_standard_collections"}, {"score": 0.0021738146596180404, "phrase": "new_perspectives"}, {"score": 0.002156402793481102, "phrase": "new_research_studies"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Cost sensitive learning", " Bayesian decision theory", " Binary classification", " Classical probabilistic models"], "paper_abstract": "Practical classification problems often involve some kind of trade-off between the decisions a classifier may take. Indeed, it may be the case that decisions are not equally good or costly; therefore, it is important for the classifier to be able to predict the risk associated with each classification decision. Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification. The objective is to quantify the trade-off between various classification decisions using probability and the costs that accompany such decisions. Within this framework, a loss function measures the rates of the costs and the risk in taking one decision over another. In this paper, we give a formal justification for a decision function under the Bayesian decision framework that comprises (i) the minimisation of Bayesian risk and (ii) an empirical decision function found by Domingos and Pazzani (1997). This new decision function has a very intuitive geometrical interpretation that can be explored on a Cartesian plane. We use this graphical interpretation to analyse different approaches to find the best decision on four different Naive Bayes (NB) classifiers: Gaussian, Bernoulli, Multinomial, and Poisson, on different standard collections. We show that the graphical interpretation significantly improves the understanding of the models and opens new perspectives for new research studies. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "A new decision to take for cost-sensitive Naive Bayes classifiers", "paper_id": "WOS:000340307000003"}