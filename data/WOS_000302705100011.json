{"auto_keywords": [{"score": 0.043110099611702445, "phrase": "target_domain"}, {"score": 0.010612387000973441, "phrase": "domain_adaptation"}, {"score": 0.009332472925213033, "phrase": "target_classifier"}, {"score": 0.009238196323479223, "phrase": "label_prediction"}, {"score": 0.008900518189301192, "phrase": "base_classifiers"}, {"score": 0.0047823219739028325, "phrase": "multiple_sources"}, {"score": 0.004733791336312872, "phrase": "domain-dependent_regularization_approach"}, {"score": 0.004591120854640081, "phrase": "new_framework"}, {"score": 0.004437613070853689, "phrase": "multiple_source_domain_adaption_problem"}, {"score": 0.004303830426772906, "phrase": "robust_decision_function"}, {"score": 0.00395291546135423, "phrase": "labeled_instances"}, {"score": 0.003899476621346112, "phrase": "source_domains"}, {"score": 0.003680274684654909, "phrase": "new_domain-dependent_regularizer"}, {"score": 0.003520971639466546, "phrase": "similar_decision_values"}, {"score": 0.003485196263406553, "phrase": "relevant_base_classifiers"}, {"score": 0.0034497831301482144, "phrase": "unlabeled_instances"}, {"score": 0.003380029058401904, "phrase": "newly_proposed_regularizer"}, {"score": 0.0027366287564324966, "phrase": "sparsity_regularizer"}, {"score": 0.0026995881394357504, "phrase": "sparse_target_classifier"}, {"score": 0.0026721361773711315, "phrase": "support_vectors"}, {"score": 0.002556359086441866, "phrase": "test_instance"}, {"score": 0.0025131738347596586, "phrase": "univerdam"}, {"score": 0.0023960880844958485, "phrase": "universum"}, {"score": 0.00235560370536971, "phrase": "generalization_ability"}, {"score": 0.0022535101994966907, "phrase": "large-scale_video_concept_detection_task"}, {"score": 0.0021928902836049363, "phrase": "email_spam_datasets"}, {"score": 0.002177991273791155, "phrase": "document_retrieval"}, {"score": 0.002163193271881447, "phrase": "comprehensive_experiments"}, {"score": 0.0021049977753042253, "phrase": "existing_multiple_source_domain_adaptation_methods"}], "paper_keywords": ["Domain adaptation machine", " domain-dependent regularizer", " multiple source domain adaptation"], "paper_abstract": "In this paper, we propose a new framework called domain adaptation machine (DAM) for the multiple source domain adaption problem. Under this framework, we learn a robust decision function (referred to as target classifier) for label prediction of instances from the target domain by leveraging a set of base classifiers which are prelearned by using labeled instances either from the source domains or from the source domains and the target domain. With the base classifiers, we propose a new domain-dependent regularizer based on smoothness assumption, which enforces that the target classifier shares similar decision values with the relevant base classifiers on the unlabeled instances from the target domain. This newly proposed regularizer can be readily incorporated into many kernel methods (e. g., support vector machines (SVM), support vector regression, and least-squares SVM (LS-SVM)). For domain adaptation, we also develop two new domain adaptation methods referred to as FastDAM and UniverDAM. In FastDAM, we introduce our proposed domain-dependent regularizer into LS-SVM as well as employ a sparsity regularizer to learn a sparse target classifier with the support vectors only from the target domain, which thus makes the label prediction on any test instance very fast. In UniverDAM, we additionally make use of the instances from the source domains as Universum to further enhance the generalization ability of the target classifier. We evaluate our two methods on the challenging TRECIVD 2005 dataset for the large-scale video concept detection task as well as on the 20 newsgroups and email spam datasets for document retrieval. Comprehensive experiments demonstrate that FastDAM and UniverDAM outperform the existing multiple source domain adaptation methods for the two applications.", "paper_title": "Domain Adaptation from Multiple Sources: A Domain-Dependent Regularization Approach", "paper_id": "WOS:000302705100011"}