{"auto_keywords": [{"score": 0.04957466661795797, "phrase": "sparse-coding_framework"}, {"score": 0.015719716506582538, "phrase": "unlabeled_data"}, {"score": 0.009457991950643367, "phrase": "new_approach"}, {"score": 0.0047306971679760175, "phrase": "human_activity_recognition"}, {"score": 0.004566567411073082, "phrase": "ubiquitous_and_mobile_computing"}, {"score": 0.004486640769041646, "phrase": "current_supervised_learning_approaches"}, {"score": 0.00422516469733364, "phrase": "sensor_data"}, {"score": 0.004136548839735555, "phrase": "prior_expert_knowledge"}, {"score": 0.004064117897000019, "phrase": "domain_boundaries"}, {"score": 0.0039648318122148555, "phrase": "unlabeled_sample_data"}, {"score": 0.003923023699005215, "phrase": "effective_activity_recognizers"}, {"score": 0.003800215925586887, "phrase": "ground_truth_annotation"}, {"score": 0.003760137090849196, "phrase": "model_estimation"}, {"score": 0.003603989174919527, "phrase": "contemporary_smartphones"}, {"score": 0.0034421126037849, "phrase": "self-taught_learning_paradigm"}, {"score": 0.0033817999428800457, "phrase": "over-complete_set"}, {"score": 0.003287482882222731, "phrase": "inherent_patterns"}, {"score": 0.0032527939106509946, "phrase": "activity_data"}, {"score": 0.003207108833360533, "phrase": "raw_sensor_data"}, {"score": 0.0031732652552570644, "phrase": "feature_space"}, {"score": 0.003095674983836725, "phrase": "effective_feature_extraction"}, {"score": 0.0030306762103395842, "phrase": "learned_feature_representations"}, {"score": 0.0030093137393901355, "phrase": "classification_backends"}, {"score": 0.0029565620771207003, "phrase": "small_amounts"}, {"score": 0.0029357204796552653, "phrase": "labeled_training_data"}, {"score": 0.0027741761880779535, "phrase": "recognition_tasks"}, {"score": 0.0027546167254869493, "phrase": "sensor_modalities"}, {"score": 0.002687235334499192, "phrase": "transportation_mode_analysis_task"}, {"score": 0.002658863179011018, "phrase": "popular_task"}, {"score": 0.0024597305738431226, "phrase": "supervised_learning"}, {"score": 0.0023826206937008257, "phrase": "practical_potential"}, {"score": 0.002275477757478827, "phrase": "popular_opportunity_dataset"}, {"score": 0.00225144317399471, "phrase": "learning_approach"}, {"score": 0.0021501863832966966, "phrase": "daily_living"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Activity recognition", " Sparse-coding", " Machine learning", " Unsupervised learning"], "paper_abstract": "We propose a sparse-coding framework for activity recognition in ubiquitous and mobile computing that alleviates two fundamental problems of current supervised learning approaches. (i) It automatically derives a compact, sparse and meaningful feature representation of sensor data that does not rely on prior expert knowledge and generalizes well across domain boundaries. (ii) It exploits unlabeled sample data for bootstrapping effective activity recognizers, i.e., substantially reduces the amount of ground truth annotation required for model estimation. Such unlabeled data is easy to obtain, e. g., through contemporary smartphones carried by users as they go about their everyday activities. Based on the self-taught learning paradigm we automatically derive an over-complete set of basis vectors from unlabeled data that captures inherent patterns present within activity data. Through projecting raw sensor data onto the feature space defined by such over-complete sets of basis vectors effective feature extraction is pursued. Given these learned feature representations, classification backends are then trained using small amounts of labeled training data. We study the new approach in detail using two datasets which differ in terms of the recognition tasks and sensor modalities. Primarily we focus on a transportation mode analysis task, a popular task in mobile-phone based sensing. The sparse-coding framework demonstrates better performance than the state-of-the-art in supervised learning approaches. More importantly, we show the practical potential of the new approach by successfully evaluating its generalization capabilities across both domain and sensor modalities by considering the popular Opportunity dataset. Our feature learning approach outperforms state-of-the-art approaches to analyzing activities of daily living. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Using unlabeled data in a sparse-coding framework for human activity recognition", "paper_id": "WOS:000345523900017"}