{"auto_keywords": [{"score": 0.04557122035267361, "phrase": "estimation_algorithm"}, {"score": 0.028145961928881916, "phrase": "memory_requirement"}, {"score": 0.004747622217685802, "phrase": "bartolucci"}, {"score": 0.004703258776569541, "phrase": "besag"}, {"score": 0.004028054020045027, "phrase": "observed_series"}, {"score": 0.003825173769489887, "phrase": "conditional_distribution"}, {"score": 0.003771631577951067, "phrase": "latent_state"}, {"score": 0.003649585593445665, "phrase": "previous_state"}, {"score": 0.003598492288058048, "phrase": "observed_data"}, {"score": 0.0034332736827447654, "phrase": "well-known_baum-welch_recursions"}, {"score": 0.0032449610741097992, "phrase": "sample_size"}, {"score": 0.0031995142224983094, "phrase": "proposed_algorithm"}, {"score": 0.003081400693055518, "phrase": "dummy_renormalizations"}, {"score": 0.003038237581227068, "phrase": "numerical_problems"}, {"score": 0.0028986672783671147, "phrase": "global_decoding"}, {"score": 0.0028580564897994175, "phrase": "latent_sequence"}, {"score": 0.0027395968397212053, "phrase": "viterbi_method"}, {"score": 0.002688532127821979, "phrase": "consistent_reduction"}, {"score": 0.002552946390854313, "phrase": "proposed_approach"}, {"score": 0.002470237978694949, "phrase": "computing_time"}, {"score": 0.0023566987138026285, "phrase": "baum-welch_recursions"}, {"score": 0.0023127544795063263, "phrase": "so-called_linear_memory_algorithm"}, {"score": 0.002291089893848235, "phrase": "churbanov"}, {"score": 0.0022696277881969896, "phrase": "winters-hilt"}, {"score": 0.0021249064184347658, "phrase": "hm_model"}, {"score": 0.0021049977753042253, "phrase": "continuous_time-series_data"}], "paper_keywords": ["expectation-maximization algorithm", " forward-backward recursions", " global decoding", " latent Markov models", " Viterbi algorithm"], "paper_abstract": "We develop the recursion for hidden Markov (HM) models proposed by Bartolucci and Besag (2002), and we show how it may be used to implement an estimation algorithm for these models that requires an amount of memory not depending on the length of the observed series of data. This recursion allows us to obtain the conditional distribution of the latent state at every occasion, given the previous state and the observed data. With respect to the estimation algorithm based on the well-known Baum-Welch recursions, which requires an amount of memory that increases with the sample size, the proposed algorithm also has the advantage of not requiring dummy renormalizations to avoid numerical problems. Moreover, it directly allows us to perform global decoding of the latent sequence of states, without the need of a Viterbi method and with a consistent reduction of the memory requirement with respect to the latter. The proposed approach is compared, in terms of computing time and memory requirement, with the algorithm based on the Baum-Welch recursions and with the so-called linear memory algorithm of Churbanov and Winters-Hilt. The comparison is also based on a series of simulations involving an HM model for continuous time-series data.", "paper_title": "A New Constant Memory Recursion for Hidden Markov Models", "paper_id": "WOS:000331122100001"}