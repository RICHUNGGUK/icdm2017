{"auto_keywords": [{"score": 0.048239757080509704, "phrase": "particular_class"}, {"score": 0.0047587234712024775, "phrase": "maximally_sparse_coefficient_expansions"}, {"score": 0.00468476935399643, "phrase": "regression_problem"}, {"score": 0.00459393336391459, "phrase": "concave_penalty_functions"}, {"score": 0.00452252857066346, "phrase": "bayesian_perspective"}, {"score": 0.00424779023663186, "phrase": "sparsity-inducing_prior_distribution"}, {"score": 0.0041167185402778425, "phrase": "variational_techniques"}, {"score": 0.0039122637074546895, "phrase": "scaled_gaussian_distributions"}, {"score": 0.003821342577980031, "phrase": "latent_variables"}, {"score": 0.0037915057447460133, "phrase": "alternative_bayesian_algorithms"}, {"score": 0.003717925116469845, "phrase": "latent_variable_space_lever-aging_this_variational_representation"}, {"score": 0.0033708244679086265, "phrase": "underlying_cost_functions"}, {"score": 0.003228517923065361, "phrase": "relevant_theoretical_properties"}, {"score": 0.003141093559655865, "phrase": "type_ii."}, {"score": 0.00310435218261112, "phrase": "common_set"}, {"score": 0.0030800962618484844, "phrase": "auxiliary_functions"}, {"score": 0.0028701222153940283, "phrase": "latent_variable_space"}, {"score": 0.002847691227421257, "phrase": "direct_comparisons"}, {"score": 0.0028143720888311537, "phrase": "coefficient_space"}, {"score": 0.002748895580860912, "phrase": "type_ii"}, {"score": 0.00268493828558848, "phrase": "standard_map_estimation"}, {"score": 0.0024822751900485758, "phrase": "possible_type"}, {"score": 0.0023220573674955776, "phrase": "local_minima"}, {"score": 0.002259123228671651, "phrase": "global_minimum"}, {"score": 0.0021551702945512494, "phrase": "appropriate_descent_method"}, {"score": 0.0021049977753042253, "phrase": "maximally_sparse_solution"}], "paper_keywords": ["Bayesian learning", " compressive sensing", " latent variable models", " source localization", " sparse priors", " sparse representations", " underdetermined inverse problems"], "paper_abstract": "Many practical methods for finding maximally sparse coefficient expansions involve solving a regression problem using a particular class of concave penalty functions. From a Bayesian perspective, this process is equivalent to maximum a posteriori (MAP) estimation using a sparsity-inducing prior distribution (Type I estimation). Using variational techniques, this distribution can always be conveniently expressed as a maximization over scaled Gaussian distributions modulated by a set of latent variables. Alternative Bayesian algorithms, which operate in latent variable space lever-aging this variational representation, lead to sparse estimators reflecting posterior information beyond the mode (Type II estimation). Currently, it is unclear how the underlying cost functions of Type I and Type II relate, nor what relevant theoretical properties exist, especially with regard to Type II. Herein a common set of auxiliary functions is used to conveniently express both Type I and Type II cost functions in either coefficient or latent variable space facilitating direct comparisons. In coefficient space, the analysis reveals that Type II is exactly equivalent to performing standard MAP estimation using a particular class of dictionary- and noise-dependent, nonfactorial coefficient priors. One prior (at least) from this class maintains several desirable advantages over all possible Type I methods and utilizes a novel, nonconvex approximation to the l(0) norm with most, and in certain quantifiable conditions all, local minima smoothed away. Importantly, the global minimum is always left unaltered unlike standard l(1)-norm relaxations. This ensures that any appropriate descent method is guaranteed to locate the maximally sparse solution.", "paper_title": "Latent Variable Bayesian Models for Promoting Sparsity", "paper_id": "WOS:000295738800050"}