{"auto_keywords": [{"score": 0.03618042519491515, "phrase": "ifn_approach"}, {"score": 0.012633645981778567, "phrase": "acoustic_differences"}, {"score": 0.010781924433891817, "phrase": "neutral_speech"}, {"score": 0.00481495049065317, "phrase": "iterative_feature_normalization_scheme_for_automatic_emotion_detection"}, {"score": 0.004487541157100498, "phrase": "robust_emotion_recognition_system"}, {"score": 0.00422177384286051, "phrase": "natural_approach"}, {"score": 0.003953068310598579, "phrase": "normalization_scheme"}, {"score": 0.003825173769489887, "phrase": "emotional_classes"}, {"score": 0.0037188360359702182, "phrase": "iterative_feature_normalization"}, {"score": 0.0035648266669315943, "phrase": "unsupervised_front-end"}, {"score": 0.00348202932445197, "phrase": "emotion_detection"}, {"score": 0.0031845067058084583, "phrase": "inter-emotional_variability"}, {"score": 0.0028446460307503343, "phrase": "feature_normalization_parameters"}, {"score": 0.002778529097886437, "phrase": "affine_transformation"}, {"score": 0.0027139447110929586, "phrase": "neutral_and_emotional_speech"}, {"score": 0.0025770807002632877, "phrase": "emotion_detection_system"}, {"score": 0.0025290375256888883, "phrase": "consecutive_iterations"}, {"score": 0.0024241817763927163, "phrase": "iemocap_database"}, {"score": 0.0023902026696615473, "phrase": "data_set"}, {"score": 0.0023566987138026285, "phrase": "free_uncontrolled_recording_conditions"}, {"score": 0.0023346234464901978, "phrase": "different_evaluation_configurations"}, {"score": 0.002196077907454079, "phrase": "better_performance"}, {"score": 0.0021049977753042253, "phrase": "global_normalization"}], "paper_keywords": ["Emotion recognition", " speaker normalization", " emotion", " features normalization"], "paper_abstract": "The externalization of emotion is intrinsically speaker-dependent. A robust emotion recognition system should be able to compensate for these differences across speakers. A natural approach is to normalize the features before training the classifiers. However, the normalization scheme should not affect the acoustic differences between emotional classes. This study presents the iterative feature normalization (IFN) framework, which is an unsupervised front-end, especially designed for emotion detection. The IFN approach aims to reduce the acoustic differences, between the neutral speech across speakers, while preserving the inter-emotional variability in expressive speech. This goal is achieved by iteratively detecting neutral speech for each speaker, and using this subset to estimate the feature normalization parameters. Then, an affine transformation is applied to both neutral and emotional speech. This process is repeated till the results from the emotion detection system are consistent between consecutive iterations. The IFN approach is exhaustively evaluated using the IEMOCAP database and a data set obtained under free uncontrolled recording conditions with different evaluation configurations. The results show that the systems trained with the IFN approach achieve better performance than systems trained either without normalization or with global normalization.", "paper_title": "Iterative Feature Normalization Scheme for Automatic Emotion Detection from Speech", "paper_id": "WOS:000333283900005"}