{"auto_keywords": [{"score": 0.03952148495928651, "phrase": "visual_features"}, {"score": 0.008680906951304242, "phrase": "generic_codebook"}, {"score": 0.00481495049065317, "phrase": "generic_codebooks"}, {"score": 0.0047559432344272, "phrase": "semantic_image_retrieval"}, {"score": 0.004204054047427536, "phrase": "text_searches"}, {"score": 0.0034720284451902083, "phrase": "specific_keyword-model"}, {"score": 0.0030497740719977835, "phrase": "algorithm_clusters"}, {"score": 0.002938821840519756, "phrase": "full_imageset"}, {"score": 0.002471973283582411, "phrase": "training_set"}, {"score": 0.0023967582707115354, "phrase": "linear_combination"}, {"score": 0.0021845387990972543, "phrase": "image_retrieval_scenario"}, {"score": 0.0021311850321045767, "phrase": "real-world_photos"}, {"score": 0.0021049977753042253, "phrase": "corresponding_manual_annotations"}], "paper_keywords": [""], "paper_abstract": "This paper is about automatically annotating images with keywords in order to be able to retrieve images with text searches. Our approach is to model keywords 'such as 'mountain' and 'city' in terms of visual features that were extracted from images. In contrast to other algorithms, each specific keyword-model considers not only its own training data but also the whole training set by utilizing correlations of visual features to refine its own model. Initially, the algorithm clusters all visual features extracted from the full imageset, captures its salient structure (e.g. mixture of clusters or patterns) and represents this as a generic codebook. Then keywords that were associated with images in the training set are encoded as a linear combination of patterns from the generic codebook. We evaluate the validity of our approach in an image retrieval scenario with two distinct large datasets of real-world photos and corresponding manual annotations.", "paper_title": "Logistic regression of generic codebooks for semantic image retrieval", "paper_id": "WOS:000239566500005"}