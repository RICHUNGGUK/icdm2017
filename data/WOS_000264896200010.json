{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "bayesian_k-means"}, {"score": 0.004466519142879166, "phrase": "new_class"}, {"score": 0.004081385289516914, "phrase": "hidden_variables"}, {"score": 0.00396050388346298, "phrase": "random_parameters"}, {"score": 0.0036188435232325337, "phrase": "classical_expectation-maximization_algorithm"}, {"score": 0.003306559295382181, "phrase": "hard_assignments"}, {"score": 0.003113441660343867, "phrase": "data_structures"}, {"score": 0.0029984960344766705, "phrase": "conga_lines"}, {"score": 0.0026986685585382347, "phrase": "model_structure"}, {"score": 0.002465593351432208, "phrase": "important_example"}, {"score": 0.0023745087648471613, "phrase": "top-down_bayesian_k-means_algorithm"}, {"score": 0.0023214774334744713, "phrase": "bottom-up_agglomerative_clustering_algorithm"}, {"score": 0.0021049977753042253, "phrase": "alternative_algorithms"}], "paper_keywords": [""], "paper_abstract": "We introduce a new class of \"maximization-expectation\" (ME) algorithms where we maximize over hidden variables but marginalize over random parameters. This reverses the roles of expectation and maximization in the classical expectation-maximization algorithm. In the context of clustering, we argue that these hard assignments open the door to very fast implementations based on data structures such as kd-trees and conga lines. The marginalization over parameters ensures that we retain the ability to infer model structure (i. e., number of clusters). As an important example, we discuss a top-down Bayesian k-means algorithm and a bottom-up agglomerative clustering algorithm. In experiments, we compare these algorithms against a number of alternative algorithms that have recently appeared in the literature.", "paper_title": "Bayesian k-Means as a \"Maximization-Expectation\" Algorithm", "paper_id": "WOS:000264896200010"}