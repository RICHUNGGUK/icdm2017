{"auto_keywords": [{"score": 0.049521347818116, "phrase": "reinforcement_learning_agents"}, {"score": 0.0442639629630938, "phrase": "learning_procedure"}, {"score": 0.03909673303902588, "phrase": "novel_method"}, {"score": 0.00481495049065317, "phrase": "task_models"}, {"score": 0.004675930527604869, "phrase": "main_objective"}, {"score": 0.004621447740470602, "phrase": "transfer_learning"}, {"score": 0.00440976345264104, "phrase": "previous_learned_task"}, {"score": 0.0041586840496756474, "phrase": "new_and_more_complex_task"}, {"score": 0.004014923642015515, "phrase": "suitable_solution"}, {"score": 0.0038534477978822133, "phrase": "reinforcement_learning_tasks"}, {"score": 0.003367077034382727, "phrase": "source_task"}, {"score": 0.0032315723330399375, "phrase": "relevant_but_different_target_task"}, {"score": 0.003101503914610488, "phrase": "target_task's_agent"}, {"score": 0.00304737045343781, "phrase": "hybrid_approach"}, {"score": 0.0029766550209309127, "phrase": "model-free_and_model-based_learning"}, {"score": 0.002790511479398411, "phrase": "source_task_model"}, {"score": 0.002600661525224505, "phrase": "potential-based_reward_shaping_functions"}, {"score": 0.0024813081174066653, "phrase": "proposed_approaches"}, {"score": 0.0024379730432321656, "phrase": "significant_results"}, {"score": 0.002409503258044183, "phrase": "performance_improvements"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement Learning", " Transfer learning", " Model-based Reinforcement Learning"], "paper_abstract": "The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning comprises a suitable solution for speeding up the learning procedure in Reinforcement Learning tasks. This work proposes a novel method for transferring models to Reinforcement Learning agents. The models of the transition and reward functions of a source task, will be transferred to a relevant but different target task. The learning algorithm of the target task's agent takes a hybrid approach, implementing both model-free and model-based learning, in order to fully exploit the presence of a source task model. Moreover, a novel method is proposed for transferring models of potential-based reward shaping functions. The empirical evaluation, of the proposed approaches, demonstrated significant results and performance improvements in the 3D Mountain Car and Server Job Scheduling tasks, by successfully using the models generated from their corresponding source tasks. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Transferring task models in Reinforcement Learning agents", "paper_id": "WOS:000317163600004"}