{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "linear_bayes"}, {"score": 0.004683540676363745, "phrase": "machine_and_statistical_learning_techniques"}, {"score": 0.004610058444728656, "phrase": "almost_all_online_advertisement_systems"}, {"score": 0.0042426974826585695, "phrase": "multi-armed_bandit_problem"}, {"score": 0.004062057297453209, "phrase": "side_information"}, {"score": 0.0040300463421312225, "phrase": "associative_reinforcement_learning"}, {"score": 0.003935513160201076, "phrase": "specific_content"}, {"score": 0.003399466007862412, "phrase": "conditional_probability_paradigm"}, {"score": 0.003359334465113303, "phrase": "bayes'_theorem"}, {"score": 0.0032034752726003025, "phrase": "bayes'_rule"}, {"score": 0.0029830992938695007, "phrase": "large_contextual_information"}, {"score": 0.0029362173680378624, "phrase": "contextual-bandits_problems"}, {"score": 0.0026594033724490172, "phrase": "second_place"}, {"score": 0.002408623172021922, "phrase": "deterministic_exploration"}, {"score": 0.0023896108055582615, "phrase": "exploitation_issue"}, {"score": 0.0023150468182777813, "phrase": "proposed_method"}, {"score": 0.002278639906498682, "phrase": "effective_dynamic_trade-off"}, {"score": 0.002155656133617155, "phrase": "random_number_generator"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Contextual bandits", " Online advertising", " Recommender systems", " One-to-one Marketing", " Empirical Bayes"], "paper_abstract": "Machine and Statistical Learning techniques are used in almost all online advertisement systems. The problem of discovering which content is more demanded (e.g. receive more clicks) can be modeled as a multi-armed bandit problem. Contextual bandits (i.e., bandits with covariates, side information or associative reinforcement learning) associate, to each specific content, several features that define the \"context\" in which it appears (e.g. user, web page, time, region). This problem can be studied in the stochastic/statistical setting by means of the conditional probability paradigm using the Bayes' theorem. However, for very large contextual information and/or real-time constraints, the exact calculation of the Bayes' rule is computationally infeasible. In this article, we present a method that is able to handle large contextual information for learning in contextual-bandits problems. This method was tested in the Challenge on Yahoo! dataset at ICML2012's Workshop \"new Challenges for Exploration & Exploitation 3\", obtaining the second place. Its basic exploration policy is deterministic in the sense that for the same input data (as a time-series) the same results are obtained. We address the deterministic exploration vs. exploitation issue, explaining the way in which the proposed method deterministically finds an effective dynamic trade-off based solely in the input-data, in contrast to other methods that use a random number generator. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Linear Bayes policy for learning in contextual-bandits", "paper_id": "WOS:000324663000024"}