{"auto_keywords": [{"score": 0.032777148853852064, "phrase": "gaussians"}, {"score": 0.00481495049065317, "phrase": "discriminant_analysis"}, {"score": 0.004489952932659799, "phrase": "high-_dimensional_data"}, {"score": 0.004434910775034831, "phrase": "large_variety"}, {"score": 0.004221379262804442, "phrase": "specific_type"}, {"score": 0.004186799638538032, "phrase": "data_distribution"}, {"score": 0.0038561471948364723, "phrase": "class_pdf"}, {"score": 0.0036855132234738736, "phrase": "da_algorithm"}, {"score": 0.0035515148917169173, "phrase": "and-_error"}, {"score": 0.0034083145672246067, "phrase": "single_formulation"}, {"score": 0.003204232566468624, "phrase": "underlying_distribution"}, {"score": 0.0030123335878664064, "phrase": "major_problem"}, {"score": 0.0029027394930867902, "phrase": "optimal_number"}, {"score": 0.0025130460190930554, "phrase": "extensive_experimental_results"}, {"score": 0.002471973283582411, "phrase": "five_databases"}, {"score": 0.0024116163689723354, "phrase": "linear_discriminant_analysis"}, {"score": 0.0023920733188276316, "phrase": "lda"}, {"score": 0.002304754792740636, "phrase": "heteroscedastic_lda"}, {"score": 0.0022577559526646904, "phrase": "nonparametric_da"}, {"score": 0.0022392316041705156, "phrase": "nda"}], "paper_keywords": ["feature extraction", " discriminant analysis", " pattern recognition", " classification", " eigenvalue decomposition", " stability criterion", " mixture of Gaussians"], "paper_abstract": "Over the years, many Discriminant Analysis ( DA) algorithms have been proposed for the study of high- dimensional data in a large variety of problems. Each of these algorithms is tuned to a specific type of data distribution ( that which best models the problem at hand). Unfortunately, in most problems the form of each class pdf is a priori unknown, and the selection of the DA algorithm that best fits our data is done over trial- and- error. Ideally, one would like to have a single formulation which can be used for most distribution types. This can be achieved by approximating the underlying distribution of each class with a mixture of Gaussians. In this approach, the major problem to be addressed is that of determining the optimal number of Gaussians per class, i. e., the number of subclasses. In this paper, two criteria able to find the most convenient division of each class into a set of subclasses are derived. Extensive experimental results are shown using five databases. Comparisons are given against Linear Discriminant Analysis ( LDA), Direct LDA ( DLDA), Heteroscedastic LDA ( HLDA), Nonparametric DA ( NDA), and Kernel- Based LDA ( K- LDA). We show that our method is always the best or comparable to the best.", "paper_title": "Subclass discriminant analysis", "paper_id": "WOS:000238162400009"}