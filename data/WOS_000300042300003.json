{"auto_keywords": [{"score": 0.03661637623619357, "phrase": "sp"}, {"score": 0.02766689192200228, "phrase": "bt"}, {"score": 0.008464164283291891, "phrase": "large-scale_multicore_clusters"}, {"score": 0.006872229574175989, "phrase": "mpi"}, {"score": 0.005632264875976437, "phrase": "bt."}, {"score": 0.0049160416306784594, "phrase": "jaguar"}, {"score": 0.004768378525561471, "phrase": "nas_parallel_benchmarks_sp"}, {"score": 0.004608879242346434, "phrase": "nas_parallel_benchmarks"}, {"score": 0.004498213310668266, "phrase": "well-known_applications"}, {"score": 0.00436890075128947, "phrase": "parallel_systems"}, {"score": 0.004284755353695709, "phrase": "multicore_clusters"}, {"score": 0.004222707054751191, "phrase": "natural_programming_paradigm"}, {"score": 0.004181839414007266, "phrase": "hybrid_programs"}, {"score": 0.004121288306013858, "phrase": "openmp"}, {"score": 0.0035100601949451028, "phrase": "scalar_pentadiagonal"}, {"score": 0.00342568771304873, "phrase": "block_tridiagonal"}, {"score": 0.0033271045779740683, "phrase": "mpi_npb"}, {"score": 0.003215659239270654, "phrase": "comparative_approach"}, {"score": 0.002861062731782219, "phrase": "mpi_counterparts"}, {"score": 0.0027383842769094354, "phrase": "argonne_national_laboratory"}, {"score": 0.0026337482179592422, "phrase": "oak_ridge_national_laboratory"}, {"score": 0.0025085435314540837, "phrase": "mpi_sp"}, {"score": 0.002389276641014013, "phrase": "mpi_bt"}, {"score": 0.002320449588711849, "phrase": "intrepid"}, {"score": 0.002242647404307089, "phrase": "performance_tools"}, {"score": 0.0021049977753042253, "phrase": "performance_characteristics"}], "paper_keywords": ["performance characteristics", " hybrid MPI", " OpenMP", " NAS Parallel Benchmarks", " multicore clusters"], "paper_abstract": "The NAS Parallel Benchmarks (NPB) are well-known applications with fixed algorithms for evaluating parallel systems and tools. Multicore clusters provide a natural programming paradigm for hybrid programs, whereby OpenMP can be used with the data sharing with the multicores that comprise a node, and MPI can be used with the communication between nodes. In this paper, we use Scalar Pentadiagonal (SP) and Block Tridiagonal (BT) benchmarks of MPI NPB 3.3 as a basis for a comparative approach to implement hybrid MPI/OpenMP versions of SP and BT. In particular, we can compare the performance of the hybrid SP and BT with the MPI counterparts on large-scale multicore clusters, Intrepid (BlueGene/P) at Argonne National Laboratory and Jaguar (Cray XT4/5) at Oak Ridge National Laboratory. Our performance results indicate that the hybrid SP outperforms the MPI SP by up to 20.76%, and the hybrid BT outperforms the MPI BT by up to 8.58% on up to 10 000 cores on Intrepid and Jaguar. We also use performance tools and MPI trace libraries available on these clusters to further investigate the performance characteristics of the hybrid SP and BT.", "paper_title": "Performance Characteristics of Hybrid MPI/OpenMP Implementations of NAS Parallel Benchmarks SP and BT on Large-Scale Multicore Clusters", "paper_id": "WOS:000300042300003"}