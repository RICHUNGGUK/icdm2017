{"auto_keywords": [{"score": 0.030353144436730508, "phrase": "solution_quality"}, {"score": 0.00481495049065317, "phrase": "convex_relaxations"}, {"score": 0.0047689174754929195, "phrase": "guaranteed_solution_bounds"}, {"score": 0.004723322467153553, "phrase": "active_learning_techniques"}, {"score": 0.004589124394375718, "phrase": "human_effort"}, {"score": 0.004523455475492876, "phrase": "data_instances"}, {"score": 0.00433200902751133, "phrase": "large_amounts"}, {"score": 0.00429057323623293, "phrase": "unlabeled_data"}, {"score": 0.004168618849008299, "phrase": "exemplar_instances"}, {"score": 0.0041287394089001405, "phrase": "manual_annotation"}, {"score": 0.003916098498242888, "phrase": "batch_mode_form"}, {"score": 0.003878625406105021, "phrase": "active_learning"}, {"score": 0.0037683358267785435, "phrase": "data_points"}, {"score": 0.0036611708297251645, "phrase": "unlabeled_set"}, {"score": 0.0034892684482068347, "phrase": "batchrank"}, {"score": 0.0034558655164987134, "phrase": "batchrand"}, {"score": 0.0033575566897536906, "phrase": "batch_selection_task"}, {"score": 0.003309452522760298, "phrase": "np-hard_optimization_problem"}, {"score": 0.0031540120386134058, "phrase": "linear_programming"}, {"score": 0.0030642636185693054, "phrase": "semi-definite_programming"}, {"score": 0.0030058503645435455, "phrase": "batch_selection_problem"}, {"score": 0.0029343925814926787, "phrase": "deterministic_bound"}, {"score": 0.0028235667474151714, "phrase": "first_relaxation"}, {"score": 0.0025892330566358503, "phrase": "first_research_effort"}, {"score": 0.0025521085778978042, "phrase": "mathematical_guarantees"}, {"score": 0.00247944491239733, "phrase": "bmal_problem"}, {"score": 0.002208853811052348, "phrase": "high_quality_solutions"}, {"score": 0.0021563019696857768, "phrase": "real-world_issues"}, {"score": 0.0021049977753042253, "phrase": "class_imbalance"}], "paper_keywords": ["Batch mode active learning", " optimization"], "paper_abstract": "Active learning techniques have gained popularity to reduce human effort in labeling data instances for inducing a classifier. When faced with large amounts of unlabeled data, such algorithms automatically identify the exemplar instances for manual annotation. More recently, there have been attempts towards a batch mode form of active learning, where a batch of data points is simultaneously selected from an unlabeled set. In this paper, we propose two novel batch mode active learning (BMAL) algorithms: BatchRank and BatchRand. We first formulate the batch selection task as an NP-hard optimization problem; we then propose two convex relaxations, one based on linear programming and the other based on semi-definite programming to solve the batch selection problem. Finally, a deterministic bound is derived on the solution quality for the first relaxation and a probabilistic bound for the second. To the best of our knowledge, this is the first research effort to derive mathematical guarantees on the solution quality of the BMAL problem. Our extensive empirical studies on 15 binary, multi-class and multi-label challenging datasets corroborate that the proposed algorithms perform at par with the state-of-the-art techniques, deliver high quality solutions and are robust to real-world issues like label noise and class imbalance.", "paper_title": "Active Batch Selection via Convex Relaxations with Guaranteed Solution Bounds", "paper_id": "WOS:000360813400001"}