{"auto_keywords": [{"score": 0.04619063797532855, "phrase": "image_contents"}, {"score": 0.00481495049065317, "phrase": "semantic_gap_between_image_contents"}, {"score": 0.004770037435656796, "phrase": "tags"}, {"score": 0.004681226806682058, "phrase": "exponential_growth"}, {"score": 0.004261617028111862, "phrase": "noisy_and_sparse_nature"}, {"score": 0.004201991568145053, "phrase": "human_generated_tags"}, {"score": 0.004009175817816061, "phrase": "image_retrieval_tasks"}, {"score": 0.003934540154108188, "phrase": "emerging_research_direction"}, {"score": 0.003861288537079326, "phrase": "low-level_visual_features"}, {"score": 0.003649585593445665, "phrase": "image_retrieval_results"}, {"score": 0.00348202932445197, "phrase": "semantic_gap"}, {"score": 0.0033377940531761985, "phrase": "critical_problem"}, {"score": 0.0032602525602322832, "phrase": "unified_framework"}, {"score": 0.0031399038722901788, "phrase": "two-level_data_fusions"}, {"score": 0.0029816229984366374, "phrase": "unified_graph"}, {"score": 0.0028986672783671147, "phrase": "visual_feature-based_image_similarity_graph"}, {"score": 0.0028580564897994175, "phrase": "image-tag_bipartite_graph"}, {"score": 0.0027916287357898544, "phrase": "novel_random_walk_model"}, {"score": 0.002688532127821979, "phrase": "fusion_parameter"}, {"score": 0.0025171669791790438, "phrase": "presented_framework"}, {"score": 0.0024356148906608025, "phrase": "pseudo_relevance_feedback_process"}, {"score": 0.002291089893848235, "phrase": "content-based_image_retrieval"}, {"score": 0.0022696277881969896, "phrase": "text-based_image_retrieval"}, {"score": 0.0022064376000055764, "phrase": "experimental_analysis"}, {"score": 0.0021755038991725147, "phrase": "large_flickr_dataset"}], "paper_keywords": ["Content-based image retrieval", " image annotation", " random walk", " text-based image retrieval"], "paper_abstract": "With the exponential growth of Web 2.0 applications, tags have been used extensively to describe the image contents on the Web. Due to the noisy and sparse nature in the human generated tags, how to understand and utilize these tags for image retrieval tasks has become an emerging research direction. As the low-level visual features can provide fruitful information, they are employed to improve the image retrieval results. However, it is challenging to bridge the semantic gap between image contents and tags. To attack this critical problem, we propose a unified framework in this paper which stems from a two-level data fusions between the image contents and tags: 1) A unified graph is built to fuse the visual feature-based image similarity graph with the image-tag bipartite graph; 2) A novel random walk model is then proposed, which utilizes a fusion parameter to balance the influences between the image contents and tags. Furthermore, the presented framework not only can naturally incorporate the pseudo relevance feedback process, but also it can be directly applied to applications such as content-based image retrieval, text-based image retrieval, and image annotation. Experimental analysis on a large Flickr dataset shows the effectiveness and efficiency of our proposed framework.", "paper_title": "Bridging the Semantic Gap Between Image Contents and Tags", "paper_id": "WOS:000282306500010"}