{"auto_keywords": [{"score": 0.0444227308407221, "phrase": "distance_measures"}, {"score": 0.03391261728779849, "phrase": "different_data_sets"}, {"score": 0.009928465580783046, "phrase": "distance_normalization"}, {"score": 0.009099312787110053, "phrase": "distance_measure"}, {"score": 0.008986602823557668, "phrase": "data_set"}, {"score": 0.00481495049065317, "phrase": "clustering_validation"}, {"score": 0.004522796947899013, "phrase": "information-theoretic_distance_measures"}, {"score": 0.004283913685201902, "phrase": "uniform_representation"}, {"score": 0.004040688646993605, "phrase": "general_form"}, {"score": 0.004007083686875716, "phrase": "conditional_entropy"}, {"score": 0.0038431888954930083, "phrase": "triangle_law"}, {"score": 0.0035648266669315943, "phrase": "external_measure"}, {"score": 0.0028926147550022607, "phrase": "critical_challenge"}, {"score": 0.0025730425700096365, "phrase": "maximum_value"}, {"score": 0.0024065032567542107, "phrase": "partition_clustering_algorithm"}, {"score": 0.0022886952510350416, "phrase": "normalized_distance_measures"}, {"score": 0.0022696277881969896, "phrase": "better_performance"}, {"score": 0.0022413233627905696, "phrase": "original_distance_measures"}, {"score": 0.002149494537191303, "phrase": "normalized_shannon_distance"}, {"score": 0.00212268510072928, "phrase": "best_performance"}], "paper_keywords": ["Clustering validation", " entropy", " information-theoretic distance measures", " K-means clustering"], "paper_abstract": "This paper studies the generalization and normalization issues of information-theoretic distance measures for clustering validation. Along this line, we first introduce a uniform representation of distance measures, defined as quasi-distance, which is induced based on a general form of conditional entropy. The quasi-distance possesses three properties: symmetry, the triangle law, and the minimum reachable. These properties ensure that the quasi-distance naturally lends itself as the external measure for clustering validation. In addition, we observe that the ranges of the distance measures are different when they apply for clustering validation on different data sets. Therefore, when comparing the performances of clustering algorithms on different data sets, distance normalization is required to equalize ranges of the distance measures. A critical challenge for distance normalization is to obtain the ranges of a distance measure when a data set is provided. To that end, we theoretically analyze the computation of the maximum value of a distance measure for a data set. Finally, we compare the performances of the partition clustering algorithm K-means on various real-world data sets. The experiments show that the normalized distance measures have better performance than the original distance measures when comparing clusterings of different data sets. Also, the normalized Shannon distance has the best performance among four distance measures under study.", "paper_title": "Information-Theoretic Distance Measures for Clustering Validation: Generalization and Normalization", "paper_id": "WOS:000268062400002"}