{"auto_keywords": [{"score": 0.04306043741334064, "phrase": "proposed_algorithm"}, {"score": 0.010612387000973441, "phrase": "semi-supervised_learning"}, {"score": 0.007607717245715026, "phrase": "maximal_margin_classifier"}, {"score": 0.004723856445649838, "phrase": "ssl"}, {"score": 0.004568525347438891, "phrase": "decision_rule"}, {"score": 0.004503525064934708, "phrase": "labeled_and_unlabeled_data"}, {"score": 0.004313996835376177, "phrase": "novel_ssl_algorithm"}, {"score": 0.004232331639807526, "phrase": "multiple_clusters"}, {"score": 0.003977403701783668, "phrase": "first_stage"}, {"score": 0.0038465304237380125, "phrase": "local_cluster_structure"}, {"score": 0.003791763445590174, "phrase": "training_data"}, {"score": 0.0035632774896594524, "phrase": "disjoint_subsets"}, {"score": 0.00349577373145384, "phrase": "second_stage"}, {"score": 0.0033806943112103397, "phrase": "second_order_cone_programming"}, {"score": 0.0033485192855776413, "phrase": "socp"}, {"score": 0.0032382660477699695, "phrase": "inductive_decision_function"}, {"score": 0.0031921317979928406, "phrase": "obtained_subsets"}, {"score": 0.003131637033429892, "phrase": "linear_classification_problems"}, {"score": 0.003072285184441679, "phrase": "knn_algorithm"}, {"score": 0.0028187792953857957, "phrase": "individual_data_points"}, {"score": 0.0026235360404084137, "phrase": "training_points"}, {"score": 0.002524982927936405, "phrase": "big_data"}, {"score": 0.0024771000236931836, "phrase": "large_amount"}, {"score": 0.0024534993283766332, "phrase": "unlabeled_data"}, {"score": 0.0023840346303791032, "phrase": "classification_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Semi-supervised learning", " K-nearest-neighbor", " Support vector machine", " Second order cone programming"], "paper_abstract": "Semi-supervised learning (SSL) involves the training of a decision rule from both labeled and unlabeled data. In this paper, we propose a novel SSL algorithm based on the multiple clusters per class assumption. The proposed algorithm consists of two stages. In the first stage, we aim to capture the local cluster structure of the training data by using the k-nearest-neighbor (kNN) algorithm to split the data into a number of disjoint subsets. In the second stage, a maximal margin classifier based on the second order cone programming (SOCP) is introduced to learn an inductive decision function from the obtained subsets globally. For linear classification problems, once the kNN algorithm has been performed, the proposed algorithm trains a classifier using only the first and second order moments of the subsets without considering individual data points. Since the number of subsets is usually much smaller than the number of training points, the proposed algorithm is efficient for handling big data sets with a large amount of unlabeled data. Despite its simplicity, the classification performance of the proposed algorithm is guaranteed by the maximal margin classifier. We demonstrate the efficiency and effectiveness of the proposed algorithm on both synthetic and real-world data sets. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "A second order cone programming approach for semi-supervised learning", "paper_id": "WOS:000323804100032"}