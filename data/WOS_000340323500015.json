{"auto_keywords": [{"score": 0.043891862951523725, "phrase": "proposed_model"}, {"score": 0.00481495049065317, "phrase": "sparse_principal_component_analysis"}, {"score": 0.004749626554302657, "phrase": "pca"}, {"score": 0.004283019592801715, "phrase": "conventional_sparse_pca_model"}, {"score": 0.0033190180867511605, "phrase": "simple_yet_efficient_algorithm"}, {"score": 0.0026071599759743833, "phrase": "current_sparse_pca_methods"}, {"score": 0.0025537472239953807, "phrase": "proposed_algorithm"}, {"score": 0.002416589459425828, "phrase": "reasonable_local_optimum"}], "paper_keywords": ["noise", " outlier", " principal component analysis", " robustness", " sparsity"], "paper_abstract": "The model for improving the robustness of sparse principal component analysis (PCA) is proposed in this paper. Instead of the l(2)-norm variance utilized in the conventional sparse PCA model, the proposed model maximizes the l(1)-norm variance, which is less sensitive to noise and outlier. To ensure sparsity, l(p)-norm (0 <= p <= 1) constraint, which is more general and effective than l(1)-norm, is considered. A simple yet efficient algorithm is developed against the proposed model. The complexity of the algorithm approximately linearly increases with both of the size and the dimensionality of the given data, which is comparable to or better than the current sparse PCA methods. The proposed algorithm is also proved to converge to a reasonable local optimum of the model. The efficiency and robustness of the algorithm is verified by a series of experiments on both synthetic and digit number image data.", "paper_title": "Robust sparse principal component analysis", "paper_id": "WOS:000340323500015"}