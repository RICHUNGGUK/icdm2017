{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "emotion_recognition"}, {"score": 0.048450141536466075, "phrase": "utterance-level_prosodic_features"}, {"score": 0.025517784084853817, "phrase": "utterance_length"}, {"score": 0.00471193241675062, "phrase": "automatic_emotion_recognition"}, {"score": 0.0045124316982387315, "phrase": "utterance-level_statistics"}, {"score": 0.004473552224535518, "phrase": "segmental_spectral_features"}, {"score": 0.004415857432588371, "phrase": "rich_information"}, {"score": 0.004084920064588169, "phrase": "mel-frequency_cepstral_coefficients"}, {"score": 0.0036500346000495317, "phrase": "speaker-independent_emotion_recognition"}, {"score": 0.0033909518548844047, "phrase": "phoneme_type_classes"}, {"score": 0.0032896745636186824, "phrase": "classification_accuracies"}, {"score": 0.0031502008922533894, "phrase": "utterance-level_spectral_features"}, {"score": 0.0030560920414081645, "phrase": "prosodic_features"}, {"score": 0.0030166226457616616, "phrase": "even_further_improvement"}, {"score": 0.0029647862293448895, "phrase": "large_number"}, {"score": 0.002939202022371694, "phrase": "class-level_spectral_features"}, {"score": 0.002888692115811987, "phrase": "feature_selection"}, {"score": 0.0027304365438083874, "phrase": "clear_gains"}, {"score": 0.0026718978608934077, "phrase": "spectral_features"}, {"score": 0.002637377202509382, "phrase": "consonant_regions"}, {"score": 0.0024928553725344933, "phrase": "vowel_features"}, {"score": 0.0024288443150616056, "phrase": "emotion_recognition_accuracy"}, {"score": 0.0022957232752482196, "phrase": "significant_dependence"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Emotions", " Emotional speech classification", " Spectral features"], "paper_abstract": "The most common approaches to automatic emotion recognition rely on utterance-level prosodic features. Recent studies have shown that utterance-level statistics of segmental spectral features also contain rich information about expressivity and emotion. In our work we introduce a more fine-grained yet robust set of spectral features: statistics of Mel-Frequency Cepstral Coefficients computed over three phoneme type classes of interest stressed vowels, unstressed vowels and consonants in the utterance. We investigate performance of our features in the task of speaker-independent emotion recognition using two publicly available datasets. Our experimental results clearly indicate that indeed both the richer set of spectral features and the differentiation between phoneme type classes are beneficial for the task. Classification accuracies are consistently higher for our features compared to prosodic or utterance-level spectral features. Combination of our phoneme class features with prosodic features leads to even further improvement. Given the large number of class-level spectral features, we expected feature selection will improve results even further, but none of several selection methods led to clear gains. Further analyses reveal that spectral features computed from consonant regions of the utterance contain more information about emotion than either stressed or unstressed vowel features. We also explore how emotion recognition accuracy depends on utterance length. We show that, while there is no significant dependence for utterance-level prosodic features, accuracy of emotion recognition using class-level spectral features increases with the utterance length. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Class-level spectral features for emotion recognition", "paper_id": "WOS:000278573700001"}