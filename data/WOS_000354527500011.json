{"auto_keywords": [{"score": 0.049193598318116316, "phrase": "multiple_servers"}, {"score": 0.04539190749928413, "phrase": "scheduling_decisions"}, {"score": 0.044585905779495894, "phrase": "layered_data"}, {"score": 0.03826559283990352, "phrase": "data_blocks"}, {"score": 0.03543704829341049, "phrase": "optimal_coding"}, {"score": 0.00481495049065317, "phrase": "adaptive_prioritized_random_linear_coding"}, {"score": 0.004770211811866158, "phrase": "scheduling_for_layered_data_delivery"}, {"score": 0.004406260054779782, "phrase": "optimal_coding_strategy"}, {"score": 0.004050996777117918, "phrase": "prioritized_random_linear_coding"}, {"score": 0.0037768417338850274, "phrase": "unequal_levels"}, {"score": 0.003504756881063581, "phrase": "decoding_delays"}, {"score": 0.0034398445347109396, "phrase": "delivery_performance"}, {"score": 0.003329117824699891, "phrase": "scheduling_decisions_problem"}, {"score": 0.0032228438235282005, "phrase": "markov"}, {"score": 0.0030891871245908665, "phrase": "effective_tools"}, {"score": 0.002989714021608139, "phrase": "reinforcement_learning_approaches"}, {"score": 0.002893434697072309, "phrase": "computational_complexity_solutions"}, {"score": 0.002853124436286834, "phrase": "adaptive_coding"}, {"score": 0.0027741761880779535, "phrase": "novel_reinforcement"}, {"score": 0.0027100527874262446, "phrase": "mdp_solution"}, {"score": 0.0026474076254702525, "phrase": "illustrative_example"}, {"score": 0.002622755848331379, "phrase": "scalable_video_transmission"}, {"score": 0.0025741370320813968, "phrase": "large_performance_gains"}, {"score": 0.002550165781755603, "phrase": "competing_methods"}, {"score": 0.002456486927713543, "phrase": "experimental_evaluation"}, {"score": 0.002388487669097511, "phrase": "continuous_playback"}, {"score": 0.0023551955270737215, "phrase": "small_quality_variations"}, {"score": 0.0022793032191981404, "phrase": "baseline_solutions"}, {"score": 0.0021247934141234988, "phrase": "temporal_evolution"}, {"score": 0.0021049977753042253, "phrase": "data_demands"}], "paper_keywords": ["Layered data", " Markov decision processes (MDP)", " prioritized random linear codes (PRLC)", " Q-learning", " rateless codes", " virtual experience"], "paper_abstract": "In this paper, we deal with the problem of jointly determining the optimal coding strategy and the scheduling decisions when receivers obtain layered data from multiple servers. The layered data is encoded by means of prioritized random linear coding (PRLC) in order to be resilient to channel loss while respecting the unequal levels of importance in the data, and data blocks are transmitted simultaneously in order to reduce decoding delays and improve the delivery performance. We formulate the optimal coding and scheduling decisions problem in our novel framework with the help of Markov decision processes (MDP), which are effective tools for modeling adapting streaming systems. Reinforcement learning approaches are then proposed to derive reduced computational complexity solutions to the adaptive coding and scheduling problems. The novel reinforcement learning approaches and the MDP solution are examined in an illustrative example for scalable video transmission. Our methods offer large performance gains over competing methods that deliver the data blocks sequentially. The experimental evaluation also shows that our novel algorithms offer continuous playback and guarantee small quality variations which is not the case for baseline solutions. Finally, our work highlights the advantages of reinforcement learning algorithms to forecast the temporal evolution of data demands and to decide the optimal coding and scheduling decisions.", "paper_title": "Adaptive Prioritized Random Linear Coding and Scheduling for Layered Data Delivery From Multiple Servers", "paper_id": "WOS:000354527500011"}