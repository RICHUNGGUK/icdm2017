{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "language_models"}, {"score": 0.004715702140552199, "phrase": "predictive_framework"}, {"score": 0.0045547927862194856, "phrase": "ranking_distributions"}, {"score": 0.004399349717218995, "phrase": "widely_used_criterion"}, {"score": 0.004161553532799662, "phrase": "task_assumptions"}, {"score": 0.0040194764936614565, "phrase": "main_drawback"}, {"score": 0.0038822311096089307, "phrase": "probability_distributions"}, {"score": 0.0037236852537136547, "phrase": "heterogeneous_models"}, {"score": 0.003621588555525458, "phrase": "evaluation_framework"}, {"score": 0.003285725064532174, "phrase": "shannon's_entropy_idea"}, {"score": 0.003086396772496527, "phrase": "rank_based_statistics"}, {"score": 0.002939737235379635, "phrase": "joint_word_sequences"}, {"score": 0.0028000270988465486, "phrase": "model_assumptions"}, {"score": 0.002648448458159033, "phrase": "english_language"}, {"score": 0.0026118503041971976, "phrase": "different_kind"}, {"score": 0.0024876840172100567, "phrase": "long-term_prediction_language_models"}, {"score": 0.0023694065215394593, "phrase": "standard_n-gram_models"}, {"score": 0.002288373472756928, "phrase": "exponential_laws"}, {"score": 0.0021947755130109696, "phrase": "letter_sequences"}, {"score": 0.0021049977753042253, "phrase": "second_mode"}], "paper_keywords": [""], "paper_abstract": "Perplexity is a widely used criterion in order to compare language models without any task assumptions. However, the main drawback is that perplexity supposes probability distributions and hence cannot compare heterogeneous models. As an evaluation framework, we propose in this article to abandon perplexity and to extend the Shannon's entropy idea which is based on model prediction performance using rank based statistics. Our methodology is able to predict joint word sequences being independent of the task or model assumptions. Experiments are carried out on the English language with different kind of language models. We show that long-term prediction language models are not more effective than the standard n-gram models. Ranking distributions follow exponential laws as already observed in predicting letter sequences. These distributions show a second mode not observed with letters and we propose to give some interpretation to this mode in this article.", "paper_title": "Evaluating language models within a predictive framework: An analysis of ranking distributions", "paper_id": "WOS:000241103500040"}