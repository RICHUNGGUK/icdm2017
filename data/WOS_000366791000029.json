{"auto_keywords": [{"score": 0.04120210347957595, "phrase": "first-hand_experience"}, {"score": 0.00481495049065317, "phrase": "twitter_annotations"}, {"score": 0.004755353864362941, "phrase": "first-hand_experiences"}, {"score": 0.004716031208088975, "phrase": "prescription_drug_use"}, {"score": 0.004677032188041644, "phrase": "self-reported_patient_data"}, {"score": 0.004543046350523362, "phrase": "valuable_knowledge_source"}, {"score": 0.004505471345843642, "phrase": "post-market_pharmacovigilance"}, {"score": 0.0043401761815767, "phrase": "popular_micro-blogging_service"}, {"score": 0.004233347974930148, "phrase": "adverse_drug_reactions"}, {"score": 0.0040949726088047225, "phrase": "micro-blog_messages"}, {"score": 0.0036756196381448015, "phrase": "laymen_annotators"}, {"score": 0.0030739103951333696, "phrase": "fleiss"}, {"score": 0.003010651173285521, "phrase": "crowdsourcing_ranks"}, {"score": 0.0029857124153761187, "phrase": "moderate_agreement"}, {"score": 0.00292426293067148, "phrase": "experienced_annotators"}, {"score": 0.002900052793326166, "phrase": "spearman"}, {"score": 0.0027818804382539444, "phrase": "gold_standard"}, {"score": 0.00274737930563124, "phrase": "crowdflower"}, {"score": 0.002668524425703961, "phrase": "supervised_machine_learning_models"}, {"score": 0.002613594490081211, "phrase": "f-score"}, {"score": 0.002496659907186784, "phrase": "bayesian_generalized_linear_model"}, {"score": 0.0023261372216194383, "phrase": "selected_set"}, {"score": 0.002268769196700254, "phrase": "information_gain_criteria"}, {"score": 0.002167235986623239, "phrase": "elsevier_inc."}, {"score": 0.0021314498639048085, "phrase": "open_access_article"}, {"score": 0.0021049977753042253, "phrase": "cc_by-nc-nd_license"}], "paper_keywords": ["Crowdsourcing", " Pharmacovigilance", " Twitter", " Natural language processing"], "paper_abstract": "Self-reported patient data has been shown to be a valuable knowledge source for post-market pharmacovigilance. In this paper we propose using the popular micro-blogging service Twitter to gather evidence about adverse drug reactions (ADRs) after firstly having identified micro-blog messages (also know as \"tweets\") that report first-hand experience. In order to achieve this goal we explore machine learning with data crowdsourced from laymen annotators. With the help of lay annotators recruited from CrowdFlower we manually annotated 1548 tweets containing keywords related to two kinds of drugs: SSRIs (eg. Paroxetine), and cognitive enhancers (eg. Ritalin). Our results show that inter-annotator agreement (Fleiss' kappa) for crowdsourcing ranks in moderate agreement with a pair of experienced annotators (Spearman's Rho = 0.471). We utilized the gold standard annotations from CrowdFlower for automatically training a range of supervised machine learning models to recognize first-hand experience. F-Score values are reported for 6 of these techniques with the Bayesian Generalized Linear Model being the best (F-Score = 0.64 and Informedness = 0.43) when combined with a selected set of features obtained by using information gain criteria. (C) 2015 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license.", "paper_title": "Crowdsourcing Twitter annotations to identify first-hand experiences of prescription drug use", "paper_id": "WOS:000366791000029"}