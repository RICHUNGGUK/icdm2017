{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "classification_trees"}, {"score": 0.0490692031182167, "phrase": "gini_index"}, {"score": 0.03607022877434213, "phrase": "missing_values"}, {"score": 0.00458940888106061, "phrase": "popular_tool"}, {"score": 0.004540734419151103, "phrase": "applied_statistics"}, {"score": 0.004421284843189731, "phrase": "impurity_reduction"}, {"score": 0.004038076978795421, "phrase": "standard_algorithms"}, {"score": 0.003952828685048176, "phrase": "major_problem"}, {"score": 0.0038283058265876713, "phrase": "standard_impurity_measures"}, {"score": 0.003459191419478298, "phrase": "high_amount"}, {"score": 0.0031760175583084274, "phrase": "new_split_selection_criterion"}, {"score": 0.0031255543155487234, "phrase": "variable_selection_bias"}, {"score": 0.003043218974273827, "phrase": "exact_distribution"}, {"score": 0.002994859407965545, "phrase": "maximally_selected_gini_gain"}, {"score": 0.002884979280317279, "phrase": "combinatorial_approach"}, {"score": 0.0027495914857929584, "phrase": "unbiased_split_selection_criterion"}, {"score": 0.0025514745448798363, "phrase": "simulation_studies"}, {"score": 0.0025109095718850376, "phrase": "real_data_study"}, {"score": 0.0024842242921857705, "phrase": "veterinary_gynecology"}, {"score": 0.002418742283474052, "phrase": "binary_classification"}, {"score": 0.002393034183276193, "phrase": "continuous_predictor_variables"}, {"score": 0.0023675986784123656, "phrase": "different_numbers"}, {"score": 0.0023051837161137674, "phrase": "proposed_method"}, {"score": 0.002256435842862802, "phrase": "categorical_and_ordinal_predictor_variables"}, {"score": 0.002105007807034302, "phrase": "elsevier"}], "paper_keywords": ["classification trees", " variable selection bias", " Gini gain", " missing values"], "paper_abstract": "Classification trees are a popular tool in applied statistics because their heuristic search approach based on impurity reduction is easy to understand and the interpretation of the output is straightforward. However, all standard algorithms suffer from a major problem: variable selection based on standard impurity measures as the Gini Index is biased. The bias is such that, e.g., splitting variables with a high amount of missing values-even if missing completely at random (MCAR)-are artificially preferred. A new split selection criterion that avoids variable selection bias is introduced. The exact distribution of the maximally selected Gini gain is derived by means of a combinatorial approach and the resulting p-value is suggested as an unbiased split selection criterion in recursive partitioning algorithms. The efficiency of the method is demonstrated in simulation studies and a real data study from veterinary gynecology in the context of binary classification and continuous predictor variables with different numbers of missing values. The proposed method is extendible to categorical and ordinal predictor variables and to other split selection criteria such as the cross-entropy. (c) 2007 Elsevier B. V. All rights reserved.", "paper_title": "Unbiased split selection for classification trees based on the Gini Index", "paper_id": "WOS:000249912400039"}