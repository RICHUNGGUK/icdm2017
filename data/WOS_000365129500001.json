{"auto_keywords": [{"score": 0.026684985479227884, "phrase": "low-quality_workers"}, {"score": 0.015719716506582538, "phrase": "label_quality"}, {"score": 0.004608320134338125, "phrase": "crowdsourcing"}, {"score": 0.004475436844227947, "phrase": "effective_and_efficient_paradigm"}, {"score": 0.0043147264956783565, "phrase": "large-scale_unlabeled_data"}, {"score": 0.0036198068824262464, "phrase": "popular_human-machine_interaction_process"}, {"score": 0.0032433414042726356, "phrase": "crowd_workers"}, {"score": 0.0031961879138783012, "phrase": "majority_voting"}, {"score": 0.0023669270875629205, "phrase": "conservative_condition"}, {"score": 0.002248553559414903, "phrase": "non-low-quality_worker"}, {"score": 0.0021049977753042253, "phrase": "aggressive_condition"}], "paper_keywords": ["unlabeled data", " crowdsourcing", " majority voting", " label quality", " worker selection"], "paper_abstract": "Crowdsourcing has been an effective and efficient paradigm for providing labels for large-scale unlabeled data. In the past few years, many methods have been developed for inferring labels from the crowd, but few theoretical analyses have been presented to support this popular human-machine interaction process. In this paper, we theoretically study the quality of labels inferred from crowd workers by majority voting and provide an analysis of label quality that shows that the label error rate decreases exponentially with the number of workers selected for each task. We also study the problem of eliminating low-quality workers from the crowd, and provide a conservative condition for eliminating low-quality workers without eliminating any non-low-quality worker with high probability. We also provide an aggressive condition for eliminating all low-quality workers with high probability.", "paper_title": "Crowdsourcing label quality: a theoretical analysis", "paper_id": "WOS:000365129500001"}