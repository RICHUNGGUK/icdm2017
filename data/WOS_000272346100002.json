{"auto_keywords": [{"score": 0.042291837632617076, "phrase": "high-dimensional_classification"}, {"score": 0.00481495049065317, "phrase": "ultrahigh_dimensional_feature_selection"}, {"score": 0.004695183699894267, "phrase": "variable_selection"}, {"score": 0.00464811139983244, "phrase": "high-dimensional_space"}, {"score": 0.004555371402128666, "phrase": "scientific_discovery"}, {"score": 0.004509694461067052, "phrase": "decision_making"}, {"score": 0.0043753813457279404, "phrase": "independence_screening"}, {"score": 0.0037423521744765075, "phrase": "linear_model"}, {"score": 0.003540392285585644, "phrase": "simple_correlation"}, {"score": 0.003469676666252421, "phrase": "sure_independence_screening_property"}, {"score": 0.002982307044758515, "phrase": "response_variable"}, {"score": 0.0028643193035515443, "phrase": "isis"}, {"score": 0.002821247621250663, "phrase": "explicit_definition"}, {"score": 0.0027370835853738626, "phrase": "general_pseudo-likelihood_framework"}, {"score": 0.0026823700218343506, "phrase": "generalized_linear_models"}, {"score": 0.0026420518944303716, "phrase": "special_case"}, {"score": 0.002524688179363081, "phrase": "new_method"}, {"score": 0.0024617483174454113, "phrase": "feature_deletion"}, {"score": 0.0024247382451294255, "phrase": "iterative_process"}, {"score": 0.0023405257495466352, "phrase": "important_features"}, {"score": 0.002282166507956957, "phrase": "popularly_used_two-sample_t-method"}, {"score": 0.002158836285985314, "phrase": "false_selection_rate"}, {"score": 0.0021263705039915198, "phrase": "feature_screening_stage"}], "paper_keywords": ["classification", " feature screening", " generalized linear models", " robust regression", " feature selection"], "paper_abstract": "Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a two-sample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv ( 2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening ( ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology.", "paper_title": "Ultrahigh Dimensional Feature Selection: Beyond The Linear Model", "paper_id": "WOS:000272346100002"}