{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "asymptotic_complexity"}, {"score": 0.04924496525218245, "phrase": "generalised_entropy"}, {"score": 0.004220086640742953, "phrase": "finite_or_infinite_strings"}, {"score": 0.0037973594873521596, "phrase": "next_element"}, {"score": 0.003129104539392737, "phrase": "shannon"}, {"score": 0.00304737045343781, "phrase": "arbitrary_loss_functions"}, {"score": 0.0029161225023394363, "phrase": "optimal_expected_loss"}, {"score": 0.002790511479398411, "phrase": "possible_outcomes"}, {"score": 0.0025328377393370642, "phrase": "asymptotic_complexities"}, {"score": 0.0024667783849055634, "phrase": "language_w.r.t_different_loss_functions"}, {"score": 0.002298902547676131, "phrase": "generalised_entropies"}, {"score": 0.0022192891418394514, "phrase": "loss_functions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": [""], "paper_abstract": "The paper explores connections between asymptotic complexity and generalised entropy. Asymptotic complexity of a language (a language is a set of finite or infinite strings) is a way of formalising the complexity of predicting the next element in a sequence: it is the loss per element of a strategy asymptotically optimal for that language. Generalised entropy extends Shannon entropy to arbitrary loss functions; it is the optimal expected loss given a distribution on possible outcomes. It turns out that the set of tuples of asymptotic complexities of a language w.r.t different loss functions can be described by means of the generalised entropies corresponding to the loss functions. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Generalised entropies and asymptotic complexities of languages", "paper_id": "WOS:000339468300005"}