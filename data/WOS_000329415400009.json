{"auto_keywords": [{"score": 0.04905512366034696, "phrase": "real-life_applications"}, {"score": 0.044026836155415534, "phrase": "phoneme-based_classification"}, {"score": 0.04136457157044735, "phrase": "cross-corpora_evaluation"}, {"score": 0.03301683043774397, "phrase": "phoneme-level_emotion_classes"}, {"score": 0.03278647633924455, "phrase": "cross-corpora_classification_performance"}, {"score": 0.023497863741722108, "phrase": "ua"}, {"score": 0.00481495049065317, "phrase": "phonetic_pattern_variability"}, {"score": 0.004734680382498411, "phrase": "robust_emotion_classifiers"}, {"score": 0.004655742198075387, "phrase": "automatic_emotion_recognition"}, {"score": 0.004556172165743172, "phrase": "accepted_importance"}, {"score": 0.004501774379030109, "phrase": "emotional_state"}, {"score": 0.004448023165057824, "phrase": "human_computer_interaction"}, {"score": 0.004300895039945019, "phrase": "phonetic_transcription"}, {"score": 0.004138673508264198, "phrase": "acted_and_spontaneous_emotions"}, {"score": 0.004030695216899165, "phrase": "high-level_results"}, {"score": 0.003963447369437198, "phrase": "sufficiently_good_classification"}, {"score": 0.0037592872970376124, "phrase": "simplified_two-class_problem"}, {"score": 0.0037412551227957937, "phrase": "namely_high_and_low_arousal_emotions"}, {"score": 0.0035741900278634616, "phrase": "gmms-based_production_probabilities"}, {"score": 0.0035570610535523664, "phrase": "mfcc"}, {"score": 0.003365641528627568, "phrase": "reduced_set"}, {"score": 0.0033494912067563915, "phrase": "indicative_vowels"}, {"score": 0.0032856586537486545, "phrase": "german_sam-pa_list"}, {"score": 0.003238581232324518, "phrase": "emotion_classification_performance"}, {"score": 0.0031845067058084583, "phrase": "emotion_challenge"}, {"score": 0.003013090871458764, "phrase": "acted_emotions"}, {"score": 0.002941461468387943, "phrase": "spontaneous_emotions"}, {"score": 0.0028784478769295204, "phrase": "experimental_conditions"}, {"score": 0.002850875630787708, "phrase": "vam"}, {"score": 0.0026715275425281894, "phrase": "comparably_low_speech_recognition_performance"}, {"score": 0.0026523079380037706, "phrase": "scant_a_priori_knowledge"}, {"score": 0.0025892330566358503, "phrase": "word-level_modeling"}, {"score": 0.002558258913944048, "phrase": "phoneme-level_modeling"}, {"score": 0.0025094672146390735, "phrase": "state-of-the-art_cross-corpora_evaluations"}, {"score": 0.0024914113399360925, "phrase": "yam"}, {"score": 0.0021771713131746636, "phrase": "zhang_et_al"}, {"score": 0.002140781064077977, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Emotion phonetic pattern", " Emotion phoneme classes", " Cross-corpora evaluation", " Affective speech", " Emotion classification", " Level of arousal classification"], "paper_abstract": "The role of automatic emotion recognition from speech is growing continuously because of the accepted importance of reacting to the emotional state of the user in human computer interaction. Most state-of-the-art emotion recognition methods are based on turn- and frame-level analysis independent from phonetic transcription. Here, we are interested in a phoneme-based classification of the level of arousal in acted and spontaneous emotions. To start, we show that our previously published classification technique which showed high-level results in the Interspeech 2009 Emotion Challenge cannot provide sufficiently good classification in cross-corpora evaluation (a condition close to real-life applications). To prove the robustness of our emotion classification techniques we use cross-corpora evaluation for a simplified two-class problem; namely high and low arousal emotions. We use emotion classes on a phoneme-level for classification. We build our speaker-independent emotion classifier with HMMs, using GMMs-based production probabilities and MFCC features. This classifier performs equally well when using a complete phoneme set, as it does in the case of a reduced set of indicative vowels (7 out of 39 phonemes in the German SAM-PA list). Afterwards we compare emotion classification performance of the technique used in the Emotion Challenge with phoneme-based classification within the same experimental setup. With phoneme-level emotion classes we increase cross-corpora classification performance by about 3.15% absolute (4.69% relative) for models trained on acted emotions (EMO-DB dataset) and evaluated on spontaneous emotions (YAM dataset); within vice versa experimental conditions (trained on VAM, tested on EMO-DB) we obtain 15.43% absolute (23.20% relative) improvement. We show that using phoneme-level emotion classes can improve classification performance even with comparably low speech recognition performance obtained with scant a priori knowledge about the language, implemented as a zero-gram for word-level modeling and a bi-gram for phoneme-level modeling. Finally we compare our results with the state-of-the-art cross-corpora evaluations on the YAM database. For training our models, we use an almost 15 times smaller training set, consisting of 456 utterances (210 low and 246 high arousal emotions) instead of 6820 utterances (4685 high and 2135 low arousal emotions). We are yet able to increase cross-corpora classification performance by about 2.25% absolute {3.22% relative) from UA = 69.7% obtained by Zhang et al. to UA = 71.95%. Crown Copyright (C) 2012 Published by Elsevier Ltd. All rights reserved.", "paper_title": "Modeling phonetic pattern variability in favor of the creation of robust emotion classifiers for real-life applications", "paper_id": "WOS:000329415400009"}