{"auto_keywords": [{"score": 0.04157802264496638, "phrase": "surrogate_measures"}, {"score": 0.018631345640137922, "phrase": "original_ir_evaluation_measures"}, {"score": 0.016380040642876226, "phrase": "arbitrarily_strong_tendency_correlation"}, {"score": 0.010612387000973441, "phrase": "direct_optimization"}, {"score": 0.01054204233556468, "phrase": "evaluation_measures"}, {"score": 0.010472159017445555, "phrase": "information_retrieval"}, {"score": 0.00773415121036628, "phrase": "surrogate_measure"}, {"score": 0.00652382754440445, "phrase": "softrank"}, {"score": 0.006480406561311692, "phrase": "approxrank"}, {"score": 0.004625506452304716, "phrase": "important_branch"}, {"score": 0.004443482801729777, "phrase": "ir_evaluation_measures"}, {"score": 0.004240116703465252, "phrase": "surrogate_functions"}, {"score": 0.004100555318595827, "phrase": "critical_issue"}, {"score": 0.0037461743696712833, "phrase": "formal_analysis"}, {"score": 0.003343007152920687, "phrase": "ir_evaluation_measure"}, {"score": 0.0032328778271536454, "phrase": "effective_optimization"}, {"score": 0.0032005497177523025, "phrase": "original_ir_evaluation_measure"}, {"score": 0.003126365132352301, "phrase": "tendency_correlations"}, {"score": 0.0030334982939315, "phrase": "direct_optimization_methods"}, {"score": 0.002827365415832384, "phrase": "data_distribution"}, {"score": 0.00216950156199795, "phrase": "better_ranking_performances"}, {"score": 0.0021262835396323623, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "public_benchmark_datasets"}], "paper_keywords": ["Tendency correlation", " Direct optimization", " Information retrieval measures", " Learning to rank"], "paper_abstract": "Direct optimization of evaluation measures has become an important branch of learning to rank for information retrieval (IR). Since IR evaluation measures are difficult to optimize due to their non-continuity and non-differentiability, most direct optimization methods optimize some surrogate functions instead, which we call surrogate measures. A critical issue regarding these methods is whether the optimization of the surrogate measures can really lead to the optimization of the original IR evaluation measures. In this work, we perform formal analysis on this issue. We propose a concept named \"tendency correlation\" to describe the relationship between a surrogate measure and its corresponding IR evaluation measure. We show that when a surrogate measure has arbitrarily strong tendency correlation with an IR evaluation measure, the optimization of it will lead to the effective optimization of the original IR evaluation measure. Then, we analyze the tendency correlations of the surrogate measures optimized in a number of direct optimization methods. We prove that the surrogate measures in SoftRank and ApproxRank can have arbitrarily strong tendency correlation with the original IR evaluation measures, regardless of the data distribution, when some parameters are appropriately set. However, the surrogate measures in SVM (MAP) , DORM (NDCG) , PermuRank (MAP) , and SVM (NDCG) cannot have arbitrarily strong tendency correlation with the original IR evaluation measures on certain distributions of data. Therefore SoftRank and ApproxRank are theoretically sounder than SVM (MAP) , DORM (NDCG) , PermuRank (MAP) , and SVM (NDCG) , and are expected to result in better ranking performances. Our theoretical findings can explain the experimental results observed on public benchmark datasets.", "paper_title": "Tendency correlation analysis for direct optimization of evaluation measures in information retrieval", "paper_id": "WOS:000284425200004"}