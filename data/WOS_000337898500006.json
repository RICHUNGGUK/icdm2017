{"auto_keywords": [{"score": 0.04934325842114906, "phrase": "evolving_bag"}, {"score": 0.04885877600670675, "phrase": "key_poses"}, {"score": 0.00481495049065317, "phrase": "adaptive_human_action_recognition"}, {"score": 0.004644014990871423, "phrase": "vision-based_human_action_recognition"}, {"score": 0.00450231514359539, "phrase": "meaningful_human_motion"}, {"score": 0.004320055598195391, "phrase": "advanced_human-computer_interaction"}, {"score": 0.0041666157975446564, "phrase": "dynamic_environments"}, {"score": 0.004123781267600542, "phrase": "adaptive_methods"}, {"score": 0.003997891221012094, "phrase": "scenario_characteristics"}, {"score": 0.0038959121068990517, "phrase": "human-robot_interaction"}, {"score": 0.0034771228725942846, "phrase": "changing_nature"}, {"score": 0.00330189927527898, "phrase": "adaptive_vision-based_human_action_recognition_method"}, {"score": 0.003168079471049896, "phrase": "evolutionary_optimization_method"}, {"score": 0.0031354779851351287, "phrase": "adaptive_and_incremental_learning"}, {"score": 0.0028864239961184364, "phrase": "learned_actions"}, {"score": 0.0028127163745564777, "phrase": "current_learning_memory"}, {"score": 0.002740885774408238, "phrase": "increasingly_more_actions"}, {"score": 0.002670884656785558, "phrase": "evolutionary_method"}, {"score": 0.0026297428083782875, "phrase": "optimal_subset"}, {"score": 0.0026026666736474404, "phrase": "training_instances"}, {"score": 0.0025493457404304446, "phrase": "parameter_values"}, {"score": 0.0025100713447593773, "phrase": "learning_phase"}, {"score": 0.0022749744354271816, "phrase": "new_actions"}, {"score": 0.0021826828321952615, "phrase": "learned_model"}, {"score": 0.0021601997088758957, "phrase": "stable_and_accurate_results"}], "paper_keywords": ["Evolutionary computing and genetic algorithms", " feature evaluation and selection", " human computer interaction", " vision and scene understanding"], "paper_abstract": "Vision-based human action recognition allows to detect and understand meaningful human motion. This makes it possible to perform advanced human-computer interaction, among other applications. In dynamic environments, adaptive methods are required to support changing scenario characteristics. Specifically, in human-robot interaction, smooth interaction between humans and robots can only be performed if these are able to evolve and adapt to the changing nature of the scenarios. In this paper, an adaptive vision-based human action recognition method is proposed. By means of an evolutionary optimization method, adaptive and incremental learning of human actions is supported. Through an evolving bag of key poses, which models the learned actions over time, the current learning memory is developed to recognize increasingly more actions or actors. The evolutionary method selects the optimal subset of training instances, features and parameter values for each learning phase, and handles the evolution of the model. The experimentation shows that our proposal achieves to adapt to new actions or actors successfully, by rearranging the learned model. Stable and accurate results have been obtained on four publicly available RGB and RGB-D datasets, unveiling the method's robustness and applicability.", "paper_title": "Adaptive Human Action Recognition With an Evolving Bag of Key Poses", "paper_id": "WOS:000337898500006"}