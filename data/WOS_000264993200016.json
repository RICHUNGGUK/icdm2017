{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "reservoir_computing"}, {"score": 0.004578382275810468, "phrase": "new_paradigm"}, {"score": 0.004442032546224076, "phrase": "recurrent_neural_network"}, {"score": 0.00418134346679498, "phrase": "key_idea"}, {"score": 0.003975781274672803, "phrase": "large_but_fixed_recurrent_part"}, {"score": 0.003780286517897356, "phrase": "dynamic_features"}, {"score": 0.003487222504700868, "phrase": "desired_information"}, {"score": 0.0029973957125353306, "phrase": "output_layer"}, {"score": 0.0027370835853738626, "phrase": "generalization_ability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reservoir computing", " Echo state network", " Variable selection", " Regularization", " Ridge regression"], "paper_abstract": "Reservoir computing is a new paradigm for using recurrent neural network with a much simpler training method. The key idea is to use a large but fixed recurrent part as a reservoir of dynamic features and to train only the output layer to extract the desired information. We propose to study how pruning some connections from the reservoir to the output layer can help on the one hand to increase the generalization ability, in much the same way as regularization techniques do, and on the other hand to improve the implementability of reservoirs in hardware. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Pruning and regularization in reservoir computing", "paper_id": "WOS:000264993200016"}