{"auto_keywords": [{"score": 0.043891862951523704, "phrase": "real_system_data"}, {"score": 0.004814951920928826, "phrase": "data-driven"}, {"score": 0.004741845603430361, "phrase": "control_for_nonlinear_distributed_parameter_systems"}, {"score": 0.004669845451682529, "phrase": "data-driven_h_infinity_control_problem"}, {"score": 0.00437021209960886, "phrase": "off-policy_learning_method"}, {"score": 0.004238452771031322, "phrase": "h_infinity_control_policy"}, {"score": 0.00411064950774695, "phrase": "mathematical_model"}, {"score": 0.004027586827290042, "phrase": "karhunen-loeve_decomposition"}, {"score": 0.003906116922372891, "phrase": "empirical_eigenfunctions"}, {"score": 0.0037307208089736835, "phrase": "reduced-order_model"}, {"score": 0.003636692152882991, "phrase": "slow_subsystem"}, {"score": 0.003563172322934695, "phrase": "singular_perturbation_theory"}, {"score": 0.003509006038574667, "phrase": "h_infinity_control_problem"}, {"score": 0.003403138002343697, "phrase": "rom"}, {"score": 0.003088384809415812, "phrase": "hji_equation"}, {"score": 0.003010496595090984, "phrase": "data-driven_off-policy_learning_approach"}, {"score": 0.0029196116767956273, "phrase": "simultaneous_policy_update_algorithm"}, {"score": 0.0028026735035370206, "phrase": "implementation_purpose"}, {"score": 0.0027600360731762997, "phrase": "neural_network"}, {"score": 0.002530359917257626, "phrase": "value_function"}, {"score": 0.002367676766637248, "phrase": "least-square_nn_weight-tuning_rule"}, {"score": 0.002284444272376043, "phrase": "weighted_residuals"}, {"score": 0.0022267859286766553, "phrase": "developed_data-driven_off-policy_learning_approach"}, {"score": 0.0021705796895687864, "phrase": "nonlinear_diffusion-reaction_process"}, {"score": 0.002126635713519053, "phrase": "obtained_results"}], "paper_keywords": ["Data driven", " distributed parameter systems (DSPs)", " Hamilton-Jacobi-Isaacs (HJI) equation", " H infinity control", " neural network (NN)", " off-policy learning", " partial differential equation (PDE)"], "paper_abstract": "The data-driven H infinity control problem of nonlinear distributed parameter systems is considered in this paper. An off-policy learning method is developed to learn the H infinity control policy from real system data rather than the mathematical model. First, Karhunen-Loeve decomposition is used to compute the empirical eigenfunctions, which are then employed to derive a reduced-order model (ROM) of slow subsystem based on the singular perturbation theory. The H infinity control problem is reformulated based on the ROM, which can be transformed to solve the Hamilton-Jacobi-Isaacs (HJI) equation, theoretically. To learn the solution of the HJI equation from real system data, a data-driven off-policy learning approach is proposed based on the simultaneous policy update algorithm and its convergence is proved. For implementation purpose, a neural network (NN)based action-critic structure is developed, where a critic NN and two action NNs are employed to approximate the value function, control, and disturbance policies, respectively. Subsequently, a least-square NN weight-tuning rule is derived with the method of weighted residuals. Finally, the developed data-driven off-policy learning approach is applied to a nonlinear diffusion-reaction process, and the obtained results demonstrate its effectiveness.", "paper_title": "Data-Driven H infinity Control for Nonlinear Distributed Parameter Systems", "paper_id": "WOS:000363242800026"}