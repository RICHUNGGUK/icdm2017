{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "clinical_records"}, {"score": 0.036169083715739785, "phrase": "average_f_score"}, {"score": 0.02619916950644925, "phrase": "pathology_reports"}, {"score": 0.023844015631451675, "phrase": "coreference_resolution"}, {"score": 0.004696407360309416, "phrase": "automatic_resolution"}, {"score": 0.004629977311316031, "phrase": "medical_concepts"}, {"score": 0.004499910169464113, "phrase": "multiple_pass_sieve_approach"}, {"score": 0.004467965784278827, "phrase": "support_vector_machines"}, {"score": 0.004220406575749275, "phrase": "lexical_similarity"}, {"score": 0.004145882022189891, "phrase": "concept_mention"}, {"score": 0.0040726680518382965, "phrase": "wikipedia_redirects"}, {"score": 0.004029359223675788, "phrase": "local_lexical_context"}, {"score": 0.003846926701870193, "phrase": "unweighted_average"}, {"score": 0.0038196005593213555, "phrase": "muc"}, {"score": 0.0037924666577694222, "phrase": "ceaf"}, {"score": 0.0036596534390380287, "phrase": "research_experiments"}, {"score": 0.003371508176308189, "phrase": "odie_dataset"}, {"score": 0.003150612723641925, "phrase": "best-performing_system"}, {"score": 0.0031170783606525856, "phrase": "reported_f_score"}, {"score": 0.0030293828200315797, "phrase": "median_system_f_score"}, {"score": 0.002975826686621829, "phrase": "eight_teams"}, {"score": 0.0026077803487003, "phrase": "best_f_score"}, {"score": 0.0024984517279573906, "phrase": "discussion_post_hoc_analysis"}, {"score": 0.002480680078868508, "phrase": "significant_performance_degradation"}, {"score": 0.0024022553322977165, "phrase": "complex_synonymy"}, {"score": 0.0022770057597574734, "phrase": "competitive_performance"}, {"score": 0.0021582723621960693, "phrase": "electronic_medical_records"}], "paper_keywords": [""], "paper_abstract": "Objective A method for the automatic resolution of coreference between medical concepts in clinical records. Materials and methods A multiple pass sieve approach utilizing support vector machines (SVMs) at each pass was used to resolve coreference. Information such as lexical similarity, recency of a concept mention, synonymy based on Wikipedia redirects, and local lexical context were used to inform the method. Results were evaluated using an unweighted average of MUC, CEAF, and B-3 coreference evaluation metrics. The datasets used in these research experiments were made available through the 2011 i2b2/VA Shared Task on Coreference. Results The method achieved an average F score of 0.821 on the ODIE dataset, with a precision of 0.802 and a recall of 0.845. These results compare favorably to the best-performing system with a reported F score of 0.827 on the dataset and the median system F score of 0.800 among the eight teams that participated in the 2011 i2b2/VA Shared Task on Coreference. On the i2b2 dataset, the method achieved an average F score of 0.906, with a precision of 0.895 and a recall of 0.918 compared to the best F score of 0.915 and the median of 0.859 among the 16 participating teams. Discussion Post hoc analysis revealed significant performance degradation on pathology reports. The pathology reports were characterized by complex synonymy and very few patient mentions. Conclusion The use of several simple lexical matching methods had the most impact on achieving competitive performance on the task of coreference resolution. Moreover, the ability to detect patients in electronic medical records helped to improve coreference resolution more than other linguistic analysis.", "paper_title": "A supervised framework for resolving coreference in clinical records", "paper_id": "WOS:000307934600027"}