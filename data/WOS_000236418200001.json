{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "support_vector_machine_classification"}, {"score": 0.004606479650820579, "phrase": "paper_studies"}, {"score": 0.00447251540886613, "phrase": "vector_machine_classification_algorithms"}, {"score": 0.003531474887552889, "phrase": "regularization_error"}, {"score": 0.0032319109979344184, "phrase": "bayes-risk_consistency"}, {"score": 0.0030464123943266673, "phrase": "learning_rates"}, {"score": 0.002706659627293686, "phrase": "uniform_convergence"}, {"score": 0.0025892330566358503, "phrase": "main_difficulty"}, {"score": 0.0021049977753042253, "phrase": "hypothesis_space"}], "paper_keywords": ["support vector machine classification", " misclassification error", " bayesrisk consistency", " consistency with hypothesis space", " mercer kernel", " regularization error"], "paper_abstract": "This paper studies support vector machine classification algorithms. We analyze the 1-norm soft margin classifier. The consistency is considered in two forms. When the regularization error decays to zero, the Bayes-risk consistency is proved and learning rates are derived by means of techniques of uniform convergence. The main difficulty we overcome here is to bound the offset. For the consistency with hypothesis space, we present a counterexample.", "paper_title": "Analysis of support vector machine classification", "paper_id": "WOS:000236418200001"}