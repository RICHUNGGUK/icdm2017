{"auto_keywords": [{"score": 0.04829574488308946, "phrase": "scientific_workflows"}, {"score": 0.042953630786438136, "phrase": "htc"}, {"score": 0.00481495049065317, "phrase": "heterogeneous_scientific_workflows"}, {"score": 0.004674952036988355, "phrase": "earthquake_science_application"}, {"score": 0.004577440795549049, "phrase": "common_computational_model"}, {"score": 0.004519908617935935, "phrase": "scientific_simulations"}, {"score": 0.004296889776409743, "phrase": "scientific_workflow_applications"}, {"score": 0.0042250123967577284, "phrase": "high-performance_computing"}, {"score": 0.004189582475932618, "phrase": "hpc"}, {"score": 0.00413684709497912, "phrase": "high-throughput_computing"}, {"score": 0.004033464035028803, "phrase": "meaningful_performance_metrics"}, {"score": 0.0036144035798673967, "phrase": "alternative_metrics"}, {"score": 0.0032115093493071366, "phrase": "large-scale_scientific_workflow_application"}, {"score": 0.0030787405733989615, "phrase": "computational_performance"}, {"score": 0.003027178161479457, "phrase": "floating-point_operations"}, {"score": 0.002951444412262927, "phrase": "workflow_performance"}, {"score": 0.0025674490885184173, "phrase": "complete_view"}, {"score": 0.0025458479274370832, "phrase": "application_performance"}, {"score": 0.0024302416436559867, "phrase": "multiple_invocations"}, {"score": 0.0023296961971785357, "phrase": "heterogeneous_applications"}, {"score": 0.0021049977753042253, "phrase": "potential_application_optimizations"}], "paper_keywords": ["large-scale simulations", " performance metrics", " scientific workflows"], "paper_abstract": "Scientific workflows are a common computational model for performing scientific simulations. They may include many jobs, many scientific codes, and many file dependencies. Since scientific workflow applications may include both high-performance computing (HPC) and high-throughput computing (HTC) jobs, meaningful performance metrics are difficult to define, as neither traditional HPC metrics nor HTC metrics fully capture the extent of the application. We describe and propose the use of alternative metrics to accurately capture the scale of scientific workflows and quantify their efficiency. In this paper, we present several specific practical scientific workflow performance metrics and discuss these metrics in the context of a large-scale scientific workflow application, the Southern California Earthquake Center CyberShake 1.0 Map calculation. Our metrics reflect both computational performance, such as floating-point operations and file access, and workflow performance, such as job and task scheduling and execution. We break down performance into three levels of granularity: the task, the workflow, and the application levels, presenting a complete view of application performance. We show how our proposed metrics can be used to compare multiple invocations of the same application, as well as executions of heterogeneous applications, quantifying the amount of work performed and the efficiency of the work. Finally, we analyze CyberShake using our proposed metrics to determine potential application optimizations.", "paper_title": "Metrics for heterogeneous scientific workflows: A case study of an earthquake science application", "paper_id": "WOS:000293573900003"}