{"auto_keywords": [{"score": 0.04346898089428129, "phrase": "loss_function"}, {"score": 0.03957696638328549, "phrase": "biased_decision_boundary"}, {"score": 0.03452767737928207, "phrase": "penalty_regularization"}, {"score": 0.03374883864749991, "phrase": "margin_compensation"}, {"score": 0.026488681744198578, "phrase": "reference_model"}, {"score": 0.00481495049065317, "phrase": "svm_class-imbalanced_learning"}, {"score": 0.004703075081891693, "phrase": "important_practical_issue"}, {"score": 0.004671588539217021, "phrase": "machine_learning"}, {"score": 0.0046092435167207095, "phrase": "support_vector_machines"}, {"score": 0.004309725894724034, "phrase": "veropoulos_et_al"}, {"score": 0.004084129504819342, "phrase": "sensitive_prototype"}, {"score": 0.004016072233242585, "phrase": "penalty-regularized_model"}, {"score": 0.0033832583769880576, "phrase": "inversed_proportional_regularized_penalty"}, {"score": 0.00333804919565419, "phrase": "imbalanced_classes"}, {"score": 0.0030688220103201836, "phrase": "decision_boundary_drift"}, {"score": 0.002917793073455365, "phrase": "unbiased_classification"}, {"score": 0.0026199146891406, "phrase": "characteristic_curve"}, {"score": 0.00251620434048784, "phrase": "relative_higher_scores"}, {"score": 0.0024493494346360415, "phrase": "optimal_performance"}, {"score": 0.0023842666006371267, "phrase": "useful_characteristics"}, {"score": 0.002282166507956957, "phrase": "future_applications"}, {"score": 0.0021697677572213086, "phrase": "highly_unbiased_accuracy"}, {"score": 0.0021479597698614355, "phrase": "complex_imbalanced_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Margin", " Cost-sensitive learning", " Class-imbalanced learning", " Support vector machines", " Classification"], "paper_abstract": "Imbalanced dataset learning is an important practical issue in machine learning, even in support vector machines (SVMs). In this study, a well known reference model for solving the problem proposed by Veropoulos et al., is first studied. From the aspect of loss function, the reference cost sensitive prototype is identified as a penalty-regularized model. Intuitively, the loss function can change not only the penalty but also the margin to recover the biased decision boundary. This study focuses mainly on the effect from the margin and then extends the model to a more general modification. As proposed in the prototype, the modification first adopts an inversed proportional regularized penalty to re-weight the imbalanced classes. In addition to the penalty regularization, the modification then employs a margin compensation to lead the margin to be lopsided, which enables the decision boundary drift. Two regularization factors, the penalty and margin. are hence suggested for achieving an unbiased classification. The margin compensation, associating with the penalty regularization, is here utilized to calibrate and refine the biased decision boundary to further reduce the bias. With the area under the receiver operating characteristic curve (AuROC) for examining the performance, the modification shows relative higher scores than the reference model, even though the optimal performance is achieved by the reference model. Some useful characteristics found empirically are also included, which may be convenient for the future applications. All the theoretical descriptions and experimental validations show the proposed model's potential to compete for highly unbiased accuracy in a complex imbalanced dataset. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Margin calibration in SVM class-imbalanced learning", "paper_id": "WOS:000272607000046"}