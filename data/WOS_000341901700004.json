{"auto_keywords": [{"score": 0.04555525338591093, "phrase": "pell"}, {"score": 0.00481495049065317, "phrase": "emotional_faces"}, {"score": 0.004767546015485477, "phrase": "previous_eye-tracking_studies"}, {"score": 0.004628101576367193, "phrase": "emotionally-inflected_utterances"}, {"score": 0.004492717291734985, "phrase": "emotionally_congruent_face"}, {"score": 0.004404658750724105, "phrase": "rigoulot"}, {"score": 0.004109770306454165, "phrase": "emotional_speech_prosody"}, {"score": 0.003969784439380027, "phrase": "specific_features"}, {"score": 0.003911253966141701, "phrase": "emotional_face"}, {"score": 0.0036673959568781734, "phrase": "individual_faces"}, {"score": 0.003421697038352743, "phrase": "emotionally-inflected_pseudoutterance"}, {"score": 0.0033545600622020464, "phrase": "congruent_or_incongruent_prosody"}, {"score": 0.0032563083791198534, "phrase": "emotional_meaning"}, {"score": 0.0030081030319492343, "phrase": "significant_effects"}, {"score": 0.0029784342137867776, "phrase": "prosody_congruency"}, {"score": 0.0029490571526071016, "phrase": "eye_movements"}, {"score": 0.0027925723953701083, "phrase": "emotion_type"}, {"score": 0.0027513507830537165, "phrase": "matching_prosody"}, {"score": 0.0026839922027139967, "phrase": "upper_part"}, {"score": 0.00263129148531588, "phrase": "sad_facial_expressions"}, {"score": 0.0025924443774713473, "phrase": "visual_attention"}, {"score": 0.0025668646315794947, "phrase": "upper_and_lower_regions"}, {"score": 0.00240662380079115, "phrase": "vocal_emotion_cues"}, {"score": 0.002278856202878812, "phrase": "salient_visual_cues"}, {"score": 0.0022120408738490437, "phrase": "holistic_impression"}, {"score": 0.0021902063413831545, "phrase": "intended_meanings"}, {"score": 0.002168586864007222, "phrase": "interpersonal_events"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speech", " Prosody", " Face", " Eye-tracking", " Emotion", " Cross-modal"], "paper_abstract": "Previous eye-tracking studies have found that listening to emotionally-inflected utterances guides visual behavior towards an emotionally congruent face (e.g., Rigoulot and Pell, 2012). Here, we investigated in more detail whether emotional speech prosody influences how participants scan and fixate specific features of an emotional face that is congruent or incongruent with the prosody. Twenty-one participants viewed individual faces expressing fear, sadness, disgust, or happiness while listening to an emotionally-inflected pseudoutterance spoken in a congruent or incongruent prosody. Participants judged whether the emotional meaning of the face and voice were the same or different (match/mismatch). Results confirm that there were significant effects of prosody congruency on eye movements when participants scanned a face, although these varied by emotion type; a matching prosody promoted more frequent looks to the upper part of fear and sad facial expressions, whereas visual attention to upper and lower regions of happy (and to some extent disgust) faces was more evenly distributed. These data suggest ways that vocal emotion cues guide how humans process facial expressions in a way that could facilitate recognition of salient visual cues, to arrive at a holistic impression of intended meanings during interpersonal events. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Emotion in the voice influences the way we scan emotional faces", "paper_id": "WOS:000341901700004"}