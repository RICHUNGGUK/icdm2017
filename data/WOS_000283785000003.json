{"auto_keywords": [{"score": 0.04836250326740317, "phrase": "multiple_modalities"}, {"score": 0.015719716506582538, "phrase": "design_review"}, {"score": 0.014576315707141739, "phrase": "natural_interaction"}, {"score": 0.014338807893362601, "phrase": "large-scale_displays"}, {"score": 0.010591474399247649, "phrase": "input_devices"}, {"score": 0.01011337154451136, "phrase": "immiview"}, {"score": 0.009281453966024845, "phrase": "multi-modal_fusion_system"}, {"score": 0.004704407458864224, "phrase": "interactive_system"}, {"score": 0.004611669085594069, "phrase": "multi-user_interaction"}, {"score": 0.004565984572487962, "phrase": "collaborative_design_review"}, {"score": 0.004431616517786392, "phrase": "visualization_setups"}, {"score": 0.00430118556814906, "phrase": "tabletpc_computers"}, {"score": 0.004244449986839909, "phrase": "architectural_design"}, {"score": 0.004188459638572375, "phrase": "content_creation"}, {"score": 0.003984968827278219, "phrase": "laser_pointers"}, {"score": 0.003958591403140206, "phrase": "speech_commands"}, {"score": 0.0039323878878973284, "phrase": "body_gestures"}, {"score": 0.003906357144047631, "phrase": "mobile_devices"}, {"score": 0.003716520760680603, "phrase": "architectural_user_requirements"}, {"score": 0.0034660938095219846, "phrase": "new_graphical_user_interface"}, {"score": 0.0034317185678527672, "phrase": "architectural_user_tasks"}, {"score": 0.003286642683771891, "phrase": "novel_stroke-based_interaction"}, {"score": 0.003254041350487688, "phrase": "simple_laser_pointers"}, {"score": 0.003054922297665861, "phrase": "multiple_users"}, {"score": 0.002955050521440921, "phrase": "different_modalities"}, {"score": 0.00290634217991954, "phrase": "modality_adequacy"}, {"score": 0.002877502286766111, "phrase": "user_task"}, {"score": 0.002792683583705368, "phrase": "multi-modal_commands"}, {"score": 0.002544422946205545, "phrase": "user_actions"}, {"score": 0.002444883254621769, "phrase": "simple_rule-based_sub-module"}, {"score": 0.002364907617070299, "phrase": "user_evaluation"}, {"score": 0.0021907299375981356, "phrase": "multi-modal_approach"}, {"score": 0.0021049977753042253, "phrase": "architectural_tasks"}], "paper_keywords": ["Mixed reality", " Design review", " Human-computer interaction", " Real-time collaborativeInteraction", " Virtual reality"], "paper_abstract": "IMMIView is an interactive system that relies on multiple modalities and multi-user interaction to support collaborative design review. It was designed to offer natural interaction in visualization setups such as large-scale displays, head mounted displays or TabletPC computers. To support architectural design, our system provides content creation and manipulation, 3D scene navigation and annotations. Users can interact with the system using laser pointers, speech commands, body gestures and mobile devices. In this paper, we describe how we design a system to answer architectural user requirements. In particular, our system takes advantage of multiple modalities to provide a natural interaction for design review. We also propose a new graphical user interface adapted to architectural user tasks, such as navigation or annotations. The interface relies on a novel stroke-based interaction supported by simple laser pointers as input devices for large-scale displays. Furthermore, input devices such as speech and body tracking allow IMMIView to support multiple users. Moreover, they allow each user to select different modalities according to their preference and modality adequacy for the user task. We present a multi-modal fusion system developed to support multi-modal commands on a collaborative, co-located, environment, i.e. with two or more users interacting at the same time, on the same system. The multi-modal fusion system listens to inputs from all the IMMIView modules in order to model user actions and issue commands. The multiple modalities are fused based on a simple rule-based sub-module developed in IMMIView and presented in this paper. User evaluation performed over IMMIView is presented. The results show that users feel comfortable with the system and suggest that users prefer the multi-modal approach to more conventional interactions, such as mouse and menus, for the architectural tasks presented.", "paper_title": "IMMIView: a multi-user solution for design review in real-time", "paper_id": "WOS:000283785000003"}