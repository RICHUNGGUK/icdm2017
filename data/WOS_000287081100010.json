{"auto_keywords": [{"score": 0.0435790868041128, "phrase": "automatic_su_detection"}, {"score": 0.04013186491383319, "phrase": "human_conversations"}, {"score": 0.03450032510577697, "phrase": "su_detection"}, {"score": 0.00481495049065317, "phrase": "sentence_boundary_detection"}, {"score": 0.004761494044422926, "phrase": "accurate_estimation"}, {"score": 0.004726184966910756, "phrase": "sentence_units"}, {"score": 0.0046390482426405324, "phrase": "spontaneous_speech"}, {"score": 0.00443638953122199, "phrase": "speech_content"}, {"score": 0.004042056914551798, "phrase": "lexical_and_prosodic_cues"}, {"score": 0.00395273370234774, "phrase": "nonverbal_cues"}, {"score": 0.0038510053729959074, "phrase": "important_role"}, {"score": 0.0037658881407605445, "phrase": "semantic_content"}, {"score": 0.003724034990158436, "phrase": "emotional_status"}, {"score": 0.0036689507925299885, "phrase": "conversational_structure"}, {"score": 0.003614678418617691, "phrase": "close_relationship"}, {"score": 0.0034954725617970294, "phrase": "additional_contributions"}, {"score": 0.0033177813460815346, "phrase": "gesture_cues"}, {"score": 0.0031140747752837826, "phrase": "multimodal_data_resources"}, {"score": 0.0030679850425428053, "phrase": "su_events"}, {"score": 0.0029667545082292376, "phrase": "collected_data_sets"}, {"score": 0.0027949428646076627, "phrase": "statistical_models"}, {"score": 0.002722930209707764, "phrase": "gestural_cues"}, {"score": 0.0026428931714865115, "phrase": "gesture_patterns"}, {"score": 0.002613487968156893, "phrase": "word_boundary's_probability"}, {"score": 0.0025178072071268534, "phrase": "data_analyses"}, {"score": 0.00247128526955136, "phrase": "novel_gestural_features"}, {"score": 0.0023719332915829268, "phrase": "gestural_features"}, {"score": 0.002285075675790214, "phrase": "discriminative_models"}, {"score": 0.002268088741853586, "phrase": "findings_in_this_paper_support_the_view"}, {"score": 0.0022096205841489786, "phrase": "multimodal_cues"}, {"score": 0.0021049977753042253, "phrase": "verbal_and_nonverbal_channels"}], "paper_keywords": ["Gesture", " Nonverbal communication", " Multimodal processing", " Sentence boundary detection"], "paper_abstract": "An accurate estimation of sentence units (SUs) in spontaneous speech is important for (1) helping listeners to better understand speech content and (2) supporting other natural language processing tasks that require sentence information. There has been much research on automatic SU detection; however, most previous studies have only used lexical and prosodic cues, but have not used nonverbal cues, e. g., gesture. Gestures play an important role in human conversations, including providing semantic content, expressing emotional status, and regulating conversational structure. Given the close relationship between gestures and speech, gestures may provide additional contributions to automatic SU detection. In this paper, we have investigated the use of gesture cues for enhancing the SU detection. Particularly, we have focused on: (1) collecting multimodal data resources involving gestures and SU events in human conversations, (2) analyzing the collected data sets to enrich our knowledge about co-occurrence of gestures and SUs, and (3) building statistical models for detecting SUs using speech and gestural cues. Our data analyses suggest that some gesture patterns influence a word boundary's probability of being an SU. On the basis of the data analyses, a set of novel gestural features were proposed for SU detection. A combination of speech and gestural features was found to provide more accurate SU predictions than using only speech features in discriminative models. Findings in this paper support the view that human conversations are processes involving multimodal cues, and so they are more effectively modeled using information from both verbal and nonverbal channels.", "paper_title": "Utilizing gestures to improve sentence boundary detection", "paper_id": "WOS:000287081100010"}