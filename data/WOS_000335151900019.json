{"auto_keywords": [{"score": 0.03337540840020653, "phrase": "error_probability"}, {"score": 0.00481495049065317, "phrase": "least_squares_superposition_codes_with_bernoulli_dictionary"}, {"score": 0.00436890075128947, "phrase": "additive_white_gaussian_noise_channel"}, {"score": 0.004028796143006953, "phrase": "least_squares"}, {"score": 0.003868778065738324, "phrase": "barron"}, {"score": 0.0038065616731879663, "phrase": "joseph"}, {"score": 0.003236596168242036, "phrase": "gaussian_distribution"}, {"score": 0.0024963543716522087, "phrase": "bernoulli_distribution"}, {"score": 0.0021049977753042253, "phrase": "central_limit_theorem-type_inequality"}], "paper_keywords": ["Central limit theorem", " channel coding theorem", " exponential error bounds", " Gaussian channel", " sparse superposition codes"], "paper_abstract": "For the additive white Gaussian noise channel with average power constraint, sparse superposition codes with least squares decoding are proposed by Barron and Joseph in 2010. The codewords are designed by using a dictionary each entry of which is drawn from a Gaussian distribution. The error probability is shown to be exponentially small for all rates up to the capacity. This paper proves that when each entry of the dictionary is drawn from a Bernoulli distribution, the error probability is also exponentially small for all rates up to the capacity. The proof is via a central limit theorem-type inequality, which we show for this analysis.", "paper_title": "Least Squares Superposition Codes With Bernoulli Dictionary Are Still Reliable at Rates up to Capacity", "paper_id": "WOS:000335151900019"}