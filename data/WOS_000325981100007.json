{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "distributed_channel_synthesis"}, {"score": 0.004600228893803583, "phrase": "extreme_operating_points"}, {"score": 0.004553822674247401, "phrase": "distributed_synthesis"}, {"score": 0.004485085675709643, "phrase": "discrete_memoryless_channel"}, {"score": 0.00437281181878921, "phrase": "stochastic_channel_output"}, {"score": 0.004241771072194993, "phrase": "compressed_description"}, {"score": 0.004177724522109952, "phrase": "channel_input"}, {"score": 0.004135562737140424, "phrase": "wyner's_common_information"}, {"score": 0.004073113268289532, "phrase": "minimum_description_rate"}, {"score": 0.00395101799538683, "phrase": "common_randomness_independent"}, {"score": 0.003793876828939456, "phrase": "necessary_description_rate"}, {"score": 0.003736567662016446, "phrase": "shannon's_mutual_information"}, {"score": 0.0036245239349157236, "phrase": "optimal_tradeoff"}, {"score": 0.0035337158050484474, "phrase": "common_randomness"}, {"score": 0.003462704420080129, "phrase": "required_rate"}, {"score": 0.003291336101694653, "phrase": "related_derivations"}, {"score": 0.0031926001871260524, "phrase": "limited_local_randomness"}, {"score": 0.0030500051941781034, "phrase": "new_insights"}, {"score": 0.003019189898170698, "phrase": "common_information_duality"}, {"score": 0.0029137604783116065, "phrase": "soft_covering_lemma"}, {"score": 0.002672750634553146, "phrase": "direct_proof"}, {"score": 0.002592523232809191, "phrase": "feasible_joint_distribution"}, {"score": 0.0024766636161181544, "phrase": "soft_covering"}, {"score": 0.002283298119790413, "phrase": "explicit_reference"}, {"score": 0.002260211578368138, "phrase": "joint_typicality"}, {"score": 0.002203509076886664, "phrase": "auxiliary_interest"}, {"score": 0.0021049977753042253, "phrase": "soft_covering_tool"}], "paper_keywords": ["Channel simulation", " channel synthesis", " common information", " random number generator", " resolvability", " reverse Shannon theorem", " soft covering", " total variation distance"], "paper_abstract": "Two familiar notions of correlation are rediscovered as the extreme operating points for distributed synthesis of a discrete memoryless channel, in which a stochastic channel output is generated based on a compressed description of the channel input. Wyner's common information is the minimum description rate needed. However, when common randomness independent of the input is available, the necessary description rate reduces to Shannon's mutual information. This paper characterizes the optimal tradeoff between the amount of common randomness used and the required rate of description. We also include a number of related derivations, including the effect of limited local randomness, rate requirements for secrecy, applications to game theory, and new insights into common information duality. Our proof makes use of a soft covering lemma, known in the literature for its role in quantifying the resolvability of a channel. The direct proof (achievability) constructs a feasible joint distribution over all parts of the system using a soft covering, from which the behavior of the encoder and decoder is inferred, with no explicit reference to joint typicality or binning. Of auxiliary interest, this paper also generalizes and strengthens this soft covering tool.", "paper_title": "Distributed Channel Synthesis", "paper_id": "WOS:000325981100007"}