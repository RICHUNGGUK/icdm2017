{"auto_keywords": [{"score": 0.04871093899700793, "phrase": "positive_semidefinite"}, {"score": 0.00481495049065317, "phrase": "mutual_information_matrices"}, {"score": 0.004576763709613412, "phrase": "discrete_random_variables"}, {"score": 0.0038179283509963695, "phrase": "mutual_information"}, {"score": 0.0027341763737751467, "phrase": "mutual_information_matrix"}], "paper_keywords": ["Information inequalities", " mutual information", " linear algebra"], "paper_abstract": "For discrete random variables X-1, ... , X-n we construct an n by n matrix. In the (i, j)-entry we put the mutual information I (X-i; X-j) between X-i and X-j. In particular, in the (i, i)-entry we put the entropy H(X-i) = I (X-i; X-i) of X-i. This matrix, called the mutual information matrix of (X-1, ... , X-n), has been conjectured to be positive semidefinite. In this paper, we give counterexamples to the conjecture, and show that the conjecture holds for up to three random variables.", "paper_title": "Mutual Information Matrices Are Not Always Positive Semidefinite", "paper_id": "WOS:000335151900015"}