{"auto_keywords": [{"score": 0.02893740369903065, "phrase": "segmentation_accuracy"}, {"score": 0.004832762679569154, "phrase": "context"}, {"score": 0.004703149944841539, "phrase": "refining_boundaries"}, {"score": 0.004648222388468414, "phrase": "tts_units"}, {"score": 0.004575977644865379, "phrase": "high_quality_synthesis"}, {"score": 0.0044002186469797476, "phrase": "segmental_units"}, {"score": 0.004281202319048493, "phrase": "careful_manual_labeling"}, {"score": 0.004214637810698362, "phrase": "human_experts"}, {"score": 0.0038817197332605647, "phrase": "two-step_procedure"}, {"score": 0.003688892770976908, "phrase": "first_step"}, {"score": 0.0036600862989330106, "phrase": "coarse_segmentation"}, {"score": 0.0036315039559860654, "phrase": "speech_data"}, {"score": 0.003561017335579207, "phrase": "speech_signals"}, {"score": 0.00351938145050947, "phrase": "corresponding_sequence"}, {"score": 0.003491894048789576, "phrase": "hidden_markov_models"}, {"score": 0.0033840673522948592, "phrase": "second_step"}, {"score": 0.003357633232360232, "phrase": "segment_boundaries"}, {"score": 0.0032924448113309797, "phrase": "proposed_context-dependent_boundary_model"}, {"score": 0.003092200512475624, "phrase": "available_data"}, {"score": 0.003056029284235528, "phrase": "structured_hierarchical_tree"}, {"score": 0.003020279889535487, "phrase": "acoustically_similar_boundaries"}, {"score": 0.0029384777371425862, "phrase": "cdbm_models"}, {"score": 0.002915514089373717, "phrase": "boundary_refinement"}, {"score": 0.002892729377519916, "phrase": "optimal_cdbm_parameters"}, {"score": 0.0028701222153940283, "phrase": "training_conditions"}, {"score": 0.002792375587031108, "phrase": "experimental_studies"}, {"score": 0.002748895580860912, "phrase": "manual_segmentation_reference"}, {"score": 0.002511672561475743, "phrase": "mandarin_chinese"}, {"score": 0.002443784692626375, "phrase": "english"}, {"score": 0.0022948742034852917, "phrase": "manual_data"}, {"score": 0.0022769287097263564, "phrase": "training_cdbms"}, {"score": 0.002250272652754455, "phrase": "new_speaker"}, {"score": 0.002206536029072657, "phrase": "well-trained_cdbm"}, {"score": 0.0021892798077661956, "phrase": "efficient_adaptation_algorithms"}, {"score": 0.0021049977753042253, "phrase": "cdbm"}], "paper_keywords": ["context-dependent boundary model", " segment refinement", " CART", " HMM adaptation"], "paper_abstract": "For producing high quality synthesis, a concatenation-based Text-to-Speech (TTS) system usually requires a large number of segmental units to cover various acoustic-phonetic contexts. However, careful manual labeling and segmentation by human experts, which is still the most reliable way to prepare such units, is labor intensive. In this paper we adopt a two-step procedure to automate the labeling, segmentation and refinement process. In the first step, coarse segmentation of speech data is performed by aligning speech signals with the corresponding sequence of Hidden Markov Models (HMMs). Then in the second step, segment boundaries are refined with a proposed Context-Dependent Boundary Model (CDBM). Classification and Regression Tree (CART) is adopted to organize available data into a structured hierarchical tree, where acoustically similar boundaries are Clustered together to train tied CDBM models for boundary refinement. Optimal CDBM parameters and training conditions are found through a series of experimental studies. Comparing with manual segmentation reference, segmentation accuracy (within a tolerance of 20 ins) is improved by the CDBMs from 78.1% (baseline) to 94.8% in Mandarin Chinese and from 81.4% to 92.7% in English, with about 1,000 manually segmented sentences used in training the models. To further reduce the amount of manual data for training CDBMs of a new speaker, we adapt a well-trained CDBM via efficient adaptation algorithms. With only 10-20 manually segmented sentences as adaptation data, the adapted CDBM achieves a segmentation accuracy of 90%.", "paper_title": "Context-dependent boundary model for refining boundaries segmentation of TTS units", "paper_id": "WOS:000236290400026"}