{"auto_keywords": [{"score": 0.035035001304309796, "phrase": "parametric_classifiers"}, {"score": 0.015719716506582538, "phrase": "hybrid_classification"}, {"score": 0.014382919810013058, "phrase": "parametric_methods"}, {"score": 0.013474417317739684, "phrase": "nonparametric_methods"}, {"score": 0.01269775786881083, "phrase": "model_assumptions"}, {"score": 0.0047292449487169345, "phrase": "assisted_posterior_estimates"}, {"score": 0.004672953146379813, "phrase": "traditional_parametric_and_nonparametric_classifiers"}, {"score": 0.004589763483242731, "phrase": "statistical_pattern_recognition"}, {"score": 0.004322967486451877, "phrase": "specific_parametric_models"}, {"score": 0.004271490815763164, "phrase": "density_functions"}, {"score": 0.004220624516321415, "phrase": "posterior_probabilities"}, {"score": 0.0041703614123283165, "phrase": "competing_classes"}, {"score": 0.00374399358035391, "phrase": "nonparametric_classifiers"}, {"score": 0.003633464433320317, "phrase": "training_sample"}, {"score": 0.003361069456060196, "phrase": "poor_performance"}, {"score": 0.002945669860541221, "phrase": "parametric_and_nonparametric_approaches"}, {"score": 0.002841545331522694, "phrase": "resulting_classifiers"}, {"score": 0.0027741761880779535, "phrase": "hybrid_classifiers"}, {"score": 0.0024604876113911173, "phrase": "possible_deviations"}, {"score": 0.0024311346712789553, "phrase": "parametric_model_assumptions"}, {"score": 0.0023033100935570755, "phrase": "multiscale_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Bayes rule", " Cross-validation", " LDA", " Misclassification rates", " Multiscale analysis", " Nearest neighbor classification", " QDA", " Stacking"], "paper_abstract": "Traditional parametric and nonparametric classifiers used for statistical pattern recognition have their own strengths and limitations. While parametric methods assume some specific parametric models for density functions or posterior probabilities of competing classes, nonparametric methods are free from such assumptions. So, when these model assumptions are correct, parametric methods outperform nonparametric classifiers, especially when the training sample is small. But, violations of these assumptions often lead to poor performance by parametric classifiers, where nonparametric methods work well. In this article, we make an attempt to overcome these limitations of parametric and nonparametric approaches and combine their strengths. The resulting classifiers, denoted the hybrid classifiers, perform like parametric classifiers when the model assumptions are valid, but unlike parametric classifiers, they also provide safeguards against possible deviations from parametric model assumptions. In this article, we propose some multiscale methods for hybrid classification, and their performance is evaluated using several simulated and benchmark data sets. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "On hybrid classification using model assisted posterior estimates", "paper_id": "WOS:000301758400023"}