{"auto_keywords": [{"score": 0.03598095324632114, "phrase": "outer_facial_expressions"}, {"score": 0.03529624542703246, "phrase": "inner_individualized_emotions"}, {"score": 0.00481495049065317, "phrase": "video_emotion"}, {"score": 0.004720419202969916, "phrase": "audiences'_facial_expression"}, {"score": 0.004447766706144458, "phrase": "novel_implicit_video_emotion_tagging_approach"}, {"score": 0.004274766964161756, "phrase": "videos'_common_emotions"}, {"score": 0.004218602458288531, "phrase": "subjects'_individualized_emotions"}, {"score": 0.003948613642680196, "phrase": "face_appearance_features"}, {"score": 0.0037699306119313154, "phrase": "spontaneous_facial_expressions"}, {"score": 0.003623199939953831, "phrase": "bayesian_networks"}, {"score": 0.0032807941194429235, "phrase": "video's_common_emotions"}, {"score": 0.0031739835218110015, "phrase": "bayesian_network"}, {"score": 0.0029903851294825023, "phrase": "emotional_tags"}, {"score": 0.0027619548979328154, "phrase": "emotion_tagging_experiment"}, {"score": 0.002671990515821213, "phrase": "nvie_database"}, {"score": 0.0026194209399161184, "phrase": "experimental_results"}, {"score": 0.0024353398669404334, "phrase": "facial_expression_recognition"}, {"score": 0.0023096222276192194, "phrase": "captured_relations"}, {"score": 0.002175913488493802, "phrase": "common_emotions"}, {"score": 0.0021049977753042253, "phrase": "common_and_individualized_emotion"}], "paper_keywords": ["Implicit emotion tagging", " Expression recognition", " Emotion inference"], "paper_abstract": "In this paper, we propose a novel implicit video emotion tagging approach by exploring the relationships between videos' common emotions, subjects' individualized emotions and subjects' outer facial expressions. First, head motion and face appearance features are extracted. Then, the spontaneous facial expressions of subjects are recognized by Bayesian networks. After that, the relationships between the outer facial expressions, the inner individualized emotions and the video's common emotions are captured by another Bayesian network, which can be used to infer the emotional tags of videos. To validate the effectiveness of our approach, an emotion tagging experiment is conducted on the NVIE database. The experimental results show that head motion features improve the performance of both facial expression recognition and emotion tagging, and that the captured relations between the outer facial expressions, the inner individualized emotions and the common emotions improve the performance of common and individualized emotion tagging.", "paper_title": "Implicit video emotion tagging from audiences' facial expression", "paper_id": "WOS:000357130400011"}