{"auto_keywords": [{"score": 0.05007844442530217, "phrase": "sparse_proximal_support_vector_machines"}, {"score": 0.049662210589786246, "phrase": "feature_selection"}, {"score": 0.04743102851574749, "phrase": "hdlss"}, {"score": 0.004731492618963217, "phrase": "high_dimensional_datasets"}, {"score": 0.004649474569983707, "phrase": "high_dimension"}, {"score": 0.004450568197899267, "phrase": "challenging_task"}, {"score": 0.0044118153390803405, "phrase": "supervised_learning"}, {"score": 0.004260134699483774, "phrase": "biomedical_applications"}, {"score": 0.004223032986655664, "phrase": "business_analytics"}, {"score": 0.004095692768056685, "phrase": "new_embedded_feature_selection_method"}, {"score": 0.003920384090110374, "phrase": "proximal_support_vector_machines"}, {"score": 0.003671338229632309, "phrase": "sparse_representation"}, {"score": 0.003529543381704363, "phrase": "equivalent_least_squares_problem"}, {"score": 0.003378385493203297, "phrase": "efficient_algorithm"}, {"score": 0.0033197446406648626, "phrase": "optimization_techniques"}, {"score": 0.0031223774204093713, "phrase": "generalization_performance"}, {"score": 0.0030547611342112693, "phrase": "feature_selection_process"}, {"score": 0.0027985846558073457, "phrase": "selected_features"}, {"score": 0.002690406375639144, "phrase": "class-specific_local_sparsity"}, {"score": 0.002508284945907166, "phrase": "data_dimensionality"}, {"score": 0.0023079225317714815, "phrase": "dimensionality_reduction"}, {"score": 0.0021610874773389096, "phrase": "free_classification_tasks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Embedded feature selection", " Sparsity", " Regularization", " Class-specific feature selection", " High dimensional datasets"], "paper_abstract": "Classification of High Dimension Low Sample Size (HDLSS) datasets is a challenging task in supervised learning. Such datasets are prevalent in various areas including biomedical applications and business analytics. In this paper, a new embedded feature selection method for HDLSS datasets is introduced by incorporating sparsity in Proximal Support Vector Machines (PSVMs). Our method, called Sparse Proximal Support Vector Machines (sPSVMs), learns a sparse representation of PSVMs by first casting it as an equivalent least squares problem and then introducing the l(1)-norm for sparsity. An efficient algorithm based on alternating optimization techniques is proposed. sPSVMs remove more than 98% of features in many high dimensional datasets without compromising on generalization performance. Stability in the feature selection process of sPSVMs is also studied and compared with other univariate filter techniques. Additionally, sPSVMs offer the advantage of interpreting the selected features in the context of the classes by inducing class-specific local sparsity instead of global sparsity like other embedded methods. sPSVMs appear to be robust with respect to data dimensionality. Moreover, sPSVMs are able to perform feature selection and classification in one step, eliminating the need for dimensionality reduction on the data. To that end, sPSVMs can be used for preprocessing free classification tasks. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Sparse Proximal Support Vector Machines for feature selection in high dimensional datasets", "paper_id": "WOS:000362613000010"}