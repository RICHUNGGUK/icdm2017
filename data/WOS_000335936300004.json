{"auto_keywords": [{"score": 0.049445419527232096, "phrase": "decision_trees"}, {"score": 0.00481495049065317, "phrase": "improving_decision_trees"}, {"score": 0.004356926418550194, "phrase": "rigid_decision_criteria"}, {"score": 0.004158290806404304, "phrase": "single_\"uncharacteristic\"_attribute"}, {"score": 0.003995226376780718, "phrase": "classification_process"}, {"score": 0.003359119985242403, "phrase": "outlier_instances"}, {"score": 0.00312138530097218, "phrase": "decision_tree_algorithm"}, {"score": 0.0026771333996253783, "phrase": "hard-to-classify_instances"}, {"score": 0.002537880772794665, "phrase": "experimental_study"}, {"score": 0.002470987929071079, "phrase": "proposed_post-processing_method"}, {"score": 0.0023739321566540682, "phrase": "predictive_performance"}, {"score": 0.0021049977753042253, "phrase": "auc_performance"}], "paper_keywords": ["decision tree", " confidence interval", " imbalanced dataset"], "paper_abstract": "Decision trees have three main disadvantages: reduced performance when the training set is small; rigid decision criteria; and the fact that a single \"uncharacteristic\" attribute might \"derail\" the classification process. In this paper we present ConfDTree (Confidence-Based Decision Tree) a post-processing method that enables decision trees to better classify outlier instances. This method, which can be applied to any decision tree algorithm, uses easy-to-implement statistical methods (confidence intervals and two-proportion tests) in order to identify hard-to-classify instances and to propose alternative routes. The experimental study indicates that the proposed post-processing method consistently and significantly improves the predictive performance of decision trees, particularly for small, imbalanced or multi-class data,sets in which an average improvement of 5%-9% in the AUC performance is reported.", "paper_title": "ConfDTree: A Statistical Method for Improving Decision Trees", "paper_id": "WOS:000335936300004"}