{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "optical_trackers"}, {"score": 0.00471700544744796, "phrase": "software_framework"}, {"score": 0.004602085616330634, "phrase": "model-based_optical_trackers"}, {"score": 0.004291393968030603, "phrase": "different_trackers"}, {"score": 0.004135458387200662, "phrase": "varying_intrinsic_and_extrinsic_camera_properties"}, {"score": 0.004001593118269276, "phrase": "environmental_conditions"}, {"score": 0.003968806706727518, "phrase": "tracker_performance"}, {"score": 0.0035956327252723483, "phrase": "framework_model"}, {"score": 0.0034935290662899488, "phrase": "interaction_task"}, {"score": 0.003464890914694523, "phrase": "input_device_geometry"}, {"score": 0.0034364867143322304, "phrase": "camera_properties"}, {"score": 0.003352658916650986, "phrase": "concrete_case"}, {"score": 0.003244050414264445, "phrase": "proposed_framework"}, {"score": 0.0032174507570710835, "phrase": "input_device_tracking"}, {"score": 0.003177958355586418, "phrase": "near-field_desktop_virtual_environment"}, {"score": 0.0030749917707689222, "phrase": "in-house_tracker"}, {"score": 0.0030372426452745073, "phrase": "artoolkitplus-based_tracker"}, {"score": 0.002999955539996569, "phrase": "fixed_set"}, {"score": 0.0027514186391258263, "phrase": "pre-recorded_interaction_task"}, {"score": 0.002629539710839523, "phrase": "minimum_required_camera_resolution"}, {"score": 0.002575940285451686, "phrase": "workbench"}, {"score": 0.0025548051498862413, "phrase": "cave"}, {"score": 0.002471973283582411, "phrase": "random_noise"}, {"score": 0.00245168871716047, "phrase": "tracker_accuracy"}, {"score": 0.0023624435546097658, "phrase": "efficient_and_simple_method"}, {"score": 0.002314271047073691, "phrase": "optical_tracker_performance"}, {"score": 0.002220846338732873, "phrase": "valuable_development_tool"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Virtual reality", " Optical tracking", " Simulation", " Evaluation"], "paper_abstract": "We describe a software framework to evaluate the performance of model-based optical trackers in virtual environments. The framework can be used to evaluate and compare the performance of different trackers under various conditions, to study the effects of varying intrinsic and extrinsic camera properties, and to study the effects of environmental conditions on tracker performance. The framework consists of a simulator that, given various input conditions, generates a series of images. The input conditions of the framework model important aspects, such as the interaction task, input device geometry, camera properties and occlusion. As a concrete case, we illustrate the usage of the proposed framework for input device tracking in a near-field desktop virtual environment. We compare the performance of an in-house tracker with an ARToolkitPlus-based tracker under a fixed set of conditions. We also show how the framework can be used to assess the quality of various camera placements given a pre-recorded interaction task. Finally, we use the framework to determine the minimum required camera resolution for a desktop, Workbench and CAVE environment, and study the influence of random noise on tracker accuracy. The framework is shown to provide an efficient and simple method to study various conditions affecting optical tracker performance. Furthermore, it can be used as a valuable development tool to aid in the construction of optical trackers. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "A simulator-based approach to evaluating optical trackers", "paper_id": "WOS:000265993100002"}