{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "poisson_channels"}, {"score": 0.004592016716046632, "phrase": "fundamental_connection"}, {"score": 0.0045302298105691615, "phrase": "information_measures"}, {"score": 0.0044692705329574, "phrase": "estimation_measures"}, {"score": 0.004409127897028587, "phrase": "gaussian_channels"}, {"score": 0.003983016985197904, "phrase": "continuous-time_setting"}, {"score": 0.0039028497942174777, "phrase": "received_signal"}, {"score": 0.0038242899532391914, "phrase": "doubly_stochastic_poisson_point_process"}, {"score": 0.0036470559917231218, "phrase": "input_signal"}, {"score": 0.00357362651129581, "phrase": "dark_current"}, {"score": 0.0031415984808041324, "phrase": "input-output_mutual_information"}, {"score": 0.002975652654536407, "phrase": "additive_dark_current"}, {"score": 0.0028569549040495163, "phrase": "expected_difference"}, {"score": 0.0024607296642187824, "phrase": "input_scaling"}, {"score": 0.00237861522566195, "phrase": "logarithmic_function"}, {"score": 0.002330664671716791, "phrase": "x_log_x._similar_relationships"}, {"score": 0.0022836785412390544, "phrase": "discrete-time_versions"}, {"score": 0.002162951519368368, "phrase": "poisson_random_variables"}, {"score": 0.0021049977753042253, "phrase": "input_symbols"}], "paper_keywords": ["mutual information", " nonlinear filtering", " optimal estimation", " point process", " Poisson process", " smoothing"], "paper_abstract": "Following the discovery of a fundamental connection between information measures and estimation measures in Gaussian channels, this paper explores the counterpart of those results in Poisson channels. In the continuous-time setting, the received signal is a doubly stochastic Poisson point process whose rate is equal to the input signal plus a dark current. It is found that, regardless of the statistics of the input, the derivative of the input-output mutual information with respect to the intensity of the additive dark current can be expressed as the expected difference between the logarithm of the input and the logarithm of its noncausal conditional mean estimate. The same holds for the derivative with respect to input scaling, but with the logarithmic function replaced by x log x. Similar relationships hold for discrete-time versions of the channel where the outputs are Poisson random variables conditioned on the input symbols.", "paper_title": "Mutual information and conditional mean estimation in Poisson channels", "paper_id": "WOS:000255505700001"}