{"auto_keywords": [{"score": 0.04670675420350503, "phrase": "pca"}, {"score": 0.010561366504805993, "phrase": "kernel_pca"}, {"score": 0.00481495049065317, "phrase": "kernel_principal_components"}, {"score": 0.0046993598473345395, "phrase": "maximum_entropy_projections"}, {"score": 0.004586531354697466, "phrase": "principal_component_analysis"}, {"score": 0.004162584980567737, "phrase": "kernel"}, {"score": 0.003964007795174772, "phrase": "nonlinear_extension"}, {"score": 0.003685067218455088, "phrase": "kernel_paradigm"}, {"score": 0.003107935254560724, "phrase": "information_theoretic_perspective"}, {"score": 0.0028541057064952876, "phrase": "optimum_entropy_projections"}, {"score": 0.002751751956416832, "phrase": "input_space"}, {"score": 0.0026530590435527527, "phrase": "gaussian_kernel"}, {"score": 0.002406799309495983, "phrase": "sample_estimate"}, {"score": 0.002348883765159185, "phrase": "renyi's_entropy"}, {"score": 0.0022371907579775796, "phrase": "parzen_window_method"}, {"score": 0.0021049977753042253, "phrase": "information_theoretic_interpretation"}], "paper_keywords": ["kernel PCA", " information-theoretic learning", " entropy projections"], "paper_abstract": "Principal Component Analysis (PCA) is a very well known statistical tool. KERNEL PCA is a nonlinear extension to PCA based on the kernel paradigm. In this paper we characterize the projections found by KERNEL PCA from a information theoretic perspective. We prove that KERNEL PCA provides optimum entropy projections in the input space when the Gaussian kernel is used for the mapping and a sample estimate of Renyi's entropy based on the Parzen window method is employed. The information theoretic interpretation motivates the choice and specifices the kernel used for the transformation to feature space.", "paper_title": "Kernel principal components are maximum entropy projections", "paper_id": "WOS:000236486300105"}