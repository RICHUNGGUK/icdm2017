{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "three-term_conjugate_gradient_method"}, {"score": 0.004622450915803531, "phrase": "sufficient_descent_condition"}, {"score": 0.0037307208089736835, "phrase": "line_search"}, {"score": 0.003545024951402821, "phrase": "exact_line_search"}, {"score": 0.0032336801561165113, "phrase": "standard_hestenes-stiefel_conjugate_gradient_method"}, {"score": 0.0029798924689472014, "phrase": "proposed_method"}, {"score": 0.002831462722290752, "phrase": "sufficient_descent_property"}, {"score": 0.002556359086441866, "phrase": "standard_wolfe_line_search"}, {"score": 0.002453934333239467, "phrase": "minimization_function"}, {"score": 0.002261204313587616, "phrase": "numerical_experiment"}, {"score": 0.0021049977753042253, "phrase": "proposed_methods"}], "paper_keywords": ["three-term conjugate gradient method", " global convergence"], "paper_abstract": "In this paper, we propose a three-term conjugate gradient method which can produce sufficient descent condition, that is, g(k)(T) dk = -parallel to gk parallel to(2). This property is independent of any line search used. When an exact line search is used, this method reduces to the standard Hestenes-Stiefel conjugate gradient method. We also introduce two variants of the proposed method which still preserve the sufficient descent property, and prove that these two methods converge globally with standard Wolfe line search even if the minimization function is nonconvex. We also report some numerical experiment to show the efficiency of the proposed methods.", "paper_title": "Some descent three-term conjugate gradient methods and their global convergence", "paper_id": "WOS:000246662600010"}