{"auto_keywords": [{"score": 0.048839643896770986, "phrase": "adp"}, {"score": 0.00481495049065317, "phrase": "partially_observable_dynamic_processes"}, {"score": 0.004773040022691547, "phrase": "adaptive_dynamic_programming_using_measured_output_data"}, {"score": 0.004731492618963217, "phrase": "approximate_dynamic_programming"}, {"score": 0.004568871736067739, "phrase": "reinforcement_learning_methods"}, {"score": 0.00433531459712787, "phrase": "feedback_control"}, {"score": 0.004297560964892607, "phrase": "dynamical_systems"}, {"score": 0.004204602964512469, "phrase": "full_information"}, {"score": 0.004149792080414689, "phrase": "system_internal_states"}, {"score": 0.004007083686875716, "phrase": "practical_situations"}, {"score": 0.003818807671883534, "phrase": "adp_methods"}, {"score": 0.003703611021999795, "phrase": "linear_dynamical_systems"}, {"score": 0.003671338229632309, "phrase": "deterministic_behavior"}, {"score": 0.003529543381704363, "phrase": "great_interest"}, {"score": 0.0034835017385881385, "phrase": "control_system_community"}, {"score": 0.003438058620195309, "phrase": "control_system_theory"}, {"score": 0.0032908057622663732, "phrase": "output_feedback"}, {"score": 0.0032054891061528896, "phrase": "stochastic_equivalent"}, {"score": 0.0030281251520610604, "phrase": "partially_observable_markov_decision_processes"}, {"score": 0.002962543869356265, "phrase": "policy_iteration"}, {"score": 0.002936709606809639, "phrase": "value_iteration_algorithms"}, {"score": 0.0028731024405707277, "phrase": "optimal_controller"}, {"score": 0.00270221723573854, "phrase": "new_methods"}, {"score": 0.00266693888510356, "phrase": "important_advantage"}, {"score": 0.0026091594819333654, "phrase": "system_dynamics"}, {"score": 0.002508284945907166, "phrase": "learning_algorithms"}, {"score": 0.002464709648714996, "phrase": "opfb_control"}, {"score": 0.0023487324083283205, "phrase": "upper_bound"}, {"score": 0.0022480304016499605, "phrase": "learned_opfb_controller"}, {"score": 0.0021801135036476136, "phrase": "polynomial_autoregressive"}, {"score": 0.0021610874773389096, "phrase": "-average_controller"}, {"score": 0.0021328586481696157, "phrase": "equivalent_performance"}, {"score": 0.0021049977753042253, "phrase": "optimal_state_variable_feedback_gain"}], "paper_keywords": ["Approximate dynamic programming (ADP)", " data-based optimal control", " policy iteration (PI)", " output feedback (OPFB)", " value iteration (VI)"], "paper_abstract": "Approximate dynamic programming (ADP) is a class of reinforcement learning methods that have shown their importance in a variety of applications, including feedback control of dynamical systems. ADP generally requires full information about the system internal states, which is usually not available in practical situations. In this paper, we show how to implement ADP methods using only measured input/output data from the system. Linear dynamical systems with deterministic behavior are considered herein, which are systems of great interest in the control system community. In control system theory, these types of methods are referred to as output feedback (OPFB). The stochastic equivalent of the systems dealt with in this paper is a class of partially observable Markov decision processes. We develop both policy iteration and value iteration algorithms that converge to an optimal controller that requires only OPFB. It is shown that, similar to Q-learning, the new methods have the important advantage that knowledge of the system dynamics is not needed for the implementation of these learning algorithms or for the OPFB control. Only the order of the system, as well as an upper bound on its \"observability index,\" must be known. The learned OPFB controller is in the form of a polynomial autoregressive moving-average controller that has equivalent performance with the optimal state variable feedback gain.", "paper_title": "Reinforcement Learning for Partially Observable Dynamic Processes: Adaptive Dynamic Programming Using Measured Output Data", "paper_id": "WOS:000286388300002"}