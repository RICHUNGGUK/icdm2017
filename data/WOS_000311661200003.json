{"auto_keywords": [{"score": 0.02973622669825186, "phrase": "pca"}, {"score": 0.00481495049065317, "phrase": "lasso_approach"}, {"score": 0.004769958800452922, "phrase": "sparse_modeling"}, {"score": 0.004681226806682058, "phrase": "research_data"}, {"score": 0.004508661664944657, "phrase": "high_dimensional_multivariate_output"}, {"score": 0.004342430078306461, "phrase": "e.g._extraction"}, {"score": 0.004261617028111862, "phrase": "underlying_patterns"}, {"score": 0.004066076430818046, "phrase": "small_subset"}, {"score": 0.004028054020045027, "phrase": "significant_variables"}, {"score": 0.003916098498242888, "phrase": "feature_selection"}, {"score": 0.0034494494982242187, "phrase": "interesting_avenue"}, {"score": 0.003322140088192527, "phrase": "sparse_solution"}, {"score": 0.0032602525602322832, "phrase": "variable_selection"}, {"score": 0.0031695693589575916, "phrase": "brief_introduction"}, {"score": 0.003125175084962886, "phrase": "mathematical_properties"}, {"score": 0.002765490759189057, "phrase": "sparse_loadings"}, {"score": 0.0027012086158226456, "phrase": "single_components"}, {"score": 0.002675914969626753, "phrase": "sparse_inverse_covariance_matrix_estimation"}, {"score": 0.0024241817763927163, "phrase": "constant_responses"}, {"score": 0.0024014759148123736, "phrase": "e.g._process_monitoring"}, {"score": 0.002301896758256575, "phrase": "real_or_synthetic_data"}, {"score": 0.0022378101636842296, "phrase": "sparse_solutions"}, {"score": 0.0021652891317252994, "phrase": "model_interpretability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sparsity", " L-1 norm", " (Bi)convex optimization", " Lasso"], "paper_abstract": "In applied research data are often collected from sources with a high dimensional multivariate output. Analysis of such data is composed of e.g. extraction and characterization of underlying patterns, and often with the aim of finding a small subset of significant variables or features. Variable and feature selection is well-established in the area of regression, whereas for other types of models this seems more difficult. Penalization of the L-1 norm provides an interesting avenue for such a problem, as it produces a sparse solution and hence embeds variable selection. In this paper a brief introduction to the mathematical properties of using the L-1 norm as a penalty is given. Examples of models extended with L-1 norm penalties/constraints are presented. The examples include PCA modeling with sparse loadings which enhance interpretability of single components. Sparse inverse covariance matrix estimation is used to unravel which variables are affecting each other, and a modified PCA to model data with (piecewise) constant responses in e.g. process monitoring is shown. All examples are demonstrated on real or synthetic data. The results indicate that sparse solutions, when appropriate, can enhance model interpretability. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "A tutorial on the Lasso approach to sparse modeling", "paper_id": "WOS:000311661200003"}