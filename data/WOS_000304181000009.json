{"auto_keywords": [{"score": 0.04535040652933245, "phrase": "video_clips"}, {"score": 0.040583449969688636, "phrase": "full-body_actions"}, {"score": 0.00481495049065317, "phrase": "product_manifold_distance"}, {"score": 0.004786519708868553, "phrase": "unsupervised_action_recognition"}, {"score": 0.004674457608503586, "phrase": "unsupervised_learning"}, {"score": 0.0046194096969523315, "phrase": "human_actions"}, {"score": 0.004444921827311577, "phrase": "inherent_biases"}, {"score": 0.004315178999089153, "phrase": "semantically_meaningful_partitions"}, {"score": 0.004239150469974544, "phrase": "first_part"}, {"score": 0.0039248128268720645, "phrase": "human_facial_expressions"}, {"score": 0.003901617424466747, "phrase": "hand_gestures"}, {"score": 0.003798916441627028, "phrase": "better_understanding"}, {"score": 0.0037319497768008264, "phrase": "semantically_relevant_clustering"}, {"score": 0.003633698607193056, "phrase": "superior_results"}, {"score": 0.0035590664550919854, "phrase": "generated_clusters"}, {"score": 0.003527550673371394, "phrase": "nominal_class_labeling"}, {"score": 0.003424492885231787, "phrase": "gross_motions"}, {"score": 0.003294990915005896, "phrase": "structural_information"}, {"score": 0.0032561349010603734, "phrase": "bof_representation"}, {"score": 0.003160981401567776, "phrase": "supervised_training"}, {"score": 0.0030960281690122745, "phrase": "poor_separation"}, {"score": 0.0030777156987044385, "phrase": "shape_labels"}, {"score": 0.0030504492628487413, "phrase": "hand_gestures_data"}, {"score": 0.0030324096245535072, "phrase": "bof"}, {"score": 0.0029263562233195423, "phrase": "second_part"}, {"score": 0.002857719256931572, "phrase": "unsupervised_mechanism"}, {"score": 0.002815638762571003, "phrase": "continuous_video_streams"}, {"score": 0.002790687647919327, "phrase": "pm_representation"}, {"score": 0.0027252240573281163, "phrase": "prior_knowledge"}, {"score": 0.0027010719929751, "phrase": "expected_number"}, {"score": 0.0026534064006291853, "phrase": "silhouette_extraction"}, {"score": 0.0026143265665547525, "phrase": "minor_tracking_errors"}, {"score": 0.002545423925130231, "phrase": "real-time_speed"}, {"score": 0.002470987929071079, "phrase": "training_\"tracklets"}, {"score": 0.002405853977978678, "phrase": "product_manifold_distance_measure"}, {"score": 0.0022404162042797262, "phrase": "incremental_learning"}, {"score": 0.0022271532243556373, "phrase": "anomalous_activities"}, {"score": 0.002194337509146714, "phrase": "video_stream"}, {"score": 0.0021492044742907382, "phrase": "publicly-available_ethz_livingroom_data"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Unsupervised learning", " Product Manifold", " Action recognition"], "paper_abstract": "This paper presents a method for unsupervised learning and recognition of human actions in video. Lacking any supervision, there is nothing except the inherent biases of a given representation to guide grouping of video clips along semantically meaningful partitions. Thus, in the first part of this paper, we compare two contemporary methods. Bag of Features (BOF) and Product Manifolds (PM), for clustering video clips of human facial expressions, hand gestures, and full-body actions, with the goal of better understanding how well these very different approaches to behavior recognition produce semantically relevant clustering of data. We show that PM yields superior results when measuring the alignment between the generated clusters and the nominal class labeling of the data set. We found that while gross motions were easily clustered by both methods, the lack of preservation of structural information inherent to the BOF representation leads to limitations that are not easily overcome without supervised training. This was evidenced by the poor separation of shape labels in the hand gestures data by BOF, and the overall poor performance on full-body actions. In the second part of this paper, we present an unsupervised mechanism for learning micro-actions in continuous video streams using the PM representation. Unlike other works, our method requires no prior knowledge of an expected number of labels/classes, requires no silhouette extraction, is tolerant to minor tracking errors and jitter, and can operate at near real-time speed. We show how to construct a set of training \"tracklets,\" how to cluster them using the Product Manifold distance measure, and how to perform detection using exemplars learned from the clusters. Further, we show that the system is amenable to incremental learning as anomalous activities are detected in the video stream. We demonstrate performance using the publicly-available ETHZ Livingroom data set. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Using a Product Manifold distance for unsupervised action recognition", "paper_id": "WOS:000304181000009"}