{"auto_keywords": [{"score": 0.04233053722268817, "phrase": "perceptual_grouping"}, {"score": 0.00481495049065317, "phrase": "hierarchical_object_models"}, {"score": 0.004777569299319633, "phrase": "captioned_images"}, {"score": 0.004594951819987416, "phrase": "recurring_associations"}, {"score": 0.0045415338200109345, "phrase": "visual_structures"}, {"score": 0.004317109072311102, "phrase": "named_object_models"}, {"score": 0.004217286731331736, "phrase": "subsequent_image_annotation"}, {"score": 0.004168240841388958, "phrase": "previous_work"}, {"score": 0.004008820473628357, "phrase": "local_features"}, {"score": 0.003931402465972151, "phrase": "small_parts"}, {"score": 0.003781005930058317, "phrase": "model_scope"}, {"score": 0.003693533648584145, "phrase": "poor_object_localization"}, {"score": 0.0032094634756436595, "phrase": "previous_framework"}, {"score": 0.0031721009574803127, "phrase": "hierarchical_configurations"}, {"score": 0.0031351720217878917, "phrase": "greater_spatial_extent"}, {"score": 0.0030745737579268876, "phrase": "resulting_hierarchical_multipart_models"}, {"score": 0.0029110398328258194, "phrase": "better_localization"}, {"score": 0.002854761308379149, "phrase": "typical_frameworks"}, {"score": 0.002821516317503421, "phrase": "object_models"}, {"score": 0.002766963729586353, "phrase": "bounding_boxes"}, {"score": 0.0026609940110902666, "phrase": "heavily_cluttered_training_scenes"}, {"score": 0.0025690867646665835, "phrase": "noisy_captions"}, {"score": 0.0022672107906933714, "phrase": "improved_precision"}, {"score": 0.002206045784326466, "phrase": "non-hierarchical_technique"}, {"score": 0.002171836484200008, "phrase": "extended_spatial_coverage"}, {"score": 0.002154930852763048, "phrase": "detected_objects"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Language-vision integration", " Object recognition", " Automatic image annotation", " Learning hierarchical models"], "paper_abstract": "We address the problem of automatically learning the recurring associations between the visual structures in images and the words in their associated captions, yielding a set of named object models that can be used for subsequent image annotation. In previous work, we used language to drive the perceptual grouping of local features into configurations that capture small parts (patches) of an object. However, model scope was poor, leading to poor object localization during detection (annotation), and ambiguity was high when part detections were weak. We extend and significantly revise our previous framework by using language to drive the perceptual grouping of parts, each a configuration in the previous framework, into hierarchical configurations that offer greater spatial extent and flexibility. The resulting hierarchical multipart models remain scale, translation and rotation invariant, but are more reliable detectors and provide better localization. Moreover, unlike typical frameworks for learning object models, our approach requires no bounding boxes around the objects to be learned, can handle heavily cluttered training scenes, and is robust in the face of noisy captions, i.e., where objects in an image may not be named in the caption, and objects named in the caption may not appear in the image. We demonstrate improved precision and recall in annotation over the non-hierarchical technique and also show extended spatial coverage of detected objects. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Discovering hierarchical object models from captioned images", "paper_id": "WOS:000304339500006"}