{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "support_vector_machines"}, {"score": 0.02620822443567432, "phrase": "svms_mixture"}, {"score": 0.00417160036938041, "phrase": "svm_mixture"}, {"score": 0.004031495920060339, "phrase": "two-stage_architecture"}, {"score": 0.003922794571908569, "phrase": "first_stage"}, {"score": 0.0038695436922618876, "phrase": "self-organizing_feature_map"}, {"score": 0.003663645143713398, "phrase": "clustering_algorithm"}, {"score": 0.0035648266669315943, "phrase": "whole_input_space"}, {"score": 0.0034450303951025704, "phrase": "tree-structured_architecture"}, {"score": 0.0031091858573262265, "phrase": "partitioned_regions"}, {"score": 0.002984173420996039, "phrase": "second_stake"}, {"score": 0.0029436264166293317, "phrase": "multiple_svms"}, {"score": 0.0028643523916756083, "phrase": "svm"}, {"score": 0.002786858120570499, "phrase": "best_fit_partitioned_regions"}, {"score": 0.002620424858132324, "phrase": "optimal_free_parameters"}, {"score": 0.002430411367515404, "phrase": "significant_improvement"}, {"score": 0.0023810183846557486, "phrase": "generalization_performance"}, {"score": 0.002285216591220303, "phrase": "single_svms_model"}], "paper_keywords": ["non-stationarity : support vector machines mixture", " self-organizing feature map"], "paper_abstract": "A mixture of support vector machines (SVMs) is proposed for time series forecasting. The SVM mixture is composed of a two-stage architecture. In the first stage, self-organizing feature map (SOM) is used as a clustering algorithm to partition the whole input space into several disjointed regions. A tree-structured architecture is adopted in the partition to avoid the problem of predetermining the number of partitioned regions. Then, in the second stake, multiple SVMs, also called SVM mixture, that best fit partitioned regions are constructed by finding the most appropriate kernel function and the optimal free parameters of SVMs. The experiment shows that the SVMs mixture achieves significant improvement in the generalization performance in comparison-with the single SVMs model. In addition, the SVMs mixture also converges faster and use fewer support vectors.", "paper_title": "A mixture of support vector machines for time series forecasting", "paper_id": "WOS:000242298400002"}