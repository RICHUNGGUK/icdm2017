{"auto_keywords": [{"score": 0.025656974412946006, "phrase": "call_options"}, {"score": 0.00481495049065317, "phrase": "functional_knowledge"}, {"score": 0.0047468516620285525, "phrase": "neural_networks"}, {"score": 0.004646496622257368, "phrase": "prior_knowledge"}, {"score": 0.004548253531664032, "phrase": "particular_task"}, {"score": 0.004357927940322685, "phrase": "learning_algorithm"}, {"score": 0.0042354707960660706, "phrase": "generalization_performance"}, {"score": 0.0031845067058084613, "phrase": "multi-layer_neural_networks"}, {"score": 0.002861302930564756, "phrase": "universal_approximator"}, {"score": 0.0026077803487003, "phrase": "new_class"}, {"score": 0.0021971473452127126, "phrase": "new_types"}, {"score": 0.0021659920594094407, "phrase": "function_classes"}], "paper_keywords": ["neural networks", " universal approximation", " monotonicity", " convexity", " call options"], "paper_abstract": "Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz(1) functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.", "paper_title": "Incorporating Functional Knowledge in Neural Networks", "paper_id": "WOS:000270824900002"}