{"auto_keywords": [{"score": 0.03152448407819682, "phrase": "adaboost"}, {"score": 0.00481495049065317, "phrase": "adaptive_distance"}, {"score": 0.004382913938427474, "phrase": "nearest_neighbor_classification"}, {"score": 0.004009288598277725, "phrase": "training_samples"}, {"score": 0.0038727120021750973, "phrase": "distance_metric"}, {"score": 0.003649277504586372, "phrase": "novel_two-level_nearest_neighbor_algorithm"}, {"score": 0.0034901729647741353, "phrase": "mean-absolute_error"}, {"score": 0.0034386894664467003, "phrase": "misclassification_rate"}, {"score": 0.003371220210416809, "phrase": "finite_and_infinite_number"}, {"score": 0.003192406091820197, "phrase": "euclidean_distance"}, {"score": 0.0031297538950335233, "phrase": "local_subspace"}, {"score": 0.003068327486877065, "phrase": "unlabeled_test_sample"}, {"score": 0.0028911669203979156, "phrase": "local_information_extraction"}, {"score": 0.002862648127577722, "phrase": "data_invariance"}, {"score": 0.002806449337145999, "phrase": "tlnn"}, {"score": 0.0026839922027139967, "phrase": "different_directions"}, {"score": 0.002618278365749875, "phrase": "tlnn_algorithm"}, {"score": 0.0025668646315794947, "phrase": "excessive_dependence"}, {"score": 0.0025289662786567896, "phrase": "statistical_method"}, {"score": 0.0024916260764216752, "phrase": "prior_knowledge"}, {"score": 0.002454835845644037, "phrase": "training_data"}, {"score": 0.0024306105794630246, "phrase": "even_the_linear_combination"}, {"score": 0.0023476849595393872, "phrase": "weak_learner"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Classification", " Nearest neighbors", " Metric learning", " Mean-absolute error", " AdaBoost"], "paper_abstract": "When there exist an infinite number of samples in the training set, the outcome from nearest neighbor classification (kNN) is independent on its adopted distance metric. However, it is impossible that the number of training samples is infinite. Therefore, selecting distance metric becomes crucial in determining the performance of kNN. We propose a novel two-level nearest neighbor algorithm (TLNN) in order to minimize the mean-absolute error of the misclassification rate of kNN with finite and infinite number of training samples. At the low-level, we use Euclidean distance to determine a local subspace centered at an unlabeled test sample. At the high-level, AdaBoost is used as guidance for local information extraction. Data invariance is maintained by TLNN and the highly stretched or elongated neighborhoods along different directions are produced. The TLNN algorithm can reduce the excessive dependence on the statistical method which learns prior knowledge from the training data. Even the linear combination of a few base classifiers produced by the weak learner in AdaBoost can yield much better kNN classifiers. The experiments on both synthetic and real world data sets provide justifications for our proposed method. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A novel two-level nearest neighbor classification algorithm using an adaptive distance metric", "paper_id": "WOS:000299979400012"}