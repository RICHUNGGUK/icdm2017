{"auto_keywords": [{"score": 0.04497393203275299, "phrase": "large_number"}, {"score": 0.014950194659568004, "phrase": "object_classifiers"}, {"score": 0.00481495049065317, "phrase": "automatic_object_detection_tasks"}, {"score": 0.004728499903204582, "phrase": "training_images"}, {"score": 0.004413846562278824, "phrase": "hiring_professionals"}, {"score": 0.004366106816225457, "phrase": "large-scale_training_images"}, {"score": 0.004272164161788242, "phrase": "object_classes"}, {"score": 0.0041200448395938526, "phrase": "large_enough_amount"}, {"score": 0.004075469816959996, "phrase": "labeled_training_images"}, {"score": 0.0038877961007492646, "phrase": "image_labeling"}, {"score": 0.003681923235979914, "phrase": "image_level"}, {"score": 0.0036157418067192136, "phrase": "object_level"}, {"score": 0.003461702544605219, "phrase": "exact_object_locations"}, {"score": 0.003338344648070587, "phrase": "large-scale_collaboratively-tagged_images"}, {"score": 0.003172969962401266, "phrase": "new_machine"}, {"score": 0.0030267296900520217, "phrase": "loosely-tagged_images"}, {"score": 0.002627117999049202, "phrase": "interobject_correlations"}, {"score": 0.002598652897946253, "phrase": "loosely-labeled_images"}, {"score": 0.002579847272930948, "phrase": "object_classifier_training"}, {"score": 0.0025334249463305875, "phrase": "multi-task_learning"}, {"score": 0.002355920629285463, "phrase": "inter-related_object_classifiers"}, {"score": 0.0023219374444738723, "phrase": "object_network"}, {"score": 0.0022718774783143203, "phrase": "inter-related_learning_tasks"}, {"score": 0.002239103860494545, "phrase": "feature_space"}, {"score": 0.0021987994275190314, "phrase": "label_space"}, {"score": 0.002120349374832685, "phrase": "higher_detection_accuracy_rates"}, {"score": 0.0021049977753042253, "phrase": "automatic_object_detection"}], "paper_keywords": ["Object network", " Loosely tagged images", " Multi-task learning", " Multi-label learning", " Multiple instance learning"], "paper_abstract": "For automatic object detection tasks, large amounts of training images are usually labeled to achieve more reliable training of the object classifiers; this is cost-expensive since it requires hiring professionals to label large-scale training images. When a large number of object classes come into view, the issue of obtaining a large enough amount of the labeled training images becomes more critical. There are three potential solutions to reduce the burden for image labeling: (1) allowing people to provide the object labels loosely at the image level rather than at the object level (e. g., loosely-tagged images without identifying the exact object locations in the images); (2) harnessing large-scale collaboratively-tagged images that are available on the Internet; and, (3) developing new machine learning algorithms that can directly leverage large-scale collaboratively-or loosely-tagged images for achieving more effective training of a large number of object classifiers. Based on these observations, a multi-task multi-label multiple instance learning (MTML-MIL) algorithm is developed in this paper by leveraging both interobject correlations and large-scale loosely-labeled images for object classifier training. By seamlessly integrating multi-task learning, multi-label learning, and multiple instance learning, our MTML-MIL algorithm can achieve more accurate training of a large number of inter-related object classifiers (where an object network is constructed for determining the inter-related learning tasks directly in the feature space rather than in the label space). Our experimental results have shown that our MTML-MIL algorithm can achieve higher detection accuracy rates for automatic object detection.", "paper_title": "Multi-task multi-label multiple instance learning", "paper_id": "WOS:000285586900005"}