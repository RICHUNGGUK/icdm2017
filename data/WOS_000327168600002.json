{"auto_keywords": [{"score": 0.049720747269749836, "phrase": "matrix-vector_multiplication"}, {"score": 0.011455912535719289, "phrase": "assigned_threads"}, {"score": 0.008986602823557677, "phrase": "different_shapes"}, {"score": 0.00481495049065317, "phrase": "thread_assignment"}, {"score": 0.004707672504464783, "phrase": "modern_gpus"}, {"score": 0.004534136929881113, "phrase": "scientific_and_engineering_computations"}, {"score": 0.004449772140968921, "phrase": "highly_parallel_computing_environment"}, {"score": 0.004366970206156021, "phrase": "computing_cores"}, {"score": 0.004269630481719451, "phrase": "numerous_data_parallel_arithmetic_computations"}, {"score": 0.00396050388346298, "phrase": "diverse_set"}, {"score": 0.0036324746826127997, "phrase": "novel_auto-tuning_method"}, {"score": 0.003382017342396959, "phrase": "result_vector"}, {"score": 0.0031965070898750912, "phrase": "nvidia's_gpu_gtx"}, {"score": 0.003021141590548526, "phrase": "optimal_number"}, {"score": 0.002920561105784389, "phrase": "auto-tuner's_result"}, {"score": 0.0028661330049356186, "phrase": "versatile_generic_matrix-vector_multiplication_kernel"}, {"score": 0.0028339894231699603, "phrase": "cuda"}, {"score": 0.002550546722948396, "phrase": "cublas"}, {"score": 0.0024842242921857705, "phrase": "warp_method"}, {"score": 0.002347843727281585, "phrase": "optimal_behavior"}, {"score": 0.002161216637621284, "phrase": "significant_improvement"}, {"score": 0.0021049977753042253, "phrase": "unstable_performance_behavior"}], "paper_keywords": ["GPU", " matrix-vector multiplication", " performance tuning", " dense linear algebra"], "paper_abstract": "Modern GPUs have evolved to become a more general processor capable of executing scientific and engineering computations. It provides a highly parallel computing environment due to its large number of computing cores, which are suitable for numerous data parallel arithmetic computations, particularly linear algebra operations. The matrix-vector multiplication is one of the most important dense linear algebraic operations. It is applied to a diverse set of applications in many fields and must therefore be fully optimized to achieve a high-performance. In this paper, we proposed a novel auto-tuning method for matrix-vector multiplication on GPUs, where the number of assigned threads that are used to compute one element of the result vector can be auto-tuned according to the size of matrix. On the Nvidia's GPU GTX 650 with the most recent Kepler architecture, we developed an auto-tuner that can automatically select the optimal number of assigned threads for calculation. Based on the auto-tuner's result, we developed a versatile generic matrix-vector multiplication kernel with the CUDA programming model. A series of experiments on different shapes and sizes of matrices were conducted for comparing the performance of our kernel with that of the kernels from CUBLAS 5.0, MAGMA 1.3 and a warp method. The experiments results show that the performance of our matrix-vector multiplication kernel is close to the optimal behavior with increasing of the size of the matrix and has very little dependency on the shape of the matrix, which is a significant improvement compared to the other three kernels that exhibit unstable performance behavior for different shapes of matrices.", "paper_title": "Auto-Tuning of Thread Assignment for Matrix-Vector Multiplication on GPUs", "paper_id": "WOS:000327168600002"}