{"auto_keywords": [{"score": 0.02759768613797393, "phrase": "proposed_methods"}, {"score": 0.01341778752363927, "phrase": "target_domain"}, {"score": 0.013241715885966576, "phrase": "dal"}, {"score": 0.008812149597060386, "phrase": "scatter_information"}, {"score": 0.00481495049065317, "phrase": "minimum_distribution_discrepancy_support_vector_machine"}, {"score": 0.004735414083933722, "phrase": "domain_adaptation"}, {"score": 0.004610866942420572, "phrase": "effective_technique"}, {"score": 0.004565007074039171, "phrase": "pattern_classification_problems"}, {"score": 0.0045196012587080114, "phrase": "prior_information"}, {"score": 0.0042281749832498, "phrase": "training_data"}, {"score": 0.004186105377259855, "phrase": "source_domain"}, {"score": 0.004144452614362445, "phrase": "testing_data"}, {"score": 0.0038513524381387364, "phrase": "target_domains"}, {"score": 0.003675681237571904, "phrase": "generalized_projected_maximum_distribution_discrepancy"}, {"score": 0.0035669891404642015, "phrase": "kernel_hilbert_space"}, {"score": 0.0035197152758635344, "phrase": "based_domain_distributions"}, {"score": 0.003438483411236772, "phrase": "projected_maximum_distribution_scatter_discrepancy"}, {"score": 0.0032489004068215407, "phrase": "gpmdd_minimization_principle"}, {"score": 0.003195149953590668, "phrase": "novel_domain_adaptation_kernelized_support_vector_machine"}, {"score": 0.0031109870010753663, "phrase": "classical_svm"}, {"score": 0.0030595112110658675, "phrase": "ls-daksvm"}, {"score": 0.0029689853349030007, "phrase": "least-square_svm"}, {"score": 0.002824005136794786, "phrase": "proposed_gpmdd_metric"}, {"score": 0.0027312956497456374, "phrase": "rkhs_embedding_domain_distributions"}, {"score": 0.002421975107313603, "phrase": "kernel_bandwidth"}, {"score": 0.002397833600728684, "phrase": "better_the_convergence"}, {"score": 0.002381872778203302, "phrase": "gpmdd_metric_minimization"}, {"score": 0.0023190823948528953, "phrase": "generalization_capability"}, {"score": 0.0022806798371116698, "phrase": "dal_experimental_results"}, {"score": 0.002265497060260791, "phrase": "artificial_and_real-world_problems"}, {"score": 0.0021476098263617954, "phrase": "existing_benchmarking_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Domain adaptation learning", " Support vector machine", " Pattern classification", " Maximum mean discrepancy", " Maximum scatter discrepancy"], "paper_abstract": "Domain adaptation learning (DAL) is a novel and effective technique to address pattern classification problems where the prior information for training is unavailable or insufficient. Its effectiveness depends on the discrepancy between the two distributions that respectively generate the training data for the source domain and the testing data for the target domain. However, DAL may not work so well when only the distribution mean discrepancy between source and target domains is considered and minimized. In this paper, we first construct a generalized projected maximum distribution discrepancy (GPMDD) metric for DAL on reproducing kernel Hilbert space (RKHS) based domain distributions by simultaneously considering both the projected maximum distribution mean and the projected maximum distribution scatter discrepancy between the source and the target domain. In the sequel, based on both the structure risk and the GPMDD minimization principle, we propose a novel domain adaptation kernelized support vector machine (DAKSVM) with respect to the classical SVM, and its two extensions called LS-DAKSVM and mu-DAKSVM with respect to the least-square SVM and the nu-SVM, respectively. Moreover, our theoretical analysis justified that the proposed GPMDD metric could effectively measure the consistency not only between the RKHS embedding domain distributions but also between the scatter information of source and target domains. Hence, the proposed methods are distinctive in that the more consistency between the scatter information of source and target domains can be achieved by tuning the kernel bandwidth, the better the convergence of GPMDD metric minimization is and thus improving the scalability and generalization capability of the proposed methods for DAL Experimental results on artificial and real-world problems indicate that the performance of the proposed methods is superior to or at least comparable with existing benchmarking methods. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "On minimum distribution discrepancy support vector machine for domain adaptation", "paper_id": "WOS:000306584200010"}