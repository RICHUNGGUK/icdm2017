{"auto_keywords": [{"score": 0.03150716787411208, "phrase": "mpi"}, {"score": 0.027870520449524507, "phrase": "mpi_stack"}, {"score": 0.00481495049065317, "phrase": "non-data-communication_overheads"}, {"score": 0.004461051281091779, "phrase": "exponential_increase"}, {"score": 0.004421156723819163, "phrase": "power_consumption"}, {"score": 0.00438161736774803, "phrase": "heat_dissipation"}, {"score": 0.004342430078306461, "phrase": "modern_high-end_computing_systems"}, {"score": 0.0041703614123283165, "phrase": "single_processing_units"}, {"score": 0.004059438255358923, "phrase": "high_performance"}, {"score": 0.003933736547198066, "phrase": "massive_number"}, {"score": 0.0037104876694493815, "phrase": "end-host_pre-"}, {"score": 0.003677280504320639, "phrase": "post-communication_processing"}, {"score": 0.003628024127607343, "phrase": "communication_stacks"}, {"score": 0.0033914308417007316, "phrase": "small_amounts"}, {"score": 0.0033160345607565565, "phrase": "communication_stack"}, {"score": 0.0031702173541386888, "phrase": "massively_parallel_systems"}, {"score": 0.0030171910207671205, "phrase": "different_non-data-communication_overheads"}, {"score": 0.00277001889871322, "phrase": "mpi_stack_overhead"}, {"score": 0.0026009105281485888, "phrase": "multi-request_operations"}, {"score": 0.0023451553255837317, "phrase": "total_system_size"}, {"score": 0.002124036931047162, "phrase": "substantial_impact"}], "paper_keywords": ["Blue Gene/P", " MPI", " non-data-communication overheads"], "paper_abstract": "With processor speeds no longer doubling every 18-24 months owing to the exponential increase in power consumption and heat dissipation, modern high-end computing systems tend to rely less on the performance of single processing units and instead rely on achieving high performance by using the parallelism of a massive number of low-frequency/low-power processing cores. Using such low-frequency cores, however, puts a premium on end-host pre- and post-communication processing required within communication stacks, such as the Message Passing Interface (MPI) implementation. Similarly, small amounts of serialization within the communication stack that were acceptable on small/medium systems can be brutal on massively parallel systems. Thus, in this paper, we study the different non-data-communication overheads within the MPI implementation on the IBM Blue Gene/P system. Specifically, we analyze various aspects of MPI, including the MPI stack overhead itself, overhead of allocating and queueing requests, queue searches within the MPI stack, multi-request operations, and various others. Our experiments, that scale up to 131,072 cores of the largest Blue Gene/P system in the world (80% of the total system size), reveal several insights into overheads in the MPI stack, which were not previously considered significant, but can have a substantial impact on such massive systems.", "paper_title": "THE IMPORTANCE OF NON-DATA-COMMUNICATION OVERHEADS IN MPI", "paper_id": "WOS:000274898000002"}