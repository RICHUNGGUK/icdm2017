{"auto_keywords": [{"score": 0.0320237018545879, "phrase": "multiclass_and_multilabel_settings"}, {"score": 0.00481495049065317, "phrase": "quasi-newton_approach"}, {"score": 0.004763895422211728, "phrase": "nonsmooth_convex_optimization_problems"}, {"score": 0.004713379149462482, "phrase": "machine_learning"}, {"score": 0.00458940888106061, "phrase": "well-known_bfgs_quasi-newton_method"}, {"score": 0.004304963945457799, "phrase": "nonsmooth_convex_objectives"}, {"score": 0.004147216579100177, "phrase": "rigorous_fashion"}, {"score": 0.004038097986290412, "phrase": "bfgs"}, {"score": 0.003767517649628532, "phrase": "descent_direction"}, {"score": 0.003687990173655665, "phrase": "wolfe"}, {"score": 0.0034963129868796033, "phrase": "technical_conditions"}, {"score": 0.0034407781868385423, "phrase": "resulting_subbfgs_algorithm"}, {"score": 0.00335016692413066, "phrase": "objective_function_value"}, {"score": 0.0031422859551159506, "phrase": "binary_hinge_loss"}, {"score": 0.0026914710445743693, "phrase": "recently_developed_bundle_method"}, {"score": 0.00256514094268541, "phrase": "direction-finding_component"}, {"score": 0.002470987929071079, "phrase": "logistic_loss"}, {"score": 0.0021620042653528846, "phrase": "publicly_available_data_sets"}, {"score": 0.0021276181317652163, "phrase": "open_source_implementation"}], "paper_keywords": ["BFGS", " variable metric methods", " Wolfe conditions", " subgradient", " risk minimization", " hinge loss", " multiclass", " multilabel", " bundle methods", " BMRM", " OCAS", " OWL-QN"], "paper_abstract": "We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L(2)-regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efficient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-finding component of our algorithm to L1-regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available.", "paper_title": "A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning", "paper_id": "WOS:000277186600006"}