{"auto_keywords": [{"score": 0.03947447987613366, "phrase": "proposed_algorithm"}, {"score": 0.00481495049065317, "phrase": "generalized_cross-validation_algorithm"}, {"score": 0.003948613642680196, "phrase": "linear_regression_models"}, {"score": 0.0035755653799043, "phrase": "sparse_model"}, {"score": 0.0034024277222012597, "phrase": "multiple_smoothing_parameters"}, {"score": 0.0032376465741070274, "phrase": "generalized_cross-validation_approach"}, {"score": 0.0029805015010906013, "phrase": "synthetic_and_real-world_data_sets"}, {"score": 0.0024842242921857705, "phrase": "faul's_fast_relevance_vector_machine"}, {"score": 0.002403478088094395, "phrase": "chen"}, {"score": 0.0022122603123901114, "phrase": "orthogonal_least_squares"}, {"score": 0.0021050084340097234, "phrase": "orr"}], "paper_keywords": [""], "paper_abstract": "We propose a fast, incremental algorithm for designing linear regression models. The proposed algorithm generates a sparse model by optimizing multiple smoothing parameters using the generalized cross-validation approach. The performances on synthetic and real-world data sets are compared with other incremental algorithms such as Tipping and Faul's fast relevance vector machine, Chen et al.'s orthogonal least squares, and Orr's regularized forward selection. The results demonstrate that the proposed algorithm is competitive.", "paper_title": "Fast generalized cross-validation algorithm for sparse model learning", "paper_id": "WOS:000242803200011"}