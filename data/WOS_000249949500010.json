{"auto_keywords": [{"score": 0.027464995397435767, "phrase": "lmrl"}, {"score": 0.00481495049065317, "phrase": "multi-agent_reinforcement_learning_technologies"}, {"score": 0.0044873778175404475, "phrase": "game_theory"}, {"score": 0.00429057323623293, "phrase": "cooperative_multi-agent_systems"}, {"score": 0.004076157223660074, "phrase": "coordinated_multi-agent_systems"}, {"score": 0.0038476708316022823, "phrase": "credit_assignment"}, {"score": 0.0037743802306714545, "phrase": "multiple_nash_equilibriums"}, {"score": 0.0034725269588525534, "phrase": "new_multi-agent_reinforcement_learning_model"}, {"score": 0.00334144503360907, "phrase": "layer_perspective"}, {"score": 0.003298856341173034, "phrase": "lmrl_model"}, {"score": 0.003194736764837261, "phrase": "off-line_training_layer"}, {"score": 0.003113804826864362, "phrase": "single_agent_reinforcement_learning_technology"}, {"score": 0.003054450075234415, "phrase": "stationary_strategy_knowledge"}, {"score": 0.00299622333197717, "phrase": "online_interaction_layer"}, {"score": 0.0029203055813347874, "phrase": "multi-agent_reinforcement_learning_technology"}, {"score": 0.0024714996534331668, "phrase": "coordination_ability"}, {"score": 0.0022302306304023602, "phrase": "single_agent_reinforcement_learning"}, {"score": 0.00220177373403577, "phrase": "nash-q."}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["reinforcement learning", " multi-agent", " layered model"], "paper_abstract": "Multi-agent reinforcement learning technologies are mainly investigated from two perspectives of the concurrence and the game theory. The former chiefly applies to cooperative multi-agent systems, while the latter usually applies to coordinated multi-agent systems. However, there exist such problems as the credit assignment and the multiple Nash equilibriums for agents with them. In this paper, we propose a new multi-agent reinforcement learning model and algorithm LMRL from a layer perspective. LMRL model is composed of an off-line training layer that employs a single agent reinforcement learning technology to acquire stationary strategy knowledge and an online interaction layer that employs a multi-agent reinforcement learning technology and the strategy knowledge that can be revised dynamically to interact with the environment. An agent with LMRL can improve its generalization capability, adaptability and coordination ability. Experiments show that the performance of LMRL can be better than those of a single agent reinforcement learning and Nash-Q. (c) 2006 Published by Elsevier Ltd.", "paper_title": "A two-layered multi-agent reinforcement learning model and algorithm", "paper_id": "WOS:000249949500010"}