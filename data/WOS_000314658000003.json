{"auto_keywords": [{"score": 0.043328886442782905, "phrase": "ssmdr"}, {"score": 0.00481495049065317, "phrase": "semisupervised_multimodal_dimensionality_reduction."}, {"score": 0.004601508848655033, "phrase": "unlabeled_data"}, {"score": 0.0043533862892678864, "phrase": "novel_semisupervised_multimodal_dimensionality_reduction"}, {"score": 0.004223708778943785, "phrase": "feature_reduction"}, {"score": 0.004056769841308405, "phrase": "local_and_multimodal_structures"}, {"score": 0.004016072233242585, "phrase": "labeled_and_unlabeled_samples"}, {"score": 0.0038964032225585117, "phrase": "data_pairs"}, {"score": 0.0038379074449947067, "phrase": "close_vicinity"}, {"score": 0.003780286517897356, "phrase": "original_space"}, {"score": 0.00363080961099728, "phrase": "embedding_space"}, {"score": 0.0035048567587934254, "phrase": "dimensionality_reduction_methods"}, {"score": 0.003282383509542342, "phrase": "unlabeled_samples"}, {"score": 0.003233075944817365, "phrase": "significant_role"}, {"score": 0.0031684791285366315, "phrase": "learning_performance"}, {"score": 0.0031208772639433145, "phrase": "proposed_discriminant_technique"}, {"score": 0.003073988343893906, "phrase": "analytical_form"}, {"score": 0.0030278017562213265, "phrase": "embedding_transformations"}, {"score": 0.0028933521924166287, "phrase": "eigen_decomposition"}, {"score": 0.0027929100418926725, "phrase": "basis_vectors"}, {"score": 0.002723301730223192, "phrase": "standard_kernel_trick"}, {"score": 0.0026155094974689595, "phrase": "nonlinear_dimensionality_reduction_scenarios"}, {"score": 0.0024617483174454113, "phrase": "extensive_simulations"}, {"score": 0.002437012847684331, "phrase": "data_visualization"}, {"score": 0.0023762535034673017, "phrase": "synthetic_and_real-world_datasets"}, {"score": 0.0022706700510423954, "phrase": "significant_advantages"}, {"score": 0.0022365262201954643, "phrase": "widely_used_techniques"}, {"score": 0.0021263705039915198, "phrase": "superior_performance"}, {"score": 0.0021049977753042253, "phrase": "multimodal_cases"}], "paper_keywords": ["semisupervised learning", " dimensionality reduction", " locality preservation", " multivariate visualization", " multimodality preservation", " classification"], "paper_abstract": "The problem of learning from both labeled and unlabeled data is considered. In this paper, we present a novel semisupervised multimodal dimensionality reduction (SSMDR) algorithm for feature reduction and extraction. SSMDR can preserve the local and multimodal structures of labeled and unlabeled samples. As a result, data pairs in the close vicinity of the original space are projected in the nearby of the embedding space. Due to overfitting, supervised dimensionality reduction methods tend to perform inefficiently when only few labeled samples are available. In such cases, unlabeled samples play a significant role in boosting the learning performance. The proposed discriminant technique has an analytical form of the embedding transformations that can be effectively obtained by applying the eigen decomposition, or finding two close optimal sets of transforming basis vectors. By employing the standard kernel trick, SSMDR can be extended to the nonlinear dimensionality reduction scenarios. We verify the feasibility and effectiveness of SSMDR through conducting extensive simulations including data visualization and classification on the synthetic and real-world datasets. Our obtained results reveal that SSMDR offers significant advantages over some widely used techniques. Compared with other methods, the proposed SSMDR exhibits superior performance on multimodal cases.", "paper_title": "SEMISUPERVISED MULTIMODAL DIMENSIONALITY REDUCTION", "paper_id": "WOS:000314658000003"}