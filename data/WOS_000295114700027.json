{"auto_keywords": [{"score": 0.048004834084269796, "phrase": "qsar_models"}, {"score": 0.037774479473222676, "phrase": "rank_order"}, {"score": 0.013986395475240136, "phrase": "qsar_predictions"}, {"score": 0.012979098687838758, "phrase": "information_content"}, {"score": 0.010435409021063343, "phrase": "modeling_methods"}, {"score": 0.007907029018008039, "phrase": "traditional_metrics"}, {"score": 0.00481495049065317, "phrase": "rank_order_entropy"}, {"score": 0.004728907889847655, "phrase": "quantitative_structure_activity_relationship_models"}, {"score": 0.004690305164360866, "phrase": "drug_discovery"}, {"score": 0.004667294241376573, "phrase": "mixed_history"}, {"score": 0.004428728205625129, "phrase": "model_performance_metrics"}, {"score": 0.004314037730743926, "phrase": "user_confidence"}, {"score": 0.0042508289538799905, "phrase": "typical_workflow_scenario"}, {"score": 0.004195418010131832, "phrase": "training_sets"}, {"score": 0.004133940019630129, "phrase": "many-fold_cross-validation_methods"}, {"score": 0.003916098498242888, "phrase": "main_purpose"}, {"score": 0.0039032723121900387, "phrase": "qsar"}, {"score": 0.003820896867623235, "phrase": "untested_set"}, {"score": 0.0037218885130614796, "phrase": "user_expectations"}, {"score": 0.003709693561373117, "phrase": "model_performance"}, {"score": 0.0036733470451885465, "phrase": "numerical_value"}, {"score": 0.0036553070575015344, "phrase": "molecular_prediction"}, {"score": 0.003613556291453534, "phrase": "end_user"}, {"score": 0.003479692058963528, "phrase": "predicted_rank_order"}, {"score": 0.003462599815177726, "phrase": "important_component"}, {"score": 0.0034512514926836667, "phrase": "predictive_qsar."}, {"score": 0.003372844338742438, "phrase": "rank_order_prediction"}, {"score": 0.003307052802564077, "phrase": "model_stability"}, {"score": 0.003296212583539166, "phrase": "high_priority"}, {"score": 0.0032266093099550955, "phrase": "qsar_rank_order_models"}, {"score": 0.0032107562030060563, "phrase": "representative_data_sets"}, {"score": 0.003200230621987751, "phrase": "descriptor_sets"}, {"score": 0.0031532895341013297, "phrase": "kendall_tau"}, {"score": 0.0031377955308198634, "phrase": "rank_order_metric"}, {"score": 0.0031121406603482112, "phrase": "shannon_entropy"}, {"score": 0.0030715271045670504, "phrase": "rank-order_stability"}, {"score": 0.0030314419455512013, "phrase": "training_set"}, {"score": 0.003006654007926993, "phrase": "data_truncation_analysis"}, {"score": 0.002890456194757947, "phrase": "order_stability"}, {"score": 0.002866817775018445, "phrase": "training_set_data_loss"}, {"score": 0.002843372121209168, "phrase": "dta_roe_model_evaluation"}, {"score": 0.0028016507334576216, "phrase": "incremental_loss"}, {"score": 0.0027924625350414655, "phrase": "training_information"}, {"score": 0.0026933542552712033, "phrase": "particular_domain"}, {"score": 0.0026234862160314397, "phrase": "roe."}, {"score": 0.00260630351276416, "phrase": "information_theory"}, {"score": 0.0025934901874706787, "phrase": "unstable_rank_order_model"}, {"score": 0.0025807396931129926, "phrase": "high_level"}, {"score": 0.0025722741181993278, "phrase": "implicit_entropy"}, {"score": 0.0025554259726979786, "phrase": "qsar_rank_order_model"}, {"score": 0.002530359917257626, "phrase": "training_set_reductions"}, {"score": 0.0025179190096139277, "phrase": "low_entropy"}, {"score": 0.0024891269874339553, "phrase": "roe_metric"}, {"score": 0.002464709648714996, "phrase": "different_sizes"}, {"score": 0.00225914082381728, "phrase": "roe_metrics"}, {"score": 0.0022113879114507577, "phrase": "roe_evaluation"}, {"score": 0.0021539955106204354, "phrase": "usable_models"}, {"score": 0.002146926767291475, "phrase": "prioritization_schemes"}, {"score": 0.0021154027872650727, "phrase": "particular_model"}, {"score": 0.0021049977753042253, "phrase": "specific_domain"}], "paper_keywords": [""], "paper_abstract": "The use of Quantitative Structure Activity Relationship models to address problems in drug discovery has a mixed history, generally resulting from the misapplication of QSAR models that were either poorly constructed or used outside of their domains of applicability. This situation has motivated the development of a variety of model performance metrics (r(2), PRESS r(2), F-tests, etc.) designed to increase user confidence in the validity of QSAR predictions. In a typical workflow scenario, QSAR models are created and validated on training sets of molecules using metrics such as Leave-One-Out or many-fold cross-validation methods that attempt to assess their internal consistency. However, few current validation methods are designed to directly address the stability of QSAR predictions in response to changes in the information content of the training set. Since the main purpose of QSAR is to quickly and accurately estimate a property of interest for an untested set of molecules, it makes sense to have a means at hand to correctly set user expectations of model performance. In fact, the numerical value of a molecular prediction is often less important to the end user than knowing the rank order of that set of molecules according to their predicted end point values. Consequently, a means for characterizing the stability of predicted rank order is an important component of predictive QSAR. Unfortunately, none of the many validation metrics currently available directly measure the stability of rank order prediction, making the development of an additional metric that can quantify model stability a high priority. To address this need, this work examines the stabilities of QSAR rank order models created from representative data sets, descriptor sets, and modeling methods that were then assessed using Kendall Tau as a rank order metric, upon which the Shannon entropy was evaluated as a means of quantifying rank-order stability. Random removal of data from the training set, also known as Data Truncation Analysis (DTA), was used as a means for systematically reducing the information content of each training set while examining both rank order performance and rank order stability in the face of training set data loss. The premise for DTA ROE model evaluation is that the response of a model to incremental loss of training information will be indicative of the quality and sufficiency of its training set, learning method, and descriptor types to cover a particular domain of applicability. This process is termed a \"rank order entropy\" evaluation or ROE. By analogy with information theory, an unstable rank order model displays a high level of implicit entropy, while a QSAR rank order model which remains nearly unchanged during training set reductions would show low entropy. In this work, the ROE metric was applied to 71 data sets of different sizes and was found to reveal more information about the behavior of the models than traditional metrics alone. Stable, or consistently performing models, did not necessarily predict rank order well. Models that performed well in rank order did not necessarily perform well in traditional metrics. In the end, it was shown that ROE metrics suggested that some QSAR models that are typically used should be discarded. ROE evaluation helps to discern which combinations of data set, descriptor set, and modeling methods lead to usable models in prioritization schemes and provides confidence in the use of a particular model within a specific domain of applicability.", "paper_title": "Rank Order Entropy: Why One Metric Is Not Enough", "paper_id": "WOS:000295114700027"}