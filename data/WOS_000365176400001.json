{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "coactive_learning"}, {"score": 0.03356912892792283, "phrase": "web_search"}, {"score": 0.004573842508130535, "phrase": "learning_system"}, {"score": 0.004515467468684217, "phrase": "human_user"}, {"score": 0.00440093312826378, "phrase": "common_goal"}, {"score": 0.004307700591183651, "phrase": "maximum_utility"}, {"score": 0.003937126962709418, "phrase": "search_engine"}, {"score": 0.0034621581245372138, "phrase": "slightly_improved_but_not_necessarily_optimal_object"}, {"score": 0.003288567185099276, "phrase": "large_quantity"}, {"score": 0.0032604908177429493, "phrase": "observable_user_behavior"}, {"score": 0.0031102878400698143, "phrase": "optimal_feedback"}, {"score": 0.0030573959252626695, "phrase": "expert_model"}, {"score": 0.0030183163057929687, "phrase": "cardinal_valuations"}, {"score": 0.0029797347088361056, "phrase": "bandit_learning"}, {"score": 0.002929056388593134, "phrase": "relaxed_requirements"}, {"score": 0.0026882700311968025, "phrase": "coactive_framework"}, {"score": 0.00249919111154604, "phrase": "cardinal_utility"}, {"score": 0.0024461474613359994, "phrase": "learning_algorithm"}, {"score": 0.002414861891567233, "phrase": "cardinal_utility_values"}, {"score": 0.002244969789802093, "phrase": "lambda-strongly_convex_loss_functions"}, {"score": 0.0021049977753042253, "phrase": "movie_recommendation_task"}], "paper_keywords": [""], "paper_abstract": "We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. Interactions in the Coactive Learning model take the following form: at each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking); the user responds by correcting the system if necessary, providing a slightly improved but not necessarily optimal object as feedback. We argue that such preference feedback can be inferred in large quantity from observable user behavior (e.g., clicks in web search), unlike the optimal feedback required in the expert model or the cardinal valuations required for bandit learning. Despite the relaxed requirements for the feedback, we show that it is possible to adapt many existing online learning algorithms to the coactive framework. In particular, we provide algorithms that achieve O(1/root T) average regret in terms of cardinal utility, even though the learning algorithm never observes cardinal utility values directly. We also provide an algorithm with O(log(1/root T) average regret in the case of lambda-strongly convex loss functions. An extensive empirical study demonstrates the applicability of our model and algorithms on a movie recommendation task, as well as ranking for web search.", "paper_title": "Coactive Learning", "paper_id": "WOS:000365176400001"}