{"auto_keywords": [{"score": 0.03201297726420698, "phrase": "mtt_framework"}, {"score": 0.02651016717860217, "phrase": "mtt"}, {"score": 0.00481495049065317, "phrase": "structured_multi-task_sparse_learning"}, {"score": 0.004616034337089482, "phrase": "particle_filter_framework"}, {"score": 0.004567596860086148, "phrase": "structured_multi-task_sparse_learning_problem"}, {"score": 0.004472234553892357, "phrase": "structured_multi-task_tracking"}, {"score": 0.004287415750101536, "phrase": "linear_combinations"}, {"score": 0.004257360656055417, "phrase": "dictionary_templates"}, {"score": 0.003996133359116611, "phrase": "multi-task_tracking"}, {"score": 0.0038989120088094185, "phrase": "popular_sparsity-inducing_mixed_norms"}, {"score": 0.0038309141193444015, "phrase": "representation_problem"}, {"score": 0.0037906837773782, "phrase": "joint_sparsity"}, {"score": 0.003737697279708501, "phrase": "particle_representations"}, {"score": 0.003659597920882104, "phrase": "previous_methods"}, {"score": 0.0034713900276078786, "phrase": "tracking_performance"}, {"score": 0.003447035529485425, "phrase": "overall_computational_complexity"}, {"score": 0.0033513085425415545, "phrase": "popular_tracker"}, {"score": 0.00332780013094594, "phrase": "mei"}, {"score": 0.003304453127732878, "phrase": "ling"}, {"score": 0.0032812562034088646, "phrase": "ieee_trans_pattern_anal"}, {"score": 0.0031565959573536194, "phrase": "special_case"}, {"score": 0.0028201570908224166, "phrase": "common_relevant_covariates"}, {"score": 0.0026844452074327265, "phrase": "pairwise_structural_correlations"}, {"score": 0.002646880721119838, "phrase": "e.g._spatial_smoothness"}, {"score": 0.002582400033640049, "phrase": "novel_framework"}, {"score": 0.0025106241866273897, "phrase": "regularized_sparse_representation"}, {"score": 0.002398208426903865, "phrase": "accelerated_proximal_gradient"}, {"score": 0.0023151639346874883, "phrase": "closed_form"}, {"score": 0.002157583854640276, "phrase": "heavy_occlusion"}, {"score": 0.002142426911242168, "phrase": "drastic_illumination_changes"}, {"score": 0.0021198904828154657, "phrase": "large_pose_variations"}, {"score": 0.0021049977753042253, "phrase": "experimental_results"}], "paper_keywords": ["Visual tracking", " Particle filter", " Graph", " Structure", " Sparse representation", " Multi-task learning"], "paper_abstract": "In this paper, we formulate object tracking in a particle filter framework as a structured multi-task sparse learning problem, which we denote as Structured Multi-Task Tracking (S-MTT). Since we model particles as linear combinations of dictionary templates that are updated dynamically, learning the representation of each particle is considered a single task in Multi-Task Tracking (MTT). By employing popular sparsity-inducing mixed norms and we regularize the representation problem to enforce joint sparsity and learn the particle representations together. As compared to previous methods that handle particles independently, our results demonstrate that mining the interdependencies between particles improves tracking performance and overall computational complexity. Interestingly, we show that the popular tracker (Mei and Ling, IEEE Trans Pattern Anal Mach Intel 33(11):2259-2272, 2011) is a special case of our MTT formulation (denoted as the tracker) when Under the MTT framework, some of the tasks (particle representations) are often more closely related and more likely to share common relevant covariates than other tasks. Therefore, we extend the MTT framework to take into account pairwise structural correlations between particles (e.g. spatial smoothness of representation) and denote the novel framework as S-MTT. The problem of learning the regularized sparse representation in MTT and S-MTT can be solved efficiently using an Accelerated Proximal Gradient (APG) method that yields a sequence of closed form updates. As such, S-MTT and MTT are computationally attractive. We test our proposed approach on challenging sequences involving heavy occlusion, drastic illumination changes, and large pose variations. Experimental results show that S-MTT is much better than MTT, and both methods consistently outperform state-of-the-art trackers.", "paper_title": "Robust Visual Tracking via Structured Multi-Task Sparse Learning", "paper_id": "WOS:000314291600008"}