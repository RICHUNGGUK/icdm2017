{"auto_keywords": [{"score": 0.031212175139997925, "phrase": "unsupervised_adaptation"}, {"score": 0.011007332716897295, "phrase": "n-gram_models"}, {"score": 0.009847071012891869, "phrase": "supervised_adaptation"}, {"score": 0.009532071923775537, "phrase": "speech_recognition_task"}, {"score": 0.00481495049065317, "phrase": "stochastic_grammars"}, {"score": 0.004703878334423852, "phrase": "supervised_and_unsupervised_adaptation"}, {"score": 0.004573953147933065, "phrase": "n-gram_language_models"}, {"score": 0.00453144352814823, "phrase": "probabilistic_context-free_grammars"}, {"score": 0.004385733482617193, "phrase": "new_domain"}, {"score": 0.0042446888127531945, "phrase": "commonly_used_approaches"}, {"score": 0.004146716743362567, "phrase": "model_interpolation"}, {"score": 0.004108161406159407, "phrase": "special_cases"}, {"score": 0.0038301525703886585, "phrase": "alternate_adaptation_approaches"}, {"score": 0.003689628075766217, "phrase": "different_adaptation_strategies"}, {"score": 0.0034398445347109396, "phrase": "adaptation_process"}, {"score": 0.0028664985822266344, "phrase": "adaptation_sample"}, {"score": 0.002839812512030847, "phrase": "limited_size"}, {"score": 0.0024912073108138613, "phrase": "multiple_word_hypotheses"}, {"score": 0.002410942838102255, "phrase": "word_lattice"}, {"score": 0.0022161977759646693, "phrase": "word_lattices"}, {"score": 0.0021049977753042253, "phrase": "well-known_good-turing_estimate"}], "paper_keywords": [""], "paper_abstract": "This paper investigates supervised and unsupervised adaptation of stochastic grammars, including n-gram language models and probabilistic context-free grammars (PCFGs), to a new domain. It is shown that the commonly used approaches of count merging and model interpolation are special cases of a more general maximum a posteriori (MAP) framework, which additionally allows for alternate adaptation approaches. This paper investigates the effectiveness of different adaptation strategies, and, in particular, focuses on the need for supervision in the adaptation process. We show that n-gram models as well as PCFGs benefit from either supervised or unsupervised MAP adaptation in various tasks. For n-gram models, we compare the benefit from supervised adaptation with that of unsupervised adaptation on a speech recognition task with an adaptation sample of limited size (about 17 h), and show that unsupervised adaptation can obtain 51% of the 7.7% adaptation gain obtained by supervised adaptation. We also investigate the benefit of using multiple word hypotheses (in the form of a word lattice) for unsupervised adaptation on a speech recognition task for which there was a much larger adaptation sample available. The use of word lattices for adaptation required the derivation of a generalization of the well-known Good-Turing estimate.", "paper_title": "MAP adaptation of stochastic grammars", "paper_id": "WOS:000233499200003"}