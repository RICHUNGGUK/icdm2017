{"auto_keywords": [{"score": 0.04805212023869437, "phrase": "real_time"}, {"score": 0.044027087409931595, "phrase": "human_voice"}, {"score": 0.04088899068598468, "phrase": "human_emotion"}, {"score": 0.00481495049065317, "phrase": "non-verbal_voice_analysis_system"}, {"score": 0.004655742198075387, "phrase": "concurrent_emotions"}, {"score": 0.004567129988306349, "phrase": "potential_application"}, {"score": 0.0037864980290391354, "phrase": "fundamental_frequency"}, {"score": 0.0036788184374592706, "phrase": "continuous_natural_speech"}, {"score": 0.0035914198812657897, "phrase": "robust_fundamental_frequencies"}, {"score": 0.0031845067058084583, "phrase": "decision-tree_logic"}, {"score": 0.003138874040786261, "phrase": "emotional_elements"}, {"score": 0.0028100102841877835, "phrase": "third_parties"}, {"score": 0.0027564303939865476, "phrase": "system_performance"}, {"score": 0.002730023736069673, "phrase": "human_subjective_classification"}, {"score": 0.0026268961477241026, "phrase": "overall_matching_rate"}, {"score": 0.0025521085778978042, "phrase": "matching_rate"}, {"score": 0.002467536552997677, "phrase": "subjects'_assessment"}, {"score": 0.00236289633510808, "phrase": "non-verbal_information"}, {"score": 0.0022956075241520064, "phrase": "human_subjective_assessment"}, {"score": 0.0022409963215568565, "phrase": "first_report"}, {"score": 0.0021049977753042253, "phrase": "numerous_applications"}], "paper_keywords": ["comparison of subjective evaluation", " spontaneous emotional speech", " conscious emotional speech", " emotional parameter"], "paper_abstract": "A non-verbal voice analysis system that recognizes, separates and ranks concurrent emotions in real time has potential application in various fields, yet such a system that could delineate emotion based solely on the sound of a human voice has not been successfully demonstrated before. Here, we propose a system that recognizes human emotion by means of analyzing the fundamental frequency of a human voice taken from continuous natural speech. The system detects robust fundamental frequencies and intonations by parameterizing them into pitch, power, and deviation of power. Based on these parameters, data was classified via decision-tree logic into the emotional elements of anger, joy, sorrow, and calmness. Degree of excitement was also extracted. The system was evaluated by third parties by matching the system performance to human subjective classification for each element. Results indicate that overall matching rate was 70%, and the matching rate was 86% when compared to the subjects' assessment of their own voices. Our system performance exceeded the baseline with non-verbal information, which was equivalent to human subjective assessment. This is the first report of a system to rigorously analyze human emotion in real time and we expect numerous applications.", "paper_title": "Non-verbal voice emotion analysis system", "paper_id": "WOS:000242255500010"}