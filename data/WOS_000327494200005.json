{"auto_keywords": [{"score": 0.038053443739184435, "phrase": "pet_model"}, {"score": 0.015376319150867701, "phrase": "discrete_and_fuzzy_labels"}, {"score": 0.011582483690951587, "phrase": "fuzzy_labels"}, {"score": 0.01013847911314723, "phrase": "discrete_labels"}, {"score": 0.008524688313232753, "phrase": "first_model"}, {"score": 0.00819004333268332, "phrase": "second_model"}, {"score": 0.004815607935434171, "phrase": "bayesian"}, {"score": 0.004666756595735701, "phrase": "classical_decision_tree_model"}, {"score": 0.004563691742821759, "phrase": "classical_machine_learning_models"}, {"score": 0.004287008195370748, "phrase": "dt_model"}, {"score": 0.004248869786808024, "phrase": "probability_estimation_trees"}, {"score": 0.004136469845987887, "phrase": "better_estimation"}, {"score": 0.0040996651541091575, "phrase": "class_probability"}, {"score": 0.003973398317120165, "phrase": "good_probability_estimation"}, {"score": 0.0038855864345030563, "phrase": "large_trees"}, {"score": 0.0037490909410974166, "phrase": "model_transparency"}, {"score": 0.0037157201349604222, "phrase": "linguistic_decision_tree"}, {"score": 0.003682648277400094, "phrase": "ldt"}, {"score": 0.003569176320452192, "phrase": "label_semantics"}, {"score": 0.0033227310662785293, "phrase": "probability_distribution"}, {"score": 0.003177392642813126, "phrase": "neighboring_fuzzy_labels"}, {"score": 0.002984504445973107, "phrase": "special_case"}, {"score": 0.002815864555488112, "phrase": "naive_bayes_classifier"}, {"score": 0.0026686444508167875, "phrase": "good_performance"}, {"score": 0.002574788013427524, "phrase": "naive_bayes_estimation"}, {"score": 0.0024401411219526774, "phrase": "small-sized_pets"}, {"score": 0.002322895056238477, "phrase": "empirical_studies"}, {"score": 0.0022112700457629494, "phrase": "shallow_depth"}, {"score": 0.0021239335561699106, "phrase": "naive_bayes"}, {"score": 0.0021049982455464515, "phrase": "pet."}], "paper_keywords": ["fuzzy labels", " label semantics", " random set", " probability estimation tree", " mass assignment", " linguistic decision tree", " naive Bayes"], "paper_abstract": "Classical decision tree model is one of the classical machine learning models for its simplicity and effectiveness in applications. However, compared to the DT model, probability estimation trees (PETs) give a better estimation on class probability. In order to get a good probability estimation, we usually need large trees which are not desirable with respect to model transparency. Linguistic decision tree (LDT) is a PET model based on label semantics. Fuzzy labels are used for building the tree and each branch is associated with a probability distribution over classes. If there is no overlap between neighboring fuzzy labels, these fuzzy labels then become discrete labels and a LDT with discrete labels becomes a special case of the PET model. In this paper, two hybrid models by combining the naive Bayes classifier and PETs are proposed in order to build a model with good performance without losing too much transparency. The first model uses naive Bayes estimation given a PET, and the second model uses a set of small-sized PETs as estimators by assuming the independence between these trees. Empirical studies on discrete and fuzzy labels show that the first model outperforms the PET model at shallow depth, and the second model is equivalent to the naive Bayes and PET.", "paper_title": "Hybrid Bayesian estimation tree learning with discrete and fuzzy labels", "paper_id": "WOS:000327494200005"}