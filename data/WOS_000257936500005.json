{"auto_keywords": [{"score": 0.0496463517482477, "phrase": "software_defect_prediction"}, {"score": 0.012219592826017341, "phrase": "metric-based_classification"}, {"score": 0.00481495049065317, "phrase": "classification_models"}, {"score": 0.004497973025639345, "phrase": "software_quality"}, {"score": 0.004377077848734575, "phrase": "predictive_classification_models"}, {"score": 0.004240116703465252, "phrase": "timely_identification"}, {"score": 0.004201774694043468, "phrase": "fault-prone_modules"}, {"score": 0.003942877273973905, "phrase": "inconsistent_findings"}, {"score": 0.0034560481756572632, "phrase": "experimental_results"}, {"score": 0.003170060683860338, "phrase": "accuracy_indicators"}, {"score": 0.0029209233709517634, "phrase": "statistical_testing_procedures"}, {"score": 0.0028813415821988156, "phrase": "empirical_findings"}, {"score": 0.002753224894944965, "phrase": "comparative_software_defect_prediction_experiments"}, {"score": 0.00266693888510356, "phrase": "large-scale_empirical_comparison"}, {"score": 0.0025716240685409513, "phrase": "nasa_metrics_data_repository"}, {"score": 0.0025137856138000014, "phrase": "appealing_degree"}, {"score": 0.002491015208328868, "phrase": "predictive_accuracy"}, {"score": 0.0022537212268979507, "phrase": "particular_classification_algorithm"}, {"score": 0.0021632743048552536, "phrase": "significant_performance_differences"}], "paper_keywords": ["complexity measures", " data mining", " formal methods", " statistical methods", " software defect prediction"], "paper_abstract": "Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.", "paper_title": "Benchmarking classification models for software defect prediction: A proposed framework and novel findings", "paper_id": "WOS:000257936500005"}