{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "reinforcement_learning"}, {"score": 0.00471228691115201, "phrase": "empirical_study"}, {"score": 0.004230749292148046, "phrase": "treatment_policies"}, {"score": 0.0041703614123283165, "phrase": "chronic_illnesses"}, {"score": 0.004023120753104693, "phrase": "off-the-shelf_reinforcement_learning_methods"}, {"score": 0.003459191419478298, "phrase": "present_methods"}, {"score": 0.0032190636347671675, "phrase": "multiple_imputation_approach"}, {"score": 0.0030830390264125923, "phrase": "missing_data"}, {"score": 0.002889672003089062, "phrase": "function_approximation"}, {"score": 0.0027675275077806744, "phrase": "highly_variable_observation_set"}, {"score": 0.002448704249007665, "phrase": "particular_action"}, {"score": 0.0022785613865764923, "phrase": "recommended_policy"}, {"score": 0.0021049977753042253, "phrase": "real_clinical_trial_data"}], "paper_keywords": ["Optimal treatment policies", " Fitted Q-iteration", " Policy uncertainty"], "paper_abstract": "This paper highlights the role that reinforcement learning can play in the optimization of treatment policies for chronic illnesses. Before applying any off-the-shelf reinforcement learning methods in this setting, we must first tackle a number of challenges. We outline some of these challenges and present methods for overcoming them. First, we describe a multiple imputation approach to overcome the problem of missing data. Second, we discuss the use of function approximation in the context of a highly variable observation set. Finally, we discuss approaches to summarizing the evidence in the data for recommending a particular action and quantifying the uncertainty around the Q-function of the recommended policy. We present the results of applying these methods to real clinical trial data of patients with schizophrenia.", "paper_title": "Informing sequential clinical decision-making through reinforcement learning: an empirical study", "paper_id": "WOS:000291607100005"}