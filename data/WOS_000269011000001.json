{"auto_keywords": [{"score": 0.04069520186081181, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "semantically_supervised_region-based_retrieval"}, {"score": 0.00469159356595623, "phrase": "unified_annotation"}, {"score": 0.004651177631663439, "phrase": "retrieval_framework"}, {"score": 0.00457138247462329, "phrase": "region_annotation"}, {"score": 0.004531997388646422, "phrase": "image_retrieval"}, {"score": 0.004492950096237056, "phrase": "performance_reinforcement"}, {"score": 0.004415857432588371, "phrase": "semantic_annotation"}, {"score": 0.004377806391261418, "phrase": "region-based_image_retrieval"}, {"score": 0.004340081802944066, "phrase": "visual_and_textual_fusion"}, {"score": 0.004247180402119585, "phrase": "soft_matching"}, {"score": 0.004210576688291019, "phrase": "bayesian_probabilistic_formulations"}, {"score": 0.004138309011478741, "phrase": "sample_insufficiency"}, {"score": 0.004102639724186862, "phrase": "sample_asymmetry"}, {"score": 0.004049709080817396, "phrase": "annotation_classifier_training_phase"}, {"score": 0.003962998498942111, "phrase": "region-level_multi-label_image_annotation_scheme"}, {"score": 0.003911862375084159, "phrase": "pair-wise_coupling_support_vector_machine"}, {"score": 0.0037786899855446, "phrase": "retrieval_phase"}, {"score": 0.0037138072936226, "phrase": "semantic-level_region_matching"}, {"score": 0.0036500346000495317, "phrase": "novel_retrieval_scheme"}, {"score": 0.0035873530542209686, "phrase": "former_work"}, {"score": 0.0032471982781283344, "phrase": "user's_judgment"}, {"score": 0.003163878786694538, "phrase": "semantic_retrieval"}, {"score": 0.0031414917668306824, "phrase": "semantically"}, {"score": 0.0029264925857346497, "phrase": "integrated_region_matching"}, {"score": 0.0028390477456009568, "phrase": "keyword-integrated_soft_region"}, {"score": 0.0026259694526633037, "phrase": "keyword_integrated_bayesian_reasoning"}, {"score": 0.0025474814283429213, "phrase": "natural_integration"}, {"score": 0.0025145641652082164, "phrase": "visual_dictionary"}, {"score": 0.0024928553725344933, "phrase": "online_content-based_search"}, {"score": 0.002449997042686432, "phrase": "relevance_feedback_phase"}, {"score": 0.0023974563359295043, "phrase": "visual_and_textual_learning"}, {"score": 0.0023562342764987254, "phrase": "user's_retrieval_target"}, {"score": 0.0022758994740331258, "phrase": "current_methods"}, {"score": 0.0021982976170703884, "phrase": "flickr_web_image_database"}], "paper_keywords": ["Image annotation", " Image retrieval", " Relevance feedback", " Visual and textual fusion", " Multi-label annotation", " Pairwise coupling", " Bagging", " Bayesian formulation", " Soft region matching", " Visual dictionary", " Visual and textual learning"], "paper_abstract": "This paper presents a unified annotation and retrieval framework, which integrates region annotation with image retrieval for performance reinforcement. To integrate semantic annotation with region-based image retrieval, visual and textual fusion is proposed for both soft matching and Bayesian probabilistic formulations. To address sample insufficiency and sample asymmetry in the annotation classifier training phase, we present a region-level multi-label image annotation scheme based on pair-wise coupling support vector machine (SVM) learning. In the retrieval phase, to achieve semantic-level region matching we present a novel retrieval scheme which differs from former work: the query example uploaded by users is automatically annotated online, and the user can judge its annotation quality. Based on the user's judgment, two novel schemes are deployed for semantic retrieval: (1) if the user judges the photo to be well annotated, Semantically supervised Integrated Region Matching is adopted, which is a keyword-integrated soft region matching method; (2) If the user judges the photo to be poorly annotated, Keyword Integrated Bayesian Reasoning is adopted, which is a natural integration of a Visual Dictionary in online content-based search. In the relevance feedback phase, we conduct both visual and textual learning to capture the user's retrieval target. Better annotation and retrieval performance than current methods were reported on both COREL 10,000 and Flickr web image database (25,000 images), which demonstrated the effectiveness of our proposed framework.", "paper_title": "Visual and textual fusion for semantically supervised region-based retrieval", "paper_id": "WOS:000269011000001"}