{"auto_keywords": [{"score": 0.04635531096465835, "phrase": "symmetric_distributions"}, {"score": 0.013347074786027204, "phrase": "symmetric_distribution_d"}, {"score": 0.01254582592394906, "phrase": "disjunction_c"}, {"score": 0.00481495049065317, "phrase": "agnostic_learning_of"}, {"score": 0.003778253716938922, "phrase": "function_p"}, {"score": 0.0037094500688222695, "phrase": "linear_combination"}, {"score": 0.0033837135367113004, "phrase": "agnostic_learning_algorithm"}, {"score": 0.00322576184471691, "phrase": "best_known_previous_bound"}, {"score": 0.0028675712105299496, "phrase": "minimum_degree"}, {"score": 0.002763988044737562, "phrase": "n_variables"}, {"score": 0.0026348886455716614, "phrase": "learning_result"}, {"score": 0.002539689093411356, "phrase": "polynomial_basis"}, {"score": 0.00244792069601419, "phrase": "simple_proof"}, {"score": 0.00241214634516517, "phrase": "product_distribution_d"}, {"score": 0.0023421547766273036, "phrase": "polynomial_p"}, {"score": 0.002159951198924152, "phrase": "blais_et_al"}], "paper_keywords": ["agnostic learning", " symmetric distribution", " polynomial approximation", " regression", " disjunction", " conjunction", " DNF", " decision tree"], "paper_abstract": "We consider the problem of approximating and learning disjunctions (or equivalently, conjunctions) on symmetric distributions over {0,1}(n). Symmetric distributions are distributions whose PDF is invariant under any permutation of the variables. We prove that for every symmetric distribution D, there exists a set of n(O)(log(1/epsilon)) functions S, such that for every disjunction c, there is function p, expressible as a linear combination of functions in S, such that p epsilon-approximates c in l(1) distance on D or E-x similar to D[vertical bar C(x) - p(x)vertical bar] <= epsilon. This implies an agnostic learning algorithm for disjunctions on symmetric distributions that runs in time n(O)(log(1/epsilon)) The best known previous bound is n(O)(1/epsilon(4)) and follows from approximation of the more general class of halfspaces (Wimmer, 2010). We also show that there exists a symmetric distribution D, such that the minimum degree of a polynomial that 1/3-approximates the disjunction of all n variables in l(1) distance on D is Omega(root n). Therefore the learning result above cannot be achieved via 1/3-regression with a polynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution D and every disjunction c, there exists a polynomial p of degree O(log (1/epsilon)) such that p epsilon-approximates c in l(1) distance on D. This was first proved by Blais et al. (2008) via a more involved argument.", "paper_title": "Agnostic Learning of Disjunctions on Symmetric Distributions", "paper_id": "WOS:000369888000035"}