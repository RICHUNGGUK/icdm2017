{"auto_keywords": [{"score": 0.04060419095661136, "phrase": "lbm"}, {"score": 0.004815744682241845, "phrase": "boltzmann"}, {"score": 0.004768739216860887, "phrase": "kepler_architecture"}, {"score": 0.00461786618294789, "phrase": "fluid_flow"}, {"score": 0.004515091332353969, "phrase": "efficient_implementation"}, {"score": 0.004486147558765763, "phrase": "massively_parallel_computing"}, {"score": 0.004386291308906104, "phrase": "local_operations"}, {"score": 0.004139563711762907, "phrase": "third_generation_nvidia_gpu_hardware"}, {"score": 0.004073515217375164, "phrase": "'kepler"}, {"score": 0.003970013879881066, "phrase": "previous_optimization_strategies"}, {"score": 0.003944550453282929, "phrase": "analyse_data"}, {"score": 0.003906660012576505, "phrase": "different_memory_types"}, {"score": 0.0036749658368075027, "phrase": "adjacent_locations"}, {"score": 0.003616303279958699, "phrase": "parallel_performance"}, {"score": 0.0035017626442279386, "phrase": "different_hardware_options"}, {"score": 0.00339083755411773, "phrase": "'_features"}, {"score": 0.0033585037194063137, "phrase": "gpu"}, {"score": 0.0032414013828487727, "phrase": "kepler_based_gpus"}, {"score": 0.0031691634837160572, "phrase": "standard_transfer"}, {"score": 0.003098530474496842, "phrase": "optimized_storage"}, {"score": 0.0030687416689082064, "phrase": "coalesced_access"}, {"score": 0.002914624187933378, "phrase": "large_numbers"}, {"score": 0.002831350132465098, "phrase": "block_size"}, {"score": 0.0027682252595993403, "phrase": "special_features"}, {"score": 0.0027327860743648165, "phrase": "detailed_results"}, {"score": 0.0025871484499191758, "phrase": "latter_case"}, {"score": 0.0025458060790109647, "phrase": "read-only_data_cache"}, {"score": 0.0025051226957195634, "phrase": "peak_performance"}, {"score": 0.002386922774536045, "phrase": "periodic_bottleneck"}, {"score": 0.0023639590354685817, "phrase": "solver_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["GPGPU", " Lattice Boltzmann", " Computational fluid dynamics", " CUDA"], "paper_abstract": "The Lattice Boltzmann Method (LBM) for solving fluid flow is naturally well suited to an efficient implementation for massively parallel computing, due to the prevalence of local operations in the algorithm. This paper presents and analyses the performance of a 3D lattice Boltzmann solver, optimized for third generation nVidia GPU hardware, also known as 'Kepler'. We provide a review of previous optimization strategies and analyse data read/write times for different memory types. In LBM, the time propagation step (known as streaming), involves shifting data to adjacent locations and is central to parallel performance; here we examine three approaches which make use of different hardware options. Two of which make use of 'performance enhancing' features of the GPU; shared memory and the new shuffle instruction found in Kepler based GPUs. These are compared to a standard transfer of data which relies instead on optimized storage to increase coalesced access. It is shown that the more simple approach is most efficient; since the need for large numbers of registers per thread in LBM limits the block size and thus the efficiency of these special features is reduced. Detailed results are obtained for a D3Q19 LBM solver, which is benchmarked on nVidia K5000M and K20C GPUs. In the latter case the use of a read-only data cache is explored, and peak performance of over 1036 Million Lattice Updates Per Second (MLUPS) is achieved. The appearance of a periodic bottleneck in the solver performance is also reported, believed to be hardware related; spikes in iteration-time occur with a frequency of around 11 Hz for both GPUs, independent of the size of the problem. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Memory transfer optimization for a lattice Boltzmann solver on Kepler architecture nVidia GPUs", "paper_id": "WOS:000340340200020"}