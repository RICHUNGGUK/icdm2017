{"auto_keywords": [{"score": 0.03700292471975772, "phrase": "bayes"}, {"score": 0.022712859236400196, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "redundant_and_irrelevant_predictors."}, {"score": 0.004762202897268383, "phrase": "naive_bayes_model"}, {"score": 0.004684157900507474, "phrase": "simple_but_often_satisfactory_supervised_classification_method"}, {"score": 0.00433645229352341, "phrase": "serious_weakness"}, {"score": 0.004149429072483132, "phrase": "redundant_predictors"}, {"score": 0.0038838500580958744, "phrase": "regularization_technique"}, {"score": 0.00379914165379475, "phrase": "computationally_efficient_classifier"}, {"score": 0.0036958398682001015, "phrase": "naive_bayes"}, {"score": 0.0036352070472201086, "phrase": "proposed_formulation"}, {"score": 0.003309877074892399, "phrase": "lars_algorithm"}, {"score": 0.0032545805890800274, "phrase": "knn"}, {"score": 0.0031150035760241705, "phrase": "real-valued_and_discrete_predictors"}, {"score": 0.002947790697936243, "phrase": "wide_range"}, {"score": 0.002851790448644913, "phrase": "experimental_section"}, {"score": 0.0027286224888843956, "phrase": "redundant_and_irrelevant_predictors"}, {"score": 0.0025963890234079333, "phrase": "high-dimensional_data"}, {"score": 0.002539689093411356, "phrase": "neuroscience_field"}, {"score": 0.0024434201175883674, "phrase": "data_cases"}, {"score": 0.002312174113289583, "phrase": "real_data_set"}, {"score": 0.0022492129111567824, "phrase": "numeric_predictors"}], "paper_keywords": ["Lasso", " regularization", " naive Bayes", " redundancy"], "paper_abstract": "The naive Bayes model is a simple but often satisfactory supervised classification method. The original naive Bayes scheme, does, however, have a serious weakness, namely, the harmful effect of redundant predictors. In this paper, we study how to apply a regularization technique to learn a computationally efficient classifier that is inspired by naive Bayes. The proposed formulation, combined with an L-1-penalty, is capable of discarding harmful, redundant predictors. A modification of the LARS algorithm is devised to solve this problem. We tackle both real-valued and discrete predictors, assuring that our method is applicable to a wide range of data. In the experimental section, we empirically study the effect of redundant and irrelevant predictors. We also test the method on a high-dimensional data set from the neuroscience field, where there are many more predictors than data cases. Finally, we run the method on a real data set than combines categorical with numeric predictors. Our approach is compared with several naive Bayes variants and other classification algorithms (SVM and kNN), and is shown to be competitive.", "paper_title": "AN L-1-REGULARIZED NAIVE BAYES-INSPIRED CLASSIFIER FOR DISCARDING REDUNDANT AND IRRELEVANT PREDICTORS", "paper_id": "WOS:000323564900001"}