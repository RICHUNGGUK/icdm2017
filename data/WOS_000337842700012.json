{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "csmmi"}, {"score": 0.009392494697736934, "phrase": "mi"}, {"score": 0.00750666606200598, "phrase": "mutual_information"}, {"score": 0.005354605745082664, "phrase": "dictionary_items"}, {"score": 0.0045877173513910055, "phrase": "gesture_recognition"}, {"score": 0.00440976345264104, "phrase": "novel_approach"}, {"score": 0.00416478405812793, "phrase": "submodular_method"}, {"score": 0.0040385352882334235, "phrase": "compact_and_discriminative_dictionary"}, {"score": 0.0039333604469462356, "phrase": "traditional_dictionary-based_algorithms"}, {"score": 0.0038309141193444015, "phrase": "shared_dictionary"}, {"score": 0.0036660436272785476, "phrase": "intraclass_and_interclass_mutual_information"}, {"score": 0.0035705347414368726, "phrase": "single_objective_function"}, {"score": 0.00352371416065881, "phrase": "class-specific_dictionary"}, {"score": 0.003477505397536663, "phrase": "objective_function"}, {"score": 0.003284145698220425, "phrase": "specific_class"}, {"score": 0.0028652153774536967, "phrase": "computational_complexity"}, {"score": 0.002790511479398411, "phrase": "novel_submodular_method"}, {"score": 0.002705808393126695, "phrase": "important_contributions"}, {"score": 0.0025892330566358503, "phrase": "state-of-the-art_end-to-end_system"}, {"score": 0.002434396049547248, "phrase": "initial_dictionary"}, {"score": 0.002381365139274329, "phrase": "sparse_coding"}, {"score": 0.0022787359717772976, "phrase": "reconstruction_errors"}, {"score": 0.002238930800075706, "phrase": "extensive_experiments"}, {"score": 0.0022192891418394514, "phrase": "synthetic_data"}, {"score": 0.0021998194157393353, "phrase": "eight_benchmark_data_sets"}, {"score": 0.0021330078238529443, "phrase": "shared_dictionary_methods"}], "paper_keywords": ["Intra-class mutual information", " inter-class mutual information", " class-specific dictionary", " dictionary learning", " Gaussian Process", " sparse coding", " gesture recognition", " action recognition"], "paper_abstract": "In this paper, we propose a novel approach called class-specific maximization of mutual information (CSMMI) using a submodular method, which aims at learning a compact and discriminative dictionary for each class. Unlike traditional dictionary-based algorithms, which typically learn a shared dictionary for all of the classes, we unify the intraclass and interclass mutual information (MI) into an single objective function to optimize class-specific dictionary. The objective function has two aims: 1) maximizing the MI between dictionary items within a specific class (intrinsic structure) and 2) minimizing the MI between the dictionary items in a given class and those of the other classes (extrinsic structure). We significantly reduce the computational complexity of CSMMI by introducing an novel submodular method, which is one of the important contributions of this paper. This paper also contributes a state-of-the-art end-to-end system for action and gesture recognition incorporating CSMMI, with feature extraction, learning initial dictionary per each class by sparse coding, CSMMI via submodularity, and classification based on reconstruction errors. We performed extensive experiments on synthetic data and eight benchmark data sets. Our experimental results show that CSMMI outperforms shared dictionary methods and that our end-to-end system is competitive with other state-of-the-art approaches.", "paper_title": "CSMMI: Class-Specific Maximization of Mutual Information for Action and Gesture Recognition", "paper_id": "WOS:000337842700012"}