{"auto_keywords": [{"score": 0.048171695241769216, "phrase": "fundamental_ratios"}, {"score": 0.038229533265156386, "phrase": "line_segments"}, {"score": 0.009625069348624038, "phrase": "different_body_points"}, {"score": 0.00481495049065317, "phrase": "weighted_fundamental_ratios"}, {"score": 0.00451491283266524, "phrase": "view-invariant_action_recognition"}, {"score": 0.004409127897028587, "phrase": "different_body_parts"}, {"score": 0.0043793594257092805, "phrase": "action_recognition"}, {"score": 0.004335081499311714, "phrase": "moving_plane"}, {"score": 0.004276736877590646, "phrase": "fixed_camera"}, {"score": 0.004233492114255375, "phrase": "fundamental_matrix_f"}, {"score": 0.0040648241451154525, "phrase": "upper_left"}, {"score": 0.0037984549828326106, "phrase": "camera_internal_parameters"}, {"score": 0.0036594395622282358, "phrase": "similar_motions"}, {"score": 0.0036101552409969037, "phrase": "varying_viewpoints"}, {"score": 0.0035494788628319903, "phrase": "human_body"}, {"score": 0.0034311578789991363, "phrase": "body_posture"}, {"score": 0.0030992666061123533, "phrase": "body_part"}, {"score": 0.0030265495849271617, "phrase": "different_actions"}, {"score": 0.0029857633082091377, "phrase": "generic_method"}, {"score": 0.0028184476518849015, "phrase": "cmu_mocap"}, {"score": 0.0027616550517346066, "phrase": "ixmas"}, {"score": 0.0026068650963042444, "phrase": "extensive_experiments"}, {"score": 0.0024607296642187824, "phrase": "noisy_localization"}, {"score": 0.0024440836179156593, "phrase": "body_points"}, {"score": 0.00237861522566195, "phrase": "different_weights"}, {"score": 0.0023070522694236583, "phrase": "partial_occlusion"}, {"score": 0.0022914434133560852, "phrase": "recognition_accuracy"}, {"score": 0.0021703067932421463, "phrase": "starting_point"}, {"score": 0.002148315508445588, "phrase": "query_video"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["View invariance", " Pose transition", " Action recognition", " Action alignment", " Fundamental ratios"], "paper_abstract": "In this paper, we fully investigate the concept of fundamental ratios, demonstrate their application and significance in view-invariant action recognition, and explore the importance of different body parts in action recognition. A moving plane observed by a fixed camera induces a fundamental matrix F between two frames, where the ratios among the elements in the upper left 2 x 2 submatrix are herein referred to as the fundamental ratios. We show that fundamental ratios are invariant to camera internal parameters and orientation, and hence can be used to identify similar motions of line segments from varying viewpoints. By representing the human body as a set of points, we decompose a body posture into a set of line segments. The similarity between two actions is therefore measured by the motion of line segments and hence by their associated fundamental ratios. We further investigate to what extent a body part plays a role in recognition of different actions and propose a generic method of assigning weights to different body points. Experiments are performed on three categories of data: the controlled CMU MoCap dataset, the partially controlled IXMAS data, and the more challenging uncontrolled UCF-CIL dataset collected on the internet. Extensive experiments are reported on testing (i) view-invariance, (ii) robustness to noisy localization of body points, (iii) effect of assigning different weights to different body points, (iv) effect of partial occlusion on recognition accuracy, and (v) determining how soon our method recognizes an action correctly from the starting point of the query video. (c) 2013 Elsevier Inc. All rights reserved.", "paper_title": "View invariant action recognition using weighted fundamental ratios", "paper_id": "WOS:000317538500002"}