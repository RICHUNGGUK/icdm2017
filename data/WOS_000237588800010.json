{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "bi-class_support_vector_machine_formulations"}, {"score": 0.0041657281535389615, "phrase": "linearly_separable_binary_classification_problem"}, {"score": 0.0033374705499418377, "phrase": "linearly_separable_case"}, {"score": 0.0030908254851076005, "phrase": "non-linearly_separable_case"}, {"score": 0.0028139168128067343, "phrase": "unique_dual_optimization_problem"}, {"score": 0.0024969812322112174, "phrase": "article_point"}, {"score": 0.002433843373780471, "phrase": "in-depth_comparison"}, {"score": 0.002332133125576037, "phrase": "associated_parameters"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["SVM", " large margin principle", " bi-classitication", " optimization", " convex hull"], "paper_abstract": "Support vector machine (SVM) theory was originally developed on the basis of a linearly separable binary classification problem, and other approaches have been later introduced for this problem. In this paper it is demonstrated that all these approaches admit the same dual problem formulation in the linearly separable case and that all the solutions are equivalent. For the non-linearly separable case, all the approaches can also be formulated as a unique dual optimization problem, however, their solutions are not equivalent. Discussions and remarks in the article point to an in-depth comparison between SVM formulations and associated parameters. (c) 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Dual unification of bi-class support vector machine formulations", "paper_id": "WOS:000237588800010"}