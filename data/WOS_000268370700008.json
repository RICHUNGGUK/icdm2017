{"auto_keywords": [{"score": 0.0496533317597282, "phrase": "speech_production"}, {"score": 0.010612387000973441, "phrase": "neurocomputational_model"}, {"score": 0.009329571073763003, "phrase": "human_neural_processes"}, {"score": 0.00827384988061062, "phrase": "production-perception_model"}, {"score": 0.00458412183693466, "phrase": "current_speech_synthesis"}, {"score": 0.004543352284098364, "phrase": "speech_recognition_systems"}, {"score": 0.0036991458715587163, "phrase": "artificial_computer-implemented_vocal_tract"}, {"score": 0.0036498637283470386, "phrase": "front-end_module"}, {"score": 0.0035374012552316573, "phrase": "articulatory_speech_movements"}, {"score": 0.003505908076430666, "phrase": "acoustic_speech_signals"}, {"score": 0.0033525843225012918, "phrase": "sensory_processing_pathways"}, {"score": 0.0033227310662785293, "phrase": "speech_knowledge"}, {"score": 0.0032638170723261538, "phrase": "training_stages"}, {"score": 0.003220315882881027, "phrase": "early_stages"}, {"score": 0.0031916366459252992, "phrase": "speech_acquisition"}, {"score": 0.003093249591867931, "phrase": "artificial_self-organizing_maps"}, {"score": 0.0030520148188164084, "phrase": "current_neurocomputational_model"}, {"score": 0.002931569771634119, "phrase": "vc-"}, {"score": 0.002716844480081738, "phrase": "basic_features"}, {"score": 0.00269263690712392, "phrase": "natural_speech_production"}, {"score": 0.002574788013427524, "phrase": "straight_forward_way"}, {"score": 0.0025291018422967083, "phrase": "speech_items"}, {"score": 0.002473129577319634, "phrase": "controlled_and_phoneme_realizations"}, {"score": 0.0024401411219526774, "phrase": "perceptually_defined_regions"}, {"score": 0.0021720199689914464, "phrase": "basic_module"}, {"score": 0.0021334650920198715, "phrase": "high-quality_speech_synthesis"}, {"score": 0.0021049977753042253, "phrase": "high_performance_speech_recognition"}], "paper_keywords": ["Speech", " Speech production", " Speech perception", " Neurocomputational model", " Artificial neural networks", " Self-organizing networks"], "paper_abstract": "The limitation in performance of current speech synthesis and speech recognition systems may result from the fact that these systems are not designed with respect to the human neural processes of speech production and perception. A neurocomputational model of speech production and perception is introduced which is organized with respect to human neural processes of speech production and perception. The production-perception model comprises all artificial computer-implemented vocal tract as a front-end module, which is capable of generating articulatory speech movements and acoustic speech signals. The structure of the production-perception model comprises motor and sensory processing pathways. Speech knowledge is collected during training stages which imitate early stages of speech acquisition. This knowledge is stored in artificial self-organizing maps. The current neurocomputational model is capable of producing and perceiving vowels, VC-, and CV-syllables (V = vowels and C = voiced plosives). Basic features of natural speech production and perception are predicted from this model in a straight forward way: Production of speech items is feedforward and feedback controlled and phoneme realizations vary within perceptually defined regions. Perception is less categorical in the case of vowels in comparison to consonants. Due to its human-like production-perception processing the model should be discussed as a basic module for more technical relevant approaches for high-quality speech synthesis and for high performance speech recognition. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Towards a neurocomputational model of speech production and perception", "paper_id": "WOS:000268370700008"}