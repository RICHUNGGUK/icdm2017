{"auto_keywords": [{"score": 0.04919288632362341, "phrase": "reinforcement_learning"}, {"score": 0.04262956699100202, "phrase": "target_task"}, {"score": 0.03809336769485716, "phrase": "pcm"}, {"score": 0.014200404348548896, "phrase": "learning_results"}, {"score": 0.014004782071666222, "phrase": "source_task"}, {"score": 0.00481495049065317, "phrase": "partially_constrained_models"}, {"score": 0.004681971826924847, "phrase": "linked_multicomponent_robot_system_control"}, {"score": 0.004552648949390462, "phrase": "hierarchical_approach"}, {"score": 0.004468415400599571, "phrase": "complex_tasks"}, {"score": 0.004406260054779782, "phrase": "markov_decision_processes"}, {"score": 0.004185632403886255, "phrase": "starting_point"}, {"score": 0.0038661089070527424, "phrase": "constrained_systems"}, {"score": 0.003741712498615481, "phrase": "under-constrained_system"}, {"score": 0.0036553070575015344, "phrase": "partially_constrained_model"}, {"score": 0.003407839062159497, "phrase": "state-action_veto_policies"}, {"score": 0.003329117824699891, "phrase": "theoretical_background"}, {"score": 0.0032522091171602557, "phrase": "training_refinements"}, {"score": 0.0031770714776989282, "phrase": "effective_action_repertoires"}, {"score": 0.0030178044717552605, "phrase": "pcm-optimal_policy"}, {"score": 0.002989714021608139, "phrase": "maximal_state_value_functions"}, {"score": 0.002839812512030847, "phrase": "linked_multicomponent_robotic_systems"}, {"score": 0.0027612315954598085, "phrase": "paradigmatic_example"}, {"score": 0.002622755848331379, "phrase": "strong_physical_constraints"}, {"score": 0.0025862068041567934, "phrase": "large_state_space"}, {"score": 0.002562123444678208, "phrase": "learning_experiments"}, {"score": 0.00246800641971897, "phrase": "accurate_but_computationally_expensive_simulation"}, {"score": 0.002333258365963461, "phrase": "hose_model"}, {"score": 0.0022793032191981404, "phrase": "pcm_transfer_learning"}, {"score": 0.0022475295254231714, "phrase": "spectacular_improvement"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Linked multicomponent robotic systems", " Transfer learning", " Hose transportation"], "paper_abstract": "Transfer learning is a hierarchical approach to reinforcement learning of complex tasks modeled as Markov Decision Processes. The learning results on the source task are used as the starting point for the learning on the target task. In this paper we deal with a hierarchy of constrained systems, where the source task is an under-constrained system, hence called the Partially Constrained Model (PCM). Constraints in the framework of reinforcement learning are dealt with by state-action veto policies. We propose a theoretical background for the hierarchy of training refinements, showing that the effective action repertoires learnt on the PCM are maximal, and that the PCM-optimal policy gives maximal state value functions. We apply the approach to learn the control of Linked Multicomponent Robotic Systems using Reinforcement Learning. The paradigmatic example is the transportation of a hose. The system has strong physical constraints and a large state space. Learning experiments in the target task are realized over an accurate but computationally expensive simulation of the hose dynamics. The PCM is obtained simplifying the hose model. Learning results of the PCM Transfer Learning show an spectacular improvement over conventional Q-learning on the target task. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Transfer learning with Partially Constrained Models: Application to reinforcement learning of linked multicomponent robot system control", "paper_id": "WOS:000320685500006"}