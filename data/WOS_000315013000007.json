{"auto_keywords": [{"score": 0.0429841867448742, "phrase": "latent_space"}, {"score": 0.010460556442829476, "phrase": "multi-label_learning"}, {"score": 0.0097330575841497, "phrase": "relevant_labels"}, {"score": 0.004814952397682485, "phrase": "max-margin"}, {"score": 0.004543708337879809, "phrase": "classification_function"}, {"score": 0.0037811784409635023, "phrase": "low-dimensional_space"}, {"score": 0.0037267132967205136, "phrase": "computational_and_space_complexities"}, {"score": 0.0035507457285983268, "phrase": "appropriate_transformation"}, {"score": 0.0033182152378613767, "phrase": "max-margin_embedding_method"}, {"score": 0.0031614748814351823, "phrase": "low-dimensional_latent_space"}, {"score": 0.0030858947334838145, "phrase": "existing_label_embedding_methods"}, {"score": 0.0030414140732789186, "phrase": "pair_of_instance_and_label_embeddings"}, {"score": 0.002954361295811154, "phrase": "cost-sensitive_multi-label_hinge_loss"}, {"score": 0.0027341763737751467, "phrase": "positive_examples"}, {"score": 0.0025059724686541263, "phrase": "joint_optimization"}, {"score": 0.0024818194799172263, "phrase": "numerical_experiments"}, {"score": 0.0024107449502335583, "phrase": "high_performance"}, {"score": 0.0023530698447434308, "phrase": "existing_embedding_methods"}, {"score": 0.0021670777282518424, "phrase": "original_label_space"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multi-label learning", " Label embedding", " Max-margin learning", " Cost-sensitive multi-label hinge loss", " One versus all (OVA)"], "paper_abstract": "Multi-label learning refers to methods for learning a classification function that predicts a set of relevant labels for an instance. Label embedding seeks a transformation which maps labels into a latent space where regression is performed to predict a set of relevant labels. The latent space is often a low-dimensional space, so computational and space complexities are reduced. However, the choice of an appropriate transformation to a latent space is not clear. In this paper we present a max-margin embedding method where both instances and labels are mapped into a low-dimensional latent space. In contrast to existing label embedding methods, the pair of instance and label embeddings is determined by minimizing a cost-sensitive multi-label hinge loss, in which label-dependent cost is applied to more penalize the misclassification of positive examples. For implementation, we employ the limited memory Broyden-Fletcher-Goldfarb-Shanno (BEGS) method to determine the instance and label embeddings by a joint optimization. Numerical experiments on a few datasets demonstrate the high performance of our method compared to existing embedding methods in the case where the dimensionality of the latent space is much smaller than that of the original label space. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Max-margin embedding for multi-label learning", "paper_id": "WOS:000315013000007"}