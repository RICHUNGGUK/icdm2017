{"auto_keywords": [{"score": 0.0047310525657970615, "phrase": "high_dimensional_datasets"}, {"score": 0.004648609688625397, "phrase": "relatively_small_number"}, {"score": 0.00454090611248773, "phrase": "learning_algorithms"}, {"score": 0.0038309141193444015, "phrase": "bag-of-words_representation"}, {"score": 0.003698442219929622, "phrase": "gene_expression_data"}, {"score": 0.0035288860880035985, "phrase": "high_number"}, {"score": 0.003175175896313016, "phrase": "learning_tasks"}, {"score": 0.0029592333031063156, "phrase": "adequate_techniques"}, {"score": 0.002924694271730544, "phrase": "feature_representation"}, {"score": 0.00270978310478061, "phrase": "memory_requirements"}, {"score": 0.0025854347094662247, "phrase": "combined_unsupervised_feature_discretization"}, {"score": 0.002555247330102505, "phrase": "feature_selection_techniques"}, {"score": 0.00249592321829865, "phrase": "medium_and_high-dimensional_datasets"}, {"score": 0.002353554841514325, "phrase": "sparse_and_dense_features"}, {"score": 0.002258745901862197, "phrase": "proposed_techniques"}, {"score": 0.0021805201234139475, "phrase": "previous_related_techniques"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feature discretization", " Feature quantization", " Feature selection", " Linde-Buzo-Gray algorithm", " Sparse data", " Support vector machines", " Naive Bayes", " k-nearest neighbor"], "paper_abstract": "Many learning problems require handling high dimensional datasets with a relatively small number of instances. Learning algorithms are thus confronted with the curse of dimensionality, and need to address it in order to be effective. Examples of these types of data include the bag-of-words representation in text classification problems and gene expression data for tumor detection/classification. Usually, among the high number of features characterizing the instances, many may be irrelevant (or even detrimental) for the learning tasks. It is thus clear that there is a need for adequate techniques for feature representation, reduction, and selection, to improve both the classification accuracy and the memory requirements. In this paper, we propose combined unsupervised feature discretization and feature selection techniques, suitable for medium and high-dimensional datasets. The experimental results on several standard datasets, with both sparse and dense features, show the efficiency of the proposed techniques as well as improvements over previous related techniques. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "An unsupervised approach to feature discretization and selection", "paper_id": "WOS:000306091900002"}