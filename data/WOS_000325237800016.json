{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sarsa_algorithm"}, {"score": 0.013986395475240136, "phrase": "action_selection_policy"}, {"score": 0.004734448316824488, "phrase": "reinforcement_learning"}, {"score": 0.0042250123967577284, "phrase": "well-known_areas"}, {"score": 0.004067635935640407, "phrase": "sarsa_algorithms"}, {"score": 0.003982741546775355, "phrase": "different_characteristics"}, {"score": 0.003850566273995209, "phrase": "faster_convergence_characteristics"}, {"score": 0.0037861265016041813, "phrase": "q-learning_algorithm"}, {"score": 0.003738502710504415, "phrase": "better_final_performance"}, {"score": 0.003584025547693836, "phrase": "local_minimum"}, {"score": 0.0034944094552943, "phrase": "longer_time"}, {"score": 0.0033078284621762817, "phrase": "action_selection_strategy"}, {"score": 0.00305285084207725, "phrase": "new_method"}, {"score": 0.0028174721236762317, "phrase": "backward_q-learning_algorithm"}, {"score": 0.002600194073758015, "phrase": "proposed_rl_algorithms"}, {"score": 0.002524428047147766, "phrase": "final_performance"}, {"score": 0.002389514188569084, "phrase": "cart-pole_balancing_control_system"}, {"score": 0.002280990744438186, "phrase": "proposed_scheme"}, {"score": 0.0022238834225283594, "phrase": "backward_q-learning_based_rl_algorithm"}, {"score": 0.002195866991284673, "phrase": "well-known_q-learning"}, {"score": 0.0021682028241336248, "phrase": "sarsa"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Backward Q-learning", " Q-learning", " Reinforcement learning", " Sarsa algorithm"], "paper_abstract": "Reinforcement learning (RI) has been applied to many fields and applications, but there are still some dilemmas between exploration and exploitation strategy for action selection policy. The well-known areas of reinforcement learning are the Q-learning and the Sarsa algorithms, but they possess different characteristics. Generally speaking, the Sarsa algorithm has faster convergence characteristics, while the Q-learning algorithm has a better final performance. However, Sarsa algorithm is easily stuck in the local minimum and Q-learning needs longer time to learn. Most literatures investigated the action selection policy. Instead of studying an action selection strategy, this paper focuses on how to combine Q-learning with the Sarsa algorithm, and presents a new method, called backward Q-learning, which can be implemented in the Sarsa algorithm and Q-learning. The backward Q-learning algorithm directly tunes the Q-values, and then the Q-values will indirectly affect the action selection policy. Therefore, the proposed RL algorithms can enhance learning speed and improve final performance. Finally, three experimental results including cliff walk, mountain car, and cart-pole balancing control system are utilized to verify the feasibility and effectiveness of the proposed scheme. All the simulations illustrate that the backward Q-learning based RL algorithm outperforms the well-known Q-learning and the Sarsa algorithm. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Backward Q-learning: The combination of Sarsa algorithm and Q-learning", "paper_id": "WOS:000325237800016"}