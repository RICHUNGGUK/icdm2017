{"auto_keywords": [{"score": 0.0363392912425693, "phrase": "gpu"}, {"score": 0.004991401386634437, "phrase": "mnist"}, {"score": 0.00481495049065317, "phrase": "improved_convergence"}, {"score": 0.004767261858417823, "phrase": "deep_belief_networks"}, {"score": 0.004720043307726269, "phrase": "graphics_processing_units"}, {"score": 0.004446349752828045, "phrase": "pre-training_time"}, {"score": 0.0043804319008535555, "phrase": "deep_belief_network"}, {"score": 0.004167653403322565, "phrase": "adaptive_step_size_technique"}, {"score": 0.004024863549087511, "phrase": "contrastive_divergence"}, {"score": 0.0037350832473790007, "phrase": "restricted_boltzmann_machine"}, {"score": 0.0036070601923916196, "phrase": "dbn_infrastructure"}, {"score": 0.0034834098902271626, "phrase": "highly_scalable_graphics_processing_unit"}, {"score": 0.0034146585649631692, "phrase": "parallel_implementation"}, {"score": 0.0033639840192975835, "phrase": "cd-k_algorithm"}, {"score": 0.003264872488939969, "phrase": "training_speed"}, {"score": 0.0032004205378150354, "phrase": "extensive_experiments"}, {"score": 0.003075296862372287, "phrase": "hhreco_databases"}, {"score": 0.002969821115740559, "phrase": "maximum_useful_depth"}, {"score": 0.0027834145465170292, "phrase": "training_samples"}, {"score": 0.0026612444314342023, "phrase": "lower-level_layer"}, {"score": 0.0026217213613095322, "phrase": "fundamental_role"}, {"score": 0.0025827837420330816, "phrase": "successful_dbn_models"}, {"score": 0.002481747081849503, "phrase": "preconceived_idea"}, {"score": 0.0022799458141165587, "phrase": "multiple_back-propagation"}, {"score": 0.0022016885701010088, "phrase": "dbns_generalization_capability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Deep learning", " Deep belief networks", " Restricted Boltzmann machines", " Contrastive divergence", " Adaptive step size", " GPU computing"], "paper_abstract": "In this paper we focus on two complementary approaches to significantly decrease pre-training time of a deep belief network (DBN). First, we propose an adaptive step size technique to enhance the convergence of the contrastive divergence (CD) algorithm, thereby reducing the number of epochs to train the restricted Boltzmann machine (RBM) that supports the DBN infrastructure. Second, we present a highly scalable graphics processing unit (GPU) parallel implementation of the CD-k algorithm, which boosts notably the training speed. Additionally, extensive experiments are conducted on the MNIST and the HHreco databases. The results suggest that the maximum useful depth of a DBN is related to the number and quality of the training samples. Moreover, it was found that the lower-level layer plays a fundamental role for building successful DBN models. Furthermore, the results contradict the preconceived idea that all the layers should be pre-trained. Finally, it is shown that by incorporating multiple back-propagation (MBP) layers, the DBNs generalization capability is remarkably improved. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Towards adaptive learning with improved convergence of deep belief networks on graphics processing units", "paper_id": "WOS:000326903500010"}