{"auto_keywords": [{"score": 0.03843856552198327, "phrase": "hdp"}, {"score": 0.00481495049065317, "phrase": "maze_navigation"}, {"score": 0.004757643419056999, "phrase": "goal_representation_heuristic_dynamic_programming"}, {"score": 0.004454377387615279, "phrase": "online_learning"}, {"score": 0.004375061788023814, "phrase": "markov_decision_process"}, {"score": 0.0039043835848369215, "phrase": "proposed_goal_network"}, {"score": 0.00374399358035391, "phrase": "actor-critic_design"}, {"score": 0.0036993855514735746, "phrase": "heuristic_dynamic_programming"}, {"score": 0.0035473867669654174, "phrase": "goal_network"}, {"score": 0.0034633412134999425, "phrase": "internal_goal_signal"}, {"score": 0.0033409791799749434, "phrase": "value_function_approximation"}, {"score": 0.0030353401945270755, "phrase": "traditional_hdp_approach"}, {"score": 0.002692200353719145, "phrase": "learning_performance"}, {"score": 0.0023171750718791713, "phrase": "theoretical_guarantee"}, {"score": 0.0022218413146898887, "phrase": "characteristics_analysis"}, {"score": 0.0021304214257321, "phrase": "neural_networks"}], "paper_keywords": ["Adaptive dynamic programming", " goal representation heuristic dynamic programming", " maze navigation/path planning", " Markov decision process", " reinforcement learning"], "paper_abstract": "Goal representation heuristic dynamic programming (GrHDP) is proposed in this paper to demonstrate online learning in the Markov decision process. In addition to the (external) reinforcement signal in literature, we develop an adaptively internal goal/reward representation for the agent with the proposed goal network. Specifically, we keep the actor-critic design in heuristic dynamic programming (HDP) and include a goal network to represent the internal goal signal, to further help the value function approximation. We evaluate our proposed GrHDP algorithm on two 2-D maze navigation problems, and later on one 3-D maze navigation problem. Compared to the traditional HDP approach, the learning performance of the agent is improved with our proposed GrHDP approach. In addition, we also include the learning performance with two other reinforcement learning algorithms, namely Sarsa(lambda) and Q-learning, on the same benchmarks for comparison. Furthermore, in order to demonstrate the theoretical guarantee of our proposed method, we provide the characteristics analysis toward the convergence of weights in neural networks in our GrHDP approach.", "paper_title": "Goal Representation Heuristic Dynamic Programming on Maze Navigation", "paper_id": "WOS:000326940600011"}