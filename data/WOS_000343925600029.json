{"auto_keywords": [{"score": 0.015719716506582538, "phrase": "wireless_sensor_networks"}, {"score": 0.011518733816404168, "phrase": "q-learning_algorithm"}, {"score": 0.004682218325765133, "phrase": "intrusion_detection_application"}, {"score": 0.0045249263666466005, "phrase": "sleep_times"}, {"score": 0.004482949028773004, "phrase": "individual_sensors"}, {"score": 0.004359328949187473, "phrase": "network_lifetime"}, {"score": 0.004305481798183612, "phrase": "tracking_error"}, {"score": 0.004160792819919432, "phrase": "partially-observable_markov_decision_process"}, {"score": 0.004096635798793164, "phrase": "continuous_stateaction_spaces"}, {"score": 0.004008468130135995, "phrase": "fuemmeler"}, {"score": 0.0039836276928821735, "phrase": "veeravalli"}, {"score": 0.003743487506598038, "phrase": "infinite_horizon"}, {"score": 0.003662892058072211, "phrase": "performance_criteria"}, {"score": 0.003431332820984854, "phrase": "function_approximation"}, {"score": 0.0034100553723847037, "phrase": "feature-based_representations"}, {"score": 0.003254610516477302, "phrase": "underlying_pomdp."}, {"score": 0.0032143649757754595, "phrase": "policy_gradient_update"}, {"score": 0.0031845067058084613, "phrase": "one-simulation_simultaneous_perturbation_stochastic_approximation_estimate"}, {"score": 0.0031062294139798273, "phrase": "q-value_parameter"}, {"score": 0.0030582832134483685, "phrase": "linear_function_approximation_architecture"}, {"score": 0.002973831814707697, "phrase": "on-policy_temporal_difference_algorithm-like_fashion"}, {"score": 0.0029097582786754444, "phrase": "feature_selection_scheme"}, {"score": 0.0027256800996910864, "phrase": "optimal_sleep-scheduling_policy"}, {"score": 0.0025691622090542325, "phrase": "function_approximation_analogue"}, {"score": 0.0024904187737391807, "phrase": "two-timescale_variant"}, {"score": 0.0024519545383399773, "phrase": "theoretical_convergence_guarantees"}, {"score": 0.0023694065215394593, "phrase": "stochastic_iterative_estimation_scheme"}, {"score": 0.002347378595930709, "phrase": "intruder's_mobility_model"}, {"score": 0.0021783541405925766, "phrase": "better_tracking_accuracy"}, {"score": 0.0021049977753042253, "phrase": "recent_prior_work"}], "paper_keywords": ["Sensor Networks", " Sleep-Wake scheduling", " Reinforcement learning", " Q-learning", " Function approximation", " Simultaneous perturbation", " SPSA"], "paper_abstract": "In this paper, we consider an intrusion detection application for Wireless Sensor Networks. We study the problem of scheduling the sleep times of the individual sensors, where the objective is to maximize the network lifetime while keeping the tracking error to a minimum. We formulate this problem as a partially-observable Markov decision process (POMDP) with continuous stateaction spaces, in a manner similar to Fuemmeler and Veeravalli (IEEE Trans Signal Process 56(5), 2091-2101, 2008). However, unlike their formulation, we consider infinite horizon discounted and average cost objectives as performance criteria. For each criterion, we propose a convergent on-policy Q-learning algorithm that operates on two timescales, while employing function approximation. Feature-based representations and function approximation is necessary to handle the curse of dimensionality associated with the underlying POMDP. Our proposed algorithm incorporates a policy gradient update using a one-simulation simultaneous perturbation stochastic approximation estimate on the faster timescale, while the Q-value parameter (arising from a linear function approximation architecture for the Q-values) is updated in an on-policy temporal difference algorithm-like fashion on the slower timescale. The feature selection scheme employed in each of our algorithms manages the energy and tracking components in a manner that assists the search for the optimal sleep-scheduling policy. For the sake of comparison, in both discounted and average settings, we also develop a function approximation analogue of the Q-learning algorithm. This algorithm, unlike the two-timescale variant, does not possess theoretical convergence guarantees. Finally, we also adapt our algorithms to include a stochastic iterative estimation scheme for the intruder's mobility model and this is useful in settings where the latter is not known. Our simulation results on a synthetic 2-dimensional network setting suggest that our algorithms result in better tracking accuracy at the cost of only a few additional sensors, in comparison to a recent prior work.", "paper_title": "Two timescale convergent Q-learning for sleep-scheduling in wireless sensor networks", "paper_id": "WOS:000343925600029"}