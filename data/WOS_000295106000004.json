{"auto_keywords": [{"score": 0.03349722987496378, "phrase": "model_structure_selection"}, {"score": 0.004815133528682739, "phrase": "gpu"}, {"score": 0.0047113960612140335, "phrase": "elm_ensembles"}, {"score": 0.004670597652149685, "phrase": "large-scale_regression"}, {"score": 0.0044524122425287005, "phrase": "large_data_sets"}, {"score": 0.004413846562278824, "phrase": "reasonable_time"}, {"score": 0.004300133642951283, "phrase": "extreme_learning_machines"}, {"score": 0.004189337956225294, "phrase": "main_purpose"}, {"score": 0.003581797255879607, "phrase": "graphics_processing_unit"}, {"score": 0.002906138793910919, "phrase": "model_training"}, {"score": 0.002831158147480665, "phrase": "multiple_cpu"}, {"score": 0.0028065957753430713, "phrase": "cpu_cores"}, {"score": 0.0027581067089091434, "phrase": "multiple_models"}, {"score": 0.0026175952455594277, "phrase": "competitive_performance"}, {"score": 0.0025611773887867255, "phrase": "regression_tasks"}, {"score": 0.0024519545383399773, "phrase": "elm_ensemble"}, {"score": 0.0024306740161601625, "phrase": "attractive_speedups"}, {"score": 0.0023886641270884973, "phrase": "single_cpu."}, {"score": 0.002347378595930709, "phrase": "proposed_approach"}, {"score": 0.002286781334850213, "phrase": "specific_type"}, {"score": 0.0021987994275190314, "phrase": "large_variety"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["ELM", " Ensemble methods", " GPU", " Parallelization", " High-performance computing"], "paper_abstract": "The paper presents an approach for performing regression on large data sets in reasonable time, using an ensemble of extreme learning machines (ELMs). The main purpose and contribution of this paper are to explore how the evaluation of this ensemble of ELMs can be accelerated in three distinct ways: (1) training and model structure selection of the individual ELMs are accelerated by performing these steps on the graphics processing unit (CPU), instead of the processor (CPU); (2) the training of ELM is performed in such a way that computed results can be reused in the model structure selection, making training plus model structure selection more efficient; (3) the modularity of the ensemble model is exploited and the process of model training and model structure selection is parallelized across multiple CPU and CPU cores, such that multiple models can be built at the same time. The experiments show that competitive performance is obtained on the regression tasks, and that the CPU-accelerated and parallelized ELM ensemble achieves attractive speedups over using a single CPU. Furthermore, the proposed approach is not limited to a specific type of ELM and can be employed for a large variety of ELMs. (C) 2011 Published by Elsevier B.V.", "paper_title": "GPU-accelerated and parallelized ELM ensembles for large-scale regression", "paper_id": "WOS:000295106000004"}