{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "contextual_information"}, {"score": 0.004662036750452837, "phrase": "spoken_dialogue_systems"}, {"score": 0.0043144799365586375, "phrase": "context_information"}, {"score": 0.003865826699096508, "phrase": "user-system_interaction"}, {"score": 0.003791621842185417, "phrase": "neutral_speaking_style"}, {"score": 0.0036710872046569532, "phrase": "new_method"}, {"score": 0.003353521531649663, "phrase": "novel_techniques"}, {"score": 0.003310447652300423, "phrase": "acoustic_normalization"}, {"score": 0.0032468685934998335, "phrase": "context_annotation"}, {"score": 0.003004494701922268, "phrase": "real_human_interactions"}, {"score": 0.0029467742734417255, "phrase": "spoken_dialogue_system"}, {"score": 0.0027982016314450717, "phrase": "non-expert_human_annotators"}, {"score": 0.0027622407190589326, "phrase": "machine-learned_classifications"}, {"score": 0.002622947550220266, "phrase": "proposed_method"}, {"score": 0.00244278788608393, "phrase": "maximum_agreement_rates"}, {"score": 0.002411383506019635, "phrase": "nonexpert_human_annotation"}, {"score": 0.0023497779149212737, "phrase": "automatic_classification_accuracy"}, {"score": 0.0022457226013538343, "phrase": "classical_approach"}, {"score": 0.0021883400011393564, "phrase": "acoustic_features"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["emotion annotation", " emotion recognition", " emotional speech", " spoken dialogue system", " dialogue context", " acoustic context", " affective computing"], "paper_abstract": "In this paper, we study the impact of considering context information for the annotation of emotions. Concretely, we propose the inclusion of the history of user-system interaction and the neutral speaking style of users. A new method to automatically include both sources of information has been developed making use of novel techniques for acoustic normalization and dialogue context annotation. We have carried out experiments with a corpus extracted from real human interactions with a spoken dialogue system. Results show that the performance of non-expert human annotators and machine-learned classifications are both affected by contextual information. The proposed method allows the annotation of more non-neutral emotions and yields values closer to maximum agreement rates for nonexpert human annotation. Moreover, automatic classification accuracy improves by 29.57% compared to the classical approach based only on acoustic features. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Influence of contextual information in emotion annotation for spoken dialogue systems", "paper_id": "WOS:000255699300006"}