{"auto_keywords": [{"score": 0.03323248941990375, "phrase": "genetic_algorithm"}, {"score": 0.014749591330506226, "phrase": "feature_selection"}, {"score": 0.014490740655215785, "phrase": "feature_ranking"}, {"score": 0.013309268196346459, "phrase": "different_categories"}, {"score": 0.01154784297441416, "phrase": "base_ranking_methods"}, {"score": 0.007400009198298037, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "new_ensemble_method_for_feature_ranking_in_text_mining._dimensionality_reduction"}, {"score": 0.00462919213721188, "phrase": "high_dimensional_data"}, {"score": 0.0045471315699087385, "phrase": "dimensionality_reduction"}, {"score": 0.0043095378396174125, "phrase": "major_reasons"}, {"score": 0.004172957742905294, "phrase": "fast_computation"}, {"score": 0.004143196873246147, "phrase": "feature_ranking_methods"}, {"score": 0.004011867299107537, "phrase": "different_measures"}, {"score": 0.00398325070540449, "phrase": "ranking_features"}, {"score": 0.003926626018798576, "phrase": "ensemble_methods"}, {"score": 0.0036553222761541468, "phrase": "heterogeneous"}, {"score": 0.0034890505116128606, "phrase": "ensemble_structure"}, {"score": 0.0034148795808317555, "phrase": "information_theoretic"}, {"score": 0.0031902155632863236, "phrase": "final_feature_subset"}, {"score": 0.0030779528295407925, "phrase": "base_methods"}, {"score": 0.003023307182674672, "phrase": "initial_population"}, {"score": 0.002937879003671346, "phrase": "convergence_time"}, {"score": 0.0028548578165712643, "phrase": "ranking_methods"}, {"score": 0.002804161833785297, "phrase": "user's_task"}, {"score": 0.002724909017963099, "phrase": "appropriate_subset"}, {"score": 0.0025364139244227436, "phrase": "good_one"}, {"score": 0.0025003054002226965, "phrase": "proposed_algorithm"}, {"score": 0.0024383449794706477, "phrase": "proper_threshold"}, {"score": 0.002285961573963468, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "classification_method"}], "paper_keywords": ["Text classification", " dimensionality reduction", " feature ranking", " ensemble based learning algorithms"], "paper_abstract": "Dimensionality reduction is a necessary task in data mining when working with high dimensional data. A type of dimensionality reduction is feature selection. Feature selection based on feature ranking has received much attention by researchers. The major reasons are its scalability, ease of use, and fast computation. Feature ranking methods can be divided into different categories and may use different measures for ranking features. Recently, ensemble methods have entered in the field of ranking and achieved more accuracy among others. Accordingly, in this paper a Heterogeneous ensemble based algorithm for feature ranking is proposed. The base ranking methods in this ensemble structure are chosen from different categories like information theoretic, distance based, and statistical methods. The results of the base ranking methods are then fused into a final feature subset by means of genetic algorithm. The diversity of the base methods improves the quality of initial population of the genetic algorithm and thus reducing the convergence time of the genetic algorithm. In most of ranking methods, it's the user's task to determine the threshold for choosing the appropriate subset of features. It is a problem, which may cause the user to try many different values to select a good one. In the proposed algorithm, the difficulty of determining a proper threshold by the user is decreased. The performance of the algorithm is evaluated on four different text datasets and the experimental results show that the proposed method outperforms all other five feature ranking methods used for comparison. One advantage of the proposed method is that it is independent to the classification method used for classification.", "paper_title": "A NEW ENSEMBLE METHOD FOR FEATURE RANKING IN TEXT MINING", "paper_id": "WOS:000320617300002"}