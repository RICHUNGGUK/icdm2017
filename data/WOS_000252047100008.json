{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "character_n-grams"}, {"score": 0.00475413908468027, "phrase": "anti-spam_filtering"}, {"score": 0.004664352260912174, "phrase": "increasing_number"}, {"score": 0.004605434064238641, "phrase": "unsolicited_e-mail_messages"}, {"score": 0.004294381488413906, "phrase": "reliable_anti-spam_filters"}, {"score": 0.004133627071344356, "phrase": "content-based_techniques"}, {"score": 0.004055511503825102, "phrase": "word-based_representation"}, {"score": 0.0039036638083325783, "phrase": "reliable_tokenizers"}, {"score": 0.003805591825381666, "phrase": "token_boundaries"}, {"score": 0.003639835714755229, "phrase": "common_practice"}, {"score": 0.0034154440860550564, "phrase": "unexpected_punctuation_marks"}, {"score": 0.00337224813954299, "phrase": "special_characters"}, {"score": 0.003144222265269426, "phrase": "alternative_low-level_representation"}, {"score": 0.0027159094186967247, "phrase": "evaluation_measures"}, {"score": 0.0022292394795878643, "phrase": "variable-length_n-grams"}, {"score": 0.0021319959284827896, "phrase": "examined_models"}, {"score": 0.0021049977753042253, "phrase": "cost-sensitive_evaluation"}], "paper_keywords": ["anti-sparn filtering", " machine learning", " n-grams"], "paper_abstract": "The increasing number of unsolicited e-mail messages (spam) reveals the need for the development of reliable anti-spam filters. The vast majority of content-based techniques rely on word-based representation of messages. Such approaches require reliable tokenizers for detecting the token boundaries. As a consequence, a common practice of spammers is to attempt to confuse tokenizers using unexpected punctuation marks or special characters within the message. In this paper we explore an alternative low-level representation based on character n-grams which avoids the use of tokcnizers and other language-dependent tools. Based on experiments on two well-known benchmark corpora and a variety of evaluation measures, we show that character n-grams are more reliable features than word-tokens despite the fact that they increase the dimensionality of the problem. Moreover, we propose a method for extracting variable-length n-grams which produces optimal classifiers among the examined models under cost-sensitive evaluation.", "paper_title": "Words versus character N-grams for anti-spam filtering", "paper_id": "WOS:000252047100008"}