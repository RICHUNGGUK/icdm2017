{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "probabilistic_neural_networks"}, {"score": 0.004630910803911567, "phrase": "probabilistic_approach"}, {"score": 0.004559270778306828, "phrase": "neural_networks"}, {"score": 0.004384960761826366, "phrase": "statistical_pattern_recognition"}, {"score": 0.004217286731331736, "phrase": "class-conditional_probability_distributions"}, {"score": 0.004055998173160551, "phrase": "product_components"}, {"score": 0.00396218934254591, "phrase": "mixture_components"}, {"score": 0.0038106195546165574, "phrase": "probabilistic_neurons"}, {"score": 0.0037516215763875225, "phrase": "neurophysiological_terms"}, {"score": 0.0035245918797629804, "phrase": "fixed_probabilistic_description"}, {"score": 0.003443031148055198, "phrase": "well_known_short-term_dynamic_properties"}, {"score": 0.0032599633778910516, "phrase": "iterative_schemes"}, {"score": 0.002810520668490478, "phrase": "dynamic_processes"}, {"score": 0.0027240799841720957, "phrase": "statistically_correct_decision_making"}, {"score": 0.002519402443228795, "phrase": "mixture_component_weights"}, {"score": 0.0024418934977061876, "phrase": "input_pattern"}, {"score": 0.00234834406550549, "phrase": "correct_recognition"}, {"score": 0.002154930852763048, "phrase": "special_case"}, {"score": 0.0021049977753042253, "phrase": "well_known_em_algorithm"}], "paper_keywords": ["probabilistic neural networks", " distribution mixtures", " EM algorithm", " recognition of numerals", " recurrent reasoning"], "paper_abstract": "When considering the probabilistic approach to neural networks in the framework of statistical pattern recognition we assume approximation of class-conditional probability distributions by finite mixtures of product components. The mixture components can be interpreted as probabilistic neurons in neurophysiological terms and, in this respect, the fixed probabilistic description contradicts the well known short-term dynamic properties of biological neurons. By introducing iterative schemes of recognition we show that some parameters of probabilistic neural networks can be \"released\" for the sake of dynamic processes without disturbing the statistically correct decision making. In particular, we can iteratively adapt the mixture component weights or modify the input pattern in order to facilitate correct recognition. Both procedures are shown to converge monotonically as a special case of the well known EM algorithm for estimating mixtures. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Iterative principles of recognition in probabilistic neural networks", "paper_id": "WOS:000259846600006"}