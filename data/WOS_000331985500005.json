{"auto_keywords": [{"score": 0.04924496525218245, "phrase": "function_approximation"}, {"score": 0.04721825881327067, "phrase": "neural_networks"}, {"score": 0.04073222114585991, "phrase": "high-dimensional_data"}, {"score": 0.015583657937346082, "phrase": "supervised_learning"}, {"score": 0.00481495049065317, "phrase": "function_approximation_using_combined_unsupervised"}, {"score": 0.004628223381645729, "phrase": "core_tasks"}, {"score": 0.0043519860930099795, "phrase": "good_approximation_results"}, {"score": 0.00431388720289049, "phrase": "good_sampling"}, {"score": 0.004257360656055417, "phrase": "data_space"}, {"score": 0.00416478405812793, "phrase": "exponentially_increasing_volume"}, {"score": 0.004020813632357377, "phrase": "data_increases"}, {"score": 0.0036499505265403377, "phrase": "function_approximation_task"}, {"score": 0.0034018288705284427, "phrase": "lower_dimensional_space"}, {"score": 0.0031151864044353245, "phrase": "mapped_lower_dimensional_data"}, {"score": 0.0030608148631528767, "phrase": "over-complete_self-organizing_maps"}, {"score": 0.0029548937818509656, "phrase": "unsupervised_learning"}, {"score": 0.0029161225023394363, "phrase": "single_hidden_layer_neural_networks"}, {"score": 0.0027660443371695024, "phrase": "two-step_procedure"}, {"score": 0.002729744174693491, "phrase": "support_vector_machines"}, {"score": 0.002705808393126695, "phrase": "bayesian_soms"}, {"score": 0.0026352496813713292, "phrase": "best_parameters"}, {"score": 0.002600661525224505, "phrase": "nonlinear_neurons"}, {"score": 0.0025665261770229757, "phrase": "hidden_layer"}, {"score": 0.002434396049547248, "phrase": "approximation_performance"}, {"score": 0.0024024377855956136, "phrase": "proposed_neural_networks"}, {"score": 0.0021049977753042253, "phrase": "original_high-dimensional_data"}], "paper_keywords": ["Function approximation", " learning", " neural network", " self-organizing map (SOM)"], "paper_abstract": "Function approximation is one of the core tasks that are solved using neural networks in the context of many engineering problems. However, good approximation results need good sampling of the data space, which usually requires exponentially increasing volume of data as the dimensionality of the data increases. At the same time, often the high-dimensional data is arranged around a much lower dimensional manifold. Here we propose the breaking of the function approximation task for high-dimensional data into two steps: 1) the mapping of the high-dimensional data onto a lower dimensional space corresponding to the manifold on which the data resides and 2) the approximation of the function using the mapped lower dimensional data. We use over-complete self-organizing maps (SOMs) for the mapping through unsupervised learning, and single hidden layer neural networks for the function approximation through supervised learning. We also extend the two-step procedure by considering support vector machines and Bayesian SOMs for the determination of the best parameters for the nonlinear neurons in the hidden layer of the neural networks used for the function approximation. We compare the approximation performance of the proposed neural networks using a set of functions and show that indeed the neural networks using combined unsupervised and supervised learning outperform in most cases the neural networks that learn the function approximation using the original high-dimensional data.", "paper_title": "Function Approximation Using Combined Unsupervised and Supervised Learning", "paper_id": "WOS:000331985500005"}