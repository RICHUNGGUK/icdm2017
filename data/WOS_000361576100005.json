{"auto_keywords": [{"score": 0.048522326887293465, "phrase": "rnnlms"}, {"score": 0.034418546954494485, "phrase": "language_models"}, {"score": 0.03387102202786076, "phrase": "cgn_data"}, {"score": 0.030336897016150905, "phrase": "wsj_data"}, {"score": 0.02672588709142411, "phrase": "sentence_length"}, {"score": 0.02660277491337371, "phrase": "word_length"}, {"score": 0.025935456999568573, "phrase": "intrinsic_information"}, {"score": 0.00481495049065317, "phrase": "recurrent_neural_network_language_models"}, {"score": 0.0047472380621488616, "phrase": "conventional_n-gram_language_models"}, {"score": 0.004592905436175987, "phrase": "fair_amount"}, {"score": 0.004571269461883144, "phrase": "research_attention"}, {"score": 0.004539005582217309, "phrase": "speech_recognition_community"}, {"score": 0.004299064805926241, "phrase": "additional_knowledge_sources"}, {"score": 0.004218602458288531, "phrase": "complementary_information_w.r.t"}, {"score": 0.003967314311312148, "phrase": "n-best_list"}, {"score": 0.003957952976206386, "phrase": "re-scoring_experiments"}, {"score": 0.003930000775297499, "phrase": "challenging_corpus"}, {"score": 0.0039114753002009485, "phrase": "spoken_dutch"}, {"score": 0.003865543010803699, "phrase": "cgn"}, {"score": 0.003643807817480753, "phrase": "word-level_linguistic_information"}, {"score": 0.003541923233726252, "phrase": "conventional_language_models"}, {"score": 0.0033943947596098583, "phrase": "socio-situational_settings"}, {"score": 0.0033150974333333214, "phrase": "discourse-level_information"}, {"score": 0.0031248392333206297, "phrase": "language_register"}, {"score": 0.003030311503552644, "phrase": "sss"}, {"score": 0.0029039692582303904, "phrase": "different_subjects"}, {"score": 0.002822745894809142, "phrase": "pos"}, {"score": 0.002763261749602258, "phrase": "second_rnnlm"}, {"score": 0.0027307746993761035, "phrase": "main_rnnlm."}, {"score": 0.002679585860537941, "phrase": "recurrent_neural_network_tandem_language_model"}, {"score": 0.002623141457203352, "phrase": "high-quality_meta-information_labels"}, {"score": 0.0023085294226961213, "phrase": "language_modeling_research"}, {"score": 0.0021351033500114735, "phrase": "improved_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Recurrent neural networks", " Language models", " Part of Speech", " Social-situational setting", " Topic", " Sentence length"], "paper_abstract": "Due to their advantages over conventional n-gram language models, recurrent neural network language models (RNNLMS) recently have attracted a fair amount of research attention in the speech recognition community. In this paper, we explore one advantage of RNNLMS, namely, the ease with which they allow the integration of additional knowledge sources. We concentrate on features that provide complementary information w.r.t. the lexical identities of the words. We refer to such information as meta-information. We single out three cases and investigate their merits by means of N-best list re-scoring experiments on a challenging corpus of spoken Dutch (referred to as CGN) as well as on the English Wall Street Journal (WSJ) corpus. First, we look at Parts of Speech (POS) tags and lemmas, two sources of word-level linguistic information that are known to make a contribution to the performance of conventional language models. We confirm that RNNLMS can benefit from these sources as well. Second, we investigate socio-situational settings (SSSs) and topics, two sources of discourse-level information that are also known to benefit language models. SSSs are present in the CGN data, and can be seen as a proxy for the language register. For the purposes of our investigation, we assume that information on the SSS can be captured at the moment at which speech is recorded. Topics, i.e., treatments of different subjects, are present in the WSJ data. In order to predict POS, lemmas, sss and topic, a second RNNLM is coupled to the main RNNLM. We refer to this architecture as a recurrent neural network tandem language model (RNNTLM). Our experimental findings show that if high-quality meta-information labels are available, both word-level and discourse-level information improve performance of language models. Third, we investigate sentence length and word length (i.e., token size), two sources of intrinsic information that are readily available for exploitation because they are known at the time of re-scoring. Intrinsic information has been largely overlooked by language modeling research. The results of both experiments on CGN data and WSJ data show that integrating sentence length and word length can achieve improvement. RNNLMS allow these features to be incorporated with ease, and obtain improved performance. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Integrating meta-information into recurrent neural network language models", "paper_id": "WOS:000361576100005"}