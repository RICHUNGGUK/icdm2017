{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "living_brains"}, {"score": 0.004690305164360866, "phrase": "natural_evolution"}, {"score": 0.004539674961949176, "phrase": "ne"}, {"score": 0.004278807087784626, "phrase": "ne_algorithms"}, {"score": 0.004209202947762995, "phrase": "distinctive_capabilities"}, {"score": 0.004181678027929924, "phrase": "biological_brains"}, {"score": 0.004140726362519362, "phrase": "recently_introduced_hypercube-based_neuroevolution"}, {"score": 0.0036914757355166966, "phrase": "large_anns"}, {"score": 0.003631390978147703, "phrase": "high-dimensional_problems"}, {"score": 0.0030414140732789186, "phrase": "node_geometry"}, {"score": 0.003021502415143067, "phrase": "implicit_information"}, {"score": 0.0029431479135737177, "phrase": "hyperneat"}, {"score": 0.002866817775018445, "phrase": "explicit_placement"}, {"score": 0.0026581913054027663, "phrase": "varying_density"}, {"score": 0.0025137856138000014, "phrase": "maze_navigation"}, {"score": 0.0024891269874339553, "phrase": "modular_retina_domains"}, {"score": 0.002416589459425828, "phrase": "new_approach"}, {"score": 0.0024007585788965655, "phrase": "natural_properties"}, {"score": 0.002377206033025253, "phrase": "neural_topography"}, {"score": 0.002361632539339945, "phrase": "geometric_regularity"}, {"score": 0.002323142666222693, "phrase": "es-hyperneat's_compact_indirect_encoding"}, {"score": 0.0022406537579986842, "phrase": "desired_class"}, {"score": 0.002225972846167256, "phrase": "ann_topographies"}, {"score": 0.0021896891011033105, "phrase": "evolutionary_search"}, {"score": 0.0021682027434117095, "phrase": "main_conclusion"}, {"score": 0.0021049977753042253, "phrase": "neural_structures"}], "paper_keywords": ["Compositional pattern-producing networks", " indirect encoding", " HyperNEAT", " neuroevolution", " artificial neural networks", " generative and developmental systems"], "paper_abstract": "Intelligence in nature is the product of living brains, which are themselves the product of natural evolution. Although researchers in the field of neuroevolution (NE) attempt to recapitulate this process, artificial neural networks (ANNs) so far evolved through NE algorithms do not match the distinctive capabilities of biological brains. The recently introduced hypercube-based neuroevolution of augmenting topologies (HyperNEAT) approach narrowed this gap by demonstrating that the pattern of weights across the connectivity of an ANN can be generated as a function of its geometry, thereby allowing large ANNs to be evolved for high-dimensional problems. Yet the positions and number of the neurons connected through this approach must be decided a priori by the user and, unlike in living brains, cannot change during evolution. Evolvable-substrate HyperNEAT (ES-HyperNEAT), introduced in this article, addresses this limitation by automatically deducing the node geometry from implicit information in the pattern of weights encoded by HyperNEAT, thereby avoiding the need to evolve explicit placement. This approach not only can evolve the location of every neuron in the network, but also can represent regions of varying density, which means resolution can increase holistically over evolution. ES-HyperNEAT is demonstrated through multi-task, maze navigation, and modular retina domains, revealing that the ANNs generated by this new approach assume natural properties such as neural topography and geometric regularity. Also importantly, ES-HyperNEAT's compact indirect encoding can be seeded to begin with a bias toward a desired class of ANN topographies, which facilitates the evolutionary search. The main conclusion is that ES-HyperNEAT significantly expands the scope of neural structures that evolution can discover.", "paper_title": "An Enhanced Hypercube-Based Encoding for Evolving the Placement, Density, and Connectivity of Neurons", "paper_id": "WOS:000310571900001"}