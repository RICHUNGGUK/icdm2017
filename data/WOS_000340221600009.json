{"auto_keywords": [{"score": 0.04744030258255144, "phrase": "rf"}, {"score": 0.040087600795755705, "phrase": "majority_class"}, {"score": 0.03814399229076315, "phrase": "feature_subspace_selection"}, {"score": 0.00481495049065317, "phrase": "imbalanced_text_categorization"}, {"score": 0.004704699244406775, "phrase": "new_random_forest"}, {"score": 0.00465650868280802, "phrase": "based_ensemble_method"}, {"score": 0.004632597963417253, "phrase": "forestmer"}, {"score": 0.00457335448397295, "phrase": "imbalanced_text_categorization_problems"}, {"score": 0.004514865196864201, "phrase": "great_success"}, {"score": 0.004388796812234033, "phrase": "text_data"}, {"score": 0.00436625473701362, "phrase": "class_imbalance"}, {"score": 0.004332657586180943, "phrase": "relatively_new_challenge"}, {"score": 0.004244318303507975, "phrase": "rf_algorithm"}, {"score": 0.004190019394361629, "phrase": "simple_random_sampling"}, {"score": 0.003928728331703105, "phrase": "gini_measure"}, {"score": 0.0039085400187964196, "phrase": "data_splitting"}, {"score": 0.0038485929962942776, "phrase": "sensitive_and_bias"}, {"score": 0.0037700860163466933, "phrase": "inherent_complex_characteristics"}, {"score": 0.0037507098392196617, "phrase": "imbalanced_text_datasets"}, {"score": 0.0036836711953021485, "phrase": "new_approaches"}, {"score": 0.0036085162483496117, "phrase": "cut-point_choice"}, {"score": 0.0035807287211852947, "phrase": "node_splitting"}, {"score": 0.0034986389502471606, "phrase": "new_tree_induction_method"}, {"score": 0.0034360901783667573, "phrase": "subspace_selection"}, {"score": 0.0034184246667842105, "phrase": "splitting_criterion"}, {"score": 0.003374655868348244, "phrase": "imbalanced_text_data"}, {"score": 0.0033486631403337555, "phrase": "key_idea"}, {"score": 0.0031886139206755374, "phrase": "positive_features"}, {"score": 0.0031640496006559095, "phrase": "minority_class"}, {"score": 0.003115485445919747, "phrase": "negative_features"}, {"score": 0.0029589086332593674, "phrase": "term_weights"}, {"score": 0.0028761616290510724, "phrase": "adequate_informative_features"}, {"score": 0.0028393160727077735, "phrase": "majority_classes"}, {"score": 0.0027885217101617224, "phrase": "classical_rf_method"}, {"score": 0.0027527957803126775, "phrase": "support_vector_machines"}, {"score": 0.0026965813247045427, "phrase": "training_data"}, {"score": 0.0026827075165830193, "phrase": "smaller_and_more_balance_subsets"}, {"score": 0.00260766437999449, "phrase": "svm_classifiers"}, {"score": 0.0025875639387943155, "phrase": "data_partitions"}, {"score": 0.0024448072654601957, "phrase": "refined_feature_subspaces"}, {"score": 0.0024322256661297764, "phrase": "data_subsets"}, {"score": 0.002407255910639214, "phrase": "imbalanced_data"}, {"score": 0.0023702799605691597, "phrase": "tree_model"}, {"score": 0.0023398998792474993, "phrase": "text_categorization_task"}, {"score": 0.0023278569001363263, "phrase": "imbalanced_dataset"}, {"score": 0.002315875759858594, "phrase": "experimental_results"}, {"score": 0.0022803012379889595, "phrase": "ohsumed"}, {"score": 0.0021544587602892466, "phrase": "standard_random_forest_and_different_variants"}, {"score": 0.0021435025057303316, "phrase": "svm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Text categorization", " Imbalanced classification", " Random forests", " SVM", " Stratified sampling"], "paper_abstract": "In this paper, we propose a new random forest (RF) based ensemble method, FORESTMER, to solve the imbalanced text categorization problems. RF has shown great success in many real-world applications. However, the problem of learning from text data with class imbalance is a relatively new challenge that needs to be addressed. A RF algorithm tends to use a simple random sampling of features in building their decision trees. As a result, it selects many subspaces that contain few, if any, informative features for the minority class. Furthermore, the Gini measure for data splitting is considered to be skew sensitive and bias towards the majority class. Due to the inherent complex characteristics of imbalanced text datasets, learning RF from such data requires new approaches to overcome challenges related to feature subspace selection and cut-point choice while performing node splitting. To this end, we propose a new tree induction method that selects splits, both feature subspace selection and splitting criterion, for RF on imbalanced text data. The key idea is to stratify features into two groups and to generate effective term weighting for the features. One group contains positive features for the minority class and the other one contains the negative features for the majority class. Then, for feature subspace selection, we effectively select features from each group based on the term weights. The advantage of our approach is that each subspace contains adequate informative features for both minority and majority classes. One difference between our proposed tree induction method and the classical RF method is that our method uses Support Vector Machines (SVM) classifier to split the training data into smaller and more balance subsets at each tree node, and then successively retrains the SVM classifiers on the data partitions to refine the model while moving down the tree. In this way, we force the classifiers to learn from refined feature subspaces and data subsets to fit the imbalanced data better. Hence, the tree model becomes more robust for text categorization task with imbalanced dataset. Experimental results on various benchmark imbalanced text datasets (Reuters-21578, Ohsumed, and imbalanced 20 newsgroup) consistently demonstrate the effectiveness of our proposed FORESTEXTER method. The performance of our proposed approach is competitive against the standard random forest and different variants of SVM algorithms. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "FORESTEXTER: An efficient random forest algorithm for imbalanced text categorization", "paper_id": "WOS:000340221600009"}