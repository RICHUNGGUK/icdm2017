{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "decision_trees"}, {"score": 0.049347793578485837, "phrase": "imprecise_probabilities"}, {"score": 0.04886626441968084, "phrase": "uncertainty_measures"}, {"score": 0.004481132747216135, "phrase": "experimental_comparison"}, {"score": 0.004435363840720883, "phrase": "different_strategies"}, {"score": 0.003802135020791801, "phrase": "final_process"}, {"score": 0.0036678472439508484, "phrase": "previously_developed_schemes"}, {"score": 0.00332670230152448, "phrase": "root_node"}, {"score": 0.003275807170434343, "phrase": "information_rank"}, {"score": 0.003176333519010176, "phrase": "class_variable"}, {"score": 0.002986329509791943, "phrase": "missing_data"}, {"score": 0.002955782731675529, "phrase": "continuous_variables"}, {"score": 0.0026261108615242557, "phrase": "appropriate_approach"}, {"score": 0.002585906251354904, "phrase": "boosting_scheme"}, {"score": 0.002546315584783112, "phrase": "excellent_way"}, {"score": 0.002468938912850425, "phrase": "decision_tree"}, {"score": 0.002357249810661667, "phrase": "good_results"}, {"score": 0.0022856052476424344, "phrase": "standard_random_forest_classifier"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Imprecise probabilities", " Credal sets", " Uncertainty measures", " Supervised classification", " Decision trees", " Ensemble methods"], "paper_abstract": "In this paper, we present an experimental comparison among different strategies for combining decision trees built by means of imprecise probabilities and uncertainty measures. It has been proven that the combination or fusion of the information obtained from several classifiers can improve the final process of the classification. We use previously developed schemes, known as Bagging and Boosting, along with a new one based on the variation of the root node via the information rank of each feature of the class variable. To this end, we applied two different approaches to deal with missing data and continuous variables. We use a set of tests on the performance of the methods analyzed here, to show that, with the appropriate approach, the Boosting scheme constitutes an excellent way to combine this type of decision tree. It should be noted that it provides good results, even compared with a standard Random Forest classifier, a successful procedure very commonly used in the literature. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Ensembles of decision trees based on imprecise probabilities and uncertainty measures", "paper_id": "WOS:000321480800009"}