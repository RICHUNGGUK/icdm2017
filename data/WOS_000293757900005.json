{"auto_keywords": [{"score": 0.04508893681963011, "phrase": "active_learning"}, {"score": 0.015271462702187138, "phrase": "learning_problem"}, {"score": 0.011262789434085907, "phrase": "disagreement_coefficient"}, {"score": 0.00481495049065317, "phrase": "agnostic_active_learning"}, {"score": 0.004574709094672837, "phrase": "hypothesis_space"}, {"score": 0.0038946650372062783, "phrase": "active_learning_algorithm"}, {"score": 0.0037273513742154237, "phrase": "previous_works"}, {"score": 0.0035934073774126856, "phrase": "label_complexity"}, {"score": 0.003315374849824563, "phrase": "intrinsic_difficulty"}, {"score": 0.002992313172550772, "phrase": "classification_problems"}, {"score": 0.002905914725757534, "phrase": "classification_boundary"}, {"score": 0.0028014064098604093, "phrase": "data_distribution"}, {"score": 0.0026035011274172753, "phrase": "smooth_function"}, {"score": 0.002528300562452668, "phrase": "upper_and_lower_bounds"}, {"score": 0.002473325513637559, "phrase": "disagreement_coefficients"}, {"score": 0.0022817585639886883, "phrase": "existing_results"}, {"score": 0.002136087390959268, "phrase": "passive_supervised_learning"}, {"score": 0.0021049977753042253, "phrase": "smooth_problems"}], "paper_keywords": ["active learning", " disagreement coefficient", " label complexity", " smooth function"], "paper_abstract": "We study pool-based active learning in the presence of noise, that is, the agnostic setting. It is known that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have an advantage. Previous works have shown that the label complexity of active learning relies on the disagreement coefficient which often characterizes the intrinsic difficulty of the learning problem. In this paper, we study the disagreement coefficient of classification problems for which the classification boundary is smooth and the data distribution has a density that can be bounded by a smooth function. We prove upper and lower bounds for the disagreement coefficients of both finitely and infinitely smooth problems. Combining with existing results, it shows that active learning is superior to passive supervised learning for smooth problems.", "paper_title": "Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning", "paper_id": "WOS:000293757900005"}