{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "hardware_support"}, {"score": 0.0053730940315998114, "phrase": "network_interfaces"}, {"score": 0.00470862827326028, "phrase": "mpsoc_applications"}, {"score": 0.004678735712556387, "phrase": "mpi"}, {"score": 0.00458997570994803, "phrase": "parallel_programming_model"}, {"score": 0.0044742996274375404, "phrase": "multi-processor_systems"}, {"score": 0.004292484293656852, "phrase": "collective_communication"}, {"score": 0.004170940184489072, "phrase": "good_speedups"}, {"score": 0.004144399274022276, "phrase": "previous_work"}, {"score": 0.004078780515171644, "phrase": "synthetic_communication-only_benchmarks"}, {"score": 0.003753882510162702, "phrase": "network_level"}, {"score": 0.0035216195857272403, "phrase": "good_estimation"}, {"score": 0.003465827053421054, "phrase": "actual_applications"}, {"score": 0.0031190537176106386, "phrase": "five_parallel_application_kernels"}, {"score": 0.0030991854367616737, "phrase": "varying_computation-to-communication_ratios"}, {"score": 0.003011328068286321, "phrase": "performance_evaluation"}, {"score": 0.002797922160172791, "phrase": "lower_computation-to-communication_ratios"}, {"score": 0.00269263690712392, "phrase": "efficient_collective_communications"}, {"score": 0.00266693888510356, "phrase": "better_scalability"}, {"score": 0.0025830324526904427, "phrase": "clock_frequency"}, {"score": 0.0025665698207861533, "phrase": "resource_usage"}, {"score": 0.002485813299166851, "phrase": "reasonable_scalability"}, {"score": 0.0024699687468983686, "phrase": "resource_utilization"}, {"score": 0.0022729292542096077, "phrase": "added_hardware"}, {"score": 0.002251227801482381, "phrase": "total_energy"}, {"score": 0.0021943626624131413, "phrase": "actual_speedup"}, {"score": 0.0021595521075096808, "phrase": "application_kernels"}], "paper_keywords": ["Design", " Experimentation", " Performance", " MPI", " FPGA", " parallel computing", " multiprocessor", " network-on-chip"], "paper_abstract": "MPI has been used as a parallel programming model for supercomputers and clusters and recently in Multi-Processor Systems-on-Chip (MPSaC). One component of MPI is collective communication and its performance is key for certain parallel applications to achieve good speedups. Previous work showed that, with synthetic communication-only benchmarks, communication improvements of up to 11.4-fold and 22-fold for broadcast and reduce operations, respectively, can be achieved by providing hardware support at the network level in a Network-on-Chip (NoC). However, these numbers do not provide a good estimation of the advantage for actual applications, as there are other factors that affect performance besides communications, such as computation. To this end, we extend our previous work by evaluating the impact of hardware support over a set of five parallel application kernels of varying computation-to-communication ratios. By introducing some useful computation to the performance evaluation, we obtain more representative results of the benefits of adding hardware support for broadcast and reduce operations. The experiments show that applications with lower computation-to-communication ratios benefit the most from hardware support as they highly depend on efficient collective communications to achieve better scalability. We also extend our work by doing more analysis on clock frequency, resource usage, power, and energy. The results show reasonable scalability for resource utilization and power in the network interfaces as the number of channels increases and that, even though more power is dissipated in the network interfaces due to the added hardware, the total energy used can still be less if the actual speedup is sufficient. The application kernels are executed in a 24-embedded-processor system distributed across four FPGAs.", "paper_title": "Benefits of Adding Hardware Support for Broadcast and Reduce Operations in MPSoC Applications", "paper_id": "WOS:000342393800002"}