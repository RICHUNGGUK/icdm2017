{"auto_keywords": [{"score": 0.004724878518219719, "phrase": "multithreaded_caches"}, {"score": 0.004528301408585875, "phrase": "memory_performance"}, {"score": 0.0044435677598780796, "phrase": "focal_point"}, {"score": 0.004401794742140806, "phrase": "microprocessor_research"}, {"score": 0.004278807087784626, "phrase": "modern_architectures"}, {"score": 0.004043003243656031, "phrase": "almost_every_microprocessor"}, {"score": 0.003948613642680196, "phrase": "on-chip_storage"}, {"score": 0.0037486121194392564, "phrase": "memory_latency"}, {"score": 0.0036610702290043387, "phrase": "limited_memory_bandwidth"}, {"score": 0.002987557906013826, "phrase": "highly_parallel_workloads"}, {"score": 0.0029594318330247614, "phrase": "significant_locality"}, {"score": 0.002917736974519881, "phrase": "massively_multithreaded_gpu."}, {"score": 0.0028495431343111897, "phrase": "memory_access_stream"}, {"score": 0.0028093921138602606, "phrase": "on-chip_caches"}, {"score": 0.002769805263310232, "phrase": "direct_result"}, {"score": 0.0026922926476971453, "phrase": "hardware_thread_scheduler"}, {"score": 0.0026293540353514075, "phrase": "hardware_scheduling_technique"}, {"score": 0.0025436978068459565, "phrase": "memory_system"}, {"score": 0.0023694065215394593, "phrase": "significant_performance_improvement"}, {"score": 0.0023470862747075228, "phrase": "previously_proposed_scheduling_mechanisms"}, {"score": 0.0022385926290967263, "phrase": "cache_management_technique"}, {"score": 0.0022070310530794097, "phrase": "cache_hit_rate"}, {"score": 0.002155412043665635, "phrase": "lru_replacement_policy"}, {"score": 0.0021049977753042253, "phrase": "optimal_cache_replacement_policy"}], "paper_keywords": [""], "paper_abstract": "The gap between processor and memory performance has become a focal point for microprocessor research and development over the past three decades. Modern architectures use two orthogonal approaches to help alleviate this issue: (1) Almost every microprocessor includes some form of on-chip storage, usually in the form of caches, to decrease memory latency and make more effective use of limited memory bandwidth. (2) Massively multithreaded architectures, such as graphics processing units (GPUs), attempt to hide the high latency to memory by rapidly switching between many threads directly in hardware. This paper explores the intersection of these two techniques. We study the effect of accelerating highly parallel workloads with significant locality on a massively multithreaded GPU. We observe that the memory access stream seen by on-chip caches is the direct result of decisions made by the hardware thread scheduler. Our work proposes a hardware scheduling technique that reacts to feedback from the memory system to create a more cache-friendly access stream. We evaluate our technique using simulations and show a significant performance improvement over previously proposed scheduling mechanisms. We demonstrate the effectiveness of scheduling as a cache management technique by comparing cache hit rate using our scheduler and an LRU replacement policy against other scheduling techniques using an optimal cache replacement policy.", "paper_title": "Learning Your Limit: Managing Massively Multithreaded Caches Through Scheduling", "paper_id": "WOS:000348301800030"}