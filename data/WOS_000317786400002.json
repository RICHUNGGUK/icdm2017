{"auto_keywords": [{"score": 0.036338654443170804, "phrase": "possibilistic_classifiers"}, {"score": 0.022419500566078915, "phrase": "normality_assumption"}, {"score": 0.010612387000973441, "phrase": "numerical_data"}, {"score": 0.009221610757026927, "phrase": "possibility_distributions"}, {"score": 0.008531463918651479, "phrase": "bayesian_classifiers"}, {"score": 0.007455899710158644, "phrase": "classical_or_flexible_bayesian_classifiers"}, {"score": 0.004786107405332454, "phrase": "naive_bayesian_classifiers"}, {"score": 0.004714749768410393, "phrase": "independence_hypotheses"}, {"score": 0.004244040105917499, "phrase": "poor_data"}, {"score": 0.0040691336206915675, "phrase": "naive_possibilistic_classifiers"}, {"score": 0.0040447436648907665, "phrase": "npc"}, {"score": 0.003984391456005223, "phrase": "possibility_theory"}, {"score": 0.003820144626025562, "phrase": "classification_tasks"}, {"score": 0.003740568332108757, "phrase": "possibilistic_classification"}, {"score": 0.0036958398682001015, "phrase": "existing_npc_deal"}, {"score": 0.003662643573658492, "phrase": "categorical_attributes"}, {"score": 0.003543445557071389, "phrase": "continuous_data"}, {"score": 0.0034178160236983226, "phrase": "first_one"}, {"score": 0.0033365390352128065, "phrase": "probability-possibility_transformation"}, {"score": 0.0033173450817240123, "phrase": "gaussian"}, {"score": 0.0031797191359738356, "phrase": "second_one"}, {"score": 0.003132236287543072, "phrase": "direct_interpretation"}, {"score": 0.0030947594580569854, "phrase": "possibilistic_formats"}, {"score": 0.003021141590548526, "phrase": "data_values"}, {"score": 0.0030030115662983956, "phrase": "different_ways"}, {"score": 0.002870449780891857, "phrase": "better_capability"}, {"score": 0.0028446460307503343, "phrase": "new_instances"}, {"score": 0.0025447921872619435, "phrase": "nearest-neighbour_heuristics"}, {"score": 0.0024917159298274297, "phrase": "proposed_possibilistic_classifiers"}, {"score": 0.0024693083555454133, "phrase": "available_information"}, {"score": 0.0023460767171603054, "phrase": "benchmarks_databases"}, {"score": 0.0022492129111567824, "phrase": "flexible_possibilistic_classifiers"}, {"score": 0.002175913488493802, "phrase": "proximity-based_possibilistic_classifiers"}, {"score": 0.002124106476971252, "phrase": "hybrid_possibilistic_classification"}, {"score": 0.0021049977753042253, "phrase": "good_ability"}], "paper_keywords": ["Naive Possibilistic Classifier", " Possibility theory", " Proximity", " Gaussian distribution", " Naive Bayesian Classifier", " Numerical data"], "paper_abstract": "Naive Bayesian Classifiers, which rely on independence hypotheses, together with a normality assumption to estimate densities for numerical data, are known for their simplicity and their effectiveness. However, estimating densities, even under the normality assumption, may be problematic in case of poor data. In such a situation, possibility distributions may provide a more faithful representation of these data. Naive Possibilistic Classifiers (NPC), based on possibility theory, have been recently proposed as a counterpart of Bayesian classifiers to deal with classification tasks. There are only few works that treat possibilistic classification and most of existing NPC deal only with categorical attributes. This work focuses on the estimation of possibility distributions for continuous data. In this paper we investigate two kinds of possibilistic classifiers. The first one is derived from classical or flexible Bayesian classifiers by applying a probability-possibility transformation to Gaussian distributions, which introduces some further tolerance in the description of classes. The second one is based on a direct interpretation of data in possibilistic formats that exploit an idea of proximity between data values in different ways, which provides a less constrained representation of them. We show that possibilistic classifiers have a better capability to detect new instances for which the classification is ambiguous than Bayesian classifiers, where probabilities may be poorly estimated and illusorily precise. Moreover, we propose, in this case, an hybrid possibilistic classification approach based on a nearest-neighbour heuristics to improve the accuracy of the proposed possibilistic classifiers when the available information is insufficient to choose between classes. Possibilistic classifiers are compared with classical or flexible Bayesian classifiers on a collection of benchmarks databases. The experiments reported show the interest of possibilistic classifiers. In particular, flexible possibilistic classifiers perform well for data agreeing with the normality assumption, while proximity-based possibilistic classifiers outperform others in the other cases. The hybrid possibilistic classification exhibits a good ability for improving accuracy.", "paper_title": "Possibilistic classifiers for numerical data", "paper_id": "WOS:000317786400002"}