{"auto_keywords": [{"score": 0.04411571818342814, "phrase": "existing_grid_simulators"}, {"score": 0.00481495049065317, "phrase": "ndgf_resources"}, {"score": 0.004542441734438654, "phrase": "new_ideas"}, {"score": 0.004494570360108983, "phrase": "job_scheduling"}, {"score": 0.004447201233380302, "phrase": "data_transfer_algorithms"}, {"score": 0.004400329132129292, "phrase": "large-scale_grid_systems"}, {"score": 0.0041074234667687875, "phrase": "realistic_workload"}, {"score": 0.0037935066882208235, "phrase": "in-depth_study"}, {"score": 0.003753498345597498, "phrase": "representative_grid_workloads"}, {"score": 0.003522124318486352, "phrase": "atlas_experiment"}, {"score": 0.003485609468499667, "phrase": "cern"}, {"score": 0.0034344835464293435, "phrase": "lhc"}, {"score": 0.003304965389155549, "phrase": "nordic_data_grid_facility"}, {"score": 0.0032708500016216425, "phrase": "atlas"}, {"score": 0.0031845067058084583, "phrase": "biggest_grid_technology_users"}, {"score": 0.0031342307827666675, "phrase": "extreme_demands"}, {"score": 0.003101153870424188, "phrase": "cpu_power"}, {"score": 0.0030684249587688826, "phrase": "data_volume"}, {"score": 0.0029098743621130004, "phrase": "data_sample"}, {"score": 0.0027594936602962075, "phrase": "processor_time"}, {"score": 0.0025892330566358503, "phrase": "synthetic_workload"}, {"score": 0.0022434880661725493, "phrase": "grid_system_designers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Grid computing", " Workload analysis", " Data model", " Data transfer"], "paper_abstract": "Evaluating new ideas for job scheduling or data transfer algorithms in large-scale grid systems is known to be notoriously challenging. Existing grid simulators expect to receive a realistic workload as an input. Such input is difficult to obtain in the absence of an in-depth study of representative grid workloads. In this work, we analyze the workload of the ATLAS experiment at CERN at the LHC, processed on the resources of Nordic Data Grid Facility. ATLAS is one of the biggest grid technology users, with extreme demands for CPU power, data volume and bandwidth. The analysis is based on the data sample with similar to 1.6 million jobs, 3029 TB of data transfer, and 873 years of processor time. Our additional contributions are (a) scalable workload models that can be used to generate a synthetic workload for a given number of jobs, (b) an open-source workload generator software integrated with existing grid simulators, and (c) suggestions for grid system designers based on the insights of our analysis. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "ATLAS grid workload on NDGF resources: Analysis, modeling, and workload generation", "paper_id": "WOS:000352174300003"}