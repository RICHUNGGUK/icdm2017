{"auto_keywords": [{"score": 0.04629659038512855, "phrase": "space-time_interest_points"}, {"score": 0.00481495049065317, "phrase": "appearance_and_distribution_information"}, {"score": 0.004692980391580608, "phrase": "action_recognition"}, {"score": 0.004574085781766281, "phrase": "existing_action_recognition_methods"}, {"score": 0.0041490029730869345, "phrase": "appearance-based_descriptors"}, {"score": 0.0035932440062595252, "phrase": "discriminative_power"}, {"score": 0.0035565113234273926, "phrase": "individual_local_space-time_descriptors"}, {"score": 0.0034663083890589235, "phrase": "potentially_useful_information"}, {"score": 0.003413284742632989, "phrase": "global_spatio-temporal_distribution"}, {"score": 0.0032256881625734777, "phrase": "novel_action_representation_method"}, {"score": 0.0031277316566463978, "phrase": "existing_interest_point"}, {"score": 0.0028807718824400697, "phrase": "interest_points"}, {"score": 0.002836679611901583, "phrase": "multiple_temporal_scales"}, {"score": 0.0027505037762786087, "phrase": "proposed_spatio-temporal_distribution_representation"}, {"score": 0.00263965046277432, "phrase": "conventional_bag"}, {"score": 0.0026126405272417783, "phrase": "words_representation"}, {"score": 0.002546315584783112, "phrase": "feature_fusion_method"}, {"score": 0.00250732953434891, "phrase": "multiple_kernel_learning"}, {"score": 0.0024186634600764766, "phrase": "weizmann"}, {"score": 0.0022390533383717715, "phrase": "view_angle"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Action recognition", " Clouds of Points", " Feature fusion", " Interest points detection", " Multiple Kernel Learning"], "paper_abstract": "Most of the existing action recognition methods represent actions as bags of space-time interest points. Specifically, space-time interest points are detected from the video and described using appearance-based descriptors. Each descriptor is then classified as a video-word and a histogram of these video-words is used for recognition. These methods therefore rely solely on the discriminative power of individual local space-time descriptors, whilst ignoring the potentially useful information about the global spatio-temporal distribution of interest points. In this paper we propose a novel action representation method which differs significantly from the existing interest point based representation in that only the global distribution information of interest points is exploited. In particular, holistic features from clouds of interest points accumulated over multiple temporal scales are extracted. Since the proposed spatio-temporal distribution representation contains different but complementary information to the conventional Bag of Words representation, we formulate a feature fusion method based on Multiple Kernel Learning. Experiments using the KTH and WEIZMANN datasets demonstrate that our approach outperforms most existing methods, in particular under occlusion and changes in view angle, clothing, and carrying condition. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Fusing appearance and distribution information of interest points for action recognition", "paper_id": "WOS:000297906200024"}