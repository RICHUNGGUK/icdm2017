{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "explainable_discrimination"}, {"score": 0.049661562626444285, "phrase": "illegal_discrimination"}, {"score": 0.03869234324830671, "phrase": "education_level"}, {"score": 0.004745179442957631, "phrase": "automated_decision_making"}, {"score": 0.004568448239746845, "phrase": "supervised_learning"}, {"score": 0.004347188587990755, "phrase": "discrimination-aware_techniques"}, {"score": 0.004296697688671517, "phrase": "sensitive_attribute"}, {"score": 0.0042344044308802445, "phrase": "discrimination-free_classifiers"}, {"score": 0.004076618798243399, "phrase": "existing_techniques"}, {"score": 0.003712727798045068, "phrase": "job_application"}, {"score": 0.0036481919610079783, "phrase": "job_candidate"}, {"score": 0.0034815088791360443, "phrase": "acceptance_rates"}, {"score": 0.0033911075605164804, "phrase": "gender_discrimination"}, {"score": 0.003303045833889043, "phrase": "different_levels"}, {"score": 0.0026524883963026987, "phrase": "refined_notion"}, {"score": 0.002637013652072109, "phrase": "conditional_non-discrimination"}, {"score": 0.0025164138447059023, "phrase": "sensitive_groups"}, {"score": 0.002380327476198495, "phrase": "algorithmic_techniques"}, {"score": 0.0022981869222589058, "phrase": "experimental_evaluation"}, {"score": 0.0022847744545661216, "phrase": "synthetic_and_real-world_classification_datasets"}, {"score": 0.0022188745774815847, "phrase": "old_ones"}, {"score": 0.0021994769642365714, "phrase": "new_context"}, {"score": 0.002136032060238817, "phrase": "undesirable_discrimination"}, {"score": 0.0021049977753042253, "phrase": "explainable_differences"}], "paper_keywords": ["Classification", " Independence", " Discrimination-aware data mining"], "paper_abstract": "Recently, the following discrimination-aware classification problem was introduced. Historical data used for supervised learning may contain discrimination, for instance, with respect to gender. The question addressed by discrimination-aware techniques is, given sensitive attribute, how to train discrimination-free classifiers on such historical data that are discriminative, with respect to the given sensitive attribute. Existing techniques that deal with this problem aim at removing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes. For example, in a job application, the education level of a job candidate could be such an explainable attribute. If the data contain many highly educated male candidates and only few highly educated women, a difference in acceptance rates between woman and man does not necessarily reflect gender discrimination, as it could be explained by the different levels of education. Even though selecting on education level would result in more males being accepted, a difference with respect to such a criterion would not be considered to be undesirable, nor illegal. Current state-of-the-art techniques, however, do not take such gender-neutral explanations into account and tend to overreact and actually start reverse discriminating, as we will show in this paper. Therefore, we introduce and analyze the refined notion of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sensitive groups can be explainable and are hence tolerable. Therefore, we develop methodology for quantifying the explainable discrimination and algorithmic techniques for removing the illegal discrimination when one or more attributes are considered as explanatory. Experimental evaluation on synthetic and real-world classification datasets demonstrates that the new techniques are superior to the old ones in this new context, as they succeed in removing almost exclusively the undesirable discrimination, while leaving the explainable differences unchanged, allowing for differences in decisions as long as they are explainable.", "paper_title": "Quantifying explainable discrimination and removing illegal discrimination in automated decision making", "paper_id": "WOS:000318374200005"}