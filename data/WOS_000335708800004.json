{"auto_keywords": [{"score": 0.02598468129300403, "phrase": "data_editing_technique"}, {"score": 0.015588521975528449, "phrase": "semi-supervised_learning"}, {"score": 0.012690798578611617, "phrase": "class_probability_values"}, {"score": 0.011716278943360343, "phrase": "unlabeled_instances"}, {"score": 0.00481495049065317, "phrase": "class_probability_estimations"}, {"score": 0.004714023408114857, "phrase": "popular_machine"}, {"score": 0.0044237033788643715, "phrase": "unlabeled_examples"}, {"score": 0.004012751049253625, "phrase": "fixed_number"}, {"score": 0.00337224813954299, "phrase": "negative_effect"}, {"score": 0.0031375577552605533, "phrase": "simple_method"}, {"score": 0.0030071857666598193, "phrase": "different_probabilities"}, {"score": 0.002944039482467119, "phrase": "distance_metric"}, {"score": 0.0028944757265206332, "phrase": "labeled_instances"}, {"score": 0.0027978304423349246, "phrase": "class_membership"}, {"score": 0.0026475981474045414, "phrase": "unique_probability_value"}, {"score": 0.0025375331660958665, "phrase": "higher-quality_examples"}, {"score": 0.0023607968604629393, "phrase": "experimental_results"}, {"score": 0.0021870332034407817, "phrase": "first_distance_metric"}], "paper_keywords": ["Co-training", " Semi-supervised learning", " Ensemble learning", " Class probability", " Distance metric", " Data editing"], "paper_abstract": "Semi-supervised learning is a popular machine learning technique where only a small number of labeled examples are available and a large pool of unlabeled examples can be obtained easily. In co-training by committee, a paradigm of semi-supervised learning, it is necessary to pick out a fixed number of most confident examples according to the ranking of class probability values at each iteration. Unfortunately, the class probability values may repeat, which results in the problem that some unlabeled instances share the same probability and will be picked out randomly. This brings a negative effect on the improvement of the performance of classifiers. In this paper, we propose a simple method to deal with this problem under the intuition that different probabilities are crucial. The distance metric between unlabeled instances and labeled instances can be combined with the probabilities of class membership of committee. Two distance metrics are considered to assign each unlabeled example a unique probability value. In order to prove that our method can get higher-quality examples and reduce the introduction of noise, a data editing technique is used to compare with our method. Experimental results verify the effectiveness of our method and the data editing technique, and also confirm that the method for the first distance metric is generally better than the data editing technique. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Improve the performance of co-training by committee with refinement of class probability estimations", "paper_id": "WOS:000335708800004"}