{"auto_keywords": [{"score": 0.036247390364484336, "phrase": "voc_datasets"}, {"score": 0.03185670102915354, "phrase": "future_challenges"}, {"score": 0.03110598817809555, "phrase": "submitted_algorithms"}, {"score": 0.00444241196374042, "phrase": "ground_truth_annotation"}, {"score": 0.004411406624893529, "phrase": "standardised_evaluation_software"}, {"score": 0.004200319352919505, "phrase": "five_challenges"}, {"score": 0.0033564619316337634, "phrase": "weak_points"}, {"score": 0.0033213460816510685, "phrase": "current_generation"}, {"score": 0.0032522091171602557, "phrase": "challenge_designers"}, {"score": 0.0028464607144683247, "phrase": "novel_evaluation_methods"}, {"score": 0.0028166655005289073, "phrase": "bootstrapping_method"}, {"score": 0.002662932198158107, "phrase": "normalised_average_precision"}, {"score": 0.0025711283781017424, "phrase": "different_proportions"}, {"score": 0.0025531499687505483, "phrase": "positive_instances"}, {"score": 0.0024651215047303593, "phrase": "multiple_algorithms"}, {"score": 0.0024307637980373552, "phrase": "hard_and_easy_images"}, {"score": 0.0023469450817019, "phrase": "joint_classifier"}, {"score": 0.0022110183535859374, "phrase": "community's_progress"}, {"score": 0.0021573582896732657, "phrase": "hoiem_et_al"}, {"score": 0.0021198271568336063, "phrase": "european_conference"}, {"score": 0.0021049977753042253, "phrase": "computer_vision"}], "paper_keywords": ["Database", " Benchmark", " Object recognition", " Object detection", " Segmentation"], "paper_abstract": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008-2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.", "paper_title": "The PASCAL Visual Object Classes Challenge: A Retrospective", "paper_id": "WOS:000348345400006"}