{"auto_keywords": [{"score": 0.02531970418094009, "phrase": "exponential_priors"}, {"score": 0.00481495049065317, "phrase": "maximum_entropy_language_models"}, {"score": 0.004764220976404241, "phrase": "movie_review_subjectivity_analysis"}, {"score": 0.004714023408114857, "phrase": "document_subjectivity_analysis"}, {"score": 0.004615202066192795, "phrase": "important_aspect"}, {"score": 0.004566567411073082, "phrase": "web_text_content_mining"}, {"score": 0.004400329132129292, "phrase": "traditional_text_categorization"}, {"score": 0.004021267064832957, "phrase": "semantic_information"}, {"score": 0.0034482025128821548, "phrase": "useful_and_meaningful_language_features"}, {"score": 0.003287482882222731, "phrase": "appropriate_language_models"}, {"score": 0.0032184697893776052, "phrase": "special_task"}, {"score": 0.003150900886669123, "phrase": "first_issue"}, {"score": 0.003036040409062437, "phrase": "local-weighting_strategy"}, {"score": 0.0029565620771207003, "phrase": "language_features"}, {"score": 0.0028487656293168795, "phrase": "different_orders"}, {"score": 0.002744888626958772, "phrase": "second_issue"}, {"score": 0.002687235334499192, "phrase": "maximum_entropy"}, {"score": 0.0026307897899760383, "phrase": "modeling_methods"}, {"score": 0.0025348408238837655, "phrase": "classical_maxent_models"}, {"score": 0.002429451930463481, "phrase": "improved_models"}, {"score": 0.0024055854129242118, "phrase": "gaussian"}, {"score": 0.0023408288327263316, "phrase": "detailed_experiments"}, {"score": 0.0022434880661725493, "phrase": "well_selected_and_weighted_language_features"}, {"score": 0.0021049977753042253, "phrase": "text_subjectivity_analysis_task"}], "paper_keywords": ["exponential prior", " language model", " maximum entropy", " n-gram", " subjectivity analysis"], "paper_abstract": "Document subjectivity analysis has become an important aspect of web text content mining. This problem is similar to traditional text categorization, thus many related classification techniques can be adapted here. However, there is one significant difference that more language or semantic information is required for better estimating the subjectivity of a document. Therefore, in this paper, our focuses are mainly on two aspects. One is how to extract useful and meaningful language features, and the other is how to construct appropriate language models efficiently for this special task. For the first issue, we conduct a Global-Filtering and Local-Weighting strategy to select and evaluate language features in a series of n-grams with different orders and within various distance-windows. For the second issue, we adopt Maximum Entropy (MaxEnt) modeling methods to construct our language model framework. Besides the classical MaxEnt models, we have also constructed two kinds of improved models with Gaussian and exponential priors respectively. Detailed experiments given in this paper show that with well selected and weighted language features, MaxEnt models with exponential priors are significantly more suitable for the text subjectivity analysis task.", "paper_title": "Constructing maximum entropy language models for movie review subjectivity analysis", "paper_id": "WOS:000254050000008"}