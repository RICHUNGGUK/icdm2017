{"auto_keywords": [{"score": 0.049698358871674404, "phrase": "object_tracking"}, {"score": 0.032046968361965765, "phrase": "posterior_probability_map"}, {"score": 0.00481495049065317, "phrase": "neural_tracker"}, {"score": 0.004719816332271661, "phrase": "fundamental_computer_vision_problem"}, {"score": 0.004517055580214742, "phrase": "behavior_analysis"}, {"score": 0.0044277810648955624, "phrase": "main_challenge"}, {"score": 0.004375061788023814, "phrase": "object_tracking_problem"}, {"score": 0.004322967486451877, "phrase": "dynamic_change"}, {"score": 0.004104269996872886, "phrase": "online_learning_neural_tracker"}, {"score": 0.003789137457228946, "phrase": "target_modeling"}, {"score": 0.003714195746979591, "phrase": "neural_algorithm"}, {"score": 0.003669941438286456, "phrase": "risk_sensitive_loss_function"}, {"score": 0.0035544816252832375, "phrase": "sample_imbalance"}, {"score": 0.0034702685641070283, "phrase": "region-based_features"}, {"score": 0.0034426417353349567, "phrase": "region-based_color_moments"}, {"score": 0.003334309053314909, "phrase": "pixel_level"}, {"score": 0.0033077610063168093, "phrase": "smaller_mobile_objects"}, {"score": 0.0031654684543975077, "phrase": "proposed_neural_classifier"}, {"score": 0.002981216610222911, "phrase": "online_learning_neural_classifier"}, {"score": 0.0028643622911111942, "phrase": "computational_burden"}, {"score": 0.002841545331522694, "phrase": "online_adaptation"}, {"score": 0.0028076592725295646, "phrase": "tracked_object"}, {"score": 0.002752075650871066, "phrase": "estimated_posterior_probability_map"}, {"score": 0.0026547761341670505, "phrase": "bounding_box"}, {"score": 0.0026126405272417783, "phrase": "scale_change"}, {"score": 0.0025918235228843444, "phrase": "improper_initialization"}, {"score": 0.0025101968585850474, "phrase": "proposed_olnt"}, {"score": 0.0024901940672239806, "phrase": "rapid_illumination_variation"}, {"score": 0.0023357913211723884, "phrase": "benchmark_video_sequences"}, {"score": 0.002244178699939059, "phrase": "well-known_trackers"}, {"score": 0.0021561514733516676, "phrase": "proposed_tracker"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Object tracking", " Neural classifier", " Gaussian activation function", " Posterior probability map", " Risk sensitive hinge loss function"], "paper_abstract": "Object tracking is a fundamental computer vision problem and is required for many high-level tasks such as activity recognition, behavior analysis and surveillance. The main challenge in the object tracking problem is the dynamic change in object/background appearance, illumination, shape and occlusion. We present an online learning neural tracker (OLNT) to differentiate the object from the background and also adapt to changes in object/background dynamics. For target modeling and object tracking, a neural algorithm based on risk sensitive loss function is proposed to handle issues related to sample imbalance and dynamics of object. Region-based features like region-based color moments for larger mobile objects and color/texture features at pixel level for smaller mobile objects are used to discriminate the object from background. The proposed neural classifier automatically determines the number of neurons required to estimate the posterior probability map. In the online learning neural classifier, only one neuron parameter is updated per tracker to reduce the computational burden during online adaptation. The tracked object is represented using an estimated posterior probability map. The posterior probability map is used to adapt the bounding box to handle the scale change and improper initialization. For illustrating the advantage of the proposed OLNT under rapid illumination variation. change in appearance, scale/size change, and occlusion, we present results from benchmark video sequences. Finally, we also present the comparison with well-known trackers in the literature and highlight the advantage of the proposed tracker. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Online learning neural tracker", "paper_id": "WOS:000290838600022"}