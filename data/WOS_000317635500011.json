{"auto_keywords": [{"score": 0.03644451033122436, "phrase": "policy_sharing"}, {"score": 0.015603376007754912, "phrase": "decision_trees"}, {"score": 0.011758926484520127, "phrase": "state_aggregation"}, {"score": 0.00481495049065317, "phrase": "multiple_mobile_robots"}, {"score": 0.004743164872278112, "phrase": "reinforcement_learning"}, {"score": 0.00448332888936155, "phrase": "continual_learning"}, {"score": 0.004399905285945236, "phrase": "dynamic_operating_environment"}, {"score": 0.004301834610818624, "phrase": "cooperative_multi-agent_systems"}, {"score": 0.00422177384286051, "phrase": "individual_agent"}, {"score": 0.004020492138367616, "phrase": "individual_agents"}, {"score": 0.003729335910488339, "phrase": "proposed_learning_algorithm"}, {"score": 0.003511613254403125, "phrase": "decision_tree"}, {"score": 0.0034332736827447654, "phrase": "multiple_agents"}, {"score": 0.003344076203780408, "phrase": "different_decision_trees"}, {"score": 0.0032817813775647756, "phrase": "lookup_tables"}, {"score": 0.003220643255032943, "phrase": "homogeneous_structure"}, {"score": 0.0030901063954739375, "phrase": "heterogeneous_structure"}, {"score": 0.0029648446274266765, "phrase": "cooperative_agents"}, {"score": 0.0028233196851239753, "phrase": "entire_trees"}, {"score": 0.0027916287357898544, "phrase": "proposed_scheme"}, {"score": 0.0027499252661633525, "phrase": "entire_decision_tree"}, {"score": 0.0025124343366192954, "phrase": "proposed_method"}, {"score": 0.0024842242921857705, "phrase": "hyper_decision_tree"}, {"score": 0.002447101794397243, "phrase": "large_amount"}, {"score": 0.0023745087648471613, "phrase": "shared_nodes"}, {"score": 0.00223570488212345, "phrase": "proposed_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Multi-agent", " Cooperation", " Sharing", " Reinforcement learning", " Mobile robot"], "paper_abstract": "Reinforcement learning is one of the more prominent machine learning technologies, because of its unsupervised learning structure and its ability to produce continual learning, even in a dynamic operating environment. Applying this learning to cooperative multi-agent systems not only allows each individual agent to learn from its own experience, but also offers the opportunity for the individual agents to learn from other agents in the system, in order to increase the speed of learning. In the proposed learning algorithm, an agent stores its experience in terms of a state aggregation, by use of a decision tree, such that policy sharing between multiple agents is eventually accomplished by merging the different decision trees of peers. Unlike lookup tables, which have a homogeneous structure for state aggregation, decision trees carried with in agents have a heterogeneous structure. The method detailed in this study allows policy sharing between cooperative agents by means merging their trees into a hyper-structure, instead of forcefully merging entire trees. The proposed scheme initially allows the entire decision tree to be translated from one agent to others. Based on the evidence, only partial leaf nodes have useful experience for use in policy sharing. The proposed method induces a hyper decision tree by using a large amount of samples that are sampled from the shared nodes. The results from simulations in a multi-agent cooperative domain illustrate that the proposed algorithms perform better than the algorithm that does not allow sharing. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Policy sharing between multiple mobile robots using decision trees", "paper_id": "WOS:000317635500011"}