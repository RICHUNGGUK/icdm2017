{"auto_keywords": [{"score": 0.0400054090969077, "phrase": "hmm"}, {"score": 0.03753985202940138, "phrase": "training_data"}, {"score": 0.03351976915027793, "phrase": "cnn"}, {"score": 0.0251977043321025, "phrase": "denoised_mfccs"}, {"score": 0.00481495049065317, "phrase": "deep_learning"}, {"score": 0.004608284537368354, "phrase": "reliable_speech_recognition"}, {"score": 0.004394350285140955, "phrase": "cautious_selection"}, {"score": 0.004362326712297089, "phrase": "sensory_features"}, {"score": 0.004283280680222718, "phrase": "high_recognition_performance"}, {"score": 0.004221072004655573, "phrase": "machine-learning_community"}, {"score": 0.004190305872545087, "phrase": "deep_learning_approaches"}, {"score": 0.0041294419262414995, "phrase": "increasing_attention"}, {"score": 0.004099340908974491, "phrase": "deep_neural_networks"}, {"score": 0.004039792863335641, "phrase": "robust_latent_features"}, {"score": 0.003952082333580496, "phrase": "revolutionary_generalization_capabilities"}, {"score": 0.003923269062268688, "phrase": "diverse_application_conditions"}, {"score": 0.003838078705946304, "phrase": "connectionist-hidden_markov_model"}, {"score": 0.003754731210715724, "phrase": "noise-robust_avsr."}, {"score": 0.003700170452390401, "phrase": "deep_denoising_autoencoder"}, {"score": 0.003633079011725194, "phrase": "noise-robust_audio_features"}, {"score": 0.003476963781647026, "phrase": "consecutive_multiple_steps"}, {"score": 0.0034516024325468653, "phrase": "deteriorated_audio_features"}, {"score": 0.0034139056533151, "phrase": "corresponding_clean_features"}, {"score": 0.003327534497764259, "phrase": "output_denoised_audio_features"}, {"score": 0.003291188255149539, "phrase": "corresponding_features"}, {"score": 0.0031961879138783012, "phrase": "convolutional_neural_network"}, {"score": 0.0031039212233596415, "phrase": "visual_features"}, {"score": 0.003081272473508291, "phrase": "raw_mouth_area_images"}, {"score": 0.002948798472453167, "phrase": "raw_images"}, {"score": 0.00291657701018444, "phrase": "corresponding_phoneme_label_outputs"}, {"score": 0.002832359116458113, "phrase": "phoneme_labels"}, {"score": 0.0028014064098604093, "phrase": "corresponding_mouth_area_input_images"}, {"score": 0.0026516301310428756, "phrase": "acquired_audio_and_visual_hmms"}, {"score": 0.0026035011274172753, "phrase": "respective_features"}, {"score": 0.00253758084385033, "phrase": "normal_and_denoised_mel-frequency_cepstral_coefficients"}, {"score": 0.002473325513637559, "phrase": "audio_features"}, {"score": 0.0023331418346468524, "phrase": "snr"}, {"score": 0.0022239665922395104, "phrase": "mshmm"}, {"score": 0.002191600750693037, "phrase": "acquired_visual_features"}, {"score": 0.0021597049163247476, "phrase": "additional_word_recognition_rate_gain"}, {"score": 0.002120485727210402, "phrase": "snr_conditions"}], "paper_keywords": ["Audio-visual speech recognition", " Feature extraction", " Deep learning", " Multi-stream HMM"], "paper_abstract": "Audio-visual speech recognition (AVSR) system is thought to be one of the most promising solutions for reliable speech recognition, particularly when the audio is corrupted by noise. However, cautious selection of sensory features is crucial for attaining high recognition performance. In the machine-learning community, deep learning approaches have recently attracted increasing attention because deep neural networks can effectively extract robust latent features that enable various recognition algorithms to demonstrate revolutionary generalization capabilities under diverse application conditions. This study introduces a connectionist-hidden Markov model (HMM) system for noise-robust AVSR. First, a deep denoising autoencoder is utilized for acquiring noise-robust audio features. By preparing the training data for the network with pairs of consecutive multiple steps of deteriorated audio features and the corresponding clean features, the network is trained to output denoised audio features from the corresponding features deteriorated by noise. Second, a convolutional neural network (CNN) is utilized to extract visual features from raw mouth area images. By preparing the training data for the CNN as pairs of raw images and the corresponding phoneme label outputs, the network is trained to predict phoneme labels from the corresponding mouth area input images. Finally, a multi-stream HMM (MSHMM) is applied for integrating the acquired audio and visual HMMs independently trained with the respective features. By comparing the cases when normal and denoised mel-frequency cepstral coefficients (MFCCs) are utilized as audio features to the HMM, our unimodal isolated word recognition results demonstrate that approximately 65 % word recognition rate gain is attained with denoised MFCCs under 10 dB signal-to-noise-ratio (SNR) for the audio signal input. Moreover, our multimodal isolated word recognition results utilizing MSHMM with denoised MFCCs and acquired visual features demonstrate that an additional word recognition rate gain is attained for the SNR conditions below 10 dB.", "paper_title": "Audio-visual speech recognition using deep learning", "paper_id": "WOS:000353243900008"}