{"auto_keywords": [{"score": 0.04140693296811696, "phrase": "gp."}, {"score": 0.00481495049065317, "phrase": "sparse_spectrum_gaussian_process_regression"}, {"score": 0.004312622975366676, "phrase": "key_novel_idea"}, {"score": 0.004081385289516914, "phrase": "spectral_representation"}, {"score": 0.003421245054271364, "phrase": "achievable_trade-offs"}, {"score": 0.0032376465741070274, "phrase": "computational_requirements"}, {"score": 0.0028993944292191433, "phrase": "existing_state-of-the-art_sparse_approximations"}, {"score": 0.0026252106881752067, "phrase": "function_space_representations"}, {"score": 0.0024569466759289055, "phrase": "new_construction"}, {"score": 0.0021049977753042253, "phrase": "covariance_function"}], "paper_keywords": ["Gaussian process", " probabilistic regression", " sparse approximation", " power spectrum", " computational efficiency"], "paper_abstract": "We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.", "paper_title": "Sparse Spectrum Gaussian Process Regression", "paper_id": "WOS:000282522400004"}