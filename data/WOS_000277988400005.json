{"auto_keywords": [{"score": 0.042188392869170255, "phrase": "bob"}, {"score": 0.033080212481548535, "phrase": "defeat_probability"}, {"score": 0.00481495049065317, "phrase": "monotone_games"}, {"score": 0.00475382761232911, "phrase": "learning_behaviors"}, {"score": 0.004633888966463624, "phrase": "special_class"}, {"score": 0.0044786435725543685, "phrase": "subsymbolic_learning_problems"}, {"score": 0.0043842558719324526, "phrase": "local_descent_direction"}, {"score": 0.004328576620172575, "phrase": "error_landscape"}, {"score": 0.004130376386308024, "phrase": "learning_procedure"}, {"score": 0.0040612863290376364, "phrase": "alice"}, {"score": 0.0038253590626087237, "phrase": "unknown_amounts"}, {"score": 0.0036813725040313002, "phrase": "fixed_effort"}, {"score": 0.0036501160751879784, "phrase": "alice's_part"}, {"score": 0.003468053168607071, "phrase": "individual_contests"}, {"score": 0.0032950412841674026, "phrase": "usual_ones"}, {"score": 0.003267054216881229, "phrase": "game_theory"}, {"score": 0.0030515222178491923, "phrase": "high_confidence"}, {"score": 0.0029617414861784525, "phrase": "archetypal_remedy"}, {"score": 0.002924074850256874, "phrase": "general_overtraining_threat"}, {"score": 0.002790004829699627, "phrase": "original_game"}, {"score": 0.002754516260621121, "phrase": "computational_learning_framework"}, {"score": 0.002696366237523415, "phrase": "approximately_correct_formulation"}, {"score": 0.002639440558428147, "phrase": "wise_use"}, {"score": 0.0026058621464926826, "phrase": "special_inferential_mechanism"}, {"score": 0.0025183881730011597, "phrase": "relevant_statistics"}, {"score": 0.0024863459251358217, "phrase": "different_trade-offs"}, {"score": 0.0023824468367751365, "phrase": "similar_statistics"}, {"score": 0.002342110338541984, "phrase": "analogous_trade-off"}, {"score": 0.0022828795331819025, "phrase": "stopping_criterion"}, {"score": 0.0022634699331607615, "phrase": "subsymbolic_learning_procedures"}, {"score": 0.002178144355363467, "phrase": "principled_stopping_rule"}, {"score": 0.0021049977753042253, "phrase": "training_session"}], "paper_keywords": ["Overtraining control", " Training stopping rule", " Monotone games", " Algorithmic inference", " Computational learning", " Subsymbolic learning"], "paper_abstract": "We deal with a special class of games against nature which correspond to subsymbolic learning problems where we know a local descent direction in the error landscape but not the amount gained at each step of the learning procedure. Namely, Alice and Bob play a game where the probability of victory grows monotonically by unknown amounts with the resources each employs. For a fixed effort on Alice's part Bob increases his resources on the basis of the results of the individual contests (victory, tie or defeat). Quite unlike the usual ones in game theory, his aim is to stop as soon as the defeat probability goes under a given threshold with high confidence. We adopt such a game policy as an archetypal remedy to the general overtraining threat of learning algorithms. Namely, we deal with the original game in a computational learning framework analogous to the Probably Approximately Correct formulation. Therein, a wise use of a special inferential mechanism (known as twisting argument) highlights relevant statistics for managing different trade-offs between observability and controllability of the defeat probability. With similar statistics we discuss an analogous trade-off at the basis of the stopping criterion of subsymbolic learning procedures. As a conclusion, we propose a principled stopping rule based solely on the behavior of the training session, hence without distracting examples into a test set. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Playing monotone games to understand learning behaviors", "paper_id": "WOS:000277988400005"}