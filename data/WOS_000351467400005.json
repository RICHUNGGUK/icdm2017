{"auto_keywords": [{"score": 0.04634865433637489, "phrase": "mixed_strategy_equilibria"}, {"score": 0.03953876560784093, "phrase": "value_functions"}, {"score": 0.0256460237418678, "phrase": "negoq"}, {"score": 0.00481495049065317, "phrase": "multiagent_reinforcement_learning_with_unshared_value_functions"}, {"score": 0.004750440867604888, "phrase": "multiagent_reinforcement_learning"}, {"score": 0.004671012181488876, "phrase": "equilibrium-based_marl"}, {"score": 0.004562027986828209, "phrase": "reinforcement_learning"}, {"score": 0.004531357177195719, "phrase": "game_theory"}, {"score": 0.0044706300051082035, "phrase": "computationally_expensive_calculation"}, {"score": 0.0038668462001710314, "phrase": "efficient_marl_algorithms"}, {"score": 0.0036883616416213206, "phrase": "pure_strategy_equilibrium_solution_concepts"}, {"score": 0.0036022217019257598, "phrase": "mixed_strategy_equilibrium"}, {"score": 0.003459191419478298, "phrase": "pure_strategy_profiles"}, {"score": 0.003412783661090541, "phrase": "equilibrium_solution_concepts"}, {"score": 0.003168413874902081, "phrase": "latter_two_solution_concepts"}, {"score": 0.003073545175760353, "phrase": "higher_payoffs"}, {"score": 0.0030118779844670353, "phrase": "theoretical_analysis"}, {"score": 0.0029714533918949798, "phrase": "strategy_profiles"}, {"score": 0.002951444412262927, "phrase": "symmetric_meta_equilibria"}, {"score": 0.0028824650336365465, "phrase": "multi-step_negotiation_process"}, {"score": 0.0026759663480185047, "phrase": "novel_marl_algorithm"}, {"score": 0.00252656192077797, "phrase": "grid-world_games"}, {"score": 0.0024591621759745648, "phrase": "marl_algorithms"}, {"score": 0.0023935561037315375, "phrase": "equilibrium_policies"}, {"score": 0.002345500021653278, "phrase": "existing_marl_algorithms"}, {"score": 0.0022906498741695094, "phrase": "nash_q-learning"}, {"score": 0.0021921581733031514, "phrase": "team_markov_games"}, {"score": 0.0021700361584578633, "phrase": "pursuit_games"}, {"score": 0.0021336600569395693, "phrase": "team-task-oriented_marl_algorithms"}, {"score": 0.0021049977753042253, "phrase": "friend_q-learning"}], "paper_keywords": ["Game theory", " multiagent reinforcement learning", " Nash equilibrium", " negotiation"], "paper_abstract": "One important approach of multiagent reinforcement learning (MARL) is equilibrium-based MARL, which is a combination of reinforcement learning and game theory. Most existing algorithms involve computationally expensive calculation of mixed strategy equilibria and require agents to replicate the other agents' value functions for equilibrium computing in each state. This is unrealistic since agents may not be willing to share such information due to privacy or safety concerns. This paper aims to develop novel and efficient MARL algorithms without the need for agents to share value functions. First, we adopt pure strategy equilibrium solution concepts instead of mixed strategy equilibria given that a mixed strategy equilibrium is often computationally expensive. In this paper, three types of pure strategy profiles are utilized as equilibrium solution concepts: pure strategy Nash equilibrium, equilibrium-dominating strategy profile, and nonstrict equilibrium-dominating strategy profile. The latter two solution concepts are strategy profiles from which agents can gain higher payoffs than one or more pure strategy Nash equilibria. Theoretical analysis shows that these strategy profiles are symmetric meta equilibria. Second, we propose a multi-step negotiation process for finding pure strategy equilibria since value functions are not shared among agents. By putting these together, we propose a novel MARL algorithm called negotiation-based Q-learning (NegoQ). Experiments are first conducted in grid-world games, which are widely used to evaluate MARL algorithms. In these games, NegoQ learns equilibrium policies and runs significantly faster than existing MARL algorithms (correlated Q-learning and Nash Q-learning). Surprisingly, we find that NegoQ also performs well in team Markov games such as pursuit games, as compared with team-task-oriented MARL algorithms (such as friend Q-learning and distributed Q-learning).", "paper_title": "Multiagent Reinforcement Learning With Unshared Value Functions", "paper_id": "WOS:000351467400005"}