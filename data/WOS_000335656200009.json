{"auto_keywords": [{"score": 0.04203238607476926, "phrase": "gp"}, {"score": 0.023912121799896588, "phrase": "proposed_model"}, {"score": 0.00481495049065317, "phrase": "sparse_gaussian_process"}, {"score": 0.004240116703465252, "phrase": "probabilistic_prediction"}, {"score": 0.004074901471704325, "phrase": "high_computational_cost"}, {"score": 0.003885085477351673, "phrase": "large_datasets"}, {"score": 0.003559675269411366, "phrase": "new_sparse_gp_model"}, {"score": 0.0034208822062692127, "phrase": "gphalf"}, {"score": 0.0033402088717816416, "phrase": "key_idea"}, {"score": 0.0032099453192629976, "phrase": "gp_model"}, {"score": 0.0029409171055888804, "phrase": "regularization_method"}, {"score": 0.00267301171407926, "phrase": "generalized_linear_regression_model"}, {"score": 0.0023532890136269986, "phrase": "sparse_gp_model"}, {"score": 0.0021731423856364003, "phrase": "sparse_solution"}, {"score": 0.002138799283553239, "phrase": "numerical_experiments"}], "paper_keywords": ["Gaussian process", " Sparse model", " Regularization", " Regression", " Machine learning"], "paper_abstract": "Gaussian Process (GP) model is an elegant tool for the probabilistic prediction. However, the high computational cost of GP prohibits its practical application on large datasets. To address this issue, this paper develops a new sparse GP model, referred to as GPHalf. The key idea is to sparsify the GP model via the newly introduced a\"\" (1/2) regularization method. To achieve this, we represent the GP as a generalized linear regression model, then use the modified a\"\" (1/2) half thresholding algorithm to optimize the corresponding objective function, thus yielding a sparse GP model. We proof that the proposed model converges to a sparse solution. Numerical experiments on both artificial and real-world datasets validate the effectiveness of the proposed model.", "paper_title": "Sparse Gaussian Process regression model based on l(1/2) regularization", "paper_id": "WOS:000335656200009"}