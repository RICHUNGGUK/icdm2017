{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "orientation_appearance_models"}, {"score": 0.0046238380150870435, "phrase": "robust_framework"}, {"score": 0.0043512358455892, "phrase": "rigid_object_tracking"}, {"score": 0.004212453011596424, "phrase": "standard_visual_camera"}, {"score": 0.0041784516085055035, "phrase": "dense_depth_maps"}, {"score": 0.00412796126884884, "phrase": "low-cost_consumer_depth_cameras"}, {"score": 0.004061593472000772, "phrase": "kinect"}, {"score": 0.003775803846431323, "phrase": "data_representation"}, {"score": 0.0036257863613963245, "phrase": "gradient_orientations"}, {"score": 0.003567455727660992, "phrase": "intensity_images"}, {"score": 0.003495855671790616, "phrase": "surface_normals"}, {"score": 0.003453584878128719, "phrase": "dense_depth_fields"}, {"score": 0.0033163269450817716, "phrase": "obtained_orientation_appearance_models"}, {"score": 0.0032762198196613084, "phrase": "fusion_approach"}, {"score": 0.003223494720348768, "phrase": "original_active_appearance_models"}, {"score": 0.0030828215204304473, "phrase": "learning_framework"}, {"score": 0.003020918749760067, "phrase": "robust_kernel"}, {"score": 0.0029722900280316216, "phrase": "euler_representation"}, {"score": 0.0028890603352364273, "phrase": "off-line_training"}, {"score": 0.002537216271965659, "phrase": "gross_measurement_errors"}, {"score": 0.002436289099444074, "phrase": "illumination_changes"}, {"score": 0.0023680327896318915, "phrase": "proposed_models"}, {"score": 0.00233936726307301, "phrase": "particle_filter"}, {"score": 0.002311047934798247, "phrase": "proposed_framework"}, {"score": 0.0021922308351126746, "phrase": "robust_performance"}, {"score": 0.0021569129153653777, "phrase": "extreme_pose_variations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Rigid object tracking", " Fusion of orientation appearance models", " Subspace learning", " Online learning", " Face analysis", " RGB-D"], "paper_abstract": "We introduce a robust framework for learning and fusing of orientation appearance models based on both texture and depth information for rigid object tracking. Our framework fuses data obtained from a standard visual camera and dense depth maps obtained by low-cost consumer depth cameras such as the Kinect. To combine these two completely different modalities, we propose to use features that do not depend on the data representation: angles. More specifically, our framework combines image gradient orientations as extracted from intensity images with the directions of surface normals computed from dense depth fields. We propose to capture the correlations between the obtained orientation appearance models using a fusion approach motivated by the original Active Appearance Models (AAMs). To incorporate these features in a learning framework, we use a robust kernel based on the Euler representation of angles which does not require off-line training, and can be efficiently implemented online. The robustness of learning from orientation appearance models is presented both theoretically and experimentally in this work. This kernel enables us to cope with gross measurement errors, missing data as well as other typical problems such as illumination changes and occlusions. By combining the proposed models with a particle filter, the proposed framework was used for performing 2D plus 3D rigid object tracking, achieving robust performance in very difficult tracking scenarios including extreme pose variations. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Online learning and fusion of orientation appearance models for robust rigid object tracking", "paper_id": "WOS:000342256700009"}