{"auto_keywords": [{"score": 0.041318788873943385, "phrase": "retrieval_precision"}, {"score": 0.025128644478658986, "phrase": "ukbench"}, {"score": 0.00481495049065317, "phrase": "image_retrieval"}, {"score": 0.004732444672509326, "phrase": "image_retrieval_algorithms"}, {"score": 0.004699837689671504, "phrase": "excellent_scalability"}, {"score": 0.00455585162011571, "phrase": "vocabulary_tree"}, {"score": 0.004416257191177784, "phrase": "compact_hashing_codes"}, {"score": 0.004266141613620429, "phrase": "visually_similar_images"}, {"score": 0.003872310849448784, "phrase": "feature_characteristics"}, {"score": 0.0038323244983352587, "phrase": "algorithmic_procedures"}, {"score": 0.0036510928952124522, "phrase": "feature-level_fusion"}, {"score": 0.0035147084172295123, "phrase": "ordered_retrieval_sets"}, {"score": 0.003418719577853084, "phrase": "multiple_retrieval_methods"}, {"score": 0.003234509549830177, "phrase": "retrieval_ranks"}, {"score": 0.003190024306903021, "phrase": "candidate_images"}, {"score": 0.0031461489526079236, "phrase": "graph-based_query"}, {"score": 0.0030814613175385703, "phrase": "multiple_graphs"}, {"score": 0.002997269132281551, "phrase": "link_analysis"}, {"score": 0.0029662917511334604, "phrase": "fused_graph"}, {"score": 0.0029356335847783195, "phrase": "retrieval_quality"}, {"score": 0.0029052913631676435, "phrase": "individual_method"}, {"score": 0.00277738101752173, "phrase": "top_candidates'_nearest_neighborhoods"}, {"score": 0.0026459054525234706, "phrase": "retrieval_methods"}, {"score": 0.0026276369627747896, "phrase": "local_or_holistic_features"}, {"score": 0.002609494276659916, "phrase": "different_query_images"}, {"score": 0.00258251424573557, "phrase": "proposed_method"}, {"score": 0.0024432508011008563, "phrase": "extensive_and_thorough_experiments"}, {"score": 0.002279660253812632, "phrase": "san_francisco"}, {"score": 0.0021269997368140066, "phrase": "e._g."}, {"score": 0.0021049977753042253, "phrase": "n-s_score"}], "paper_keywords": ["Large-scale image retrieval", " vocabulary tree", " hashing", " graph-based fusion", " query specific fusion"], "paper_abstract": "Recently two lines of image retrieval algorithms demonstrate excellent scalability: 1) local features indexed by a vocabulary tree, and 2) holistic features indexed by compact hashing codes. Although both of them are able to search visually similar images effectively, their retrieval precision may vary dramatically among queries. Therefore, combining these two types of methods is expected to further enhance the retrieval precision. However, the feature characteristics and the algorithmic procedures of these methods are dramatically different, which is very challenging for the feature-level fusion. This motivates us to investigate how to fuse the ordered retrieval sets, i.e., the ranks of images, given by multiple retrieval methods, to boost the retrieval precision without sacrificing their scalability. In this paper, we model retrieval ranks as graphs of candidate images and propose a graph-based query specific fusion approach, where multiple graphs are merged and reranked by conducting a link analysis on a fused graph. The retrieval quality of an individual method is measured on-the-fly by assessing the consistency of the top candidates' nearest neighborhoods. Hence, it is capable of adaptively integrating the strengths of the retrieval methods using local or holistic features for different query images. This proposed method does not need any supervision, has few parameters, and is easy to implement. Extensive and thorough experiments have been conducted on four public datasets, i.e., the UKbench, Corel-5K, Holidays and the large-scale San Francisco Landmarks datasets. Our proposed method has achieved very competitive performance, including state-of-the-art results on several data sets, e. g., the N-S score 3.83 for UKbench.", "paper_title": "Query Specific Rank Fusion for Image Retrieval", "paper_id": "WOS:000351213400008"}