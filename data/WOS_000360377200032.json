{"auto_keywords": [{"score": 0.02900730847963855, "phrase": "mcn"}, {"score": 0.00481495049065317, "phrase": "cross-media_correlation"}, {"score": 0.004660453113061494, "phrase": "novel_method"}, {"score": 0.004610058444728656, "phrase": "multimedia_document_content_analysis"}, {"score": 0.004535481695403089, "phrase": "multimodal_data_correlations"}, {"score": 0.004342430078306461, "phrase": "different_modalities"}, {"score": 0.004180234323211199, "phrase": "better_multimedia"}, {"score": 0.004135010810625326, "phrase": "understanding_results"}, {"score": 0.004002242925856335, "phrase": "single_modality"}, {"score": 0.0036091892088392775, "phrase": "first_stage"}, {"score": 0.0034742854447532678, "phrase": "training_multimedia_data"}, {"score": 0.0034366728236876016, "phrase": "modality_semantic_documents"}, {"score": 0.0033262524445013303, "phrase": "quantized_multimodal_features"}, {"score": 0.003236941658990869, "phrase": "multivariate_gaussian_distributions"}, {"score": 0.003167217223469026, "phrase": "continuous_quantity"}, {"score": 0.0031329184724341592, "phrase": "latent_topic"}, {"score": 0.0030821633718368206, "phrase": "model_parameters"}, {"score": 0.0029830992938695007, "phrase": "multimodal_correlations"}, {"score": 0.002934764205541183, "phrase": "latent_topic_space"}, {"score": 0.0028404241985421096, "phrase": "second_stage"}, {"score": 0.0027641218567092665, "phrase": "multimodal_correlation_network"}, {"score": 0.0026607205525625995, "phrase": "initialized_multimodal_correlations"}, {"score": 0.0026033756286033285, "phrase": "new_mechanism"}, {"score": 0.0025611773887867255, "phrase": "inter-modality_correlations"}, {"score": 0.0025334249463305875, "phrase": "intra-modality_similarities"}, {"score": 0.002347378595930709, "phrase": "multimedia_content_analysis"}, {"score": 0.00230932021677234, "phrase": "experimental_results"}, {"score": 0.002284290601837099, "phrase": "image-audio_data_retrieval"}, {"score": 0.002210814150677161, "phrase": "content-oriented_web_page_recommendation"}, {"score": 0.00217496508306238, "phrase": "ustoday"}], "paper_keywords": ["Multimedia documents", " Multimodal", " MAD", " MCN", " Correlation propagation"], "paper_abstract": "This paper presents a novel method for multimedia document content analysis through modeling multimodal data correlations. We hypothesize that the correlation of different modalities from the same data source can help achieve better multimedia content understanding results than one which explores a single modality. We turn this task into two parts: multimedia data fusion and multimodal correlation propagation. During the first stage, we re-organize the training multimedia data into Modality semAntic Documents (MADs) after extracting quantized multimodal features, and then use multivariate Gaussian distributions to characterize the continuous quantity by latent topic modeling. Model parameters are asymmetrically learned to initialize multimodal correlations in the latent topic space. Accordingly, during the second stage, we construct a Multimodal Correlation Network (MCN) based on the initialized multimodal correlations, and a new mechanism of propagating inter-modality correlations and intra-modality similarities in MCN is further proposed to take the complementary from cross-modalities to facilitate multimedia content analysis. The experimental results of image-audio data retrieval on a 10-categories dataset and content-oriented web page recommendation on the USTODAY dataset show the effectiveness of our method.", "paper_title": "Content-oriented multimedia document understanding through cross-media correlation", "paper_id": "WOS:000360377200032"}