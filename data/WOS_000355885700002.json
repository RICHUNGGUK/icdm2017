{"auto_keywords": [{"score": 0.03864467197500145, "phrase": "thambs"}, {"score": 0.008606332113998031, "phrase": "robotic_agent"}, {"score": 0.006230855162366447, "phrase": "virtual_agent"}, {"score": 0.004696491408936132, "phrase": "attention_model"}, {"score": 0.004657653216262924, "phrase": "social_cues"}, {"score": 0.004580933283492972, "phrase": "interaction_participants"}, {"score": 0.004376378707877484, "phrase": "artificial_agent"}, {"score": 0.004268663239979243, "phrase": "previous_work"}, {"score": 0.004112020122418323, "phrase": "implemented_social_behaviours"}, {"score": 0.004010785080336424, "phrase": "facial_gestures"}, {"score": 0.0033964919992320024, "phrase": "external_audio_and_visual_stimuli"}, {"score": 0.0032717489668326275, "phrase": "human_user"}, {"score": 0.0030867203659106727, "phrase": "robotic_platform"}, {"score": 0.003048449231088915, "phrase": "controlled_experimental_setting"}, {"score": 0.002960979624379786, "phrase": "behavioural-performance_variables"}, {"score": 0.0029121251930855664, "phrase": "self-reported_ratings"}, {"score": 0.002828556054338357, "phrase": "human_subjects"}, {"score": 0.002602734354498382, "phrase": "even_a_rudimentary_display"}, {"score": 0.002528021281333922, "phrase": "significantly_increased_attention"}, {"score": 0.002404885278896803, "phrase": "user_behaviour"}, {"score": 0.002268769196700254, "phrase": "self-report_ratings"}, {"score": 0.002158233639156017, "phrase": "successful_human-robot_interactive_behaviours"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Human-robot interaction", " Attention model", " Social interaction", " Evaluation", " Engagement"], "paper_abstract": "Social cues facilitate engagement between interaction participants, whether they be two (or more) humans or a human and an artificial agent such as a robot. Previous work specific to human-agent/robot interaction has demonstrated the efficacy of implemented social behaviours, such as eye-gaze or facial gestures, for demonstrating the illusion of engagement and positively impacting interaction with a human. We describe the implementation of THAMBS, The Thinking Head Attention Model and Behavioural System, which is used to model attention controlling how a virtual agent reacts to external audio and visual stimuli within the context of an interaction with a human user. We evaluate the efficacy of THAMBS for a virtual agent mounted on a robotic platform in a controlled experimental setting, and collect both task- and behavioural-performance variables, along with self-reported ratings of engagement. Our results show that human subjects noticeably engaged more often, and in more interesting ways, with the robotic agent when THAMBS was activated, indicating that even a rudimentary display of attention by the robot elicits significantly increased attention by the human. Back-channelling had less of an effect on user behaviour. THAMBS and back-channelling did not interact and neither had an effect on self-report ratings. Our results concerning THAMBS hold implications for the design of successful human-robot interactive behaviours. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "C'Mon dude!: Users adapt their behaviour to a robotic agent with an attention model", "paper_id": "WOS:000355885700002"}