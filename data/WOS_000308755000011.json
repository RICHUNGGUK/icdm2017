{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "image_windows"}, {"score": 0.0047310525657970615, "phrase": "generic_objectness_measure"}, {"score": 0.004583686207645165, "phrase": "image_window"}, {"score": 0.004302522426263979, "phrase": "well-defined_boundary"}, {"score": 0.004139223748460922, "phrase": "amorphous_background_elements"}, {"score": 0.00395417485449239, "phrase": "bayesian_framework"}, {"score": 0.003737697279708501, "phrase": "closed_boundary"}, {"score": 0.003672500644750565, "phrase": "innovative_cue"}, {"score": 0.003621160116056119, "phrase": "closed_boundary_characteristic"}, {"score": 0.003483631502938922, "phrase": "new_cue"}, {"score": 0.003434922209075514, "phrase": "state-of-the-art_saliency_measure"}, {"score": 0.003212663446866797, "phrase": "interest_point_operators"}, {"score": 0.003178904945309528, "phrase": "hog_detector"}, {"score": 0.0030906010704382374, "phrase": "automatic_object_segmentation"}, {"score": 0.0029109914689364465, "phrase": "small_numberof_windows"}, {"score": 0.002790511479398411, "phrase": "location_priors"}, {"score": 0.002770920590799572, "phrase": "modern_class-specific_object_detectors"}, {"score": 0.002609840513713399, "phrase": "expensive_class-specific_model"}, {"score": 0.0025733173062050018, "phrase": "second_application"}, {"score": 0.0025106241866273897, "phrase": "complementary_score"}, {"score": 0.0024667783849055634, "phrase": "class-specific_model"}, {"score": 0.0023233376117560317, "phrase": "valuable_focus"}, {"score": 0.0023070189467375374, "phrase": "attention_mechanism"}, {"score": 0.002242879896352752, "phrase": "weakly_supervised_learning"}, {"score": 0.002227125088932002, "phrase": "object_categories"}, {"score": 0.0022114807037424944, "phrase": "unsupervised_pixelwise_segmentation"}, {"score": 0.002188219483646592, "phrase": "object_tracking"}], "paper_keywords": ["Objectness measure", " object detection", " object recognition"], "paper_abstract": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object segmentation. Finally, we present two applications of objectness. In the first, we sample a small numberof windows according to their objectness probability and give an algorithm to employ them as location priors for modern class-specific object detectors. As we show experimentally, this greatly reduces the number of windows evaluated by the expensive class-specific model. In the second application, we use objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives. As shown in several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video. Computing objectness is very efficient and takes only about 4 sec. per image.", "paper_title": "Measuring the Objectness of Image Windows", "paper_id": "WOS:000308755000011"}