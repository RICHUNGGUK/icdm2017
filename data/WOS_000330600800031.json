{"auto_keywords": [{"score": 0.0324337131484804, "phrase": "sbbs"}, {"score": 0.02101367060912524, "phrase": "redundant_data"}, {"score": 0.01061238630452144, "phrase": "backtracking_sub-blocks"}, {"score": 0.01049978439416546, "phrase": "duplicate_data_detection"}, {"score": 0.009090550786171192, "phrase": "data_deduplication"}, {"score": 0.004662427600500598, "phrase": "explosive_growth"}, {"score": 0.004563427013973105, "phrase": "storage_systems"}, {"score": 0.004490552331642995, "phrase": "huge_storage_pressure"}, {"score": 0.004255902303152008, "phrase": "duplicate_copies"}, {"score": 0.004055176557424983, "phrase": "storage-optimization_technique"}, {"score": 0.003969018740062018, "phrase": "data_footprint"}, {"score": 0.0039055990919333082, "phrase": "multiple_copies"}, {"score": 0.0036422425344358037, "phrase": "data_detection_techniques"}, {"score": 0.0034148795808317555, "phrase": "corresponding_parts"}, {"score": 0.003342280102112269, "phrase": "hash_techniques"}, {"score": 0.003167441161387607, "phrase": "efficient_sliding"}, {"score": 0.002953711377604328, "phrase": "duplicate_data_detection_precision"}, {"score": 0.0029064676523923886, "phrase": "traditional_sliding_blocking"}, {"score": 0.002681315076008658, "phrase": "-failed_segments"}, {"score": 0.002652639568848117, "phrase": "experimental_results"}, {"score": 0.0025546627686955656, "phrase": "duplicate_detection_precision"}, {"score": 0.002473560912156024, "phrase": "traditional_sb_algorithm"}, {"score": 0.0023950275815082297, "phrase": "content-defined_chunking"}, {"score": 0.002209406458446019, "phrase": "equal_chunks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Data deduplication", " Duplicate data detection", " Sliding blocking algorithm", " Backtracking", " SBBS", " Content-defined chunking algorithm"], "paper_abstract": "With the explosive growth of data, storage systems are facing huge storage pressure due to a mass of redundant data caused by the duplicate copies or regions of files. Data deduplication is a storage-optimization technique that reduces the data footprint by eliminating multiple copies of redundant data and storing only unique data. The basis of data deduplication is duplicate data detection techniques, which divide files into a number of parts, compare corresponding parts between files via hash techniques and find out redundant data. This paper proposes an efficient sliding blocking algorithm with backtracking sub-blocks called SBBS for duplicate data detection. SBBS improves the duplicate data detection precision of the traditional sliding blocking (SB) algorithm via backtracking the left/right 1/4 and 1/2 sub-blocks in matching-failed segments. Experimental results show that SBBS averagely improves the duplicate detection precision by 6.5% compared with the traditional SB algorithm and by 16.5% compared with content-defined chunking (CDC) algorithm, and it does not increase much extra storage overhead when SBBS divides the files into equal chunks of size 8 kB. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "SBBS: A sliding blocking algorithm with backtracking sub-blocks for duplicate data detection", "paper_id": "WOS:000330600800031"}