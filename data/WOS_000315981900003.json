{"auto_keywords": [{"score": 0.04232478027153052, "phrase": "learning_process"}, {"score": 0.01629753625220647, "phrase": "levy_process"}, {"score": 0.010568193496377665, "phrase": "risk_bounds"}, {"score": 0.007385433020210429, "phrase": "stochastic_processes"}, {"score": 0.006441886367578295, "phrase": "time-dependent_samples"}, {"score": 0.00481495049065317, "phrase": "learning_processes"}, {"score": 0.004773904938276846, "phrase": "levy_processes"}, {"score": 0.0047332625887550935, "phrase": "levy"}, {"score": 0.004496502325391506, "phrase": "poisson"}, {"score": 0.004438937956639353, "phrase": "brownian"}, {"score": 0.004326188427877999, "phrase": "important_role"}, {"score": 0.004252707756312684, "phrase": "machine_learning"}, {"score": 0.0034033029171804106, "phrase": "traditional_statistical_learning_theory"}, {"score": 0.0027940789980683, "phrase": "asymptotical_behavior"}, {"score": 0.0026538960171433963, "phrase": "deviation_inequalities"}, {"score": 0.0026199603775129516, "phrase": "symmetrization_inequality"}, {"score": 0.0025315664299815537, "phrase": "resultant_inequalities"}, {"score": 0.0024252458095689847, "phrase": "covering_number"}, {"score": 0.002353483216544667, "phrase": "resulting_risk_bounds"}, {"score": 0.0023035249767734286, "phrase": "asymptotic_convergence"}, {"score": 0.0021049977753042253, "phrase": "related_results"}], "paper_keywords": ["Levy process", " risk bound", " deviation inequality", " symmetrization inequality", " statistical learning theory", " time-dependent"], "paper_abstract": "Levy processes refer to a class of stochastic processes, for example, Poisson processes and Brownian motions, and play an important role in stochastic processes and machine learning. Therefore, it is essential to study risk bounds of the learning process for time-dependent samples drawn from a Levy process (or briefly called learning process for Levy process). It is noteworthy that samples in this learning process are not independently and identically distributed (i.i.d.). Therefore, results in traditional statistical learning theory are not applicable (or at least cannot be applied directly), because they are obtained under the sample-i.i.d. assumption. In this paper, we study risk bounds of the learning process for time-dependent samples drawn from a Levy process, and then analyze the asymptotical behavior of the learning process. In particular, we first develop the deviation inequalities and the symmetrization inequality for the learning process. By using the resultant inequalities, we then obtain the risk bounds based on the covering number. Finally, based on the resulting risk bounds, we study the asymptotic convergence and the rate of convergence of the learning process for Levy process. Meanwhile, we also give a comparison to the related results under the sample-i.i.d. assumption.", "paper_title": "Risk Bounds of Learning Processes for Levy Processes", "paper_id": "WOS:000315981900003"}