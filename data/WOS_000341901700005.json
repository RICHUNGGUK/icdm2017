{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "joint_attention"}, {"score": 0.004777034835192062, "phrase": "situated_human-robot_interaction"}, {"score": 0.00395111389294443, "phrase": "user's_and_the_robot's_gaze"}, {"score": 0.0036940852452587093, "phrase": "task_progression"}, {"score": 0.003607404191062938, "phrase": "face-to-face_setting"}, {"score": 0.0034811680446852054, "phrase": "random_gaze_behaviour"}, {"score": 0.0032546105164772995, "phrase": "paper_board"}, {"score": 0.003115908199991072, "phrase": "turn-taking_cues"}, {"score": 0.0030068193773923387, "phrase": "robot's_speech"}, {"score": 0.002947868524740683, "phrase": "participants'_subjective_rating"}, {"score": 0.00289007009553812, "phrase": "verbal_responses"}, {"score": 0.002867268370615848, "phrase": "gaze_behaviour"}, {"score": 0.0028334016912935165, "phrase": "drawing_activity"}, {"score": 0.0027126012542151015, "phrase": "robot's_gaze"}, {"score": 0.0026175952455594277, "phrase": "robot's_verbal_and_gaze_behaviour"}, {"score": 0.002546002312015341, "phrase": "users'_turn-taking_behaviour"}, {"score": 0.002456816881853114, "phrase": "users'_gaze"}, {"score": 0.0023613726075968986, "phrase": "robot_instructions"}, {"score": 0.0022250978492127163, "phrase": "previous_instruction"}, {"score": 0.002172811149894415, "phrase": "user's_level"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Turn-taking", " Feedback", " Joint attention", " Prosody", " Gaze", " Uncertainty"], "paper_abstract": "In this paper, we present a study where a robot instructs a human on how to draw a route on a map. The human and robot are seated face-to-face with the map placed on the table between them. The user's and the robot's gaze can thus serve several simultaneous functions: as cues to joint attention, turn-taking, level of understanding and task progression. We have compared this face-to-face setting with a setting where the robot employs a random gaze behaviour, as well as a voice-only setting where the robot is hidden behind a paper board. In addition to this, we have also manipulated turn-taking cues such as completeness and filled pauses in the robot's speech. By analysing the participants' subjective rating, task completion, verbal responses, gaze behaviour, and drawing activity, we show that the users indeed benefit from the robot's gaze when talking about landmarks, and that the robot's verbal and gaze behaviour has a strong effect on the users' turn-taking behaviour. We also present an analysis of the users' gaze and lexical and prosodic realisation of feedback after the robot instructions, and show that these cues reveal whether the user has yet executed the previous instruction, as well as the user's level of uncertainty. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Turn-taking, feedback and joint attention in situated human-robot interaction", "paper_id": "WOS:000341901700005"}