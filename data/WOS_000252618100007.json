{"auto_keywords": [{"score": 0.04329327094677383, "phrase": "value_functions"}, {"score": 0.011720930116639619, "phrase": "tabular_representation"}, {"score": 0.004488734023485018, "phrase": "learning_agent"}, {"score": 0.004283576270012874, "phrase": "generalization_methods"}, {"score": 0.004055998173160551, "phrase": "action_policy"}, {"score": 0.0037662852047763112, "phrase": "supervized_learning_method"}, {"score": 0.0032220148724914867, "phrase": "higher_performance"}, {"score": 0.0029800360974218836, "phrase": "supervized_function_approximator"}, {"score": 0.0029224275921964724, "phrase": "machine_learning_technique"}, {"score": 0.002865929543375884, "phrase": "state_space_discretization"}, {"score": 0.0027886573969044042, "phrase": "nearest_prototype_classifiers"}, {"score": 0.002766963729586353, "phrase": "decision_trees"}, {"score": 0.0027134630181926854, "phrase": "second_learning_phase"}, {"score": 0.002681859084438076, "phrase": "space_discretization"}, {"score": 0.0026402908398883832, "phrase": "first_phase"}, {"score": 0.002549096826249, "phrase": "value_function"}, {"score": 0.0025095811903520274, "phrase": "previous_phase"}, {"score": 0.0024134449026937586, "phrase": "different_domains"}, {"score": 0.002366763442497185, "phrase": "learning_phases"}, {"score": 0.002154930852763048, "phrase": "learned_behavior"}, {"score": 0.0021049977753042253, "phrase": "wiley_periodicals"}], "paper_keywords": [""], "paper_abstract": "When applying reinforcement learning in domains with very large or continuous state spaces, the experience obtained by the learning agent in the interaction with the environment must be generalized. The generalization methods are usually based on the approximation of the value functions used to compute the action policy and tackled in two different ways. On the one hand by using an approximation of the value functions based on a supervized learning method. On the other hand, by discretizing the environment to use a tabular representation of the value functions. In this work, we propose an algorithm that uses both approaches to use the benefits of both mechanisms, allowing a higher performance. The approach is based on two learning phases. In the first one, a learner is used as a supervized function approximator, but using a machine learning technique which also outputs a state space discretization of the environment, such as nearest prototype classifiers or decision trees do. In the second learning phase, the space discretization computed in the first phase is used to obtain a tabular representation of the value function computed in the previous phase, allowing a tuning of such value function approximation. Experiments in different domains show that executing both learning phases improves the results obtained executing only the first one. The results take into account the resources used and the performance of the learned behavior. (c) 2008 Wiley Periodicals, Inc.", "paper_title": "Two steps reinforcement learning", "paper_id": "WOS:000252618100007"}