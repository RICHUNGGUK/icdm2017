{"auto_keywords": [{"score": 0.04928275842421276, "phrase": "normalized_retrievability_scoring_function"}, {"score": 0.021302113548476765, "phrase": "retrieval_bias"}, {"score": 0.008470889419872629, "phrase": "standard_retrievability_scoring_function"}, {"score": 0.00481495049065317, "phrase": "efficient_retrievability_ranks"}, {"score": 0.004483585361583022, "phrase": "large_scale_queries"}, {"score": 0.0043721365634105565, "phrase": "standard_retrieval_models"}, {"score": 0.0042455945330942746, "phrase": "different_retrieval_models"}, {"score": 0.00396988421137599, "phrase": "retrieval_bias_analysis"}, {"score": 0.003712011863784581, "phrase": "retrieval_bias_experiments"}, {"score": 0.003589404014552974, "phrase": "highly_skewed_distribution"}, {"score": 0.0035296217979800463, "phrase": "standard_retrievability_calculation_function"}, {"score": 0.003398714712611042, "phrase": "vocabulary_richness"}, {"score": 0.003245270848972794, "phrase": "large_vocabulary"}, {"score": 0.0031512402798625056, "phrase": "theoretically_large_probability"}, {"score": 0.002860992017190221, "phrase": "retrievability_scores"}, {"score": 0.0027203027216256013, "phrase": "unbiased_representation"}, {"score": 0.0026193328696791306, "phrase": "vocabulary_differences"}, {"score": 0.0024800509241747013, "phrase": "retrieval_models"}, {"score": 0.0024284701569889113, "phrase": "long_documents"}, {"score": 0.0023284972218506157, "phrase": "retrievability_scoring_function"}, {"score": 0.002309000874224222, "phrase": "better_effectiveness"}, {"score": 0.0022420373745572837, "phrase": "retrievability_ranks"}, {"score": 0.0021049977753042253, "phrase": "known-items_search_method"}], "paper_keywords": ["Information systems evaluation", " Documents accessibility", " Documents findability", " Known-items search", " Patent retrieval", " Recall-oriented retrieval"], "paper_abstract": "In this paper, we perform a number of experiments with large scale queries to analyze the retrieval bias of standard retrieval models. These experiments analyze how far different retrieval models differ in terms of retrieval bias that they imposed on the collection. Along with the retrieval bias analysis, we also exploit a limitation of standard retrievability scoring function and propose a normalized retrievability scoring function. Results of retrieval bias experiments show us that when a collection contains highly skewed distribution, then the standard retrievability calculation function does not take into account the differences in vocabulary richness across documents of collection. In such case, documents having large vocabulary produce many more queries and such documents thus have theoretically large probability of retrievability via a much large number of queries. We thus propose a normalized retrievability scoring function that tries to mitigate this effect by normalizing the retrievability scores of documents relative to their total number of queries. This provides an unbiased representation of the retrieval bias that could occurred due to vocabulary differences between the documents of collection without automatically inflicting a penalty on the retrieval models that favor or disfavor long documents. Finally, in order to examine, which retrievability scoring function has better effectiveness than other for correctly producing the retrievability ranks of documents, we perform a comparison between the both functions on the basis of known-items search method. Experiments on known-items search show that normalized retrievability scoring function has better effectiveness than the standard retrievability scoring function.", "paper_title": "Producing efficient retrievability ranks of documents using normalized retrievability scoring function", "paper_id": "WOS:000336277700006"}