{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "port-hamiltonian_systems"}, {"score": 0.014016997581538621, "phrase": "control_law"}, {"score": 0.0121191571617076, "phrase": "eb-pbc"}, {"score": 0.004777917315737992, "phrase": "passivity-based_control"}, {"score": 0.004632597963417253, "phrase": "intuitive_way"}, {"score": 0.004405779372462873, "phrase": "desired_storage_function"}, {"score": 0.004206236029936448, "phrase": "performance_considerations"}, {"score": 0.004046843083499649, "phrase": "complex_partial_differential_equation"}, {"score": 0.003833749901796579, "phrase": "reinforcement_learning"}, {"score": 0.0038042806687810515, "phrase": "rl"}, {"score": 0.0037314328720368453, "phrase": "energy-balancing_passivity-based_control"}, {"score": 0.003576127546417498, "phrase": "pbc"}, {"score": 0.0035212516951180946, "phrase": "closed-loop_energy"}, {"score": 0.0034140224493254935, "phrase": "stored_and_supplied_energies"}, {"score": 0.0032592499906265882, "phrase": "systems's_pde_matching_conditions"}, {"score": 0.003147778277286763, "phrase": "global_desired_hamiltonian"}, {"score": 0.003111472132927442, "phrase": "performance_criteria"}, {"score": 0.00282470997972535, "phrase": "near-optimal_control_policies"}, {"score": 0.002792119634261638, "phrase": "desired_closed-loop_energy_landscape"}, {"score": 0.0026347074232350503, "phrase": "energy_shaping"}, {"score": 0.0024957862318504753, "phrase": "passivity_theory"}, {"score": 0.0024574537875026634, "phrase": "rl_perspective"}, {"score": 0.0023278569001363263, "phrase": "ac_framework"}, {"score": 0.0022481685132781626, "phrase": "resulting_parameterization"}, {"score": 0.0021378442381693847, "phrase": "pendulum_swing-up_problem"}, {"score": 0.0021049977753042253, "phrase": "real-life_experiments"}], "paper_keywords": ["Actor-critic (AC)", " energy-balancing (EB)", " passivity-based control (PBC)", " port-Hamiltonian (PH) systems", " reinforcement learning (RL)"], "paper_abstract": "Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning (RL) approach into the energy-balancing passivity-based control (EB-PBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems's PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust. The parameters of the control law are found by using actor-critic (AC) RL, enabling the search for near-optimal control policies satisfying a desired closed-loop energy landscape. The advantage is that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the RL perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the AC framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.", "paper_title": "Reinforcement Learning for Port-Hamiltonian Systems", "paper_id": "WOS:000353152700012"}