{"auto_keywords": [{"score": 0.027045639145406863, "phrase": "wanbia"}, {"score": 0.00481495049065317, "phrase": "naive_bayes_attribute_independence_assumption"}, {"score": 0.00406083086983316, "phrase": "great_interest"}, {"score": 0.003984665807155269, "phrase": "machine_learning_community"}, {"score": 0.003909923690310641, "phrase": "numerous_approaches"}, {"score": 0.0038124354878360032, "phrase": "naive_bayes_classifier"}, {"score": 0.003764603272900618, "phrase": "attribute_weighting"}, {"score": 0.003194580334752371, "phrase": "highly_predictive_attributes"}, {"score": 0.0027625134766418266, "phrase": "conditional_independence_assumption"}, {"score": 0.0025933275734892508, "phrase": "weighted_naive_bayes_algorithm"}, {"score": 0.0023587312574818208, "phrase": "error_objective_functions"}, {"score": 0.002299830460180699, "phrase": "extensive_evaluations"}, {"score": 0.0022002633747977593, "phrase": "competitive_alternative"}, {"score": 0.0021317885325570604, "phrase": "art_classifiers"}, {"score": 0.0021049977753042253, "phrase": "random_forest"}], "paper_keywords": ["classification", " naive Bayes", " attribute independence assumption", " weighted naive Bayes classification"], "paper_abstract": "Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE.", "paper_title": "Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting", "paper_id": "WOS:000323367000009"}