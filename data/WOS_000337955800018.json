{"auto_keywords": [{"score": 0.03172845301281709, "phrase": "proposed_framework"}, {"score": 0.030276009524237438, "phrase": "training_images"}, {"score": 0.00481495049065317, "phrase": "visual_attributes"}, {"score": 0.004700468701207975, "phrase": "middle-level_semantic_cue"}, {"score": 0.00458869631642791, "phrase": "low-level_image_features"}, {"score": 0.00455202992136173, "phrase": "high-level_object_classes"}, {"score": 0.00437302709192313, "phrase": "specific_semantic_categories"}, {"score": 0.003876996400581697, "phrase": "application-dependent_ontology"}, {"score": 0.003606711217432998, "phrase": "well-defined_set"}, {"score": 0.0033822643082593285, "phrase": "image_attribute_adaptation"}, {"score": 0.00319732428387928, "phrase": "well-defined_auxiliary_image"}, {"score": 0.003146360480030653, "phrase": "target_image_set"}, {"score": 0.003071429173571846, "phrase": "appropriate_attributes"}, {"score": 0.0030468495296763617, "phrase": "target_images"}, {"score": 0.0029386456078004863, "phrase": "non-linear_mapping_function"}, {"score": 0.0029034358982402346, "phrase": "multiple_base_kernels"}, {"score": 0.002733598121537801, "phrase": "reproducing_kernel_hilbert_space"}, {"score": 0.002615381301695394, "phrase": "data_distributions"}, {"score": 0.0025944417620244924, "phrase": "auxiliary_and_target_images"}, {"score": 0.002512342528545883, "phrase": "un-labeled_images"}, {"score": 0.0024623517958288228, "phrase": "semi-supervised_learning_process"}, {"score": 0.0024036710362692042, "phrase": "robust_loss_function"}, {"score": 0.0023463854193479274, "phrase": "shared_irrelevance"}, {"score": 0.002253920389285441, "phrase": "auxiliary-target_image_sets"}, {"score": 0.0022001958412800745, "phrase": "better_performance"}, {"score": 0.002156402793481102, "phrase": "target_testing_images"}], "paper_keywords": ["Image attributes", " domain adaptation", " transfer learning", " semi-supervised learning", " multiple kernel learning", " robust multiple kernel regression"], "paper_abstract": "Visual attributes can be considered as a middle-level semantic cue that bridges the gap between low-level image features and high-level object classes. Thus, attributes have the advantage of transcending specific semantic categories or describing objects across categories. Since attributes are often human-nameable and domain specific, much work constructs attribute annotations ad hoc or take them from an application-dependent ontology. To facilitate other applications with attributes, it is necessary to develop methods which can adapt a well-defined set of attributes to novel images. In this paper, we propose a framework for image attribute adaptation. The goal is to automatically adapt the knowledge of attributes from a well-defined auxiliary image set to a target image set, thus assisting in predicting appropriate attributes for target images. In the proposed framework, we use a non-linear mapping function corresponding to multiple base kernels to map each training images of both the auxiliary and the target sets to a Reproducing Kernel Hilbert Space (RKHS), where we reduce the mismatch of data distributions between auxiliary and target images. In order to make use of un-labeled images, we incorporate a semi-supervised learning process. We also introduce a robust loss function into our framework to remove the shared irrelevance and noise of training images. Experiments on two couples of auxiliary-target image sets demonstrate that the proposed framework has better performance of predicting attributes for target testing images, compared to three baselines and two state-of-the-art domain adaptation methods.", "paper_title": "Image Attribute Adaptation", "paper_id": "WOS:000337955800018"}