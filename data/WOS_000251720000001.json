{"auto_keywords": [{"score": 0.04917331388552706, "phrase": "classification_problem"}, {"score": 0.00481495049065317, "phrase": "statistical_networks"}, {"score": 0.004546755482549647, "phrase": "measurement_vector"}, {"score": 0.004132411680987228, "phrase": "measurement_variables"}, {"score": 0.004034841146538891, "phrase": "computational_cost"}, {"score": 0.003939565240409143, "phrase": "better_understanding"}, {"score": 0.003902085339580381, "phrase": "class_separability"}, {"score": 0.0037556843869894566, "phrase": "existing_literature"}, {"score": 0.003529364798514874, "phrase": "separability_information"}, {"score": 0.0034459835582278746, "phrase": "dimension_reduction_procedures"}, {"score": 0.0033645655483989746, "phrase": "linear_classifiers"}, {"score": 0.00326939082861424, "phrase": "competing_classes"}, {"score": 0.0031018195951638882, "phrase": "ideal_\"features"}, {"score": 0.002778604641920052, "phrase": "dimension_reduction"}, {"score": 0.002752139271586703, "phrase": "feature_extraction"}, {"score": 0.002687070469080394, "phrase": "projection_pursuit_regression_model"}, {"score": 0.002648768009464535, "phrase": "single_hidden_layer_perceptron_model"}, {"score": 0.002549269480908849, "phrase": "special_cases"}, {"score": 0.0024771000236931836, "phrase": "iterative_algorithm"}, {"score": 0.002327648608736437, "phrase": "cross-validation_method"}, {"score": 0.0022617389804167943, "phrase": "ideal_number"}, {"score": 0.002176747110119268, "phrase": "extensive_simulation_study"}, {"score": 0.0021049977753042253, "phrase": "fully_automatic_method"}], "paper_keywords": ["artificial neural networks", " backfitting", " classification using splines", " cross-validation", " feature selection", " projection pursuit regression"], "paper_abstract": "In a classification problem, quite often the dimension of the measurement vector is large. Some of these measurements may not be important for separating the classes. Removal of these measurement variables not only reduces the computational cost but also leads to better understanding of class separability. There are some methods in the existing literature for reducing the dimensionality of a classification problem without losing much of the separability information. However, these dimension reduction procedures usually work well for linear classifiers. In the case where competing classes are not linearly separable, one has to look for ideal \"features\" which could be some transformations of one or more measurements. In this paper, we make an attempt to tackle both, the problems of dimension reduction and feature extraction, by considering a projection pursuit regression model. The single hidden layer perceptron model and some other popular models can be viewed as special cases of this model. An iterative algorithm based on backfitting is proposed to select the features dynamically, and cross-validation method is used to select the ideal number of features. We carry out an extensive simulation study to show the effectiveness of this fully automatic method.", "paper_title": "Feature extraction for classification using statistical networks", "paper_id": "WOS:000251720000001"}