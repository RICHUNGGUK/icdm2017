{"auto_keywords": [{"score": 0.04606447337059914, "phrase": "facial_expressions"}, {"score": 0.00481495049065317, "phrase": "motion_primitives"}, {"score": 0.004780030485538593, "phrase": "unsupervised_grouping"}, {"score": 0.0046938292389998824, "phrase": "human_actions"}, {"score": 0.004509594355983191, "phrase": "novel_representation"}, {"score": 0.004476878957065829, "phrase": "articulated_human_actions"}, {"score": 0.004364224683011796, "phrase": "main_goals"}, {"score": 0.004316812490871324, "phrase": "proposed_approach"}, {"score": 0.0039268117782861696, "phrase": "unlabeled_datasets"}, {"score": 0.003898307690037192, "phrase": "unsupervised_clustering"}, {"score": 0.003786337629480068, "phrase": "high-level_subactions"}, {"score": 0.003704467870496114, "phrase": "hierarchical_clustering"}, {"score": 0.0036775717816768133, "phrase": "observed_optical_flow"}, {"score": 0.003381905288924267, "phrase": "video_words"}, {"score": 0.0033329590158056935, "phrase": "meaningful_representation"}, {"score": 0.003296713221268488, "phrase": "visual_interpretation"}, {"score": 0.003272767888056937, "phrase": "textual_labeling"}, {"score": 0.0032371745435372168, "phrase": "primitive_action"}, {"score": 0.003201967054773186, "phrase": "atomic_subaction"}, {"score": 0.0031671412657333364, "phrase": "directional_motion"}, {"score": 0.0030205408413942272, "phrase": "four-dimensional_gaussian_distributions"}, {"score": 0.00296597499768551, "phrase": "k-shot_learning"}, {"score": 0.0029123920023263446, "phrase": "primitive_labels"}, {"score": 0.0028702214190712036, "phrase": "test_video"}, {"score": 0.0028286597212365504, "phrase": "kl_divergence"}, {"score": 0.0026976845075643314, "phrase": "similar_strings"}, {"score": 0.002678078466226442, "phrase": "training_videos"}, {"score": 0.0024987660207458555, "phrase": "hidden_markov_model"}, {"score": 0.0024268966855267153, "phrase": "extensive_experiments"}, {"score": 0.002280948283235427, "phrase": "six_human_actions"}, {"score": 0.002264364091368269, "phrase": "gesture_datasets"}, {"score": 0.0022397131088479514, "phrase": "composite_dataset"}, {"score": 0.002128169201317242, "phrase": "discriminative_nature"}, {"score": 0.0021049977753042253, "phrase": "proposed_representation"}], "paper_keywords": ["Human actions", " one-shot learning", " unsupervised clustering", " gestures", " facial expressions", " action representation", " action recognition", " motion primitives", " motion patterns", " histogram of motion primitives", " motion primitives strings", " Hidden Markov model"], "paper_abstract": "This paper proposes a novel representation of articulated human actions and gestures and facial expressions. The main goals of the proposed approach are: 1) to enable recognition using very few examples, i.e., one or k-shot learning, and 2) meaningful organization of unlabeled datasets by unsupervised clustering. Our proposed representation is obtained by automatically discovering high-level subactions or motion primitives, by hierarchical clustering of observed optical flow in four-dimensional, spatial, and motion flow space. The completely unsupervised proposed method, in contrast to state-of-the-art representations like bag of video words, provides a meaningful representation conducive to visual interpretation and textual labeling. Each primitive action depicts an atomic subaction, like directional motion of limb or torso, and is represented by a mixture of four-dimensional Gaussian distributions. For one-shot and k-shot learning, the sequence of primitive labels discovered in a test video are labeled using KL divergence, and can then be represented as a string and matched against similar strings of training videos. The same sequence can also be collapsed into a histogram of primitives or be used to learn a Hidden Markov model to represent classes. We have performed extensive experiments on recognition by one and k-shot learning as well as unsupervised action clustering on six human actions and gesture datasets, a composite dataset, and a database of facial expressions. These experiments confirm the validity and discriminative nature of the proposed representation.", "paper_title": "Discovering Motion Primitives for Unsupervised Grouping and One-Shot Learning of Human Actions, Gestures, and Expressions", "paper_id": "WOS:000319060600008"}