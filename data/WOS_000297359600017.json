{"auto_keywords": [{"score": 0.040134789762190114, "phrase": "locality"}, {"score": 0.016404939654013503, "phrase": "la"}, {"score": 0.00481495049065317, "phrase": "mpi_parallel_applications"}, {"score": 0.004656082803151484, "phrase": "mpi_communications"}, {"score": 0.004597861947580946, "phrase": "dynamic-compi"}, {"score": 0.004296155586683981, "phrase": "mpi"}, {"score": 0.004003348169928476, "phrase": "first_technique"}, {"score": 0.0038778727136483046, "phrase": "romio"}, {"score": 0.003485437176235259, "phrase": "file_accesses"}, {"score": 0.00299632091372281, "phrase": "second_technique"}, {"score": 0.0029587924529332695, "phrase": "adaptive-compi"}, {"score": 0.002897283610433351, "phrase": "run-time_compression"}, {"score": 0.002500987934534128, "phrase": "real_hpc_applications"}, {"score": 0.0023879771786116228, "phrase": "considered_scenarios"}, {"score": 0.002367983991444061, "phrase": "important_reductions"}, {"score": 0.002338306897057454, "phrase": "execution_time"}, {"score": 0.002204646157481869, "phrase": "additional_benefits"}, {"score": 0.002131720106846481, "phrase": "total_communication_time"}, {"score": 0.0021049977753042253, "phrase": "network_contention"}], "paper_keywords": ["MPI library", " Parallel techniques", " Clusters architectures", " Compression algorithms", " Collective I/O", " Adaptive systems", " Heuristics"], "paper_abstract": "This work presents an optimization of MPI communications, called Dynamic-CoMPI, which uses two techniques in order to reduce the impact of communications and non-contiguous I/O requests in parallel applications. These techniques are independent of the application and complementaries to each other. The first technique is an optimization of the Two-Phase collective I/O technique from ROMIO, called Locality aware strategy for Two-Phase I/O (LA-Two-Phase I/O). In order to increase the locality of the file accesses, LA-Two-Phase I/O employs the Linear Assignment Problem (LAP) for finding an optimal I/O data communication schedule. The main purpose of this technique is the reduction of the number of communications involved in the I/O collective operation. The second technique, called Adaptive-CoMPI, is based on run-time compression of MPI messages exchanged by applications. Both techniques can be applied on every application, because both of them are transparent for the users. Dynamic-CoMPI has been validated by using several MPI benchmarks and real HPC applications. The results show that, for many of the considered scenarios, important reductions in the execution time are achieved by reducing the size and the number of the messages. Additional benefits of our approach are the reduction of the total communication time and the network contention, thus enhancing, not only performance, but also scalability.", "paper_title": "Dynamic-CoMPI: dynamic optimization techniques for MPI parallel applications", "paper_id": "WOS:000297359600017"}