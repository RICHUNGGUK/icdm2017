{"auto_keywords": [{"score": 0.0480015295510878, "phrase": "model_compensation"}, {"score": 0.040171231939978944, "phrase": "model_compensation_methods"}, {"score": 0.00481495049065317, "phrase": "noise-corrupted_speech"}, {"score": 0.004699916946246509, "phrase": "speech_recognisers"}, {"score": 0.004496080495247288, "phrase": "incoming_observations"}, {"score": 0.00445997627516653, "phrase": "model_compensation_techniques"}, {"score": 0.004406360418456518, "phrase": "recogniser's_state-conditional_distributions"}, {"score": 0.004266501496765747, "phrase": "target_environment"}, {"score": 0.003999907278478564, "phrase": "gaussian_speech"}, {"score": 0.003935892934907777, "phrase": "corrupted_speech_distribution"}, {"score": 0.0038885527668812807, "phrase": "closed_form"}, {"score": 0.003749908582313067, "phrase": "parametric_distribution"}, {"score": 0.00367775741468104, "phrase": "gaussian"}, {"score": 0.0036017714982462252, "phrase": "gaussians"}, {"score": 0.0033628392198449134, "phrase": "non-parametric_method"}, {"score": 0.003269162006886173, "phrase": "corrupted_speech_observation"}, {"score": 0.0031398318568608505, "phrase": "noise_distributions"}, {"score": 0.0031020367403802773, "phrase": "mismatch_function"}, {"score": 0.0029553380263320195, "phrase": "theoretical_bound"}, {"score": 0.0028042108531816943, "phrase": "novel_method"}, {"score": 0.0027704444816096505, "phrase": "performance_comparison"}, {"score": 0.002639385610609024, "phrase": "kl_divergence"}, {"score": 0.002607598760956807, "phrase": "ideal_compensation"}, {"score": 0.002385872451981323, "phrase": "taylor_series"}, {"score": 0.0023666757095801524, "phrase": "vts"}, {"score": 0.0022006705963260433, "phrase": "gaussian-for-gaussian_compensation"}, {"score": 0.0021566565951738658, "phrase": "speech_recognition"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Speech recognition", " Noise-robustness"], "paper_abstract": "One way of making speech recognisers more robust to noise is model compensation. Rather than enhancing the incoming observations, model compensation techniques modify a recogniser's state-conditional distributions so they model the speech in the target environment. Because the interaction between speech and noise is non-linear, even for Gaussian speech and noise the corrupted speech distribution has no closed form. Thus, model compensation methods approximate it with a parametric distribution, such as a Gaussian or a mixture of Gaussians. The impact of this approximation has never been quantified. This paper therefore introduces a non-parametric method to compute the likelihood of a corrupted speech observation. It uses sampling and, given speech and noise distributions and a mismatch function, is exact in the limit. It therefore gives a theoretical bound for model compensation. Though computing the likelihood is computationally expensive, the novel method enables a performance comparison based on the criterion that model compensation methods aim to minimise: the KL divergence to the ideal compensation. It gives the point where the Kullback-Leibler (KL) divergence is zero. This paper examines the performance of various compensation methods, such as vector Taylor series (VTS) and data-driven parallel model combination (DPMC). It shows that more accurate modelling than Gaussian-for-Gaussian compensation improves the performance of speech recognition. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Importance sampling to compute likelihoods of noise-corrupted speech", "paper_id": "WOS:000311524800018"}