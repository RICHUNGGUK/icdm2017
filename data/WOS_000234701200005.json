{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "language_recognition"}, {"score": 0.004770211811866158, "phrase": "support_vector_machines"}, {"score": 0.004573953147933065, "phrase": "powerful_technique"}, {"score": 0.00453144352814823, "phrase": "pattern_classification"}, {"score": 0.004385733482617193, "phrase": "high-dimensional_space"}, {"score": 0.004185632403886255, "phrase": "critical_aspect"}, {"score": 0.003994624397219346, "phrase": "inner_product"}, {"score": 0.003848088904418221, "phrase": "high_dimensional_mapping"}, {"score": 0.003604421041893448, "phrase": "key_part"}, {"score": 0.0033919477901198716, "phrase": "feature_vectors"}, {"score": 0.003206916619690171, "phrase": "generalized_linear_discriminants"}, {"score": 0.00297576670831646, "phrase": "explicit_expansion"}, {"score": 0.002948066398723126, "phrase": "svm_feature_space"}, {"score": 0.0028133741785871867, "phrase": "support_vectors"}, {"score": 0.0027741761880779535, "phrase": "single_model_vector"}, {"score": 0.002735522834514124, "phrase": "low_computational_complexity"}, {"score": 0.002672457917975491, "phrase": "svm"}, {"score": 0.002622755848331379, "phrase": "simpler_mean-squared_error_classifier"}, {"score": 0.0023996890513009743, "phrase": "gaussian_mixture_models"}, {"score": 0.0022686625017583387, "phrase": "language_evaluations"}, {"score": 0.0021649437083408425, "phrase": "traditional_gmm_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": [""], "paper_abstract": "Support vector machines (SVMs) have proven to be a powerful technique for pattern classification. SVMs map inputs into a high-dimensional space and then separate classes with a hyperplane. A critical aspect of using SVMs successfully is the design of the inner product, the kernel, induced by the high dimensional mapping. We consider the application of SVMs to speaker and language recognition. A key part of our approach is the use of a kernel that compares sequences of feature vectors and produces a measure of similarity. Our sequence kernel is based upon generalized linear discriminants. We show that this strategy has several important properties. First, the kernel uses an explicit expansion into SVM feature space-this property makes it possible to collapse all support vectors into a single model vector and have low computational complexity. Second, the SVM builds upon a simpler mean-squared error classifier to produce a more accurate system. Finally, the system is competitive and complimentary to other approaches, such as Gaussian mixture models (GMMs). We give results for the 2003 NIST speaker and language evaluations of the system and also show fusion with the traditional GMM approach. (c) 2005 Elsevier Ltd. All rights reserved.", "paper_title": "Support vector machines for speaker and language recognition", "paper_id": "WOS:000234701200005"}