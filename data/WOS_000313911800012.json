{"auto_keywords": [{"score": 0.038639607688939065, "phrase": "compiler_optimizations"}, {"score": 0.00481495049065317, "phrase": "layout-oblivious_compiler_optimization"}, {"score": 0.00476323161560059, "phrase": "matrix_computations"}, {"score": 0.004611369021812453, "phrase": "mathematical_operations"}, {"score": 0.004488505301335364, "phrase": "preconceived_data_structures"}, {"score": 0.004007083686875716, "phrase": "widely_used_matrix_computations"}, {"score": 0.00394264312173188, "phrase": "linpack_library"}, {"score": 0.00385832528624328, "phrase": "complex_internal_organizations"}, {"score": 0.0038168424095396205, "phrase": "data_structures"}, {"score": 0.00355782527287734, "phrase": "data-layout-oblivious_optimization_methodology"}, {"score": 0.0034442607586033657, "phrase": "abstract_representation"}, {"score": 0.003352388337911934, "phrase": "complex_implementation_details"}, {"score": 0.003057910095857785, "phrase": "varying_state-of-the-art_compiler_technologies"}, {"score": 0.0028503222085563026, "phrase": "pluto"}, {"score": 0.0028195727965553367, "phrase": "epod."}, {"score": 0.0026710653215026685, "phrase": "computational_kernel"}, {"score": 0.002613870237087035, "phrase": "different_data_layouts"}, {"score": 0.0025717770066591076, "phrase": "alternative_implementations"}, {"score": 0.0025031188149336257, "phrase": "common_set"}, {"score": 0.002320449502337321, "phrase": "data_layout"}, {"score": 0.0021510820434023207, "phrase": "conventional_approaches"}, {"score": 0.0021049977753042253, "phrase": "unified_representation"}], "paper_keywords": ["Design", " Performance", " Compiler", " optimization", " pattern", " matrix computation", " layout"], "paper_abstract": "Most scientific computations serve to apply mathematical operations to a set of preconceived data structures, e. g., matrices, vectors, and grids. In this article, we use a number of widely used matrix computations from the LINPACK library to demonstrate that complex internal organizations of data structures can severely degrade the effectiveness of compiler optimizations. We then present a data-layout-oblivious optimization methodology, where by isolating an abstract representation of the computations from complex implementation details of their data, we enable these computations to be much more accurately analyzed and optimized through varying state-of-the-art compiler technologies. We evaluated our approach on an Intel 8-core platform using two source-to-source compiler infrastructures, Pluto and EPOD. Our results show that while the efficiency of a computational kernel differs when using different data layouts, the alternative implementations typically benefit from a common set of optimizations on the operations. Therefore separately optimizing the operations and the data layout of a computation could dramatically enhance the effectiveness of compiler optimizations compared with the conventional approaches of using a unified representation.", "paper_title": "Layout-Oblivious Compiler Optimization for Matrix Computations", "paper_id": "WOS:000313911800012"}