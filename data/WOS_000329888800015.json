{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "handwritten_word"}, {"score": 0.003876996400581697, "phrase": "query-by-example_handwritten_word"}, {"score": 0.0029504765287722465, "phrase": "historical_handwritten_documents"}, {"score": 0.0027446057983474994, "phrase": "bag-of-visual-words_model"}, {"score": 0.0022630008787375435, "phrase": "user's_needs"}, {"score": 0.002209060366991921, "phrase": "classic_ranked_list"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Handwritten word spotting", " Query by example", " Relevance feedback", " Query fusion", " Multidimensional scaling"], "paper_abstract": "In this paper, we study the effect of taking the user into account in a query-by-example handwritten word spotting framework. Several off-the-shelf query fusion and relevance feedback strategies have been tested in the handwritten word spotting context. The increase in terms of precision when the user is included in the loop is assessed using two datasets of historical handwritten documents and two baseline word spotting approaches both based on the bag-of-visual-words model. We finally present two alternative ways of presenting the results to the user that might be more attractive and suitable to the user's needs than the classic ranked list. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Boosting the handwritten word spotting experience by including the user in the loop", "paper_id": "WOS:000329888800015"}