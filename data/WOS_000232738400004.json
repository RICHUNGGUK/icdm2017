{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "ridge_regression"}, {"score": 0.015556930206334117, "phrase": "reduced_rank_data"}, {"score": 0.004714666548704314, "phrase": "hawkins"}, {"score": 0.004665315202654531, "phrase": "yin"}, {"score": 0.004520272777451894, "phrase": "data_anal"}, {"score": 0.0037005154613244363, "phrase": "direct_implementation"}, {"score": 0.0031428202743657057, "phrase": "alternative_algorithm"}, {"score": 0.002339246282204109, "phrase": "ridge_regression_estimator"}, {"score": 0.002219040041886641, "phrase": "generalized_cross-validation_score"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["bidiagonal factorization", " cross validation", " matrix factorization", " ridge regression"], "paper_abstract": "Hawkins and Yin (Comput. Statist. Data Anal. 40 (2002) 253) describe an algorithm for ridge regression of reduced rank data, i.e. data where p, the number of variables, is larger than n, the number of observations. Whereas a direct implementation of ridge regression in this setting requires calculations of order O(np(2) + p(3)), their algorithm uses only calculations of order O(np(2)). In this paper, we describe an alternative algorithm based on a factorization of the (transposed) design matrix. This approach is numerically more stable, further reduces the amount of calculations and needs less memory. In particular, we show that the factorization can be calculated in O(n(2)p) operations. Once the factorization is obtained, for any value of the ridge parameter the ridge regression estimator can be calculated in O(np) operations and the generalized cross-validation score in O(n) operations. (c) 2004 Elsevier B.V. All rights reserved.", "paper_title": "An even faster algorithm for ridge regression of reduced rank data", "paper_id": "WOS:000232738400004"}