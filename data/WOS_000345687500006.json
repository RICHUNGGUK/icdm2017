{"auto_keywords": [{"score": 0.035671887773907354, "phrase": "theoretical_bound"}, {"score": 0.0048208015580140234, "phrase": "domain"}, {"score": 0.00476527679607755, "phrase": "weighted_majority_votes"}, {"score": 0.004732444672509326, "phrase": "perturbed_variation-based_self-labeling"}, {"score": 0.004619295212985837, "phrase": "domain_adaptation_problem"}, {"score": 0.004340553321613485, "phrase": "different_distributions"}, {"score": 0.004295752645085445, "phrase": "key_applied_issue"}, {"score": 0.0041211075804202905, "phrase": "new_distribution"}, {"score": 0.004022515690178811, "phrase": "label_information"}, {"score": 0.003939880387831148, "phrase": "classification_models"}, {"score": 0.0038857317730249114, "phrase": "weighted_majority_vote"}, {"score": 0.0038190873308414333, "phrase": "real-valued_functions"}, {"score": 0.003740620962133175, "phrase": "germain"}, {"score": 0.0032121901685844803, "phrase": "well_performing_majority_vote_learning_algorithm"}, {"score": 0.002956037056206635, "phrase": "domain_adaptation_scenario"}, {"score": 0.002875261849352716, "phrase": "recent_perturbed_variation_divergence"}, {"score": 0.002825900139534348, "phrase": "harel"}, {"score": 0.0028063923845869547, "phrase": "mannor"}, {"score": 0.002701474449898187, "phrase": "target_risk"}, {"score": 0.002627638918403063, "phrase": "mincq"}, {"score": 0.002279660253812632, "phrase": "original_process"}, {"score": 0.002149231173009159, "phrase": "rotation_and_translation_synthetic_problem"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Machine learning", " Classification", " Domain adaptation", " Majority vote", " PAC-Bayes"], "paper_abstract": "In machine learning, the domain adaptation problem arrives when the test (target) and the train (source) data are generated from different distributions. A key applied issue is thus the design of algorithms able to generalize on a new distribution, for which we have no label information. We focus on learning classification models defined as a weighted majority vote over a set of real-valued functions. In this context, Germain et al. [1] have shown that a measure of disagreement between these functions is crucial to control. The core of this measure is a theoretical bound the C-bound [21 which involves the disagreement and leads to a well performing majority vote learning algorithm in usual non-adaptative supervised setting: MinCq. In this work, we propose a framework to extend MinCq to a domain adaptation scenario. This procedure takes advantage of the recent perturbed variation divergence between distributions proposed by Harel and Mannor [3]. justified by a theoretical bound on the target risk of the vote, we provide to MinCq a target sample labeled thanks to a perturbed variation-based self-labeling focused on the regions where the source and target marginals appear similar. We also study the influence of our self-labeling, from which we deduce an original process for tuning the hyperparameters. Finally, our framework called PV-MinCq shows very promising results on a rotation and translation synthetic problem. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Domain adaptation of weighted majority votes via perturbed variation-based self-labeling", "paper_id": "WOS:000345687500006"}