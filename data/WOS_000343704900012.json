{"auto_keywords": [{"score": 0.04971414456915432, "phrase": "kernel_methods"}, {"score": 0.030085977154011265, "phrase": "model_complexity"}, {"score": 0.00481495049065317, "phrase": "model_selection"}, {"score": 0.004652016085325825, "phrase": "major_role"}, {"score": 0.004443276312972159, "phrase": "unknown_underlying_function"}, {"score": 0.004359078118356132, "phrase": "sample_vectors"}, {"score": 0.0043258453436258405, "phrase": "known_outcomes"}, {"score": 0.00411589720697736, "phrase": "nonlinear_functions"}, {"score": 0.003976523732243618, "phrase": "first_problem"}, {"score": 0.003901135641717704, "phrase": "bias-versus-variance_tradeoff"}, {"score": 0.003769006950727621, "phrase": "underlying_function"}, {"score": 0.0032585499655442404, "phrase": "second_problem"}, {"score": 0.0031481164109237636, "phrase": "appropriate_parameters"}, {"score": 0.0031121406603482112, "phrase": "kernel_function"}, {"score": 0.002983700956478823, "phrase": "new_smoothing_kernel_criterion"}, {"score": 0.0028936226331006563, "phrase": "estimated_function"}, {"score": 0.0027741761880779535, "phrase": "multiobjective_optimization"}, {"score": 0.0024633601573822114, "phrase": "learned_function"}, {"score": 0.002361632539339945, "phrase": "model_fit"}, {"score": 0.002272794656458419, "phrase": "extensive_experimental_evaluations"}, {"score": 0.0022125996721544514, "phrase": "machine_learning"}, {"score": 0.0021956951486095805, "phrase": "pattern_recognition"}, {"score": 0.0021705796895687864, "phrase": "computer_vision"}, {"score": 0.002113086135962444, "phrase": "proposed_approach_yields"}, {"score": 0.0021049977753042253, "phrase": "smaller_estimation_errors"}], "paper_keywords": ["Kernel methods", " kernel optimization", " optimization", " Pareto optimality", " regression"], "paper_abstract": "Regression plays a major role in many scientific and engineering problems. The goal of regression is to learn the unknown underlying function from a set of sample vectors with known outcomes. In recent years, kernel methods in regression have facilitated the estimation of nonlinear functions. However, two major (interconnected) problems remain open. The first problem is given by the bias-versus-variance tradeoff. If the model used to estimate the underlying function is too flexible (i.e., high model complexity), the variance will be very large. If the model is fixed (i.e., low complexity), the bias will be large. The second problem is to define an approach for selecting the appropriate parameters of the kernel function. To address these two problems, this paper derives a new smoothing kernel criterion, which measures the roughness of the estimated function as a measure of model complexity. Then, we use multiobjective optimization to derive a criterion for selecting the parameters of that kernel. The goal of this criterion is to find a tradeoff between the bias and the variance of the learned function. That is, the goal is to increase the model fit while keeping the model complexity in check. We provide extensive experimental evaluations using a variety of problems in machine learning, pattern recognition, and computer vision. The results demonstrate that the proposed approach yields smaller estimation errors as compared with methods in the state of the art.", "paper_title": "Multiobjective Optimization for Model Selection in Kernel Methods in Regression", "paper_id": "WOS:000343704900012"}