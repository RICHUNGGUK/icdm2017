{"auto_keywords": [{"score": 0.03426342587038006, "phrase": "proposed_stereo_method"}, {"score": 0.00481495049065317, "phrase": "postcapture_image_refocusing"}, {"score": 0.004760518730536645, "phrase": "dense_correspondence"}, {"score": 0.004618342952383313, "phrase": "stereoscopic_images"}, {"score": 0.004566123591480369, "phrase": "fundamental_problem"}, {"score": 0.004531637625717547, "phrase": "computer_vision"}, {"score": 0.004412965940679449, "phrase": "important_applications"}, {"score": 0.004363058588000252, "phrase": "intensive_past_research_efforts"}, {"score": 0.004168986485488861, "phrase": "depth_information"}, {"score": 0.0040290957541568775, "phrase": "input_images"}, {"score": 0.0039986494357811715, "phrase": "weakly_textured_regions"}, {"score": 0.0038207243221070166, "phrase": "computational_efficiency"}, {"score": 0.0037918465913284478, "phrase": "estimation_quality"}, {"score": 0.003748937184382714, "phrase": "hybrid_minimum_spanning_tree-based_stereo_matching_method"}, {"score": 0.0036093693971289754, "phrase": "efficient_nonlocal_cost_aggregation"}, {"score": 0.0034618177773967015, "phrase": "resulting_costs"}, {"score": 0.003371066990851953, "phrase": "large_textureless_regions"}, {"score": 0.0033455764382772754, "phrase": "fine_depth_discontinuities"}, {"score": 0.0032826873794020253, "phrase": "standard_middlebury_stereo_benchmark_show"}, {"score": 0.0032087739900195232, "phrase": "prior_local_and_nonlocal_aggregation-based_methods"}, {"score": 0.0031246358910995316, "phrase": "low_texture_regions"}, {"score": 0.002951673109860487, "phrase": "increasing_desire"}, {"score": 0.0029182434479165884, "phrase": "expressive_depth-induced_photo_effects"}, {"score": 0.0028095140358025, "phrase": "emerging_application"}, {"score": 0.0026945722090537397, "phrase": "real-world_stereo_image_pair"}, {"score": 0.0026040262154057607, "phrase": "accurate_thinlens_model"}, {"score": 0.0025843206540392184, "phrase": "synthetic_depth"}, {"score": 0.0025453546203590364, "phrase": "field_rendering"}, {"score": 0.002497470177389344, "phrase": "user-stroke_placement"}, {"score": 0.0024785689583828796, "phrase": "camera-specific_parameters"}, {"score": 0.0024411935317269705, "phrase": "pixel-adapted_gaussian"}, {"score": 0.0024043803470189455, "phrase": "principled_way"}, {"score": 0.002279841453843241, "phrase": "off-line_step"}, {"score": 0.0021865218861454256, "phrase": "in-focus_regions"}, {"score": 0.002137236968970666, "phrase": "touch_screen"}, {"score": 0.0021049977753042253, "phrase": "synthetically_refocused_images"}], "paper_keywords": ["Stereo matching", " depth estimation", " cost aggregation", " depth of field", " post-capture refocusing"], "paper_abstract": "Estimating dense correspondence or depth information from a pair of stereoscopic images is a fundamental problem in computer vision, which finds a range of important applications. Despite intensive past research efforts in this topic, it still remains challenging to recover the depth information both reliably and efficiently, especially when the input images contain weakly textured regions or are captured under uncontrolled, real-life conditions. Striking a desired balance between computational efficiency and estimation quality, a hybrid minimum spanning tree-based stereo matching method is proposed in this paper. Our method performs efficient nonlocal cost aggregation at pixel-level and region-level, and then adaptively fuses the resulting costs together to leverage their respective strength in handling large textureless regions and fine depth discontinuities. Experiments on the standard Middlebury stereo benchmark show that the proposed stereo method outperforms all prior local and nonlocal aggregation-based methods, achieving particularly noticeable improvements for low texture regions. To further demonstrate the effectiveness of the proposed stereo method, also motivated by the increasing desire to generate expressive depth-induced photo effects, this paper is tasked next to address the emerging application of interactive depth-of-field rendering given a real-world stereo image pair. To this end, we propose an accurate thinlens model for synthetic depth-of-field rendering, which considers the user-stroke placement and camera-specific parameters and performs the pixel-adapted Gaussian blurring in a principled way. Taking similar to 1.5 s to process a pair of 640 x 360 images in the off-line step, our system named Scribble2focus allows users to interactively select in-focus regions by simple strokes using the touch screen and returns the synthetically refocused images instantly to the user.", "paper_title": "Efficient Hybrid Tree-Based Stereo Matching With Applications to Postcapture Image Refocusing", "paper_id": "WOS:000340099800007"}