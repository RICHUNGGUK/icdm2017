{"auto_keywords": [{"score": 0.04937554412418187, "phrase": "multilayer_perceptrons"}, {"score": 0.014912190296895849, "phrase": "parameter_space"}, {"score": 0.013736563163372618, "phrase": "fisher_information_matrix"}, {"score": 0.00481495049065317, "phrase": "parameter_spaces"}, {"score": 0.004786519708868553, "phrase": "hierarchical_systems"}, {"score": 0.004605748839642513, "phrase": "hidden_units"}, {"score": 0.004524624065803229, "phrase": "geometrical_manifold"}, {"score": 0.004405596416783185, "phrase": "neural_networks"}, {"score": 0.004315178999089153, "phrase": "statistical_model"}, {"score": 0.004264343940547844, "phrase": "riemannian_metric"}, {"score": 0.003948145581074458, "phrase": "gaussian_mixture_probability_densities"}, {"score": 0.003924827704606888, "phrase": "arma"}, {"score": 0.00378767277402891, "phrase": "standard_statistical_paradigm"}, {"score": 0.003754140294151593, "phrase": "cramer-rao_theorem"}, {"score": 0.003633698607193056, "phrase": "strange_behaviors"}, {"score": 0.0036122174327316054, "phrase": "parameter_estimation"}, {"score": 0.0035908627899097407, "phrase": "hypothesis_testing"}, {"score": 0.0035696339388072086, "phrase": "bayesian_inference"}, {"score": 0.0035485301437669656, "phrase": "model_selection"}, {"score": 0.0032755054812060444, "phrase": "ordinary_statistical_theories"}, {"score": 0.0029437707610254255, "phrase": "statistical_manifolds"}, {"score": 0.002900426826591769, "phrase": "gaussian_mixtures"}, {"score": 0.002824005136794786, "phrase": "simple_toy_models"}, {"score": 0.0027741761880779535, "phrase": "explicit_solutions"}, {"score": 0.0027252240573281163, "phrase": "maximum_likelihood_estimator"}, {"score": 0.002669200981678889, "phrase": "gaussian_distribution"}, {"score": 0.00258347644721642, "phrase": "model_selection_criteria"}, {"score": 0.002560608637344898, "phrase": "aic"}, {"score": 0.002545453857932236, "phrase": "bic"}, {"score": 0.0025228755641693917, "phrase": "mdl"}, {"score": 0.0024421593308032565, "phrase": "bayesian"}, {"score": 0.002220551141101661, "phrase": "natural_gradient_method"}, {"score": 0.0021555948898187944, "phrase": "singular_geometrical_structure"}, {"score": 0.002123831297790248, "phrase": "generalization_error"}, {"score": 0.0021049977753042253, "phrase": "training_error"}], "paper_keywords": [""], "paper_abstract": "The parameter spaces of hierarchical systems such as multilayer perceptrons include singularities due to the symmetry and degeneration of hidden units. A parameter space forms a geometrical manifold, called the neuromanifold in the case of neural networks. Such a model is identified with a statistical model, and a Riemannian metric is given by the Fisher information matrix. However, the matrix degenerates at singularities. Such a singular structure is ubiquitous not only in multilayer perceptrons but also in the gaussian mixture probability densities, ARMA time-series model, and many other cases. The standard statistical paradigm of the Cramer-Rao theorem does not hold, and the singularity gives rise to strange behaviors in parameter estimation, hypothesis testing, Bayesian inference, model selection, and in particular, the dynamics of learning from examples. Prevailing theories so far have not paid much attention to the problem caused by singularity, relying only on ordinary statistical theories developed for regular (nonsingular) models. Only recently have researchers remarked on the effects of singularity, and theories are now being developed. This article gives an overview of the phenomena caused by the singularities of statistical manifolds related to multilayer perceptrons and gaussian mixtures. We demonstrate our recent results on these problems. Simple toy models are also used to show explicit solutions. We explain that the maximum likelihood estimator is no longer subject to the gaussian distribution even asymptotically, because the Fisher information matrix degenerates, that the model selection criteria such as AIC, BIC, and MDL fail to hold in these models, that a smooth Bayesian prior becomes singular in such models, and that the trajectories of dynamics of learning are strongly affected by the singularity, causing plateaus or slow manifolds in the parameter space. The natural gradient method is shown to perform well because it takes the singular geometrical structure into account. The generalization error and the training error are studied in some examples.", "paper_title": "Singularities affect dynamics of learning in neuromanifolds", "paper_id": "WOS:000237456900001"}