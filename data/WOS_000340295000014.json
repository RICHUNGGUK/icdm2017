{"auto_keywords": [{"score": 0.041996689288621714, "phrase": "mnne"}, {"score": 0.015719716506582538, "phrase": "nearest_neighbor_classification_error"}, {"score": 0.004690305164360866, "phrase": "nonparametric_dimension_reduction"}, {"score": 0.003802135020791801, "phrase": "favorable_criterion"}, {"score": 0.003703611021999795, "phrase": "supervised_linear_dimension_reduction"}, {"score": 0.0031223774204093713, "phrase": "mutual_information"}, {"score": 0.002737960968372391, "phrase": "bayes_optimal_criterion"}, {"score": 0.0025977542995083624, "phrase": "kernel_density_estimation"}, {"score": 0.0024325244760735566, "phrase": "nonparametric_algorithm"}, {"score": 0.0023694065215394593, "phrase": "mnne._experiments"}, {"score": 0.0023079225317714815, "phrase": "benchmark_data_sets"}, {"score": 0.0021049977753042253, "phrase": "existing_nonparametric_sldr_methods"}], "paper_keywords": ["Bayes optimal criterion", " nearest neighbor classification error (NN error)", " nonparametric methods", " supervised linear-dimension reduction (SLDR)"], "paper_abstract": "In this brief, we show that minimizing nearest neighbor classification error (MNNE) is a favorable criterion for supervised linear dimension reduction (SLDR). We prove that MNNE is better than maximizing mutual information in the sense of being a proxy of the Bayes optimal criterion. Based on kernel density estimation, we derive a nonparametric algorithm for MNNE. Experiments on benchmark data sets show the superiority of MNNE over existing nonparametric SLDR methods.", "paper_title": "Minimizing Nearest Neighbor Classification Error for Nonparametric Dimension Reduction", "paper_id": "WOS:000340295000014"}