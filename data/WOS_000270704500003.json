{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "synchronization_structure"}, {"score": 0.004766388573192505, "phrase": "parallel_programming"}, {"score": 0.00469445816161498, "phrase": "restricted_synchronization_structure"}, {"score": 0.004647105813224402, "phrase": "so-called_structured_parallel_programming_paradigms"}, {"score": 0.004507882471601552, "phrase": "programmer_productivity"}, {"score": 0.003991305899593133, "phrase": "programming_approach"}, {"score": 0.003736567662016446, "phrase": "potential_loss"}, {"score": 0.003642962673467835, "phrase": "parallel_computations"}, {"score": 0.003587924960158317, "phrase": "programming_model"}, {"score": 0.00351582806482809, "phrase": "computation_graph"}, {"score": 0.0034277338119963886, "phrase": "series-parallel_topology"}, {"score": 0.003341839487905525, "phrase": "well-known_structured_programming_models"}, {"score": 0.003258090521354736, "phrase": "analytical_model"}, {"score": 0.0030811340338342454, "phrase": "simple_parameters"}, {"score": 0.0030038988468808845, "phrase": "dag_topology"}, {"score": 0.0029735479904284706, "phrase": "workload_distribution"}, {"score": 0.0028551716387011637, "phrase": "wide_range"}, {"score": 0.0028263192354046245, "phrase": "synthetic_and_real-world_parallel_computations"}, {"score": 0.0027835848648596513, "phrase": "shared_and_distributed-memory_machines"}, {"score": 0.002540377258232509, "phrase": "performance_loss"}, {"score": 0.0024766636161181544, "phrase": "series-parallel_structured_model"}, {"score": 0.002226017732566121, "phrase": "workload_variability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Parallel programming models", " Task graphs", " Performance prediction"], "paper_abstract": "The restricted synchronization structure of so-called structured parallel programming paradigms has an advantageous effect on programmer productivity, cost modeling, and scheduling complexity. However, imposing these restrictions can lead to a loss of parallelism, compared to using a programming approach that does not impose synchronization structure. In this paper we study the potential loss of parallelism when expressing parallel computations into a programming model which limits the computation graph (DAG) to series-parallel topology, which characterizes all well-known structured programming models. We present an analytical model that approximately captures this loss of parallelism in terms of simple parameters that are related to DAG topology and workload distribution. We validate the model using a wide range of synthetic and real-world parallel computations running on shared and distributed-memory machines. Although the loss of parallelism is theoretically unbounded, our measurements show that for all above applications the performance loss due to choosing a series-parallel structured model is invariably limited up to 10%. In all cases, the loss of parallelism is predictable provided the topology and workload variability of the DAG are known. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Performance implications of synchronization structure in parallel programming", "paper_id": "WOS:000270704500003"}