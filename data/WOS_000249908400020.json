{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "neural_networks"}, {"score": 0.004762893197005439, "phrase": "confidence_rating"}, {"score": 0.0045850648862091085, "phrase": "input-output_knowledge_discovery"}, {"score": 0.004225950334493496, "phrase": "large_amount"}, {"score": 0.004046020290832207, "phrase": "general_technique"}, {"score": 0.003958937342925219, "phrase": "computational_burden"}, {"score": 0.0038737214010638745, "phrase": "operational_phase"}, {"score": 0.0037087324991876727, "phrase": "weighted_sum"}, {"score": 0.0035701213515857227, "phrase": "wide_variety"}, {"score": 0.0034554281349858836, "phrase": "multi-net_or_radial_basis_function_networks"}, {"score": 0.0032546105164772995, "phrase": "sum_terms"}, {"score": 0.002999386721353559, "phrase": "partial_output"}, {"score": 0.002918826667294298, "phrase": "overall_network_classification_criterion"}, {"score": 0.002719325177422626, "phrase": "network_units"}, {"score": 0.002386062690026682, "phrase": "binary_classification_problems"}, {"score": 0.002347378595930709, "phrase": "realadaboost_and_rbf_networks"}, {"score": 0.002284290601837099, "phrase": "important_computational_savings"}, {"score": 0.0022228943786652914, "phrase": "significant_degradation"}, {"score": 0.00217496508306238, "phrase": "recognition_accuracy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["artificial neural networks", " fast classification", " neural networks ensembles", " RealAdaboost", " radial basis function networks"], "paper_abstract": "Neural networks have become very useful tools for input-output knowledge discovery. However, some of the most powerful schemes require very complex machines and, thus, a large amount of calculation. This paper presents a general technique to reduce the computational burden associated with the operational phase of most neural networks that calculate their output as a weighted sum of terms, which comprises a wide variety of schemes, such as Multi-Net or Radial Basis Function networks. Basically, the idea consists on sequentially evaluating the sum terms, using a series of thresholds which are associated with the confidence that a partial output will coincide with the overall network classification criterion. Furthermore, we design some procedures for conveniently sorting out the network units, so that the most important ones are evaluated first. The possibilities of this strategy are illustrated with some experiments on a benchmark of binary classification problems, using RealAdaboost and RBF networks, which show that important computational savings can be achieved without significant degradation in terms of recognition accuracy. (c) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Fast evaluation of neural networks via confidence rating", "paper_id": "WOS:000249908400020"}