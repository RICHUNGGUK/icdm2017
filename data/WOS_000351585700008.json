{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "database_saliency"}, {"score": 0.04639787699031749, "phrase": "image_retrieval"}, {"score": 0.030468642146266864, "phrase": "visual_words"}, {"score": 0.004776669953449352, "phrase": "fast_image_retrieval"}, {"score": 0.004481132747216135, "phrase": "promising_performance"}, {"score": 0.004322967486451877, "phrase": "retrieval_efficiency"}, {"score": 0.004271490815763164, "phrase": "large-scale_database"}, {"score": 0.004187049198138107, "phrase": "practical_usage"}, {"score": 0.004104269996872886, "phrase": "relevant_images"}, {"score": 0.0038043055953253047, "phrase": "\"database_saliency"}, {"score": 0.00374399358035391, "phrase": "distinctiveness_score"}, {"score": 0.0034289106230496816, "phrase": "saliency-inspired_fast_image_retrieval_scheme"}, {"score": 0.003053598205821532, "phrase": "bottom-up_saliency_mechanism"}, {"score": 0.0030171910207671205, "phrase": "database_saliency_value"}, {"score": 0.002922207118572208, "phrase": "posterior_probability"}, {"score": 0.002898930717812433, "phrase": "local_patches"}, {"score": 0.002841545331522694, "phrase": "concurrent_information"}, {"score": 0.002686821995335494, "phrase": "top-down_saliency_mechanism"}, {"score": 0.0024604876113911173, "phrase": "database_saliency_values"}, {"score": 0.0023925401023682717, "phrase": "common_retrieval_benchmarks"}, {"score": 0.0023545757115891407, "phrase": "oxford"}, {"score": 0.002335877216786916, "phrase": "paris"}, {"score": 0.0023079225317714815, "phrase": "thorough_experiments"}, {"score": 0.002244178699939059, "phrase": "offline_database_saliency_computation"}, {"score": 0.0021389638834794136, "phrase": "online_retrieval"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_bow-based_image_retrieval_schemes"}], "paper_keywords": ["Bag-of-visual-words (BoW)", " bottom-up saliency", " database saliency", " image retrieval", " top-down saliency"], "paper_abstract": "The bag-of-visual-words (BoW) model is effective for representing images and videos in many computer vision problems, and achieves promising performance in image retrieval. Nevertheless, the level of retrieval efficiency in a large-scale database is not acceptable for practical usage. Considering that the relevant images in the database of a given query are more likely to be distinctive than ambiguous, this paper defines \"database saliency\" as the distinctiveness score calculated for every image to measure its overall \"saliency\" in the database. By taking advantage of database saliency, we propose a saliency-inspired fast image retrieval scheme, S-sim, which significantly improves efficiency while retains state-of-the-art accuracy in image retrieval. There are two stages in S-sim: the bottom-up saliency mechanism computes the database saliency value of each image by hierarchically decomposing a posterior probability into local patches and visual words, the concurrent information of visual words is then bottom-up propagated to estimate the distinctiveness, and the top-down saliency mechanism discriminatively expands the query via a very low-dimensional linear SVM trained on the top-ranked images after initial search, ranking images are then sorted on their distances to the decision boundary as well as the database saliency values. We comprehensively evaluate S-sim on common retrieval benchmarks, e.g., Oxford and Paris datasets. Thorough experiments suggest that, because of the offline database saliency computation and online low-dimensional SVM, our approach significantly speeds up online retrieval and outperforms the state-of-the-art BoW-based image retrieval schemes.", "paper_title": "Database Saliency for Fast Image Retrieval", "paper_id": "WOS:000351585700008"}