{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "ibp"}, {"score": 0.004642292792558686, "phrase": "four-parameter_ibp_compound_dirichlet_process"}, {"score": 0.004435112270935294, "phrase": "sparse_non-negative_vectors"}, {"score": 0.0043947942344037105, "phrase": "potentially_an_unbounded_number"}, {"score": 0.00419861271409079, "phrase": "icdp"}, {"score": 0.004122604238556912, "phrase": "sparse_matrices"}, {"score": 0.004066498558415824, "phrase": "infinite_number"}, {"score": 0.0039928721865882, "phrase": "power-law_characteristics"}, {"score": 0.0039027036493682887, "phrase": "four-parameter_icdp"}, {"score": 0.003849579089885659, "phrase": "nonparametric_topic"}, {"score": 0.003660879039645457, "phrase": "large_text_corpora"}, {"score": 0.003611034322786669, "phrase": "power-law_distribution"}, {"score": 0.0035294580355761506, "phrase": "natural_languages"}, {"score": 0.0034027387694902287, "phrase": "latent_ibp_compound_dirichlet_allocation"}, {"score": 0.0032955848190195343, "phrase": "power-law_distributions"}, {"score": 0.0030668695279736907, "phrase": "pitman-yor"}, {"score": 0.0028995712226514746, "phrase": "sparse_variant"}, {"score": 0.0028600623410146796, "phrase": "hierarchical_pitman-yor_process"}, {"score": 0.0027322025124213566, "phrase": "efficient_and_simple_collapsed_gibbs_sampler"}, {"score": 0.0026704265616705023, "phrase": "collapsed_gibbs_sampler"}, {"score": 0.0026461080202599694, "phrase": "latent_dirichlet_allocation"}, {"score": 0.0025162511343171, "phrase": "wide_range"}, {"score": 0.0024147474002390763, "phrase": "widely_used_hierarchical_dirichlet_process"}, {"score": 0.002275299774554847, "phrase": "benchmark_corpora"}, {"score": 0.0021735249249471614, "phrase": "real_data"}], "paper_keywords": ["Bayesian nonparametrics", " power-law distribution", " sparse modelling", " topic modelling", " clustering", " bag-of-words representation", " Gibbs sampling"], "paper_abstract": "We introduce the four-parameter IBP compound Dirichlet process (ICDP), a stochastic process that generates sparse non-negative vectors with potentially an unbounded number of entries. If we repeatedly sample from the ICDP we can generate sparse matrices with an infinite number of columns and power-law characteristics. We apply the four-parameter ICDP to sparse nonparametric topic modelling to account for the very large number of topics present in large text corpora and the power-law distribution of the vocabulary of natural languages. The model, which we call latent IBP compound Dirichlet allocation (LIDA), allows for power-law distributions, both, in the number of topics summarising the documents and in the number of words defining each topic. It can be interpreted as a sparse variant of the hierarchical Pitman-Yor process when applied to topic modelling. We derive an efficient and simple collapsed Gibbs sampler closely related to the collapsed Gibbs sampler of latent Dirichlet allocation (LDA), making the model applicable in a wide range of domains. Our nonparametric Bayesian topic model compares favourably to the widely used hierarchical Dirichlet process and its heavy tailed version, the hierarchical Pitman-Yor process, on benchmark corpora. Experiments demonstrate that accounting for the power-distribution of real data is beneficial and that sparsity provides more interpretable results.", "paper_title": "Latent IBP Compound Dirichlet Allocation", "paper_id": "WOS:000349625500009"}