{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "automatic_sign_language_recognition"}, {"score": 0.04472882712454684, "phrase": "non-manual_cues"}, {"score": 0.0035605741849764187, "phrase": "facial_expressions"}, {"score": 0.002962543869356265, "phrase": "recognition_phase"}, {"score": 0.0028108690791085536, "phrase": "computer_vision_issues"}, {"score": 0.00266693888510356, "phrase": "facial_features"}, {"score": 0.0025977542995083624, "phrase": "eye_gaze"}, {"score": 0.0023079225317714815, "phrase": "classification_approaches"}, {"score": 0.0021049977753042253, "phrase": "overall_sign_language_recognition_architecture"}], "paper_keywords": ["Automatic sign language recognition", " Facial expressions", " Head pose", " Eye gaze"], "paper_abstract": "Present work deals with the incorporation of non-manual cues in automatic sign language recognition. More specifically, eye gaze, head pose, and facial expressions are discussed in relation to their grammatical and syntactic function and means of including them in the recognition phase are investigated. Computer vision issues related to extracting facial features, eye gaze, and head pose cues are presented and classification approaches for incorporating these non-manual cues into the overall Sign Language recognition architecture are introduced.", "paper_title": "Non-manual cues in automatic sign language recognition", "paper_id": "WOS:000329238400005"}