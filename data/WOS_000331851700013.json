{"auto_keywords": [{"score": 0.04855239486253465, "phrase": "extreme_learning_machine"}, {"score": 0.03696244992309851, "phrase": "computational_complexity"}, {"score": 0.02633555152922206, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "multi-output_two-stage"}, {"score": 0.004246729915110547, "phrase": "regressor_terms"}, {"score": 0.004094639048362357, "phrase": "model_error_reduction_ratio"}, {"score": 0.003916098498242888, "phrase": "sparse_model"}, {"score": 0.003853115465608543, "phrase": "highly_noisy_learning_conditions"}, {"score": 0.003806541299696042, "phrase": "main_objective"}, {"score": 0.0036553070575015344, "phrase": "generalization_capability"}, {"score": 0.0035819501337406596, "phrase": "multi-output_regression_problems"}, {"score": 0.003398015125667192, "phrase": "novel_multi-output"}, {"score": 0.0033163269450817716, "phrase": "model_construction"}, {"score": 0.003133252930886608, "phrase": "new_algorithm"}, {"score": 0.0030953529970646626, "phrase": "nonlinear_parameters"}, {"score": 0.0029602552900111407, "phrase": "gaussian_function"}, {"score": 0.0028890603352364273, "phrase": "polynomial_term"}, {"score": 0.0028081571732259314, "phrase": "elm."}, {"score": 0.0027854565462509095, "phrase": "initial_multi-output_litp_model"}, {"score": 0.002707444334463544, "phrase": "termination_criteria"}, {"score": 0.002674681176379472, "phrase": "first_stage"}, {"score": 0.0026103363762901423, "phrase": "selected_regressor"}, {"score": 0.0025578967205807843, "phrase": "insignificant_ones"}, {"score": 0.002506507889886399, "phrase": "second_stage"}, {"score": 0.002436289099444074, "phrase": "optimized_compact_model"}, {"score": 0.0023970487263732737, "phrase": "regularized_parameters"}, {"score": 0.0023016843709107297, "phrase": "proper_regression_context"}, {"score": 0.002255431275670053, "phrase": "fast_implementation"}, {"score": 0.0022101055924375725, "phrase": "simulation_results"}, {"score": 0.0021569129153653777, "phrase": "proposed_technique"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme learning machine", " Multi-output linear-in-the-parameters (LITP) model", " Regularization", " Two-stage stepwise selection"], "paper_abstract": "This paper investigates the construction of linear-in-the-parameters (LITP) models for multi-output regression problems. Most existing stepwise forward algorithms choose the regressor terms one by one, each time maximizing the model error reduction ratio. The drawback is that such procedures cannot guarantee a sparse model, especially under highly noisy learning conditions. The main objective of this paper is to improve the sparsity and generalization capability of a model for multi-output regression problems, while reducing the computational complexity. This is achieved by proposing a novel multi-output two-stage locally regularized model construction (MTLRMC) method using the extreme learning machine (ELM). In this new algorithm, the nonlinear parameters in each term, such as the width of the Gaussian function and the power of a polynomial term, are firstly determined by the ELM. An initial multi-output LITP model is then generated according to the termination criteria in the first stage. The significance of each selected regressor is checked and the insignificant ones are replaced at the second stage. The proposed method can produce an optimized compact model by using the regularized parameters. Further, to reduce the computational complexity, a proper regression context is used to allow fast implementation of the proposed method. Simulation results confirm the effectiveness of the proposed technique. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "A multi-output two-stage locally regularized model construction method using the extreme learning machine", "paper_id": "WOS:000331851700013"}