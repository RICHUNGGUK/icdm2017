{"auto_keywords": [{"score": 0.0406718782151503, "phrase": "ddelta"}, {"score": 0.015719716506582538, "phrase": "delta_compression"}, {"score": 0.0047409388179452255, "phrase": "efficient_data_reduction_approach"}, {"score": 0.004620093797504386, "phrase": "similar_data_chunks"}, {"score": 0.004525629293824217, "phrase": "storage_systems"}, {"score": 0.004410248125081398, "phrase": "main_challenges"}, {"score": 0.004123781267600542, "phrase": "steadily_increasing_storage"}, {"score": 0.004081385289516914, "phrase": "network_bandwidth"}, {"score": 0.003531474887552889, "phrase": "data_deduplication_techniques"}, {"score": 0.003405948098114941, "phrase": "basic_idea"}, {"score": 0.003234299589505403, "phrase": "delta_encoding"}, {"score": 0.0031354779851351287, "phrase": "novel_approach"}, {"score": 0.0030872017736888113, "phrase": "gear-based_chunking"}, {"score": 0.0030554300179552415, "phrase": "spooky-based_fingerprinting"}, {"score": 0.002977419551510206, "phrase": "duplicate_strings"}, {"score": 0.0029467742734417255, "phrase": "delta_calculation"}, {"score": 0.002841971477743441, "phrase": "content_locality"}, {"score": 0.0028127163745564777, "phrase": "redundant_data"}, {"score": 0.0025361866386341796, "phrase": "ddelta_prototype"}, {"score": 0.0024971144603069006, "phrase": "real-world_datasets"}, {"score": 0.002420762293555924, "phrase": "encoding_speedup"}, {"score": 0.002358917661465133, "phrase": "decoding_speedup"}, {"score": 0.00226322835801988, "phrase": "xdelta"}, {"score": 0.0022399174232611853, "phrase": "zdelta"}, {"score": 0.002194011800331242, "phrase": "comparable_level"}, {"score": 0.0021714122348552747, "phrase": "compression_ratio"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Delta compression", " Deduplication", " Content locality", " Content-defined chunking", " Fingerprinting"], "paper_abstract": "Delta compression is an efficient data reduction approach to removing redundancy among similar data chunks and files in storage systems. One of the main challenges facing delta compression is its low encoding speed, a worsening problem in face of the steadily increasing storage and network bandwidth and speed. In this paper, we present Ddelta, a deduplication-inspired fast delta compression scheme that effectively leverages the simplicity and efficiency of data deduplication techniques to improve delta encoding/decoding performance. The basic idea behind Ddelta is to (1) accelerate the delta encoding and decoding processes by a novel approach of combining Gear-based chunking and Spooky-based fingerprinting for fast identification of duplicate strings for delta calculation, and (2) exploit content locality of redundant data to detect more duplicates by greedily scanning the areas immediately adjacent to already detected duplicate chunks/strings. Our experimental evaluation of a Ddelta prototype based on real-world datasets shows that Ddelta achieves an encoding speedup of 2.5x-8x and a decoding speedup of 2x-20x over the classic delta-compression approaches Xdelta and Zdelta while achieving a comparable level of compression ratio. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Ddelta: A deduplication-inspired fast delta compression approach", "paper_id": "WOS:000342266200017"}