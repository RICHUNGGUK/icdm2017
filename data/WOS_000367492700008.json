{"auto_keywords": [{"score": 0.04955111375780364, "phrase": "intel_tbb."}, {"score": 0.00481495049065317, "phrase": "victim_selection_policy"}, {"score": 0.004709333998298903, "phrase": "wide_adoption"}, {"score": 0.004657393640152439, "phrase": "chip_multiprocessors"}, {"score": 0.004530023871441992, "phrase": "almost_all_ict_segments"}, {"score": 0.004191540451296395, "phrase": "parallel_programming"}, {"score": 0.004076858589744185, "phrase": "energy_efficiency"}, {"score": 0.0038782499819752423, "phrase": "new_set"}, {"score": 0.0037931018103829427, "phrase": "parallelization_overheads"}, {"score": 0.00370981613076245, "phrase": "sub-linear_speedups"}, {"score": 0.003608266640131664, "phrase": "energy_consumption"}, {"score": 0.0034324073240680213, "phrase": "specific_operations"}, {"score": 0.0033570147767493716, "phrase": "new_tasks"}, {"score": 0.003283272764794988, "phrase": "task_queue"}, {"score": 0.003123203316511694, "phrase": "failed_steals"}, {"score": 0.0030545815929619306, "phrase": "largest_overhead"}, {"score": 0.002905629026254221, "phrase": "tbb's_victim_selection_policy"}, {"score": 0.0028260302580927856, "phrase": "new_occupancy-aware_policy"}, {"score": 0.0027031706928674092, "phrase": "pseudo-random_policy"}, {"score": 0.00262910358108176, "phrase": "previous_paper"}, {"score": 0.002486987042061596, "phrase": "\"oracle_scheme"}, {"score": 0.0024054226746197706, "phrase": "tbb's_random_victim_selection_approach"}, {"score": 0.0023136311468961125, "phrase": "execution_times"}, {"score": 0.0021763973978248005, "phrase": "tbb's_default_policy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Intel TBB", " Victim selection", " Parallelization overheads"], "paper_abstract": "The wide adoption of Chip Multiprocessors (CMPs) in almost all ICT segments has triggered a change in the way software needs to be developed. Parallel programming maximizes the performance and energy efficiency of CMPs, but also comes with a new set of challenges. Parallelization overheads can account for sub-linear speedups and can increase the energy consumption of applications. In past experiments we looked at specific operations such as spawning new tasks, dequeuing the task queue and task stealing for Intel TBB. Our results showed that failed steals account for the largest overhead. In this work, we focus on TBB's victim selection policy. We implement a new occupancy-aware policy and we improve the implementation of the pseudo-random policy we proposed in a previous paper. We compare the results of our new policies against an \"oracle scheme\" as well as against TBB's random victim selection approach. Our results show improvements in execution times and energy-efficiency of up to 11.23% and 14.72% respectively when compared to TBB's default policy. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Tuning the victim selection policy of Intel TBB", "paper_id": "WOS:000367492700008"}