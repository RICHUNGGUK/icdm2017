{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multi-class_learning_problems"}, {"score": 0.0046970764288494764, "phrase": "multilayer_perceptrons"}, {"score": 0.00443308765153716, "phrase": "good_convergence"}, {"score": 0.004396600680348434, "phrase": "ae_-nail_part"}, {"score": 0.004342430078306461, "phrase": "original_larger-scale_data_sets"}, {"score": 0.0041666157975446564, "phrase": "small-scale_subsets"}, {"score": 0.004014452814186653, "phrase": "n-class_problem"}, {"score": 0.0039649719120779066, "phrase": "two-class_problems"}, {"score": 0.0038678251731669865, "phrase": "class-modular_mlps"}, {"score": 0.003726533059542558, "phrase": "class-modular_mlp"}, {"score": 0.0036352070472201086, "phrase": "decision_boundaries"}, {"score": 0.0034165310204389682, "phrase": "represented_class"}, {"score": 0.0033743949483348626, "phrase": "neighboring_ones"}, {"score": 0.0033053159828082095, "phrase": "two-class_problem"}, {"score": 0.003171358146412577, "phrase": "unbalanced_training_data"}, {"score": 0.003145223158153384, "phrase": "locally_sparse_and_weak_distribution_regions"}, {"score": 0.0029437270961011077, "phrase": "minority_classes"}, {"score": 0.0028953973490895746, "phrase": "thin_regions"}, {"score": 0.0028360962841331634, "phrase": "suitable_enlargement_factors"}, {"score": 0.00276653157720066, "phrase": "effective_range"}, {"score": 0.002676418520290942, "phrase": "correction_coefficient"}, {"score": 0.0024842242921857705, "phrase": "economic_learning_subsets"}, {"score": 0.0024535580495414783, "phrase": "virtual_balance"}, {"score": 0.0024333239726872604, "phrase": "imbalanced_training_sets"}, {"score": 0.002373615081691254, "phrase": "generalization_regions"}, {"score": 0.0022679195103377124, "phrase": "extended_handwritten_digital_recognitions"}, {"score": 0.0022306602656266545, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["task decomposition", " multi-class learning data sets", " modular multilayer perceptrons", " unbalanced classes", " weak distribution regions", " output amendment"], "paper_abstract": "One of keys for multilayer perceptrons (MLPs) to solve the multi-class learning problems is how to make them act good convergence and ae -nail part of the original larger-scale data sets. This paper, neralization performances merely through learning small-scale subsets, i.e., a si first decomposes an n-class problem into n two-class problems, and then uses n class-modular MLPs to solve them one by one. A class-modular MLP is responsible for forming the decision boundaries of its represented class, and thus can be trained only by the samples from the represented class and some neighboring ones. When solving a two-class problem, an MLP has to face with such unfavorable situations as unbalanced training data, locally sparse and weak distribution regions, and open decision boundaries. One of solutions is that the samples from the minority classes or in the thin regions are virtually reinforced by suitable enlargement factors. And next, the effective range of an MLP is localized by a correction coefficient related to the distribution of its represented class. In brief, this paper focuses on the formation of economic learning subsets, the virtual balance of imbalanced training sets, and the localization of generalization regions of MLPs. The results for the letter and the extended handwritten digital recognitions show that the proposed methods are effective. (C) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Task decomposition and modular single-hidden-layer perceptron classifiers for multi-class learning problems", "paper_id": "WOS:000246534800009"}