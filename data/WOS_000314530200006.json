{"auto_keywords": [{"score": 0.04315348129771582, "phrase": "robbins"}, {"score": 0.03070536580711638, "phrase": "lai"}, {"score": 0.009579535728626005, "phrase": "ucb"}, {"score": 0.005125110112591681, "phrase": "burnetas"}, {"score": 0.005053261908540933, "phrase": "katehakis"}, {"score": 0.00481495049065317, "phrase": "weak-consistent_policies"}, {"score": 0.004747429622531595, "phrase": "stochastic_multi-armed_bandit_problem"}, {"score": 0.004486640769041646, "phrase": "lower_bounds"}, {"score": 0.004392565073466714, "phrase": "classical_model"}, {"score": 0.004330941610682453, "phrase": "stochastic_multi-armed_bandit"}, {"score": 0.004240116703465252, "phrase": "well-known_result"}, {"score": 0.003629557750546267, "phrase": "logarithmic_bound"}, {"score": 0.0035533891890176823, "phrase": "consistent_policies"}, {"score": 0.0030199761431186434, "phrase": "logarithmic_bounds"}, {"score": 0.002853808773854451, "phrase": "hannan_consistency"}, {"score": 0.002603012037901889, "phrase": "adaptive_policy"}, {"score": 0.0021501863832966966, "phrase": "popular_upper_confidence_bounds"}], "paper_keywords": ["stochastic bandits", " regret lower bounds", " consistency", " selectivity", " UCB policies"], "paper_abstract": "This paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit. A well-known result of Lai and Robbins, which has then been extended by Burnetas and Katehakis, has established the presence of a logarithmic bound for all consistent policies. We relax the notion of consistency, and exhibit a generalisation of the bound. We also study the existence of logarithmic bounds in general and in the case of Hannan consistency. Moreover, we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment. To get these results, we study variants of popular Upper Confidence Bounds (UCB) policies.", "paper_title": "Lower Bounds and Selectivity of Weak-Consistent Policies in Stochastic Multi-Armed Bandit Problem", "paper_id": "WOS:000314530200006"}