{"auto_keywords": [{"score": 0.04721001669280442, "phrase": "bpunn"}, {"score": 0.012997848328155542, "phrase": "bpsnn"}, {"score": 0.010815583386726696, "phrase": "existing_networks"}, {"score": 0.008713768692145006, "phrase": "boolean_functions"}, {"score": 0.004904042664266892, "phrase": "bpups"}, {"score": 0.004636756166068091, "phrase": "neural_networks"}, {"score": 0.004557943646825813, "phrase": "binary_product-unit_neural_network"}, {"score": 0.004465126965932753, "phrase": "binary_pi-sigma_neural_network"}, {"score": 0.004374192072307276, "phrase": "network_weights"}, {"score": 0.004299823165101385, "phrase": "one-step_training"}, {"score": 0.004197817125707719, "phrase": "addition_\"sigma"}, {"score": 0.004028525144566374, "phrase": "special_weighting_operations"}, {"score": 0.003919459003028067, "phrase": "logical_operators"}, {"score": 0.0037356237848699867, "phrase": "boolean_algebra"}, {"score": 0.0033701115914053083, "phrase": "proposed_two_neural_networks"}, {"score": 0.0032340913850061764, "phrase": "complete_truth_table"}, {"score": 0.003211959446076516, "phrase": "n_variables"}, {"score": 0.0031572878619497624, "phrase": "false_assignments"}, {"score": 0.0031249312057114237, "phrase": "corresponding_boolean_function"}, {"score": 0.002771001596749451, "phrase": "hidden_nodes"}, {"score": 0.002569294946452846, "phrase": "incomplete_truth_tables"}, {"score": 0.0024910622362019573, "phrase": "complete_truth_tables"}, {"score": 0.002278152865899792, "phrase": "supporting_numerical_experiments"}, {"score": 0.002193634795186667, "phrase": "risk_bounds"}], "paper_keywords": ["Binary pi-sigma neural network", " binary product-unit neural network", " Boolean function", " principle conjunctive normal form", " principle disjunctive normal form"], "paper_abstract": "In order to more efficiently realize Boolean functions by using neural networks, we propose a binary product-unit neural network (BPUNN) and a binary pi-sigma neural network (BPSNN). The network weights can be determined by one-step training. It is shown that the addition \"sigma,\" the multiplication \" pi,\" and two kinds of special weighting operations in BPUNN and BPSNN can implement the logical operators \".,\" \".,\" and \" -\" on Boolean algebra < Z(2), boolean OR, boolean AND, - 0, 1 > (Z(2) = {0, 1}), respectively. The proposed two neural networks enjoy the following advantages over the existing networks: 1) for a complete truth table of N variables with both truth and false assignments, the corresponding Boolean function can be realized by accordingly choosing a BPUNN or a BPSNN such that at most 2(N-1) hidden nodes are needed, while O(2(N)), precisely 2(N) or at most 2(N), hidden nodes are needed by existing networks; 2) a new network BPUPS based on a collaboration of BPUNN and BPSNN can be defined to deal with incomplete truth tables, while the existing networks can only deal with complete truth tables; and 3) the values of the weights are all simply -1 or 1, while the weights of all the existing networks are real numbers. Supporting numerical experiments are provided as well. Finally, we present the risk bounds of BPUNN, BPSNN, and BPUPS, and then analyze their probably approximately correct learnability.", "paper_title": "Binary Higher Order Neural Networks for Realizing Boolean Functions", "paper_id": "WOS:000290414400003"}