{"auto_keywords": [{"score": 0.043733065812188825, "phrase": "convex_combination"}, {"score": 0.011514597087976258, "phrase": "optimal_weights"}, {"score": 0.010915740571385238, "phrase": "convex_estimator"}, {"score": 0.00481495049065317, "phrase": "cross-validation_error_estimator"}, {"score": 0.004707188147442449, "phrase": "data_points"}, {"score": 0.004627944735913613, "phrase": "remaining_points"}, {"score": 0.0046018264570209765, "phrase": "computing_errors"}, {"score": 0.004524348699657099, "phrase": "left-out_points"}, {"score": 0.004144263368848345, "phrase": "\"zero_bootstrap\"_error"}, {"score": 0.004062937864702115, "phrase": "designed_classifier"}, {"score": 0.003971938976270902, "phrase": "low-biased_resubstitution"}, {"score": 0.003938340235432683, "phrase": "high-biased_zero_bootstrap"}, {"score": 0.0039050245929822354, "phrase": "convex_error_estimator"}, {"score": 0.0038283757748142156, "phrase": "unweighted_average"}, {"score": 0.0036795454918516794, "phrase": "feature-label_distribution"}, {"score": 0.003597102547969587, "phrase": "optimal_convex_combination"}, {"score": 0.003157547760145984, "phrase": "large_amount"}, {"score": 0.002975099633631871, "phrase": "main_body"}, {"score": 0.0029249294679606656, "phrase": "companion_website"}, {"score": 0.0028919522313933525, "phrase": "tabulated_results"}, {"score": 0.002843180211561427, "phrase": "classification_rules"}, {"score": 0.0026411384812121503, "phrase": "relevant_error_measures"}, {"score": 0.002611412331665652, "phrase": "mse"}, {"score": 0.0025965998485915024, "phrase": "mae"}, {"score": 0.002574590695234404, "phrase": "optimal_convex_estimator"}, {"score": 0.0025097154697345096, "phrase": "full_set"}, {"score": 0.002474380950654509, "phrase": "general_trends"}, {"score": 0.00241887505439182, "phrase": "general_conclusion"}, {"score": 0.0023445770108033288, "phrase": "substantial_improvement"}, {"score": 0.002311562103644741, "phrase": "classification_rule"}, {"score": 0.0022405528759521856, "phrase": "optimal_convex_bootstrap_estimators"}, {"score": 0.0021778895748464024, "phrase": "non-optimized_convex_estimators"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["bootstrap", " classification", " cross-validation", " error estimation", " feature-set ranking", " optimal estimation", " resubstitution", " companion website"], "paper_abstract": "A cross-validation error estimator is obtained by repeatedly leaving out some data points, deriving classifiers on the remaining points, computing errors for these classifiers on the left-out points, and then averaging these errors. The 0.632 bootstrap estimator is obtained by averaging the errors of classifiers designed from points drawn with replacement and then taking a convex combination of this \"zero bootstrap\" error with the resubstitution error for the designed classifier. This gives a convex combination of the low-biased resubstitution and the high-biased zero bootstrap. Another convex error estimator suggested in the literature is the unweighted average of resubstitution and cross-validation. This paper treats the following question: Given a feature-label distribution and classification rule. what is the optimal convex combination of two error estimators, i.e. what are the optimal weights for the convex combination. This problem is considered by finding the weights to minimize the MSE of a convex estimator. It also considers optimality under the constraint that the resulting estimator be unbiased. Owing to the large amount of results coming from the various feature-label models and error estimators, a portion of the results are presented herein and the main body of results appears on a companion website. In the tabulated results, each table treats the classification rules considered for the model, various Bayes errors, and various sample sizes. Each table includes the optimal weights, mean errors and standard deviations for the relevant error measures. and the MSE and MAE for the optimal convex estimator. Many observations can be made by considering the full set of experiments. Some general trends are outlined in the paper. The general conclusion is that optimizing the weights of a convex estimator can provide substantial improvement, depending on the classification rule. data model, sample size and component estimators. Optimal convex bootstrap estimators are applied to feature-set ranking to illustrate their potential advantage over non-optimized convex estimators. (c) 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Optimal convex error estimators for classification", "paper_id": "WOS:000239050200017"}