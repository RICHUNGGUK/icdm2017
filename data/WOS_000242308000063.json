{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "ensemble-based_active_learning"}, {"score": 0.0044695432851061525, "phrase": "training_instances"}, {"score": 0.004274257668843717, "phrase": "data_acquisition"}, {"score": 0.004027031316965272, "phrase": "candidate_training_instance"}, {"score": 0.0037940502405699765, "phrase": "ensemble_members"}, {"score": 0.0035744997458371335, "phrase": "binary_classification"}, {"score": 0.0033177813460815346, "phrase": "multi-class_problems"}, {"score": 0.0028795712616923462, "phrase": "ensemble_disagreement"}, {"score": 0.0027741761880779535, "phrase": "uncertainty_sampling"}, {"score": 0.0025556529150386168, "phrase": "active_learning"}, {"score": 0.002234491924392104, "phrase": "active_learning_strategies"}, {"score": 0.0021049977753042253, "phrase": "disagreement_measure"}], "paper_keywords": [""], "paper_abstract": "Ensemble-based active learning has been proven to efficiently reduce the number of training instances and thus the cost of data acquisition. To determine the utility of a candidate training instance, the disagreement about its class value among the ensemble members is used. While the disagreement for binary classification is easily determined using margins, the adaption to multi-class problems is not straightforward and little studied in the literature. In this paper we consider four approaches to measure ensemble disagreement, including margins, uncertainty sampling and entropy, and evaluate them empirically on various ensemble strategies for active learning. We show that margins outperform the other disagreement measures on three of four active learning strategies. Our experiments also show that some active learning strategies are more sensitive to the choice of disagreement measure than others.", "paper_title": "Multi-class ensemble-based active learning", "paper_id": "WOS:000242308000063"}