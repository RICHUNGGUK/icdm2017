{"auto_keywords": [{"score": 0.037549471362292176, "phrase": "common_parameterization"}, {"score": 0.00481495049065317, "phrase": "sparse_classification"}, {"score": 0.0047283116134926645, "phrase": "multiple-view_learning"}, {"score": 0.00460125407969429, "phrase": "multiple_camera_viewpoints"}, {"score": 0.004397000006837371, "phrase": "human_actions"}, {"score": 0.004088807948906792, "phrase": "feature_fusion_approach"}, {"score": 0.0038368442918503072, "phrase": "different_camera_viewpoints"}, {"score": 0.003767738552860633, "phrase": "multiple-view_dimensionality_reduction"}, {"score": 0.003287482882222731, "phrase": "available_viewpoints"}, {"score": 0.0032282396745967504, "phrase": "canonical_correlation_analysis"}, {"score": 0.002947611960613497, "phrase": "sparse_sequence_classifier"}, {"score": 0.0025716240685409513, "phrase": "proper_number"}, {"score": 0.002391067964886602, "phrase": "proposed_system"}, {"score": 0.002203016940021265, "phrase": "ixmas"}, {"score": 0.0021049977753042253, "phrase": "successful_results"}], "paper_keywords": ["Human Action Recognition", " Multiple View Learning", " L1 regularization"], "paper_abstract": "Employing multiple camera viewpoints in the recognition of human actions increases performance. This paper presents a feature fusion approach to efficiently combine 2D observations extracted from different camera viewpoints. Multiple-view dimensionality reduction is employed to learn a common parameterization of 2D action descriptors computed for each one of the available viewpoints. Canonical correlation analysis and their variants are employed to obtain such parameterizations. A sparse sequence classifier based on L1 regularization is proposed to avoid the problem of having to choose the proper number of dimensions of the common parameterization. The proposed system is employed in the classification of the Inria Xmas Motion Acquisition Sequences (IXMAS) data set with successful results.", "paper_title": "Human action recognition with sparse classification and multiple-view learning", "paper_id": "WOS:000342812800006"}