{"auto_keywords": [{"score": 0.04093270606843931, "phrase": "hpc_applications"}, {"score": 0.030643651862528227, "phrase": "mtf"}, {"score": 0.00481495049065317, "phrase": "dynamic_information"}, {"score": 0.004761968711051715, "phrase": "high_performance_computing_applications"}, {"score": 0.004726970265271088, "phrase": "high_performance_computing"}, {"score": 0.004456044928556639, "phrase": "large_number"}, {"score": 0.004216112320418982, "phrase": "specific_tasks"}, {"score": 0.003974349782102708, "phrase": "software_engineers"}, {"score": 0.003664315990489347, "phrase": "inter-process_communication_traces"}, {"score": 0.003597284507308585, "phrase": "hpc_application"}, {"score": 0.0035054881943660096, "phrase": "different_formats"}, {"score": 0.0032199943889464873, "phrase": "exchange_format"}, {"score": 0.0029906503184034634, "phrase": "de_facto_standard"}, {"score": 0.0029686314779877525, "phrase": "inter-process_communication"}, {"score": 0.0029467742734417255, "phrase": "high_performance_computing_systems"}, {"score": 0.002850385670169814, "phrase": "well-known_requirements"}, {"score": 0.0028189599390180536, "phrase": "standard_exchange_format"}, {"score": 0.0026768142335374156, "phrase": "mpi_traces"}, {"score": 0.0026084451971464867, "phrase": "better_synergy"}, {"score": 0.0025230952890994236, "phrase": "mtf_toolkit"}, {"score": 0.002422552850701705, "phrase": "query_engine"}, {"score": 0.0023519508354777215, "phrase": "mtf_traces"}, {"score": 0.002258212866482022, "phrase": "large_trace"}, {"score": 0.002176235362654451, "phrase": "analysis_tool"}, {"score": 0.0021601997088758957, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["High Performance Computing systems", " Inter-process communication traces", " Message passing interface", " Standard exchange format"], "paper_abstract": "High Performance Computing (HPC) systems tend to be complex to debug and analyze due to the large number of processes they involve and the way they communicate with each other to perform specific tasks. Recently, there has been an increase in the number of tools to help software engineers analyze the behavior of HPC applications. These tools provide several features that facilitate the understanding and analysis of the information contained in inter-process communication traces generated from running an HPC application. They, however, use different formats to represent traces, which hinders interoperability and sharing of data. In this paper, we address this by proposing an exchange format called MTF (MPI Trace Format) for representing and exchanging traces generated from HPC applications based on the MPI (Message Passing Interface) standard, which is a de facto standard for inter-process communication for high performance computing systems. The design of MTF is validated against well-known requirements for a standard exchange format, with an objective being to lead the work towards standardizing the way MPI traces are represented in order to allow better synergy among tools. We have also developed an MTF toolkit that supports the generation of MTF traces equipped with a query engine to facilitate the retrieval of data from MTF traces. Finally, we show how MTF can carry a large trace generated using a commercial off the shelf MPI trace analysis tool. Crown Copyright (C) 2010 Published by Elsevier B.V. All rights reserved.", "paper_title": "An exchange format for representing dynamic information generated from High Performance Computing applications", "paper_id": "WOS:000287285000005"}