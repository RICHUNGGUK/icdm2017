{"auto_keywords": [{"score": 0.03646802660069554, "phrase": "linear_combiner"}, {"score": 0.03553867513587153, "phrase": "output_layer"}, {"score": 0.004815047744679995, "phrase": "volterra"}, {"score": 0.004798937560652061, "phrase": "filtering_and_principal_component_analysis"}, {"score": 0.004767343085732912, "phrase": "echo"}, {"score": 0.004580243025274209, "phrase": "encouraging_compromise"}, {"score": 0.00443013401197632, "phrase": "resulting_mathematical_model"}, {"score": 0.004299228562179176, "phrase": "wide_range"}, {"score": 0.004270665568018583, "phrase": "nonlinear_dynamics"}, {"score": 0.00421410521089045, "phrase": "fixed_weights"}, {"score": 0.0041721750101390825, "phrase": "recurrent_connections"}, {"score": 0.004130660282810338, "phrase": "echo_state_approach"}, {"score": 0.004089556946377719, "phrase": "well-known_difficulties"}, {"score": 0.004048860957751714, "phrase": "recurrent_neural_network_training_strategies"}, {"score": 0.0038642157757169315, "phrase": "underlying_structure"}, {"score": 0.00378767277402891, "phrase": "feedback_loops"}, {"score": 0.003749969582830009, "phrase": "dynamical_reservoir"}, {"score": 0.003687960013746783, "phrase": "overall_training_process"}, {"score": 0.003404244237372396, "phrase": "linear_nature"}, {"score": 0.0032815823097599316, "phrase": "available_information"}, {"score": 0.0032489004068215407, "phrase": "higher-order_statistics"}, {"score": 0.00306973798582805, "phrase": "novel_architecture"}, {"score": 0.0029492339651639734, "phrase": "volterra_filter_structure"}, {"score": 0.002900426826591769, "phrase": "principal_component_analysis_technique"}, {"score": 0.002824005136794786, "phrase": "effective_signals"}, {"score": 0.0027131212233443137, "phrase": "processing_capability"}, {"score": 0.00260657976055413, "phrase": "training_process"}, {"score": 0.0025806029313384504, "phrase": "proposed_architecture"}, {"score": 0.0024875444192203485, "phrase": "representative_information_extraction_problems"}, {"score": 0.0023739321566540682, "phrase": "convolutive_mixtures"}, {"score": 0.0023502683992739845, "phrase": "obtained_results"}, {"score": 0.002288309252254464, "phrase": "already_proposed_esn_versions"}, {"score": 0.0022279798606617356, "phrase": "novel_network_proposal"}, {"score": 0.002183776628088228, "phrase": "promising_tool"}, {"score": 0.0021476098263617954, "phrase": "signal_processing_tasks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Echo state networks", " Volterra filtering", " Principal component analysis", " Channel equalization", " Source separation"], "paper_abstract": "Echo state networks (ESNs) can be interpreted as promoting an encouraging compromise between two seemingly conflicting objectives: (i) simplicity of the resulting mathematical model and (ii) capability to express a wide range of nonlinear dynamics. By imposing fixed weights to the recurrent connections, the echo state approach avoids the well-known difficulties faced by recurrent neural network training strategies, but still preserves, to a certain extent, the potential of the underlying structure due to the existence of feedback loops within the dynamical reservoir. Moreover, the overall training process is relatively simple, as it amounts essentially to adapting the readout, which usually corresponds to a linear combiner. However, the linear nature of the output layer may limit the capability of exploring the available information, since higher-order statistics of the signals are not taken into account. In this work, we present a novel architecture for an ESN in which the linear combiner is replaced by a Volterra filter structure. Additionally, the principal component analysis technique is used to reduce the number of effective signals transmitted to the output layer. This idea not only improves the processing capability of the network, but also preserves the simplicity of the training process. The proposed architecture is then analyzed in the context of a set of representative information extraction problems, more specifically supervised and unsupervised channel equalization, and blind separation of convolutive mixtures. The obtained results, when compared to those produced by already proposed ESN versions, highlight the benefits brought by the novel network proposal and characterize it as a promising tool to deal with challenging signal processing tasks. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "An extended echo state network using Volterra filtering and principal component analysis", "paper_id": "WOS:000306162600032"}