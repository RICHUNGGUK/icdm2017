{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "hard_support_vector_regression"}, {"score": 0.04159267344622788, "phrase": "regularization_technique"}, {"score": 0.004662036750452837, "phrase": "hsvr"}, {"score": 0.0041773934798743405, "phrase": "main_reason"}, {"score": 0.003865826699096508, "phrase": "upper_bound"}, {"score": 0.003791621842185417, "phrase": "lagrange_multipliers"}, {"score": 0.003531474887552889, "phrase": "greedy_stagewise_based_algorithm"}, {"score": 0.003486123304074186, "phrase": "approximately_train_hsvr"}, {"score": 0.003289117944111902, "phrase": "maximal_predicted_discrepancy"}, {"score": 0.002927780883930371, "phrase": "early_stopping_rule"}, {"score": 0.0027801631944598206, "phrase": "regression_machine"}, {"score": 0.0025725382476862305, "phrase": "well-known_software"}, {"score": 0.0023346234464901978, "phrase": "support_vectors"}, {"score": 0.002319566486828135, "phrase": "finally_experimental_results"}, {"score": 0.0022749744354271816, "phrase": "synthetic_and_real-world_benchmark_data_sets"}, {"score": 0.0021742244283295986, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd"}], "paper_keywords": ["Support vector regression", " Greedy algorithm", " Regularization technique"], "paper_abstract": "The hard support vector regression (HSVR) usually has a risk of suffering from overfitting due to the presence of noise The main reason is that It does not utilize the regularization technique to set an upper bound on the Lagrange multipliers so they can be magnified infinitely Hence we propose a greedy stagewise based algorithm to approximately train HSVR At each iteration the sample which has the maximal predicted discrepancy is selected and its weight is updated only once so as to avoid being excessively magnified Actually this early stopping rule can implicitly control the capacity of the regression machine which is equivalent to a regularization technique In addition compared with the well-known software LIBSVM2 82 our algorithm to a certain extent has advantages in both the training time and the number of support vectors Finally experimental results on the synthetic and real-world benchmark data sets also corroborate the efficacy of the proposed algorithm (C) 2010 Elsevier Ltd All rights reserved", "paper_title": "A fast method to approximately train hard support vector regression", "paper_id": "WOS:000285370800013"}