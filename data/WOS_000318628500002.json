{"auto_keywords": [{"score": 0.03884976197665516, "phrase": "np"}, {"score": 0.013489541262615413, "phrase": "optimal_solution"}, {"score": 0.00481495049065317, "phrase": "approximation_stability"}, {"score": 0.004775335634705251, "phrase": "common_approach"}, {"score": 0.004684157900507474, "phrase": "data_objects"}, {"score": 0.004620093797504386, "phrase": "metric_space"}, {"score": 0.0045318666714939905, "phrase": "natural_distance-based_objective"}, {"score": 0.004420891955499197, "phrase": "min-sum_score"}, {"score": 0.004218602458288531, "phrase": "implicit_hope"}, {"score": 0.00409264825541748, "phrase": "chosen_objective"}, {"score": 0.0040366412779632085, "phrase": "desired_\"target\"_clustering"}, {"score": 0.003421245054271364, "phrase": "polynomial_time_algorithms"}, {"score": 0.0027361626349903744, "phrase": "low-error_clusterings"}, {"score": 0.002676418520290942, "phrase": "values_c"}, {"score": 0.0024637379406427856, "phrase": "k-means_objectives"}, {"score": 0.0023507916951658455, "phrase": "target_clustering"}, {"score": 0.002305799666781492, "phrase": "min-sum_objective"}, {"score": 0.002286781334850213, "phrase": "target_clusters"}, {"score": 0.0021049977753042253, "phrase": "np-hard_value"}], "paper_keywords": ["Algorithms", " Theory", " Clustering", " Approximation Algorithms", " k-Median", " k-Means", " Min-Sum", " Clustering Accuracy"], "paper_abstract": "A common approach to clustering data is to view data objects as points in a metric space, and then to optimize a natural distance-based objective such as the k-median, k-means, or min-sum score. For applications such as clustering proteins by function or clustering images by subject, the implicit hope in taking this approach is that the optimal solution for the chosen objective will closely match the desired \"target\" clustering (e. g., a correct clustering of proteins by function or of images by who is in them). However, most distance-based objectives, including those mentioned here, are NP-hard to optimize. So, this assumption by itself is not sufficient, assuming P not equal NP, to achieve clusterings of low-error via polynomial time algorithms. In this article, we show that we can bypass this barrier if we slightly extend this assumption to ask that for some small constant c, not only the optimal solution, but also all c-approximations to the optimal solution, differ from the target on at most some epsilon fraction of points-we call this (c, epsilon)-approximation-stability. We show that under this condition, it is possible to efficiently obtain low-error clusterings even if the property holds only for values c for which the objective is known to be NP-hard to approximate. Specifically, for any constant c > 1, (c, epsilon)-approximation-stability of k-median or k-means objectives can be used to efficiently produce a clustering of error O(epsilon) with respect to the target clustering, as can stability of the min-sum objective if the target clusters are sufficiently large. Thus, we can perform nearly as well in terms of agreement with the target clustering as if we could approximate these objectives to this NP-hard value.", "paper_title": "Clustering under Approximation Stability", "paper_id": "WOS:000318628500002"}