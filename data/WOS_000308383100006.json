{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "new_family"}, {"score": 0.013313820490999143, "phrase": "mittag-leffler_function"}, {"score": 0.011702524986275786, "phrase": "fractional_calculus"}, {"score": 0.0047747348769918, "phrase": "fractional_entropies"}, {"score": 0.004122699822340296, "phrase": "exactly_the_inverse"}, {"score": 0.003986581218661184, "phrase": "mittag-leffler_logarithm"}, {"score": 0.003903789851015775, "phrase": "fractional_order"}, {"score": 0.00377487157059352, "phrase": "generalized_informational_entropies"}, {"score": 0.0033141428467975795, "phrase": "shannon's_entropy"}, {"score": 0.003286420941205958, "phrase": "renyi's_entropy"}, {"score": 0.00325893016361821, "phrase": "tsallis'_entropy"}, {"score": 0.003138030910104256, "phrase": "tsallis'_generalized_logarithm"}, {"score": 0.00284899583294417, "phrase": "quantum_mechanics"}, {"score": 0.0027088948471991454, "phrase": "informational_entropy"}, {"score": 0.002532724486603177, "phrase": "maximum_entropy_principle"}, {"score": 0.00246964807581858, "phrase": "uncertain_definition"}, {"score": 0.002438699976481979, "phrase": "fuzzy_definition"}, {"score": 0.002338306897057454, "phrase": "gaussian_normal_law"}, {"score": 0.002270495425498003, "phrase": "new_formulation"}, {"score": 0.002158780936704905, "phrase": "fractional_fisher_information"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Informational entropy", " Shannon entropy", " Fractional calculus", " Mittag-Leffler function", " Generalized entropy", " Fisher information"], "paper_abstract": "By generalizing the basic functional equation f(xy) = f(x) + f(y) in the form f(beta)(xy) = f(beta)(x)+ f(beta)(y), beta > 1, one can derive a family of solutions which are exactly the inverse of the Mittag-Leffler function, referred to as Mittag-Leffler logarithm, or logarithm of fractional order. This result provides a new family of generalized informational entropies which are indexed by a parameter clearly related to fractals, via fractional calculus, and which is quite relevant in the presence in defect of observation. The relation with Shannon's entropy, Renyi's entropy and Tsallis' entropy is clarified, and it is shown that Tsallis' generalized logarithm has a significance in terms of fractional calculus. The case beta = 2 looks like directly relevant to amplitude of probability in quantum mechanics, and provides an approach to the definition of \"amplitude of informational entropy\". One examines the kind of result one can so obtain in applying the maximum entropy principle. In the presence of uncertain definition (or fuzzy definition) the Mittag-Leffler function would be more relevant than the Gaussian normal law. To some extent, this new formulation could be fully supported by the derivation of a new family of fractional Fisher information. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Derivation of an amplitude of information in the setting of a new family of fractional entropies", "paper_id": "WOS:000308383100006"}