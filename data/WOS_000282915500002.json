{"auto_keywords": [{"score": 0.04068367224498143, "phrase": "machine_learning_systems"}, {"score": 0.00481495049065317, "phrase": "machine_learning"}, {"score": 0.004738325072560588, "phrase": "machine_learning's_ability"}, {"score": 0.00455202992136173, "phrase": "changing_and_complex_situations"}, {"score": 0.004338076511024884, "phrase": "fundamental_tool"}, {"score": 0.00426900754050533, "phrase": "computer_security"}, {"score": 0.003022465992278672, "phrase": "formal_structure"}, {"score": 0.002374855916752648, "phrase": "spambayes"}, {"score": 0.0021049977753042253, "phrase": "new_lines"}], "paper_keywords": ["Security", " Adversarial learning", " Adversarial environments"], "paper_abstract": "Machine learning's ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.", "paper_title": "The security of machine learning", "paper_id": "WOS:000282915500002"}