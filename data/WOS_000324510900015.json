{"auto_keywords": [{"score": 0.04461328156994424, "phrase": "video_sequences"}, {"score": 0.030410275128128987, "phrase": "localized_soft-assignment_coding"}, {"score": 0.00481495049065317, "phrase": "laplacian_pyramid_coding"}, {"score": 0.0047787596516662, "phrase": "action_recognition"}, {"score": 0.004689462599037994, "phrase": "new_descriptor"}, {"score": 0.004654210559456924, "phrase": "local_representation"}, {"score": 0.0046192222897398685, "phrase": "human_actions"}, {"score": 0.004365022891478405, "phrase": "spatio-temporal_features"}, {"score": 0.00409372989087314, "phrase": "laplacian_pyramid"}, {"score": 0.004062937864702115, "phrase": "efficiently_encoding_spatio-temporal_regions"}, {"score": 0.003956971099850658, "phrase": "structural_planes"}, {"score": 0.003927203683176278, "phrase": "motion_templates"}, {"score": 0.0037674258625824113, "phrase": "structural_and_motion_features"}, {"score": 0.0035331426502479687, "phrase": "sub-band_feature_maps"}, {"score": 0.003440945901489461, "phrase": "two-stage_feature_extraction"}, {"score": 0.0033511469200347907, "phrase": "motion-related_edge"}, {"score": 0.003083849594367653, "phrase": "gabor_filtering"}, {"score": 0.003026127736667187, "phrase": "filter_banks"}, {"score": 0.0029920129598858545, "phrase": "spatial_neighbors"}, {"score": 0.002958281631770322, "phrase": "obtained_local_features"}, {"score": 0.0027325449069867222, "phrase": "image_templates"}, {"score": 0.0026915398934872366, "phrase": "mhi"}, {"score": 0.0026712641194357174, "phrase": "top"}, {"score": 0.0025335509856200433, "phrase": "proposed_laplacian_pyramid_coding_descriptor"}, {"score": 0.0024395426929046415, "phrase": "multi-scale_analysis"}, {"score": 0.002340147988348115, "phrase": "robust_representation"}, {"score": 0.002305014991156598, "phrase": "experimental_results"}, {"score": 0.002279011030331704, "phrase": "benchmark_kth_dataset"}, {"score": 0.002169667676959813, "phrase": "proposed_method"}, {"score": 0.002153316680437517, "phrase": "human_action_recognition"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action recognition", " Laplacian pyramid", " Localized soft-assignment coding", " Max pooling"], "paper_abstract": "We present a new descriptor for local representation of human actions. In contrast to state-of-the-art descriptors, which use spatio-temporal features to describe cuboids detected from video sequences, we propose to employ a 2D descriptor based on the Laplacian pyramid for efficiently encoding spatio-temporal regions of interest. Image templates including structural planes and motion templates, are firstly extracted from a cuboid to encode the structural and motion features. A 2D Laplacian pyramid is then performed to decompose each of those images into a series of sub-band feature maps, which is followed by a two-stage feature extraction, i.e., Gabor filtering and max pooling. Motion-related edge and orientation information is enhanced after the filtering. To capture more discriminative and invariant features, max pooling is applied to the outputs of Gabor filtering, between scales within filter banks and over spatial neighbors. The obtained local features associated with cuboids are fed to the localized soft-assignment coding with max pooling on the Bag-of-Words (BoWs) model to represent an action. The image templates, i.e., MHI and TOP, explicitly encode the motion and structure information in the video sequences and the proposed Laplacian pyramid coding descriptor provides an informative representation of them due to the multi-scale analysis. The employment of localized soft-assignment coding and max pooling gives a robust representation of actions. Experimental results on the benchmark KTH dataset and the newly released and challenging HMDB51 dataset demonstrate the effectiveness of the proposed method for human action recognition. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "A local descriptor based on Laplacian pyramid coding for action recognition", "paper_id": "WOS:000324510900015"}