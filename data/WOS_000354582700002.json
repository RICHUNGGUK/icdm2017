{"auto_keywords": [{"score": 0.026403962817394162, "phrase": "appearance-based_representation"}, {"score": 0.014363813690007561, "phrase": "proposed_mpgd_representation"}, {"score": 0.008742401515817545, "phrase": "human-human_interactions"}, {"score": 0.00481495049065317, "phrase": "human-human_interactions_analysis"}, {"score": 0.004689210758225518, "phrase": "novel_view-invariant"}, {"score": 0.004620773128779254, "phrase": "geometric_descriptor"}, {"score": 0.0045399591300830456, "phrase": "human-human_interaction_representation"}, {"score": 0.00448686632980506, "phrase": "semantic_meaning"}, {"score": 0.004293215616253398, "phrase": "anatomical_planes"}, {"score": 0.0042429960722503305, "phrase": "motion_profile"}, {"score": 0.004205715668397308, "phrase": "pose_profile"}, {"score": 0.0038845358652982286, "phrase": "human-human_interaction_analysis"}, {"score": 0.0038617405001251325, "phrase": "namely_human-human_interaction_classification"}, {"score": 0.0037941508352767462, "phrase": "human-human_interaction_classification_problem"}, {"score": 0.0037387272682935686, "phrase": "hierarchical_classification_framework"}, {"score": 0.0036949697378821458, "phrase": "representation_layer"}, {"score": 0.003640989676914951, "phrase": "classification_framework"}, {"score": 0.003566735116775593, "phrase": "performed_interaction"}, {"score": 0.003535375475035123, "phrase": "input_video"}, {"score": 0.0033627909221532527, "phrase": "human-human_interaction_prediction_problem"}, {"score": 0.003294191205745385, "phrase": "ongoing_human-human_interaction"}, {"score": 0.00317047105369676, "phrase": "prediction_framework"}, {"score": 0.003133343395515431, "phrase": "proposed_mpgd"}, {"score": 0.0030966491684507646, "phrase": "accumulated_histograms-based_representation"}, {"score": 0.00303346212256215, "phrase": "accumulated_histograms"}, {"score": 0.0029367553751816237, "phrase": "support-vector-machine_classifiers"}, {"score": 0.002843122846060439, "phrase": "ongoing_interaction"}, {"score": 0.002688352108538072, "phrase": "microsoft_kinect_sensor"}, {"score": 0.002641245084026035, "phrase": "video_dataset"}, {"score": 0.0022996234912148463, "phrase": "average_accuracy"}, {"score": 0.002226259272038318, "phrase": "human-human_interaction_prediction_framework"}, {"score": 0.002200164358389725, "phrase": "average_prediction_accuracy"}, {"score": 0.002155230514219992, "phrase": "interaction_video"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Human-human interaction classification", " Human-human interaction prediction", " Bag of words", " Motion-pose geometric descriptor"], "paper_abstract": "In this paper, we present a novel view-invariant, motion-pose geometric descriptor (MPGD) as a human-human interaction representation to capture the semantic meaning of body-parts between two interacting humans. The proposed MPGD representation is based on utilizing the concept of anatomical planes to construct a motion profile and a pose profile for each human. Those two profiles are then concatenated to form a descriptor for the two interacting humans. Using the proposed MPGD representation, we study two problems related to human-human interaction analysis, namely human-human interaction classification and prediction. For the human-human interaction classification problem, we propose a hierarchical classification framework consisting of a representation layer and three classification layers. The classification framework aims to realize what is the performed interaction in an input video by understanding how and when each individual performed sub-activities to each other over time. The human-human interaction prediction problem aims to predict the class of ongoing human-human interaction at its early stages. To do so, we propose a prediction framework that utilizes the proposed MPGD to construct an accumulated histograms-based representation for an ongoing interaction. The accumulated histograms of MPGDs are then used to train a set of support-vector-machine classifiers with a probabilistic output to predict the class of an ongoing interaction. In order to evaluate our proposed MPGD representation and both the classification and the prediction frameworks, we utilize a Microsoft Kinect sensor to capture human-human interactions in a video dataset that consists of 12 interactions performed by 12 individuals. We evaluate the performance of our proposed classification framework and compare the results with an appearance-based representation and a representation that combines both the MPGD representation and the appearance-based representation. On the one hand, our proposed MPGD representation performance has shown promising results compared to the appearance-based representation with an average accuracy of 94.86% in classifying human-human interactions. On the other hand, human-human interaction prediction framework has achieved an average prediction accuracy of 82.46% with only 50% of the interaction video being observed. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Anatomical-plane-based representation for human-human interactions analysis", "paper_id": "WOS:000354582700002"}