{"auto_keywords": [{"score": 0.0452514208810669, "phrase": "candidate_solutions"}, {"score": 0.00481495049065317, "phrase": "model-based_methods"}, {"score": 0.004714023408114857, "phrase": "widespread_applications"}, {"score": 0.004639712384096822, "phrase": "hard_nondifferentiable_optimization_problems"}, {"score": 0.004085713541752114, "phrase": "recent_convergence_analysis"}, {"score": 0.003936910716320182, "phrase": "sample_size"}, {"score": 0.003522124318486352, "phrase": "model-based_algorithms"}, {"score": 0.0032014433546265694, "phrase": "stochastic_averaging_procedure"}, {"score": 0.002863921785266393, "phrase": "function_evaluations"}, {"score": 0.0028037753646260937, "phrase": "high-quality_solutions"}, {"score": 0.0027159094186967247, "phrase": "underlying_algorithms"}, {"score": 0.002658863179011018, "phrase": "parallel_computation"}, {"score": 0.0026168641536413978, "phrase": "detailed_implementation"}, {"score": 0.0025214217930287003, "phrase": "exemplary_algorithm"}, {"score": 0.002516807356021149, "phrase": "model"}, {"score": 0.0024423826953133844, "phrase": "annealing_random_search"}, {"score": 0.00241658945942583, "phrase": "stochastic_averaging"}, {"score": 0.002255431275670053, "phrase": "small_constant_value"}, {"score": 0.002196342933346204, "phrase": "global_convergence_property"}, {"score": 0.0021731423856364003, "phrase": "mars-sa"}, {"score": 0.002138799283553239, "phrase": "numerical_examples"}], "paper_keywords": ["Algorithms", " Performance", " Theory", " Global optimization", " model-based algorithms", " stochastic approximation"], "paper_abstract": "The model-based methods have recently found widespread applications in solving hard nondifferentiable optimization problems. These algorithms are population-based and typically require hundreds of candidate solutions to be sampled at each iteration. In addition, recent convergence analysis of these algorithms also stipulates a sample size that increases polynomially with the number of iterations. In this article, we aim to improve the efficiency of model-based algorithms by reducing the number of candidate solutions generated per iteration. This is carried out through embedding a stochastic averaging procedure within these methods to make more efficient use of the past sampling information. This procedure not only can potentially reduce the number of function evaluations needed to obtain high-quality solutions, but also makes the underlying algorithms more amenable for parallel computation. The detailed implementation of our approach is demonstrated through an exemplary algorithm instantiation called Model-based Annealing Random Search with Stochastic Averaging (MARS-SA), which maintains the per iteration sample size at a small constant value. We establish the global convergence property of MARS-SA and provide numerical examples to illustrate its performance.", "paper_title": "Model-Based Annealing Random Search with Stochastic Averaging", "paper_id": "WOS:000345582900004"}