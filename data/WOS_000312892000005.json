{"auto_keywords": [{"score": 0.049766045322984266, "phrase": "visual_saliency_modeling"}, {"score": 0.028403083934058767, "phrase": "evaluation_scores"}, {"score": 0.00481495049065317, "phrase": "quantitative_analysis_of_human-model_agreement"}, {"score": 0.004690305164360866, "phrase": "visual_attention"}, {"score": 0.004598933498016774, "phrase": "biological_and_machine_vision_systems"}, {"score": 0.004127164752958245, "phrase": "image_regions"}, {"score": 0.003941882222595145, "phrase": "\"visual_saliency"}, {"score": 0.00389048276567065, "phrase": "bottom-up_visual_saliency"}, {"score": 0.003814632693659797, "phrase": "numerous_research_efforts"}, {"score": 0.003728000957725218, "phrase": "computer_vision"}, {"score": 0.0036793800647811365, "phrase": "available_models"}, {"score": 0.0036194914688460656, "phrase": "different_datasets"}, {"score": 0.0034342984674830533, "phrase": "different_evaluation_scores"}, {"score": 0.0033233797886807375, "phrase": "human_eye_tracking"}, {"score": 0.0032266093099550955, "phrase": "direct_comparison"}, {"score": 0.0031223774204093713, "phrase": "exhaustive_comparison"}, {"score": 0.002914288956035066, "phrase": "model_rankings"}, {"score": 0.0027833043858224078, "phrase": "existing_datasets"}, {"score": 0.0026581913054027663, "phrase": "computational_complexity_analysis"}, {"score": 0.0025638362414466278, "phrase": "competitive_eye_movement_prediction_accuracy"}, {"score": 0.0024485643104830814, "phrase": "eye_movement_datasets"}, {"score": 0.002377206033025253, "phrase": "future_work"}, {"score": 0.002218668420806254, "phrase": "rapidly_growing_field"}, {"score": 0.0021825034787117986, "phrase": "unified_comparison_framework"}, {"score": 0.0021610874773389096, "phrase": "future_efforts"}, {"score": 0.002125859121671525, "phrase": "pascal_voc_challenge"}, {"score": 0.0021049977753042253, "phrase": "object_recognition"}], "paper_keywords": ["Bottom-up attention", " eye movement prediction", " model comparison", " visual attention", " visual saliency"], "paper_abstract": "Visual attention is a process that enables biological and machine vision systems to select the most relevant regions from a scene. Relevance is determined by two components: 1) top-down factors driven by task and 2) bottom-up factors that highlight image regions that are different from their surroundings. The latter are often referred to as \"visual saliency.\" Modeling bottom-up visual saliency has been the subject of numerous research efforts during the past 20 years, with many successful applications in computer vision and robotics. Available models have been tested with different datasets (e. g., synthetic psychological search arrays, natural images or videos) using different evaluation scores (e. g., search slopes, comparison to human eye tracking) and parameter settings. This has made direct comparison of models difficult. Here, we perform an exhaustive comparison of 35 state-of-the-art saliency models over 54 challenging synthetic patterns, three natural image datasets, and two video datasets, using three evaluation scores. We find that although model rankings vary, some models consistently perform better. Analysis of datasets reveals that existing datasets are highly center-biased, which influences some of the evaluation scores. Computational complexity analysis shows that some models are very fast, yet yield competitive eye movement prediction accuracy. Different models often have common easy/difficult stimuli. Furthermore, several concerns in visual saliency modeling, eye movement datasets, and evaluation scores are discussed and insights for future work are provided. Our study allows one to assess the state-of-the-art, helps to organizing this rapidly growing field, and sets a unified comparison framework for gauging future efforts, similar to the PASCAL VOC challenge in the object recognition and detection domains.", "paper_title": "Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study", "paper_id": "WOS:000312892000005"}