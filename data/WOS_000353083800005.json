{"auto_keywords": [{"score": 0.029735506681517452, "phrase": "gpu_computing"}, {"score": 0.014385627209853724, "phrase": "massively_computations"}, {"score": 0.01068784219624087, "phrase": "vector_instructions"}, {"score": 0.004814952278495465, "phrase": "multi-gpu"}, {"score": 0.004773040022691547, "phrase": "fdtd_scheme"}, {"score": 0.004752221133109136, "phrase": "vibroacoustic_applications"}, {"score": 0.004568871736067739, "phrase": "longitudinal_and_transversal_waves"}, {"score": 0.004539005582217309, "phrase": "stratified_media"}, {"score": 0.004421471944783611, "phrase": "acceleration_strategy"}, {"score": 0.004383013327354432, "phrase": "fdtd"}, {"score": 0.004232278141066795, "phrase": "bidimensional_scheme"}, {"score": 0.004204602964512469, "phrase": "fdtd_method"}, {"score": 0.004095692768056685, "phrase": "first_implementation"}, {"score": 0.004068907083697369, "phrase": "open_source_message"}, {"score": 0.0040334640350288, "phrase": "ompi"}, {"score": 0.003911817572798092, "phrase": "biprocessor_station"}, {"score": 0.003852371777780562, "phrase": "cpu_code_version"}, {"score": 0.003818818175185215, "phrase": "simd"}, {"score": 0.003793849879477278, "phrase": "sse"}, {"score": 0.0037198551869445334, "phrase": "avx"}, {"score": 0.003607630811220324, "phrase": "multi-core_platforms"}, {"score": 0.0035527910918846397, "phrase": "second_implementation"}, {"score": 0.003529543381704363, "phrase": "multi-gpu_code_version"}, {"score": 0.003445623521481353, "phrase": "cuda"}, {"score": 0.003334309053314909, "phrase": "accurate_analysis"}, {"score": 0.0032908057622663732, "phrase": "different_code_versions"}, {"score": 0.0032764307492734145, "phrase": "shared_memory_approaches"}, {"score": 0.003212636137126158, "phrase": "gpu"}, {"score": 0.003108735847242537, "phrase": "distributed_solutions"}, {"score": 0.002949598537993663, "phrase": "shared_memory_schemes"}, {"score": 0.002936709606809639, "phrase": "cpu_computing"}, {"score": 0.0028794008436429596, "phrase": "simulation_sizes"}, {"score": 0.0028480459059007468, "phrase": "cache_memory"}, {"score": 0.002749980017065414, "phrase": "tuned_cpu_version"}, {"score": 0.002690406375639144, "phrase": "explicit_vector_instructions"}, {"score": 0.0026494723416072316, "phrase": "memory_bandwidth"}, {"score": 0.0026321198895843173, "phrase": "limiting_factor"}, {"score": 0.0025694584194998356, "phrase": "sequential_version"}, {"score": 0.002535908872480815, "phrase": "memory_approach"}, {"score": 0.0024918548761791435, "phrase": "best_option"}, {"score": 0.002464709648714996, "phrase": "homogeneous_behaviour"}, {"score": 0.0024060240102889673, "phrase": "upper_limit"}, {"score": 0.0023333450140633726, "phrase": "peak_values"}, {"score": 0.0022235353755570794, "phrase": "earth_crust_profile"}, {"score": 0.0021563569154036907, "phrase": "acceleration_strategies"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["FDTD", " GPU computing", " OMPI", " SIMD extensions"], "paper_abstract": "The Finite-Difference Time-Domain (FDTD) method is applied to the analysis of vibroacoustic problems and to study the propagation of longitudinal and transversal waves in a stratified media. The potential of the scheme and the relevance of each acceleration strategy for massively computations in FDTD are demonstrated in this work. In this paper, we propose two new specific implementations of the bidimensional scheme of the FDTD method using multi-CPU and multi-GPU, respectively. In the first implementation, an open source message passing interface (OMPI) has been included in order to massively exploit the resources of a biprocessor station with two Intel Xeon processors. Moreover, regarding CPU code version, the streaming SIMD extensions (SSE) and also the advanced vectorial extensions (AVX) have been included with shared memory approaches that take advantage of the multi-core platforms. On the other hand, the second implementation called the multi-GPU code version is based on Peer-to-Peer communications available in CUDA on two GPUs (NVIDIA GTX 670). Subsequently, this paper presents an accurate analysis of the influence of the different code versions including shared memory approaches, vector instructions and multi-processors (both CPU and GPU) and compares them in order to delimit the degree of improvement of using distributed solutions based on multi-CPU and multi-GPU. The performance of both approaches was analysed and it has been demonstrated that the addition of shared memory schemes to CPU computing improves substantially the performance of vector instructions enlarging the simulation sizes that use efficiently the cache memory of CPUs. In this case GPU computing is slightly twice times faster than the fine tuned CPU version in both cases one and two nodes. However, for massively computations explicit vector instructions do not worth it since the memory bandwidth is the limiting factor and the performance tends to be the same than the sequential version with auto-vectorisation and also shared memory approach. In this scenario GPU computing is the best option since it provides a homogeneous behaviour. More specifically, the speedup of GPU computing achieves an upper limit of 12 for both one and two GPUs, whereas the performance reaches peak values of 80 GFlops and 146 GFlops for the performance for one GPU and two GPUs respectively. Finally, the method is applied to an earth crust profile in order to demonstrate the potential of our approach and the necessity of applying acceleration strategies in these type of applications. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Multi-GPU and multi-CPU accelerated FDTD scheme for vibroacoustic applications", "paper_id": "WOS:000353083800005"}