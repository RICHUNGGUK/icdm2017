{"auto_keywords": [{"score": 0.04638331353283265, "phrase": "visual_retrieval"}, {"score": 0.004527035067279947, "phrase": "gift"}, {"score": 0.004230068512904886, "phrase": "ad-hoc_retrieval"}, {"score": 0.00405127920820053, "phrase": "textual_retrieval"}, {"score": 0.0039040344230094164, "phrase": "lower_scores"}, {"score": 0.003808845109556467, "phrase": "text_retrieval"}, {"score": 0.003738973626996355, "phrase": "medical_retrieval"}, {"score": 0.003558830226733501, "phrase": "gabor_filters"}, {"score": 0.0033457659530207306, "phrase": "visual_features"}, {"score": 0.0031649018957020337, "phrase": "feedback_runs"}, {"score": 0.002797121441432477, "phrase": "target_image"}, {"score": 0.00230950802983053, "phrase": "machine_learning"}, {"score": 0.0021049977753042253, "phrase": "optimised_learning_strategies"}], "paper_keywords": [""], "paper_abstract": "This article describes the use of medGIFT and easyIR for three of four ImageCLEF 2005 tasks. All results rely on two systems: the GNU Image Finding Tool (GIFT) for visual retrieval, and easyIR for text. For ad-hoc retrieval, two visual runs were submitted. No textual retrieval was attempted, resulting in lower scores than those using text retrieval. For medical retrieval, visual retrieval was performed with several configurations of Gabor filters and grey level/color quantisations as well as combinations of text and visual features. Due to a lack of resources no feedback runs were created, an area where medGIFT performed best in 2004. For classification, a retrieval with the target image was performed and the first N = 1; 5; 10 results used to calculate scores for classes by simply adding up the scores for each class. No machine learning was performed, so results were surprisingly good and only topped by systems with optimised learning strategies.", "paper_title": "The use of MedGIFT and EasyIR for ImageCLEF 2005", "paper_id": "WOS:000241359000079"}