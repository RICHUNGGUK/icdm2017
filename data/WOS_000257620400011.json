{"auto_keywords": [{"score": 0.048313706083567, "phrase": "adaboost"}, {"score": 0.03890549897638756, "phrase": "adaboostsvm"}, {"score": 0.012289753535844904, "phrase": "component_classifiers"}, {"score": 0.00481495049065317, "phrase": "svm-based_component_classifiers"}, {"score": 0.004664352260912174, "phrase": "component_classifier"}, {"score": 0.004505405926927455, "phrase": "boosting_principle"}, {"score": 0.0044795842057358, "phrase": "svm"}, {"score": 0.004427966503146096, "phrase": "easy_classifier"}, {"score": 0.004143196873246147, "phrase": "second_international_workshop"}, {"score": 0.004119313626110943, "phrase": "multiple_classifier_systems"}, {"score": 0.004001938457469328, "phrase": "strong_component_classifiers"}, {"score": 0.003832092687255493, "phrase": "properly_designed_rbfsvm"}, {"score": 0.003777088546146369, "phrase": "rbf_kernel"}, {"score": 0.003554531289427866, "phrase": "better_generalization_performance"}, {"score": 0.003513645211548667, "phrase": "imbalanced_classification_problems"}, {"score": 0.0034832884999421374, "phrase": "key_idea"}, {"score": 0.0033937776228549557, "phrase": "trained_rbfsvm_component_classifiers"}, {"score": 0.0033547343924455455, "phrase": "large_a_values"}, {"score": 0.0033257460346754687, "phrase": "weak_learning"}, {"score": 0.003287482882222731, "phrase": "sigma_values"}, {"score": 0.0032309093154028663, "phrase": "boosting_iteration_proceeds"}, {"score": 0.003156984631072011, "phrase": "rbfsvm_component_classifiers"}, {"score": 0.0030936850548131923, "phrase": "better_generalization"}, {"score": 0.0030580836029381817, "phrase": "adaboost_approach"}, {"score": 0.0030404363386284756, "phrase": "svm_component_classifiers"}, {"score": 0.0028694530267725925, "phrase": "decision_trees"}, {"score": 0.0027159110355996956, "phrase": "valentini"}, {"score": 0.00270023138867066, "phrase": "dietterich"}, {"score": 0.002669976122388806, "phrase": "bias"}, {"score": 0.0026307897899760383, "phrase": "support_vector_machines"}, {"score": 0.0025929837295811673, "phrase": "svm-based_ensemble_methods"}, {"score": 0.0025631294247072476, "phrase": "machine_learning_research"}, {"score": 0.0024899850926034567, "phrase": "heterogeneous_svms"}, {"score": 0.0024049552650867935, "phrase": "diverse_adaboostsvm"}, {"score": 0.0023566987138026285, "phrase": "original_adaboost"}, {"score": 0.0022565201625076876, "phrase": "rbfsvm_component"}, {"score": 0.002211235429059514, "phrase": "good_balance"}, {"score": 0.0021857667493036786, "phrase": "promising_results"}, {"score": 0.0021543421286324945, "phrase": "benchmark_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["AdaBoost", " Support Vector Machine", " component classifier", " diversity"], "paper_abstract": "The use of SVM (Support Vector Machine) as component classifier in AdaBoost may seem like going against the grain of the Boosting principle since SVM is not an easy classifier to train. Moreover, Wickramaratna et al. [2001. Performance degradation in boosting. In: Proceedings of the Second International Workshop on Multiple Classifier Systems, pp. 11-21] show that AdaBoost with strong component classifiers is not viable. In this paper, we shall show that AdaBoost incorporating properly designed RBFSVM (SVM with the RBF kernel) component classifiers, which we call AdaBoostSVM, can perform as well as SVM. Furthermore, the proposed AdaBoostSVM demonstrates better generalization performance than SVM on imbalanced classification problems. The key idea of AdaBoostSVM is that for the sequence of trained RBFSVM component classifiers, starting with large a values (implying weak learning), the sigma values are reduced progressively as the Boosting iteration proceeds. This effectively produces a set of RBFSVM component classifiers whose model parameters are aclaptively different manifesting in better generalization as compared to AdaBoost approach with SVM component classifiers using a fixed (optimal) sigma value. From benchmark data sets, we show that our AdaBoostSVM approach outperforms other AdaBoost approaches using component classifiers such as Decision Trees and Neural Networks. AdaBoostSVM can be seen as a proof of concept of the idea proposed in Valentini and Dietterich [2004. Bias-variance analysis of support vector machines for the development of SVM-based ensemble methods. Journal of Machine Learning Research 5, 725-775] that Adaboost with heterogeneous SVMs could work well. Moreover, we extend AdaBoostSVM to the Diverse AdaBoostSVM to address the reported accuracy/diversity dilemma of the original Adaboost. By designing parameter adjusting strategies, the distributions of accuracy and diversity over RBFSVM component classifiers are tuned to maintain a good balance between them and promising results have been obtained on benchmark data sets. (C) 2007 Published by Elsevier Ltd.", "paper_title": "AdaBoost with SVM-based component classifiers", "paper_id": "WOS:000257620400011"}