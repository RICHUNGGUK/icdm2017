{"auto_keywords": [{"score": 0.04716744507704791, "phrase": "q-learning_one"}, {"score": 0.04277739174650543, "phrase": "optimal_path"}, {"score": 0.025798947833546873, "phrase": "distance-and-frequency_technique"}, {"score": 0.00481495049065317, "phrase": "agent_state_occurrence_frequency"}, {"score": 0.004740126725341152, "phrase": "knowledge_sharing"}, {"score": 0.00468476935399643, "phrase": "agent's_learning_process"}, {"score": 0.004575977644865379, "phrase": "learning_techniques"}, {"score": 0.004434824354230475, "phrase": "multiple-lookahead-levels_one"}, {"score": 0.00423118169200579, "phrase": "initial_exploratory_path"}, {"score": 0.003943047070755799, "phrase": "reinforcement_learning_technique"}, {"score": 0.0038817197332605647, "phrase": "distance_measure"}, {"score": 0.0037915057447460133, "phrase": "primary_gauge"}, {"score": 0.0037471850931318942, "phrase": "autonomous_agent's_action_selection"}, {"score": 0.00351938145050947, "phrase": "initial_information"}, {"score": 0.003424107910734022, "phrase": "agent's_goal"}, {"score": 0.003357633232360232, "phrase": "agent's_first_perceived_internal_model"}, {"score": 0.0029384777371425862, "phrase": "exploratory_or_hypothetical_paths"}, {"score": 0.002622465142275264, "phrase": "agent's_state_occurrence_frequency"}, {"score": 0.0025215486544004134, "phrase": "proposed_distance-only_technique"}, {"score": 0.0024920359789965783, "phrase": "computation_speed_performance_analysis"}, {"score": 0.0021049977753042253, "phrase": "agents'_learning_process"}], "paper_keywords": ["Reinforcement learning", " Multiagent systems", " Machine learning", " Robotics"], "paper_abstract": "Reinforcement learning techniques like the Q-Learning one as well as the Multiple-Lookahead-Levels one that we introduced in our prior work require the agent to complete an initial exploratory path followed by as many hypothetical and physical paths as necessary to find the optimal path to the goal. This paper introduces a reinforcement learning technique that uses a distance measure to the goal as a primary gauge for an autonomous agent's action selection. In this paper, we take advantage of the first random walk to acquire initial information about the goal. Once the agent's goal is reached, the agent's first perceived internal model of the environment is updated to reflect and include said goal. This is done by the agent tracing back its steps to its origin starting point. We show in this paper, no exploratory or hypothetical paths are required after the goal is initially reached or detected, and the agent requires a maximum of two physical paths to find the optimal path to the goal. The agent's state occurrence frequency is introduced as well and used to support the proposed Distance-Only technique. A computation speed performance analysis is carried out, and the Distance-and-Frequency technique is shown to require less computation time than the Q-Learning one. Furthermore, we present and demonstrate how multiple agents using the Distance-and-Frequency technique can share knowledge of the environment and study the effect of that knowledge sharing on the agents' learning process.", "paper_title": "Reinforcement learning technique using agent state occurrence frequency with analysis of knowledge sharing on the agent's learning process in multiagent environments", "paper_id": "WOS:000297359600024"}