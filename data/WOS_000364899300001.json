{"auto_keywords": [{"score": 0.049626383724219854, "phrase": "numa_systems"}, {"score": 0.045314716664604494, "phrase": "manycore_processors"}, {"score": 0.03227065987094905, "phrase": "data_distribution"}, {"score": 0.031243977204465007, "phrase": "numa"}, {"score": 0.00481495049065317, "phrase": "openmp_programs"}, {"score": 0.004657542074154025, "phrase": "performance_degradation"}, {"score": 0.00459165923324494, "phrase": "nonuniform_data_access_latencies"}, {"score": 0.0042354707960660706, "phrase": "numa_nodes"}, {"score": 0.004195418010131832, "phrase": "manycore_processor_caches"}, {"score": 0.004038943596016986, "phrase": "nonuniform_latencies"}, {"score": 0.0037432199549135826, "phrase": "low-level_architectural_knowledge"}, {"score": 0.00370780481244907, "phrase": "existing_task_scheduling_policies"}, {"score": 0.003419943984098415, "phrase": "locality-aware_scheduling"}, {"score": 0.003261133560558666, "phrase": "existing_scheduling"}, {"score": 0.0031693980719538287, "phrase": "numa_effects"}, {"score": 0.003022186855664124, "phrase": "locality-aware_scheduling_technique"}, {"score": 0.0029935730127943496, "phrase": "task-based_openmp_programs"}, {"score": 0.002696101728474703, "phrase": "runtime_system"}, {"score": 0.002657890175649468, "phrase": "task_data_dependence_information"}, {"score": 0.00258307991054107, "phrase": "openmp_tasks"}, {"score": 0.002546466113418944, "phrase": "data_stall_times"}, {"score": 0.0024630345285372958, "phrase": "four-socket_amd_opteron_machine"}, {"score": 0.002186712881521657, "phrase": "scientific_benchmarks"}, {"score": 0.0021557052355026048, "phrase": "default_policies"}, {"score": 0.0021049977753042253, "phrase": "architecture-oblivious_approach"}], "paper_keywords": [""], "paper_abstract": "Performance degradation due to nonuniform data access latencies has worsened on NUMA systems and can now be felt onchip in manycore processors. Distributing data across NUMA nodes and manycore processor caches is necessary to reduce the impact of nonuniform latencies. However, techniques for distributing data are error-prone and fragile and require low-level architectural knowledge. Existing task scheduling policies favor quick load-balancing at the expense of locality and ignore NUMA node/manycore cache access latencies while scheduling. Locality-aware scheduling, in conjunction with or as a replacement for existing scheduling, is necessary to minimize NUMA effects and sustain performance. We present a data distribution and locality-aware scheduling technique for task-based OpenMP programs executing on NUMA systems and manycore processors. Our technique relieves the programmer from thinking of NUMA system/manycore processor architecture details by delegating data distribution to the runtime system and uses task data dependence information to guide the scheduling of OpenMP tasks to reduce data stall times. We demonstrate our technique on a four-socket AMD Opteron machine with eight NUMA nodes and on the TILEPro64 processor and identify that data distribution and locality-aware task scheduling improve performance up to 69% for scientific benchmarks compared to default policies and yet provide an architecture-oblivious approach for programmers.", "paper_title": "Locality-Aware Task Scheduling and Data Distribution for OpenMP Programs on NUMA Systems and Manycore Processors", "paper_id": "WOS:000364899300001"}