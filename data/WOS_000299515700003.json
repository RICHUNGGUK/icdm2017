{"auto_keywords": [{"score": 0.04252420525659764, "phrase": "jido"}, {"score": 0.00481495049065317, "phrase": "pivotal_research_area"}, {"score": 0.004708056866527125, "phrase": "huge_societal_potential"}, {"score": 0.004645057834614705, "phrase": "assistant_robots"}, {"score": 0.00450130420152996, "phrase": "natural_and_easy-to-use_user_interfaces"}, {"score": 0.004303591748086452, "phrase": "flexible_multimodal_interface"}, {"score": 0.004005083535629333, "phrase": "vision_system"}, {"score": 0.003951453722517865, "phrase": "stereo_head"}, {"score": 0.0038810583785936505, "phrase": "pan-tilt_unit"}, {"score": 0.0037948184359488284, "phrase": "collaborative_particle_filters"}, {"score": 0.0037272031155266556, "phrase": "upper_human_body"}, {"score": 0.0035794251590584563, "phrase": "bi-manual_gestures"}, {"score": 0.003361069456060196, "phrase": "proper_handling"}, {"score": 0.00333097896868937, "phrase": "natural_artifacts"}, {"score": 0.003198860258228543, "phrase": "view_field"}, {"score": 0.0031702173541386888, "phrase": "hand_deformation"}, {"score": 0.0029767499830258754, "phrase": "speech_recognition"}, {"score": 0.00295008998986066, "phrase": "understanding_system"}, {"score": 0.002897482096228207, "phrase": "julius_engine"}, {"score": 0.00277001889871322, "phrase": "deictic_and_anaphoric_utterances"}, {"score": 0.0027328817607818207, "phrase": "second_contribution"}, {"score": 0.0026841368358805407, "phrase": "probabilistic_and_multi-hypothesis_interpreter_framework"}, {"score": 0.00249767641333441, "phrase": "classification_rates"}, {"score": 0.00247529638625566, "phrase": "multimodal_commands"}, {"score": 0.0023451553255837317, "phrase": "successful_live_experiments"}, {"score": 0.0023241387549221408, "phrase": "human-centered_settings"}, {"score": 0.0022218413146898887, "phrase": "interactive_manipulation_task"}, {"score": 0.0021049977753042253, "phrase": "safe_object_exchanges"}], "paper_keywords": ["Human-robot interaction", " Multiple object tracking", " Two-handed gesture recognition", " Vision and speech probabilistic fusion"], "paper_abstract": "Assistance is currently a pivotal research area in robotics, with huge societal potential. Since assistant robots directly interact with people, finding natural and easy-to-use user interfaces is of fundamental importance. This paper describes a flexible multimodal interface based on speech and gesture modalities in order to control our mobile robot named Jido. The vision system uses a stereo head mounted on a pan-tilt unit and a bank of collaborative particle filters devoted to the upper human body extremities to track and recognize pointing/symbolic mono but also bi-manual gestures. Such framework constitutes our first contribution, as it is shown, to give proper handling of natural artifacts (self-occlusion, camera out of view field, hand deformation) when performing 3D gestures using one or the other hand even both. A speech recognition and understanding system based on the Julius engine is also developed and embedded in order to process deictic and anaphoric utterances. The second contribution deals with a probabilistic and multi-hypothesis interpreter framework to fuse results from speech and gesture components. Such interpreter is shown to improve the classification rates of multimodal commands compared to using either modality alone. Finally, we report on successful live experiments in human-centered settings. Results are reported in the context of an interactive manipulation task, where users specify local motion commands to Jido and perform safe object exchanges.", "paper_title": "Two-handed gesture recognition and fusion with speech to command a robot", "paper_id": "WOS:000299515700003"}