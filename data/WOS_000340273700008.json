{"auto_keywords": [{"score": 0.04571345706034808, "phrase": "boolean_function"}, {"score": 0.004776853267973129, "phrase": "maximize_mutual_information"}, {"score": 0.004554488671294504, "phrase": "simply_stated_conjecture"}, {"score": 0.004447201233380302, "phrase": "maximum_mutual_information"}, {"score": 0.004240116703465252, "phrase": "noisy_inputs"}, {"score": 0.0037040785566765954, "phrase": "y-n"}, {"score": 0.0033937776228549557, "phrase": "memoryless_binary_symmetric_channel"}, {"score": 0.0033402088717816416, "phrase": "crossover_probability_alpha"}, {"score": 0.0022614266476954467, "phrase": "substantial_evidence"}, {"score": 0.0021049977753042253, "phrase": "discrete_isoperimetric_inequalities"}], "paper_keywords": ["Boolean functions", " mutual information", " extremal inequality", " isoperimetric inequality"], "paper_abstract": "We pose a simply stated conjecture regarding the maximum mutual information a Boolean function can reveal about noisy inputs. Specifically, let X-n be independent identically distributed Bernoulli(1/2), and let Y-n be the result of passing X-n through a memoryless binary symmetric channel with crossover probability alpha. For any Boolean function b : {0, 1}(n) -> {0, 1}, we conjecture that I (b(X-n); Y-n) <= 1 - H(alpha). While the conjecture remains open, we provide substantial evidence supporting its validity. Connections are also made to discrete isoperimetric inequalities.", "paper_title": "Which Boolean Functions Maximize Mutual Information on Noisy Inputs?", "paper_id": "WOS:000340273700008"}