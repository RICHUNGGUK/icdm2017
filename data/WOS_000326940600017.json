{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "nonlinear_projection_trick"}, {"score": 0.037482702260810255, "phrase": "kernel_trick"}, {"score": 0.01529163007875935, "phrase": "kernel_methods"}, {"score": 0.00873583072708748, "phrase": "dot_product"}, {"score": 0.0046046429665581555, "phrase": "kernel_principal_component_analysis"}, {"score": 0.004553760258766995, "phrase": "pca"}, {"score": 0.004477870190590885, "phrase": "support_vector_machines"}, {"score": 0.00428222231579898, "phrase": "direct_calculations"}, {"score": 0.0039380388343479384, "phrase": "effective_dimensionality"}, {"score": 0.0038725824361608243, "phrase": "kernel_space"}, {"score": 0.003724034990158436, "phrase": "training_samples"}, {"score": 0.0034630606658026595, "phrase": "input_data"}, {"score": 0.003405472159599442, "phrase": "reduced_dimensional_kernel_space"}, {"score": 0.003256526411907174, "phrase": "eigenvalue_decomposition"}, {"score": 0.003202361402737197, "phrase": "kernel_matrix"}, {"score": 0.003149094460617212, "phrase": "proposed_method"}, {"score": 0.0027535633837592597, "phrase": "arbitrary_algorithms"}, {"score": 0.002189105604712615, "phrase": "kernel_version"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Dimensionality reduction", " kernel methods", " kernel PCA (KPCA)", " KPCA-L1", " nonlinear projection trick", " support vector machines"], "paper_abstract": "In kernel methods such as kernel principal component analysis (PCA) and support vector machines, the so called kernel trick is used to avoid direct calculations in a high (virtually infinite) dimensional kernel space. In this brief, based on the fact that the effective dimensionality of a kernel space is less than the number of training samples, we propose an alternative to the kernel trick that explicitly maps the input data into a reduced dimensional kernel space. This is easily obtained by the eigenvalue decomposition of the kernel matrix. The proposed method is named as the nonlinear projection trick in contrast to the kernel trick. With this technique, the applicability of the kernel methods is widened to arbitrary algorithms that do not use the dot product. The equivalence between the kernel trick and the nonlinear projection trick is shown for several conventional kernel methods. In addition, we extend PCA-L1, which uses L-1-norm instead of L-2-norm (or dot product), into a kernel version and show the effectiveness of the proposed approach.", "paper_title": "Nonlinear Projection Trick in Kernel Methods: An Alternative to the Kernel Trick", "paper_id": "WOS:000326940600017"}