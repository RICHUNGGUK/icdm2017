{"auto_keywords": [{"score": 0.04888510471747535, "phrase": "weak_learners"}, {"score": 0.04012557526596847, "phrase": "optimal_number"}, {"score": 0.03043761851574792, "phrase": "evolutionary_algorithms"}, {"score": 0.025430618937224293, "phrase": "pattern_distributor"}, {"score": 0.00481495049065317, "phrase": "recursive_supervised_learning"}, {"score": 0.0044314880575692175, "phrase": "single_strong_learner"}, {"score": 0.004152725837204292, "phrase": "supervised_learning"}, {"score": 0.003991040678278726, "phrase": "shelf_classifier"}, {"score": 0.0036729522133392175, "phrase": "earlier_work"}, {"score": 0.0036072171658966348, "phrase": "rphp_algorithm"}, {"score": 0.0034918342672996066, "phrase": "global_search"}, {"score": 0.0034666972896389777, "phrase": "weak_learning"}, {"score": 0.0034417406427881075, "phrase": "pattern_distribution"}, {"score": 0.00333163309034114, "phrase": "global_search_component"}, {"score": 0.0032601856292942106, "phrase": "cluster_based_combinatorial_optimization"}, {"score": 0.0031672925196105663, "phrase": "output_space"}, {"score": 0.00296781730323553, "phrase": "combinatorial_optimization_problem"}, {"score": 0.002760836859397696, "phrase": "\"difficult\"_clusters"}, {"score": 0.002672454243212711, "phrase": "easy_patterns"}, {"score": 0.0026151058979100596, "phrase": "focused_learning"}, {"score": 0.002423882939663573, "phrase": "validation_patterns"}, {"score": 0.0021746545596056125, "phrase": "empirical_studies"}, {"score": 0.002158979137177815, "phrase": "generally_good_performance"}, {"score": 0.0021049977753042253, "phrase": "art_methods"}], "paper_keywords": ["topology based selection", " recursive learning", " task decomposition", " neural networks", " evolutionary algorithms"], "paper_abstract": "The use of combinations of weak learners to learn a dataset has been shown to be better than the use of a single strong learner. In fact, the idea is so successful that boosting, an algorithm combining several weak learners for supervised learning, has been considered to be the best off the shelf classifier. However, some problems still exist, including determining the optimal number of weak learners and the over fitting of data. In an earlier work, we developed the RPHP algorithm which solves both these problems by using a combination of global search, weak learning and pattern distribution. In this chapter, we revise the global search component by replacing it with a cluster based combinatorial optimization. Patterns are clustered according to the output space of the problem, i.e., natural clusters are formed based on patterns belonging to each class. A combinatorial optimization problem is therefore created, which is solved using evolutionary algorithms. The evolutionary algorithms identify the \"easy\" and the \"difficult\" clusters in the system. The removal of the easy patterns then gives way to the focused learning of the more complicated patterns. The problem therefore becomes recursively simpler. Over fitting is overcome by using a set of validation patterns along with a pattern distributor. An algorithm is also proposed to use the pattern distributor to determine the optimal number of recursions and hence the optimal number of weak learners for the problem. Empirical studies show generally good performance when compared to other state of the art methods.", "paper_title": "Clustering and combinatorial optimization in recursive supervised learning", "paper_id": "WOS:000243140800003"}