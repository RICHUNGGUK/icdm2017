{"auto_keywords": [{"score": 0.03354792148096693, "phrase": "hsmm"}, {"score": 0.014310497768800555, "phrase": "temporal_phases"}, {"score": 0.01143576057351547, "phrase": "aus"}, {"score": 0.008567459006312119, "phrase": "vdhmm"}, {"score": 0.00481495049065317, "phrase": "facial_actions"}, {"score": 0.004748222479566159, "phrase": "duration_models"}, {"score": 0.00465634671242413, "phrase": "finegrained_changes"}, {"score": 0.004630423009638876, "phrase": "facial_expression"}, {"score": 0.0046046429665581555, "phrase": "action_units"}, {"score": 0.004502947025978661, "phrase": "facs"}, {"score": 0.004222845792972848, "phrase": "face_videos"}, {"score": 0.0041642891310893, "phrase": "facial_expression_recognition_systems"}, {"score": 0.0039822874870125095, "phrase": "support_vector_machine"}, {"score": 0.00389427992231721, "phrase": "hidden_markov_models"}, {"score": 0.003703282519292336, "phrase": "input_image_sequence"}, {"score": 0.0036723696468167126, "phrase": "major_drawback"}, {"score": 0.003434146115330295, "phrase": "spontaneous_expressions"}, {"score": 0.003311604528903634, "phrase": "efficient_duration_modeling"}, {"score": 0.0032839503822188563, "phrase": "temporal_behavior"}, {"score": 0.003220315882881027, "phrase": "hidden_semi"}, {"score": 0.003028760537974909, "phrase": "au"}, {"score": 0.0029364923713649026, "phrase": "au's_state_duration_distributions"}, {"score": 0.002887634989058733, "phrase": "geometrical_and_appearance_based_measurements"}, {"score": 0.0027458730206408005, "phrase": "pair-wise_svm_classifiers"}, {"score": 0.002722930209707764, "phrase": "frame-based_classification"}, {"score": 0.0025604233506352375, "phrase": "thorough_investigation"}, {"score": 0.002524860443787105, "phrase": "duration_modeling"}, {"score": 0.0024967652017219115, "phrase": "au_recognition"}, {"score": 0.0024828347422508196, "phrase": "extensive_comparison"}, {"score": 0.0024689818140496878, "phrase": "state-of-art_svm-hmm_approaches"}, {"score": 0.002414336861844913, "phrase": "average_recognition_rate"}, {"score": 0.0022575359484509187, "phrase": "au's_temporal_phases"}, {"score": 0.0021708046703940026, "phrase": "underlying_structure"}, {"score": 0.0021526564019119466, "phrase": "au_events"}], "paper_keywords": ["Facial action units (AUs)", " Hidden semi-Markov models (HSMMs)", " Variable duration semi-Markov model (VDHMM)"], "paper_abstract": "Being able to automatically analyze finegrained changes in facial expression into action units (AUs), of the Facial Action Coding System (FACS), and their temporal models (i.e., sequences of temporal phases, neutral, onset, apex, and offset), in face videos would greatly benefit for facial expression recognition systems. Previous works, considered combining, per AU, a discriminative frame-based Support Vector Machine (SVM) and a dynamic generative Hidden Markov Models (HMM), to detect the presence of the AU in question and its temporal segments in an input image sequence. The major drawback of HMMs, is that they do not model well time dependent dynamics as the ones of AUs, especially when dealing with spontaneous expressions. To alleviate this problem, in this paper, we exploit efficient duration modeling of the temporal behavior of AUs, and we propose hidden semi-Markov model (HSMM) and variable duration semi-Markov model (VDHMM) to recognize the dynamics of AU's. Such models allow the parameterization and inference of the AU's state duration distributions. Within our system, geometrical and appearance based measurements, as well as their first derivatives, modeling both the dynamics and the appearance of AUs, are applied to pair-wise SVM classifiers for a frame-based classification. The output of which are then fed as evidence to the HSMM or VDHMM for inferring AUs temporal phases. A thorough investigation into the aspect of duration modeling and its application to AU recognition through extensive comparison to state-of-art SVM-HMM approaches are presented. For comparison, an average recognition rate of 64.83 % and 64.66 % is achieved for the HSMM and VDHMM respectively. Our framework has several benefits: (1) it models the AU's temporal phases duration; (2) it does not require any assumption about the underlying structure of the AU events, and (3) compared to HMM, the proposed HSMM and VDHMM duration models reduce the duration error of the temporal phases of an AU, and they are especially better in recognizing the offset ending of an AU.", "paper_title": "Recognition of facial actions and their temporal segments based on duration models", "paper_id": "WOS:000364019400010"}