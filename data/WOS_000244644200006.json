{"auto_keywords": [{"score": 0.04934553301452922, "phrase": "evolino"}, {"score": 0.00481495049065317, "phrase": "recurrent_networks"}, {"score": 0.00456159659731398, "phrase": "gradient-based_lstm_recurrent_neural_networks"}, {"score": 0.004190019394361629, "phrase": "gradient_information"}, {"score": 0.004031238480458909, "phrase": "training_rnns"}, {"score": 0.0039085400187964196, "phrase": "numerous_local_minima"}, {"score": 0.003674192201248236, "phrase": "novel_method"}, {"score": 0.00317221667574882, "phrase": "optimal_linear_mappings"}, {"score": 0.003123527559542239, "phrase": "hidden_state"}, {"score": 0.0029361086955646625, "phrase": "pseudo-inverse-based_linear_regression"}, {"score": 0.002802941205762953, "phrase": "quadratic_programming"}, {"score": 0.0026143990748127253, "phrase": "first_evolutionary_recurrent_support_vector_machines"}, {"score": 0.0025151755329660837, "phrase": "evolino-based_lstm"}, {"score": 0.002419708657850547, "phrase": "echo_state"}, {"score": 0.002364182482204759, "phrase": "jaeger"}, {"score": 0.0022222152430170254, "phrase": "higher_accuracy"}, {"score": 0.0021544587602892466, "phrase": "conventional_gradient_descent_rnns"}, {"score": 0.0021049977753042253, "phrase": "gradient-based_lstm."}], "paper_keywords": [""], "paper_abstract": "In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases, we present a novel method: EVOlution of systems with LINear Outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent support vector machines. We show that Evolino-based LSTM can solve tasks that Echo State nets (Jaeger, 2004a) cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM.", "paper_title": "Training recurrent networks by Evolino", "paper_id": "WOS:000244644200006"}