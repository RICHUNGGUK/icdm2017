{"auto_keywords": [{"score": 0.037043006768315144, "phrase": "linear"}, {"score": 0.00481495049065317, "phrase": "faster_dimension_reduction"}, {"score": 0.0047057084749407485, "phrase": "high-dimensional_vector_spaces"}, {"score": 0.004260134699483774, "phrase": "text_documents"}, {"score": 0.004163427731140163, "phrase": "vector_space"}, {"score": 0.003901135641717704, "phrase": "huge_corpora"}, {"score": 0.0036580194082286286, "phrase": "pca"}, {"score": 0.003585986761045987, "phrase": "lower-dimensional_space"}, {"score": 0.00346449481422526, "phrase": "attractive_approach"}, {"score": 0.0033857878144822906, "phrase": "mapped_input"}, {"score": 0.003308862957060602, "phrase": "popular_algorithms"}, {"score": 0.0032585499655442404, "phrase": "linear_spaces"}, {"score": 0.003208999546225013, "phrase": "principal-component_analysis"}, {"score": 0.0031243359031399806, "phrase": "johnson"}, {"score": 0.0029722900280316216, "phrase": "computer_science"}, {"score": 0.002949598537993663, "phrase": "seminal_work"}, {"score": 0.0029305107926238965, "phrase": "lindenstrauss"}, {"score": 0.0028496055358577512, "phrase": "underlying_technique"}, {"score": 0.002599177223533834, "phrase": "dense_matrix"}, {"score": 0.002370704665003412, "phrase": "computational_version"}, {"score": 0.0023525950486853937, "phrase": "heisenberg's_uncertainty_principle"}, {"score": 0.0022990901047984197, "phrase": "significant_speedup"}, {"score": 0.002246799280512825, "phrase": "practical_simplicity"}, {"score": 0.0022211005680689666, "phrase": "standard_johnson-lindenstrauss_projection"}], "paper_keywords": [""], "paper_abstract": "Data represented geometrically in high-dimensional vector spaces can be found in many applications. Images and videos, are often represented by assigning a dimension for every pixel (and time). Text documents may be represented in a vector space where each word in the dictionary incurs a dimension. The need to manipulate such data in huge corpora such as the web and to support various query types gives rise to the question of how to represent the data in a lower-dimensional space to allow more space and time efficient computation. Linear mappings are an attractive approach to this problem because the mapped input can be readily fed into popular algorithms that operate on linear spaces (such as principal-component analysis, PCA) while avoiding the curse of dimensionality. The fact that such mappings even exist became known in computer science following seminal work by Johnson and Lindenstrauss in the early 1980s. The underlying technique is often called \"random projection.\" The complexity of the mapping itself, essentially the product of a vector with a dense matrix, did not attract much attention until recently. In 2006, we discovered a way to \"sparsify\" the matrix via a computational version of Heisenberg's Uncertainty Principle. This led to a significant speedup, which also retained the practical simplicity of the standard Johnson-Lindenstrauss projection. We describe the improvement in this article, together with some of its applications.", "paper_title": "Faster Dimension Reduction", "paper_id": "WOS:000274029100019"}