{"auto_keywords": [{"score": 0.04367269561371385, "phrase": "kinect"}, {"score": 0.00481495049065317, "phrase": "blurred_shape_model"}, {"score": 0.004666756595735701, "phrase": "quantitative_analysis"}, {"score": 0.004563691742821759, "phrase": "smart_environments"}, {"score": 0.004462892860919036, "phrase": "depth_maps"}, {"score": 0.004403480614319231, "phrase": "increasing_interest"}, {"score": 0.004306205029641728, "phrase": "cheap_multisensor_devices"}, {"score": 0.004248869786808024, "phrase": "structured_light"}, {"score": 0.004009074013302036, "phrase": "strong_need"}, {"score": 0.003902992784453769, "phrase": "rich_object_representations"}, {"score": 0.0036173729405604674, "phrase": "computationally_efficient_descriptors"}, {"score": 0.0035532533960012298, "phrase": "open_issue"}, {"score": 0.00341309513558946, "phrase": "novel_point_cloud_descriptor"}, {"score": 0.003220315882881027, "phrase": "structure_density"}, {"score": 0.0031916366459252992, "phrase": "local_variabilities"}, {"score": 0.0031071175640144943, "phrase": "shape_voxel_distances"}, {"score": 0.0030656984915295275, "phrase": "neighborhood_propagation_strategy"}, {"score": 0.0030248298759040695, "phrase": "proposed_sbsm"}, {"score": 0.002778317177178732, "phrase": "multiple_categories"}, {"score": 0.0027535633837592597, "phrase": "complex_objects"}, {"score": 0.002716844480081738, "phrase": "human_hand"}, {"score": 0.0026330549554033876, "phrase": "sbsm_complexity"}, {"score": 0.0025518429503461736, "phrase": "object_voxels"}, {"score": 0.0025291018422967083, "phrase": "experimental_evaluation"}, {"score": 0.002506562885555113, "phrase": "public_depth_multiclass_object_data"}, {"score": 0.0024401411219526774, "phrase": "novel_hand"}, {"score": 0.002418393046947528, "phrase": "data_sets"}, {"score": 0.0023968383387667404, "phrase": "significant_performance_improvements"}, {"score": 0.0021049977753042253, "phrase": "real-time_automatic_hand_pose_recognition"}], "paper_keywords": ["Depth image analysis", " human computer interaction (HCI)", " image descriptors", " object and pose recognition", " smart environments"], "paper_abstract": "The use of depth maps is of increasing interest after the advent of cheap multisensor devices based on structured light, such as Kinect. In this context, there is a strong need of powerful 3-D shape descriptors able to generate rich object representations. Although several 3-D descriptors have been already proposed in the literature, the research of discriminative and computationally efficient descriptors is still an open issue. In this paper, we propose a novel point cloud descriptor called spherical blurred shape model (SBSM) that successfully encodes the structure density and local variabilities of an object based on shape voxel distances and a neighborhood propagation strategy. The proposed SBSM is proven to be rotation and scale invariant, robust to noise and occlusions, highly discriminative for multiple categories of complex objects like the human hand, and computationally efficient since the SBSM complexity is linear to the number of object voxels. Experimental evaluation in public depth multiclass object data, 3-D facial expressions data, and a novel hand poses data sets show significant performance improvements in relation to state-of-the-art approaches. Moreover, the effectiveness of the proposal is also proved for object spotting in 3-D scenes and for real-time automatic hand pose recognition in human computer interaction scenarios.", "paper_title": "Spherical Blurred Shape Model for 3-D Object and Pose Recognition: Quantitative Analysis and HCI Applications in Smart Environments", "paper_id": "WOS:000345629000012"}