{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "consistent_collective_activity_recognition"}, {"score": 0.004691684576246597, "phrase": "novel_method"}, {"score": 0.004571559854487074, "phrase": "collective_activities"}, {"score": 0.004487635647709636, "phrase": "multiple_persons"}, {"score": 0.003897989308486044, "phrase": "recent_studies"}, {"score": 0.0038548668648948044, "phrase": "contextual_information"}, {"score": 0.0036598289615980837, "phrase": "spatial_and_temporal_consistency"}, {"score": 0.0033480826365657303, "phrase": "temporary_misclassification"}, {"score": 0.0032987614613108345, "phrase": "multiple_collective_activities"}, {"score": 0.0030856268625708695, "phrase": "individual_recognition_results"}, {"score": 0.0030628094110419697, "phrase": "fully_connected_conditional_random_fields"}, {"score": 0.0029185415814209055, "phrase": "video_clip"}, {"score": 0.002875529792120695, "phrase": "interaction_strength"}, {"score": 0.002770745509774383, "phrase": "previous_methods"}, {"score": 0.002630414143385786, "phrase": "constant_area"}, {"score": 0.0025724636254035337, "phrase": "\"multi-scale''_interactions"}, {"score": 0.0022757667100089243, "phrase": "experimental_results"}, {"score": 0.002152440300818582, "phrase": "art_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b._v."}], "paper_keywords": ["Collective activity recognition", " Fully connected model", " CRFs", " Spatial and temporal consistency"], "paper_abstract": "We propose a novel method for consistent collective activity recognition in video images. Collective activities are activities performed by multiple persons, such as queuing in a line, talking together, and waiting at an intersection. Since it is often difficult to differentiate between these activities using the appearance of only an individual person, the models proposed in recent studies exploit the contextual information of other people nearby. However, these models do not sufficiently consider the spatial and temporal consistency in a group (e.g., they consider the consistency in only the adjacent area), and therefore, they cannot effectively deal with temporary misclassification or simultaneously consider multiple collective activities in a scene. To overcome this drawback, this paper describes a method to integrate the individual recognition results via fully connected conditional random fields (CRFs), which consider all the interactions among the people in a video clip and alter the interaction strength in accordance with the degree of their similarity. Unlike previous methods that restrict the interactions among the people heuristically (e.g., within a constant area), our method describes the \"multi-scale'' interactions in various features, i.e., position, size, motion, and time sequence, in order to allow various types, sizes, and shapes of groups to be treated. Experimental results on two challenging video datasets indicate that our model outperforms not only other graph topologies but also state-of-the art models. (C) 2014 Elsevier B. V. All rights reserved.", "paper_title": "A fully connected model for consistent collective activity recognition in videos", "paper_id": "WOS:000334315000012"}