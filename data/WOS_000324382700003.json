{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "optimization_framework"}, {"score": 0.004768649568826951, "phrase": "joint_view"}, {"score": 0.004722791767189843, "phrase": "rate_scalable_coding"}, {"score": 0.004677372885331908, "phrase": "multi-view_video_content"}, {"score": 0.004587835274176236, "phrase": "texture_plus_depth_format"}, {"score": 0.004329331617724, "phrase": "coded_views"}, {"score": 0.004205570965859848, "phrase": "aggregate_distortion"}, {"score": 0.0041051328120736575, "phrase": "synthesized_views"}, {"score": 0.003799509133780117, "phrase": "optimal_performance"}, {"score": 0.0037267132967205136, "phrase": "discrete_set"}, {"score": 0.0036908381347418805, "phrase": "transmission_rates"}, {"score": 0.0035507457285983268, "phrase": "user_interaction_model"}, {"score": 0.0034826993555190765, "phrase": "view_selection_actions"}, {"score": 0.0033830587647810132, "phrase": "markov_chain"}, {"score": 0.003334309053314909, "phrase": "discrete_state-space"}, {"score": 0.003131023880006021, "phrase": "user-action-driven_coding_strategies"}, {"score": 0.0030414140732789186, "phrase": "client's_performance"}, {"score": 0.002954361295811154, "phrase": "video_quality"}, {"score": 0.0027741761880779535, "phrase": "multi-view_wavelet-based_coder"}, {"score": 0.0027209713296448296, "phrase": "uniform_rate_allocation_strategy"}, {"score": 0.0025181367444581993, "phrase": "arbitrarily_fine_granularity"}, {"score": 0.00249386680748172, "phrase": "encoding_bit_rates"}, {"score": 0.0024342079318127423, "phrase": "novel_functionality"}, {"score": 0.0022202042044679984, "phrase": "interactivity-aware_coding"}, {"score": 0.0021670777282518424, "phrase": "conventional_allocation_techniques"}, {"score": 0.0021049977753042253, "phrase": "client's_view_selection_actions"}], "paper_keywords": ["Multiview imaging", " depth-image-based rendering", " bit allocation", " view and rate scalable encoding", " user-action-driven coding", " view selection Markov model"], "paper_abstract": "We derive an optimization framework for joint view and rate scalable coding of multi-view video content represented in the texture plus depth format. The optimization enables the sender to select the subset of coded views and their encoding rates such that the aggregate distortion over a continuum of synthesized views is minimized. We construct the view and rate embedded bitstream such that it delivers optimal performance simultaneously over a discrete set of transmission rates. In conjunction, we develop a user interaction model that characterizes the view selection actions of the client as a Markov chain over a discrete state-space. We exploit the model within the context of our optimization to compute user-action-driven coding strategies that aim at enhancing the client's performance in terms of latency and video quality. Our optimization outperforms the state-of-the-art H.264 SVC codec as well as a multi-view wavelet-based coder equipped with a uniform rate allocation strategy, across all scenarios studied in our experiments. Equally important, we can achieve an arbitrarily fine granularity of encoding bit rates, while providing a novel functionality of view embedded encoding, unlike the other encoding methods that we examined. Finally, we observe that the interactivity-aware coding delivers superior performance over conventional allocation techniques that do not anticipate the client's view selection actions in their operation.", "paper_title": "User-Action-Driven View and Rate Scalable Multiview Video Coding", "paper_id": "WOS:000324382700003"}