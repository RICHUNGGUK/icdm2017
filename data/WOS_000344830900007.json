{"auto_keywords": [{"score": 0.03083599242349429, "phrase": "additional_latency"}, {"score": 0.014856098748233956, "phrase": "near-threshold_voltages"}, {"score": 0.004394424341776717, "phrase": "near-threshold_region"}, {"score": 0.004246333442872459, "phrase": "future_processors"}, {"score": 0.0041821205529541045, "phrase": "reliable_operation"}, {"score": 0.004134596278123439, "phrase": "minimum_voltage"}, {"score": 0.004103212598803194, "phrase": "vccmin"}, {"score": 0.003980032599437366, "phrase": "process_variations"}, {"score": 0.003934795713147731, "phrase": "sram_margins"}, {"score": 0.0037161794591176033, "phrase": "multicore_processors"}, {"score": 0.0036321600255449613, "phrase": "data_locality"}, {"score": 0.0035908627899097407, "phrase": "high_performance"}, {"score": 0.0035096666710980108, "phrase": "high_bit-cell_fault_rates"}, {"score": 0.0032396224508754387, "phrase": "design_trade-off"}, {"score": 0.0031542917620807093, "phrase": "fault_rate"}, {"score": 0.003106541164572609, "phrase": "extreme_vccmin"}, {"score": 0.0028565082986974602, "phrase": "low_fault_rates"}, {"score": 0.002824005136794786, "phrase": "additional_constant_latency"}, {"score": 0.0027918707809612, "phrase": "cache_capacity"}, {"score": 0.002537880772794665, "phrase": "fault-free_cache_lines"}, {"score": 0.0024899186481178075, "phrase": "capacity_bottleneck"}, {"score": 0.0024428607154967806, "phrase": "correction_mechanism"}, {"score": 0.0023424328921363585, "phrase": "extensive_simulations"}, {"score": 0.0021211305348270413, "phrase": "significant_reduction"}, {"score": 0.0021049977753042253, "phrase": "energy_consumption"}], "paper_keywords": ["Design", " Performance", " NTV"], "paper_abstract": "Research has shown that operating in the near-threshold region is expected to provide up to 10x energy efficiency for future processors. However, reliable operation below a minimum voltage (Vccmin) cannot be guaranteed due to process variations. Because SRAM margins can easily be violated at near-threshold voltages, their bit-cell failure rates are expected to rise steeply. Multicore processors rely on fast private L1 caches to exploit data locality and achieve high performance. In the presence of high bit-cell fault rates, traditionally an L1 cache either sacrifices capacity or incurs additional latency to correct the faults. We observe that L1 cache sensitivity to hit latency offers a design trade-off between capacity and latency. When fault rate is high at extreme vccmin, it is beneficial to recover L1 cache capacity, even if it comes at the cost of additional latency. However, at low fault rates, the additional constant latency to recover cache capacity degrades performance. With this trade-off in mind, we propose a Non-Uniform Cache Access L1 architecture (NUCA-L1) that avoids additional latency on accesses to fault-free cache lines. To mitigate the capacity bottleneck, it deploys a correction mechanism to recover capacity at the cost of additional latency. Using extensive simulations of a 64-core multicore, we demonstrate that at various bit-cell fault rates, our proposed private NUCA-L1 cache architecture performs better than state-of-the-art schemes, along with a significant reduction in energy consumption.", "paper_title": "NUCA-L1: A Non-Uniform Access Latency Level-1 Cache Architecture for Multicores Operating at Near-Threshold Voltages", "paper_id": "WOS:000344830900007"}