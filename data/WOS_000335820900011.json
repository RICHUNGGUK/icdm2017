{"auto_keywords": [{"score": 0.037389954986041085, "phrase": "distribution_d"}, {"score": 0.00481495049065317, "phrase": "coordinatewise_maxima"}, {"score": 0.004763565662615503, "phrase": "convex_hull"}, {"score": 0.004712726613162057, "phrase": "planar_point"}, {"score": 0.004596192494271827, "phrase": "computational_geometry"}, {"score": 0.004187917910692495, "phrase": "planar_points"}, {"score": 0.004143196873246147, "phrase": "input_point"}, {"score": 0.003912595576016982, "phrase": "independent_sample_pi"}, {"score": 0.0032947371184395237, "phrase": "first_few_inputs"}, {"score": 0.003259522768246318, "phrase": "salient_features"}, {"score": 0.003066945490483524, "phrase": "opt-chd"}, {"score": 0.003023307182674672, "phrase": "expected_depth"}, {"score": 0.0029909851863915283, "phrase": "optimal_linear_comparison_tree"}, {"score": 0.0027054473441439422, "phrase": "self-improving_algorithm"}, {"score": 0.0025273383208209922, "phrase": "new_tools"}, {"score": 0.0025003054002226965, "phrase": "linear_comparison_trees"}, {"score": 0.0024296194239886676, "phrase": "general_linear_comparison_tree"}, {"score": 0.0024036292057373803, "phrase": "restricted_version"}, {"score": 0.0023356699564158426, "phrase": "running_time"}, {"score": 0.0022941723937662927, "phrase": "interesting_feature"}, {"score": 0.0022696277881969896, "phrase": "interleaved_search_procedure"}, {"score": 0.002237308726827077, "phrase": "likeliest_point"}, {"score": 0.0021975548867457623, "phrase": "minimal_computation"}, {"score": 0.0021201492891323587, "phrase": "optimal_algorithm"}, {"score": 0.0021054477208091275, "phrase": "d."}], "paper_keywords": ["self-improving algorithms", " planar maxima", " planar convex hulls", " entropy optimal"], "paper_abstract": "Finding the coordinatewise maxima and the convex hull of a planar point set are probably the most classic problems in computational geometry. We consider these problems in the self-improving setting. Here, we have n distributions D-1,..., D-n of planar points. An input point set (p(1),..., p(n)) is generated by taking an independent sample pi from each D-i, so the input is distributed according to the product D = Pi(i) D-i. A self-improving algorithm repeatedly gets inputs from the distribution D (which is a priori unknown), and it tries to optimize its running time for D. The algorithm uses the first few inputs to learn salient features of the distribution D before it becomes fine-tuned to D. Let OPT-MAX(D) (resp., OPT-CHD) be the expected depth of an optimal linear comparison tree computing the maxima (resp., convex hull) for D. Our maxima algorithm eventually achieves expected running time O(OPT-MAX(D) +n) furthermore, we give a self-improving algorithm for convex hulls with expected running time O(OPT-CHD +n log log n). Our results require new tools for understanding linear comparison trees. In particular, we convert a general linear comparison tree to a restricted version that can then be related to the running time of our algorithms. Another interesting feature is an interleaved search procedure to determine the likeliest point to be extremal with minimal computation. This allows our algorithms to be competitive with the optimal algorithm for D.", "paper_title": "SELF-IMPROVING ALGORITHMS FOR COORDINATEWISE MAXIMA AND CONVEX HULLS", "paper_id": "WOS:000335820900011"}