{"auto_keywords": [{"score": 0.03410735276717892, "phrase": "pattern_redundancy"}, {"score": 0.004816293322450458, "phrase": "markov"}, {"score": 0.004646496622257368, "phrase": "arbitrary_alphabets"}, {"score": 0.004580769304731227, "phrase": "recent_work"}, {"score": 0.003029382820031577, "phrase": "lower_and_upper_bounds"}, {"score": 0.0028207566216121856, "phrase": "hidden_markov_models"}, {"score": 0.0027025233739857374, "phrase": "small_number"}, {"score": 0.0024108455980729284, "phrase": "upper_bounds"}, {"score": 0.002309754152767196, "phrase": "growth_rate"}, {"score": 0.0022287497627555895, "phrase": "multidimensional_integer_partitions"}, {"score": 0.0021049977753042253, "phrase": "hayman's_theorem"}], "paper_keywords": ["hidden Markov models (HMMs)", " integer partitions", " large alphabets", " multidimensional partitions", " patterns", " redundancy", " universal compression"], "paper_abstract": "Recent work has considered encoding a string by separately conveying its symbols and its pattern-the order in which the symbols appear. It was shown that the patterns of independent and identically distributed (i.i.d.) strings can be losslessly compressed with diminishing per-symbol redundancy. In this correspondence, the pattern redundancy of distributions with memory is considered. Close lower and upper bounds are established on the pattern redundancy of strings generated by Hidden Markov models (HMMs) with a small number of states, showing in particular that their per-symbol pattern redundancy diminishes with increasing string length. The upper bounds are obtained by analyzing the growth rate of the number of multidimensional integer partitions, and the lower bounds, using Hayman's theorem.", "paper_title": "Universal compression of Markov and related sources over arbitrary alphabets", "paper_id": "WOS:000240076700020"}