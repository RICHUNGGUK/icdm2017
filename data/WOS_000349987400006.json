{"auto_keywords": [{"score": 0.04594846582703762, "phrase": "face_recognition"}, {"score": 0.04281249624380956, "phrase": "intra-class_variance_dictionaries"}, {"score": 0.04148530057857466, "phrase": "reconstruction_coefficients"}, {"score": 0.00481495049065317, "phrase": "global_nor_local"}, {"score": 0.004756233569671719, "phrase": "patch-based_representation"}, {"score": 0.004509919457793852, "phrase": "regularized_patch-based_representation"}, {"score": 0.004121672175779706, "phrase": "gallery_images_patches"}, {"score": 0.003797690272118125, "phrase": "group_sparsity_constraint"}, {"score": 0.0036453427552753533, "phrase": "gallery_images"}, {"score": 0.003556873173040169, "phrase": "sparsity_constraint"}, {"score": 0.003331275574328097, "phrase": "patch-based_image_representation"}, {"score": 0.0031975786158658158, "phrase": "side_effect"}, {"score": 0.0029100272315053253, "phrase": "gallery_patches"}, {"score": 0.0028744731986533076, "phrase": "right_person"}, {"score": 0.0027817658627604653, "phrase": "manually_designed_intra-class_variance_dictionaries"}, {"score": 0.0025945450101602825, "phrase": "probe_images"}, {"score": 0.002541910967437926, "phrase": "face_recognition_accuracy"}, {"score": 0.002510843167476253, "phrase": "single_sample"}, {"score": 0.002459902948098627, "phrase": "experimental_results"}, {"score": 0.0024001337476063094, "phrase": "yale_b"}, {"score": 0.0023805350945827024, "phrase": "cmu-pie"}, {"score": 0.002351438692744682, "phrase": "lfw"}, {"score": 0.002294294558114909, "phrase": "sparse_coding_related_face_recognition_methods"}, {"score": 0.002220257443705344, "phrase": "face_representation_methods"}, {"score": 0.0021751997984619585, "phrase": "best_performance"}, {"score": 0.002148604369150115, "phrase": "encouraging_results"}, {"score": 0.0021049977753042253, "phrase": "regularized_patch-based_face_representation"}], "paper_keywords": ["Single sample per person", " Regularized patch-based representation", " Group sparsity", " Intra-class variance dictionary"], "paper_abstract": "This paper presents a regularized patch-based representation for single sample per person face recognition. We represent each image by a collection of patches and seek their sparse representations under the gallery images patches and intra-class variance dictionaries at the same time. For the reconstruction coefficients of all the patches from the same image, by imposing a group sparsity constraint on the reconstruction coefficients corresponding to the patches from the gallery images, and by imposing a sparsity constraint on the reconstruction coefficients corresponding to the intra-class variance dictionaries, our formulation harvests the advantages of both patch-based image representation and global image representation, i.e. our method overcomes the side effect of those patches which are severely corrupted by the variances in face recognition, while enforcing those less discriminative patches to be constructed by the gallery patches from the right person. Moreover, instead of using the manually designed intra-class variance dictionaries, we propose to learn the intra-class variance dictionaries which not only greatly accelerate the prediction of the probe images but also improve the face recognition accuracy in the single sample per person scenario. Experimental results on the AR, Extended Yale B, CMU-PIE, and LFW datasets show that our method outperforms sparse coding related face recognition methods as well as some other specially designed single sample per person face representation methods, and achieves the best performance. These encouraging results demonstrate the effectiveness of regularized patch-based face representation for single sample per person face recognition.", "paper_title": "Neither Global Nor Local: Regularized Patch-Based Representation for Single Sample Per Person Face Recognition", "paper_id": "WOS:000349987400006"}