{"auto_keywords": [{"score": 0.04977329136950648, "phrase": "model_selection"}, {"score": 0.02960464465828457, "phrase": "estimated_model"}, {"score": 0.00481495049065317, "phrase": "strong_consistency"}, {"score": 0.0045598218623312675, "phrase": "pattern_recognition"}, {"score": 0.004530705581549137, "phrase": "probabilistic_inference"}, {"score": 0.0044873778175404475, "phrase": "bayesian_network"}, {"score": 0.0043181530164057135, "phrase": "markov_chain"}, {"score": 0.004276849188982683, "phrase": "conditional_probability"}, {"score": 0.004168618849008299, "phrase": "class_y"}, {"score": 0.004076157223660074, "phrase": "attribute_value_x"}, {"score": 0.0038973170902076707, "phrase": "equivalence_relation"}, {"score": 0.0037382581492847104, "phrase": "x_x"}, {"score": 0.0036553070575015344, "phrase": "right_arrow"}, {"score": 0.003194736764837261, "phrase": "n_samples"}, {"score": 0.0029866270401697906, "phrase": "information_criteria"}, {"score": 0.002958021635132416, "phrase": "form_empirical_entropy_h_plus_penalty_term"}, {"score": 0.0027125596072406525, "phrase": "independent_parameters"}, {"score": 0.0026268961477241026, "phrase": "real_nonnegative_sequence"}, {"score": 0.002535779689260244, "phrase": "autoregressive_processes"}, {"score": 0.0023553236684739887, "phrase": "true_model"}, {"score": 0.0021667128606162717, "phrase": "hannan"}, {"score": 0.0021528659668069642, "phrase": "quinn"}], "paper_keywords": ["error probability", " Hannan and Quinn's procedure", " law of the iterated logarithm", " Kullback-Leibler divergence", " model selection", " strong consistency"], "paper_abstract": "This paper considers model selection in classification. In many applications such as pattern recognition, probabilistic inference using a Bayesian network, prediction of the next in a sequence based on a Markov chain, the conditional probability P(Y = y vertical bar X = x) of class y is an element of Y given attribute value x is an element of X is utilized. By model we mean the equivalence relation in X: for x, x' is an element of X x similar to x' double left right arrow P(Y = y vertical bar X = x) = P(Y = y vertical bar X = x') for all y is an element of Y By classification we mean the number of such equivalence classes is finite. We estimate the model from n samples z(n) = (x(i), y(i))(i=1)(n) is an element of (X x Y)(n), using information criteria in the form empirical entropy H plus penalty term (k/2)d(n) (the model such that H + (k/2)d(n) is minimized is the estimated model), where k is the number of independent parameters in the model, and {d(n)}(n=1)(infinity) is a real nonnegative sequence such that lim sup(n) d(n)/n = 0. For autoregressive processes, although the definitions of H and k are different, it is known that the estimated model almost surely coincides with the true model as n -> infinity if {d(n)}(n=1)(infinity) > {2loglogn}(n=1)(infinity), and that it does not if {d(n)}(n=1)(infinity) < {2loglogn}(n=1)(infinity) (Hannan and Quinn). The problem whether the same property is true for classification was open. This paper solves the problem in the affirmative.", "paper_title": "On strong consistency of model selection in classification", "paper_id": "WOS:000241805700002"}