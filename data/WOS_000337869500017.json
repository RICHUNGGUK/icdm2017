{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "correlation_matrices"}, {"score": 0.04963913874714364, "phrase": "truncated_vines"}, {"score": 0.015435256974963694, "phrase": "factor_analysis"}, {"score": 0.004640398886726948, "phrase": "classical_multivariate_analysis"}, {"score": 0.0045765783794477505, "phrase": "modern_copula_modeling"}, {"score": 0.004472146692254812, "phrase": "central_concept"}, {"score": 0.004431041346291019, "phrase": "dependence_modeling"}, {"score": 0.004390312150344245, "phrase": "multivariate_normal_distributions"}, {"score": 0.004231089088705995, "phrase": "correlation_parameters"}, {"score": 0.004002972723329302, "phrase": "large_correlation_matrices"}, {"score": 0.0035663245542174224, "phrase": "attractive_alternative"}, {"score": 0.0035010071030752883, "phrase": "graphical_models"}, {"score": 0.0032514447965530323, "phrase": "correlation_matrix"}, {"score": 0.0031918759137610523, "phrase": "algebraically_independent_correlations"}, {"score": 0.0031625006830124512, "phrase": "partial_correlations"}, {"score": 0.0030056887638095883, "phrase": "so-called_truncation"}, {"score": 0.0028172726995962173, "phrase": "factor_models"}, {"score": 0.0027275289970185015, "phrase": "combined_model"}, {"score": 0.0026899455459275575, "phrase": "individual_benefits"}, {"score": 0.002407290605654707, "phrase": "tree_algorithms"}, {"score": 0.0023522423291484212, "phrase": "modified_em_algorithm"}, {"score": 0.0023198180380964305, "phrase": "combined_factor-vine_model"}, {"score": 0.0022458849724980904, "phrase": "simulation_study"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multivariate normal", " Partial correlations", " Regular vines", " Markov trees", " EM algorithm"], "paper_abstract": "Both in classical multivariate analysis and in modern copula modeling, correlation matrices are a central concept of dependence modeling using multivariate normal distributions and copulas. Since the number of correlation parameters quadratically increases with the number of variables, parsimonious parameterizations of large correlation matrices in terms of O(d) parameters are important. While factor analysis is commonly used for this purpose, the use of vines is an attractive alternative: vines are graphical models based on a sequence of trees, and are based on the decomposition of a correlation matrix in terms of algebraically independent correlations and partial correlations. By limiting the number of trees, with the so-called truncation, parsimonious parameterizations of correlation matrices may be found. Moreover, truncated vines and factor models may be joined to define a combined model, with individual benefits from each of the two approaches. The different parameterizations and how they are estimated for data are discussed. In particular, spanning tree algorithms for truncated vines and a modified EM algorithm for the combined factor-vine model are proposed and evaluated in a simulation study. Three applications to psychometric and finance data sets illustrate the different parsimonious models. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Parsimonious parameterization of correlation matrices using truncated vines and factor analysis", "paper_id": "WOS:000337869500017"}