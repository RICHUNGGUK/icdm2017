{"auto_keywords": [{"score": 0.041469740173514034, "phrase": "nonlinear_models"}, {"score": 0.027773607799187222, "phrase": "smkl"}, {"score": 0.00481495049065317, "phrase": "sparse_multiple_kernel_learning_for_signal_processing_applications"}, {"score": 0.004674952036988355, "phrase": "model_development"}, {"score": 0.004583878086896926, "phrase": "small_number"}, {"score": 0.004553914287137776, "phrase": "relevant_groups"}, {"score": 0.004406994912939125, "phrase": "learned_parameters"}, {"score": 0.004278807087784626, "phrase": "linear_models"}, {"score": 0.0041136473765874815, "phrase": "multiple_kernel_learning"}, {"score": 0.003877737556673229, "phrase": "multiple_kernel"}, {"score": 0.003802135020791801, "phrase": "convex_primal_problem_formulations"}, {"score": 0.00376488621224566, "phrase": "kernel_weights"}, {"score": 0.0036553070575015344, "phrase": "sparsest_possible_solution"}, {"score": 0.0036194914688460656, "phrase": "main_reason"}, {"score": 0.0035722806911457545, "phrase": "convex_primal_formulation"}, {"score": 0.003537275726870251, "phrase": "efficient_implementations"}, {"score": 0.0035141291835319682, "phrase": "kernel-based_methods"}, {"score": 0.003445591238183763, "phrase": "dual_problem"}, {"score": 0.0033452741392850523, "phrase": "additional_log-based_concave_penalty_term"}, {"score": 0.0031740669025323606, "phrase": "generalized_iterative_learning_algorithm"}, {"score": 0.003091767402370757, "phrase": "linear_combination"}, {"score": 0.0030614565460256897, "phrase": "concave_penalty_term"}, {"score": 0.002982068153662372, "phrase": "parameter_estimation"}, {"score": 0.0029528295737527026, "phrase": "primal_space"}, {"score": 0.0028762499230524812, "phrase": "natural_extension"}, {"score": 0.0028016507334576216, "phrase": "\"kernel_trick"}, {"score": 0.0027560092576928595, "phrase": "new_algorithm"}, {"score": 0.0027289810731258865, "phrase": "sparse_multiple_kernel"}, {"score": 0.0026581913054027663, "phrase": "group-feature_selection"}, {"score": 0.0025722741181993278, "phrase": "existing_efficient_single_kernel_algorithms"}, {"score": 0.002538687900036229, "phrase": "sparser_solution"}, {"score": 0.0024325244760735566, "phrase": "existing_multiple_kernel_learning_framework"}, {"score": 0.0023928819778638055, "phrase": "signal_processing_examples"}, {"score": 0.002346160830406661, "phrase": "mass_spectra"}, {"score": 0.002330790244117364, "phrase": "cancer_detection"}, {"score": 0.0023155201226300955, "phrase": "hyperspectral_imagery"}, {"score": 0.0023003498123420237, "phrase": "land_cover_classification"}, {"score": 0.002278013024573271, "phrase": "nir"}], "paper_keywords": ["Composite kernel learning", " feature group selection", " heterogeneous data fusion", " sensor selection"], "paper_abstract": "In many signal processing applications, grouping of features during model development and the selection of a small number of relevant groups can be useful to improve the interpretability of the learned parameters. While a lot of work based on linear models has been reported to solve this problem, in the last few years, multiple kernel learning has come up as a candidate to solve this problem in nonlinear models. Since all of the multiple kernel learning algorithms to date use convex primal problem formulations, the kernel weights selected by these algorithms are not strictly the sparsest possible solution. The main reason for using a convex primal formulation is that efficient implementations of kernel-based methods invariably rely on solving the dual problem. This work proposes the use of an additional log-based concave penalty term in the primal problem to induce sparsity in terms of groups of parameters. A generalized iterative learning algorithm, which can be used with a linear combination of this concave penalty term with other penalty terms, is given for model parameter estimation in the primal space. It is then shown that a natural extension of the method to nonlinear models using the \"kernel trick\" results in a new algorithm, called Sparse Multiple Kernel Learning (SMKL), which generalizes group-feature selection to kernel selection. SMKL is capable of exploiting existing efficient single kernel algorithms while providing a sparser solution in terms of the number of kernels used as compared to the existing multiple kernel learning framework. A number of signal processing examples based on the use of mass spectra for cancer detection, hyperspectral imagery for land cover classification, and NIR spectra from wheat, fescue grass, and diesel are given to highlight the ability of SMKL to achieve a very high accuracy with a very few kernels.", "paper_title": "Sparse Multiple Kernel Learning for Signal Processing Applications", "paper_id": "WOS:000275569300002"}