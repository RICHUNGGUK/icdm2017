{"auto_keywords": [{"score": 0.04654263670906759, "phrase": "bernoulli_distribution"}, {"score": 0.00481495049065317, "phrase": "topic_modeling"}, {"score": 0.0047379542803396915, "phrase": "selective_supervised_latent_dirichlet_allocation"}, {"score": 0.004606148097788071, "phrase": "prediction_performance"}, {"score": 0.004550783060775205, "phrase": "widely_studied_supervised_probabilistic_topic_models"}, {"score": 0.00421520166010848, "phrase": "strongly_or_weakly_discriminative_one"}, {"score": 0.004016072233242585, "phrase": "discrimination_power"}, {"score": 0.003473178781851477, "phrase": "topic_modeling_domain"}, {"score": 0.0032956583072431423, "phrase": "traditional_natural_language_processing_domain"}, {"score": 0.0032428780373184207, "phrase": "new_perspective"}, {"score": 0.0031909403420958752, "phrase": "general_framework"}, {"score": 0.003165283275312972, "phrase": "supervised_lda"}, {"score": 0.0029792984108210525, "phrase": "gaussian_linear_model"}, {"score": 0.0028498718992429825, "phrase": "word_selection_mechanism"}, {"score": 0.002826949449085849, "phrase": "singe-label_document_classification"}, {"score": 0.0027370835853738626, "phrase": "variational_inference"}, {"score": 0.0026932247685847246, "phrase": "intractable_posterior"}, {"score": 0.0026500668745484957, "phrase": "maximum-likelihood_estimation"}, {"score": 0.0025554669315362424, "phrase": "textual_documents"}], "paper_keywords": ["Topic modeling", " latent Dirichlet allocation", " supervised learning", " classification"], "paper_abstract": "We propose selective supervised Latent Dirichlet Allocation (ssLDA) to boost the prediction performance of the widely studied supervised probabilistic topic models. We introduce a Bernoulli distribution for each word in one given document to select this word as a strongly or weakly discriminative one with respect to its assigned topic. The Bernoulli distribution is parameterized by the discrimination power of the word for its assigned topic. As a result, the document is represented as a \"bag-of-selective-words\" instead of the probabilistic \"bag-of-topics\" in the topic modeling domain or the flat \"bag-of-words\" in the traditional natural language processing domain to form a new perspective. Inheriting the general framework of supervised LDA (sLDA), ssLDA can also predict many types of response specified by a Gaussian Linear Model (GLM). Focusing on the utilization of this word selection mechanism for singe-label document classification in this paper, we conduct the variational inference for approximating the intractable posterior and derive a maximum-likelihood estimation of parameters in ssLDA. The experiments reported on textual documents show that ssLDA not only performs competitively over \"state-of-the-art\" classification approaches based on both the flat \"bag-of-words\" and probabilistic \"bag-of-topics\" representation in terms of classification performance, but also has the ability to discover the discrimination power of the words specified in the topics (compatible with our rational knowledge).", "paper_title": "Probabilistic Word Selection via Topic Modeling", "paper_id": "WOS:000353890600015"}