{"auto_keywords": [{"score": 0.0495290999002817, "phrase": "internet_computing"}, {"score": 0.039452474962437374, "phrase": "download_time"}, {"score": 0.00481495049065317, "phrase": "efficient_distributed_web_crawling_utilizing_internet_resources"}, {"score": 0.0046240999614456605, "phrase": "personal_computing_resources"}, {"score": 0.004415189385015778, "phrase": "large-scale_web_applications"}, {"score": 0.004167218012926657, "phrase": "dht-based_distributed_web_crawling_model"}, {"score": 0.0036272258853286433, "phrase": "web_crawling_tasks"}, {"score": 0.003503497128638099, "phrase": "system's_throughput"}, {"score": 0.003138768436081888, "phrase": "crawler-crawlee_rtts"}, {"score": 0.002945175711537758, "phrase": "network_coordinate_system"}, {"score": 0.002861160152640875, "phrase": "underlying_dht."}, {"score": 0.0027795345823932406, "phrase": "waiting_time"}, {"score": 0.0026846436189970446, "phrase": "incoming_crawling_tasks"}, {"score": 0.00265373679629359, "phrase": "light-loaded_crawlers"}, {"score": 0.0024049552650867935, "phrase": "simple_web_site_partition_method"}, {"score": 0.002349884235026956, "phrase": "large_web_site"}, {"score": 0.0023228223366301226, "phrase": "smaller_pieces"}, {"score": 0.0022434880661725493, "phrase": "task_granularity"}, {"score": 0.0021543421286324945, "phrase": "real_internet_tests"}, {"score": 0.0021049977753042253, "phrase": "satisfactory_results"}], "paper_keywords": ["Internet computing", " distributed Web crawling", " DHT", " network coordinate system", " load balancing"], "paper_abstract": "Internet computing is proposed to exploit personal computing resources across the Internet in order to build large-scale Web applications at lower cost. In this paper, a DHT-based distributed Web crawling model based on the concept of Internet computing is proposed. Also, we propose two optimizations to reduce the download time and waiting time of the Web crawling tasks in order to increase the system's throughput and update rate. Based on our contributor-friendly download scheme, the improvement on the download time is achieved by shortening the crawler-crawlee RTTs. In order to accurately estimate the RTTs, a network coordinate system is combined with the underlying DHT. The improvement on the waiting time is achieved by redirecting the incoming crawling tasks to light-loaded crawlers in order to keep the queue on each crawler equally sized. We also propose a simple Web site partition method to split a large Web site into smaller pieces in order to reduce the task granularity. All the methods proposed are evaluated through real Internet tests and simulations showing satisfactory results.", "paper_title": "Efficient Distributed Web Crawling Utilizing Internet Resources", "paper_id": "WOS:000285488900010"}