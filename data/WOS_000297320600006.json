{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "n-person_general-sum_multiagent_reinforcement"}, {"score": 0.004087207102190335, "phrase": "existing_learning_process"}, {"score": 0.003973435596133265, "phrase": "markov"}, {"score": 0.003725470458263889, "phrase": "successive_extensive_form_games"}, {"score": 0.0035189057079817285, "phrase": "estimated_value"}, {"score": 0.002881793492919646, "phrase": "boltzmann_distribution"}, {"score": 0.0027025233739857374, "phrase": "proposed_exploration_strategies"}, {"score": 0.0024281181691350085, "phrase": "nash_equilibrium_points"}, {"score": 0.0022933216359142736, "phrase": "sequential_action_selection"}, {"score": 0.0021352776064254195, "phrase": "dynamic_task_multiagent_systems"}], "paper_keywords": ["Multiagent reinforcement learning", " Nash equilibrium points", " exploration-exploitation tradeoff", " Markov games"], "paper_abstract": "In this paper, two novel exploration strategies are proposed for n-person general-sum multiagent reinforcement learning with sequential action selection. The existing learning process, called extensive Markov game, is considered as a set of successive extensive form games with perfect information. We introduce an estimated value for taking actions in games with respect to other agents' preferences which is called associative Q-value. They can be used to select actions probabilistically according to Boltzmann distribution. Simulation results present the effectiveness of the proposed exploration strategies that are used in our previously introduced extensive-Q learning methods. Regarding the complexity of existing methods of computing Nash equilibrium points, if it is possible to assume sequential action selection among agents, extensive-Q will be more convenient for dynamic task multiagent systems with more than two agents.", "paper_title": "Exploration strategies in n-Person general-sum multiagent reinforcement learning with sequential action selection", "paper_id": "WOS:000297320600006"}