{"auto_keywords": [{"score": 0.049469816714638924, "phrase": "reward_functions"}, {"score": 0.00481495049065317, "phrase": "novel_acquisition_function"}, {"score": 0.0046632033374901715, "phrase": "essential_component"}, {"score": 0.004024242000713225, "phrase": "reliable_success_measures"}, {"score": 0.0038230844545071303, "phrase": "extensive_task_knowledge"}, {"score": 0.0037024804835018373, "phrase": "undesired_emergent_behavior"}, {"score": 0.0034063576796640603, "phrase": "action_policy"}, {"score": 0.0032777653153451265, "phrase": "reward_function"}, {"score": 0.0031743093007496736, "phrase": "human_expert"}, {"score": 0.003034916887102753, "phrase": "reward_model"}, {"score": 0.002977061391551628, "phrase": "gaussian_process"}, {"score": 0.0028100102841877835, "phrase": "bayesian_optimization_literature"}, {"score": 0.0026187449947322845, "phrase": "af"}, {"score": 0.0023029886389559122, "phrase": "learned_reward_function"}, {"score": 0.002244596395352896, "phrase": "similar_task"}, {"score": 0.002145942273032774, "phrase": "proposed_novel_af"}, {"score": 0.0021049977753042253, "phrase": "real_robot_pendulum_swing-up_task"}], "paper_keywords": ["Reinforcement learning", " Active learning", " Bayesian optimization", " Preference learning", " Inverse reinforcement learning", " Reward functions", " Acquisition functions"], "paper_abstract": "Reward functions are an essential component of many robot learning methods. Defining such functions, however, remains hard in many practical applications. For tasks such as grasping, there are no reliable success measures available. Defining reward functions by hand requires extensive task knowledge and often leads to undesired emergent behavior. We introduce a framework, wherein the robot simultaneously learns an action policy and a model of the reward function by actively querying a human expert for ratings. We represent the reward model using a Gaussian process and evaluate several classical acquisition functions (AFs) from the Bayesian optimization literature in this context. Furthermore, we present a novel AF, expected policy divergence. We demonstrate results of our method for a robot grasping task and show that the learned reward function generalizes to a similar task. Additionally, we evaluate the proposed novel AF on a real robot pendulum swing-up task.", "paper_title": "Active reward learning with a novel acquisition function", "paper_id": "WOS:000361459500011"}