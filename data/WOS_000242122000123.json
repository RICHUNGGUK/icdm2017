{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "positive_regions"}, {"score": 0.004494570360108983, "phrase": "existing_discretization_methods"}, {"score": 0.004363846362238565, "phrase": "k-interval_discretization"}, {"score": 0.004154332348800334, "phrase": "equal_frequency_methods"}, {"score": 0.0033452741392850523, "phrase": "discretization_algorithm"}, {"score": 0.0030017207229983385, "phrase": "condition_attributes"}, {"score": 0.0026933542552712033, "phrase": "decision_attribute"}, {"score": 0.0025137856138000014, "phrase": "classification_rules"}, {"score": 0.0024405312482831646, "phrase": "data_set"}, {"score": 0.0021682027434117095, "phrase": "least_condition_attributes"}, {"score": 0.0021049977753042253, "phrase": "highest_classification_precision"}], "paper_keywords": [""], "paper_abstract": "Most of the existing discretization methods such as k-interval discretization, equal width and equal frequency methods do not take the dependencies of decision attributes on condition attributes into account. In this paper, we propose a discretization algorithm that can keep the dependencies of the decision attribute on condition attributes, or keep the positive regions of the partition of the decision attribute. In the course of inducing classification rules from a data set, keeping these dependencies can achieve getting the set of the least condition attributes and the highest classification precision.", "paper_title": "A discretization algorithm that keeps positive regions of all the decision classes", "paper_id": "WOS:000242122000123"}