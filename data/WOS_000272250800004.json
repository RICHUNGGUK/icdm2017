{"auto_keywords": [{"score": 0.0047709540557306284, "phrase": "bayes_classifier"}, {"score": 0.0047273577269638725, "phrase": "based_on_credal_decision_trees._variable_selection_methods"}, {"score": 0.0037231102027070724, "phrase": "decision_trees"}, {"score": 0.003459191419478298, "phrase": "useful_tool"}, {"score": 0.0033651019362620866, "phrase": "relevant_and_informative_variables"}, {"score": 0.00327356326590461, "phrase": "mainclass_variable"}, {"score": 0.0031553611552558986, "phrase": "naive_bayes_classifier"}, {"score": 0.002673957615079041, "phrase": "split_criterion"}, {"score": 0.0026252106881752067, "phrase": "better_performance"}, {"score": 0.0025537472239953807, "phrase": "complete_decision_tree"}, {"score": 0.0022450768409733807, "phrase": "filter-wrappers_election_method"}, {"score": 0.0021049977753042253, "phrase": "best_possible_split_criterion"}], "paper_keywords": ["Variable selection", " classification", " decision tree", " Naive Bayes", " imprecise probabilities", " uncertainty measures"], "paper_abstract": "Variable selection methods play an important role in the field of attribute mining. In the last few years, several feature selection methods have appeared showing that the use of a set of decision trees learnt from a database can be a useful tool for selecting relevant and informative variables regarding a mainclass variable. With the Naive Bayes classifier as reference, in this article, our aims are two fold: (1) to study what split criterion has better performance when a complete decision tree is used to select variables; and (2) to present a filter-wrappers election method using decision trees built with the best possible split criterion obtained in (1).", "paper_title": "A FILTER-WRAPPER METHOD TO SELECT VARIABLES FOR THE NAIVE BAYES CLASSIFIER BASED ON CREDAL DECISION TREES", "paper_id": "WOS:000272250800004"}