{"auto_keywords": [{"score": 0.041040679407137085, "phrase": "shape_space"}, {"score": 0.00481495049065317, "phrase": "riemannian_shape_manifolds"}, {"score": 0.004651271219032404, "phrase": "human_gestures"}, {"score": 0.004475893575036647, "phrase": "riemannian_geometry"}, {"score": 0.004441616106463486, "phrase": "shape_spaces"}, {"score": 0.004357061494162031, "phrase": "human_gesture"}, {"score": 0.004307099985737249, "phrase": "temporal_sequence"}, {"score": 0.00427410962921379, "phrase": "human_poses"}, {"score": 0.0041287394089001405, "phrase": "associated_human_silhouette"}, {"score": 0.003882356585452982, "phrase": "closed_curves"}, {"score": 0.003485913728272778, "phrase": "first_template-based_approach"}, {"score": 0.0034326732528567826, "phrase": "dynamic_time"}, {"score": 0.003328611206488653, "phrase": "different_trajectories"}, {"score": 0.0033030907500120397, "phrase": "elastic_geodesic_distances"}, {"score": 0.00322769360059123, "phrase": "gesture_templates"}, {"score": 0.0031418958350109038, "phrase": "aligned_trajectories"}, {"score": 0.003093893243443715, "phrase": "second_approach"}, {"score": 0.003034916887102753, "phrase": "graphical_model_approach"}, {"score": 0.0029885438439583075, "phrase": "exemplar-based_hidden_markov_model"}, {"score": 0.0028317319086202217, "phrase": "non-parametric_statistical_models"}, {"score": 0.0026728137770370176, "phrase": "markov_model"}, {"score": 0.002571841741339248, "phrase": "proposed_approaches"}, {"score": 0.002542298702137655, "phrase": "extensive_set"}, {"score": 0.002446245096967702, "phrase": "action_recognition_applications"}, {"score": 0.002282380944889266, "phrase": "different_classes"}, {"score": 0.002154226045704907, "phrase": "background_subtraction"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Gesture recognition", " Action recognition", " Riemannian manifolds", " Shape space", " Silhouette-based approaches"], "paper_abstract": "This paper addresses the problem of recognizing human gestures from videos using models that are built from the Riemannian geometry of shape spaces. We represent a human gesture as a temporal sequence of human poses, each characterized by a contour of the associated human silhouette. The shape of a contour is viewed as a point on the shape space of closed curves and, hence, each gesture is characterized and modeled as a trajectory on this shape space. We propose two approaches for modeling these trajectories. In the first template-based approach, we use dynamic time warping (DTW) to align the different trajectories using elastic geodesic distances on the shape space. The gesture templates are then calculated by averaging the aligned trajectories. In the second approach, we use a graphical model approach similar to an exemplar-based hidden Markov model, where we cluster the gesture shapes on the shape space, and build non-parametric statistical models to capture the variations within each cluster. We model each gesture as a Markov model of transitions between these clusters. To evaluate the proposed approaches, an extensive set of experiments was performed using two different data sets representing gesture and action recognition applications. The proposed approaches not only are successfully able to represent the shape and dynamics of the different classes for recognition, but are also robust against some errors resulting from segmentation and background subtraction. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Silhouette-based gesture and action recognition via modeling trajectories on Riemannian shape manifolds", "paper_id": "WOS:000287772400014"}