{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "penalized_trimmed_squares"}, {"score": 0.004751216062917875, "phrase": "multiple_outliers"}, {"score": 0.004646852519870358, "phrase": "regression_models"}, {"score": 0.004585332915477379, "phrase": "business_economics_engineers"}, {"score": 0.004232875294230749, "phrase": "single_outlying_observation"}, {"score": 0.00408501512749186, "phrase": "high_breakdown_robust_estimators"}, {"score": 0.003907403442285109, "phrase": "data_sample"}, {"score": 0.003737485125097945, "phrase": "least_squares"}, {"score": 0.0035590664550919854, "phrase": "reasonable_high_breakdown"}, {"score": 0.003389136094439662, "phrase": "objective_function"}, {"score": 0.003359119985242403, "phrase": "penalty_cost"}, {"score": 0.003198705576491802, "phrase": "upper_bound"}, {"score": 0.0031562971707460802, "phrase": "residual_error"}, {"score": 0.003114449258490062, "phrase": "feasible_regression_line"}, {"score": 0.0030731691096403924, "phrase": "pts"}, {"score": 0.0028492532869274743, "phrase": "better_efficiency"}, {"score": 0.0026771333996253783, "phrase": "regression_line"}, {"score": 0.0025950024597986366, "phrase": "new_class"}, {"score": 0.002572001496778222, "phrase": "regression_estimates"}, {"score": 0.002482013330276732, "phrase": "new_gpts_estimator"}, {"score": 0.0023424328921363585, "phrase": "influence_function"}, {"score": 0.0022604585778195152, "phrase": "numerical_examples"}, {"score": 0.0022304616068189575, "phrase": "monte_carlo_simulation_study"}, {"score": 0.002200861826643147, "phrase": "generalized_pts_estimate"}, {"score": 0.002152397315548125, "phrase": "robust_and_efficiency_properties"}, {"score": 0.002105007807034302, "phrase": "elsevier"}], "paper_keywords": ["Robust regression", " Monte Carlo simulation", " Penalized Trimmed Squares", " Unmasking outliers", " Bounded influence"], "paper_abstract": "Multiple outliers are frequently encountered in regression models used in business economics engineers and applied studies The ordinary least squares (OLS) estimator fails even in the presence of a single outlying observation To overcome this problem a class of high breakdown robust estimators (insensitive to outliers up to 50% of the data sample) has been introduced as an alternative to the least squares regression Among them the Penalized Trimmed Squares (PTS) is a reasonable high breakdown estimator This estimator is defined by the minimization of an objective function where penalty cost for deleting an outlier is added which serves as an upper bound on the residual error for any feasible regression line Since the PTS does not require presetting the number of outliers to delete from the data set it has better efficiency with respect to other estimators However small outliers remain influential causing bias to the regression line In this work we present a new class of regression estimates called generalized PTS (GPTS) The new GPTS estimator is defined as the PTS but with penalties suitable for bounding the influence function of all observations We show with some numerical examples and a Monte Carlo simulation study that the generalized PTS estimate has very good performance for both robust and efficiency properties (C) 2010 Elsevier B V All rights reserved", "paper_title": "Locally and globally robust Penalized Trimmed Squares regression", "paper_id": "WOS:000284987900013"}