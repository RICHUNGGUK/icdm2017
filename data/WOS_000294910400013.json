{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gpu"}, {"score": 0.049176226826599265, "phrase": "lbg"}, {"score": 0.004665483918844599, "phrase": "som_training"}, {"score": 0.0046069985410261746, "phrase": "vector_quantization"}, {"score": 0.004435885269134359, "phrase": "effective_technique"}, {"score": 0.004325339932975582, "phrase": "wide_range"}, {"score": 0.004164644263639545, "phrase": "image_compression"}, {"score": 0.004112411350177274, "phrase": "pattern_recognition"}, {"score": 0.003984677652600657, "phrase": "vq"}, {"score": 0.003934681262271618, "phrase": "codebook_training"}, {"score": 0.003788444178071508, "phrase": "frequently_used_training_algorithms"}, {"score": 0.003693973656103879, "phrase": "self-organizing_map"}, {"score": 0.0035342634200440435, "phrase": "desktop_computers"}, {"score": 0.0034244153725855, "phrase": "programmable_graphics_processing_units"}, {"score": 0.003235193484261443, "phrase": "codebook_training_acceleration"}, {"score": 0.0031148723927633955, "phrase": "gpu_algorithms"}, {"score": 0.0030757650386969903, "phrase": "lbg_training"}, {"score": 0.0029613556844064713, "phrase": "large_amount"}, {"score": 0.0029241700853970013, "phrase": "data_transfer"}, {"score": 0.0027976491158911514, "phrase": "large_number"}, {"score": 0.0027625134766418266, "phrase": "rendering_passes"}, {"score": 0.0027106334164712057, "phrase": "training_iteration"}, {"score": 0.002609770322855842, "phrase": "novel_gpu-based_training_implementation"}, {"score": 0.002544653460614838, "phrase": "som"}, {"score": 0.0024038923350585962, "phrase": "random_write_ability"}, {"score": 0.002373690110881497, "phrase": "vertex_shader"}, {"score": 0.0021049977753042253, "phrase": "previous_approach"}], "paper_keywords": ["Graphics processing units", " Vector quantization", " LBG, SOM"], "paper_abstract": "Vector quantization (VQ) is an effective technique applicable in a wide range of areas, such as image compression and pattern recognition. The most time-consuming procedure of VQ is codebook training, and two of the frequently used training algorithms are LBG and self-organizing map (SOM). Nowadays, desktop computers are usually equipped with programmable graphics processing units (GPUs), whose parallel data-processing ability is ideal for codebook training acceleration. Although there are some GPU algorithms for LBG training, their implementations suffer from a large amount of data transfer between CPU and GPU and a large number of rendering passes within a training iteration. This paper presents a novel GPU-based training implementation for LBG and SOM training. More specifically, we utilize the random write ability of vertex shader to reduce the overheads mentioned above. Our experimental results show that our approach can run four times faster than the previous approach.", "paper_title": "A GPU implementation for LBG and SOM training", "paper_id": "WOS:000294910400013"}