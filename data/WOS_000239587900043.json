{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "p-norm_push"}, {"score": 0.004652016085325825, "phrase": "supervised_ranking"}, {"score": 0.004171402455813099, "phrase": "ranked_list"}, {"score": 0.003634371940842838, "phrase": "general_form"}, {"score": 0.003592859432554987, "phrase": "convex_objective"}, {"score": 0.003531474887552889, "phrase": "high-scoring_examples"}, {"score": 0.0029722900280316216, "phrase": "specific_type"}, {"score": 0.002530359917257626, "phrase": "p-norm_objective"}, {"score": 0.002444544487772349, "phrase": "corresponding_boosting-style_algorithm"}, {"score": 0.0022815262950738814, "phrase": "uci_data"}, {"score": 0.0021049977753042253, "phrase": "specific_sense"}], "paper_keywords": [""], "paper_abstract": "We are interested in supervised ranking with the following twist: our goal is to design algorithms that perform especially well near the top of the ranked list, and are only required to perform sufficiently well on the rest of the list. Towards this goal, we provide a general form of convex objective that gives high-scoring examples more importance. This \"push\" near the top of the list can be chosen to be arbitrarily large or small. We choose l(p)-norms to provide a specific type of push; as p becomes large, the algorithm concentrates harder near the top of the list. We derive a generalization bound based on the p-norm objective. We then derive a corresponding boosting-style algorithm, and illustrate the usefulness of the algorithm through experiments on UCI data. We prove that the minimizer of the objective is unique in a specific sense.", "paper_title": "Ranking with a p-norm push", "paper_id": "WOS:000239587900043"}