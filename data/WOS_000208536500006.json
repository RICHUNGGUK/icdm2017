{"auto_keywords": [{"score": 0.04897776997473009, "phrase": "driving_simulator"}, {"score": 0.00481495049065317, "phrase": "multimodal_signal_processing_and"}, {"score": 0.004361428012479218, "phrase": "software_design"}, {"score": 0.0042859292226559535, "phrase": "multimodal_driving_simulator"}, {"score": 0.004138813595009761, "phrase": "multimodal_driver's_focus"}, {"score": 0.003973523379376352, "phrase": "driver's_fatigue_state_detection"}, {"score": 0.003792657064634775, "phrase": "driver's_focus"}, {"score": 0.003705323407433896, "phrase": "fatigue_state"}, {"score": 0.003619993477999133, "phrase": "video_data"}, {"score": 0.0032978134802348433, "phrase": "input_multimodal_interface"}, {"score": 0.0032406665327678616, "phrase": "passive_modalities"}, {"score": 0.003039445153076014, "phrase": "output_multimodal_user_interface"}, {"score": 0.002952146666681891, "phrase": "alert_messages"}, {"score": 0.0025816953009851072, "phrase": "active_input_modalities"}, {"score": 0.002507511169156787, "phrase": "meta-user_interface"}, {"score": 0.0023932142697579506, "phrase": "output_modalities"}, {"score": 0.0022841153001549193, "phrase": "case_study"}, {"score": 0.002205560249972527, "phrase": "multimodal_signal_processing_and_multimodal_interaction_components"}, {"score": 0.002129701089817251, "phrase": "openinterface"}, {"score": 0.0021049977753042253, "phrase": "icare."}], "paper_keywords": ["Attention level", " Component", " Driving simulator", " Facial movement analysis", " ICARE", " Interaction modality", " OpenInterface", " Software architecture", " Multimodal interaction"], "paper_abstract": "In this paper we focus on the software design of a multimodal driving simulator that is based on both multimodal driver's focus of attention detection as well as driver's fatigue state detection and prediction. Capturing and interpreting the driver's focus of attention and fatigue state is based on video data (e.g., facial expression, head movement, eye tracking). While the input multimodal interface relies on passive modalities only (also called attentive user interface), the output multimodal user interface includes several active output modalities for presenting alert messages including graphics and text on a mini-screen and in the windshield, sounds, speech and vibration (vibration wheel). Active input modalities are added in the meta-User Interface to let the user dynamically select the output modalities. The driving simulator is used as a case study for studying its software architecture based on multimodal signal processing and multimodal interaction components considering two software platforms, OpenInterface and ICARE.", "paper_title": "MULTIMODAL SIGNAL PROCESSING AND INTERACTION FOR A DRIVING SIMULATOR: COMPONENT-BASED ARCHITECTURE", "paper_id": "WOS:000208536500006"}