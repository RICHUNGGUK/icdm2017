{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "semantic_correlation"}, {"score": 0.019182753042953422, "phrase": "manifest_concepts"}, {"score": 0.015613917306866325, "phrase": "cross-modal_retrieval"}, {"score": 0.007788418238935512, "phrase": "content_correlation"}, {"score": 0.005830103146992519, "phrase": "training_data"}, {"score": 0.0049493989408731275, "phrase": "latent_semantic_concepts"}, {"score": 0.004685224205630963, "phrase": "multimedia_technology"}, {"score": 0.004653339900995785, "phrase": "effective_cross-modal_retrieval_methods"}, {"score": 0.004558977045730675, "phrase": "key_point"}, {"score": 0.004436116555817464, "phrase": "heterogeneous_modalities"}, {"score": 0.00417160036938041, "phrase": "high_level"}, {"score": 0.0040452928361255445, "phrase": "human_understanding"}, {"score": 0.0038960785110831162, "phrase": "semantic_model"}, {"score": 0.0037140731069193896, "phrase": "multimedia_data"}, {"score": 0.003688773272422118, "phrase": "different_modalities"}, {"score": 0.003613899384402803, "phrase": "semantic_concepts"}, {"score": 0.003577032245103867, "phrase": "probabilistic_generation_framework"}, {"score": 0.003540539868937462, "phrase": "cross-modal_semantic_generation_model"}, {"score": 0.003352088714126572, "phrase": "cross-modal_retrieval_task"}, {"score": 0.003284026247389735, "phrase": "ideal_case"}, {"score": 0.0031198362555601377, "phrase": "manifest_cmsgm"}, {"score": 0.003025277245730244, "phrase": "cmsgm"}, {"score": 0.002994396880146532, "phrase": "manifest_semantic_concepts"}, {"score": 0.0026114748685439116, "phrase": "asymmetric_spectral_clustering"}, {"score": 0.0024055884143050037, "phrase": "combinative_cmsgm"}, {"score": 0.0023167153974120083, "phrase": "experimental_results"}, {"score": 0.002300936026389169, "phrase": "wikipedia"}, {"score": 0.0022234951091677085, "phrase": "better_performance"}, {"score": 0.002126725585391261, "phrase": "c-cmsgm"}, {"score": 0.0021049977753042253, "phrase": "good_performance"}], "paper_keywords": ["Cross-modal retrieval", " Semantic correlation", " Generation model", " Semantic concept"], "paper_abstract": "With the development of multimedia technology, effective cross-modal retrieval methods are increasingly demanded. The key point of cross-modal retrieval is analyzing the correlation of heterogeneous modalities. There are mainly two types of correlation: content correlation and semantic correlation. Semantic correlation is constructed at a high level of abstraction which is more close to the human understanding than content correlation. In this paper, we investigate a semantic model to construct the semantic correlation for cross-modal retrieval. We assume that the semantic correlation of multimedia data from different modalities can be conditionally generated by semantic concepts in a probabilistic generation framework. The cross-modal semantic generation model (CMSGM) is proposed based on this assumption. We consider three cases of the cross-modal retrieval task. The first is the ideal case that all manifest concepts exist in training data for constructing the correlation, and we propose manifest CMSGM (M-CMSGM) which directly uses CMSGM on the manifest semantic concepts for retrieval. The second is the case that there are no manifest concepts in training data, and latent CMSGM (L-CMSGM) based on latent semantic concepts is proposed for this case, where the latent semantic concepts are learned by asymmetric spectral clustering. The last is the most general case that some of the manifest concepts exist, and we combine M-CMSGM and L-CMSGM to get combinative CMSGM (C-CMSGM) to solve this case. Experimental results on Wikipedia featured articles and MIR Flickr show that our methods have better performance compared with previous state-of-the-art methods. And C-CMSGM can maintain good performance in the case that manifest concepts are lacking, which confirms the robustness and practicality of C-CMSGM.", "paper_title": "Analyzing semantic correlation for cross-modal retrieval", "paper_id": "WOS:000361493900001"}