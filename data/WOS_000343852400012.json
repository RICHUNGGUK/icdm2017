{"auto_keywords": [{"score": 0.030580277430729972, "phrase": "class_discrimination"}, {"score": 0.00481495049065317, "phrase": "kernel_reference_discriminant_analysis"}, {"score": 0.004747997855358171, "phrase": "linear_discriminant_analysis"}, {"score": 0.0046824550644258035, "phrase": "lda"}, {"score": 0.004552648949390462, "phrase": "kernel_discriminant_analysis"}, {"score": 0.00448933120642516, "phrase": "kda"}, {"score": 0.004395984840790689, "phrase": "well-known_and_widely_used_techniques"}, {"score": 0.004215057618794356, "phrase": "dimensionality_reduction"}, {"score": 0.004069963083642548, "phrase": "optimal_discriminant_space"}, {"score": 0.00379452936798156, "phrase": "normal_distributions"}, {"score": 0.0035625556447293804, "phrase": "kernel_space"}, {"score": 0.0033919477901198716, "phrase": "class_representation"}, {"score": 0.0033213460816510685, "phrase": "corresponding_class"}, {"score": 0.00329813918116714, "phrase": "mean_vectors"}, {"score": 0.003010757286626405, "phrase": "classes_representation"}, {"score": 0.0028265625216338875, "phrase": "resulted_feature_space"}, {"score": 0.002653606593103781, "phrase": "optimization_scheme"}, {"score": 0.0025801648787263662, "phrase": "optimal_class_representation"}, {"score": 0.0024912073108138613, "phrase": "fisher_ratio_maximization"}, {"score": 0.002439308179084911, "phrase": "nonlinear_data_projection"}, {"score": 0.0023551955270737215, "phrase": "standard_approach"}, {"score": 0.002306123367679187, "phrase": "proposed_optimization_scheme"}, {"score": 0.002226592967032469, "phrase": "reduced-dimensionality_feature_space"}, {"score": 0.0021801944769738618, "phrase": "higher_classification_rates"}, {"score": 0.002149799391146313, "phrase": "publicly_available_data_sets"}], "paper_keywords": ["Kernel Discriminant Analysis", " Kernel Spectral Regression", " Optimized class representation"], "paper_abstract": "Linear Discriminant Analysis (LDA) and its nonlinear version Kernel Discriminant Analysis (KDA) are well-known and widely used techniques for supervised feature extraction and dimensionality reduction. They determine an optimal discriminant space for (non)linear data projection based on certain assumptions, e.g. on using normal distributions (either on the input or in the kernel space) for each class and employing class representation by the corresponding class mean vectors. However, there might be other vectors that can be used for classes representation, in order to increase class discrimination in the resulted feature space. In this paper, we propose an optimization scheme aiming at the optimal class representation, in terms of Fisher ratio maximization, for nonlinear data projection. Compared to the standard approach, the proposed optimization scheme increases class discrimination in the reduced-dimensionality feature space and achieves higher classification rates in publicly available data sets. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Kernel Reference Discriminant Analysis", "paper_id": "WOS:000343852400012"}