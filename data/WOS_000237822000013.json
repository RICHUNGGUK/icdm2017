{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "intelligent_music_information_retrieval"}, {"score": 0.040103314250503376, "phrase": "dwch"}, {"score": 0.004780780407878596, "phrase": "efficient_and_intelligent_music_information_retrieval"}, {"score": 0.0045971138571344345, "phrase": "ultimate_goal"}, {"score": 0.00456448259718331, "phrase": "building_personal_music_information_retrieval_systems"}, {"score": 0.0042809854543795255, "phrase": "preeminent_functions"}, {"score": 0.003846926701870193, "phrase": "daubechies_wavelet_coefficient_histograms"}, {"score": 0.0037789720824759503, "phrase": "music_feature_extraction"}, {"score": 0.003752126270661821, "phrase": "music_information_retrieval"}, {"score": 0.0035822005511799036, "phrase": "daubechies_wavelet_filter"}, {"score": 0.0034444213065548688, "phrase": "sound_features"}, {"score": 0.003419943984098415, "phrase": "classification_algorithms"}, {"score": 0.0033475472608077823, "phrase": "tzanetakis_shows"}, {"score": 0.0032883848851259123, "phrase": "timbral_features"}, {"score": 0.003265029780768611, "phrase": "mfcc"}, {"score": 0.0032419142698350014, "phrase": "fft"}, {"score": 0.003161870571621082, "phrase": "multiclass_extensions"}, {"score": 0.0031393948330209224, "phrase": "support_vector_machine"}, {"score": 0.0030293828200315797, "phrase": "significant_improvement"}, {"score": 0.0029971349193857093, "phrase": "previously_known_result"}, {"score": 0.002561658528837694, "phrase": "emotional_labeling"}, {"score": 0.0025343769150179764, "phrase": "adjective_pairs"}, {"score": 0.002359759876402679, "phrase": "semi-supervised_classification_algorithm"}, {"score": 0.002326304140277375, "phrase": "artist_groups"}, {"score": 0.0022933216359142736, "phrase": "similar_artist_lists"}, {"score": 0.00223672097810462, "phrase": "semi-supervised_learning_algorithm"}, {"score": 0.0021049977753042253, "phrase": "proof-of-concept_experiment"}], "paper_keywords": ["clustering", " FFT", " machine learning", " music information retrieval", " wavelet"], "paper_abstract": "Efficient and intelligent music information retrieval is a very important topic of the 21st century. With the ultimate goal of building personal music information retrieval systems, this paper studies the problem of intelligent music information retrieval. Huron [10] points out that since the preeminent functions of music are social and psychological, the most useful characterization would be based on four types of information: genre, emotion, style, and similarity. This paper introduces Daubechies Wavelet Coefficient Histograms (DWCH) for music feature extraction for music information retrieval. The histograms are computed from the coefficients of the db(8) Daubechies wavelet filter applied to 3 s of music. A comparative study of sound features and classification algorithms on a dataset compiled by Tzanetakis shows that combining DWCH with timbral features (MFCC and FFT), with the use of multiclass extensions of support vector machine, achieves approximately 80% of accuracy, which is a significant improvement over the previously known result on this dataset. On another dataset the combination achieves 75% of accuracy. The paper also studies the issue of detecting emotion in music. Rating of two subjects in the three bipolar adjective pairs are used. The accuracy of around 70% was achieved in predicting emotional labeling in these adjective pairs. The paper also studies the problem of identifying groups of artists based on their lyrics and sound using a semi-supervised classification algorithm. Identification of artist groups based on the Similar Artist lists at All Music Guide is attempted. The semi-supervised learning algorithm resulted in nontrivial increases in the accuracy to more than 70%. Finally, the paper conducts a proof-of-concept experiment on similarity search using the feature set.", "paper_title": "Toward intelligent music information retrieval", "paper_id": "WOS:000237822000013"}