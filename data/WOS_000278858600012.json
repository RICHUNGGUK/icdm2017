{"auto_keywords": [{"score": 0.04236408605341747, "phrase": "first_problem"}, {"score": 0.03660909854594217, "phrase": "feature_subsets"}, {"score": 0.030553852119055133, "phrase": "second_problem"}, {"score": 0.012268540499875477, "phrase": "minimal_bayes_risk"}, {"score": 0.009905853205084227, "phrase": "exhaustive_search"}, {"score": 0.00481495049065317, "phrase": "discrete_feature_selection"}, {"score": 0.00475924837336259, "phrase": "optimal_classification"}, {"score": 0.00464976086880886, "phrase": "classification_problem"}, {"score": 0.004464156619275582, "phrase": "random_variables"}, {"score": 0.004386887374401584, "phrase": "prescribed_discrete_sample_space"}, {"score": 0.003927516959113487, "phrase": "feature_subset"}, {"score": 0.0035989687070420977, "phrase": "increasing_ordering"}, {"score": 0.0035366216368204182, "phrase": "bayes_risks"}, {"score": 0.003375574760845742, "phrase": "obvious_monotonicity_constraint"}, {"score": 0.003259604990188875, "phrase": "probability_distribution"}, {"score": 0.00280126189041041, "phrase": "minimal_feature_subset"}, {"score": 0.0022708315711029423, "phrase": "mild_assumptions"}, {"score": 0.002129701089817251, "phrase": "practical_implications"}], "paper_keywords": ["Feature evaluation and selection", " classifier design and evaluation", " machine learning"], "paper_abstract": "Consider a classification problem involving only discrete features that are represented as random variables with some prescribed discrete sample space. In this paper, we study the complexity of two feature selection problems. The first problem consists in finding a feature subset of a given size k that has minimal Bayes risk. We show that for any increasing ordering of the Bayes risks of the feature subsets (consistent with an obvious monotonicity constraint), there exists a probability distribution that exhibits that ordering. This implies that solving the first problem requires an exhaustive search over the feature subsets of size k. The second problem consists of finding the minimal feature subset that has minimal Bayes risk. In the light of the complexity of the first problem, one may think that solving the second problem requires an exhaustive search over all of the feature subsets. We show that, under mild assumptions, this is not true. We also study the practical implications of our solutions to the second problem.", "paper_title": "On the Complexity of Discrete Feature Selection for Optimal Classification", "paper_id": "WOS:000278858600012"}