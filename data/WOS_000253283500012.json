{"auto_keywords": [{"score": 0.04747663842253327, "phrase": "adaboost"}, {"score": 0.04174572010371398, "phrase": "local_error"}, {"score": 0.040638144135625165, "phrase": "training_instance"}, {"score": 0.00481495049065317, "phrase": "classification_problems"}, {"score": 0.0046046429665581555, "phrase": "-resampling_version"}, {"score": 0.004330321475256457, "phrase": "classification_tasks"}, {"score": 0.0034245614958572012, "phrase": "next_classifier's_training_set"}, {"score": 0.0033301694494391643, "phrase": "novel_instance"}, {"score": 0.0032747835779400212, "phrase": "similarity_information"}, {"score": 0.0028316582436089064, "phrase": "training_instances"}, {"score": 0.0025892330566358503, "phrase": "synthetic_and_several_benchmark_real-world_data_sets"}, {"score": 0.00253193979630071, "phrase": "uci"}, {"score": 0.0024620842899061614, "phrase": "proposed_method"}, {"score": 0.0024211009463218484, "phrase": "prediction_accuracy"}, {"score": 0.002354302193867031, "phrase": "classification_noise"}, {"score": 0.002263861731320045, "phrase": "diversity-accuracy_patterns"}, {"score": 0.0022261705957258506, "phrase": "ensemble_classifiers"}, {"score": 0.002176887956041267, "phrase": "kappa-error_diagrams"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["adaboost", " local boosting", " weak learning algorithm", " classification noise", " kappa-error diagram"], "paper_abstract": "Based on the boosting-by-resampling version of Adaboost, a local boosting algorithm for dealing with classification tasks is proposed in this paper. Its main idea is that in each iteration, a local error is calculated for every training instance and a function of this local error is utilized to update the probability that the instance is selected to be part of next classifier's training set. When classifying a novel instance, the similarity information between it and each training instance is taken into account. Meanwhile, a parameter is introduced into the process of updating the probabilities assigned to training instances so that the algorithm can be more accurate than Adaboost. The experimental results on synthetic and several benchmark real-world data sets available from the UCI repository show that the proposed method improves the prediction accuracy and the robustness to classification noise of Adaboost. Furthermore, the diversity-accuracy patterns of the ensemble classifiers are investigated by kappa-error diagrams. (C) 2007 Elsevier B.V. All rights reserved.", "paper_title": "A local boosting algorithm for solving classification problems", "paper_id": "WOS:000253283500012"}