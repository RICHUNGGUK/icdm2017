{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "evaluation_data"}, {"score": 0.004658426838467567, "phrase": "interactive_question_answering_systems"}, {"score": 0.0045443671106012405, "phrase": "interactive_question"}, {"score": 0.00418387341007444, "phrase": "traditional_evaluation_measures"}, {"score": 0.003757480190200645, "phrase": "relevance_judgments"}, {"score": 0.0036052633308076933, "phrase": "multi-user_evaluations"}, {"score": 0.0021049977753042253, "phrase": "initial_support"}], "paper_keywords": [""], "paper_abstract": "Evaluating interactive question answering (QA) systems with real users can be challenging because traditional evaluation measures based on the relevance of items returned are difficult to employ since relevance judgments can be unstable in multi-user evaluations. The work reported in this paper evaluates, in distinguishing among a set of interactive QA systems, the effectiveness of three questionnaires: a Cognitive Workload Questionnaire (NASA TLX), and Task and System Questionnaires customized to a specific interactive QA application. These Questionnaires were evaluated with four systems, seven analysts, and eight scenarios during a 2-week workshop. Overall, results demonstrate that all three Questionnaires are effective at distinguishing among systems, with the Task Questionnaire being the most sensitive. Results also provide initial support for the validity and reliability of the Questionnaires.", "paper_title": "Questionnaires for eliciting evaluation data from users of interactive question answering systems", "paper_id": "WOS:000278274100007"}