{"auto_keywords": [{"score": 0.04100887100723609, "phrase": "neural_circuitry"}, {"score": 0.03541239344095569, "phrase": "physical_robot"}, {"score": 0.035050648308006416, "phrase": "real-world_environment"}, {"score": 0.004815316573636744, "phrase": "gpu"}, {"score": 0.004680851157467745, "phrase": "visually_guided_robot_navigation"}, {"score": 0.004534428070512318, "phrase": "novel_cluttered_environments"}, {"score": 0.004502513902377422, "phrase": "apparent_ease"}, {"score": 0.004330941610682453, "phrase": "behavioral_dynamics"}, {"score": 0.003800215925586887, "phrase": "self-motion_variables"}, {"score": 0.003746871277361436, "phrase": "current_direction"}, {"score": 0.0035408501923902477, "phrase": "active_steering_control"}, {"score": 0.003466536738232506, "phrase": "cortical_neural_network_model"}, {"score": 0.0034421126037849, "phrase": "visually_guided_navigation"}, {"score": 0.0032527939106509946, "phrase": "rate_based_motion_energy_model"}, {"score": 0.0031732652552570644, "phrase": "spiking_neural_network_model"}, {"score": 0.0031287080713373783, "phrase": "mt."}, {"score": 0.0030738557655525913, "phrase": "cortical_representation"}, {"score": 0.0030521898646210413, "phrase": "optic_flow"}, {"score": 0.0029565620771207003, "phrase": "motion_discontinuities"}, {"score": 0.0028437313714204087, "phrase": "goal_location"}, {"score": 0.00281371157349156, "phrase": "motor_commands"}, {"score": 0.0026682872124177233, "phrase": "robot_trajectories"}, {"score": 0.0026307897899760383, "phrase": "human_behavioral_data"}, {"score": 0.002575526825728026, "phrase": "neural_signals"}, {"score": 0.002530359917257626, "phrase": "cortical_area"}, {"score": 0.002521531010158969, "phrase": "mt"}, {"score": 0.0024947959375982614, "phrase": "sufficient_motion_information"}, {"score": 0.0024423826953133844, "phrase": "human-like_paths"}, {"score": 0.00225144317399471, "phrase": "underlying_model"}, {"score": 0.0022355610034493225, "phrase": "brain_function"}, {"score": 0.0021885820862000962, "phrase": "anatomical_constraints"}, {"score": 0.0021654633404434623, "phrase": "physical_body"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Obstacle avoidance", " Spiking neural network", " Robot navigation", " Motion energy", " MT", " GPU"], "paper_abstract": "Humans and other terrestrial animals use vision to traverse novel cluttered environments with apparent ease. On one hand, although much is known about the behavioral dynamics of steering in humans, it remains unclear how relevant perceptual variables might be represented in the brain. On the other hand, although a wealth of data exists about the neural circuitry that is concerned with the perception of self-motion variables such as the current direction of travel, little research has been devoted to investigating how this neural circuitry may relate to active steering control. Here we present a cortical neural network model for visually guided navigation that has been embodied on a physical robot exploring a real-world environment. The model includes a rate based motion energy model for area V1, and a spiking neural network model for cortical area MT. The model generates a cortical representation of optic flow, determines the position of objects based on motion discontinuities, and combines these signals with the representation of a goal location to produce motor commands that successfully steer the robot around obstacles toward the goal. The model produces robot trajectories that closely match human behavioral data. This study demonstrates how neural signals in a model of cortical area MT might provide sufficient motion information to steer a physical robot on human-like paths around obstacles in a real-world environment, and exemplifies the importance of embodiment, as behavior is deeply coupled not only with the underlying model of brain function, but also with the anatomical constraints of the physical body it controls. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "A GPU-accelerated cortical neural network model for visually guided robot navigation", "paper_id": "WOS:000366701700007"}