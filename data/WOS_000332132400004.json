{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "supervised_learning"}, {"score": 0.034887398305923065, "phrase": "differential_operators"}, {"score": 0.004191377834552202, "phrase": "input_space"}, {"score": 0.004095567486538121, "phrase": "limit_case"}, {"score": 0.003978866233891427, "phrase": "single_points"}, {"score": 0.003910441483033426, "phrase": "proposed_approach"}, {"score": 0.0037989951856280423, "phrase": "classical_learning_context"}, {"score": 0.0037336520904376687, "phrase": "adopted_framework"}, {"score": 0.003503497128638099, "phrase": "loss_function"}, {"score": 0.0033839745649214548, "phrase": "additive_regularization_term"}, {"score": 0.0032309093154028663, "phrase": "smoothness_properties"}, {"score": 0.003138768436081888, "phrase": "representer_theorems"}, {"score": 0.0030141556259370675, "phrase": "optimization_problem"}, {"score": 0.0029281780903782284, "phrase": "labeled_regions"}, {"score": 0.002877769867854964, "phrase": "unique_solution"}, {"score": 0.00273167822885383, "phrase": "linear_combination"}, {"score": 0.00270023138867066, "phrase": "kernel_functions"}, {"score": 0.0025044454602720597, "phrase": "relevant_situation"}, {"score": 0.002391067964886602, "phrase": "multi-dimensional_intervals"}, {"score": 0.0022176487309947266, "phrase": "prior_knowledge"}, {"score": 0.0021794454466797382, "phrase": "logical_propositions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Supervised learning", " Kernel machines", " Propositional rules", " Variational calculus", " Infinite-dimensional optimization", " Representer theorems"], "paper_abstract": "Supervised learning is investigated, when the data are represented not only by labeled points but also labeled regions of the input space. In the limit case, such regions degenerate to single points and the proposed approach changes back to the classical learning context. The adopted framework entails the minimization of a functional obtained by introducing a loss function that involves such regions. An additive regularization term is expressed via differential operators that model the smoothness properties of the desired input/output relationship. Representer theorems are given, proving that the optimization problem associated to learning from labeled regions has a unique solution, which takes on the form of a linear combination of kernel functions determined by the differential operators together with the regions themselves. As a relevant situation, the case of regions given by multi-dimensional intervals (i.e., \"boxes\") is investigated, which models prior knowledge expressed by logical propositions. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "A theoretical framework for supervised learning from regions", "paper_id": "WOS:000332132400004"}