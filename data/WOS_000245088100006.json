{"auto_keywords": [{"score": 0.025595744903613434, "phrase": "jewel"}, {"score": 0.00481495049065317, "phrase": "speech-driven_talking_face"}, {"score": 0.0045709675813472884, "phrase": "articulatory_modelling_approach"}, {"score": 0.004492406361795448, "phrase": "acoustic_speech"}, {"score": 0.004440780432053927, "phrase": "realistic_mouth_animation"}, {"score": 0.003933118499284644, "phrase": "dynamic_bayesian_network"}, {"score": 0.0036482665232275583, "phrase": "multiple-stream_structure"}, {"score": 0.0035855066449812273, "phrase": "shared_articulator_layer"}, {"score": 0.0030492472433481764, "phrase": "visual_articulatory_movements"}, {"score": 0.0030141556259370675, "phrase": "audio_speech"}, {"score": 0.002911278281894321, "phrase": "linguistic_fact"}, {"score": 0.002877769867854964, "phrase": "different_articulators"}, {"score": 0.0027475383418803724, "phrase": "baum-welch_dbn_inversion"}, {"score": 0.0026384167183586015, "phrase": "optimal_facial_parameters"}, {"score": 0.0025631294247072476, "phrase": "trained_avam"}, {"score": 0.0025336179740814905, "phrase": "maximum_likelihood"}, {"score": 0.002447101794397243, "phrase": "extensive_objective_and_subjective_evaluations"}, {"score": 0.0022828113789751694, "phrase": "phonemic_hmm_approaches"}, {"score": 0.0022565201625076876, "phrase": "facial_parameters"}, {"score": 0.0021794454466797382, "phrase": "true_parameters"}, {"score": 0.0021049977753042253, "phrase": "synthesized_facial_animation_sequences"}], "paper_keywords": ["articulatory model", " Baum-Welch DBN inversion (DBNI)", " dynamic Bayesian networks (DBNs)", " facial animation", " mouth-synching", " talking face"], "paper_abstract": "This paper presents an articulatory modelling approach to convert acoustic speech into realistic mouth animation. We directly model the movements of articulators, such as lips, tongue, and teeth, using a dynamic Bayesian network (DBN)-based audio-visual articulatory model (AVAM). A multiple-stream structure with a shared articulator layer is adopted in the model to synchronously associate the two building blocks of speech, i.e., audio and video. This model not only describes the synchronization between visual articulatory movements and audio speech, but also reflects the linguistic fact that different articulators evolve asynchronously. We also present a Baum-Welch DBN inversion (DBNI) algorithm to generate optimal facial parameters from audio given the trained AVAM under maximum likelihood (ML) criterion. Extensive objective and subjective evaluations on the JEWEL audio-visual dataset demonstrate that compared with phonemic HMM approaches, facial parameters estimated by our approach follow the true parameters more accurately, and the synthesized facial animation sequences are so lively that 38% of them are undistinguishable.", "paper_title": "Realistic mouth-synching for speech-driven talking face using articulatory modelling", "paper_id": "WOS:000245088100006"}