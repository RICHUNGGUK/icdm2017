{"auto_keywords": [{"score": 0.048201896565263386, "phrase": "concept_learning"}, {"score": 0.026626575420999866, "phrase": "concept_change"}, {"score": 0.00481495049065317, "phrase": "unknown_dynamics"}, {"score": 0.004585332915477379, "phrase": "target_concepts"}, {"score": 0.004484596684703308, "phrase": "short-term_changes"}, {"score": 0.00436661740263741, "phrase": "long-term_changes"}, {"score": 0.0035590664550919854, "phrase": "target_concept"}, {"score": 0.0034807976670686628, "phrase": "incremental_decision_tree"}, {"score": 0.003404244237372396, "phrase": "incoming_examples"}, {"score": 0.003314591878914152, "phrase": "decision_tree"}, {"score": 0.0032706520949775065, "phrase": "time_window"}, {"score": 0.0031562971707460802, "phrase": "main_parameter"}, {"score": 0.002874726518572154, "phrase": "learning_algorithm"}, {"score": 0.0027131212233443137, "phrase": "current_dynamics"}, {"score": 0.0026771333996253783, "phrase": "data_stream"}, {"score": 0.002427372728532664, "phrase": "evaluated_methods"}, {"score": 0.0023633857274905977, "phrase": "concept_drift"}, {"score": 0.002250415129116109, "phrase": "different_speeds"}, {"score": 0.0021333110194180997, "phrase": "different_areas"}, {"score": 0.0021049977753042253, "phrase": "problem_domain"}], "paper_keywords": ["incremental algorithms", " online learning", " concept drift", " decision trees", " robust learners"], "paper_abstract": "In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the field of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain ( virtual drift).", "paper_title": "Learning in environments with unknown dynamics: Towards more robust concept learners", "paper_id": "WOS:000252744900003"}