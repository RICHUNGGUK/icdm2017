{"auto_keywords": [{"score": 0.042070971187967834, "phrase": "scaling_cut_criterion"}, {"score": 0.029898139696794914, "phrase": "cut_criterion"}, {"score": 0.027140291257134692, "phrase": "optimal_dimension"}, {"score": 0.00481495049065317, "phrase": "criterion-based_discriminant_analysis"}, {"score": 0.004658426838467567, "phrase": "dimension_reduction"}, {"score": 0.0045318666714939905, "phrase": "major_problem"}, {"score": 0.00443308765153716, "phrase": "machine_learning"}, {"score": 0.004384504920139083, "phrase": "pattern_recognition"}, {"score": 0.004218602458288531, "phrase": "scaling_cut"}, {"score": 0.004195418010131832, "phrase": "criterion-based_supervised_dimension_reduction_methods"}, {"score": 0.003820144626025562, "phrase": "data_distribution"}, {"score": 0.003716273882758735, "phrase": "homoscedastic_gaussian"}, {"score": 0.003536346954940084, "phrase": "computational_complexity"}, {"score": 0.003459191419478298, "phrase": "cut_criterion-based_dimension_reduction"}, {"score": 0.003328184259386487, "phrase": "localization_strategy"}, {"score": 0.00327356326590461, "phrase": "input_data"}, {"score": 0.0032198357970567595, "phrase": "localized_-nearest_neighbor_graph"}, {"score": 0.002996992362027833, "phrase": "between-class_margin"}, {"score": 0.002669042572650594, "phrase": "nonlinear_variability"}, {"score": 0.002363806685815014, "phrase": "dimension_reduction_methods"}, {"score": 0.002163930783583212, "phrase": "better_and_efficient_performance"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_dimension_reduction_techniques"}], "paper_keywords": ["Dimension reduction", " Scaling cut criterion", " Local scaling cut criterion", " Kernel scaling cut criterion", " Local kernel scaling cut criterion", " Optimal dimension scaling cut criterion"], "paper_abstract": "Dimension reduction has always been a major problem in many applications of machine learning and pattern recognition. In this paper, the scaling cut criterion-based supervised dimension reduction methods for data analysis are proposed. The scaling cut criterion can eliminate the limit of the hypothesis that data distribution of each class is homoscedastic Gaussian. To obtain a more reasonable mapping matrix and reduce the computational complexity, local scaling cut criterion-based dimension reduction is raised, which utilized the localization strategy of the input data. The localized -nearest neighbor graph is introduced , which relaxes the within-class variance and enlarges the between-class margin. Moreover, by kernelizing the scaling cut criterion and local scaling cut criterion, both methods are extended to efficiently model the nonlinear variability of the data. Furthermore, the optimal dimension scaling cut criterion is proposed, which can automatically select the optimal dimension for the dimension reduction methods. The approaches have been tested on several datasets, and the results have shown a better and efficient performance compared with other linear and nonlinear dimension reduction techniques.", "paper_title": "Scaling cut criterion-based discriminant analysis for supervised dimension reduction", "paper_id": "WOS:000354197500006"}