{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "human_action_detection"}, {"score": 0.0483556980899456, "phrase": "human-level_synthetic_intelligence"}, {"score": 0.04241767252094026, "phrase": "low-level_features"}, {"score": 0.004580243025274209, "phrase": "novel_approach"}, {"score": 0.0045196012587080114, "phrase": "associative_memory_model"}, {"score": 0.00448958076531243, "phrase": "generalized_hough_transform"}, {"score": 0.004327981760540394, "phrase": "classification_system"}, {"score": 0.004186105377259855, "phrase": "proposed_ght-based_associative_memory_model"}, {"score": 0.004075946554498088, "phrase": "machine_learning_architectures"}, {"score": 0.003995226376780718, "phrase": "specific_human_action"}, {"score": 0.003942299471749811, "phrase": "existing_machine_learning_architectures"}, {"score": 0.003903063128339768, "phrase": "restructuring_capability"}, {"score": 0.0038385317554884713, "phrase": "important_process"}, {"score": 0.00378767277402891, "phrase": "conceptual_structures"}, {"score": 0.0035789061301304977, "phrase": "existing_human_action_recognition_algorithms"}, {"score": 0.003543273649210823, "phrase": "spatial_temporal_boundaries"}, {"score": 0.0035197152758635344, "phrase": "action_objects"}, {"score": 0.003438483411236772, "phrase": "side_effect"}, {"score": 0.003415619374740772, "phrase": "temporal_ambiguity"}, {"score": 0.003381607100127276, "phrase": "proposed_system"}, {"score": 0.0033479323773364716, "phrase": "preprocessing_procedure"}, {"score": 0.0032815823097599316, "phrase": "video_sequence"}, {"score": 0.003238078694483823, "phrase": "compact_representation"}, {"score": 0.003173898798475791, "phrase": "image_and_motion_features"}, {"score": 0.0030290342352603124, "phrase": "appearance_motion"}, {"score": 0.002988868585379496, "phrase": "training_procedure"}, {"score": 0.0029394074636168435, "phrase": "learnt_codebook"}, {"score": 0.0029198522863606953, "phrase": "ght"}, {"score": 0.002861961643289598, "phrase": "associative_memory_learning"}, {"score": 0.002795867549039675, "phrase": "test_video_clip"}, {"score": 0.0027680095380207734, "phrase": "hough_voting_framework"}, {"score": 0.0027131212233443137, "phrase": "salient_segments"}, {"score": 0.0026504553106526016, "phrase": "multiple_patches"}, {"score": 0.0025978919678702793, "phrase": "similar_appearance"}, {"score": 0.002529421374791293, "phrase": "detected_patches"}, {"score": 0.0024792523879679815, "phrase": "associative_memory"}, {"score": 0.0024545413637350765, "phrase": "missing_patches"}, {"score": 0.002405853977978678, "phrase": "whole_action_object"}, {"score": 0.0023268399760599336, "phrase": "target_action_object"}, {"score": 0.002295964130868963, "phrase": "action_type"}, {"score": 0.002265497060260791, "phrase": "probabilistic_hough_voting_scheme"}, {"score": 0.002220551141101661, "phrase": "proposed_method"}, {"score": 0.0022057677637040396, "phrase": "good_performance"}, {"score": 0.0021620042653528846, "phrase": "detection_accuracy"}, {"score": 0.0021476098263617954, "phrase": "recognition_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Action object shapes", " Generalized Hough transform", " Associative memory", " Hypergraph", " Human action detection and recognition"], "paper_abstract": "This paper, investigating the goal of human-level synthetic intelligence, presents a novel approach to learning an associative memory model using Generalized Hough Transform (GHT) [1]. A human action detection and classification system is also constructed to verify the effectiveness of the proposed GHT-based associative memory model. Existing human action classification systems use machine learning architectures and low-level features to characterize a specific human action. However, existing machine learning architectures often lack restructuring capability, which is an important process of forming the conceptual structures in human-level synthetic intelligence. The gap between low-level features and high-level human intelligence also degrades the performance of existing human action recognition algorithms when the spatial temporal boundaries of action objects are ambiguous. To eliminate the side effect of temporal ambiguity, the proposed system uses a preprocessing procedure to extract key-frames from a video sequence and provide a compact representation for this video. The image and motion features of patches extracted from each key-frame are collected and used to train an appearance motion codebook. The training procedure, based on the learnt codebook and GHT, constructs a hypergraph for associative memory learning. For each key-frame of a test video clip, the Hough voting framework is also used to detect salient segments, which are further partitioned into multiple patches, by grouping blocks of similar appearance and motions. The features of the detected patches are used to query the associative memory and retrieve missing patches from key-frames to recall the whole action object. These patches are then used to locate the target action object and classify the action type simultaneously using a probabilistic Hough voting scheme. Results show that the proposed method gives good performance on several publicly available datasets in terms of detection accuracy and recognition rate. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "GHT-based associative memory learning and its application to Human action detection and classification", "paper_id": "WOS:000321232900020"}