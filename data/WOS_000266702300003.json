{"auto_keywords": [{"score": 0.029191698423245326, "phrase": "ag-art_architectures"}, {"score": 0.013770894024797314, "phrase": "category_proliferation_problem"}, {"score": 0.00481495049065317, "phrase": "ag-art"}, {"score": 0.0047007816199415905, "phrase": "art_architectures"}, {"score": 0.004605069682942661, "phrase": "classification_problems"}, {"score": 0.004465126965932753, "phrase": "artmap_architectures"}, {"score": 0.004434608130268514, "phrase": "genetic_algorithms"}, {"score": 0.004329418448610183, "phrase": "generalization_performance"}, {"score": 0.004270429323002124, "phrase": "adaptive_resonance_theory"}, {"score": 0.0041548413176193235, "phrase": "previous_effort"}, {"score": 0.004098221047046135, "phrase": "evolutionary_fuzzy_artmap"}, {"score": 0.003987275549870311, "phrase": "genetic_fuzzy_artmap"}, {"score": 0.003839594092937135, "phrase": "improved_genetic_algorithm"}, {"score": 0.0038133400238856896, "phrase": "fam"}, {"score": 0.003523907167337646, "phrase": "major_advantages"}, {"score": 0.0034878064529259424, "phrase": "proposed_improved_genetic_algorithm"}, {"score": 0.0034167069467881143, "phrase": "ca_parameters"}, {"score": 0.0032452142903668666, "phrase": "classification_problem"}, {"score": 0.0031899784784921222, "phrase": "resulting_genetically_engineered_art_architectures"}, {"score": 0.002790103629701475, "phrase": "computational_cost"}, {"score": 0.002733187900351625, "phrase": "gfam"}, {"score": 0.0025342267883803255, "phrase": "better_performance"}, {"score": 0.002423519259896573, "phrase": "gfam."}, {"score": 0.002390436017272046, "phrase": "ag-art's_performance"}, {"score": 0.002325615071949044, "phrase": "classification_literature"}, {"score": 0.0022703369939978185, "phrase": "competitive_generalization_performance"}, {"score": 0.0022163699144426155, "phrase": "smaller_size_classifiers"}, {"score": 0.002141487053045766, "phrase": "ag-art's_performance_gains"}, {"score": 0.0021049977753042253, "phrase": "reasonable_computational_budget"}], "paper_keywords": ["Machine learning", " Classification", " ARTMAP", " Genetic algorithms", " Genetic operators", " Category proliferation"], "paper_abstract": "This paper focuses on classification problems, and in particular on the evolution of ARTMAP architectures using genetic algorithms, with the objective of improving generalization performance and alleviating the adaptive resonance theory (ART) category proliferation problem. In a previous effort, we introduced evolutionary fuzzy ARTMAP (FAM), referred to as genetic Fuzzy ARTMAP (GFAM). In this paper we apply an improved genetic algorithm to FAM and extend these ideas to two other ART architectures; ellipsoidal ARTMAP (EAM) and Gaussian ARTMAP (CAM). One of the major advantages of the proposed improved genetic algorithm is that it adapts the CA parameters automatically, and in a way that takes into consideration the intricacies of the classification problem under consideration. The resulting genetically engineered ART architectures are justifiably referred to as AG-FAM, AG-EAM and AG-GAM or collectively as AG-ART (adaptive genetically engineered ART). We compare the performance (in terms of accuracy, size, and computational cost) of the AG-ART architectures with GFAM, and other ART architectures that have appeared in the literature and attempted to solve the category proliferation problem. Our results demonstrate that AG-ART architectures exhibit better performance than their other ART counterparts (semi-supervised ART) and better performance than GFAM. We also compare AG-ART's performance to other related results published in the classification literature, and demonstrate that AG-ART architectures exhibit competitive generalization performance and, quite often, produce smaller size classifiers in solving the same classification problems. We also show that AG-ART's performance gains are achieved within a reasonable computational budget. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "AG-ART: An adaptive approach to evolving ART architectures", "paper_id": "WOS:000266702300003"}