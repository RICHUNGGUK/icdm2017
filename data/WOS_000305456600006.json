{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "structured_sparsity"}, {"score": 0.004761494044422926, "phrase": "alternating_direction_methods"}, {"score": 0.004579005794486596, "phrase": "sparse_learning_problems"}, {"score": 0.004528156929805637, "phrase": "high_dimensional_feature_space"}, {"score": 0.004428139418750855, "phrase": "structured_sparsity-inducing_norm"}, {"score": 0.004258372598568333, "phrase": "group_structure"}, {"score": 0.004049590638590415, "phrase": "considerable_challenge"}, {"score": 0.00400459716272594, "phrase": "optimization_algorithms"}, {"score": 0.003808209866576388, "phrase": "regularization_term"}, {"score": 0.003541357671562449, "phrase": "group_lasso"}, {"score": 0.003348838095597268, "phrase": "unified_framework"}, {"score": 0.0032747835779400212, "phrase": "augmented_lagrangian_method"}, {"score": 0.002911961514073769, "phrase": "core_building-blocks"}, {"score": 0.002800158709933889, "phrase": "new_algorithms"}, {"score": 0.002677616534005393, "phrase": "accelerated_versions"}, {"score": 0.0025178072071268534, "phrase": "epsilon-optimal_solution"}, {"score": 0.002341164727388944, "phrase": "alternating_direction"}, {"score": 0.0023151088048303705, "phrase": "lagrangian_and_fista_methods"}, {"score": 0.002251227801482381, "phrase": "data_sets"}, {"score": 0.0021406417439887907, "phrase": "relative_merits"}], "paper_keywords": ["structured sparsity", " overlapping Group Lasso", " alternating direction methods", " variable splitting", " augmented Lagrangian"], "paper_abstract": "We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l(1)/l(2)-norm and the l(1)/l(infinity)-norm. We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efficiently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions of these algorithms require O(1/root epsilon) iterations to obtain an epsilon-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms.", "paper_title": "Structured Sparsity via Alternating Direction Methods", "paper_id": "WOS:000305456600006"}