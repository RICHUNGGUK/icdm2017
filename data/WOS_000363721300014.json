{"auto_keywords": [{"score": 0.047446295972823384, "phrase": "recurrent_neural_network"}, {"score": 0.03923307184680597, "phrase": "convergence_time"}, {"score": 0.015719716506582538, "phrase": "finite-time_stability"}, {"score": 0.015074435841361757, "phrase": "time-varying_sylvester_equation"}, {"score": 0.01039883858137025, "phrase": "upper_bound"}, {"score": 0.003950846696972114, "phrase": "new_finite-time_stability_criterion"}, {"score": 0.0035283552859817764, "phrase": "sign-bi-power_activation_function"}, {"score": 0.002853808773854451, "phrase": "tunable_activation_function"}, {"score": 0.0021350169717509714, "phrase": "theoretical_analysis"}, {"score": 0.0021049977753042253, "phrase": "numerical_simulations"}], "paper_keywords": ["Finite-time stability", " The convergence time", " conservative", " Sylvester equation", " Recurrent neural network"], "paper_abstract": "This paper investigates finite-time stability and its application for solving time-varying Sylvester equation by recurrent neural network. Firstly, a new finite-time stability criterion is given and a less conservative upper bound of the convergence time is also derived. Secondly, a sign-bi-power activation function with a linear term is presented for the recurrent neural network. The estimation of the upper bound of the convergence time is more less conservative. Thirdly, it is proposed a tunable activation function with three tunable positive parameters for the recurrent neural network. These parameters are not only helpful to reduce conservatism of the upper bound of the convergence time, accelerate convergence but also reduce sensitivity to additive noise. The effectiveness of our methods is shown by both theoretical analysis and numerical simulations.", "paper_title": "Finite-Time Stability and Its Application for Solving Time-Varying Sylvester Equation by Recurrent Neural Network", "paper_id": "WOS:000363721300014"}