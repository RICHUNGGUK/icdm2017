{"auto_keywords": [{"score": 0.03572270209581421, "phrase": "rl"}, {"score": 0.015719434236122657, "phrase": "reinforcement_learning"}, {"score": 0.015559687685868052, "phrase": "game_theory"}, {"score": 0.010651717511501098, "phrase": "new_direction"}, {"score": 0.0043875257506180865, "phrase": "major_technique"}, {"score": 0.004342430078306461, "phrase": "adaptive_optimal_control"}, {"score": 0.004297795905522757, "phrase": "non-linear_systems"}, {"score": 0.004145143433523008, "phrase": "rl_algorithms"}, {"score": 0.0040186038781533946, "phrase": "strong_constraint"}, {"score": 0.003916098498242888, "phrase": "environment_dynamics"}, {"score": 0.003757480190200645, "phrase": "markov_decision_process"}, {"score": 0.0036427316134070007, "phrase": "mdp_framework"}, {"score": 0.0035866734383189366, "phrase": "single_agent_operating"}, {"score": 0.003531474887552889, "phrase": "stationary_environment"}, {"score": 0.0031354779851351287, "phrase": "markov_games"}, {"score": 0.0030872017736888113, "phrase": "alternative_system_model"}, {"score": 0.0029467742734417255, "phrase": "rl_based_approaches"}, {"score": 0.002755104031926742, "phrase": "broad_areas"}, {"score": 0.0026297428083782875, "phrase": "interesting_and_challenging_avenue"}, {"score": 0.0025892330566358503, "phrase": "intelligent_and_reliable_controllers"}, {"score": 0.0024842242921857705, "phrase": "representative_rl_algorithms"}, {"score": 0.002358917661465133, "phrase": "recent_direction"}, {"score": 0.0022283519468680475, "phrase": "open_issues"}, {"score": 0.0021826828321952615, "phrase": "future_research_directions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Game theory", " Markov games", " Markov game based RL control"], "paper_abstract": "Reinforcement learning (RL) has now evolved as a major technique for adaptive optimal control of non-linear systems. However, majority of the RL algorithms proposed so far impose a strong constraint on the structure of environment dynamics by assuming that it operates as a Markov decision process (MDP). An MDP framework envisages a single agent operating in a stationary environment thereby limiting the scope of application of RL to control problems. Recently, a new direction of research has focused on proposing Markov games as an alternative system model to enhance the generality and robustness of the RL based approaches. This paper aims to present this new direction that seeks to synergize broad areas of RL and Game theory, as an interesting and challenging avenue for designing intelligent and reliable controllers. First, we briefly review some representative RL algorithms for the sake of completeness and then describe the recent direction that seeks to integrate RL and game theory. Finally, open issues are identified and future research directions outlined. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Synergizing reinforcement learning and game theory-A new direction for control", "paper_id": "WOS:000275377500002"}