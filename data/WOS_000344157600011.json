{"auto_keywords": [{"score": 0.003976978140059542, "phrase": "participants'_own_resources"}, {"score": 0.003284361433827464, "phrase": "distributed_hash_table_abstraction"}, {"score": 0.0031649018957020337, "phrase": "load_balance"}, {"score": 0.0030876784650745973, "phrase": "subscription_delegation"}, {"score": 0.0030309961956273028, "phrase": "overloaded_peers"}, {"score": 0.0029753513692467315, "phrase": "bottom-up_tree_search_technique"}, {"score": 0.002831894638104635, "phrase": "fault_tolerance"}, {"score": 0.0027457584014804574, "phrase": "lightweight_replication_scheme"}, {"score": 0.0025972447844047515, "phrase": "experimental_results"}, {"score": 0.002518227743688769, "phrase": "fault-tolerance_properties"}, {"score": 0.0022811346113905295, "phrase": "internal_system_messages"}, {"score": 0.002239225269052105, "phrase": "even_the_simultaneous_failure"}], "paper_keywords": ["Content-based publish/subscribe", " DHT", " distributed hash table", " P2P", " peer-to-peer", " pub/sub"], "paper_abstract": "Peer-to-peer (P2P) networks can offer benefits to distributed content-based publish/subscribe data dissemination systems. In particular, since a P2P network's aggregate resources grow as the number of participants increases, scalability can be achieved using no infrastructure other than the participants' own resources. This paper proposes algorithms for supporting content-based publish/subscribe in which subscriptions can specify a range of interest and publications a range of values. The algorithms are built over a distributed hash table abstraction and are completely decentralized. Load balance is addressed by subscription delegation away from overloaded peers and a bottom-up tree search technique that avoids root hotspots. Furthermore, fault tolerance is achieved with a lightweight replication scheme that quickly detects and recovers from faults. Experimental results support the scalability and fault-tolerance properties of the algorithms: For example, doubling the number of subscriptions does not double internal system messages, and even the simultaneous failure of 20% of the peers in the system requires less than 2 min to fully recover.", "paper_title": "Infrastructure-Free Content-Based Publish/Subscribe", "paper_id": "WOS:000344157600011"}