{"auto_keywords": [{"score": 0.04346060492379688, "phrase": "unified_elm"}, {"score": 0.03549931087351771, "phrase": "sparse_elm"}, {"score": 0.007775708938718487, "phrase": "testing_time"}, {"score": 0.00481495049065317, "phrase": "sparse_extreme_learning_machine"}, {"score": 0.004736434518535051, "phrase": "extreme_learning_machine"}, {"score": 0.004583205112737672, "phrase": "single-hidden-layer_feedforward_neural_networks"}, {"score": 0.00445318311339027, "phrase": "hidden_layer"}, {"score": 0.004238775572746022, "phrase": "training_data"}, {"score": 0.00405127920820053, "phrase": "single_framework"}, {"score": 0.003968806706727518, "phrase": "different_learning_methods"}, {"score": 0.003904034809926225, "phrase": "slfns"}, {"score": 0.0038720442355114045, "phrase": "least_square_support_vector_machines"}, {"score": 0.0038403151693333017, "phrase": "proximal_support_vector_machines"}, {"score": 0.003464890914694523, "phrase": "large-scale_applications"}, {"score": 0.0032979090806670493, "phrase": "alternative_solution"}, {"score": 0.003230723298995081, "phrase": "storage_space"}, {"score": 0.003074991770768925, "phrase": "matrix_inversion"}, {"score": 0.00292674497467906, "phrase": "training_size"}, {"score": 0.002843581231499026, "phrase": "training_time"}, {"score": 0.0028202559386070873, "phrase": "large-scale_problems"}, {"score": 0.0026403934420630155, "phrase": "efficient_training_algorithm"}, {"score": 0.0025865679621853667, "phrase": "sparse_elm."}, {"score": 0.002565345470805401, "phrase": "quadratic_programming_problem"}, {"score": 0.002461810159687652, "phrase": "smallest_possible_sub-problems"}, {"score": 0.002352778833267701, "phrase": "svm"}, {"score": 0.002314271047073693, "phrase": "better_generalization_performance"}, {"score": 0.002220846338732873, "phrase": "similar_generalization_performance"}, {"score": 0.002202617924488607, "phrase": "binary_classification_applications"}, {"score": 0.0021488235554917128, "phrase": "large-scale_binary_classification_problems"}, {"score": 0.0021049977753042253, "phrase": "even_faster_training_speed"}], "paper_keywords": ["Classification", " extreme learning machine (ELM)", " quadratic programming (QP)", " sequential minimal optimization (SMO)", " sparse ELM", " support vector machine (SVM)", " unified ELM"], "paper_abstract": "Extreme learning machine (ELM) was initially proposed for single-hidden-layer feedforward neural networks (SLFNs). In the hidden layer (feature mapping), nodes are randomly generated independently of training data. Furthermore, a unified ELM was proposed, providing a single framework to simplify and unify different learning methods, such as SLFNs, least square support vector machines, proximal support vector machines, and so on. However, the solution of unified ELM is dense, and thus, usually plenty of storage space and testing time are required for large-scale applications. In this paper, a sparse ELM is proposed as an alternative solution for classification, reducing storage space and testing time. In addition, unified ELM obtains the solution by matrix inversion, whose computational complexity is between quadratic and cubic with respect to the training size. It still requires plenty of training time for large-scale problems, even though it is much faster than many other traditional methods. In this paper, an efficient training algorithm is specifically developed for sparse ELM. The quadratic programming problem involved in sparse ELM is divided into a series of smallest possible sub-problems, each of which are solved analytically. Compared with SVM, sparse ELM obtains better generalization performance with much faster training speed. Compared with unified ELM, sparse ELM achieves similar generalization performance for binary classification applications, and when dealing with large-scale binary classification problems, sparse ELM realizes even faster training speed than unified ELM.", "paper_title": "Sparse Extreme Learning Machine for Classification", "paper_id": "WOS:000342228100013"}