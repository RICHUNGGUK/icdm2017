{"auto_keywords": [{"score": 0.01334723790703484, "phrase": "vms"}, {"score": 0.00481495049065317, "phrase": "virtualized_environments"}, {"score": 0.004762202897268383, "phrase": "cloud_computing_infrastructures"}, {"score": 0.00471003041684707, "phrase": "vast_processing_power"}, {"score": 0.0046073860169839305, "phrase": "diverse_set"}, {"score": 0.004556901872357751, "phrase": "computing_workloads"}, {"score": 0.004457579615095599, "phrase": "service-oriented_deployments"}, {"score": 0.004408729661992651, "phrase": "high-performance_computing"}, {"score": 0.004126623209579942, "phrase": "large_number"}, {"score": 0.003926940703030417, "phrase": "vm"}, {"score": 0.0038624981944755813, "phrase": "important_challenge"}, {"score": 0.0035755653799043, "phrase": "generic_ethernet"}, {"score": 0.003328184259386487, "phrase": "zero-copy_characteristics"}, {"score": 0.00325555555110395, "phrase": "xen's_memory_sharing_techniques"}, {"score": 0.003030247482854539, "phrase": "nearly_the_same_raw_performance"}, {"score": 0.002996992362027833, "phrase": "open-mx_running"}, {"score": 0.002947790697936243, "phrase": "non-virtualized_environment"}, {"score": 0.002883439044844259, "phrase": "latency_front"}, {"score": 0.0027437235598539904, "phrase": "virtualization_layers"}, {"score": 0.0024434201175883674, "phrase": "non-virtualized_case"}, {"score": 0.002261666793030715, "phrase": "even_smaller_messages"}, {"score": 0.002175913488493802, "phrase": "network_adapters"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Virtualization", " Open-MX", " 10G Ethernet"], "paper_abstract": "Cloud computing infrastructures provide vast processing power and host a diverse set of computing workloads, ranging from service-oriented deployments to high-performance computing (HPC) applications. As HPC applications scale to a large number of VMs, providing near-native network I/O performance to each peer VM is an important challenge. In this paper we present Xen2MX, a paravirtual interconnection framework over generic Ethernet, binary compatible with Myrinet/MX and wire compatible with MXoE. Xen2MX combines the zero-copy characteristics of Open-MX with Xen's memory sharing techniques. Experimental evaluation of our prototype implementation shows that Xen2MX is able to achieve nearly the same raw performance as Open-MX running in a non-virtualized environment. On the latency front, Xen2MX performs as close as 96% to the case where virtualization layers are not present. Regarding throughput, Xen2MX saturates a 10 Gbps link, achieving 1159 MB/s, compared to 1192 MB/s of the non-virtualized case. Scales efficiently with the number of VMs, saturating the link for even smaller messages when 40 single-core VMs put pressure on the network adapters. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Xen2MX: High-performance communication in virtualized environments", "paper_id": "WOS:000341338500015"}