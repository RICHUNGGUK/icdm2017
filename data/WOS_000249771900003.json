{"auto_keywords": [{"score": 0.04521749909490238, "phrase": "guiding_points"}, {"score": 0.01335514503955383, "phrase": "interframe_motion_estimation"}, {"score": 0.00481495049065317, "phrase": "omnidirectional_vision"}, {"score": 0.0047132864404860865, "phrase": "autonomous_visual_guidance"}, {"score": 0.00448886315956469, "phrase": "coaxial_omnidirectional_stereopsis"}, {"score": 0.004380673342749118, "phrase": "range_r"}, {"score": 0.004236139082910124, "phrase": "trilateration_method"}, {"score": 0.004121423202649493, "phrase": "known_positions"}, {"score": 0.0039250703697261595, "phrase": "unknown_environment"}, {"score": 0.003703969862557761, "phrase": "special_markings"}, {"score": 0.0036478772206217282, "phrase": "known_locations"}, {"score": 0.003570766626733069, "phrase": "previous_motion_history"}, {"score": 0.0033901931552297792, "phrase": "dynamic_models"}, {"score": 0.0033695566513755096, "phrase": "robot's_motion"}, {"score": 0.0032983098564490208, "phrase": "autonomous_robots"}, {"score": 0.003278230817258323, "phrase": "useful_speeds"}, {"score": 0.0032483404695661296, "phrase": "initial_estimate"}, {"score": 0.003218721776850253, "phrase": "robot's_rotation_omega"}, {"score": 0.003121923422694974, "phrase": "angular_optic_flow"}, {"score": 0.0030934538358409216, "phrase": "onmidirectional_image"}, {"score": 0.003065243071851165, "phrase": "new_noniterative_optic_flow_method"}, {"score": 0.002857312568125609, "phrase": "fixed_coordinate_frame"}, {"score": 0.0028226141720397793, "phrase": "rotation_omega"}, {"score": 0.0027210199396428465, "phrase": "single_fixed_point"}, {"score": 0.002704445718144686, "phrase": "unknown_location"}, {"score": 0.0025912115192027485, "phrase": "large_number"}, {"score": 0.0024525461181496753, "phrase": "robot's_translation"}, {"score": 0.002393317277328894, "phrase": "robust_clustering_algorithm"}, {"score": 0.002386014719597134, "phrase": "clumat"}, {"score": 0.002364240149100421, "phrase": "rotation_and_translation_errors"}, {"score": 0.002265217884758413, "phrase": "autonomously_moving_robot"}, {"score": 0.002224056824144391, "phrase": "interframe_rotation"}, {"score": 0.00218364206035443, "phrase": "presented_vision_methods"}, {"score": 0.0021571068309076845, "phrase": "real_images"}, {"score": 0.00214396012113301, "phrase": "real_robotics_scenarios"}, {"score": 0.002105002791163301, "phrase": "elsevier"}], "paper_keywords": ["self-localization", " motion estimation", " omnidirectional vision", " omnistereo", " omniflow"], "paper_abstract": "This paper presents two related methods for autonomous visual guidance of robots: localization by trilateration, and interframe motion estimation. Both methods use coaxial omnidirectional stereopsis (omnistereo), which returns the range r to objects or guiding points detected in the images. The trilateration method achieves self-localization using r from the three nearest objects at known positions. The interframe motion estimation is more general, being able to use any features in an unknown environment. The guiding points are detected automatically on the basis of their perceptual significance and thus they need not have either special markings or be placed at known locations. The interframe motion estimation does not require previous motion history, making it well suited for detecting acceleration (in 20th of a second) and thus supporting dynamic models of robot's motion which will gain in importance when autonomous robots achieve useful speeds. An initial estimate of the robot's rotation omega (the visual compass) is obtained from the angular optic flow in an onmidirectional image. A new noniterative optic flow method has been developed for this purpose. Adding omega to all observed (robot relative) bearings theta gives true bearings towards objects (relative to a fixed coordinate frame). The rotation omega and the r, theta coordinates obtained at two frames for a single fixed point at unknown location are sufficient to estimate the translation of the robot. However, a large number of guiding points are typically detected and matched in most real images. Each such point provides a solution for the robot's translation. The solutions are combined by a robust clustering algorithm Clumat that reduces rotation and translation errors. Simulator experiments are included for all the presented methods. Real images obtained from ScitosG5 autonomously moving robot were used to test the interframe rotation and to show that the presented vision methods are applicable to real images in real robotics scenarios. (c) 2067 Elsevier B.V. All rights reserved.", "paper_title": "Instantaneous robot self-localization and motion estimation with omnidirectional vision", "paper_id": "WOS:000249771900003"}