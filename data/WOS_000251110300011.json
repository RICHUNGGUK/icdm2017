{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multiagent_environment"}, {"score": 0.004760394308239882, "phrase": "temporal-difference-fusion_architecture"}, {"score": 0.004420472010542188, "phrase": "adaptive_resonance_theory"}, {"score": 0.004296264970612285, "phrase": "self-organizing_neural_networks"}, {"score": 0.004199406257273994, "phrase": "td_methods"}, {"score": 0.0041517954514423625, "phrase": "real-time_reinforcement_learning"}, {"score": 0.003877211065759289, "phrase": "td-falcon_networks"}, {"score": 0.003683213029483699, "phrase": "dynamic_multiagent_environment"}, {"score": 0.0036207206487324506, "phrase": "minefield_navigation"}, {"score": 0.003459191419478298, "phrase": "navigation_task"}, {"score": 0.0034004869983267085, "phrase": "td-falcon_agent_teams"}, {"score": 0.0031573626285069157, "phrase": "explicit_mechanism"}, {"score": 0.0030337085953063125, "phrase": "traditional_q-learning_agents"}, {"score": 0.002999274091205852, "phrase": "gradient-descent-based_feedforward_neural_networks"}, {"score": 0.0027218799769229596, "phrase": "significantly_poorer_level"}, {"score": 0.002442025065383455, "phrase": "high-level_compressed_state_representation"}, {"score": 0.0023329572140896237, "phrase": "best_results"}, {"score": 0.00224151735457093, "phrase": "td-falcon_team"}, {"score": 0.0021908807203548345, "phrase": "rprop-based_reinforcement_learners"}, {"score": 0.0021291870676682406, "phrase": "task_completion_rate"}, {"score": 0.0021049977753042253, "phrase": "learning_efficiency"}], "paper_keywords": ["multiagent cooperative learning", " reinforcement learning (RL)", " self-organizing neural architectures"], "paper_abstract": "Temporal-Difference-Fusion Architecture for Learning, Cognition, and Navigation (TD-FALCON) is a generalization of adaptive resonance theory (a class of self-organizing neural networks) that incorporates TD methods for real-time reinforcement learning. In this paper, we investigate how a team of TD-FALCON networks may cooperate to learn and function in a dynamic multiagent environment based on minefield navigation and a predator/prey pursuit tasks. Experiments on the navigation task demonstrate that TD-FALCON agent teams are able to adapt and function well in a multiagent environment without an explicit mechanism of collaboration. In comparison, traditional Q-learning agents using gradient-descent-based feedforward neural networks, trained with the standard backpropagation and the resilient-propagation (RPROP) algorithms, produce a significantly poorer level of performance. For the predator/prey pursuit task, we experiment with various cooperative strategies and find that a combination of a high-level compressed state representation and a hybrid reward function produces the best results. Using the same cooperative strategy, the TD-FALCON team also outperforms the RPROP-based reinforcement learners in terms of both task completion rate and learning efficiency.", "paper_title": "Self-organizing neural architectures and cooperative learning in a multiagent environment", "paper_id": "WOS:000251110300011"}