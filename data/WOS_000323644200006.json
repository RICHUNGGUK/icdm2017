{"auto_keywords": [{"score": 0.029680504926650964, "phrase": "emotional_state"}, {"score": 0.00481495049065317, "phrase": "exploring_cross-modality"}, {"score": 0.004644146045859622, "phrase": "psycholinguistic_studies"}, {"score": 0.004602395776895036, "phrase": "human_communication"}, {"score": 0.004499647408293102, "phrase": "human_interaction"}, {"score": 0.004339979174946844, "phrase": "spoken_style"}, {"score": 0.00414830454375092, "phrase": "synchronization_pattern"}, {"score": 0.0038590090616766434, "phrase": "emotion_level"}, {"score": 0.0038242899532391914, "phrase": "cross-modality_settings"}, {"score": 0.003738847528809348, "phrase": "multimodal_emotion_recognition_systems"}, {"score": 0.0036060781902674207, "phrase": "acoustic_features"}, {"score": 0.003525493807130663, "phrase": "facial_expressions"}, {"score": 0.0034467039994594065, "phrase": "dyadic_interactions"}, {"score": 0.003249966782844541, "phrase": "similar_emotions"}, {"score": 0.0032061793819537633, "phrase": "strong_mutual_influence"}, {"score": 0.00298238929037041, "phrase": "mutual_information_framework"}, {"score": 0.0029025435402480326, "phrase": "strong_relation"}, {"score": 0.0028764048059439205, "phrase": "facial_and_acoustic_features"}, {"score": 0.002699889709390707, "phrase": "strong_dependence"}, {"score": 0.002675571068567923, "phrase": "heterogeneous_modalities"}, {"score": 0.0026514708899935333, "phrase": "conversational_partners"}, {"score": 0.002568812627436125, "phrase": "expressive_behaviors"}, {"score": 0.002522738549524317, "phrase": "complementary_information"}, {"score": 0.002389403849342894, "phrase": "classification_experiments"}, {"score": 0.0022631003204259224, "phrase": "emotion_recognition_experiments"}, {"score": 0.002232579371411544, "phrase": "iemocap_and_semaine_databases"}, {"score": 0.0021337783230121286, "phrase": "emotional_entrainment_effect"}, {"score": 0.0021049977753042253, "phrase": "statistically_significant_improvements"}], "paper_keywords": ["Entrainment", " multimodal interaction", " cross-subject multimodal emotion recognition", " facial expressions", " emotionally expressive speech"], "paper_abstract": "Psycholinguistic studies on human communication have shown that during human interaction individuals tend to adapt their behaviors mimicking the spoken style, gestures, and expressions of their conversational partners. This synchronization pattern is referred to as entrainment. This study investigates the presence of entrainment at the emotion level in cross-modality settings and its implications on multimodal emotion recognition systems. The analysis explores the relationship between acoustic features of the speaker and facial expressions of the interlocutor during dyadic interactions. The analysis shows that 72 percent of the time the speakers displayed similar emotions, indicating strong mutual influence in their expressive behaviors. We also investigate the cross-modality, cross-speaker dependence, using mutual information framework. The study reveals a strong relation between facial and acoustic features of one subject with the emotional state of the other subject. It also shows strong dependence between heterogeneous modalities across conversational partners. These findings suggest that the expressive behaviors from one dialog partner provide complementary information to recognize the emotional state of the other dialog partner. The analysis motivates classification experiments exploiting cross-modality, cross-speaker information. The study presents emotion recognition experiments using the IEMOCAP and SEMAINE databases. The results demonstrate the benefit of exploiting this emotional entrainment effect, showing statistically significant improvements.", "paper_title": "Exploring Cross-Modality Affective Reactions for Audiovisual Emotion Recognition", "paper_id": "WOS:000323644200006"}