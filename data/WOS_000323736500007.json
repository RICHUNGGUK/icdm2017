{"auto_keywords": [{"score": 0.03588915745850245, "phrase": "convergence_rate_o"}, {"score": 0.0324337131484804, "phrase": "nonconvex_problems"}, {"score": 0.030393345412521606, "phrase": "descent_direction"}, {"score": 0.00481495049065317, "phrase": "composite_functions"}, {"score": 0.004618166136069052, "phrase": "optimization_problems"}, {"score": 0.0045607070754363245, "phrase": "objective_function"}, {"score": 0.004283913685201902, "phrase": "black-box_oracle"}, {"score": 0.0040916230716372265, "phrase": "known_structure"}, {"score": 0.003990385737207714, "phrase": "good_properties"}, {"score": 0.0038431888954930083, "phrase": "convex_and_nonconvex_cases"}, {"score": 0.003685972800046028, "phrase": "first_part"}, {"score": 0.0035947360742084253, "phrase": "convex_problems"}, {"score": 0.0034911335714333507, "phrase": "primal_and_dual_variants"}, {"score": 0.0034476484413811987, "phrase": "gradient_method"}, {"score": 0.0031056437850836326, "phrase": "iteration_counter"}, {"score": 0.002762601646452813, "phrase": "general_nonsmooth"}, {"score": 0.0024573577356299765, "phrase": "efficient_\"line_search\"_procedures"}, {"score": 0.0024065032567542107, "phrase": "additional_computational_work"}, {"score": 0.0023566987138026285, "phrase": "unknown_problem_class_parameters"}, {"score": 0.0022507188208913394, "phrase": "small_constant_factor"}, {"score": 0.0021766418372878835, "phrase": "preliminary_computational_experiments"}, {"score": 0.0021049977753042253, "phrase": "accelerated_scheme"}], "paper_keywords": ["Local optimization", " Convex Optimization", " Nonsmooth optimization", " Complexity theory", " Black-box model", " Optimal methods", " Structural optimization", " l(1)-Regularization"], "paper_abstract": "In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms: one is smooth and given by a black-box oracle, and another is a simple general convex function with known structure. Despite the absence of good properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the first part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (with convergence rate O (1/k)), and an accelerated multistep version with convergence rate O (1/k(2)), where k is the iteration counter. For nonconvex problems with this structure, we prove convergence to a point from which there is no descent direction. In contrast, we show that for general nonsmooth, nonconvex problems, even resolving the question of whether a descent direction exists from a point is NP-hard. For all methods, we suggest some efficient \"line search\" procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme.", "paper_title": "Gradient methods for minimizing composite functions", "paper_id": "WOS:000323736500007"}