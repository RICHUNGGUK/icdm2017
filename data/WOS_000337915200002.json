{"auto_keywords": [{"score": 0.03410735276717892, "phrase": "parameter_space"}, {"score": 0.00481495049065317, "phrase": "metropolis_output"}, {"score": 0.0046797114400479135, "phrase": "efficient_implementation"}, {"score": 0.004635477242873163, "phrase": "generalized_linear_latent_variable_models"}, {"score": 0.004296264970612285, "phrase": "high-dimensional_problems"}, {"score": 0.004255642270420417, "phrase": "chib"}, {"score": 0.004215397034321004, "phrase": "jeliazkov"}, {"score": 0.004155742403177802, "phrase": "local_reversibility"}, {"score": 0.0040969285074699, "phrase": "metropolis-hastings_algorithm"}, {"score": 0.003944112858235306, "phrase": "full_conditional_densities"}, {"score": 0.0037432199549135826, "phrase": "distributional_assumptions"}, {"score": 0.0036207206487324506, "phrase": "simulation_algorithm"}, {"score": 0.0034526191487098093, "phrase": "reduced_markov_chain"}, {"score": 0.003436242862104006, "phrase": "monte_carlo"}, {"score": 0.0029935730127943496, "phrase": "latent_variable_models"}, {"score": 0.0028274743316389437, "phrase": "latent_variables"}, {"score": 0.0026077803487003, "phrase": "multi-block_metropolis-within-gibbs_algorithm"}, {"score": 0.0024984517279573906, "phrase": "single_run"}, {"score": 0.002371018116266096, "phrase": "counterpart_one-block_algorithm"}, {"score": 0.0021049977753042253, "phrase": "simulated_and_real-life_data_sets"}], "paper_keywords": ["generalized linear latent variable models", " Laplace-Metropolis estimator", " MCMC", " Bayes factor"], "paper_abstract": "The marginal likelihood can be notoriously difficult to compute, and particularly so in high-dimensional problems. Chib and Jeliazkov employed the local reversibility of the Metropolis-Hastings algorithm to construct an estimator in models where full conditional densities are not available analytically. The estimator is free of distributional assumptions and is directly linked to the simulation algorithm. However, it generally requires a sequence of reduced Markov chain Monte Carlo runs which makes the method computationally demanding especially in cases when the parameter space is large. In this article, we study the implementation of this estimator on latent variable models which embed independence of the responses to the observables given the latent variables (conditional or local independence). This property is employed in the construction of a multi-block Metropolis-within-Gibbs algorithm that allows to compute the estimator in a single run, regardless of the dimensionality of the parameter space. The counterpart one-block algorithm is also considered here, by pointing out the difference between the two approaches. The paper closes with the illustration of the estimator in simulated and real-life data sets.", "paper_title": "Marginal likelihood estimation from the Metropolis output: tips and tricks for efficient implementation in generalized linear latent variable models", "paper_id": "WOS:000337915200002"}