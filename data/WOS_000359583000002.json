{"auto_keywords": [{"score": 0.03650115677354156, "phrase": "different_modalities"}, {"score": 0.015719716506582538, "phrase": "multimodal_representations"}, {"score": 0.014405290625498562, "phrase": "shallow_structures"}, {"score": 0.011760629459302053, "phrase": "redundant_information"}, {"score": 0.004776484866276242, "phrase": "orthogonal_deep_structure"}, {"score": 0.004719359108879602, "phrase": "large-scale_multimodal_data"}, {"score": 0.004533806239310673, "phrase": "efficient_retrieval"}, {"score": 0.0044795698035664695, "phrase": "fundamental_problem"}, {"score": 0.004355516897622305, "phrase": "multimodal_representation_learning"}, {"score": 0.004234884779773412, "phrase": "learning_ability"}, {"score": 0.004052007312730483, "phrase": "multiple_modalities"}, {"score": 0.003987474771008829, "phrase": "multimodal_deep_learning"}, {"score": 0.003845994935767022, "phrase": "multimodal_data"}, {"score": 0.0036798490617699227, "phrase": "compact_and_accurate_representations"}, {"score": 0.0034926916198124484, "phrase": "different_complexities"}, {"score": 0.0034232607917050392, "phrase": "deep_models"}, {"score": 0.003368707839819003, "phrase": "open_problem"}, {"score": 0.0032753168662834516, "phrase": "aforementioned_problem"}, {"score": 0.003171740347925348, "phrase": "hashing-based_orthogonal_deep_model"}, {"score": 0.0031337465656628132, "phrase": "accurate_and_compact_multimodal_representations"}, {"score": 0.003010347277067459, "phrase": "inter-modality_correlations"}, {"score": 0.002974281028713475, "phrase": "accurate_representations"}, {"score": 0.0028229071631385634, "phrase": "hashing-based_model"}, {"score": 0.0027890803075309926, "phrase": "compact_hash_codes"}, {"score": 0.0027556576784848207, "phrase": "proposed_orthogonal_structure"}, {"score": 0.002636489396049042, "phrase": "orthogonal_regularizer"}, {"score": 0.002604890546456604, "phrase": "weighting_matrices"}, {"score": 0.002472269923319476, "phrase": "learned_codes"}, {"score": 0.0023558375480288297, "phrase": "different_characteristics"}, {"score": 0.0023182554418193927, "phrase": "effective_representations"}, {"score": 0.0022721178679337025, "phrase": "different_number"}, {"score": 0.0022179605282337395, "phrase": "comprehensive_experiments"}, {"score": 0.002130545585846588, "phrase": "retrieval_tasks"}, {"score": 0.0021049977753042253, "phrase": "existing_algorithms"}], "paper_keywords": ["Deep learning", " multimodal hashing", " similarity search"], "paper_abstract": "As large-scale multimodal data are ubiquitous in many real-world applications, learning multimodal representations for efficient retrieval is a fundamental problem. Most existing methods adopt shallow structures to perform multimodal representation learning. Due to a limitation of learning ability of shallow structures, they fail to capture the correlation of multiple modalities. Recently, multimodal deep learning was proposed and had proven its superiority in representing multimodal data due to its high nonlinearity. However, in order to learn compact and accurate representations, how to reduce the redundant information lying in the multimodal representations and incorporate different complexities of different modalities in the deep models is still an open problem. In order to address the aforementioned problem, in this paper we propose a hashing-based orthogonal deep model to learn accurate and compact multimodal representations. The method can better capture the intra-modality and inter-modality correlations to learn accurate representations. Meanwhile, in order to make the representations compact, the hashing-based model can generate compact hash codes and the proposed orthogonal structure can reduce the redundant information lying in the codes by imposing orthogonal regularizer on the weighting matrices. We also theoretically prove that, in this case, the learned codes are guaranteed to be approximately orthogonal. Moreover, considering the different characteristics of different modalities, effective representations can be attained with different number of layers for different modalities. Comprehensive experiments on three real-world datasets demonstrate a substantial gain of our method on retrieval tasks compared with existing algorithms.", "paper_title": "Learning Compact Hash Codes for Multimodal Representations Using Orthogonal Deep Structure", "paper_id": "WOS:000359583000002"}