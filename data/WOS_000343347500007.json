{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayesian_variable_selection"}, {"score": 0.008964018039473248, "phrase": "ghosh"}, {"score": 0.008898366816984972, "phrase": "clyde"}, {"score": 0.006367313105760198, "phrase": "haar_algorithm"}, {"score": 0.006297357841217216, "phrase": "largest_group"}, {"score": 0.005850114988005147, "phrase": "sandwich_algorithms"}, {"score": 0.004779564521350969, "phrase": "markov_chain_monte_carlo"}, {"score": 0.004555781299332848, "phrase": "model_averaging"}, {"score": 0.004489046493029801, "phrase": "high-dimensional_co-variates"}, {"score": 0.004406994912939125, "phrase": "model_space"}, {"score": 0.004169692306713217, "phrase": "sampling_models"}, {"score": 0.004123781267600542, "phrase": "posterior_distribution"}, {"score": 0.0039016690475620185, "phrase": "orthogonal_design_matrices"}, {"score": 0.0037602557461171478, "phrase": "traditional_gibbs_samplers"}, {"score": 0.0036508108698357932, "phrase": "rao-blackwellized_estimates"}, {"score": 0.003557653536102669, "phrase": "original_non-orthogonal_problem"}, {"score": 0.0034925663938977715, "phrase": "excellent_performance"}, {"score": 0.003378385493203297, "phrase": "design_matrix"}, {"score": 0.003316567137142017, "phrase": "empirical_results"}, {"score": 0.0032558762499901727, "phrase": "strong_multicollinearity"}, {"score": 0.003080365929916083, "phrase": "novel_sandwich_algorithms"}, {"score": 0.0027469703311742647, "phrase": "optimum_algorithm"}, {"score": 0.002706659627293686, "phrase": "parameter_expansion_data_augmentation"}, {"score": 0.002598821419664902, "phrase": "theoretical_insight"}, {"score": 0.0023346234464901978, "phrase": "pxda_algorithms"}, {"score": 0.0022834017104377525, "phrase": "simulation_studies"}, {"score": 0.0022665781920241245, "phrase": "real_data"}, {"score": 0.002192389791199368, "phrase": "substantial_gains"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Asymptotic variance", " Data augmentation", " Markov chain Monte Carlo", " Multicollinearity", " Rao-Blackwellization"], "paper_abstract": "Markov chain Monte Carlo (MCMC) algorithms have greatly facilitated the popularity of Bayesian variable selection and model averaging in problems with high-dimensional co-variates where enumeration of the model space is infeasible. A variety of such algorithms have been proposed in the literature for sampling models from the posterior distribution in Bayesian variable selection. Ghosh and Clyde proposed a method to exploit the properties of orthogonal design matrices. Their data augmentation algorithm scales up the computation tremendously compared to traditional Gibbs samplers, and leads to the availability of Rao-Blackwellized estimates of quantities of interest for the original non-orthogonal problem. The algorithm has excellent performance when the correlations among the columns of the design matrix are small, but empirical results suggest that moderate to strong multicollinearity leads to slow mixing. This motivates the need to develop a class of novel sandwich algorithms for Bayesian variable selection that improves upon the algorithm of Ghosh and Clyde. It is proved that the Haar algorithm with the largest group that acts on the space of models is the optimum algorithm, within the parameter expansion data augmentation (PXDA) class of sandwich algorithms. The result provides theoretical insight but using the largest group is computationally prohibitive so two new computationally viable sandwich algorithms are developed, which are inspired by the Haar algorithm, but do not necessarily belong to the class of PXDA algorithms. It is illustrated via simulation studies and real data analysis that several of the sandwich algorithms can offer substantial gains in the presence of multicollinearity. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Sandwich algorithms for Bayesian variable selection", "paper_id": "WOS:000343347500007"}