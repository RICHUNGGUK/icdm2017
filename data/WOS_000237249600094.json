{"auto_keywords": [{"score": 0.03671879024692415, "phrase": "lle"}, {"score": 0.00481495049065317, "phrase": "locally_linear_embedding"}, {"score": 0.004456044928556639, "phrase": "powerful_approach"}, {"score": 0.004286708921931472, "phrase": "high-dimensional_data"}, {"score": 0.0040708542958968605, "phrase": "lower-dimensional_space"}, {"score": 0.003767203575060969, "phrase": "training_examples"}, {"score": 0.003353521531649663, "phrase": "invalid_results"}, {"score": 0.0025892330566358503, "phrase": "neighbor_line"}, {"score": 0.0024586427394152196, "phrase": "lle_learning"}, {"score": 0.0022749744354271816, "phrase": "enriched_training"}], "paper_keywords": [""], "paper_abstract": "Locally linear embedding (LLE) is a powerful approach for mapping high-dimensional data nonlinearly to a lower-dimensional space. However, when the training examples are not densely sampled, LLE often returns invalid results. In this paper, the NO E (Neighbor Line-based LLE) approach is proposed, which generates some virtual examples with the help of neighbor line such that the LLE learning can be executed on an enriched training set. Experiments show that NL(3)E outperforms LLE in visualization.", "paper_title": "Neighbor line-based locally linear embedding", "paper_id": "WOS:000237249600094"}