{"auto_keywords": [{"score": 0.050077322760007634, "phrase": "nas_parallel_benchmarks"}, {"score": 0.03171939562914437, "phrase": "openmp"}, {"score": 0.004671588539217021, "phrase": "multicore_platforms"}, {"score": 0.0045324756541157574, "phrase": "additional_levels"}, {"score": 0.004181487624349488, "phrase": "mpi"}, {"score": 0.004097884868954017, "phrase": "pgas"}, {"score": 0.003780286517897356, "phrase": "integrated_performance_monitoring_tool"}, {"score": 0.00363080961099728, "phrase": "computation_time"}, {"score": 0.003469676666252421, "phrase": "run_time"}, {"score": 0.0032658707633520592, "phrase": "infiniband"}, {"score": 0.002821247621250663, "phrase": "particular_cases"}, {"score": 0.002682370021834353, "phrase": "upc_versions"}, {"score": 0.0026420518944303716, "phrase": "equal_performance"}, {"score": 0.0024742098091226203, "phrase": "memory_usage"}, {"score": 0.0024125253170522816, "phrase": "performance_differences"}, {"score": 0.0023287359930308864, "phrase": "quad-core_and_hex-core_processors"}, {"score": 0.002158836285985314, "phrase": "hex-core_system"}, {"score": 0.0021263705039915198, "phrase": "increased_contention"}, {"score": 0.0021049977753042253, "phrase": "network_resources"}], "paper_keywords": ["Programming model", " performance study", " UPC", " OpenMP", " MPI", " memory usage"], "paper_abstract": "Harnessing the power of multicore platforms is challenging due to the additional levels of parallelism present. In this paper we use the NAS Parallel Benchmarks to study three programming models, MPI, OpenMP and PGAS to understand their performance and memory usage characteristics on current multicore architectures. To understand these characteristics we use the Integrated Performance Monitoring tool and other ways to measure communication versus computation time, as well as the fraction of the run time spent in OpenMP. The benchmarks are run on two different Cray XT5 systems and an Infiniband cluster. Our results show that in general the three programming models exhibit very similar performance characteristics. In a few cases, OpenMP is significantly faster because it explicitly avoids communication. For these particular cases, we were able to re-write the UPC versions and achieve equal performance to OpenMP. Using OpenMP was also the most advantageous in terms of memory usage. Also we compare performance differences between the two Cray systems, which have quad-core and hex-core processors. We show that at scale the performance is almost always slower on the hex-core system because of increased contention for network resources.", "paper_title": "A programming model performance study using the NAS parallel benchmarks", "paper_id": "WOS:000286459500004"}