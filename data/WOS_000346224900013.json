{"auto_keywords": [{"score": 0.04981936527401865, "phrase": "extreme_learning_machine"}, {"score": 0.00481495049065317, "phrase": "sparse_semi-supervised"}, {"score": 0.004462105963334103, "phrase": "increasing_attention"}, {"score": 0.004366106816225457, "phrase": "low_computational_cost"}, {"score": 0.004318881178805575, "phrase": "excellent_performance"}, {"score": 0.004203030435498069, "phrase": "large_number"}, {"score": 0.004157561334726201, "phrase": "labeled_instances"}, {"score": 0.003958937342925219, "phrase": "hidden_nodes"}, {"score": 0.003811010743703935, "phrase": "better_learning"}, {"score": 0.0037289703210308905, "phrase": "elm."}, {"score": 0.0035701213515857227, "phrase": "sparse_semi-supervised_extreme_learning_machine"}, {"score": 0.0034742854447532678, "phrase": "joint_sparse_regularization"}, {"score": 0.003308195943172624, "phrase": "model_structure"}, {"score": 0.003272375502853897, "phrase": "joint_sparse_regularization_technology"}, {"score": 0.0030654278282822027, "phrase": "labeled_training_samples"}, {"score": 0.002934764205541183, "phrase": "greedy-algorithms_based_model_selection_approaches"}, {"score": 0.0027943943921568456, "phrase": "joint_sparse_constraints"}, {"score": 0.0027491084541806823, "phrase": "training_model"}, {"score": 0.00271935796016183, "phrase": "elm"}, {"score": 0.0026607205525625995, "phrase": "convex_programming"}, {"score": 0.002575849014190219, "phrase": "laplacian"}, {"score": 0.0025059724686541263, "phrase": "full_use"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sparse semi-supervised learning", " Extreme learning machine", " l(2,1)-Norm", " Joint sparse regularization", " Laplacian"], "paper_abstract": "Extreme Learning Machine (ELM) has received increasing attention for its simple principle, low computational cost and excellent performance. However, a large number of labeled instances are often required, and the number of hidden nodes should be manually tuned, for better learning and generalization of ELM. In this paper, we propose a Sparse Semi-Supervised Extreme Learning Machine (S3ELM) via joint sparse regularization for classification, which can automatically prune the model structure via joint sparse regularization technology, to achieve more accurate, efficient and robust classification, when only a small number of labeled training samples are available. Different with most of greedy-algorithms based model selection approaches, by using l(2,1)-norm, S3ELM casts a joint sparse constraints on the training model of ELM and formulate a convex programming. Moreover, with a Laplacian, S3ELM can make full use of the information from both the labeled and unlabeled samples. Some experiments are taken on several benchmark datasets, and the results show that S3ELM is computationally attractive and outperforms its counterparts. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Joint sparse regularization based Sparse Semi-Supervised Extreme Learning Machine (S3ELM) for classification", "paper_id": "WOS:000346224900013"}