{"auto_keywords": [{"score": 0.04790747004319555, "phrase": "source_domains"}, {"score": 0.04724754241017836, "phrase": "target_domain"}, {"score": 0.01562983498856178, "phrase": "imbalanced_distributions"}, {"score": 0.014055423883867992, "phrase": "mstl"}, {"score": 0.012968885710218678, "phrase": "negative_transfer"}, {"score": 0.008588801233516486, "phrase": "proposed_approach"}, {"score": 0.00481495049065317, "phrase": "handling_negative_transfer"}, {"score": 0.004759442693862545, "phrase": "multiple_source_transfer_learning"}, {"score": 0.004731928080086235, "phrase": "transfer_learning"}, {"score": 0.0044524122425287005, "phrase": "multiple_relevant_domains"}, {"score": 0.0041170581247889654, "phrase": "mstl._first"}, {"score": 0.0038849770915807603, "phrase": "highly_irrelevant_sources"}, {"score": 0.0036872695974989565, "phrase": "improper_judgement"}, {"score": 0.0036553070575015344, "phrase": "source_domains'_relevance"}, {"score": 0.003623620572273257, "phrase": "target_task"}, {"score": 0.003592207774973802, "phrase": "existing_mstl_methods"}, {"score": 0.0035199624440843892, "phrase": "relevant_sources"}, {"score": 0.0034995881411519925, "phrase": "balanced_distributions"}, {"score": 0.003311799410921826, "phrase": "novel_two-phase_framework"}, {"score": 0.0032546105164772995, "phrase": "multiple_sources"}, {"score": 0.00320770578058916, "phrase": "irrelevant_sources"}, {"score": 0.003189133102243104, "phrase": "imbalanced_class_distributions"}, {"score": 0.0031431690368229443, "phrase": "effective_supervised_local_weight_scheme"}, {"score": 0.0030888830649190282, "phrase": "proper_weight"}, {"score": 0.003062091606343783, "phrase": "source_domain's_classifier"}, {"score": 0.0029830992938695007, "phrase": "local_region"}, {"score": 0.002931569771634119, "phrase": "second_phase"}, {"score": 0.0028393931457694156, "phrase": "optimization_problem"}, {"score": 0.0028065957753430713, "phrase": "training_error_minimization"}, {"score": 0.0027741761880779535, "phrase": "weighted_predictions"}, {"score": 0.0027262456979186976, "phrase": "theoretical_analysis"}, {"score": 0.002469830206270585, "phrase": "online_processing_scenario"}, {"score": 0.0023990982005125763, "phrase": "extensive_experiments"}, {"score": 0.002385196172577967, "phrase": "disease_prediction"}, {"score": 0.00237137451107918, "phrase": "spam_filtering"}, {"score": 0.0023507916951658455, "phrase": "detection_datasets"}, {"score": 0.0022967713922705, "phrase": "proposed_two-phase_approach"}, {"score": 0.002218054404179068, "phrase": "imbalanced_distribution_challenges"}, {"score": 0.0021733853238695296, "phrase": "proposed_online_approach"}, {"score": 0.0021607883988873492, "phrase": "comparable_performance"}, {"score": 0.0021420294823879292, "phrase": "offline_scheme"}, {"score": 0.0021049977753042253, "phrase": "wiley_periodicals"}], "paper_keywords": ["multiple source", " transfer learning", " negative transfer", " imbalanced distribution"], "paper_abstract": "Transfer learning has benefited many real-world applications where labeled data are abundant in source domains but scarce in the target domain. As there are usually multiple relevant domains where knowledge can be transferred, multiple source transfer learning (MSTL) has recently attracted much attention. However, we are facing two major challenges when applying MSTL. First, without knowledge about the difference between source and target domains, negative transfer occurs when knowledge is transferred from highly irrelevant sources. Second, existence of imbalanced distributions in classes, where examples in one class dominate, can lead to improper judgement on the source domains' relevance to the target task. Since existing MSTL methods are usually designed to transfer from relevant sources with balanced distributions, they will fail in applications where these two challenges persist. In this article, we propose a novel two-phase framework to effectively transfer knowledge from multiple sources even when there exists irrelevant sources and imbalanced class distributions. First, an effective supervised local weight scheme is proposed to assign a proper weight to each source domain's classifier based on its ability of predicting accurately on each local region of the target domain. The second phase then learns a classifier for the target domain by solving an optimization problem which concerns both training error minimization and consistency with weighted predictions gained from source domains. A theoretical analysis shows that as the number of source domains increases, the probability that the proposed approach has an error greater than a bound is becoming exponentially small. We further extend the proposed approach to an online processing scenario to conduct transfer learning on continuously arriving data. Extensive experiments on disease prediction, spam filtering and intrusion detection datasets demonstrate that: (i) the proposed two-phase approach outperforms existing MSTL approaches due to its ability of tackling negative transfer and imbalanced distribution challenges, and (ii) the proposed online approach achieves comparable performance to the offline scheme. (C) 2014 Wiley Periodicals, Inc.", "paper_title": "On Handling Negative Transfer and Imbalanced Distributions in Multiple Source Transfer Learning", "paper_id": "WOS:000364191700003"}