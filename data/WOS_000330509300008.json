{"auto_keywords": [{"score": 0.0374287769677244, "phrase": "analytical_model"}, {"score": 0.026687370638160488, "phrase": "expected_miss-rate"}, {"score": 0.00481495049065317, "phrase": "permanent_faults"}, {"score": 0.004732785510739708, "phrase": "traditional_performance_cost_benefits"}, {"score": 0.004620093797504386, "phrase": "technology_scaling"}, {"score": 0.004463723624910583, "phrase": "static_and_dynamic_variations"}, {"score": 0.0044026610128451256, "phrase": "higher_probability"}, {"score": 0.004195418010131832, "phrase": "prime_design_constraint"}, {"score": 0.004123781267600542, "phrase": "sram_cells"}, {"score": 0.004011687811047391, "phrase": "processor_area"}, {"score": 0.003809628721278519, "phrase": "paramount_importance"}, {"score": 0.0037704502704526996, "phrase": "effective_methodologies"}, {"score": 0.00369329349984294, "phrase": "reliability_techniques"}, {"score": 0.0033767221949228834, "phrase": "permanent_cell_failure"}, {"score": 0.0032510690712452147, "phrase": "faulty_bits"}, {"score": 0.003044912038752554, "phrase": "fault_maps"}, {"score": 0.0029214769181390653, "phrase": "previous_approaches"}, {"score": 0.0028321862184182913, "phrase": "miss-rate_trends"}, {"score": 0.0027551040319267446, "phrase": "future_technology_nodes"}, {"score": 0.0026525207297080937, "phrase": "key_findings"}, {"score": 0.0026161698049580804, "phrase": "proposed_model"}, {"score": 0.0025361866386341796, "phrase": "negligible_impact"}, {"score": 0.002375253757224999, "phrase": "fault_map_methodology"}, {"score": 0.0021049977753042253, "phrase": "sequential_execution"}], "paper_keywords": ["Reliability", " caches", " yield", " fault tolerance"], "paper_abstract": "The traditional performance cost benefits we have enjoyed for decades from technology scaling are challenged by several critical constraints including reliability. Increases in static and dynamic variations are leading to higher probability of parametric and wear-out failures and are elevating reliability into a prime design constraint. In particular, SRAM cells used to build caches that dominate the processor area are usually minimum sized and more prone to failure. It is therefore of paramount importance to develop effective methodologies that facilitate the exploration of reliability techniques for caches. To this end, we present an analytical model that can determine for a given cache configuration, address trace, and random probability of permanent cell failure the exact expected miss rate and its standard deviation when blocks with faulty bits are disabled. What distinguishes our model is that it is fully analytical, it avoids the use of fault maps, and yet, it is both exact and simpler than previous approaches. The analytical model is used to produce the miss-rate trends (expected miss-rate) for future technology nodes for both uncorrelated and clustered faults. Some of the key findings based on the proposed model are (i) block disabling has a negligible impact on the expected miss-rate unless probability of failure is equal or greater than 2.6e-4, (ii) the fault map methodology can accurately calculate the expected miss-rate as long as 1,000 to 10,000 fault maps are used, and (iii) the expected miss-rate for execution of parallel applications increases with the number of threads and is more pronounced for a given probability of failure as compared to sequential execution.", "paper_title": "Modeling the Impact of Permanent Faults in Caches", "paper_id": "WOS:000330509300008"}