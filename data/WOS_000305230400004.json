{"auto_keywords": [{"score": 0.04081197366040995, "phrase": "cl"}, {"score": 0.010611795989702941, "phrase": "distilled_labelsets"}, {"score": 0.007590712382914357, "phrase": "original_labels"}, {"score": 0.006712990611336591, "phrase": "original_label_vector"}, {"score": 0.004786107405332454, "phrase": "multi-label_learning"}, {"score": 0.0047289360793096044, "phrase": "single-label_classification_methods"}, {"score": 0.004686504090684836, "phrase": "multi-label_learning_problems"}, {"score": 0.004506968388996092, "phrase": "high_dimensionality"}, {"score": 0.004439753960631346, "phrase": "existing_methods"}, {"score": 0.0038431888954930083, "phrase": "original_label_matrix"}, {"score": 0.0036406779213379154, "phrase": "popular_binary_classification_methods"}, {"score": 0.0035328029694928296, "phrase": "new_label"}, {"score": 0.0035010659378711816, "phrase": "fast_recovery_algorithm"}, {"score": 0.003407549468745519, "phrase": "predicted_new_labels"}, {"score": 0.0033667896755965464, "phrase": "recovery_algorithm"}, {"score": 0.0033365390352128065, "phrase": "\"labelset_distilling_method"}, {"score": 0.003122824878906245, "phrase": "recursive_clustering"}, {"score": 0.002940406485632161, "phrase": "explicit_joint_distribution"}, {"score": 0.002870449780891857, "phrase": "geometric_inference"}, {"score": 0.0027272537812174365, "phrase": "kullback-leibler_divergence_based_hypothesis_tests"}, {"score": 0.002678433668156676, "phrase": "new_labels"}, {"score": 0.0026068329296254087, "phrase": "training_samples"}, {"score": 0.002560162916190598, "phrase": "different_labels"}, {"score": 0.002506766880001808, "phrase": "learning_process"}, {"score": 0.0024693083555454133, "phrase": "compressed_labels"}, {"score": 0.0024250944519924383, "phrase": "label_dependence"}, {"score": 0.0024105326885334962, "phrase": "dls_based_tests"}, {"score": 0.002360250030403706, "phrase": "recovery_bounds"}, {"score": 0.002290227534601535, "phrase": "label_compression"}, {"score": 0.0022764737473071296, "phrase": "multi-label_classification_performance_improvement"}, {"score": 0.0021049977753042253, "phrase": "text_classification"}], "paper_keywords": [""], "paper_abstract": "Directly applying single-label classification methods to the multi-label learning problems substantially limits both the performance and speed due to the imbalance, dependence and high dimensionality of the given label matrix. Existing methods either ignore these three problems or reduce one with the price of aggravating another. In this paper, we propose a {0,1} label matrix compression and recovery method termed \"compressed labeling (CL)\" to simultaneously solve or at least reduce these three problems. CL first compresses the original label matrix to improve balance and independence by preserving the signs of its Gaussian random projections. Afterward, we directly utilize popular binary classification methods (e.g., support vector machines) for each new label. A fast recovery algorithm is developed to recover the original labels from the predicted new labels. In the recovery algorithm, a \"labelset distilling method\" is designed to extract distilled labelsets (DLs), i.e., the frequently appeared label subsets from the original labels via recursive clustering and subtraction. Given a distilled and an original label vector, we discover that the signs of their random projections have an explicit joint distribution that can be quickly computed from a geometric inference. Based on this observation, the original label vector is exactly determined after performing a series of Kullback-Leibler divergence based hypothesis tests on the distribution about the new labels. CL significantly improves the balance of the training samples and reduces the dependence between different labels. Moreover, it accelerates the learning process by training fewer binary classifiers for compressed labels, and makes use of label dependence via DLs based tests. Theoretically, we prove the recovery bounds of CL which verifies the effectiveness of CL for label compression and multi-label classification performance improvement brought by label correlations preserved in DLs. We show the effectiveness, efficiency and robustness of CL via 5 groups of experiments on 21 datasets from text classification, image annotation, scene classification, music categorization, genomics and web page classification.", "paper_title": "Compressed labeling on distilled labelsets for multi-label learning", "paper_id": "WOS:000305230400004"}