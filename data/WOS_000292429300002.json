{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "eye-gaze_movements"}, {"score": 0.03940134622178509, "phrase": "users'_interactions"}, {"score": 0.004574449773348889, "phrase": "intelligent_hci"}, {"score": 0.004401955895933783, "phrase": "increasing_numbers"}, {"score": 0.004345909305473054, "phrase": "human-centered_applications"}, {"score": 0.004024242000713225, "phrase": "intelligent_applications"}, {"score": 0.003922378980031565, "phrase": "users'_actions"}, {"score": 0.0036553070575015344, "phrase": "important_step"}, {"score": 0.003608732492631878, "phrase": "automatic_interpretation"}, {"score": 0.00334144503360907, "phrase": "context-sensing_problem"}, {"score": 0.0032359856233193504, "phrase": "generic_and_application-independent_framework"}, {"score": 0.003054450075234415, "phrase": "computer_interface"}, {"score": 0.002977061391551631, "phrase": "layered_hidden_markov_models"}, {"score": 0.0029391032882625473, "phrase": "lhmm"}, {"score": 0.0027741761880779535, "phrase": "keyboard_and_mouse_interactions"}, {"score": 0.0027212777001495154, "phrase": "main_contribution"}, {"score": 0.0026693851900137953, "phrase": "proposed_framework"}, {"score": 0.0025195550121331367, "phrase": "task_model"}, {"score": 0.0024874156497091994, "phrase": "variant_applications"}, {"score": 0.0024399717841513354, "phrase": "different_monitoring_purposes"}, {"score": 0.0024088451312725924, "phrase": "experimental_results"}, {"score": 0.002259054486157494, "phrase": "good_predictive_accuracy"}, {"score": 0.0022159566043745724, "phrase": "relatively_small_amount"}, {"score": 0.0021876814398268775, "phrase": "training_data"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Activity recognition", " Eye-tracking", " Human-computer interactions", " Layered Hidden Markov Models"], "paper_abstract": "The need for intelligent HCI has been reinforced by the increasing numbers of human-centered applications in our daily life. However, in order to respond adequately, intelligent applications must first interpret users' actions. Identifying the context in which users' interactions occur is an important step toward automatic interpretation of behavior. In order to address a part of this context-sensing problem, we propose a generic and application-independent framework for activity recognition of users interacting with a computer interface. Our approach uses Layered Hidden Markov Models (LHMM) and is based on eye-gaze movements along with keyboard and mouse interactions. The main contribution of the proposed framework is the ability to relate users' interactions to a task model in variant applications and for different monitoring purposes. Experimental results from two user studies show that our activity recognition technique is able to achieve good predictive accuracy with a relatively small amount of training data. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Activity recognition using eye-gaze movements and traditional interactions", "paper_id": "WOS:000292429300002"}