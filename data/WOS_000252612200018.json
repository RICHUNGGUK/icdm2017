{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "hilbert_spaces"}, {"score": 0.004466519142879166, "phrase": "separable_hilbert_space"}, {"score": 0.004269630481719451, "phrase": "k-means_clustering_scheme"}, {"score": 0.004143196873246147, "phrase": "empirical_squared_error"}, {"score": 0.003785836059995376, "phrase": "expected_squared_distance"}, {"score": 0.003701401653850141, "phrase": "random_vector_x"}, {"score": 0.0035648266669315943, "phrase": "cluster_centers"}, {"score": 0.003356675879109443, "phrase": "almost_surely_bounded_x"}, {"score": 0.0032817813775647756, "phrase": "expected_excess_clustering_risk"}, {"score": 0.002976019715894587, "phrase": "severe_computational_problems"}, {"score": 0.0028021527740710508, "phrase": "dimension_reduction_strategy"}, {"score": 0.0026986685585382347, "phrase": "johnson-lindenstrauss-type_random_projections"}, {"score": 0.0025219085140220773, "phrase": "computational_complexity"}, {"score": 0.0023566987138026285, "phrase": "random_projection"}, {"score": 0.002252602622785975, "phrase": "low-dimensional_space"}, {"score": 0.0021693692831601745, "phrase": "random_projections"}], "paper_keywords": ["clustering", " empirical risk minimization", " Hilbert space", " k-means", " random projections", " vector quantization"], "paper_abstract": "Based on n randomly drawn vectors in a separable Hilbert space, one may construct a k-means clustering scheme by minimizing an empirical squared error. We investigate the risk of such a clustering scheme, defined as the expected squared distance of a random vector X from the set of cluster centers. Our main result states that, for an almost surely bounded X, the expected excess clustering risk is O(root 1/n). Since clustering in high (or even infinite)-dimensional spaces may lead to severe computational problems, we examine the properties of a dimension reduction strategy for clustering based on Johnson-Lindenstrauss-type random projections. Our results reflect a tradeoff between accuracy and computational complexity when one uses k-means clustering after random projection of the data to a low-dimensional space. We argue that random projections work better than other simplistic dimension reduction schemes.", "paper_title": "On the performance of clustering in Hilbert spaces", "paper_id": "WOS:000252612200018"}