{"auto_keywords": [{"score": 0.04862717445897776, "phrase": "different_views"}, {"score": 0.0465247817138105, "phrase": "image-text_classification"}, {"score": 0.046069464194864115, "phrase": "multilingual_document_classification"}, {"score": 0.004367581653370531, "phrase": "document_classification"}, {"score": 0.004191829338769843, "phrase": "unlabeled_multiview_examples"}, {"score": 0.0038810583785936505, "phrase": "time_consuming_task"}, {"score": 0.003782655283668953, "phrase": "multiview_self-learning_strategy"}, {"score": 0.003724810609448597, "phrase": "different_voting_classifiers"}, {"score": 0.0036303546926971966, "phrase": "margin_distributions"}, {"score": 0.003574830654211291, "phrase": "unlabeled_training_data"}, {"score": 0.0034841647157189985, "phrase": "view-specific_classifier"}, {"score": 0.003143849342926373, "phrase": "automatic_margin-threshold"}, {"score": 0.0030171910207671205, "phrase": "unlabeled_examples"}, {"score": 0.002986329509791943, "phrase": "final_class_labels"}, {"score": 0.0027363971333961967, "phrase": "previous_pseudo-labels"}, {"score": 0.002708399936381754, "phrase": "new_view-specific_classifiers"}, {"score": 0.0026261108615242557, "phrase": "labeled_and_pseudo-labeled_training_data"}, {"score": 0.002456272788591051, "phrase": "experimental_results"}, {"score": 0.0024186620196958867, "phrase": "nus-wide_collection"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multiview learning", " Self-learning", " Image annotation", " Multilingual document categorization"], "paper_abstract": "In many applications, observations are available with different views. This is, for example, the case with image-text classification, multilingual document classification or document classification on the web. In addition, unlabeled multiview examples can be easily acquired, but assigning labels to these examples is usually a time consuming task. We describe a multiview self-learning strategy which trains different voting classifiers on different views. The margin distributions over the unlabeled training data, obtained with each view-specific classifier are then used to estimate an upper-bound on their transductive Bayes error. Minimizing this upper-bound provides an automatic margin-threshold which is used to assign pseudo-labels to unlabeled examples. Final class labels are then assigned to these examples, by taking a vote on the pool of the previous pseudo-labels. New view-specific classifiers are then trained using the labeled and pseudo-labeled training data. We consider applications to image-text classification and to multilingual document classification. We present experimental results on the NUS-WIDE collection and on Reuters RCV1-RCV2 which show that despite its simplicity, our approach is competitive with other state-of-the-art techniques. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Multiview self-learning", "paper_id": "WOS:000350934600013"}