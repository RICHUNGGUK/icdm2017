{"auto_keywords": [{"score": 0.035020303026695156, "phrase": "large_scope"}, {"score": 0.034020352831309594, "phrase": "emotional_manifestations"}, {"score": 0.00481495049065317, "phrase": "-type_emotion_recognition"}, {"score": 0.004783105609960904, "phrase": "future_audio-based_surveillance_systems"}, {"score": 0.004657808551825796, "phrase": "automatic_emotion_recognition"}, {"score": 0.004505771917193233, "phrase": "emotional_manifestation"}, {"score": 0.004402295697276237, "phrase": "speech_processing"}, {"score": 0.0043731680194078046, "phrase": "fear-type_emotions"}, {"score": 0.004329835888760592, "phrase": "abnormal_situations"}, {"score": 0.004230382915464492, "phrase": "human_life"}, {"score": 0.004078675940346967, "phrase": "new_application"}, {"score": 0.004051680770486568, "phrase": "emotion_recognition_-_public_safety"}, {"score": 0.003829290275367182, "phrase": "extreme_emotional_manifestations"}, {"score": 0.003691913669486176, "phrase": "safe_corpus"}, {"score": 0.00361906209437624, "phrase": "fictional_and_emotional_corpus"}, {"score": 0.0035712919163869176, "phrase": "fiction_movies"}, {"score": 0.0033639840192975835, "phrase": "normal_and_abnormal_situations"}, {"score": 0.003034682308336483, "phrase": "strong_emotions"}, {"score": 0.0029648894235196886, "phrase": "interesting_support"}, {"score": 0.002925728709297618, "phrase": "high_variety"}, {"score": 0.0028584344002749395, "phrase": "task-dependent_annotation_strategy"}, {"score": 0.002746644177161797, "phrase": "situation_evolution"}, {"score": 0.002701361707441567, "phrase": "emotion_recognition_system"}, {"score": 0.002595698433525986, "phrase": "unknown_speakers"}, {"score": 0.00256140152313695, "phrase": "noisy_sound_environments"}, {"score": 0.0024941578231560055, "phrase": "neutral_classification"}, {"score": 0.0024286751532861476, "phrase": "dissociated_acoustic_models"}, {"score": 0.0024045637620292697, "phrase": "voiced_and_unvoiced_contents"}, {"score": 0.002318180580715058, "phrase": "decision_step"}, {"score": 0.0022951636331743066, "phrase": "classification_system"}, {"score": 0.0021617722702730407, "phrase": "error_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["fear-type emotions recognition", " fiction corpus", " annotation scheme", " acoustic features of emotions", " machine learning", " threatening situations", " civil safety"], "paper_abstract": "This paper addresses the issue of automatic emotion recognition in speech. We focus on a type of emotional manifestation which has been rarely studied in speech processing: fear-type emotions occurring during abnormal situations (here, unplanned events where human life is threatened). This study is dedicated to a new application in emotion recognition - public safety. The starting point of this work is the definition and the collection of data illustrating extreme emotional manifestations in threatening situations. For this purpose we develop the SAFE corpus (situation analysis in a fictional and emotional corpus) based on fiction movies. It consists of 7 h of recordings organized into 400 audiovisual sequences. The corpus contains recordings of both normal and abnormal situations and provides a large scope of contexts and therefore a large scope of emotional manifestations. In this way, not only it addresses the issue of the lack of corpora illustrating strong emotions, but also it forms an interesting support to study a high variety of emotional manifestations. We define a task-dependent annotation strategy which has the particularity to describe simultaneously the emotion and the situation evolution in context. The emotion recognition system is based on these data and must handle a large scope of unknown speakers and situations in noisy sound environments. It consists of a fear vs. neutral classification. The novelty of our approach relies on dissociated acoustic models of the voiced and unvoiced contents of speech. The two are then merged at the decision step of the classification system. The results are quite promising given the complexity and the diversity of the data: the error rate is about 30%. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Fear-type emotion recognition for future audio-based surveillance systems", "paper_id": "WOS:000257296600004"}