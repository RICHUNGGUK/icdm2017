{"auto_keywords": [{"score": 0.04333695708487271, "phrase": "sarsa"}, {"score": 0.00481495049065317, "phrase": "reinforcement_learning_techniques"}, {"score": 0.004518442937844318, "phrase": "different_opposition_schemes"}, {"score": 0.0037336520904376687, "phrase": "type-ii_opposites"}, {"score": 0.0032355864829403413, "phrase": "opposition-based_schemes"}, {"score": 0.0031845067058084583, "phrase": "regular_learning_methods"}, {"score": 0.003036040409062437, "phrase": "learning_process"}, {"score": 0.0027594936602962075, "phrase": "state_space"}, {"score": 0.002548331088496964, "phrase": "proposed_methods"}, {"score": 0.002225691463551672, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Opposition-based learning", " Reinforcement learning", " Q-learning", " Sarsa", " Reservoir management", " Grid world"], "paper_abstract": "In this paper, we present different opposition schemes for four reinforcement learning methods: Q-learning, Q(lambda), Sarsa, and Sarsa(lambda) under assumptions that are reasonable for many real-world problems where type-II opposites generally better reflect the nature of the problem at hand. It appears that the aggregation of opposition-based schemes with regular learning methods can significantly speed up the learning process, especially where the number of observations is small or the state space is large. We verify the performance of the proposed methods using two different applications: a grid-world problem and a single water reservoir management problem. Crown Copyright (C) 2014 Published by Elsevier Inc. All rights reserved.", "paper_title": "Oppositional extension of reinforcement learning techniques", "paper_id": "WOS:000337199200008"}