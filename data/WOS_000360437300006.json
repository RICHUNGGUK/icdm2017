{"auto_keywords": [{"score": 0.04960770684597511, "phrase": "sparse_representations"}, {"score": 0.00481495049065317, "phrase": "stable_multilevel_dictionaries"}, {"score": 0.0046741260449047976, "phrase": "learned_dictionaries"}, {"score": 0.004339744430169516, "phrase": "sparse_models"}, {"score": 0.004296998284839309, "phrase": "large-scale_applications"}, {"score": 0.003853583126988071, "phrase": "global_dictionaries"}, {"score": 0.0037222908218253054, "phrase": "test_data"}, {"score": 0.003649277504586372, "phrase": "training_samples"}, {"score": 0.003371220210416809, "phrase": "large_scale_data"}, {"score": 0.0032724821705972357, "phrase": "proposed_learning_algorithm"}, {"score": 0.0029490571526071016, "phrase": "hierarchical_dictionary"}, {"score": 0.0029199689963238726, "phrase": "multiple_levels"}, {"score": 0.002834409847642238, "phrase": "information-theoretic_scheme"}, {"score": 0.002618278365749875, "phrase": "ensemble_approach"}, {"score": 0.002579622880371082, "phrase": "robust_dictionaries"}, {"score": 0.0025289662786567896, "phrase": "proposed_dictionaries"}, {"score": 0.0024916260764216752, "phrase": "sparse_code"}, {"score": 0.002467038647952607, "phrase": "novel_test_data"}, {"score": 0.0023947191132328233, "phrase": "low-complexity_pursuit_procedure"}, {"score": 0.0022675820982741347, "phrase": "proposed_algorithm"}, {"score": 0.002147180332779021, "phrase": "multilevel_dictionaries"}, {"score": 0.0021259846592569386, "phrase": "compressed_recovery"}, {"score": 0.0021049977753042253, "phrase": "subspace_learning_applications"}], "paper_keywords": ["Compressed sensing", " dictionary learning", " generalization", " sparse representations", " stability"], "paper_abstract": "Sparse representations using learned dictionaries are being increasingly used with success in several data processing and machine learning applications. The increasing need for learning sparse models in large-scale applications motivates the development of efficient, robust, and provably good dictionary learning algorithms. Algorithmic stability and generalizability are desirable characteristics for dictionary learning algorithms that aim to build global dictionaries, which can efficiently model any test data similar to the training samples. In this paper, we propose an algorithm to learn dictionaries for sparse representations from large scale data, and prove that the proposed learning algorithm is stable and generalizable asymptotically. The algorithm employs a 1-D subspace clustering procedure, the K -hyperline clustering, to learn a hierarchical dictionary with multiple levels. We also propose an information-theoretic scheme to estimate the number of atoms needed in each level of learning and develop an ensemble approach to learn robust dictionaries. Using the proposed dictionaries, the sparse code for novel test data can be computed using a low-complexity pursuit procedure. We demonstrate the stability and generalization characteristics of the proposed algorithm using simulations. We also evaluate the utility of the multilevel dictionaries in compressed recovery and subspace learning applications.", "paper_title": "Learning Stable Multilevel Dictionaries for Sparse Representations", "paper_id": "WOS:000360437300006"}