{"auto_keywords": [{"score": 0.033502945522523205, "phrase": "conversion_function"}, {"score": 0.00481495049065317, "phrase": "energy_management_policies"}, {"score": 0.004695900553353526, "phrase": "single_sensor_node"}, {"score": 0.004618166136069052, "phrase": "finite_buffer"}, {"score": 0.00417793854311445, "phrase": "optimal_energy_management_policies"}, {"score": 0.004007083686875716, "phrase": "energy_harvesting_sources"}, {"score": 0.00390792986752692, "phrase": "network_performance"}, {"score": 0.0036553070575015344, "phrase": "discounted_cost_markov_decision_process_framework"}, {"score": 0.003334309053314909, "phrase": "optimal_policy"}, {"score": 0.0029169007179202164, "phrase": "heuristic_policies"}, {"score": 0.0025730425700096365, "phrase": "optimal_policies"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_case"}], "paper_keywords": ["Q-learning", " energy management policies", " energy harvesting", " sensor networks"], "paper_abstract": "In this paper, we consider the problem of finding optimal energy management policies in the presence of energy harvesting sources to maximize network performance. We formulate this problem in the discounted cost Markov decision process framework and apply two reinforcement learning algorithms. Prior work [1] obtains optimal policy in the case when the conversion function mapping energy to data transmitted is linear and provides heuristic policies in the case when the same is nonlinear. Our algorithms, however, provide optimal policies regardless of the form of the conversion function. Through simulations, our policies are seen to outperform those of [1] in the nonlinear case.", "paper_title": "Q-Learning Based Energy Management Policies for a Single Sensor Node with Finite Buffer", "paper_id": "WOS:000209696500022"}