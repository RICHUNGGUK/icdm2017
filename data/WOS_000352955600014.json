{"auto_keywords": [{"score": 0.049546921528857375, "phrase": "training_decision_trees"}, {"score": 0.047223064025491056, "phrase": "vrm"}, {"score": 0.028268263440393716, "phrase": "erm"}, {"score": 0.00481495049065317, "phrase": "vicinal-risk_minimization"}, {"score": 0.004579005794486596, "phrase": "vapnik's_vicinal_risk_minimization"}, {"score": 0.004354572391793908, "phrase": "decision_margins"}, {"score": 0.004027031316965272, "phrase": "labeling_decisions"}, {"score": 0.003829548069657859, "phrase": "global_regularization"}, {"score": 0.0037658881407605445, "phrase": "decision_tree_structure"}, {"score": 0.0036826452656927877, "phrase": "training_phase"}, {"score": 0.0036214184763804034, "phrase": "decision_tree"}, {"score": 0.003501991125315889, "phrase": "total_probability"}, {"score": 0.0034245614958572012, "phrase": "labeled_training_examples"}, {"score": 0.003220315882881027, "phrase": "resulting_classifier"}, {"score": 0.0031315358193567708, "phrase": "necessary_minimization"}, {"score": 0.0028316582436089064, "phrase": "synthetic_and_benchmark_real_datasets"}, {"score": 0.0027535633837592597, "phrase": "statistical_superiority"}, {"score": 0.002722930209707764, "phrase": "vrm_training"}, {"score": 0.00269263690712392, "phrase": "conventional_empirical_risk_minimization"}, {"score": 0.0025178072071268534, "phrase": "synthetic_and_real_datasets"}, {"score": 0.0023941575109692336, "phrase": "statistical_difference"}], "paper_keywords": ["Decision trees", " Vicinal-risk minimization", " Decision trees", " Classification"], "paper_abstract": "We propose the use of Vapnik's vicinal risk minimization (VRM) for training decision trees to approximately maximize decision margins. We implement VRM by propagating uncertainties in the input attributes into the labeling decisions. In this way, we perform a global regularization over the decision tree structure. During a training phase, a decision tree is constructed to minimize the total probability of misclassifying the labeled training examples, a process which approximately maximizes the margins of the resulting classifier. We perform the necessary minimization using an appropriate meta-heuristic (genetic programming) and present results over a range of synthetic and benchmark real datasets. We demonstrate the statistical superiority of VRM training over conventional empirical risk minimization (ERM) and the well-known C4.5 algorithm, for a range of synthetic and real datasets. We also conclude that there is no statistical difference between trees trained by ERM and using C4.5. Training with VRM is shown to be more stable and repeatable than by ERM. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "The use of vicinal-risk minimization for training decision trees", "paper_id": "WOS:000352955600014"}