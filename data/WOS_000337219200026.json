{"auto_keywords": [{"score": 0.049781331549484456, "phrase": "linear_transformation"}, {"score": 0.046481645015042976, "phrase": "bow_model"}, {"score": 0.04417757503221301, "phrase": "visual_words"}, {"score": 0.031231446495867378, "phrase": "new_dataset"}, {"score": 0.030667754796283645, "phrase": "transformation_matrix"}, {"score": 0.02418038411436446, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "codebook_bias"}, {"score": 0.004755279364266046, "phrase": "sparsity_and_f-norm_constraints"}, {"score": 0.004638136142210615, "phrase": "visual_words_model"}, {"score": 0.004609425869837347, "phrase": "bow"}, {"score": 0.004495738533629062, "phrase": "visual_applications"}, {"score": 0.004412397783369937, "phrase": "local_features"}, {"score": 0.0043576936014830875, "phrase": "corresponding_codebook"}, {"score": 0.003894772585498127, "phrase": "limited_number"}, {"score": 0.003870541730075597, "phrase": "training_images"}, {"score": 0.0036363418376496484, "phrase": "computational_time"}, {"score": 0.0035912243639260846, "phrase": "generalization_power"}, {"score": 0.00338441786634626, "phrase": "dataset_bias"}, {"score": 0.0033633514001658086, "phrase": "linear_codebook_transformation"}, {"score": 0.0033319964917118204, "phrase": "unsupervised_manner"}, {"score": 0.0032498004320510076, "phrase": "local_feature_space"}, {"score": 0.003179538613508042, "phrase": "linearly_independent_basis_vectors"}, {"score": 0.003072174266479278, "phrase": "basis_vectors"}, {"score": 0.0029777101656842803, "phrase": "pre-learned_codebooks"}, {"score": 0.0028325548627901004, "phrase": "under-determined_problem"}, {"score": 0.00253916011199984, "phrase": "discriminative_visual_words"}, {"score": 0.0024842242921857705, "phrase": "sparsity_constraints"}, {"score": 0.0023630680726174627, "phrase": "alternative_optimization_algorithm"}, {"score": 0.0023191700719613685, "phrase": "optimal_linear_transformation"}, {"score": 0.0022903576812801432, "phrase": "encoding_parameters"}, {"score": 0.002247807356218318, "phrase": "labeled_images"}, {"score": 0.002212951892867028, "phrase": "target_dataset"}, {"score": 0.0021991611806433634, "phrase": "image_classification_experimental_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Codebook bias", " Linear transformation", " Sparsity", " Alternative optimization"], "paper_abstract": "The bag of visual words model (BoW) and its variants have demonstrated their effectiveness for visual applications. The BoW model first extracts local features and generates the corresponding codebook where the elements of a codebook are viewed as visual words. However, the codebook is dataset dependent and has to be generated for each image dataset. Besides, when we only have a limited number of training images, the codebook generated correspondingly may not be able to encode images well. This requires a lot of computational time and weakens the generalization power of the BoW model. To solve these problems, in this paper, we propose to undo the dataset bias by linear codebook transformation in an unsupervised manner. To represent each point in the local feature space, we need a number of linearly independent basis vectors. We view the codebook as a linear transformation of these basis vectors. In this way, we can transform the pre-learned codebooks for a new dataset using the pseudo-inverse of the transformation matrix. However, this is an under-determined problem which may lead to many solutions. Besides, not all of the visual words are equally important for the new dataset. It would be more effective if we can make some selection and choose the discriminative visual words for transformation. Specifically, the sparsity constraints and the F-norm of the transformation matrix are used in this paper. We propose an alternative optimization algorithm to jointly search for the optimal linear transformation matrixes and the encoding parameters. The proposed method needs no labeled images from either the source dataset or the target dataset. Image classification experimental results on several image datasets show the effectiveness of the proposed method. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Undoing the codebook bias by linear transformation with sparsity and F-norm constraints for image classification", "paper_id": "WOS:000337219200026"}