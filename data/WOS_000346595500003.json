{"auto_keywords": [{"score": 0.04920822493417558, "phrase": "bottom-up_saliency"}, {"score": 0.047094807036930655, "phrase": "keypoint_detector"}, {"score": 0.02796399871371536, "phrase": "best_result"}, {"score": 0.00481495049065317, "phrase": "bik-bus"}, {"score": 0.004655576530003855, "phrase": "major_problems"}, {"score": 0.004339108074589304, "phrase": "new_method"}, {"score": 0.004247112325475329, "phrase": "point_clouds"}, {"score": 0.0040193730627071135, "phrase": "category_recognition"}, {"score": 0.003922099634585906, "phrase": "public_database"}, {"score": 0.0037921662574137535, "phrase": "neural_architecture"}, {"score": 0.003757480190200645, "phrase": "primate_visual_system"}, {"score": 0.0034911335714333507, "phrase": "visual_environment"}, {"score": 0.003459191419478298, "phrase": "saliency_map"}, {"score": 0.0034066004178183117, "phrase": "conspicuity_maps"}, {"score": 0.0033548062793973144, "phrase": "different_modalities"}, {"score": 0.0032635467579751423, "phrase": "color_information"}, {"score": 0.0031942814007173254, "phrase": "purely_stimulus-driven_manner"}, {"score": 0.002799810685137358, "phrase": "next_most_salient_location"}, {"score": 0.0027741761880779535, "phrase": "main_conclusions"}, {"score": 0.002731971103291716, "phrase": "similar_average_number"}, {"score": 0.00259320616168125, "phrase": "evaluated_metrics"}, {"score": 0.0025459275963373496, "phrase": "recognition_experiments"}, {"score": 0.0025148871904409095, "phrase": "second_best_detector"}, {"score": 0.002424012799998538, "phrase": "unique_drawback"}, {"score": 0.0024018107447434022, "phrase": "computational_time"}, {"score": 0.00226583335721762, "phrase": "big_differences"}, {"score": 0.0022382002476510573, "phrase": "recognition_performance"}, {"score": 0.0021049977753042253, "phrase": "desired_task"}], "paper_keywords": ["3D keypoints", " 3D interest points", " 3D object recognition", " performance evaluation"], "paper_abstract": "One of the major problems found when developing a 3D recognition system involves the choice of keypoint detector and descriptor. To help solve this problem, we present a new method for the detection of 3D keypoints on point clouds and we perform benchmarking between each pair of 3D keypoint detector and 3D descriptor to evaluate their performance on object and category recognition. These evaluations are done in a public database of real 3D objects. Our keypoint detector is inspired by the behavior and neural architecture of the primate visual system. The 3D keypoints are extracted based on a bottom-up 3D saliency map, that is, a map that encodes the saliency of objects in the visual environment. The saliency map is determined by computing conspicuity maps (a combination across different modalities) of the orientation, intensity, and color information in a bottom-up and in a purely stimulus-driven manner. These three conspicuity maps are fused into a 3D saliency map and, finally, the focus of attention (or keypoint location) is sequentially directed to the most salient points in this map. Inhibiting this location automatically allows the system to attend to the next most salient location. The main conclusions are: with a similar average number of keypoints, our 3D keypoint detector outperforms the other eight 3D keypoint detectors evaluated by achieving the best result in 32 of the evaluated metrics in the category and object recognition experiments, when the second best detector only obtained the best result in eight of these metrics. The unique drawback is the computational time, since biologically inspired 3D keypoint based on bottom-up saliency is slower than the other detectors. Given that there are big differences in terms of recognition performance, size and time requirements, the selection of the keypoint detector and descriptor has to be matched to the desired task and we give some directions to facilitate this choice.", "paper_title": "BIK-BUS: Biologically Motivated 3D Keypoint Based on Bottom-Up Saliency", "paper_id": "WOS:000346595500003"}