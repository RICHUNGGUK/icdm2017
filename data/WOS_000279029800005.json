{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "generic_calibrated_sensors"}, {"score": 0.004491678520807451, "phrase": "vision_sensors"}, {"score": 0.004288261454002681, "phrase": "known_calibrated_projection_model"}, {"score": 0.0042388570312522, "phrase": "tracking_architecture"}, {"score": 0.004173865018011237, "phrase": "particle_filtering_methods"}, {"score": 0.003789561910103601, "phrase": "projection_model"}, {"score": 0.0036178264212372497, "phrase": "self-motion_measurements"}, {"score": 0.0035076666315744525, "phrase": "observation_model"}, {"score": 0.0033877275374263314, "phrase": "special_points"}, {"score": 0.003087500254099842, "phrase": "color_distributions"}, {"score": 0.0030283730114326014, "phrase": "object's_occluding_contour"}, {"score": 0.0028576796380512157, "phrase": "image_processing_routines"}, {"score": 0.0028138046006772567, "phrase": "color_segmentation"}, {"score": 0.0027175263159330523, "phrase": "motion_blur"}, {"score": 0.0026965813247045427, "phrase": "optical_distortions"}, {"score": 0.002644920521740564, "phrase": "omnidirectional_sensors"}, {"score": 0.0025842289219860795, "phrase": "tracking_applications"}, {"score": 0.0025643086619242814, "phrase": "different_objects"}, {"score": 0.002129584983647679, "phrase": "realistic_tracking_sequences"}, {"score": 0.0021049977753042253, "phrase": "ground_truth"}], "paper_keywords": ["Omnidirectional vision", " Tracking", " Particle filtering"], "paper_abstract": "We present a color and shape based 3D tracking system suited to a large class of vision sensors. The method is applicable, in principle, to any known calibrated projection model. The tracking architecture is based on particle filtering methods where each particle represents the 3D state of the object, rather than its state in the image, therefore overcoming the nonlinearity caused by the projection model. This allows the use of realistic 3D motion models and easy incorporation of self-motion measurements. All nonlinearities are concentrated in the observation model so that each particle projects a few tens of special points onto the image, on (and around) the 3D object's surface. The likelihood of each state is then evaluated by comparing the color distributions inside and outside the object's occluding contour. Since only pixel access operations are required, the method does not require the use of image processing routines like edge/feature extraction, color segmentation or 3D reconstruction, which can be sensitive to motion blur and optical distortions typical in applications of omnidirectional sensors to robotics. We show tracking applications considering different objects (balls, boxes), several projection models (catadioptric, dioptric, perspective) and several challenging scenarios (clutter, occlusion, illumination changes, motion and optical blur). We compare our methodology against a state-of-the-art alternative, both in realistic tracking sequences and with ground truth generated data. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Tracking objects with generic calibrated sensors: An algorithm based on color and 3D shape features", "paper_id": "WOS:000279029800005"}