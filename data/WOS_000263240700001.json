{"auto_keywords": [{"score": 0.04684814051146252, "phrase": "marl"}, {"score": 0.00481495049065317, "phrase": "multi-agent_reinforcement_learning"}, {"score": 0.004763565662615503, "phrase": "common_interest"}, {"score": 0.0046875099587735825, "phrase": "sum_stochastic_games"}, {"score": 0.004539005582217305, "phrase": "multi_agent_reinforcement_learning"}, {"score": 0.004395185177346749, "phrase": "continually_growing_attention"}, {"score": 0.004143196873246147, "phrase": "different_subtasks"}, {"score": 0.003926626018798576, "phrase": "theoretical_convergence_results"}, {"score": 0.003526735797659508, "phrase": "learning_process"}, {"score": 0.0033965836912510385, "phrase": "comprehensive_empirical_study"}, {"score": 0.00334229277905058, "phrase": "mgs"}, {"score": 0.0031167895790050405, "phrase": "important_algorithms"}, {"score": 0.0029221312451219203, "phrase": "different_approaches"}, {"score": 0.0028293966058473476, "phrase": "friendq"}, {"score": 0.0027991418754844347, "phrase": "oal"}, {"score": 0.0027692585945308474, "phrase": "wolf"}, {"score": 0.0027395968397212053, "phrase": "foeq"}, {"score": 0.0027103019468006726, "phrase": "rmax"}, {"score": 0.0025962029048520324, "phrase": "fully_cooperative_and_fully_competitive_domains"}, {"score": 0.002460295872115956, "phrase": "informal_analysis"}, {"score": 0.0024209250169088575, "phrase": "resulting_learning_processes"}, {"score": 0.0022941723937662927, "phrase": "new_learning_algorithms"}, {"score": 0.0022453451864218477, "phrase": "existing_algorithms"}, {"score": 0.0022213218042340735, "phrase": "specific_tasks"}, {"score": 0.0021392418081444798, "phrase": "formal_analysis"}, {"score": 0.0021049977753042253, "phrase": "learning_processes"}], "paper_keywords": ["reinforcement learning", " multi-agent reinforcement learning", " stochastic games"], "paper_abstract": "Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the field, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to specific tasks, and may guide further research and formal analysis of the learning processes.", "paper_title": "Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study", "paper_id": "WOS:000263240700001"}