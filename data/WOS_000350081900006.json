{"auto_keywords": [{"score": 0.04926639323881632, "phrase": "cross-modal_retrieval"}, {"score": 0.014752026794041542, "phrase": "text_query"}, {"score": 0.014381594349167741, "phrase": "vice_versa"}, {"score": 0.012502325224113615, "phrase": "corr-rbm"}, {"score": 0.00481495049065317, "phrase": "deep_correspondence"}, {"score": 0.004773904938276846, "phrase": "boltzmann_machine"}, {"score": 0.004326188427877999, "phrase": "considerable_attention"}, {"score": 0.004270960586542813, "phrase": "rapid_growth"}, {"score": 0.004234532601573088, "phrase": "multi-modal_web_data"}, {"score": 0.004127094600862448, "phrase": "different_modalities"}, {"score": 0.00385368199735867, "phrase": "correspondence_restricted_boltzmann_machine"}, {"score": 0.0037398106866046972, "phrase": "original_features"}, {"score": 0.0037078961288626185, "phrase": "bimodal_data"}, {"score": 0.003522027550662857, "phrase": "low-dimensional_common_space"}, {"score": 0.003447349911141396, "phrase": "heterogeneous_data"}, {"score": 0.003137074867443131, "phrase": "correlation_loss_function"}, {"score": 0.0030969799559421806, "phrase": "single_objective_function"}, {"score": 0.0030054007508185858, "phrase": "correlation_loss"}, {"score": 0.002854645802851, "phrase": "objective_function"}, {"score": 0.0025207284405252914, "phrase": "main_building_block"}, {"score": 0.002414861891567233, "phrase": "comparison_experiments"}, {"score": 0.0023035249767734286, "phrase": "significantly_better_results"}, {"score": 0.0021973099304606076, "phrase": "searching_images"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Cross-modal", " RBM", " Retrieval", " Deep Learning", " Multi-modal"], "paper_abstract": "The task of cross-modal retrieval, i.e., using a text query to search for images or vice versa, has received considerable attention with the rapid growth of multi-modal web data. Modeling the correlations between different modalities is the key to tackle this problem. In this paper, we propose a correspondence restricted Boltzmann machine (Corr-RBM) to map the original features of bimodal data, such as image and text in our setting, into a low-dimensional common space, in which the heterogeneous data are comparable. In our Corr-RBM, two RBMs built for image and text, respectively are connected at their individual hidden representation layers by a correlation loss function. A single objective function is constructed to trade off the correlation loss and likelihoods of both modalities. Through the optimization of this objective function, our Corr-RBM is able to capture the correlations between two modalities and learn the representation of each modality simultaneously. Furthermore, we construct two deep neural structures using Corr-RBM as the main building block for the task of cross-modal retrieval. A number of comparison experiments are performed on three public real-world data sets. All of our models show significantly better results than state-of-the-art models in both searching images via text query and vice versa. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Deep correspondence restricted Boltzmann machine for cross-modal retrieval", "paper_id": "WOS:000350081900006"}