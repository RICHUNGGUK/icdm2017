{"auto_keywords": [{"score": 0.04371491795340491, "phrase": "articulatory_movements"}, {"score": 0.04345077631293021, "phrase": "input_text"}, {"score": 0.040644071678013975, "phrase": "acoustic_features"}, {"score": 0.034897097982662134, "phrase": "fully_context-dependent_models"}, {"score": 0.00481495049065317, "phrase": "hmm-based_prediction"}, {"score": 0.004578030096508602, "phrase": "speaker's_mouth"}, {"score": 0.004549242970806199, "phrase": "text_input"}, {"score": 0.00452063603944825, "phrase": "hidden_markov_models"}, {"score": 0.0044079880067019765, "phrase": "human_articulatory_movements"}, {"score": 0.0043527164193299574, "phrase": "electromagnetic_articulography"}, {"score": 0.004325358572101206, "phrase": "ema"}, {"score": 0.004112411350177274, "phrase": "suitable_model_sequence"}, {"score": 0.003934681262271618, "phrase": "unified_acoustic-articulatory_hmms"}, {"score": 0.0038365781437074017, "phrase": "acoustic_signal"}, {"score": 0.003579179502431823, "phrase": "context-dependent_modeling"}, {"score": 0.0035231224373946457, "phrase": "supplementary_acoustic_input"}, {"score": 0.0034244153725855, "phrase": "unified_acoustic-articulatory_models"}, {"score": 0.003360146665246448, "phrase": "sole_input"}, {"score": 0.003276321257403678, "phrase": "monophone_and_quinphone_models"}, {"score": 0.0029427043216782604, "phrase": "quinphone_models"}, {"score": 0.002860212113080887, "phrase": "best_performance_overall"}, {"score": 0.002824292971306322, "phrase": "unified_acoustic-articulatory_quinphone_hmms"}, {"score": 0.002780025943393764, "phrase": "acoustic_and_articulatory_model_parameters"}, {"score": 0.002753798541636975, "phrase": "synchronous-state_sequence"}, {"score": 0.0026513335370556474, "phrase": "rms_error"}, {"score": 0.002609770322855842, "phrase": "correlation_coefficient"}, {"score": 0.0022212306049833397, "phrase": "acoustic_inputs"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Hidden Markov model", " Articulatory features", " Parameter generation"], "paper_abstract": "This paper presents an investigation into predicting the movement of a speaker's mouth from text input using hidden Markov models (HMM). A corpus of human articulatory movements, recorded by electromagnetic articulography (EMA), is used to train HMMs. To predict articulatory movements for input text, a suitable model sequence is selected and a maximum-likelihood parameter generation (MLPG) algorithm is used to generate output articulatory trajectories. Unified acoustic-articulatory HMMs are introduced to integrate acoustic features when an acoustic signal is also provided with the input text. Several aspects of this method are analyzed in this paper, including the effectiveness of context-dependent modeling, the role of supplementary acoustic input, and the appropriateness of certain model structures for the unified acoustic-articulatory models. When text is the sole input, we find that fully context-dependent models significantly outperform monophone and quinphone models, achieving an average root mean square (RMS) error of 1.945 mm and an average correlation coefficient of 0.600. When both text and acoustic features are given as input to the system, the difference between the performance of quinphone models and fully context-dependent models is no longer significant. The best performance overall is achieved using unified acoustic-articulatory quinphone HMMs with separate clustering of acoustic and articulatory model parameters, a synchronous-state sequence, and a dependent-feature model structure, with an RMS error of 0.900 mm and a correlation coefficient of 0.855 on average. Finally, we also apply the same quinphone HMMs to the acoustic-articulatory, or inversion, mapping problem, where only acoustic input is available. An average root mean square (RMS) error of 1.076 mm and an average correlation coefficient of 0.812 are achieved. Taken together, our results demonstrate how text and acoustic inputs both contribute to the prediction of articulatory movements in the method used. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "An Analysis of HMM-based prediction of articulatory movements", "paper_id": "WOS:000280917000005"}