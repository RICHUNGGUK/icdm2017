{"auto_keywords": [{"score": 0.02628533816143664, "phrase": "lspp"}, {"score": 0.00481495049065317, "phrase": "low-rank_representation"}, {"score": 0.004773476989344654, "phrase": "enhanced_data_representation"}, {"score": 0.004651177631663439, "phrase": "latent_low-rank_representation"}, {"score": 0.004551647527068771, "phrase": "robust_and_promising_results"}, {"score": 0.004454237726525018, "phrase": "feature_extraction"}, {"score": 0.004174287115855492, "phrase": "similar_principal_and_salient_features"}, {"score": 0.003911862375084159, "phrase": "enhanced_performance"}, {"score": 0.0038613835226952117, "phrase": "boosted_version"}, {"score": 0.0035873530542209686, "phrase": "appropriate_laplacian_regularization"}, {"score": 0.003465189575252443, "phrase": "local_features"}, {"score": 0.0033471722682967046, "phrase": "data_matrix"}, {"score": 0.0032331613500559764, "phrase": "low-rank_matrices"}, {"score": 0.0031502008922533894, "phrase": "principal_and_salient_features"}, {"score": 0.002990592465417758, "phrase": "correlated_features"}, {"score": 0.002790254164160135, "phrase": "outputted_bi-directional_low-rank_codes"}, {"score": 0.002766171878646582, "phrase": "rlrr"}, {"score": 0.002525489173110693, "phrase": "feature_learning"}, {"score": 0.0024928553725344933, "phrase": "supervised_extension"}, {"score": 0.0024183363693242943, "phrase": "discriminant_subspace_learning"}, {"score": 0.0023257824499250653, "phrase": "robust_representation"}, {"score": 0.0022857899334246946, "phrase": "real_images"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Low-rank representation", " Subspace recovery", " Similarity preservation", " Laplacian regularization", " Enhanced representation", " Feature learning"], "paper_abstract": "Latent Low-Rank Representation (LatLRR) delivers robust and promising results for subspace recovery and feature extraction through mining the so-called hidden effects, but the locality of both similar principal and salient features cannot be preserved in the optimizations. To solve this issue for achieving enhanced performance, a boosted version of LatLRR, referred to as Regularized Low-Rank Representation (rLRR), is proposed through explicitly including an appropriate Laplacian regularization that can maximally preserve the similarity among local features. Resembling LatLRR, rLRR decomposes given data matrix from two directions by seeking a pair of low-rank matrices. But the similarities of principal and salient features can be effectively preserved by rLRR. As a result, the correlated features are well grouped and the robustness of representations is also enhanced. Based on the outputted bi-directional low-rank codes by rLRR, an unsupervised subspace learning framework termed Low-rank Similarity Preserving Projections (LSPP) is also derived for feature learning. The supervised extension of LSPP is also discussed for discriminant subspace learning. The validity of rLRR is examined by robust representation and decomposition of real images. Results demonstrated the superiority of our rLRR and LSPP in comparison to Other related state-of-the-art algorithms. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Similarity preserving low-rank representation for enhanced data representation and effective subspace learning", "paper_id": "WOS:000335099800009"}