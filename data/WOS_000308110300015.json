{"auto_keywords": [{"score": 0.04925367071541481, "phrase": "data_replication"}, {"score": 0.034940932061321935, "phrase": "read_access_time"}, {"score": 0.00481495049065317, "phrase": "historical_access_record"}, {"score": 0.0047732596509542135, "phrase": "proactive_deletion"}, {"score": 0.004650330504968421, "phrase": "popular_technology"}, {"score": 0.004550300345169981, "phrase": "cloud_storage"}, {"score": 0.0045108906244892165, "phrase": "data_grids"}, {"score": 0.004262881313926916, "phrase": "network_traffic"}, {"score": 0.00420760449108585, "phrase": "access_time"}, {"score": 0.004153041453795458, "phrase": "data_availability"}, {"score": 0.004099183062034004, "phrase": "natural_and_man-made_disasters"}, {"score": 0.0038568987435766014, "phrase": "better_system_performance"}, {"score": 0.0036926236934657864, "phrase": "better_fault-tolerance"}, {"score": 0.0035199624440843892, "phrase": "large_number"}, {"score": 0.0034143003168921114, "phrase": "huge_update_overhead"}, {"score": 0.00326881479036966, "phrase": "updating_cost"}, {"score": 0.00321236577162671, "phrase": "file_popularity"}, {"score": 0.003170667618004276, "phrase": "important_factor"}, {"score": 0.00303553181618577, "phrase": "access_fluctuations"}, {"score": 0.0030092017034068666, "phrase": "historical_file_popularity"}, {"score": 0.002931569771634119, "phrase": "really_popular_files"}, {"score": 0.002843519597398796, "phrase": "dynamic_data_replication_strategy"}, {"score": 0.0027104531025934865, "phrase": "historical_access_records"}, {"score": 0.0025611773887867255, "phrase": "second_one"}, {"score": 0.0025279105573394727, "phrase": "proactive_deletion_method"}, {"score": 0.0024412911465483225, "phrase": "replica_number"}, {"score": 0.0023990982005125763, "phrase": "optimal_balance"}, {"score": 0.0023371689326096476, "phrase": "write_update_overhead"}, {"score": 0.0023068049914566975, "phrase": "unified_cost_model"}], "paper_keywords": ["Data replication", " Read overhead", " Update overhead", " Historical access record", " Proactive deletion"], "paper_abstract": "Data replication is becoming a popular technology in many fields such as cloud storage, Data grids and P2P systems. By replicating files to other servers/nodes, we can reduce network traffic and file access time and increase data availability to react natural and man-made disasters. However, it does not mean that more replicas can always have a better system performance. Replicas indeed decrease read access time and provide better fault-tolerance, but if we consider write access, maintaining a large number of replications will result in a huge update overhead. Hence, a trade-off between read access time and write updating cost is needed. File popularity is an important factor in making decisions about data replication. To avoid data access fluctuations, historical file popularity can be used for selecting really popular files. In this research, a dynamic data replication strategy is proposed based on two ideas. The first one employs historical access records which are useful for picking up a file to replicate. The second one is a proactive deletion method, which is applied to control the replica number to reach an optimal balance between the read access time and the write update overhead. A unified cost model is used as a means to measure and compare the performance of our data replication algorithm and other existing algorithms. The results indicate that our new algorithm performs much better than those algorithms.", "paper_title": "A novel dynamic network data replication scheme based on historical access record and proactive deletion", "paper_id": "WOS:000308110300015"}