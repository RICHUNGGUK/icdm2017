{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "generalized_dirichlet_mixture_modeling"}, {"score": 0.004709193603747991, "phrase": "possibilistic_approach"}, {"score": 0.004667541072056214, "phrase": "generalized_dirichlet_mixture_parameter_estimation"}, {"score": 0.004544770921520536, "phrase": "feature_weighting"}, {"score": 0.004484596684703308, "phrase": "proposed_algorithm"}, {"score": 0.004386063799793622, "phrase": "unsupervised_learning_of_finite_generalized_dirichlet_mixture_models"}, {"score": 0.003977505904363489, "phrase": "beta_distributions"}, {"score": 0.0038728151007982378, "phrase": "optimal_relevance_weights"}, {"score": 0.0036390887584943723, "phrase": "noisy_and_high-dimensional_feature_spaces"}, {"score": 0.003434662174045939, "phrase": "data_sample"}, {"score": 0.003389136094439662, "phrase": "first_one"}, {"score": 0.0033442114331500407, "phrase": "posterior_probability"}, {"score": 0.0032129675521135616, "phrase": "estimated_distribution"}, {"score": 0.0031703706626348507, "phrase": "second_membership"}, {"score": 0.002992195343569767, "phrase": "noise_points"}, {"score": 0.002824005136794786, "phrase": "distribution_parameters"}, {"score": 0.0027741761880779535, "phrase": "relevance_weights"}, {"score": 0.0025950024597986366, "phrase": "optimal_number"}, {"score": 0.002537880772794665, "phrase": "unsupervised_and_efficient_way"}, {"score": 0.0024600113832396187, "phrase": "possibilistic_membership_function"}, {"score": 0.0023424328921363585, "phrase": "similar_algorithms"}, {"score": 0.0023010815714140467, "phrase": "synthetic_data"}, {"score": 0.002250415129116109, "phrase": "noisy_and_high_dimensional_features"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Unsupervised learning", " Mixture model", " Feature weighting", " Generalized Dirichlet mixture", " Possibilistic approach", " Image collection categorization"], "paper_abstract": "We propose a possibilistic approach for Generalized Dirichlet mixture parameter estimation, data clustering, and feature weighting. The proposed algorithm, called Robust and Unsupervised Learning of Finite Generalized Dirichlet Mixture Models (RULe_GDM), exploits a property of the Generalized Dirichlet distributions that transforms the data to make the features independent and follow Beta distributions. Then, it learns optimal relevance weights for each feature within each cluster. This property makes RULe_GDM suitable for noisy and high-dimensional feature spaces. In addition, RULe_GDM associates two types of memberships with each data sample. The first one is the posterior probability and indicates how well a sample fits each estimated distribution. The second membership represents the degree of typicality and is used to identify and discard noise points and outliers. RULe_GDM minimizes one objective function which combines learning the two membership functions, distribution parameters, and the relevance weights for each feature within each distribution. We also extend our algorithm to find the optimal number of clusters in an unsupervised and efficient way by exploiting some properties of the possibilistic membership function. The performance of RULe_GDM is illustrated and compared to similar algorithms. We use synthetic data to illustrate its robustness to noisy and high dimensional features. We also compare our approach to other relevant algorithms using several standard data sets. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Unsupervised clustering and feature weighting based on Generalized Dirichlet mixture modeling", "paper_id": "WOS:000336706000003"}