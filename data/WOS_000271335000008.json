{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sparse_b-spline"}, {"score": 0.004750178261675602, "phrase": "human_activity_recognition"}, {"score": 0.0046389067165295466, "phrase": "local_image"}, {"score": 0.004545598514506788, "phrase": "subsequent_creation"}, {"score": 0.00451491283266524, "phrase": "visual_codebooks"}, {"score": 0.004335081499311714, "phrase": "video_retrieval_applications"}, {"score": 0.00414830454375092, "phrase": "new_set"}, {"score": 0.004120289747942224, "phrase": "visual_descriptors"}, {"score": 0.0040648241451154525, "phrase": "local_space-time_description"}, {"score": 0.00396954278887152, "phrase": "proposed_descriptors"}, {"score": 0.003916098498242888, "phrase": "spatiotemporal_salient_points"}, {"score": 0.003863370968575808, "phrase": "estimated_optical_flow_field"}, {"score": 0.0037600281647068522, "phrase": "geometrical_properties"}, {"score": 0.0037346256869699975, "phrase": "three-dimensional_piecewise_polynomials"}, {"score": 0.00357362651129581, "phrase": "spatiotemporal_locations"}, {"score": 0.0035494788628319903, "phrase": "salient_points"}, {"score": 0.003249966782844541, "phrase": "neighborhood_dimensions"}, {"score": 0.0031737251921267725, "phrase": "corresponding_spatiotemporal_salient_points"}, {"score": 0.003036832649117453, "phrase": "camera_motion"}, {"score": 0.0029959082125682918, "phrase": "camera_panning"}, {"score": 0.0029455250501196286, "phrase": "motion_component"}, {"score": 0.002886179212100513, "phrase": "local_median_filters"}, {"score": 0.0028569549040495163, "phrase": "optical_flow_field"}, {"score": 0.002771040606604632, "phrase": "whole_dataset"}, {"score": 0.002678598911294797, "phrase": "'visual_verbs"}, {"score": 0.002598034157587518, "phrase": "cluster_center"}, {"score": 0.0025543250296793554, "phrase": "resulting_codebook"}, {"score": 0.0025113494092413604, "phrase": "verbs'_approach"}, {"score": 0.0024193249334041557, "phrase": "small_temporal_windows"}, {"score": 0.0023625233416109917, "phrase": "boosting_algorithm"}, {"score": 0.002275939921439533, "phrase": "relevance_vector_machines"}, {"score": 0.002214965506028584, "phrase": "presented_results"}, {"score": 0.002185092297972985, "phrase": "human_actions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Optical flow", " Median filter", " Spatiotemporal salient points", " B-splines", " Gentleboost", " Relevance Vector Machine"], "paper_abstract": "The extraction and quantization of local image and video descriptors for the subsequent creation of visual codebooks is a technique that has proved very effective for image and video retrieval applications. In this paper we build on this concept and propose a new set of visual descriptors that provide a local space-time description of the visual activity. The proposed descriptors are extracted at spatiotemporal salient points detected on the estimated optical flow field for a given image sequence and are based on geometrical properties of three-dimensional piecewise polynomials, namely B-splines. The latter are fitted on the spatiotemporal locations of salient points that fall within a given spatiotemporal neighborhood. Our descriptors are invariant in translation and scaling in space-time. The latter is ensured by coupling the neighborhood dimensions to the scale at which the corresponding spatiotemporal salient points are detected. In addition, in order to provide robustness against camera motion (e.g. global translation due to camera panning) we subtract the motion component that is estimated by applying local median filters on the optical flow field. The descriptors that are extracted across the whole dataset are clustered in order to create a codebook of 'visual verbs', where each verb corresponds to a cluster center. We use the resulting codebook in a 'bag of verbs' approach in order to represent the motion of the subjects within small temporal windows. Finally, we use a boosting algorithm in order to select the most discriminative temporal windows of each class and Relevance Vector Machines (RVM) for classification. The presented results using three different databases of human actions verify the effectiveness of our method. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Sparse B-spline polynomial descriptors for human activity recognition", "paper_id": "WOS:000271335000008"}