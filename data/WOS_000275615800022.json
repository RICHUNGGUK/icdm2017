{"auto_keywords": [{"score": 0.044658987607950375, "phrase": "mse"}, {"score": 0.00481495049065317, "phrase": "supervised_learning"}, {"score": 0.004760394308239882, "phrase": "single-layer_neural_networks"}, {"score": 0.004600389726895716, "phrase": "novel_supervised_learning_method"}, {"score": 0.004548253531664032, "phrase": "single-layer_feedforward_neural_networks"}, {"score": 0.004420472010542188, "phrase": "alternative_objective_function"}, {"score": 0.004081385289516914, "phrase": "neuron's_nonlinear_activation_functions"}, {"score": 0.003683213029483699, "phrase": "linear_equations"}, {"score": 0.0034395119150793787, "phrase": "regular_methods"}, {"score": 0.0034004869983267085, "phrase": "theoretical_study"}, {"score": 0.0032860401904845522, "phrase": "approximated_equivalence"}, {"score": 0.0032302647179383915, "phrase": "global_optimum"}, {"score": 0.0031754329320561317, "phrase": "objective_function"}, {"score": 0.0031037644581516973, "phrase": "regular_mse_criterion"}, {"score": 0.002999274091205852, "phrase": "proposed_alternative_mse_function"}, {"score": 0.0028653893877383188, "phrase": "presented_method"}, {"score": 0.002690975370565776, "phrase": "exhaustive_experimental_study"}, {"score": 0.0022802596433757565, "phrase": "proposed_method"}, {"score": 0.0021908807203548345, "phrase": "highest_performance"}, {"score": 0.0021659920594094407, "phrase": "low-demanding_computational_requirements"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd"}], "paper_keywords": ["Single-layer neural networks", " Global optimum", " Supervised learning method", " Least squares", " Convex optimization", " Incremental learning"], "paper_abstract": "This paper proposes a novel supervised learning method for single-layer feedforward neural networks This approach uses an alternative objective function to that based on the MSE, which measures the errors before the neuron's nonlinear activation functions instead of after them In this case, the solution can be easily obtained solving systems of linear equations, le, requiring much less computational power than the one associated with the regular methods A theoretical study is included to proof the approximated equivalence between the global optimum of the objective function based on the regular MSE criterion and the one of the proposed alternative MSE function Furthermore, it is shown that the presented method has the capability of allowing incremental and distributed learning An exhaustive experimental study is also presented to verify the soundness and efficiency of the method This study contains 10 classification and 16 regression problems In addition, a comparison with other high performance learning algorithms shows that the proposed method exhibits, in average, the highest performance and low-demanding computational requirements (C) 2009 Elsevier Ltd All rights reserved", "paper_title": "A new convex objective function for the supervised learning of single-layer neural networks", "paper_id": "WOS:000275615800022"}