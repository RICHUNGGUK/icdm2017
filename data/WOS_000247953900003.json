{"auto_keywords": [{"score": 0.029377421672557427, "phrase": "google"}, {"score": 0.00481495049065317, "phrase": "definitional_question"}, {"score": 0.0046659637258752535, "phrase": "published_medical_literature"}, {"score": 0.0046242451772036145, "phrase": "online_medical_resources"}, {"score": 0.004582897918101983, "phrase": "important_sources"}, {"score": 0.004481132747216135, "phrase": "patient_treatment_decisions"}, {"score": 0.004441059405327222, "phrase": "traditional_sources"}, {"score": 0.00438161736774803, "phrase": "information_retrieval"}, {"score": 0.0040777192615062815, "phrase": "user's_query"}, {"score": 0.003969250376327401, "phrase": "returned_documents"}, {"score": 0.003933736547198066, "phrase": "large_knowledge_repositories"}, {"score": 0.0036443694410100507, "phrase": "clinical_setting"}, {"score": 0.003563370084662192, "phrase": "novel_algorithms"}, {"score": 0.003406713877432236, "phrase": "medical_definitional_question"}, {"score": 0.0033011569787033297, "phrase": "medqa"}, {"score": 0.0032423089921154503, "phrase": "large_number"}, {"score": 0.0032132782979796895, "phrase": "electronic_documents"}, {"score": 0.0031702173541386888, "phrase": "short_and_coherent_answers"}, {"score": 0.003113696267229798, "phrase": "definitional_questions"}, {"score": 0.0023557346035701096, "phrase": "pertinent_information"}, {"score": 0.0023241387549221408, "phrase": "different_documents"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["question answering", " information retrieval", " question analysis", " text summarization", " machine-learning", " evaluation"], "paper_abstract": "The published medical literature and online medical resources are important sources to help physicians make patient treatment decisions. Traditional sources used for information retrieval (e.g., PubMed) often return a list of documents in response to a user's query. Frequently the number of returned documents from large knowledge repositories is large and makes information seeking practical only \"after hours\" and not in the clinical setting. This study developed novel algorithms, and designed, implemented, and evaluated a medical definitional question answering system (MedQA). MedQA automatically analyzed a large number of electronic documents to generate short and coherent answers in response to definitional questions (i.e., questions with the format of \"What is X?\"). Our preliminary cognitive evaluation shows that MedQA out-performed three other online information systems (Google, OneLook, and PubMed) in two important efficiency criteria; namely, time spent and number of actions taken for a physician to identify a definition. It is our contention that question answering systems that aggregate pertinent information scattered across different documents have the potential to address clinical information needs within a timeframe necessary to meet the demands of clinicians. Published by Elsevier Inc.", "paper_title": "Development, implementation, and a cognitive evaluation of a definitional question answering system for physicians", "paper_id": "WOS:000247953900003"}