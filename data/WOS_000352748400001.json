{"auto_keywords": [{"score": 0.049774466498282044, "phrase": "state_machines"}, {"score": 0.02819827616624647, "phrase": "uml_activity_diagrams"}, {"score": 0.02567955532299056, "phrase": "test_cases"}, {"score": 0.008918096701246882, "phrase": "manual_test_case_derivation"}, {"score": 0.00617990172375057, "phrase": "diagram_types"}, {"score": 0.00515332396717891, "phrase": "activity_diagrams"}, {"score": 0.005104261802847735, "phrase": "higher_perceived_comprehensibility"}, {"score": 0.0046979667479838585, "phrase": "context"}, {"score": 0.004619356874848538, "phrase": "difficult_and_challenging_task"}, {"score": 0.004560780566066473, "phrase": "model-based_testing"}, {"score": 0.004502943680955863, "phrase": "complete_and_unambiguous_system_models"}, {"score": 0.004306205029641728, "phrase": "system_level"}, {"score": 0.004211069233133584, "phrase": "behavioral_models"}, {"score": 0.004078780515171644, "phrase": "manual_test_case"}, {"score": 0.003251328737784481, "phrase": "controlled_student_experiment"}, {"score": 0.0027800937731919276, "phrase": "higher_error-proneness"}, {"score": 0.0025339578578194143, "phrase": "missing_test_steps"}, {"score": 0.002438581238635582, "phrase": "wrong_field"}, {"score": 0.002180371753380285, "phrase": "clear_rules"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["System testing", " Test design", " Model-based testing", " Controlled experiment"], "paper_abstract": "Context: It is a difficult and challenging task to fully automatize model-based testing because this demands complete and unambiguous system models as input. Therefore, in practice, test cases, especially on the system level, are still derived manually from behavioral models like UML activity diagrams or state machines. But this kind of manual test case derivation is error-prone and knowing these errors makes it possible to provide guidelines to reduce them. Objective: The objective of the study presented in this paper therefore is to examine which errors are possible and actually made when manually deriving test cases from UML activity diagrams or state machines and whether there are differences between these diagram types. Method: We investigate the errors made when deriving test cases manually in a controlled student experiment. The experiment was performed and internally replicated with overall 84 participants divided into three groups at two institutions. Results: As a result of our experiment, we provide a taxonomy of errors made and their frequencies. In addition, our experiment provides evidence that activity diagrams have a higher perceived comprehensibility but also a higher error-proneness than state machines with regard to manual test case derivation. This information helps to develop guidelines for manual test case derivation from UML activity diagrams and state machines. Conclusion: Most errors observed were due to missing test steps, conditions or results, or content was written into the wrong field. As activity diagrams have a higher perceived comprehensibility, but also more error-prone than state machines, both diagram types are useful for manual test case derivation. Their application depends on the context and should be complemented with clear rules on how to derive test cases. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Manual test case derivation from UML activity diagrams and state machines: A controlled experiment", "paper_id": "WOS:000352748400001"}