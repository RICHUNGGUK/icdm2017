{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "spectral_projected_gradient"}, {"score": 0.004065156038847781, "phrase": "gradient_vector"}, {"score": 0.003945467978172845, "phrase": "search_direction"}, {"score": 0.0038676325043529524, "phrase": "large-scale_optimization"}, {"score": 0.003716520760680603, "phrase": "efficient_algorithms"}, {"score": 0.0034317185678527672, "phrase": "step_lengths"}, {"score": 0.0033306180744375616, "phrase": "novel_ideas"}, {"score": 0.003044785536429499, "phrase": "underlying_local_hessian"}, {"score": 0.00286795254228461, "phrase": "standard_decrease"}, {"score": 0.0027834145465170292, "phrase": "objective_function"}, {"score": 0.0026217213613095322, "phrase": "so-called_spectral_projected_gradient_methods"}, {"score": 0.0022799458141165587, "phrase": "low-cost_schemes"}, {"score": 0.0022127018991294047, "phrase": "optimization_problem"}, {"score": 0.0021049977753042253, "phrase": "positive_definite_matrices"}], "paper_keywords": ["spectral projected gradient methods", " nonmonotone line search", " large scale problems", " convex constrained problems"], "paper_abstract": "Over the last two decades, it has been observed that using the gradient vector as a search direction in large-scale optimization may lead to efficient algorithms. The effectiveness relies on choosing the step lengths according to novel ideas that are related to the spectrum of the underlying local Hessian rather than related to the standard decrease in the objective function. A review of these so-called spectral projected gradient methods for convex constrained optimization is presented. To illustrate the performance of these low-cost schemes, an optimization problem on the set of positive definite matrices is described.", "paper_title": "Spectral Projected Gradient Methods: Review and Perspectives", "paper_id": "WOS:000345288900001"}