{"auto_keywords": [{"score": 0.04976971084510215, "phrase": "hrl"}, {"score": 0.0301731459509384, "phrase": "proposed_algorithms"}, {"score": 0.014369875621153062, "phrase": "prior_work"}, {"score": 0.013304264959285899, "phrase": "average_reward_optimality_criterion"}, {"score": 0.009011671152258748, "phrase": "hierarchically_and_recursively_optimal_policies"}, {"score": 0.008641477993759994, "phrase": "hierarchically_optimal_discounted_reward_algorithm"}, {"score": 0.008558212738287312, "phrase": "recursively_optimal_discounted_reward_algorithm"}, {"score": 0.00481495049065317, "phrase": "hierarchical_reinforcement_learning"}, {"score": 0.004722254884093624, "phrase": "general_framework"}, {"score": 0.004571692719970733, "phrase": "large_state_and_action_spaces"}, {"score": 0.004284755990425297, "phrase": "maxq"}, {"score": 0.004243290079603214, "phrase": "phams"}, {"score": 0.004161553532799662, "phrase": "discrete-time_discounted_reward_semi-markov_decision_process"}, {"score": 0.003938383970047866, "phrase": "wide_class"}, {"score": 0.003912925129722523, "phrase": "continuing_tasks"}, {"score": 0.0038750439121298503, "phrase": "discounted_framework"}, {"score": 0.0038375280115736958, "phrase": "average_reward"}, {"score": 0.0038251496327282785, "phrase": "rl"}, {"score": 0.0036671822784770463, "phrase": "flat_policy_representations"}, {"score": 0.0033815186230026243, "phrase": "average_reward_smdp_model"}, {"score": 0.0030086877092638945, "phrase": "hierarchically_and_recursively_optimal_average_reward_policies"}, {"score": 0.0029892206557359836, "phrase": "discrete-time_and_continuous-time_average_reward_smdp_models"}, {"score": 0.0028195727965553367, "phrase": "first_problem"}, {"score": 0.0027922467043592597, "phrase": "relatively_simple_agv_scheduling_task"}, {"score": 0.0025578967205807843, "phrase": "second_problem"}, {"score": 0.0025331002386232014, "phrase": "larger_agv_scheduling_task"}, {"score": 0.0024601402359002056, "phrase": "continuous-time_models"}, {"score": 0.002420516633871837, "phrase": "hierarchical_task_decomposition"}, {"score": 0.002195794209219447, "phrase": "non-hierarchical_average_reward_algorithm"}, {"score": 0.0021464286727045623, "phrase": "proposed_hierarchical_average_reward_algorithms"}], "paper_keywords": ["semi-Markov decision processes", " hierarchical reinforcement learning", " average reward reinforcement learning", " hierarchical and recursive optimality"], "paper_abstract": "Hierarchical reinforcement learning (HRL) is a general framework for scaling reinforcement learning (RL) to problems with large state and action spaces by using the task (or action) structure to restrict the space of policies. Prior work in HRL including HAMs, options, MAXQ, and PHAMs has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model. The average reward optimality criterion has been recognized to be more appropriate for a wide class of continuing tasks than the discounted framework. Although average reward RL has been studied for decades, prior work has been largely limited to flat policy representations. In this paper, we develop a framework for HRL based on the average reward optimality criterion. We investigate two formulations of HRL based on the average reward SMDP model, both for discrete-time and continuous-time. These formulations correspond to two notions of optimality that have been previously explored in HRL: hierarchical optimality and recursive optimality. We present algorithms that learn to find hierarchically and recursively optimal average reward policies under discrete-time and continuous-time average reward SMDP models. We use two automated guided vehicle (AGV) scheduling tasks as experimental testbeds to study the empirical performance of the proposed algorithms. The first problem is a relatively simple AGV scheduling task, in which the hierarchically and recursively optimal policies are different. We compare the proposed algorithms with three other HRL methods, including a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm on this problem. The second problem is a larger AGV scheduling task. We model this problem using both discrete-time and continuous-time models. We use a hierarchical task decomposition in which the hierarchically and recursively optimal policies are the same for this problem. We compare the performance of the proposed algorithms with a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm, as well as a non-hierarchical average reward algorithm. The results show that the proposed hierarchical average reward algorithms converge to the same performance as their discounted reward counterparts.", "paper_title": "Hierarchical average reward reinforcement learning", "paper_id": "WOS:000252744900004"}