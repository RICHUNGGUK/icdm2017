{"auto_keywords": [{"score": 0.049715817986079144, "phrase": "kernel_methods"}, {"score": 0.015253145825317881, "phrase": "large_data_sets"}, {"score": 0.0065795760569198475, "phrase": "kde"}, {"score": 0.005672825460925494, "phrase": "qp_problems"}, {"score": 0.00481495049065317, "phrase": "kernel_density_estimation"}, {"score": 0.004565007074039171, "phrase": "standard_support_vector_machine"}, {"score": 0.004530371221172796, "phrase": "support_vector_regression_trainings"}, {"score": 0.004394424341776717, "phrase": "space_complexities"}, {"score": 0.004262539473250432, "phrase": "training_set_size"}, {"score": 0.003980032599437366, "phrase": "naive_method"}, {"score": 0.003919830848124645, "phrase": "quadratic_programming"}, {"score": 0.003659954070354848, "phrase": "kernel_density_estimate"}, {"score": 0.0035096666710980108, "phrase": "approximation_techniques"}, {"score": 0.0030595112110658675, "phrase": "entropy-based_integrated-squared-error_criterion"}, {"score": 0.0028132529128565282, "phrase": "latest_advance"}, {"score": 0.0027918707809612, "phrase": "fast_data_reduction"}, {"score": 0.002687366778212309, "phrase": "resulted_fastkde_method"}, {"score": 0.0025867643733794724, "phrase": "theoretical_guarantee"}, {"score": 0.002470987929071079, "phrase": "time_complexity"}, {"score": 0.002360381056508366, "phrase": "data_points"}, {"score": 0.0022806798371116698, "phrase": "different_benchmarking_data_sets"}, {"score": 0.002246124435189668, "phrase": "proposed_method"}, {"score": 0.002229043128610864, "phrase": "comparable_performance"}, {"score": 0.002203663884543687, "phrase": "state-of-art_method"}, {"score": 0.0021455612994325424, "phrase": "wide_range"}, {"score": 0.0021049977753042253, "phrase": "fast_learning"}], "paper_keywords": ["Kernel density estimate (KDE)", " kernel methods", " quadratic programming (QP)", " sampling", " support vector machine (SVM)"], "paper_abstract": "Kernel methods such as the standard support vector machine and support vector regression trainings take O(N-3) time and O(N-2) space complexities in their naive implementations, where N is the training set size. It is thus computationally infeasible in applying them to large data sets, and a replacement of the naive method for finding the quadratic programming (QP) solutions is highly desirable. By observing that many kernel methods can be linked up with kernel density estimate (KDE) which can be efficiently implemented by some approximation techniques, a new learning method called fast KDE (FastKDE) is proposed to scale up kernel methods. It is based on establishing a connection between KDE and the QP problems formulated for kernel methods using an entropy-based integrated-squared-error criterion. As a result, FastKDE approximation methods can be applied to solve these QP problems. In this paper, the latest advance in fast data reduction via KDE is exploited. With just a simple sampling strategy, the resulted FastKDE method can be used to scale up various kernel methods with a theoretical guarantee that their performance does not degrade a lot. It has a time complexity of O(m(3)) where m is the number of the data points sampled from the training set. Experiments on different benchmarking data sets demonstrate that the proposed method has comparable performance with the state-of-art method and it is effective for a wide range of kernel methods to achieve fast learning in large data sets.", "paper_title": "Kernel Density Estimation, Kernel Methods, and Fast Learning in Large Data Sets", "paper_id": "WOS:000328948900001"}