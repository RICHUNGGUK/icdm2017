{"auto_keywords": [{"score": 0.04952003448686007, "phrase": "latent_structures"}, {"score": 0.03474317600964099, "phrase": "latent_variables"}, {"score": 0.00481495049065317, "phrase": "greedy_learning_of_binary_latent_trees"}, {"score": 0.004575977644865379, "phrase": "underlying_data"}, {"score": 0.004487241576747549, "phrase": "rich_class"}, {"score": 0.0044002186469797476, "phrase": "latent_trees"}, {"score": 0.004264463695140275, "phrase": "visible_variables"}, {"score": 0.0040686130623959, "phrase": "hlc"}, {"score": 0.0040056739564861285, "phrase": "zhang"}, {"score": 0.003974071684351861, "phrase": "kocka"}, {"score": 0.0038817197332605647, "phrase": "search_algorithm"}, {"score": 0.003761900996118551, "phrase": "bayesian_network_structure_learning"}, {"score": 0.003674461410107989, "phrase": "good_solutions"}, {"score": 0.0034375594065869032, "phrase": "bin-g_algorithm"}, {"score": 0.0032539388572032563, "phrase": "bottom-up_fashion"}, {"score": 0.0032158817866929563, "phrase": "bin-a_algorithm"}, {"score": 0.0031658283119347396, "phrase": "tree_structure"}, {"score": 0.003141093559655865, "phrase": "agglomerative_hierarchical_clustering"}, {"score": 0.0029966788947444535, "phrase": "bin-g."}, {"score": 0.002892729377519916, "phrase": "binary_trees"}, {"score": 0.002847691227421257, "phrase": "hlc_models"}, {"score": 0.002825435048559866, "phrase": "comparable_quality"}, {"score": 0.0028033523242224833, "phrase": "zhang's_solutions"}, {"score": 0.002748895580860912, "phrase": "cross-validated_log-likelihood"}, {"score": 0.0025816239578784067, "phrase": "comprehensive_comparison"}, {"score": 0.0024436119167523156, "phrase": "interpretable_latent_structures"}, {"score": 0.002424506147893058, "phrase": "real-world_data"}, {"score": 0.0023961265476627166, "phrase": "large_number"}, {"score": 0.002312960805824996, "phrase": "restricted_version"}, {"score": 0.002197891020396052, "phrase": "topic_models"}], "paper_keywords": ["Unsupervised learning", " latent variable model", " hierarchical latent class model", " greedy methods"], "paper_abstract": "Inferring latent structures from observations helps to model and possibly also understand underlying data generating processes. A rich class of latent structures is the latent trees, i.e., tree-structured distributions involving latent variables where the visible variables are leaves. These are also called hierarchical latent class (HLC) models. Zhang and Kocka [21] proposed a search algorithm for learning such models in the spirit of Bayesian network structure learning. While such an approach can find good solutions, it can be computationally expensive. As an alternative, we investigate two greedy procedures: The BIN-G algorithm determines both the structure of the tree and the cardinality of the latent variables in a bottom-up fashion. The BIN-A algorithm first determines the tree structure using agglomerative hierarchical clustering, and then determines the cardinality of the latent variables as for BIN-G. We show that even with restricting ourselves to binary trees, we obtain HLC models of comparable quality to Zhang's solutions (in terms of cross-validated log-likelihood), while being generally faster to compute. This claim is validated by a comprehensive comparison on several data sets. Furthermore, we demonstrate that our methods are able to estimate interpretable latent structures on real-world data with a large number of variables. By applying our method to a restricted version of the 20 newsgroups data, these models turn out to be related to topic models, and on data from the PASCAL Visual Object Classes (VOC) 2007 challenge, we show how such tree-structured models help us understand how objects co-occur in images. For reproducibility of all experiments in this paper, all code and data sets (or links to data) are available at http://people.kyb.tuebingen.mpg.de/harmeling/code/ltt-1.4.tar.", "paper_title": "Greedy Learning of Binary Latent Trees", "paper_id": "WOS:000289524000002"}