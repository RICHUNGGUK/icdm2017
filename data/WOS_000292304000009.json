{"auto_keywords": [{"score": 0.04409532278951086, "phrase": "bayesian"}, {"score": 0.00481495049065317, "phrase": "partially_observable_markov_decision_processes"}, {"score": 0.004492406361795448, "phrase": "elegant_solution"}, {"score": 0.004415189385015778, "phrase": "exploration-exploitation_trade-off"}, {"score": 0.00402514392911723, "phrase": "standard_markov_decision_processes"}, {"score": 0.003887894704361635, "phrase": "primary_focus"}, {"score": 0.0036063061558261546, "phrase": "partially_observable_domains"}, {"score": 0.003503497128638099, "phrase": "bayes-adaptive_partially_observable_markov_decision_processes"}, {"score": 0.003156984631072011, "phrase": "pomdp_domain"}, {"score": 0.002861160152640875, "phrase": "partial_observability"}, {"score": 0.0026691455928407022, "phrase": "important_contribution"}, {"score": 0.0025631294247072476, "phrase": "theoretical_results"}, {"score": 0.0024049552650867935, "phrase": "good_learning_performance"}, {"score": 0.002349884235026956, "phrase": "approximate_algorithms"}, {"score": 0.0023228223366301226, "phrase": "belief_tracking"}, {"score": 0.0021921063460682293, "phrase": "empirical_results"}, {"score": 0.002129527338875116, "phrase": "model_estimate"}, {"score": 0.0021049977753042253, "phrase": "agent's_return"}], "paper_keywords": ["reinforcement learning", " Bayesian inference", " partially observable Markov decision processes"], "paper_abstract": "Bayesian learning methods have recently been shown to provide an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.", "paper_title": "A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes", "paper_id": "WOS:000292304000009"}