{"auto_keywords": [{"score": 0.03788547968512631, "phrase": "audio_signal"}, {"score": 0.00481495049065317, "phrase": "training_surrogate_sensors"}, {"score": 0.004775726266706856, "phrase": "musical_gesture_acquisition_systems"}, {"score": 0.004659951478263083, "phrase": "music_performers"}, {"score": 0.004603115982404017, "phrase": "common_task"}, {"score": 0.004565609517511076, "phrase": "interactive_electroacoustic_music"}, {"score": 0.004509919457793852, "phrase": "captured_gestures"}, {"score": 0.004382591802405206, "phrase": "synthesis_algorithms"}, {"score": 0.004206879557938045, "phrase": "music_transcription"}, {"score": 0.0040713749949601915, "phrase": "musical_gestures"}, {"score": 0.003956379470617348, "phrase": "\"traditional\"_musical_instruments"}, {"score": 0.0037667216301666196, "phrase": "\"indirect_acquisition"}, {"score": 0.0035423376631308567, "phrase": "invasive_modification"}, {"score": 0.003513443975425428, "phrase": "existing_instruments"}, {"score": 0.003372460634140967, "phrase": "relatively_straightforward_and_reliable_sensor_measurements"}, {"score": 0.003277140908834426, "phrase": "indirect_acquisition_approaches"}, {"score": 0.003237116128682586, "phrase": "sophisticated_signal_processing"}, {"score": 0.0030944828434567966, "phrase": "relevant_information"}, {"score": 0.002921975748248106, "phrase": "machine_learning_model"}, {"score": 0.002898127432512101, "phrase": "indirect_acquisition"}, {"score": 0.0028046593475268174, "phrase": "resulting_trained_\"surrogate\"_sensor"}, {"score": 0.002459902948098627, "phrase": "gesture_information"}, {"score": 0.002322689143237591, "phrase": "large_amounts"}, {"score": 0.002303720729387236, "phrase": "training_data"}, {"score": 0.002256973121326987, "phrase": "minimum_effort"}, {"score": 0.0022385401780982204, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "drum_surface"}], "paper_keywords": ["Gesture recognition", " machine learning", " new interfaces for musical expression", " surrogate sensors", " virtual sensors"], "paper_abstract": "Capturing the gestures of music performers is a common task in interactive electroacoustic music. The captured gestures can be mapped to sounds, synthesis algorithms, visuals, etc., or used for music transcription. Two of the most common approaches for acquiring musical gestures are: 1) \"hyper-instruments\" which are \"traditional\" musical instruments enhanced with sensors for directly detecting the gestures and 2) \"indirect acquisition\" in which the only sensor is a microphone capturing the audio signal. Hyper-instruments require invasive modification of existing instruments which is frequently undesirable. However, they provide relatively straightforward and reliable sensor measurements. On the other hand, indirect acquisition approaches typically require sophisticated signal processing and possibly machine learning algorithms in order to extract the relevant information from the audio signal. The idea of using direct sensor(s) to train a machine learning model for indirect acquisition is proposed in this paper. The resulting trained \"surrogate\" sensor can then be used in place of the original direct invasive sensor(s) that were used for training. That way, the instrument can be used unmodified in performance while still providing the gesture information that a hyper-instrument would provide. In addition, using this approach, large amounts of training data can be collected with minimum effort. Experimental results supporting this idea are provided in two detection contexts: 1) strike position on a drum surface and 2) strum direction on a sitar.", "paper_title": "Training Surrogate Sensors in Musical Gesture Acquisition Systems", "paper_id": "WOS:000286386900006"}