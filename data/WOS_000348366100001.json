{"auto_keywords": [{"score": 0.03637557639052407, "phrase": "sparse_embedding_vector"}, {"score": 0.0343649692424503, "phrase": "sample_space"}, {"score": 0.00481495049065317, "phrase": "sparse_embedding"}, {"score": 0.004474468584252318, "phrase": "large-scale_image_retrieval"}, {"score": 0.004430920292612374, "phrase": "fast_approximate_similarity_search"}, {"score": 0.004387793971765289, "phrase": "efficient_data_storage"}, {"score": 0.004240116703465252, "phrase": "knn_graph"}, {"score": 0.004198839649063847, "phrase": "high_dimensional_data_points"}, {"score": 0.00413767314434795, "phrase": "low_dimensional_manifold_space"}, {"score": 0.003661275004971829, "phrase": "effective_and_efficient_hashing_approach"}, {"score": 0.003520687960750365, "phrase": "training_sample_space"}, {"score": 0.0034020946201397057, "phrase": "learned_dictionary"}, {"score": 0.0031767199435197243, "phrase": "linear_spectral_clustering_method"}, {"score": 0.0030397596289795143, "phrase": "sparse_vector"}, {"score": 0.0030101325901285537, "phrase": "normalized_probabilities"}, {"score": 0.002548331088496964, "phrase": "least_variance_encoding_model"}, {"score": 0.0024383967028975616, "phrase": "sparse_embedding_feature"}, {"score": 0.0023677483882740317, "phrase": "coding_coefficients"}, {"score": 0.002333193766510394, "phrase": "hash_codes"}, {"score": 0.002265586601922693, "phrase": "binarization_threshold"}, {"score": 0.0021891781017389783, "phrase": "experimental_results"}, {"score": 0.0021678232353129472, "phrase": "benchmark_data_sets"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Hashing", " manifold learning", " image retrieval", " dictionary learning"], "paper_abstract": "Hashing is becoming increasingly important in large-scale image retrieval for fast approximate similarity search and efficient data storage. Many popular hashing methods aim to preserve the kNN graph of high dimensional data points in the low dimensional manifold space, which is, however, difficult to achieve when the number of samples is big. In this paper, we propose an effective and efficient hashing approach by sparsely embedding a sample in the training sample space and encoding the sparse embedding vector over a learned dictionary. To this end, we partition the sample space into clusters via a linear spectral clustering method, and then represent each sample as a sparse vector of normalized probabilities that it falls into its several closest clusters. This actually embeds each sample sparsely in the sample space. The sparse embedding vector is employed as the feature of each sample for hashing. We then propose a least variance encoding model, which learns a dictionary to encode the sparse embedding feature, and consequently binarize the coding coefficients as the hash codes. The dictionary and the binarization threshold are jointly optimized in our model. Experimental results on benchmark data sets demonstrated the effectiveness of the proposed approach in comparison with state-of-the-art methods.", "paper_title": "A Sparse Embedding and Least Variance Encoding Approach to Hashing", "paper_id": "WOS:000348366100001"}