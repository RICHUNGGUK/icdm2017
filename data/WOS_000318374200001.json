{"auto_keywords": [{"score": 0.04712665802421912, "phrase": "anomaly_detection"}, {"score": 0.015719716506582538, "phrase": "big_data"}, {"score": 0.012782564730131682, "phrase": "fundamental_limit"}, {"score": 0.004771435352180973, "phrase": "density_estimation"}, {"score": 0.004706895428234964, "phrase": "ubiquitous_base_modelling_mechanism"}, {"score": 0.004377077848734575, "phrase": "density_estimation_methods"}, {"score": 0.004317848984770719, "phrase": "kernel_density_estimator"}, {"score": 0.004278807087784626, "phrase": "-nearest_neighbour_density_estimator"}, {"score": 0.003871869188502677, "phrase": "existing_algorithms"}, {"score": 0.0037506567448470163, "phrase": "first_density_estimation_method"}, {"score": 0.0036998728605395384, "phrase": "average_case"}, {"score": 0.0030847461428168614, "phrase": "asymptotic_analysis"}, {"score": 0.0030429511467945525, "phrase": "new_density_estimator"}, {"score": 0.0029076695661972114, "phrase": "existing_density_estimators"}, {"score": 0.0027910539201878833, "phrase": "dbscan"}, {"score": 0.0027657776883712633, "phrase": "lof"}, {"score": 0.0027411049428562088, "phrase": "bayesian"}, {"score": 0.002548331088496964, "phrase": "new_density_estimation_method"}, {"score": 0.002316105585203396, "phrase": "new_method"}, {"score": 0.0022333012657436307, "phrase": "small_data_size"}, {"score": 0.002133937537670312, "phrase": "new_benchmark"}, {"score": 0.0021049977753042253, "phrase": "density-based_algorithms"}], "paper_keywords": ["Density estimation", " Density-based algorithms"], "paper_abstract": "Density estimation is the ubiquitous base modelling mechanism employed for many tasks including clustering, classification, anomaly detection and information retrieval. Commonly used density estimation methods such as kernel density estimator and -nearest neighbour density estimator have high time and space complexities which render them inapplicable in problems with big data. This weakness sets the fundamental limit in existing algorithms for all these tasks. We propose the first density estimation method, having average case sub-linear time complexity and constant space complexity in the number of instances, that stretches this fundamental limit to an extent that dealing with millions of data can now be done easily and quickly. We provide an asymptotic analysis of the new density estimator and verify the generality of the method by replacing existing density estimators with the new one in three current density-based algorithms, namely DBSCAN, LOF and Bayesian classifiers, representing three different data mining tasks of clustering, anomaly detection and classification. Our empirical evaluation results show that the new density estimation method significantly improves their time and space complexities, while maintaining or improving their task-specific performances in clustering, anomaly detection and classification. The new method empowers these algorithms, currently limited to small data size only, to process big data-setting a new benchmark for what density-based algorithms can achieve.", "paper_title": "DEMass: a new density estimator for big data", "paper_id": "WOS:000318374200001"}