{"auto_keywords": [{"score": 0.038497051044126554, "phrase": "exponential_loss"}, {"score": 0.013727281915217366, "phrase": "adaboost"}, {"score": 0.0047472380621488616, "phrase": "adaboost_algorithm"}, {"score": 0.004422632150619634, "phrase": "\"strong\"_hypothesis"}, {"score": 0.004043003243656031, "phrase": "\"exponential_loss"}, {"score": 0.003967314311312148, "phrase": "previous_work"}, {"score": 0.0038382392088338784, "phrase": "weak_learning_assumption"}, {"score": 0.0034920505103165403, "phrase": "adaboost's_computed_parameter_vector"}, {"score": 0.0033150974333333244, "phrase": "parameter_vector"}, {"score": 0.002917736974519881, "phrase": "lower_bounds"}, {"score": 0.0028630535573013686, "phrase": "polynomial_dependence"}, {"score": 0.0025436978068459565, "phrase": "best_possible_value"}, {"score": 0.002460825108719896, "phrase": "data_set"}, {"score": 0.002281376328251423, "phrase": "constant_factors"}, {"score": 0.0021049977753042253, "phrase": "optimal_exponential_loss"}], "paper_keywords": ["AdaBoost", " optimization", " coordinate descent", " convergence rate"], "paper_abstract": "The AdaBoost algorithm was designed to combine many \"weak\" hypotheses that perform slightly better than random guessing into a \"strong\" hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the \"exponential loss.\" Unlike previous work, our proofs do not require a weak learning assumption, nor do they require that minimizers of the exponential loss are finite. Our first result shows that the exponential loss of AdaBoost's computed parameter vector will be at most epsilon more than that of any parameter vector of l (1)-norm bounded by B in a number of rounds that is at most a polynomial in B and 1/epsilon. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/epsilon iterations, AdaBoost achieves a value of the exponential loss that is at most epsilon more than the best possible value, where C depends on the data set. We show that this dependence of the rate on epsilon is optimal up to constant factors, that is, at least Omega(1/epsilon) rounds are necessary to achieve within epsilon of the optimal exponential loss.", "paper_title": "The Rate of Convergence of AdaBoost", "paper_id": "WOS:000324799600004"}