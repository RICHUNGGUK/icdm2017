{"auto_keywords": [{"score": 0.04901626150275964, "phrase": "temporal-difference_learning"}, {"score": 0.015719716506582538, "phrase": "temporal-difference_search"}, {"score": 0.014192306395463924, "phrase": "master-level_play"}, {"score": 0.013635281741520885, "phrase": "key_idea"}, {"score": 0.013335197465812163, "phrase": "value_function"}, {"score": 0.01258513476870453, "phrase": "value_function_approximation"}, {"score": 0.01236283903413206, "phrase": "related_states"}, {"score": 0.01225314963852555, "phrase": "monte-carlo_tree_search"}, {"score": 0.011983113684815914, "phrase": "high-performance_search"}, {"score": 0.004005083535629333, "phrase": "real_experience"}, {"score": 0.003916098498242888, "phrase": "future_value_estimates"}, {"score": 0.003693846909452865, "phrase": "recent_algorithm"}, {"score": 0.003376216135716632, "phrase": "mean_outcome"}, {"score": 0.0033459905006404207, "phrase": "simulated_episodes"}, {"score": 0.0032132782979796895, "phrase": "search_tree"}, {"score": 0.003141830113006653, "phrase": "new_approach"}, {"score": 0.003085813569061001, "phrase": "markov_decision_processes"}, {"score": 0.0029236680654022664, "phrase": "simulation-based_search"}, {"score": 0.0027950561494657633, "phrase": "simulated_experience"}, {"score": 0.0024641814990854463, "phrase": "million_binary_features"}, {"score": 0.0024421008630165046, "phrase": "simple_patterns"}, {"score": 0.0023770362063833903, "phrase": "explicit_search_tree"}, {"score": 0.0023241387549221408, "phrase": "unenhanced_monte-carlo_tree_search"}, {"score": 0.0022118619998034742, "phrase": "simple_alpha-beta_search"}], "paper_keywords": ["Reinforcement learning", " Temporal-difference learning", " Monte-Carlo search", " Simulation based search", " Computer Go"], "paper_abstract": "Temporal-difference learning is one of the most successful and broadly applied solutions to the reinforcement learning problem; it has been used to achieve master-level play in chess, checkers and backgammon. The key idea is to update a value function from episodes of real experience, by bootstrapping from future value estimates, and using value function approximation to generalise between related states. Monte-Carlo tree search is a recent algorithm for high-performance search, which has been used to achieve master-level play in Go. The key idea is to use the mean outcome of simulated episodes of experience to evaluate each state in a search tree. We introduce a new approach to high-performance search in Markov decision processes and two-player games. Our method, temporal-difference search, combines temporal-difference learning with simulation-based search. Like Monte-Carlo tree search, the value function is updated from simulated experience; but like temporal-difference learning, it uses value function approximation and bootstrapping to efficiently generalise between related states. We apply temporal-difference search to the game of 9x9 Go, using a million binary features matching simple patterns of stones. Without any explicit search tree, our approach outperformed an unenhanced Monte-Carlo tree search with the same number of simulations. When combined with a simple alpha-beta search, our program also outperformed all traditional (pre-Monte-Carlo) search and machine learning programs on the 9x9 Computer Go Server.", "paper_title": "Temporal-difference search in computer Go", "paper_id": "WOS:000302417100003"}