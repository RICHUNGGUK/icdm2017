{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "neyman-pearson_classification"}, {"score": 0.0046018264570209765, "phrase": "support_vector_machine"}, {"score": 0.004324031677173549, "phrase": "neyman-pearson_criteria"}, {"score": 0.004039995317682942, "phrase": "straightforward_way"}, {"score": 0.003971938976270902, "phrase": "cost-sensitive_svm."}, {"score": 0.0037532257703888315, "phrase": "especially_accurate_error_estimation"}, {"score": 0.003627800727103479, "phrase": "svm_parameters"}, {"score": 0.003447451122274614, "phrase": "poor_classifier_performance"}, {"score": 0.0032208091960514128, "phrase": "usual_cost-sensitive_svm"}, {"score": 0.0027017308406240563, "phrase": "simple_yet_powerful_approach"}, {"score": 0.002552781867125423, "phrase": "extensive_experimental_study"}, {"score": 0.0022919762167777427, "phrase": "dramatic_performance_gains"}, {"score": 0.002202742267484358, "phrase": "descent_strategies"}, {"score": 0.0021655683497137234, "phrase": "significant_gains"}, {"score": 0.002141134176057931, "phrase": "computational_efficiency"}], "paper_keywords": ["Minimax classification", " Neyman-Pearson classification", " support vector machine", " error estimation", " parameter selection"], "paper_abstract": "This paper studies the training of support vector machine (SVM) classifiers with respect to the minimax and Neyman-Pearson criteria. In principle, these criteria can be optimized in a straightforward way using a cost-sensitive SVM. In practice, however, because these criteria require especially accurate error estimation, standard techniques for tuning SVM parameters, such as cross-validation, can lead to poor classifier performance. To address this issue, we first prove that the usual cost-sensitive SVM, here called the 2C-SVM, is equivalent to another formulation called the 2 nu-SVM. We then exploit a characterization of the 2 nu-SVM parameter space to develop a simple yet powerful approach to error estimation based on smoothing. In an extensive experimental study, we demonstrate that smoothing significantly improves the accuracy of cross-validation error estimates, leading to dramatic performance gains. Furthermore, we propose coordinate descent strategies that offer significant gains in computational efficiency, with little to no loss in performance.", "paper_title": "Tuning Support Vector Machines for Minimax and Neyman-Pearson Classification", "paper_id": "WOS:000281000700013"}