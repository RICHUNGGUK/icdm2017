{"auto_keywords": [{"score": 0.0445181707495043, "phrase": "unlabeled_examples"}, {"score": 0.009602702300430277, "phrase": "cotrade"}, {"score": 0.00481495049065317, "phrase": "data_editing"}, {"score": 0.004652016085325825, "phrase": "major_semi-supervised_learning_paradigms"}, {"score": 0.004174824849669107, "phrase": "training_set"}, {"score": 0.0040334640350288, "phrase": "co-training_process"}, {"score": 0.003954837422946628, "phrase": "initial_rounds"}, {"score": 0.003428665921276366, "phrase": "co-training_style_algorithms"}, {"score": 0.0031223774204093713, "phrase": "different_views"}, {"score": 0.0030464123943266673, "phrase": "novel_co-training_algorithm"}, {"score": 0.003016544840824688, "phrase": "cotrade."}, {"score": 0.0028715299874549245, "phrase": "label_communication_process"}, {"score": 0.0027469703311742647, "phrase": "classifier's_predictions"}, {"score": 0.002640781900605126, "phrase": "specific_data"}, {"score": 0.0025262061502713683, "phrase": "predicted_labels"}, {"score": 0.0025014259919355453, "phrase": "higher_confidence"}, {"score": 0.0022890371899188466, "phrase": "undesirable_classification_noise"}, {"score": 0.0021363670305843403, "phrase": "unlabeled_data"}, {"score": 0.0021049977753042253, "phrase": "better_generalization_performance"}], "paper_keywords": ["Bias-variance decomposition", " co-training", " data editing", " machine learning", " semi-supervised learning"], "paper_abstract": "Co-training is one of the major semi-supervised learning paradigms that iteratively trains two classifiers on two different views, and uses the predictions of either classifier on the unlabeled examples to augment the training set of the other. During the co-training process, especially in initial rounds when the classifiers have only mediocre accuracy, it is quite possible that one classifier will receive labels on unlabeled examples erroneously predicted by the other classifier. Therefore, the performance of co-training style algorithms is usually unstable. In this paper, the problem of how to reliably communicate labeling information between different views is addressed by a novel co-training algorithm named COTRADE. In each labeling round, COTRADE carries out the label communication process in two steps. First, confidence of either classifier's predictions on unlabeled examples is explicitly estimated based on specific data editing techniques. Secondly, a number of predicted labels with higher confidence of either classifier are passed to the other one, where certain constraints are imposed to avoid introducing undesirable classification noise. Experiments on several real-world datasets across three domains show that COTRADE can effectively exploit unlabeled data to achieve better generalization performance.", "paper_title": "COTRADE: Confident Co-Training With Data Editing", "paper_id": "WOS:000297342100014"}