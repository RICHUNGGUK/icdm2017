{"auto_keywords": [{"score": 0.047257170857088485, "phrase": "central_location"}, {"score": 0.012011160885542322, "phrase": "decentralized_estimation"}, {"score": 0.011084001295687698, "phrase": "encoder_design"}, {"score": 0.00787222871066652, "phrase": "dslvq"}, {"score": 0.00481495049065317, "phrase": "distortion_sensitive_learning_vector_quantization"}, {"score": 0.004732215871738028, "phrase": "supervised_learning"}, {"score": 0.004664352260912174, "phrase": "multiple_sources"}, {"score": 0.00461075951152053, "phrase": "original_data"}, {"score": 0.004289419884432487, "phrase": "constrained_communication_channels"}, {"score": 0.004240116703465252, "phrase": "limited_power"}, {"score": 0.004215677131963292, "phrase": "bitlength_constraints"}, {"score": 0.0041074234667687875, "phrase": "encoded_data"}, {"score": 0.003887894704361635, "phrase": "single_codeword"}, {"score": 0.0037880260900233064, "phrase": "target_quantity"}, {"score": 0.0037444643255115265, "phrase": "received_codewords"}, {"score": 0.0035442676479940497, "phrase": "lvq"}, {"score": 0.003523822585025555, "phrase": "classification_algorithm"}, {"score": 0.003306559295382181, "phrase": "known_distributions"}, {"score": 0.003287482882222731, "phrase": "source_observations"}, {"score": 0.0032215751881194328, "phrase": "empirical_samples"}, {"score": 0.0031845067058084583, "phrase": "dslvq_approach"}, {"score": 0.0031478633984287235, "phrase": "previously_proposed_regression_tree"}, {"score": 0.0030141556259370675, "phrase": "regression_tree"}, {"score": 0.0029197159945990015, "phrase": "encoder_regions"}, {"score": 0.0029028648824177715, "phrase": "axis-parallel_splits"}, {"score": 0.002661430072871348, "phrase": "different_data_distributions"}, {"score": 0.0024972048358485, "phrase": "limited_application_potential"}, {"score": 0.002418923027024191, "phrase": "four-source_setting"}, {"score": 0.002289431798120276, "phrase": "similar_performance"}, {"score": 0.0022499947003073654, "phrase": "small_number"}, {"score": 0.0021921063460682293, "phrase": "large_number"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Quantization", " Distributed estimation", " Non-linear estimation", " Vector quantization", " Aerosol retrieval"], "paper_abstract": "A typical approach in supervised learning when data comes from multiple sources is to send original data from all sources to a central location and train a predictor that estimates a certain target quantity. This can be inefficient and costly in applications with constrained communication channels, due to limited power and/or bitlength constraints. Under such constraints, one potential solution is to send encoded data from sources and use a decoder at the central location. Data at each source is summarized into a single codeword and sent to a central location, where a target quantity is estimated using received codewords. This problem is known as Decentralized Estimation. In this paper we propose a variant of the Learning Vector Quantization (LVQ) classification algorithm, the Distortion Sensitive LVQ (DSLVQ), to be used for encoder design in decentralized estimation. Unlike most related research that assumes known distributions of source observations, we assume that only a set of empirical samples is available. DSLVQ approach is compared to previously proposed Regression Tree and Deterministic Annealing (DA) approaches for encoder design in the same setting. While Regression Tree is very fast to train, it is limited to encoder regions with axis-parallel splits. On the other hand, DA is known to provide state-of-the-art performance. However, its training complexity grows with the number of sources that have different data distributions, due to over-parametrization. Our experiments on several synthetic and one real-world remote sensing problem show that DA has limited application potential as it is highly impractical to train even in a four-source setting, while DSLVQ is as simple and fast to train as the Regression Tree. In addition, DSLVQ shows similar performance to DA in experiments with small number of sources and outperforms DA in experiments with large number of sources, while consistently outperforming the Regression Tree algorithm. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Decentralized Estimation using distortion sensitive learning vector quantization", "paper_id": "WOS:000318889800002"}