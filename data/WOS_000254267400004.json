{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "infinite_web"}, {"score": 0.004257360656055417, "phrase": "dynamic_pages"}, {"score": 0.004014923642015515, "phrase": "web_pages"}, {"score": 0.0037862398004482253, "phrase": "web_search_engines"}, {"score": 0.0035705347414368726, "phrase": "indexing_web_pages"}, {"score": 0.0032505921489759224, "phrase": "\"infinite\"_web_sites"}, {"score": 0.0030295361111006866, "phrase": "deep_users"}, {"score": 0.0026781477395106993, "phrase": "significant_portion"}, {"score": 0.0026313840069327713, "phrase": "web_site_content"}, {"score": 0.0025254115243261875, "phrase": "proposed_models"}, {"score": 0.0024667783849055634, "phrase": "real_data"}, {"score": 0.0024379730432321656, "phrase": "page_views"}, {"score": 0.0021049977753042253, "phrase": "start_page"}], "paper_keywords": ["user sessions", " user navigation analysis"], "paper_abstract": "Many publicly available Web pages are generated dynamically upon request, and contain links to other dynamically generated pages. Web sites that are built with dynamic pages can create, in principle, a very large amount of Web pages. This poses a problem for the crawlers of Web search engines, as the network and storage resources required for indexing Web pages are neither infinite nor free. In this article, several probabilistic models for user browsing in \"infinite\" Web sites are proposed and studied. These models aim at predicting how deep users go while exploring; Web sites. We use these models to estimate how deep a crawler must go to download a significant portion of the Web site content that is actually visited. The proposed models are validated against real data on page views in several Web sites, showing that, in both theory and practice, a crawler needs to download just a few levels, no more than 3 to 5 \"clicks\" away from the start page, to reach 90% of the pages that users actually visit.", "paper_title": "Crawling the infinite web", "paper_id": "WOS:000254267400004"}