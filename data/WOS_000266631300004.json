{"auto_keywords": [{"score": 0.04357203863023536, "phrase": "j._nocedal"}, {"score": 0.011170667639921137, "phrase": "j.l._morales"}, {"score": 0.011010540225898063, "phrase": "large-scale_unconstrained_optimization"}, {"score": 0.010965703108686884, "phrase": "comput"}, {"score": 0.010920118166088188, "phrase": "optim"}, {"score": 0.00970788284657821, "phrase": "w.w._hager"}, {"score": 0.009667861353219847, "phrase": "h._zhang"}, {"score": 0.009509402729729936, "phrase": "guaranteed_descent"}, {"score": 0.008993360035439674, "phrase": "hybrid_method"}, {"score": 0.00481495049065317, "phrase": "advanced_large-scale_minimization_algorithms"}, {"score": 0.0047652354078063916, "phrase": "inverse_ill-posed_problems"}, {"score": 0.004628734508930814, "phrase": "ill-posed_inverse_problem"}, {"score": 0.004599994523941615, "phrase": "parabolized_navier-stokes_equation_model"}, {"score": 0.004561950666122737, "phrase": "adjoint_parameter_estimation"}, {"score": 0.004477493263660398, "phrase": "nonlinear_conjugate-gradient"}, {"score": 0.004180919839686364, "phrase": "d.c._liu"}, {"score": 0.004112030817696958, "phrase": "bfgs"}, {"score": 0.004086475264701076, "phrase": "large_scale_minimization"}, {"score": 0.003928731234289177, "phrase": "newton"}, {"score": 0.003863568356785813, "phrase": "s.g_nash"}, {"score": 0.003831591603702392, "phrase": "truncated_newton_methods"}, {"score": 0.0038157807910402367, "phrase": "siam"}, {"score": 0.003807782174484578, "phrase": "j._sci"}, {"score": 0.0036225373842094207, "phrase": "lanczos_method"}, {"score": 0.003577738716688041, "phrase": "anal"}, {"score": 0.0034391233753323855, "phrase": "morales"}, {"score": 0.003424850598599353, "phrase": "nocedal"}, {"score": 0.0031712922100918706, "phrase": "cost_function"}, {"score": 0.00313849462329633, "phrase": "adjoint_method"}, {"score": 0.0031189785992954844, "phrase": "detailed_description"}, {"score": 0.0030995835539697893, "phrase": "algorithmic_form"}, {"score": 0.0030867203659106727, "phrase": "minimization_algorithms"}, {"score": 0.0030611534228319717, "phrase": "minimization_comparison"}, {"score": 0.0030231983220281836, "phrase": "inviscid_case"}, {"score": 0.002973322370940366, "phrase": "hager"}, {"score": 0.00288800021194675, "phrase": "efficient_fine_search"}, {"score": 0.0026575837571549067, "phrase": "appl"}, {"score": 0.002581165831422692, "phrase": "viscous_case"}, {"score": 0.0025228045950882703, "phrase": "cg"}, {"score": 0.0025122917818007164, "phrase": "d.f._shanno"}, {"score": 0.0025018597221349945, "phrase": "k.h._phua"}, {"score": 0.0024554476124885806, "phrase": "unconstrained_multivariate_functions"}, {"score": 0.002445267759793304, "phrase": "acm"}, {"score": 0.002273495463485887, "phrase": "efficient_line_search"}, {"score": 0.0021627301390216964, "phrase": "adequate_choice"}, {"score": 0.0021403407314754637, "phrase": "cg-descent_method"}, {"score": 0.0021049977753042253, "phrase": "l-bfgs_and_t-n_iterations"}], "paper_keywords": ["large-scale minimization methods", " inverse problems", " adjoint parameter estimation", " ill-posed problems"], "paper_abstract": "We compare the performance of several robust large-scale minimization algorithms for the unconstrained minimization of an ill-posed inverse problem. The parabolized Navier-Stokes equation model was used for adjoint parameter estimation. The methods compared consist of three versions of nonlinear conjugate-gradient (CG) method. quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS), the limited-memory quasi-Newton (L-BFGS) [D.C. Liu and J. Nocedal, Oil the limited memory BFGS method for large scale minimization, Math. Program. 45 (1989), pp. 503-528], truncated Newton (T-N) method [S.G Nash, Preconditioning of truncated Newton methods, SIAM J. Sci. Stat. Comput. 6 (1985), pp. 599-616, S.G. Nash. Newton-type minimization via the Lanczos method, SIAM J. Numer. Anal, 21 (1984), pp. 770-788] and a new hybrid algorithm proposed by Morales and Nocedal [J.L. Morales and J. Nocedal, Enriched methods for large-scale unconstrained optimization, Comput. Optim. Apple 21 (2002). pp. 143-154]. For all the methods employed and tested, the gradient of the cost function is obtained via all adjoint method. A detailed description of the algorithmic form of minimization algorithms employed in the minimization comparison is provided. For the inviscid case, the CG-descent method of Hager [W.W. Hager and H. Zhang, A new conjugate gradient method with guaranteed descent and efficient fine search, SIAM J. Optim. 16 (1) (2005), pp. 170192] performed the best followed closely by the hybrid method [J.L. Morales and J. Nocedal. Enriched methods for large-scale unconstrained optimization, Comput. Optim. Appl. 21 (2002),pp. 143-154], while in the viscous case, the hybrid method emerged as the best performed followed by CG [D.F. Shanno and K.H. Phua, Remark on algorithm 500. Minimization of unconstrained multivariate functions. ACM Trails. Math. Softw. 6 (1980). pp. 618-622] and CG-descent [W.W. Hager and H. Zhang, A new conjugate gradient method with guaranteed descent and efficient line search, SIAM J. Optim. 16 (1) (2005), pp. 170-192]. This required all adequate choice of parameters in the CG-descent method as well as controlling the number of L-BFGS and T-N iterations to be interlaced in the hybrid method.", "paper_title": "Comparison of advanced large-scale minimization algorithms for the solution of inverse ill-posed problems", "paper_id": "WOS:000266631300004"}