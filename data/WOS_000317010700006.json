{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "physical_locality"}, {"score": 0.00464811139983244, "phrase": "many-core_architectures"}, {"score": 0.004578382275810468, "phrase": "efficient_way"}, {"score": 0.0044870272550540415, "phrase": "growing_numbers"}, {"score": 0.004331501320993783, "phrase": "energy_and_latency_costs"}, {"score": 0.004202472962684885, "phrase": "parallel_programs"}, {"score": 0.004077272409331917, "phrase": "existing_designs"}, {"score": 0.004016072233242585, "phrase": "functional_communication_layer"}, {"score": 0.0037423521744765075, "phrase": "primary_concern"}, {"score": 0.003594369823549604, "phrase": "cache_coherence"}, {"score": 0.003469676666252421, "phrase": "communication_behavior"}, {"score": 0.0034348484776384643, "phrase": "parallel_applications"}, {"score": 0.0033832583769880576, "phrase": "observed_sharing_patterns"}, {"score": 0.0033492947290751996, "phrase": "considerable_locality"}, {"score": 0.0033156708990372047, "phrase": "shared_data_accesses"}, {"score": 0.0032494292163722065, "phrase": "consecutive_ids"}, {"score": 0.003152531962718477, "phrase": "strong_physical_locality"}, {"score": 0.0031208772639433145, "phrase": "adjacent_cores"}, {"score": 0.0029374939071038146, "phrase": "proximity_coherence"}, {"score": 0.002764856305713066, "phrase": "nearby_caches"}, {"score": 0.0027370835853738626, "phrase": "new_dedicated_links"}, {"score": 0.0025119730908615104, "phrase": "careful_analysis"}, {"score": 0.0023287359930308864, "phrase": "load_misses"}, {"score": 0.002202894673025544, "phrase": "overall_execution_time"}, {"score": 0.0021049977753042253, "phrase": "network-on-chip_traffic"}], "paper_keywords": ["Proximity coherence", " CMP", " cache design", " network-on-chip", " physical locality"], "paper_abstract": "Many-core architectures provide an efficient way of harnessing the growing numbers of transistors available. However, energy and latency costs of communication increasingly limit the parallel programs running on these platforms. Existing designs provide a functional communication layer, but not necessarily the most efficient solution. Due to power limitations, efficiency is now a primary concern that motivates us to look again at cache coherence. First, we analyze the communication behavior of parallel applications. The observed sharing patterns reveal considerable locality of shared data accesses between threads with consecutive IDs. This pattern corresponds to strong physical locality between adjacent cores in a chip-multiprocessor (CMP). This paper explores the design of Proximity Coherence: a novel scheme in which L1 load misses are optimistically forwarded to nearby caches via new dedicated links. We exploit these patterns and improve the efficiency of communication. The results show that careful analysis leads to the design of a more efficient coherence protocol. The protocol reduces the latency of load misses by up to 33 percent (17 percent, on average), improving overall execution time by up to 13 percent. Furthermore, it also reduces network-on-chip traffic by 19 percent and energy consumption by up to 30 percent.", "paper_title": "Designing a Physical Locality Aware Coherence Protocol for Chip-Multiprocessors", "paper_id": "WOS:000317010700006"}