{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sparse_recovery"}, {"score": 0.004611554879923759, "phrase": "sparsest_solution"}, {"score": 0.004527024978801684, "phrase": "underdetermined_system"}, {"score": 0.00430907756010007, "phrase": "efficient_sparse_recovery_algorithms"}, {"score": 0.004230068512904886, "phrase": "novel_viewpoint"}, {"score": 0.004178198896815672, "phrase": "convex_conjugacy"}, {"score": 0.0039040344230094164, "phrase": "convex_conjugated_loss_functions"}, {"score": 0.003832423454781517, "phrase": "smooth_approximation"}, {"score": 0.0036477941360224435, "phrase": "additive_form"}, {"score": 0.0034720284451902083, "phrase": "loss_functions"}, {"score": 0.0033664873319204027, "phrase": "sparse_recovery_problem"}, {"score": 0.0033047037762521984, "phrase": "augmented_quadratic_constraint_problem"}, {"score": 0.0031649018957020337, "phrase": "alternate_minimization"}, {"score": 0.0029937855454238507, "phrase": "auxiliary_vector"}, {"score": 0.0029570400446722223, "phrase": "hq"}, {"score": 0.0029207251108325006, "phrase": "minimizer_function"}, {"score": 0.0026458370516131255, "phrase": "feasible_and_sparser_solution"}, {"score": 0.0025812459787060097, "phrase": "extensive_experiments"}, {"score": 0.0025495426598675583, "phrase": "random_sparse_signals"}, {"score": 0.002518227743688769, "phrase": "robust_face_recognition"}, {"score": 0.0022670785913397637, "phrase": "minimization_algorithms"}, {"score": 0.0022117133940361025, "phrase": "computational_cost"}, {"score": 0.0021845387990972543, "phrase": "estimation_error"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sparse representation", " Half-quadratic minimization", " L1 minimization"], "paper_abstract": "Sparse recovery aims to find the sparsest solution of an underdetermined system X beta=y. This paper studies simple yet efficient sparse recovery algorithms from a novel viewpoint of convex conjugacy. To this end, we induce a family of convex conjugated loss functions as a smooth approximation of l(0)-norm. Then we apply the additive form of half-quadratic (HQ) optimization to solve these loss functions and to reformulate the sparse recovery problem as an augmented quadratic constraint problem that can be efficiently computed by alternate minimization. At each iteration, we compute the auxiliary vector of HQ via minimizer function and then we project this vector into the nullspace of the homogeneous linear system X beta=0 such that a feasible and sparser solution is obtained. Extensive experiments on random sparse signals and robust face recognition corroborate our claims and validate that our method outperforms the state-of-the-art l(1) minimization algorithms in terms of computational cost and estimation error. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "A fast convex conjugated algorithm for sparse recovery", "paper_id": "WOS:000320476500019"}