{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "fusion_of_multiple_behaviors_using_layered_reinforcement_learning"}, {"score": 0.00441883624011048, "phrase": "new_tasks"}, {"score": 0.004371660146981191, "phrase": "human_demonstration"}, {"score": 0.004324985524216987, "phrase": "independent_practice"}, {"score": 0.004255902303152008, "phrase": "proposed_process"}, {"score": 0.00409895142748329, "phrase": "first_phase"}, {"score": 0.004055176557424983, "phrase": "state-action_data"}, {"score": 0.003969018740062018, "phrase": "human_demonstrations"}, {"score": 0.0038846843242954935, "phrase": "aggregated_state_space"}, {"score": 0.003741371936431574, "phrase": "decision_tree"}, {"score": 0.0036815763368083197, "phrase": "similar_states"}, {"score": 0.0036227329311981195, "phrase": "reinforcement_learning"}, {"score": 0.003451766520526601, "phrase": "tree_induction"}, {"score": 0.003342280102112269, "phrase": "control_policy"}, {"score": 0.0027841357057119317, "phrase": "q-learning_algorithm"}, {"score": 0.0027395968397212053, "phrase": "composed_outputs"}, {"score": 0.002695768552609587, "phrase": "organized_basic_behaviors"}, {"score": 0.002652639568848117, "phrase": "motor_level"}, {"score": 0.002382182688788248, "phrase": "experimental_results"}, {"score": 0.0023314867226678555, "phrase": "learned_complicated_behaviors"}, {"score": 0.0022696277881969896, "phrase": "individual_basic_behaviors"}, {"score": 0.0021049977753042253, "phrase": "dynamic_environment"}], "paper_keywords": ["Behavior-based control", " intelligent robots", " reinforcement learning"], "paper_abstract": "This study introduces a method to enable a robot to learn how to perform new tasks through human demonstration and independent practice. The proposed process consists of two interconnected phases; in the first phase, state-action data are obtained from human demonstrations, and an aggregated state space is learned in terms of a decision tree that groups similar states together through reinforcement learning. Without the postprocess of trimming, in tree induction, the tree encodes a control policy that can be used to control the robot by means of repeatedly improving itself. Once a variety of behaviors is learned, more elaborate behaviors can be generated by selectively organizing several behaviors using another Q-learning algorithm. The composed outputs of the organized basic behaviors on the motor level are weighted using the policy learned through Q-learning. This approach uses three diverse Q-learning algorithms to learn complex behaviors. The experimental results show that the learned complicated behaviors, organized according to individual basic behaviors by the three Q-learning algorithms on different levels, can function more adaptively in a dynamic environment.", "paper_title": "Fusion of Multiple Behaviors Using Layered Reinforcement Learning", "paper_id": "WOS:000305584400018"}