{"auto_keywords": [{"score": 0.033224158496117734, "phrase": "human_parsing"}, {"score": 0.00481495049065317, "phrase": "video_context"}, {"score": 0.004633888966463624, "phrase": "novel_semi-supervised_learning_strategy"}, {"score": 0.004516962628112147, "phrase": "existing_human_parsing_datasets"}, {"score": 0.0043842558719324526, "phrase": "required_tedious_human_labeling"}, {"score": 0.004201381218977942, "phrase": "rich_contexts"}, {"score": 0.004148014765950199, "phrase": "easily_available_web_videos"}, {"score": 0.0040779081518317415, "phrase": "existing_human_parser"}, {"score": 0.003958049372486465, "phrase": "large_number"}, {"score": 0.0039244530915794025, "phrase": "unlabeled_videos"}, {"score": 0.0037446875860947916, "phrase": "cross-frame_contexts"}, {"score": 0.0036813725040313002, "phrase": "human_pose_co-estimation"}, {"score": 0.003527712410706358, "phrase": "satisfactory_human_parsing_results"}, {"score": 0.003409399407390298, "phrase": "sift_flow"}, {"score": 0.0033804443117334535, "phrase": "super-pixel_matching"}, {"score": 0.0032810180076973806, "phrase": "different_frames"}, {"score": 0.00317095235382607, "phrase": "pose_estimation"}, {"score": 0.003117307618682836, "phrase": "individual_frames"}, {"score": 0.0030645676308053444, "phrase": "parsed_video_frames"}, {"score": 0.0029998918671747168, "phrase": "reference_corpus"}, {"score": 0.002961741486178455, "phrase": "non-parametric_human_parsing_component"}, {"score": 0.0028139168128067343, "phrase": "video_co-parsing"}, {"score": 0.002754516260621121, "phrase": "active_learning_method"}, {"score": 0.0027194778713860715, "phrase": "human_guidance"}, {"score": 0.002572709810193776, "phrase": "pose_estimation_results"}, {"score": 0.0024969812322112174, "phrase": "reliable_frames"}, {"score": 0.002465210678474529, "phrase": "seed_frames"}, {"score": 0.0023221983158528163, "phrase": "human_feedback"}, {"score": 0.0022828795331819025, "phrase": "better_fashion_parser"}, {"score": 0.0022634699331607615, "phrase": "extensive_experiments"}, {"score": 0.0021688641066186817, "phrase": "fashion_icon"}, {"score": 0.002123051344001408, "phrase": "encouraging_performance_gain"}], "paper_keywords": ["Information retrieval", " professional communication"], "paper_abstract": "In this paper, we propose a novel semi-supervised learning strategy to address human parsing. Existing human parsing datasets are relatively small due to the required tedious human labeling. We present a general, affordable and scalable solution, which harnesses the rich contexts in those easily available web videos to boost any existing human parser. First, we crawl a large number of unlabeled videos from the web. Then for each video, the cross-frame contexts are utilized for human pose co-estimation, and then video co-parsing to obtain satisfactory human parsing results for all frames. More specifically, SIFT flow and super-pixel matching are used to build correspondences across different frames, and these correspondences then contextualize the pose estimation and human parsing in individual frames. Finally these parsed video frames are used as the reference corpus for the non-parametric human parsing component of the whole solution. To further improve the accuracy of video co-parsing, we propose an active learning method to incorporate human guidance, where the labelers are required to assess the accuracies of the pose estimation results of certain selected video frames. Then we take reliable frames as the seed frames to guide the video pose co-estimation. Our human parsing framework can then easily incorporate the human feedback to train a better fashion parser. Extensive experiments on two benchmark fashion datasets as well as a newly collected challenging Fashion Icon dataset well demonstrate the encouraging performance gain from our general pipeline for human parsing.", "paper_title": "Fashion Parsing With Video Context", "paper_id": "WOS:000358233000019"}