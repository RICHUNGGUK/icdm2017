{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "wallace"}, {"score": 0.048218523365851855, "phrase": "snob"}, {"score": 0.008519484907284998, "phrase": "unsupervised_learning"}, {"score": 0.007112331615020068, "phrase": "freeman"}, {"score": 0.00447341965028879, "phrase": "snob_program"}, {"score": 0.004039995317682942, "phrase": "minimum_message_length_principle"}, {"score": 0.0035869274480340727, "phrase": "compact_coding"}, {"score": 0.003546513036085932, "phrase": "j._roy."}, {"score": 0.0031308163228919773, "phrase": "class_parameters"}, {"score": 0.0026411384812121503, "phrase": "dowe"}, {"score": 0.0024534188907003726, "phrase": "mixture_modelling"}, {"score": 0.002215274527909304, "phrase": "particular_attention"}, {"score": 0.0021049977753042253, "phrase": "definite_assignment"}], "paper_keywords": ["computer program", " EM algorithm", " minimum message length", " mixture model", " cluster analysis"], "paper_abstract": "We describe the Snob program for unsupervised learning as it has evolved from its beginning in the 1960s until its present form. Snob uses the minimum message length principle expounded in Wallace and Freeman (Wallace, C.S. and Freeman, P.R. (1987) Estimation and inference by Compact coding. J. Roy. Statist. Soc. Ser. B, 49, 240-252.) and we indicate how Snob estimates class parameters using the approach of that paper. We will survey the evolution of Snob from these beginnings to the state that it has reached as described by Wallace and Dowe (Wallace, C.S. and Dowe, D.L. (2000) MMM mixture modelling of multi-state, Poisson, Von Mises Circular and Gaussian distributions. Stat. Comput., 10, 73-83.) We pay particular attention to the revision of Snob in the 1980s where definite assignment of things to classes was abandoned.", "paper_title": "Wallace's approach to unsupervised learning: The Snob program", "paper_id": "WOS:000258862500004"}