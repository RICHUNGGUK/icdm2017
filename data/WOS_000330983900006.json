{"auto_keywords": [{"score": 0.049352273134126406, "phrase": "test_suites"}, {"score": 0.012811375745116071, "phrase": "test_suite"}, {"score": 0.011829781319488851, "phrase": "source_code"}, {"score": 0.010555144011309176, "phrase": "test_cases"}, {"score": 0.00879267962069338, "phrase": "existing_static_black-box_tcp_techniques"}, {"score": 0.00481495049065317, "phrase": "topic_models"}, {"score": 0.004778258911940497, "phrase": "software_development_teams"}, {"score": 0.004375789703278455, "phrase": "source_code_change"}, {"score": 0.004195418010131832, "phrase": "development_teams"}, {"score": 0.0038125469099459905, "phrase": "static_black-box_test_case_prioritization"}, {"score": 0.0037835262601707003, "phrase": "tcp"}, {"score": 0.003347105144425849, "phrase": "previously_unused_data_source"}, {"score": 0.003018196463874475, "phrase": "text_analysis_algorithm"}, {"score": 0.002949598537993663, "phrase": "linguistic_data"}, {"score": 0.0028715299874549245, "phrase": "test_case"}, {"score": 0.002806256131739047, "phrase": "high_priority"}, {"score": 0.0027635643494432365, "phrase": "different_functionalities"}, {"score": 0.0027319713065986654, "phrase": "sut."}, {"score": 0.0026191799769765085, "phrase": "multiple_real-world_open_source_systems"}, {"score": 0.0025694584194998356, "phrase": "apache_ant"}, {"score": 0.00254983442025512, "phrase": "apache_derby"}, {"score": 0.002444544487772349, "phrase": "comparable_or_better_performance"}, {"score": 0.0024073421672242486, "phrase": "static_black-box_tcp_methods"}, {"score": 0.0021789194955896124, "phrase": "sut_runtime_behavior"}, {"score": 0.002145750895912557, "phrase": "sut_specification_models"}, {"score": 0.0021049977753042253, "phrase": "sut_source_code"}], "paper_keywords": ["Testing and debugging", " Test case prioritization", " Topic models"], "paper_abstract": "Software development teams use test suites to test changes to their source code. In many situations, the test suites are so large that executing every test for every source code change is infeasible, due to time and resource constraints. Development teams need to prioritize their test suite so that as many distinct faults as possible are detected early in the execution of the test suite. We consider the problem of static black-box test case prioritization (TCP), where test suites are prioritized without the availability of the source code of the system under test (SUT). We propose a new static black-box TCP technique that represents test cases using a previously unused data source in the test suite: the linguistic data of the test cases, i.e., their identifier names, comments, and string literals. Our technique applies a text analysis algorithm called topic modeling to the linguistic data to approximate the functionality of each test case, allowing our technique to give high priority to test cases that test different functionalities of the SUT. We compare our proposed technique with existing static black-box TCP techniques in a case study of multiple real-world open source systems: several versions of Apache Ant and Apache Derby. We find that our static black-box TCP technique outperforms existing static black-box TCP techniques, and has comparable or better performance than two existing execution-based TCP techniques. Static black-box TCP methods are widely applicable because the only input they require is the source code of the test cases themselves. This contrasts with other TCP techniques which require access to the SUT runtime behavior, to the SUT specification models, or to the SUT source code.", "paper_title": "Static test case prioritization using topic models", "paper_id": "WOS:000330983900006"}