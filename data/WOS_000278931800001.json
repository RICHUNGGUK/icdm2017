{"auto_keywords": [{"score": 0.03400226043083397, "phrase": "user's_context"}, {"score": 0.00481495049065317, "phrase": "contextual_information_retrieval_effectiveness"}, {"score": 0.004657739551687038, "phrase": "increasing_prominence"}, {"score": 0.004555781299332848, "phrase": "wide_range"}, {"score": 0.00447251540886613, "phrase": "electronic_media"}, {"score": 0.004423284899590467, "phrase": "traditional_information_retrieval_systems"}, {"score": 0.0037325918533747953, "phrase": "growing_interest"}, {"score": 0.003705130724701598, "phrase": "contextual_information_retrieval"}, {"score": 0.0035708152831417104, "phrase": "user's_search_background"}, {"score": 0.003353521531649663, "phrase": "retrieval_accuracy"}, {"score": 0.0033288399506044763, "phrase": "contextual_information_retrieval_systems"}, {"score": 0.0032800186243040663, "phrase": "different_definitions"}, {"score": 0.003243871564587106, "phrase": "core_concept"}, {"score": 0.003091767402370757, "phrase": "document_relevance_measurement"}, {"score": 0.002850385670169814, "phrase": "evaluation_methodologies"}, {"score": 0.0027982016314450717, "phrase": "traditional_information_retrieval"}, {"score": 0.0027368368719283298, "phrase": "evaluation_campaigns"}, {"score": 0.0026277995163948263, "phrase": "information_retrieval_process"}, {"score": 0.002551231163230566, "phrase": "critical_review"}, {"score": 0.0025324393547676623, "phrase": "existing_evaluation_methodologies"}, {"score": 0.0025137856138000014, "phrase": "contextual_information_retrieval_area"}, {"score": 0.002431525473795068, "phrase": "standard_evaluation_frameworks"}, {"score": 0.0023606625824479956, "phrase": "comprehensive_survey"}, {"score": 0.002343271162668978, "phrase": "contextual_information_retrieval_evaluation_methodologies"}, {"score": 0.0022250584824198218, "phrase": "retrieval_effectiveness"}, {"score": 0.0021601997088758957, "phrase": "research_challenges"}, {"score": 0.002120624523437494, "phrase": "substantive_research_area"}, {"score": 0.0021049977753042253, "phrase": "future_research"}], "paper_keywords": ["Context", " Profile", " Relevance", " Empirical evaluation"], "paper_abstract": "The increasing prominence of information arising from a wide range of sources delivered over electronic media has made traditional information retrieval systems less effective. Indeed, users are overwhelmed by the information delivered by such systems in response to their queries, particularly when the latter are ambiguous. In order to tackle this problem, the state-of-the-art reveals that there is a growing interest towards contextual information retrieval which relies on various sources of evidence issued from the user's search background and environment like interests, preferences, time and location, in order to improve the retrieval accuracy. Contextual information retrieval systems are based on different definitions of the core concept of user's context, various user's context modeling approaches and several techniques of document relevance measurement, but all share the goal of providing the most useful information to the users in accordance with their context. However, the evaluation methodologies conceived in the past several years for traditional information retrieval and widely used in the evaluation campaigns have been challenged by the consideration of user's context in the information retrieval process. Thus, we recognize that a critical review of existing evaluation methodologies in contextual information retrieval area is needed in order to design and develop standard evaluation frameworks. We present in this paper a comprehensive survey of contextual information retrieval evaluation methodologies and provide insights into how and why they are appropriate to measure the retrieval effectiveness. We also highlight some of the research challenges ahead that would constitute substantive research area for future research.", "paper_title": "Evaluation of contextual information retrieval effectiveness: overview of issues and research", "paper_id": "WOS:000278931800001"}