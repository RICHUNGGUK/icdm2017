{"auto_keywords": [{"score": 0.04987640488714887, "phrase": "positive_bayesian_network_classifiers"}, {"score": 0.026309879001410744, "phrase": "positive_class"}, {"score": 0.004814978380654875, "phrase": "wrapper"}, {"score": 0.004714023408114857, "phrase": "information_retrieval_framework"}, {"score": 0.0044424917428102445, "phrase": "particular_class"}, {"score": 0.004404994123834589, "phrase": "big_sets"}, {"score": 0.0043678116213858, "phrase": "unlabelled_objects"}, {"score": 0.004064117897000019, "phrase": "machine_learning_community"}, {"score": 0.0039119492240482, "phrase": "binary_classifiers"}, {"score": 0.003829877278269543, "phrase": "negative_examples"}, {"score": 0.0036090884921108086, "phrase": "bayesian_network_classifiers"}, {"score": 0.003578600071301459, "phrase": "positive_and_unlabelled_examples"}, {"score": 0.003533347967582184, "phrase": "main_drawback"}, {"score": 0.003400984514637656, "phrase": "previous_knowledge"}, {"score": 0.0031912706211861324, "phrase": "wrapper_approach"}, {"score": 0.0029817668105720924, "phrase": "optimal_value"}, {"score": 0.0028944757265206332, "phrase": "positive_examples"}, {"score": 0.002809732883291955, "phrase": "positive_unlabelled_learning_problems"}, {"score": 0.0027741761880779535, "phrase": "non-trivial_question"}, {"score": 0.002603012037901889, "phrase": "new_guiding_metric"}, {"score": 0.002505412457777352, "phrase": "optimal_a_priori_probability"}, {"score": 0.0023408288327263316, "phrase": "proposed_metric"}, {"score": 0.0023111922860677672, "phrase": "wrapper_classifiers"}, {"score": 0.002177762848946681, "phrase": "wrapper_bayesian_network"}, {"score": 0.0021501863832966966, "phrase": "competitive_results"}, {"score": 0.0021049977753042253, "phrase": "actual_a_priori_probability"}], "paper_keywords": ["Positive unlabelled learning", " Bayesian network classifiers", " Wrapper classifiers", " Classifier evaluation", " Pseudo F"], "paper_abstract": "In the information retrieval framework, there are problems where the goal is to recover objects of a particular class from big sets of unlabelled objects. In some of these problems, only examples from the class we want to recover are available. For such problems, the machine learning community has developed algorithms that are able to learn binary classifiers in the absence of negative examples. Among them, we can find the positive Bayesian network classifiers, algorithms that induce Bayesian network classifiers from positive and unlabelled examples. The main drawback of these algorithms is that they require some previous knowledge about the a priori probability distribution of the class. In this paper, we propose a wrapper approach to tackle the learning when no such information is available, setting this probability at the optimal value in terms of the recovery of positive examples. The evaluation of classifiers in positive unlabelled learning problems is a non-trivial question. We have also worked on this problem, and we have proposed a new guiding metric to be used in the search for the optimal a priori probability of the positive class that we have called the pseudo F. We have empirically tested the proposed metric and the wrapper classifiers on both synthetic and real-life datasets. The results obtained in this empirical comparison show that the wrapper Bayesian network classifiers provide competitive results, particularly when the actual a priori probability of the positive class is high.", "paper_title": "Wrapper positive Bayesian network classifiers", "paper_id": "WOS:000310871700006"}