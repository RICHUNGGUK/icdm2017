{"auto_keywords": [{"score": 0.011328424034649669, "phrase": "filter_approaches"}, {"score": 0.009648093092571468, "phrase": "target_classifier"}, {"score": 0.008853726988527, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "predictive_forward_selection"}, {"score": 0.004725385523804256, "phrase": "widely_used_pattern_classification_algorithms"}, {"score": 0.004654928974748418, "phrase": "support_vector_machines"}, {"score": 0.004466519142879166, "phrase": "irrelevant_or_redundant_features"}, {"score": 0.004416465443332974, "phrase": "training_data"}, {"score": 0.004096751549280256, "phrase": "achieved_accuracy"}, {"score": 0.003975416846030462, "phrase": "feature_selection_algorithms"}, {"score": 0.0035514484574435574, "phrase": "considered_feature_subset"}, {"score": 0.0033315236647127734, "phrase": "higher_performance"}, {"score": 0.003113441660343867, "phrase": "widely_used_wrapper_approach"}, {"score": 0.002976019715894587, "phrase": "required_evaluations"}, {"score": 0.0028877818105020434, "phrase": "experience_knowledge"}, {"score": 0.0023566987138026285, "phrase": "presented_method"}, {"score": 0.0023127544795063263, "phrase": "traditional_wrapper_approaches"}, {"score": 0.002295406551990453, "phrase": "significantly_less_evaluations"}, {"score": 0.0022696277881969896, "phrase": "target_algorithm"}, {"score": 0.002218933616777076, "phrase": "statistically_significant_better_results"}, {"score": 0.0021530945641993152, "phrase": "embedded_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feature selection", " Meta-learning", " SVM", " Classification", " Forward selection"], "paper_abstract": "Most of the widely used pattern classification algorithms, such as Support Vector Machines (SVM), are sensitive to the presence of irrelevant or redundant features in the training data. Automatic feature selection algorithms aim at selecting a subset of features present in a given dataset so that the achieved accuracy of the following classifier can be maximized. Feature selection algorithms are generally categorized into two broad categories: algorithms that do not take the following classifier into account (the filter approaches), and algorithms that evaluate the following classifier for each considered feature subset (the wrapper approaches). Filter approaches are typically faster, but wrapper approaches deliver a higher performance. In this paper, we present the algorithm - Predictive Forward Selection - based on the widely used wrapper approach forward selection. Using ideas from meta-learning, the number of required evaluations of the target classifier is reduced by using experience knowledge gained during past feature selection runs on other datasets. We have evaluated our approach on 59 real-world datasets with a focus on SVM as the target classifier. We present comparisons with state-of-the-art wrapper and filter approaches as well as one embedded method for SVM according to accuracy and run-time. The results show that the presented method reaches the accuracy of traditional wrapper approaches requiring significantly less evaluations of the target algorithm. Moreover, our method achieves statistically significant better results than the filter approaches as well as the embedded method. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Efficient feature size reduction via predictive forward selection", "paper_id": "WOS:000331669300010"}