{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "wall-floor_features"}, {"score": 0.049477536567807595, "phrase": "autonomous_flight"}, {"score": 0.04908067956118528, "phrase": "man-made_environments"}, {"score": 0.004481954307639453, "phrase": "man-made_indoor_environments"}, {"score": 0.004425616928975799, "phrase": "micro_aerial_vehicle"}, {"score": 0.004296889776409743, "phrase": "frontal_camera"}, {"score": 0.004242868712089958, "phrase": "downward-facing_sonar"}, {"score": 0.004189523950071494, "phrase": "odometry_inputs"}, {"score": 0.004067635935640407, "phrase": "distant_features"}, {"score": 0.003818210967854446, "phrase": "parallel_tracking"}, {"score": 0.0037070855410818986, "phrase": "wall_structure"}, {"score": 0.003660452313567116, "phrase": "lateral_collisions"}, {"score": 0.0035240305920962766, "phrase": "traditional_monocular_slam_approaches"}, {"score": 0.0033926758710459866, "phrase": "feature-poor_environments"}, {"score": 0.0032115093493071366, "phrase": "common_dependency"}, {"score": 0.0031845067058084583, "phrase": "feature-rich_environments"}, {"score": 0.003027178161479457, "phrase": "low-dimensional_landmarks"}, {"score": 0.0029020077365708966, "phrase": "geometric_structure"}, {"score": 0.002556625752785268, "phrase": "difficult_robot_motions"}, {"score": 0.0024926348205500715, "phrase": "visual_data"}, {"score": 0.002471661547432869, "phrase": "odometry_measurements"}, {"score": 0.0024405312482831646, "phrase": "principled_manner"}, {"score": 0.002242758902593467, "phrase": "completely_featureless_environment"}, {"score": 0.0021773853011112882, "phrase": "small_commercially_available_quad-rotor_platform"}, {"score": 0.0021408862709571615, "phrase": "typical_feature-poor_indoor_environment"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Quadrotor", " MAV", " Autonomous navigation", " Indoor", " SLAM", " Monocular", " Parallel tracking and mapping", " Vistas", " Wall-Floor Features"], "paper_abstract": "We propose a solution towards the problem of autonomous flight in man-made indoor environments with a micro aerial vehicle (MAV), using a frontal camera, a downward-facing sonar, and odometry inputs. While steering an MAV towards distant features that we call vistas, we build a map of the environment in a parallel tracking and mapping fashion to infer the wall structure and avoid lateral collisions in real-time. Our framework overcomes the limitations of traditional monocular SLAM approaches that are prone to failure when operating in feature-poor environments and when the camera purely rotates. First, we overcome the common dependency on feature-rich environments by detecting Wall-Floor Features (WFFs), a novel type of low-dimensional landmarks that are specifically designed for man-made environments to capture the geometric structure of the scene. We show that WFFs not only reveal the structure of the scene, but can also be tracked reliably. Second, we cope with difficult robot motions and environments by fusing the visual data with odometry measurements in a principled manner. This allows the robot to continue tracking when it purely rotates and when it temporarily navigates across a completely featureless environment. We demonstrate our results on a small commercially available quad-rotor platform flying in a typical feature-poor indoor environment. Published by Elsevier B.V.", "paper_title": "Vistas and parallel tracking and mapping with Wall-Floor Features: Enabling autonomous flight in man-made environments", "paper_id": "WOS:000342266700006"}