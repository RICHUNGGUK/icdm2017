{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "linear_regressions"}, {"score": 0.004415675459182871, "phrase": "linear_regression_models"}, {"score": 0.004278807087784626, "phrase": "maximum_likelihood"}, {"score": 0.004178935413340453, "phrase": "expectation_maximization"}, {"score": 0.003626626504149897, "phrase": "maximum_likelihood_estimates"}, {"score": 0.0028405715222684183, "phrase": "simulation_study"}, {"score": 0.002407078478216306, "phrase": "simulated_data_sets"}, {"score": 0.0023694065215394593, "phrase": "simulation_results"}, {"score": 0.0021049977753042253, "phrase": "true_regression_lines"}], "paper_keywords": ["mixtures of linear regressions", " maximum likelihood estimation", " EM algorithm", " classification EM algorithm", " simulation study"], "paper_abstract": "In most applications, the parameters of a mixture of linear regression models are estimated by maximum likelihood using the expectation maximization (EM) algorithm. In this article, we propose the comparison of three algorithms to compute maximum likelihood estimates of the parameters of these models: the EM algorithm, the classification EM algorithm and the stochastic EM algorithm. The comparison of the three procedures was done through a simulation study of the performance (computational effort, statistical properties of estimators and goodness of fit) of these approaches on simulated data sets. Simulation results show that the choice of the approach depends essentially on the configuration of the true regression lines and the initialization of the algorithms.", "paper_title": "Fitting mixtures of linear regressions", "paper_id": "WOS:000273913800016"}