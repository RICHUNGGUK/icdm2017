{"auto_keywords": [{"score": 0.02246846748295755, "phrase": "gaussian"}, {"score": 0.00481495049065317, "phrase": "gaussian_process_framework"}, {"score": 0.004767826806287385, "phrase": "multi-task_learning"}, {"score": 0.004428728205625129, "phrase": "individual_learning_tasks"}, {"score": 0.004385367803204002, "phrase": "gaussian_process_models"}, {"score": 0.004154332348800334, "phrase": "different_data_sets"}, {"score": 0.004073359218158693, "phrase": "joint_priors"}, {"score": 0.003896871009090446, "phrase": "previous_gaussian_process_models"}, {"score": 0.003247868216333148, "phrase": "target_task"}, {"score": 0.0031223774204093713, "phrase": "focused_gaussian_process_model"}, {"score": 0.0030614565460256897, "phrase": "\"explaining_away\"_model"}, {"score": 0.002986969236641105, "phrase": "additional_tasks"}, {"score": 0.0026538282523433684, "phrase": "key_problem"}, {"score": 0.0026277995163948263, "phrase": "negative_transfer"}, {"score": 0.0023117182131967523, "phrase": "single-task_learning"}, {"score": 0.0022223176391038785, "phrase": "hierarchical_dirichlet_processes"}, {"score": 0.002157538586313944, "phrase": "predictive_structure_learning"}, {"score": 0.002125859121671525, "phrase": "symmetric_multi-task_learning"}], "paper_keywords": ["Gaussian processes", " Multi-task learning", " Transfer learning", " Negative transfer"], "paper_abstract": "Multi-task learning, learning of a set of tasks together, can improve performance in the individual learning tasks. Gaussian process models have been applied to learning a set of tasks on different data sets, by constructing joint priors for functions underlying the tasks. In these previous Gaussian process models, the setting has been symmetric in the sense that all the tasks have been assumed to be equally important, whereas in settings such as transfer learning the goal is asymmetric, to enhance performance in a target task given the other tasks. We propose a focused Gaussian process model which introduces an \"explaining away\" model for each of the additional tasks to model their non-related variation, in order to focus the transfer to the task-of-interest. This focusing helps reduce the key problem of negative transfer, which may cause performance to even decrease if the tasks are not related closely enough. In experiments, our model improves performance compared to single-task learning, symmetric multi-task learning using hierarchical Dirichlet processes, transfer learning based on predictive structure learning, and symmetric multi-task learning with Gaussian processes.", "paper_title": "Focused multi-task learning in a Gaussian process framework", "paper_id": "WOS:000307717100007"}