{"auto_keywords": [{"score": 0.039510674444560676, "phrase": "proposed_ispgp_algorithm"}, {"score": 0.02057340672367024, "phrase": "snelson"}, {"score": 0.00965625332898315, "phrase": "gp"}, {"score": 0.008826526056243431, "phrase": "gaussian_process"}, {"score": 0.00481495049065317, "phrase": "novel_method"}, {"score": 0.004613516457919661, "phrase": "whole_training_data"}, {"score": 0.004441517985511521, "phrase": "sparse_approximation"}, {"score": 0.004215398078430739, "phrase": "ghahramani"}, {"score": 0.004116440453049531, "phrase": "proposed_method"}, {"score": 0.003962899903266524, "phrase": "ispgp"}, {"score": 0.003815064377019477, "phrase": "ghahramani's_work"}, {"score": 0.0034690732089830045, "phrase": "online_incremental_training_data"}, {"score": 0.003355515315708125, "phrase": "likelihood_weighting_scheme"}, {"score": 0.0032302647179383915, "phrase": "representational_power"}, {"score": 0.0031096747349602344, "phrase": "incremental_learning_algorithm"}, {"score": 0.0029935730127943496, "phrase": "infinite_data"}, {"score": 0.0028955350879319355, "phrase": "sparse_pseudo-input"}, {"score": 0.0025586128304269616, "phrase": "conventional_gp_algorithm"}, {"score": 0.0024984517279573906, "phrase": "training_data"}, {"score": 0.002416589459425828, "phrase": "computational_cost"}, {"score": 0.0023936956017015696, "phrase": "memory_requirement"}, {"score": 0.002304263635960015, "phrase": "large_training_data"}, {"score": 0.0022715929119140194, "phrase": "significant_performance_degradation"}, {"score": 0.0021557052355026048, "phrase": "ghahramani's_spgp_algorithm"}, {"score": 0.0021049977753042253, "phrase": "performance_degradation"}], "paper_keywords": ["Gaussian process regression", " incremental learning", " pseudo-data"], "paper_abstract": "In this paper, we devise a novel method that incrementally learns pseudo-data, which represent the whole training data set for Gaussian Process (GP) regression. The method involves sparse approximation of the GP by extending the work of Snelson and Ghahramani. We call the proposed method Incremental Sparse Pseudo-input Gaussian Process (ISPGP) regression. Unlike the Snelson and Ghahramani's work, the proposed ISPGP algorithm allows for training from either a huge amount of training data by scanning through it only once or an online incremental training data set. We also design a likelihood weighting scheme to incrementally determine pseudo-data while maintaining the representational power. Due to the nature of the incremental learning algorithm, the proposed ISPGP algorithm can theoretically work with infinite data to which the conventional GP or Sparse Pseudo-input Gaussian Process ( SPGP) algorithm is not applicable. From our experimental results on the KIN40K data set, we can see that the proposed ISPGP algorithm is comparable to the conventional GP algorithm using the same number of training data. It also significantly reduces the computational cost and memory requirement in regression and is scalable to a large training data set without significant performance degradation. Although the proposed ISPGP algorithm performs slightly worse than Snelson and Ghahramani's SPGP algorithm, the level of performance degradation is acceptable.", "paper_title": "INCREMENTAL SPARSE PSEUDO-INPUT GAUSSIAN PROCESS REGRESSION", "paper_id": "WOS:000315523100001"}