{"auto_keywords": [{"score": 0.04959913342990779, "phrase": "mapreduce"}, {"score": 0.007548565862980879, "phrase": "mpi"}, {"score": 0.005811740368624667, "phrase": "map_function"}, {"score": 0.00481495049065317, "phrase": "mro-mpi"}, {"score": 0.004624751696958501, "phrase": "optimized_data_exchange_policy"}, {"score": 0.004509694461067052, "phrase": "programming_model"}, {"score": 0.004419703921314433, "phrase": "large-scale_data"}, {"score": 0.004266501496765747, "phrase": "message_passing_interface"}, {"score": 0.004077272409331917, "phrase": "algorithmic_parallelization"}, {"score": 0.003955787027818301, "phrase": "efficient_communication_infrastructure"}, {"score": 0.003876806615011266, "phrase": "original_implementation"}, {"score": 0.003780286517897356, "phrase": "reduce_function"}, {"score": 0.0033492947290751996, "phrase": "whole_running_time"}, {"score": 0.0030895394261534776, "phrase": "adapted_structure"}, {"score": 0.0030431198971423937, "phrase": "mapreduce_programming_model"}, {"score": 0.003012560489169148, "phrase": "fast_intensive_data_processing"}, {"score": 0.0028355237433457313, "phrase": "reduce_functions"}, {"score": 0.002750934994181092, "phrase": "partial_intermediate_data"}, {"score": 0.0026823700218343506, "phrase": "pipeline_fashion"}, {"score": 0.002655426270718136, "phrase": "mpi."}, {"score": 0.0024617483174454113, "phrase": "experimental_results"}, {"score": 0.0023762535034673017, "phrase": "distributed_inverted_indexing"}, {"score": 0.0023523750536537102, "phrase": "distributed_approximate_similarity_search"}, {"score": 0.0023053339328456234, "phrase": "good_speedup"}, {"score": 0.0022592313766072658, "phrase": "earlier_versions"}, {"score": 0.0022029011679222136, "phrase": "hadoop"}, {"score": 0.0021697677572213086, "phrase": "available_mpi-mapreduce_implementations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Map Reduce overlapping", " MPI-MapReduce", " Parallel Map Reduce", " Big data", " Large scale data processing"], "paper_abstract": "MapReduce is a programming model proposed to simplify large-scale data processing. In contrast, the message passing interface (MPI) standard is extensively used for algorithmic parallelization, as it accommodates an efficient communication infrastructure. In the original implementation of MapReduce, the reduce function can only start processing following termination of the map function. If the map function is slow for any reason, this will affect the whole running time. In this paper, we propose MapReduce overlapping using MPI, which is an adapted structure of the MapReduce programming model for fast intensive data processing. Our implementation is based on running the map and the reduce functions concurrently in parallel by exchanging partial intermediate data between them in a pipeline fashion using MPI. At the same time, we maintain the usability and the simplicity of MapReduce. Experimental results based on three different applications (WordCount, Distributed Inverted Indexing and Distributed Approximate Similarity Search) show a good speedup compared to the earlier versions of MapReduce such as Hadoop and the available MPI-MapReduce implementations. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "MRO-MPI: MapReduce overlapping using MPI and an optimized data exchange policy", "paper_id": "WOS:000329269000008"}