{"auto_keywords": [{"score": 0.04623023541681038, "phrase": "prediction_error"}, {"score": 0.0048149576419795614, "phrase": "dopamine"}, {"score": 0.004257806363128601, "phrase": "formal_models"}, {"score": 0.004216112320418982, "phrase": "animal_learning"}, {"score": 0.004174824849669107, "phrase": "current_hypotheses"}, {"score": 0.004133940019630129, "phrase": "dopamine_function"}, {"score": 0.003916098498242888, "phrase": "model-free_reinforcement_learning_method"}, {"score": 0.003709693561373117, "phrase": "important_rescorla-wagner"}, {"score": 0.003496868378204657, "phrase": "model-based_adaptation"}, {"score": 0.003378385493203297, "phrase": "good_account"}, {"score": 0.0033452741392850523, "phrase": "empirical_data"}, {"score": 0.003296212583539166, "phrase": "dopamine_neuron_firing_patterns"}, {"score": 0.0031845067058084583, "phrase": "latent_inhibition"}, {"score": 0.003153290472345615, "phrase": "kamin"}, {"score": 0.0030464123943266673, "phrase": "model-free_reinforcement_learning"}, {"score": 0.002928682556953258, "phrase": "phasic_dopamine_functions"}, {"score": 0.0028293966058473476, "phrase": "phasic_dopamine"}, {"score": 0.0028016507334576216, "phrase": "valence-dependent_\"reward\"_processing"}, {"score": 0.0027741761880779535, "phrase": "valence-independent_\"salience\"_processing"}, {"score": 0.0025892330566358503, "phrase": "distal_rewards"}, {"score": 0.002346160830406661, "phrase": "dopamine_dysfunction"}, {"score": 0.0022223176391038785, "phrase": "existing_accounts"}, {"score": 0.0021896891011033105, "phrase": "novel_predictions"}, {"score": 0.002125859121671525, "phrase": "dopamine_neurons"}], "paper_keywords": ["dopamine", " prediction error", " associative learning", " blocking", " latent inhibition", " overshadowing", " schizophrenia", " reinforcement learning", " incentive salience", " motivated behavior", " temporal difference algorithm", " Rescorla-Wagner learning rule", " psychosis"], "paper_abstract": "The notion of prediction error has established itself at the heart of formal models of animal learning and current hypotheses of dopamine function. Several interpretations of prediction error have been offered, including the model-free reinforcement learning method known as temporal difference learning (TD), and the important Rescorla-Wagner (RW) learning rule. Here, we present a model-based adaptation of these ideas that provides a good account of empirical data pertaining to dopamine neuron firing patterns and associative learning paradigms such as latent inhibition, Kamin blocking and overshadowing. Our departure from model-free reinforcement learning also offers: 1) a parsimonious distinction between tonic and phasic dopamine functions; 2) a potential generalization of the role of phasic dopamine from valence-dependent \"reward\" processing to valence-independent \"salience\" processing; 3) an explanation for the selectivity of certain dopamine manipulations on motivation for distal rewards; and 4) a plausible link between formal notions of prediction error and accounts of disturbances of thought in schizophrenia (in which dopamine dysfunction is strongly implicated). The model distinguishes itself from existing accounts by offering novel predictions pertaining to the firing of dopamine neurons in various untested behavioral scenarios.", "paper_title": "Dopamine, prediction error and associative learning: A model-based account", "paper_id": "WOS:000237770700005"}