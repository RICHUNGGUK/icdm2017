{"auto_keywords": [{"score": 0.04550863708222468, "phrase": "target_video"}, {"score": 0.04401425964885099, "phrase": "facial_expressions"}, {"score": 0.00481495049065317, "phrase": "facial_reenactment"}, {"score": 0.004650330504968421, "phrase": "real-time_transfer"}, {"score": 0.0044718206943536514, "phrase": "source_video"}, {"score": 0.004262881313926916, "phrase": "ad-hoc_control"}, {"score": 0.004153041453795458, "phrase": "target_actor"}, {"score": 0.003958937342925219, "phrase": "photo-realistic_re-rendering"}, {"score": 0.003724911315685105, "phrase": "newly-synthesized_expressions"}, {"score": 0.0036288825833675127, "phrase": "real_video"}, {"score": 0.0034742854447532678, "phrase": "facial_performances"}, {"score": 0.003311799410921826, "phrase": "commodity_rgb-d_sensor"}, {"score": 0.003170667618004276, "phrase": "parametric_model"}, {"score": 0.0030223381739019894, "phrase": "input_color"}, {"score": 0.002906138793910919, "phrase": "scene_lighting"}, {"score": 0.002868404184756449, "phrase": "expression_transfer"}, {"score": 0.0027104531025934865, "phrase": "parameter_space"}, {"score": 0.002652039172817654, "phrase": "target_parameters"}, {"score": 0.0025611773887867255, "phrase": "major_challenge"}, {"score": 0.0025279105573394727, "phrase": "convincing_re-rendering"}, {"score": 0.0024950747439601863, "phrase": "synthesized_target_face"}, {"score": 0.002462664392109308, "phrase": "corresponding_video_stream"}, {"score": 0.0024095777415804346, "phrase": "careful_consideration"}, {"score": 0.0023782753249846794, "phrase": "lighting_and_shading_design"}, {"score": 0.0022967713922705, "phrase": "real-world_environment"}, {"score": 0.0022277448907881306, "phrase": "live_setup"}, {"score": 0.0021702292394814144, "phrase": "video_conference_feed"}, {"score": 0.0021049977753042253, "phrase": "different_person"}], "paper_keywords": ["faces", " real-time", " depth camera", " expression transfer"], "paper_abstract": "We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor. The novelty of our approach lies in the transfer and photo-realistic re-rendering of facial deformations and detail into the target video in a way that the newly-synthesized expressions are virtually indistinguishable from a real video. To achieve this, we accurately capture the facial performances of the source and target subjects in real-time using a commodity RGB-D sensor. For each frame, we jointly fit a parametric model for identity, expression, and skin reflectance to the input color and depth data, and also reconstruct the scene lighting. For expression transfer, we compute the difference between the source and target expressions in parameter space, and modify the target parameters to match the source expressions. A major challenge is the convincing re-rendering of the synthesized target face into the corresponding video stream. This requires a careful consideration of the lighting and shading design, which both must correspond to the real-world environment. We demonstrate our method in a live setup, where we modify a video conference feed such that the facial expressions of a different person (e.g., translator) are matched in real-time.", "paper_title": "Real-time Expression Transfer for Facial Reenactment", "paper_id": "WOS:000363671200020"}