{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "video_annotation"}, {"score": 0.00477190623242136, "phrase": "large_scale"}, {"score": 0.004750527930368202, "phrase": "labeled_datasets"}, {"score": 0.004686963264680882, "phrase": "key_importance"}, {"score": 0.004582897918101983, "phrase": "automatic_video_analysis_tools"}, {"score": 0.0041703614123283165, "phrase": "algorithms'_evaluation_phase"}, {"score": 0.004023120753104693, "phrase": "multimedia_and_computer_vision_communities"}, {"score": 0.003916098498242888, "phrase": "growing_number"}, {"score": 0.0038810583785936505, "phrase": "available_datasets"}, {"score": 0.0037272031155266556, "phrase": "annotation_tools"}, {"score": 0.003660788124282824, "phrase": "user_needs"}, {"score": 0.003563370084662192, "phrase": "human_concentration"}, {"score": 0.0034841647157189985, "phrase": "high_quality_ground_truth_data"}, {"score": 0.00333097896868937, "phrase": "large_video_ground_truths"}, {"score": 0.0032423089921154503, "phrase": "object_categories"}, {"score": 0.0031277316566463978, "phrase": "isolated_research_groups"}, {"score": 0.0030171910207671205, "phrase": "collaborative_web-based_platform"}, {"score": 0.0028844769395018595, "phrase": "easy_and_intuitive_user_interface"}, {"score": 0.0028458096625791625, "phrase": "plain_video_annotation"}, {"score": 0.0027825094419000637, "phrase": "generated_ground_truths"}, {"score": 0.0026720866990543744, "phrase": "large_part"}, {"score": 0.0024864612830452254, "phrase": "generated_annotations"}, {"score": 0.0023241387549221408, "phrase": "current_date"}, {"score": 0.0022419349154472806, "phrase": "comparative_performance_evaluation"}, {"score": 0.0021723899772112423, "phrase": "existing_state"}, {"score": 0.002143247921132179, "phrase": "art_methods"}, {"score": 0.0021049977753042253, "phrase": "annotation_time"}], "paper_keywords": ["Ground truth data", " Video labeling", " Object detection", " Object tracking", " Image segmentation"], "paper_abstract": "Large scale labeled datasets are of key importance for the development of automatic video analysis tools as they, from one hand, allow multi-class classifiers training and, from the other hand, support the algorithms' evaluation phase. This is widely recognized by the multimedia and computer vision communities, as witnessed by the growing number of available datasets; however, the research still lacks in annotation tools able to meet user needs, since a lot of human concentration is necessary to generate high quality ground truth data. Nevertheless, it is not feasible to collect large video ground truths, covering as much scenarios and object categories as possible, by exploiting only the effort of isolated research groups. In this paper we present a collaborative web-based platform for video ground truth annotation. It features an easy and intuitive user interface that allows plain video annotation and instant sharing/integration of the generated ground truths, in order to not only alleviate a large part of the effort and time needed, but also to increase the quality of the generated annotations. The tool has been on-line in the last four months and, at the current date, we have collected about 70,000 annotations. A comparative performance evaluation has also shown that our system outperforms existing state of the art methods in terms of annotation time, annotation quality and system's usability.", "paper_title": "An innovative web-based collaborative platform for video annotation", "paper_id": "WOS:000336807300019"}