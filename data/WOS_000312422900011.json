{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "speech-related_facial_movements"}, {"score": 0.004533427352547707, "phrase": "emotional_facial_expressions"}, {"score": 0.004417848254181138, "phrase": "user's_video"}, {"score": 0.004323776691055635, "phrase": "facial_expressions"}, {"score": 0.004213519983188569, "phrase": "small_tracking_errors"}, {"score": 0.0040186038781533946, "phrase": "incongruent_facial_movements"}, {"score": 0.003949974260471287, "phrase": "speech_perception"}, {"score": 0.0036553070575015344, "phrase": "facial_tracking_phase"}, {"score": 0.003623949225126944, "phrase": "speech_articulatory_parameters"}, {"score": 0.0033971539563683174, "phrase": "landmark_positions"}, {"score": 0.003324743991757282, "phrase": "articulatory_parameters"}, {"score": 0.0031166151196841308, "phrase": "inner_mouth_images"}, {"score": 0.0030501665085457606, "phrase": "noise_perception_experiment"}, {"score": 0.002810292047426252, "phrase": "human_auditory_visual_conditions"}, {"score": 0.002738523189244011, "phrase": "human_auditory-only_conditions"}, {"score": 0.0026570998719084153, "phrase": "vocalic_context"}, {"score": 0.0025892330566358503, "phrase": "avatar_auditory_visual_presentation"}, {"score": 0.0023548511815224098, "phrase": "depth_information"}, {"score": 0.002197801146305747, "phrase": "impaired_people"}, {"score": 0.0021789194955896102, "phrase": "information_technologies"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Talking head", " Auditory-visual speech", " Puppetry", " Facial animation", " Face tracking"], "paper_abstract": "Several puppetry techniques have been recently proposed to transfer emotional facial expressions to an avatar from a user's video. Whereas generation of facial expressions may not be sensitive to small tracking errors, generation of speech-related facial movements would be severely impaired. Since incongruent facial movements can drastically influence speech perception, we proposed a more effective method to transfer speech-related facial movements from a user to an avatar. After a facial tracking phase, speech articulatory parameters (controlling the jaw and the lips) were determined from the set of landmark positions. Two additional processes calculated the articulatory parameters which controlled the eyelids and the tongue from the 2D Discrete Cosine Transform coefficients of the eyes and inner mouth images. A speech in noise perception experiment was conducted on 25 participants to evaluate the system. Increase in intelligibility was shown for the avatar and human auditory visual conditions compared to the avatar and human auditory-only conditions, respectively. Depending on the vocalic context, the results of the avatar auditory visual presentation were different: all the consonants were better perceived in /a/ vocalic context compared to /i/ and /u/ because of the lack of depth information retrieved from video. This method could be used to accurately animate avatars for hearing impaired people using information technologies and telecommunication. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Control of speech-related facial movements of an avatar from video", "paper_id": "WOS:000312422900011"}