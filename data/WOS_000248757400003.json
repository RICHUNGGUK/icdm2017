{"auto_keywords": [{"score": 0.043226782433942644, "phrase": "slmm_approach"}, {"score": 0.037085211414711015, "phrase": "slmm_model"}, {"score": 0.00481495049065317, "phrase": "large_margin_machines"}, {"score": 0.004714023408114857, "phrase": "data_distributions"}, {"score": 0.004566567411073082, "phrase": "new_large_margin_classifier"}, {"score": 0.004195418010131832, "phrase": "data_distribution"}, {"score": 0.004021267064832957, "phrase": "\"structured\"_learning_models"}, {"score": 0.003936910716320182, "phrase": "radial_basis_function_networks"}, {"score": 0.0038953958723049287, "phrase": "gaussian_mixture_models"}, {"score": 0.003773449694108617, "phrase": "\"unstructured\"_large_margin_learning_schemes"}, {"score": 0.0036167509502276294, "phrase": "maxi-min_margin_machines"}, {"score": 0.0034299649219748513, "phrase": "\"structured_degree"}, {"score": 0.0032355864829403413, "phrase": "existing_structured_and_unstructured_learning_models"}, {"score": 0.0031342307827666675, "phrase": "ward's_agglomerative_hierarchical_clustering"}, {"score": 0.003036040409062437, "phrase": "data_mappings"}, {"score": 0.002988101395789399, "phrase": "kernel_space"}, {"score": 0.0029098743621130004, "phrase": "underlying_data_structure"}, {"score": 0.0028487656293168795, "phrase": "slmm_training"}, {"score": 0.0028037753646260937, "phrase": "sequential_second_order_cone_programming"}, {"score": 0.0025348408238837655, "phrase": "noise_tolerance"}, {"score": 0.0024553821147953463, "phrase": "theoretical_importance"}, {"score": 0.0023408288327263316, "phrase": "existing_approaches"}, {"score": 0.002219790619936024, "phrase": "novel_insight"}, {"score": 0.002196342933346204, "phrase": "learning_models"}], "paper_keywords": ["large margin learning", " weighted mahalanobis distance (WMD)", " homospace", " structured learning", " agglomerative hierarchical clustering", " second order cone programming (SOCP)"], "paper_abstract": "This paper proposes a new large margin classifier-the structured large margin machine (SLMM)-that is sensitive to the structure of the data distribution. The SLMM approach incorporates the merits of \"structured\" learning models, such as radial basis function networks and Gaussian mixture models, with the advantages of \"unstructured\" large margin learning schemes, such as support vector machines and maxi-min margin machines. We derive the SLMM model from the concepts of \"structured degree\" and \"homospace\", based on an analysis of existing structured and unstructured learning models. Then, by using Ward's agglomerative hierarchical clustering on input data (or data mappings in the kernel space) to extract the underlying data structure, we formulate SLMM training as a sequential second order cone programming. Many promising features of the SLMM approach are illustrated, including its accuracy, scalability, extensibility, and noise tolerance. We also demonstrate the theoretical importance of the SLMM model by showing that it generalizes existing approaches, such as SVMs and (MS)-S-4, provides novel insight into learning models, and lays a foundation for conceiving other \"structured\" classifiers.", "paper_title": "Structured large margin machines: sensitive to data distributions", "paper_id": "WOS:000248757400003"}