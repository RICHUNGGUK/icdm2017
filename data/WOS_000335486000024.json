{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "exponential_stability"}, {"score": 0.04902194563396072, "phrase": "delayed_recurrent_neural_networks"}, {"score": 0.0037658881407605445, "phrase": "sufficient_conditions"}, {"score": 0.003256526411907174, "phrase": "lyapunov-krasvoskii"}, {"score": 0.0031140747752837826, "phrase": "partitioning_approach"}, {"score": 0.002847540262784593, "phrase": "previous_results"}, {"score": 0.0026330549554033876, "phrase": "numerical_examples"}, {"score": 0.002276566401216226, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Recurrent neural networks", " Exponential stability", " Time delay", " Delay partitioning approach"], "paper_abstract": "In this paper, we investigate exponential stability of delayed recurrent neural networks. By using the delay partitioning method, some sufficient conditions are established to guarantee exponential stability of delayed recurrent neural networks under two different conditions with constructing new Lyapunov-Krasvoskii functional. This partitioning approach can reduce the conservatism comparing with some previous results of stability. At last, numerical examples are given out to show the effectiveness and advantage of our results. Crown Copyright (C) 2014 Published by Elsevier B.V. All rights reserved.", "paper_title": "New criteria for exponential stability of delayed recurrent neural networks", "paper_id": "WOS:000335486000024"}