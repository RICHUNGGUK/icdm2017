{"auto_keywords": [{"score": 0.03789823017882356, "phrase": "salient_motion_detection"}, {"score": 0.00481495049065317, "phrase": "visual_data"}, {"score": 0.004772818068176176, "phrase": "wireless_surveillance_networks"}, {"score": 0.004710306419574744, "phrase": "wireless_visual_sensor_networks"}, {"score": 0.004567596860086148, "phrase": "imaging_data"}, {"score": 0.004468304427278713, "phrase": "resource_constraints"}, {"score": 0.004371160978838441, "phrase": "sheer_volume"}, {"score": 0.004332894955218135, "phrase": "surveillance_videos"}, {"score": 0.004183137595671171, "phrase": "actionable_intelligence"}, {"score": 0.004056334733843591, "phrase": "energy-efficient_image_prioritization_framework"}, {"score": 0.0038989120088094185, "phrase": "traditional_wvsns"}, {"score": 0.003847802030433298, "phrase": "proposed_framework"}, {"score": 0.0038141000450978746, "phrase": "semantically_relevant_information"}, {"score": 0.003698442219929622, "phrase": "sink_node"}, {"score": 0.003477505397536663, "phrase": "human_cognitive_processes"}, {"score": 0.0034319005131102495, "phrase": "camera_node"}, {"score": 0.003342471131048328, "phrase": "bootstrapping_procedure"}, {"score": 0.003170520716404193, "phrase": "salient_motion"}, {"score": 0.0031289290660162145, "phrase": "sensor_node"}, {"score": 0.0030608148631528767, "phrase": "high_or_low_priority"}, {"score": 0.002670296619878048, "phrase": "high-priority_camera_nodes"}, {"score": 0.0026121403051734744, "phrase": "reliable_radio_channels"}, {"score": 0.0025665261770229757, "phrase": "timely_and_reliable_transmission"}, {"score": 0.0024024377855956136, "phrase": "single_and_multi-camera_monitoring"}, {"score": 0.0023090525096074264, "phrase": "proposed_method"}, {"score": 0.0022687189700002254, "phrase": "salient_event_coverage"}, {"score": 0.002248816575548875, "phrase": "reduced_computational_and_transmission_costs"}, {"score": 0.0021613897794439227, "phrase": "semantically_relevant_visual_information"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Image prioritization", " Wireless visual sensor networks", " Monitoring applications", " Salient activity detection"], "paper_abstract": "In wireless visual sensor networks (WVSNs), streaming all imaging data is impractical due to resource constraints. Moreover, the sheer volume of surveillance videos inhibits the ability of analysts to extract actionable intelligence. In this work, an energy-efficient image prioritization framework is presented to cope with the fragility of traditional WVSNs. The proposed framework selects semantically relevant information before it is transmitted to a sink node. This is based on salient motion detection, which works on the principle of human cognitive processes. Each camera node estimates the background by a bootstrapping procedure, thus increasing the efficiency of salient motion detection. Based on the salient motion, each sensor node is classified as being high or low priority. This classification is dynamic, such that camera nodes toggle between high-priority and low-priority status depending on the coverage of the region of interest. High-priority camera nodes are allowed to access reliable radio channels to ensure the timely and reliable transmission of data. We compare the performance of this framework with other state-ofthe-art methods for both single and multi-camera monitoring. The results demonstrate the usefulness of the proposed method in terms of salient event coverage and reduced computational and transmission costs, as well as in helping analysts find semantically relevant visual information. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Saliency-directed prioritization of visual data in wireless surveillance networks", "paper_id": "WOS:000349736700003"}