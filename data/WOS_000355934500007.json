{"auto_keywords": [{"score": 0.038880200368473436, "phrase": "pso"}, {"score": 0.00481495049065317, "phrase": "hard_continuous_optimization"}, {"score": 0.00446240364863009, "phrase": "particle_swarm_optimization"}, {"score": 0.004328689762198895, "phrase": "optimized_learning_procedure"}, {"score": 0.0040525061916685924, "phrase": "new_version"}, {"score": 0.00395101799538683, "phrase": "new_particles"}, {"score": 0.003911135156992, "phrase": "six_informants"}, {"score": 0.003587924960158317, "phrase": "good_particles"}, {"score": 0.0035337158050484474, "phrase": "longer_time"}, {"score": 0.003341839487905525, "phrase": "local_basins"}, {"score": 0.003258090521354736, "phrase": "non-separable_complex_problems"}, {"score": 0.00317643367669255, "phrase": "pso_versions"}, {"score": 0.003019189898170698, "phrase": "local_search_procedure"}, {"score": 0.0025146979283050923, "phrase": "best_recent_proposals"}, {"score": 0.0024766636161181544, "phrase": "current_state"}, {"score": 0.0023780209525032688, "phrase": "local_search"}, {"score": 0.002226017732566121, "phrase": "local_search_mechanisms"}, {"score": 0.002181227522730908, "phrase": "high_rate"}, {"score": 0.002126502291103601, "phrase": "large_number"}], "paper_keywords": ["Particle swarm optimization", " Fully informed PSO", " Multiple trajectory search", " Benchmarking functions"], "paper_abstract": "In our previous works, we empirically showed that a number of informants may endow particle swarm optimization (PSO) with an optimized learning procedure in comparison with other combinations of informants. In this way, the new version PSO6, that evolves new particles from six informants (neighbors), performs more accurately than other existing versions of PSO and is able to generate good particles for a longer time. Despite this advantage, PSO6 may show certain attraction to local basins derived from its moderate performance on non-separable complex problems (typically observed in PSO versions). In this paper, we incorporate a local search procedure to the PSO6 with the aim of correcting this disadvantage. We compare the performance of our proposal (PSO6-Mtsls) on a set of 40 benchmark functions against that of other PSO versions, as well as against the best recent proposals in the current state of the art (with and without local search). The results support our conjecture that the (quasi)-optimally informed PSO, hybridized with local search mechanisms, reaches a high rate of success on a large number of complex (non-separable) continuous optimization functions.", "paper_title": "Hybrid PSO6 for hard continuous optimization", "paper_id": "WOS:000355934500007"}