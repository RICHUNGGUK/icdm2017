{"auto_keywords": [{"score": 0.04302051049727595, "phrase": "probabilistic_predictions"}, {"score": 0.033502945522523205, "phrase": "sparse_gaussian_processes"}, {"score": 0.00481495049065317, "phrase": "marginalized_neural_network_mixtures"}, {"score": 0.004735254735115024, "phrase": "large-scale_regression"}, {"score": 0.004618166136069052, "phrase": "regression_tasks"}, {"score": 0.004541712637353326, "phrase": "traditional_neural_networks"}, {"score": 0.004248294537944227, "phrase": "gaussian_processes"}, {"score": 0.0033622909109253616, "phrase": "massive_data_sets"}, {"score": 0.0030160945545795468, "phrase": "similar_performance"}, {"score": 0.002660574167091781, "phrase": "marginalized_output_weights"}, {"score": 0.0021049977753042253, "phrase": "representative_large_data_sets"}], "paper_keywords": ["Bayesian models", " Gaussian processes", " large data sets", " multilayer perceptrons", " regression"], "paper_abstract": "For regression tasks, traditional neural networks (NNs) have been superseded by Gaussian processes, which provide probabilistic predictions (input-dependent error bars), improved accuracy, and virtually no overfitting. Due to their high computational cost, in scenarios with massive data sets, one has to resort to sparse Gaussian processes, which strive to achieve similar performance with much smaller computational effort. In this context, we introduce a mixture of NNs with marginalized output weights that can both provide probabilistic predictions and improve on the performance of sparse Gaussian processes, at the same computational cost. The effectiveness of this approach is shown experimentally on some representative large data sets.", "paper_title": "Marginalized Neural Network Mixtures for Large-Scale Regression", "paper_id": "WOS:000283422600013"}