{"auto_keywords": [{"score": 0.044483924461994694, "phrase": "first_paradigm"}, {"score": 0.028026285296228158, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "correlative_multilabel_video_annotation"}, {"score": 0.004780030485538593, "phrase": "temporal_kernels"}, {"score": 0.004745362527093198, "phrase": "automatic_video_annotation"}, {"score": 0.0046938292389998824, "phrase": "important_ingredient"}, {"score": 0.0046597835420572825, "phrase": "semantic-level_video_browsing"}, {"score": 0.004117215958803593, "phrase": "pre-trained_binary_classifier"}, {"score": 0.003998983496539902, "phrase": "rich_information"}, {"score": 0.003955523458325766, "phrase": "video_concepts"}, {"score": 0.003898307690037192, "phrase": "limited_success"}, {"score": 0.003745179888113379, "phrase": "second_paradigm"}, {"score": 0.003704467870496114, "phrase": "extra_step"}, {"score": 0.003624361893008392, "phrase": "first_individual_classifiers"}, {"score": 0.0035719191251314918, "phrase": "multiple_detections"}, {"score": 0.003357343256340741, "phrase": "error_propagation"}, {"score": 0.003308751289007707, "phrase": "first_step"}, {"score": 0.003272767888056937, "phrase": "second_fusion"}, {"score": 0.003144134096095217, "phrase": "video_annotation_method"}, {"score": 0.00296597499768551, "phrase": "model_correlations"}, {"score": 0.002808104467715375, "phrase": "complementary_information"}, {"score": 0.0027876981671591202, "phrase": "different_labels"}, {"score": 0.002727362288104992, "phrase": "video_clips"}, {"score": 0.0026878636586288363, "phrase": "temporally_ordered_frame_sequences"}, {"score": 0.002601065207135226, "phrase": "rich_temporal_information"}, {"score": 0.002480602069295564, "phrase": "cml_method"}, {"score": 0.002444668348634501, "phrase": "discriminative_information"}, {"score": 0.0024268966855267153, "phrase": "hidden_markov_models"}, {"score": 0.002280948283235427, "phrase": "proposed_approach"}, {"score": 0.002183230464873613, "phrase": "first_and_second_paradigms"}, {"score": 0.002159460831267798, "phrase": "widely_used_trecvid_data"}], "paper_keywords": ["Algorithms", " Theory", " Experimentation", " Video annotation", " multilabeling", " concept correlation", " temporal kernel"], "paper_abstract": "Automatic video annotation is an important ingredient for semantic-level video browsing, search and navigation. Much attention has been paid to this topic in recent years. These researches have evolved through two paradigms. In the first paradigm, each concept is individually annotated by a pre-trained binary classifier. However, this method ignores the rich information between the video concepts and only achieves limited success. Evolved from the first paradigm, the methods in the second paradigm add an extra step on the top of the first individual classifiers to fuse the multiple detections of the concepts. However, the performance of these methods can be degraded by the error propagation incurred in the first step to the second fusion one. In this article, another paradigm of the video annotation method is proposed to address these problems. It simultaneously annotates the concepts as well as model correlations between them in one step by the proposed Correlative Multilabel (CML) method, which benefits from the compensation of complementary information between different labels. Furthermore, since the video clips are composed by temporally ordered frame sequences, we extend the proposed method to exploit the rich temporal information in the videos. Specifically, a temporal-kernel is incorporated into the CML method based on the discriminative information between Hidden Markov Models (HMMs) that are learned from the videos. We compare the performance between the proposed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. As to be shown, superior performance of the proposed method is gained.", "paper_title": "Correlative Multilabel Video Annotation with Temporal Kernels", "paper_id": "WOS:000261155700003"}