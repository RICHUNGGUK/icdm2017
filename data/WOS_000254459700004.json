{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "computationally_efficient_algorithm"}, {"score": 0.004587844134436212, "phrase": "kearns"}, {"score": 0.004543717788338176, "phrase": "schapire"}, {"score": 0.00393032656248792, "phrase": "labelled_examples"}, {"score": 0.0037267132967205136, "phrase": "malicious_noise"}, {"score": 0.0035679633126097115, "phrase": "future_examples"}, {"score": 0.0034995881411519925, "phrase": "additive_epsilon"}, {"score": 0.0034491650585057754, "phrase": "optimal_halfspace"}, {"score": 0.0029830992938695007, "phrase": "log-concave_distribution"}, {"score": 0.002954439981898582, "phrase": "rn"}, {"score": 0.002869793011316547, "phrase": "boolean_disjunctions"}, {"score": 0.002530359917257626, "phrase": "natural_noise-tolerant_arbitrary-distribution_generalization"}, {"score": 0.00249386680748172, "phrase": "well-known_\"low-degree"}, {"score": 0.0024342329371013294, "phrase": "linial"}, {"score": 0.0024107589479085783, "phrase": "mansour"}, {"score": 0.002375985919336319, "phrase": "nisan"}, {"score": 0.0023079225317714815, "phrase": "cant_improvements"}, {"score": 0.002274630121183437, "phrase": "running_time"}, {"score": 0.0022094759531152072, "phrase": "fastest_known_algorithm"}, {"score": 0.0021254913611527455, "phrase": "challenging_open_problem"}, {"score": 0.0021049977753042253, "phrase": "computational_learning_theory"}], "paper_keywords": ["agnostic learning", " halfspaces", " Fourier"], "paper_abstract": "We give a computationally efficient algorithm that learns (under distributional assumptions) a halfspace in the difficult agnostic framework of Kearns, Schapire, and Sellie [ Mach. Learn., 17 (1994), pp. 115-141], where a learner is given access to a distribution on labelled examples but where the labelling may be arbitrary (similar to malicious noise). It constructs a hypothesis whose error rate on future examples is within an additive epsilon of the optimal halfspace, in time poly(n) for any constant epsilon > 0, for the uniform distribution over{- 1, 1} n or unit sphere in R(n), as well as any log-concave distribution in Rn. It also agnostically learns Boolean disjunctions in time 2((O) over bar)(root n) with respect to any distribution. Our algorithm, which performs L(1) polynomial regression, is a natural noise-tolerant arbitrary-distribution generalization of the well-known \"low-degree\" Fourier algorithm of Linial, Mansour, and Nisan. We observe that signi. cant improvements on the running time of our algorithm would yield the fastest known algorithm for learning parity with noise, a challenging open problem in computational learning theory.", "paper_title": "Agnostically learning halfspaces", "paper_id": "WOS:000254459700004"}