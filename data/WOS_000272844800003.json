{"auto_keywords": [{"score": 0.049509300669177574, "phrase": "user_generated_video"}, {"score": 0.03655733938936528, "phrase": "camera_motion"}, {"score": 0.00481495049065317, "phrase": "camera_motion-based"}, {"score": 0.004427784354339762, "phrase": "ugv"}, {"score": 0.004357627943575288, "phrase": "rich_camera_motion_structure"}, {"score": 0.0038195342195623764, "phrase": "new_concept"}, {"score": 0.0037740295668681014, "phrase": "camera_view"}, {"score": 0.00374399358035391, "phrase": "temporal_segmentation"}, {"score": 0.00371419685149762, "phrase": "ugv."}, {"score": 0.003640730820018083, "phrase": "video_summary"}, {"score": 0.003611751858608048, "phrase": "unique_properties"}, {"score": 0.0034981163169818803, "phrase": "video_annotation"}, {"score": 0.0034152340901585374, "phrase": "powerful_feature"}, {"score": 0.0031654684543975077, "phrase": "camera_person's_interests"}, {"score": 0.003053598205821532, "phrase": "viewers'_attention"}, {"score": 0.0029931603112397084, "phrase": "new_location-based_saliency_map"}, {"score": 0.002922207118572208, "phrase": "camera_motion_parameters"}, {"score": 0.0027741761880779535, "phrase": "color_contrast"}, {"score": 0.002752075650871066, "phrase": "object_motion"}, {"score": 0.00255068452076921, "phrase": "subjective_evaluation"}, {"score": 0.0024506642262508735, "phrase": "viewers'_preferences"}, {"score": 0.0023545567809591804, "phrase": "human_visual_attention"}, {"score": 0.0023171750718791713, "phrase": "tracking_experiment"}, {"score": 0.002262209824105154, "phrase": "high_dependency"}, {"score": 0.0022174005294867604, "phrase": "fixation_points"}, {"score": 0.002147540520631409, "phrase": "camera_movement"}], "paper_keywords": ["Content-based video analysis", " eye tracking", " home video", " motion-based analysis", " regions of interest", " saliency maps", " user generated video", " video summarization"], "paper_abstract": "In this paper we propose a system for the analysis of user generated video (UGV). UGV often has a rich camera motion structure that is generated at the time the video is recorded by the person taking the video, i.e., the \"camera person.\" We exploit this structure by defining a new concept known as camera view for temporal segmentation of UGV. The segmentation provides a video summary with unique properties that is useful in applications such as video annotation. Camera motion is also a powerful feature for identification of keyframes and regions of interest (ROIs) since it is an indicator of the camera person's interests in the scene and can also attract the viewers' attention. We propose a new location-based saliency map which is generated based on camera motion parameters. This map is combined with other saliency maps generated using features such as color contrast, object motion and face detection to determine the ROIs. In order to evaluate our methods we conducted several user studies. A subjective evaluation indicated that our system produces results that is consistent with viewers' preferences. We also examined the effect of camera motion on human visual attention through an eye tracking experiment. The results showed a high dependency between the distribution of fixation points of the viewers and the direction of camera movement which is consistent with our location-based saliency map.", "paper_title": "Camera Motion-Based Analysis of User Generated Video", "paper_id": "WOS:000272844800003"}