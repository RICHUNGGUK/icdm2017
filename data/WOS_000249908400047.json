{"auto_keywords": [{"score": 0.03848267384550147, "phrase": "output_weights"}, {"score": 0.013365936081592854, "phrase": "hidden_nodes"}, {"score": 0.013132048160312043, "phrase": "continuous_sampling_distribution"}, {"score": 0.004816072210410139, "phrase": "convex"}, {"score": 0.004797835176186123, "phrase": "incremental_extreme_learning_machine"}, {"score": 0.004729977262447766, "phrase": "conventional_neural_network_theories"}, {"score": 0.004629977311316031, "phrase": "universal_approximation"}, {"score": 0.0045971138571344345, "phrase": "incremental_constructive_feedforward_networks"}, {"score": 0.00456448259718331, "phrase": "random_hidden_nodes"}, {"score": 0.0045320819060445045, "phrase": "ieee_transactions_on_neural_networks"}, {"score": 0.004311598785718415, "phrase": "new_theory"}, {"score": 0.004250588556735512, "phrase": "single-hidden-layer_feedforward_networks"}, {"score": 0.004175533280178052, "phrase": "randomly_generated_additive"}, {"score": 0.004145882022189891, "phrase": "radial_basis_function"}, {"score": 0.003944112858235306, "phrase": "universal_approximators"}, {"score": 0.003902165732101058, "phrase": "resulting_incremental_extreme_learning_machine"}, {"score": 0.0034567253581991226, "phrase": "new_node"}, {"score": 0.0032883848851259123, "phrase": "convergence_rate"}, {"score": 0.0031393948330209224, "phrase": "existing_nodes"}, {"score": 0.003094920033010818, "phrase": "convex_optimization_method"}, {"score": 0.003061976628792769, "phrase": "new_hidden_node"}, {"score": 0.002912803994461996, "phrase": "piecewise_continuous_computational_hidden_nodes"}, {"score": 0.0027121844541913367, "phrase": "adjustable_hidden_node_parameters"}, {"score": 0.002664221051021009, "phrase": "function_approximation_point"}, {"score": 0.0026264601495742295, "phrase": "hidden_node_parameters"}, {"score": 0.0025525322651112365, "phrase": "rbf_networks"}, {"score": 0.0025343769150179764, "phrase": "trigonometric_networks"}, {"score": 0.002516350372379076, "phrase": "threshold_networks"}, {"score": 0.0024984517279573906, "phrase": "fuzzy_inference_systems"}, {"score": 0.002480680078868508, "phrase": "fully_complex_neural_networks"}, {"score": 0.0024630345285372958, "phrase": "high-order_networks"}, {"score": 0.002445514186654732, "phrase": "ridge_polynomial_networks"}, {"score": 0.0024281181691350085, "phrase": "wavelet_networks"}, {"score": 0.0021971605959277963, "phrase": "elm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["generalized feedforward networks", " incremental extreme learning machine", " convergence rate", " random hidden nodes", " convex optimization"], "paper_abstract": "Unlike the conventional neural network theories and implementations, Huang et al. [Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Transactions on Neural Networks 17(4) (2006) 879-892] have recently proposed a new theory to show that single-hidden-layer feedforward networks (SLFNs) with randomly generated additive or radial basis function (RBF) hidden nodes (according to any continuous sampling distribution) can work as universal approximators and the resulting incremental extreme learning machine (I-ELM) outperforms many popular learning algorithms. I-ELM randomly generates the hidden nodes and analytically calculates the output weights of SLFNs, however, I-ELM does not recalculate the output weights of all the existing nodes when a new node is added. This paper shows that while retaining the same simplicity, the convergence rate of I-ELM can be further improved by recalculating the output weights of the existing nodes based on a convex optimization method when a new hidden node is randomly added. Furthermore, we show that given a type of piecewise continuous computational hidden nodes (possibly not neural alike nodes), if SLFNs f(n)(x) = Sigma(n)(i=1) beta(i) G(x,a(i),b(i)) can work as universal approximators with adjustable hidden node parameters, V from a function approximation point of view the hidden node parameters of such \"generalized\" SLFNs (including sigmoid networks, RBF networks, trigonometric networks, threshold networks, fuzzy inference systems, fully complex neural networks, high-order networks, ridge polynomial networks, wavelet networks, etc.) can actually be randomly generated according to any continuous sampling distribution. In theory, the parameters of these SLFNs can be analytically determined by ELM instead of being tuned. (c)., 2007 Elsevier B.V. All rights reserved.", "paper_title": "Convex incremental extreme learning machine", "paper_id": "WOS:000249908400047"}