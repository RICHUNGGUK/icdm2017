{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "markov"}, {"score": 0.004342430078306461, "phrase": "candidate_abstractions"}, {"score": 0.00415050117148714, "phrase": "different_combination"}, {"score": 0.00409723267214392, "phrase": "state_components"}, {"score": 0.003916098498242888, "phrase": "new_approach"}, {"score": 0.003816197777388722, "phrase": "effective_abstraction_selection"}, {"score": 0.0035543706594585076, "phrase": "existing_approaches"}, {"score": 0.0034780098310329176, "phrase": "mdp"}, {"score": 0.003267925215811654, "phrase": "abstraction_part"}, {"score": 0.0032051601987130207, "phrase": "learning_agent's_decision-making_process"}, {"score": 0.0031233388358844188, "phrase": "agent's_action_space"}, {"score": 0.0026060359255636444, "phrase": "original_mdp"}, {"score": 0.002555950807368337, "phrase": "optimal_policy"}, {"score": 0.002380381895973937, "phrase": "increasing_complexity"}, {"score": 0.0021883400011393564, "phrase": "context-specific_structure"}, {"score": 0.0021049977753042253, "phrase": "wiley_periodicals"}], "paper_keywords": ["reinforcement learning", " model-free learning", " structure learning", " abstraction selection"], "paper_abstract": "This article addresses reinforcement learning problems based on factored Markov decision processes (MDPs) in which the agent must choose among a set of candidate abstractions, each build up from a different combination of state components. We present and evaluate a new approach that can perform effective abstraction selection that is more resource-efficient and/or more general than existing approaches. The core of the approach is to make selection of an abstraction part of the learning agent's decision-making process by augmenting the agent's action space with internal actions that select the abstraction it uses. We prove that under certain conditions this approach results in a derived MDP whose solution yields both the optimal abstraction for the original MDP and the optimal policy under that abstraction. We examine our approach in three domains of increasing complexity: contextual bandit problems, episodic MDPs, and general MDPs with context-specific structure. (c) 2013 Wiley Periodicals, Inc.", "paper_title": "EFFICIENT ABSTRACTION SELECTION IN REINFORCEMENT LEARNING", "paper_id": "WOS:000344852100001"}