{"auto_keywords": [{"score": 0.0489928999746962, "phrase": "nist_scheme"}, {"score": 0.00481495049065317, "phrase": "survey_parser"}, {"score": 0.004634254474150777, "phrase": "different_metrics"}, {"score": 0.004260134699483774, "phrase": "parser-produced_syntactic_tree"}, {"score": 0.004068907083697369, "phrase": "correct_tree"}, {"score": 0.0036274201504044685, "phrase": "correct_constituents"}, {"score": 0.0035179764329354877, "phrase": "constituent_labels"}, {"score": 0.00346449481422526, "phrase": "bracketing_accuracy"}, {"score": 0.003160200215473349, "phrase": "better_alternative"}, {"score": 0.0030414140732789186, "phrase": "parser_output"}, {"score": 0.002949598537993663, "phrase": "correct_match"}, {"score": 0.002435190484324197, "phrase": "syntactic_annotation"}, {"score": 0.002272794656458419, "phrase": "empirical_scores"}, {"score": 0.0021049977753042253, "phrase": "future_research"}], "paper_keywords": [""], "paper_abstract": "Different metrics have been proposed for the estimation of how good a parser-produced syntactic tree is when judged by a correct tree from the treebank. The emphasis of measurement has been on the number of correct constituents in terms of constituent labels and bracketing accuracy. This article proposes the use of the NIST scheme as a better alternative for the evaluation of parser output in terms of correct match, substitution, deletion, and insertion. It describes an experiment to measure the performance of the Survey Parser that was used to complete the syntactic annotation of the International Corpus of English. This article will finally report empirical scores for the performance of the parser and outline some future research.", "paper_title": "Evaluating the performance of the survey parser with the NIST scheme", "paper_id": "WOS:000236109700019"}