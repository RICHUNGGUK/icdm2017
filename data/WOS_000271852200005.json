{"auto_keywords": [{"score": 0.04764602699421295, "phrase": "large_amount"}, {"score": 0.04581394571995767, "phrase": "information_retrieval"}, {"score": 0.03412859855504083, "phrase": "positive_data"}, {"score": 0.029958935081844246, "phrase": "small_amount"}, {"score": 0.025529957652864792, "phrase": "objective_function"}, {"score": 0.00481495049065317, "phrase": "positive_and_unlabeled_examples"}, {"score": 0.00477924690801092, "phrase": "topic-sensitive_plsa."}, {"score": 0.004536592536813498, "phrase": "positive_and_negative_examples"}, {"score": 0.0044695432851061525, "phrase": "classification_system"}, {"score": 0.0039380388343479384, "phrase": "learning_system"}, {"score": 0.003865376734263166, "phrase": "unlabeled_data"}, {"score": 0.003696390664266446, "phrase": "positive_and_unlabeled_data"}, {"score": 0.0036281710561308377, "phrase": "critical_problem"}, {"score": 0.003601235776892337, "phrase": "machine_learning"}, {"score": 0.003102488033561956, "phrase": "novel_algorithm"}, {"score": 0.0030794433253250476, "phrase": "topic-sensitive_plsa"}, {"score": 0.00296675450822924, "phrase": "original_probabilistic_latent_semantic_analysis"}, {"score": 0.0028795712616923462, "phrase": "purely_unsupervised_framework"}, {"score": 0.0028053842615436706, "phrase": "supervision_information"}, {"score": 0.002613487968156893, "phrase": "users'_interests"}, {"score": 0.00247128526955136, "phrase": "penalty_terms"}, {"score": 0.0023281003994670714, "phrase": "observed_data"}, {"score": 0.002234491924392104, "phrase": "iterative_algorithm"}, {"score": 0.002193193339812606, "phrase": "local_optimum"}, {"score": 0.0021526564019119466, "phrase": "experimental_evaluation"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["Semisupervised learning", " topic-sensitive probabilistic latent semantic analysis", " document classification"], "paper_abstract": "It is often difficult and time-consuming to provide a large amount of positive and negative examples for training a classification system in many applications such as information retrieval. Instead, users often find it easier to indicate just a few positive examples of what he or she likes, and thus, these are the only labeled examples available for the learning system. A large amount of unlabeled data are easier to obtain. How to make use of the positive and unlabeled data for learning is a critical problem in machine learning and information retrieval. Several approaches for solving this problem have been proposed in the past, but most of these methods do not work well when only a small amount of labeled positive data are available. In this paper, we propose a novel algorithm called Topic-Sensitive pLSA to solve this problem. This algorithm extends the original probabilistic latent semantic analysis (pLSA), which is a purely unsupervised framework, by injecting a small amount of supervision information from the user. The supervision from users is in the form of indicating which documents fit the users' interests. The supervision is encoded into a set of constraints. By introducing the penalty terms for these constraints, we propose an objective function that trades off the likelihood of the observed data and the enforcement of the constraints. We develop an iterative algorithm that can obtain the local optimum of the objective function. Experimental evaluation on three data corpora shows that the proposed method can improve the performance especially only with a small amount of labeled positive data.", "paper_title": "Learning with Positive and Unlabeled Examples Using Topic-Sensitive PLSA", "paper_id": "WOS:000271852200005"}