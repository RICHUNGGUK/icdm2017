{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "visual_quality"}, {"score": 0.01135084965762365, "phrase": "image_information"}, {"score": 0.009039389580315478, "phrase": "distorted_image"}, {"score": 0.004767261858417823, "phrase": "fundamental_importance"}, {"score": 0.0047357308775561255, "phrase": "numerous_image_and_video_processing_applications"}, {"score": 0.004657808551825796, "phrase": "quality_assessment"}, {"score": 0.004344232223147987, "phrase": "perceptually_consistent_manner"}, {"score": 0.004315487055548749, "phrase": "image_qa_algorithms"}, {"score": 0.004272724011826495, "phrase": "image_quality"}, {"score": 0.004133204817078675, "phrase": "\"perfect\"_image"}, {"score": 0.004092240621596351, "phrase": "perceptual_space"}, {"score": 0.003984968827278219, "phrase": "qa_methods"}, {"score": 0.003906357144047631, "phrase": "quality_prediction"}, {"score": 0.0038676325043529524, "phrase": "salient_physiological_and_psychovisual_features"}, {"score": 0.003829290275367182, "phrase": "human_visual_system"}, {"score": 0.0037412912454999046, "phrase": "signal_fidelity_measures"}, {"score": 0.0036311037851931498, "phrase": "image_qa_problem"}, {"score": 0.003595097948724078, "phrase": "information_fidelity_problem"}, {"score": 0.0034317185678527672, "phrase": "distortion_process"}, {"score": 0.003319569552937513, "phrase": "qa_systems"}, {"score": 0.0032217623556814543, "phrase": "\"natural\"_images"}, {"score": 0.0031476805104529377, "phrase": "\"human_consumption"}, {"score": 0.0030855348881460107, "phrase": "sophisticated_models"}, {"score": 0.002925728709297618, "phrase": "information_fidelity_criterion"}, {"score": 0.00290634217991954, "phrase": "image_qa"}, {"score": 0.0026745503978347143, "phrase": "image_information_measure"}, {"score": 0.002587081502365341, "phrase": "reference_image"}, {"score": 0.002535975801308951, "phrase": "reference_information"}, {"score": 0.0024125742420201, "phrase": "visual_information_fidelity_measure"}, {"score": 0.002318180580715058, "phrase": "extensive_subjective_study"}, {"score": 0.0022573076362326135, "phrase": "recent_state-of-the-art_image_qa_algorithms"}, {"score": 0.0021403049512708494, "phrase": "subjective_study"}, {"score": 0.0021049977753042253, "phrase": "live_website"}], "paper_keywords": ["image information", " image quality assessment (QA)", " information fidelity", " natural scene statistics (NSS)"], "paper_abstract": "Measurement of visual quality is of fundamental importance to numerous image and video processing applications. The goal of quality assessment (QA) research is to design algorithms that can automatically assess the quality of images or videos in a perceptually consistent manner. Image QA algorithms generally interpret image quality as fidelity or similarity with a \"reference\" or \"perfect\" image in some perceptual space. Such \"full-reference\" QA methods attempt to achieve consistency in quality prediction by modeling salient physiological and psychovisual features of the human visual system (HVS), or by signal fidelity measures. In this paper, we approach the image QA problem as an information fidelity problem. Specifically, we propose to quantify the loss of image information to the distortion process and explore the relationship between image information and visual quality. QA systems are invariably involved with judging the visual quality of \"natural\" images and videos that are meant for \"human consumption.\" Researchers have developed sophisticated models to capture the statistics of such natural signals. Using these models, we previously presented an information fidelity criterion for image QA that related image quality with the amount of information shared between a reference and a distorted image. In this paper, we propose an image information measure that quantifies the information that is present in the reference image and how much of this reference information can be extracted from the distorted image. Combining these two quantities, we propose a visual information fidelity measure for image QA. We validate the performance of our algorithm with an extensive subjective study involving 779 images and show that our method outperforms recent state-of-the-art image QA algorithms by a sizeable margin in our simulations. The code and the data from the subjective study are available at the LIVE website.", "paper_title": "Image information and visual quality", "paper_id": "WOS:000234761400017"}