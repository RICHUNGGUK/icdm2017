{"auto_keywords": [{"score": 0.029052207715183923, "phrase": "single_signs"}, {"score": 0.00481495049065317, "phrase": "continuous_sign_language_sentences"}, {"score": 0.004612147352416241, "phrase": "probabilistic_framework"}, {"score": 0.004494570360108983, "phrase": "recurring_signs"}, {"score": 0.004456044928556639, "phrase": "multiple_sign_language_video_sequences"}, {"score": 0.0038825121288253435, "phrase": "adjacent_signs"}, {"score": 0.0038326698261576023, "phrase": "sentence_video"}, {"score": 0.0037348894128879082, "phrase": "multidimensional_time_series_representation"}, {"score": 0.003546722347907148, "phrase": "skin_color_blobs"}, {"score": 0.0034562118159899772, "phrase": "color_video_sequences"}, {"score": 0.0033971539563683174, "phrase": "probabilistic_relational_distribution"}, {"score": 0.0032820385246266773, "phrase": "contour_and_edge_pixels"}, {"score": 0.0032398798149196432, "phrase": "skin_blobs"}, {"score": 0.0030898642396380662, "phrase": "low_dimensional_space"}, {"score": 0.0030239842475439814, "phrase": "relational_distributions"}, {"score": 0.0029722900280316216, "phrase": "time_series_trajectories"}, {"score": 0.0028963961037320805, "phrase": "multiple_sentences"}, {"score": 0.0028591768694918, "phrase": "iterated_conditional_modes"}, {"score": 0.0028349857332807347, "phrase": "icm"}, {"score": 0.0024799423362011582, "phrase": "mixed_collection"}, {"score": 0.002427035025924384, "phrase": "extracted_signemes"}, {"score": 0.0022457226013538343, "phrase": "different_contexts"}, {"score": 0.0021695394558562927, "phrase": "learned_sign_models"}, {"score": 0.0021049977753042253, "phrase": "test_sequences"}], "paper_keywords": ["pattern extraction", " sign language recognition", " signeme extraction", " sign modeling", " iterated conditional modes"], "paper_abstract": "We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is first transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences.", "paper_title": "Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs", "paper_id": "WOS:000309580600004"}