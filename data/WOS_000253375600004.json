{"auto_keywords": [{"score": 0.027020321613825064, "phrase": "mdp."}, {"score": 0.00481495049065317, "phrase": "near-optimal_policies"}, {"score": 0.004764859786310636, "phrase": "bellman-residual_minimization_based_fitted_policy_iteration"}, {"score": 0.004359500411441827, "phrase": "near-optimal_policy"}, {"score": 0.004291617358806677, "phrase": "continuous_space"}, {"score": 0.004224786847720058, "phrase": "markovian_decision_problem"}, {"score": 0.0040942189695983185, "phrase": "value-function-based_methods"}, {"score": 0.003926359151817743, "phrase": "fixed_policy"}, {"score": 0.0037261429556468217, "phrase": "policy-iteration_algorithm"}, {"score": 0.0035920758080335655, "phrase": "empirical_risk_minimization"}, {"score": 0.0035361001070845677, "phrase": "risk_function"}, {"score": 0.003480993634130542, "phrase": "high_magnitudes"}, {"score": 0.0031513240597392843, "phrase": "computed_policy"}, {"score": 0.003069867656852932, "phrase": "mixing_rate"}, {"score": 0.0028378798711397235, "phrase": "novel_capacity_concept"}, {"score": 0.002693018258362651, "phrase": "approximation_power"}, {"score": 0.0024377918036784336, "phrase": "linear_parameterization"}, {"score": 0.002387214876538434, "phrase": "new_algorithm"}, {"score": 0.002337684811187166, "phrase": "least-squares_policy_iteration"}, {"score": 0.002218299213266519, "phrase": "first_theoretical_result"}, {"score": 0.0021383680500215267, "phrase": "continuous_state-spaces"}, {"score": 0.0021049977753042253, "phrase": "single_trajectory"}], "paper_keywords": ["reinforcement learning", " policy iteration", " Bellman-residual minimization", " least-squares temporal difference learning", " off-policy learning", " nonparametric regression", " least-squares regression", " finite-sample bounds"], "paper_abstract": "In this paper we consider the problem of finding a near-optimal policy in a continuous space, discounted Markovian Decision Problem (MDP) by employing value-function-based methods when only a single trajectory of a fixed policy is available as the input. We study a policy-iteration algorithm where the iterates are obtained via empirical risk minimization with a risk function that penalizes high magnitudes of the Bellman-residual. Our main result is a finite-sample, high-probability bound on the performance of the computed policy that depends on the mixing rate of the trajectory, the capacity of the function set as measured by a novel capacity concept (the VC-crossing dimension), the approximation power of the function set and the controllability properties of the MDP. Moreover, we prove that when a linear parameterization is used the new algorithm is equivalent to Least-Squares Policy Iteration. To the best of our knowledge this is the first theoretical result for off-policy control learning over continuous state-spaces using a single trajectory.", "paper_title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path", "paper_id": "WOS:000253375600004"}