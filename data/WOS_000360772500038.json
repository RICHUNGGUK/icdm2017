{"auto_keywords": [{"score": 0.04890702046127258, "phrase": "conditional_log_likelihood"}, {"score": 0.00481495049065317, "phrase": "superparent-one"}, {"score": 0.004601045179623499, "phrase": "superparent_one-dependence_estimators"}, {"score": 0.004378469469879815, "phrase": "high_classification_accuracy"}, {"score": 0.00418387341007444, "phrase": "performance_improvement"}, {"score": 0.004149429072483132, "phrase": "individual_spodes"}, {"score": 0.003948613642680196, "phrase": "entire_ensemble_model"}, {"score": 0.003773049633656302, "phrase": "entire_ensemble"}, {"score": 0.00371115485111022, "phrase": "better_weight_distribution"}, {"score": 0.0036502716977381004, "phrase": "greedy_strategy"}, {"score": 0.0036052638669284947, "phrase": "spode"}, {"score": 0.003531474887552889, "phrase": "ensemble_spode_algorithm"}, {"score": 0.0033190180867511605, "phrase": "maximum_conditional_probability"}, {"score": 0.0032780806856932423, "phrase": "global_optimization_goal"}, {"score": 0.0032109670662222416, "phrase": "over-fitting_problem"}, {"score": 0.0031582637043009562, "phrase": "least_squares_error"}, {"score": 0.0030680994147125364, "phrase": "hierarchical_weights"}, {"score": 0.0029805015010906013, "phrase": "spode."}, {"score": 0.0029559346879594254, "phrase": "second_weight_layer"}, {"score": 0.0028953973490895746, "phrase": "local_spode_model"}, {"score": 0.002847858807513713, "phrase": "stochastic_gradient_descent_method"}, {"score": 0.002789528662623221, "phrase": "best_parameters"}, {"score": 0.0027437235598539904, "phrase": "good_scalability"}, {"score": 0.0025999744005191713, "phrase": "existing_ensemble"}, {"score": 0.002504880563070155, "phrase": "better_time_complexity"}, {"score": 0.0024333239726872604, "phrase": "public_benchmark"}, {"score": 0.0022962708568492734, "phrase": "state-of-the-art_ensemble"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Classification", " Gradient methods", " Machine learning", " Modeling structured", " Conditional likelihood"], "paper_abstract": "The ensemble of SuperParent one-dependence estimators (SPODEs) is one of the most effective improved algorithms. It achieves high classification accuracy while decreasing variance. However, most existing approaches only focus on performance improvement of individual SPODEs in selection and weighting procedures but overlook the importance of the entire ensemble model. Based on the assumption that the performance of the entire ensemble classifier can obtain better weight distribution than using the greedy strategy inside each SPODE, we propose an ensemble SPODE algorithm by maximizing conditional log likelihood (EODE-CLL). First, we choose the maximum conditional probability as the global optimization goal, which can avoid over-fitting problem compared with the least squares error. Second, the algorithm assigns hierarchical weights for SPODEs and the attributes inside SPODE. The second weight layer can help fully optimize local SPODE model. Finally, stochastic gradient descent method is used to search best parameters. It has good scalability, which has spawned batch and distributed version. Compared to the existing ensemble SPODEs, our proposed model achieves more accurate and robust classification results, while shows better time complexity. We conduct experiments on a public benchmark containing 36 datasets. The results of the experiments show that our EODE-CLL significantly outperforms state-of-the-art ensemble SPODE methods in terms of accuracy, F-measure, bias, and variance. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Efficient learning ensemble SuperParent-one-dependence estimator by maximizing conditional log likelihood", "paper_id": "WOS:000360772500038"}