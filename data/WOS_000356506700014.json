{"auto_keywords": [{"score": 0.04928409246925515, "phrase": "gaussian_random_field"}, {"score": 0.00481495049065317, "phrase": "bayesian_estimation"}, {"score": 0.004682414867878963, "phrase": "rkhs._reconstruction"}, {"score": 0.004553510708976456, "phrase": "noisy_data"}, {"score": 0.00445293569103488, "phrase": "machine_learning"}, {"score": 0.00428222231579898, "phrase": "regularized_optimization_problem"}, {"score": 0.004211069233133584, "phrase": "infinite-dimensional_reproducing_kernel_hilbert_space"}, {"score": 0.0039380388343479384, "phrase": "observed_data"}, {"score": 0.0038725824361608243, "phrase": "corresponding_rkhs_norm"}, {"score": 0.0037869901077566526, "phrase": "data_fit"}, {"score": 0.0036826452656927877, "phrase": "quadratic_loss"}, {"score": 0.003561205990687285, "phrase": "known_statistical_interpretation"}, {"score": 0.0034824716857663114, "phrase": "noisy_measurements"}, {"score": 0.0034245614958572012, "phrase": "rkhs_estimate"}, {"score": 0.003028219703809522, "phrase": "rkhs."}, {"score": 0.002895721231882758, "phrase": "statistical_interpretation"}, {"score": 0.0027690086059100495, "phrase": "absolute_value"}, {"score": 0.0027382142651723408, "phrase": "vapnik"}, {"score": 0.0027077654903829053, "phrase": "huber"}, {"score": 0.0025461742505323236, "phrase": "rkhs"}, {"score": 0.0023807981747706376, "phrase": "firm_statistical_foundation"}, {"score": 0.0023021895404322767, "phrase": "unknown_regularization_parameters"}, {"score": 0.002189105604712615, "phrase": "numerical_scheme"}, {"score": 0.0021406417439887907, "phrase": "bayesian_estimator"}, {"score": 0.0021049977753042253, "phrase": "absolute_value_loss"}], "paper_keywords": ["Gaussian processes", " kernel-based regularization", " Markov chain Monte Carlo (MCMC)", " regularization networks", " representer theorem", " reproducing kernel Hilbert spaces (RKHSs)", " support vector regression"], "paper_abstract": "Reconstruction of a function from noisy data is key in machine learning and is often formulated as a regularized optimization problem over an infinite-dimensional reproducing kernel Hilbert space (RKHS). The solution suitably balances adherence to the observed data and the corresponding RKHS norm. When the data fit is measured using a quadratic loss, this estimator has a known statistical interpretation. Given the noisy measurements, the RKHS estimate represents the posterior mean (minimum variance estimate) of a Gaussian random field with covariance proportional to the kernel associated with the RKHS. In this brief, we provide a statistical interpretation when more general losses are used, such as absolute value, Vapnik or Huber. Specifically, for any finite set of sampling locations (that includes where the data were collected), the maximum a posteriori estimate for the signal samples is given by the RKHS estimate evaluated at the sampling locations. This connection establishes a firm statistical foundation for several stochastic approaches used to estimate unknown regularization parameters. To illustrate this, we develop a numerical scheme that implements a Bayesian estimator with an absolute value loss. This estimator is used to learn a function from measurements contaminated by outliers.", "paper_title": "The Connection Between Bayesian Estimation of a Gaussian Random Field and RKHS", "paper_id": "WOS:000356506700014"}