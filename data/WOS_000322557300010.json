{"auto_keywords": [{"score": 0.029464507705449238, "phrase": "classical_definition"}, {"score": 0.00481495049065317, "phrase": "private_information"}, {"score": 0.0047051051659828275, "phrase": "online_query_auditing_problem"}, {"score": 0.004270347498153512, "phrase": "new_query"}, {"score": 0.0040215052006051235, "phrase": "true_answer"}, {"score": 0.003911578552479064, "phrase": "fundamental_problem"}, {"score": 0.003683565150278339, "phrase": "previous_work"}, {"score": 0.003484864867147635, "phrase": "previously_suggested_auditors"}, {"score": 0.003281643474564305, "phrase": "large_fraction"}, {"score": 0.0030476702431093687, "phrase": "new_model"}, {"score": 0.0029780219173522115, "phrase": "query_denials"}, {"score": 0.002843450389745497, "phrase": "simulatable_auditing_algorithm"}, {"score": 0.0028172726995962147, "phrase": "max_queries"}, {"score": 0.0026651772919894534, "phrase": "sensitive_value"}, {"score": 0.0025683581446365165, "phrase": "known_limitations"}, {"score": 0.0024522530645471065, "phrase": "probabilistic_notion"}, {"score": 0.002341384304635523, "phrase": "semantic_security"}, {"score": 0.0022878396720586044, "phrase": "stun_queries"}, {"score": 0.002225196379204285, "phrase": "simulatable_fashion"}, {"score": 0.002204697943520258, "phrase": "probabilistic_compromise"}, {"score": 0.002164264600379492, "phrase": "distributional_assumptions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Auditing", " Privacy-preserving", " Simulation paradigm"], "paper_abstract": "Imagine a data set consisting of private information about individuals. The online query auditing problem is: given a sequence of queries that have already been posed about the data, their corresponding answers and given a new query, deny the answer if privacy can be breached or give the true answer otherwise. We investigate the fundamental problem that query denials leak information. This problem was largely overlooked in previous work on auditing. Because of this oversight, some of the previously suggested auditors can be used by an attacker to compromise the privacy of a large fraction of the individuals in the data. To overcome this problem, we introduce a new model called simulatable auditing where query denials provably do not leak information. We present a simulatable auditing algorithm for max queries under the classical definition of privacy where a breach occurs if a sensitive value is fully compromised. Because of the known limitations of the classical definition of compromise, we describe a probabilistic notion of (partial) compromise, closely related to the notion of semantic security. We demonstrate that stun queries can be audited in a simulatable fashion under probabilistic compromise, making some distributional assumptions. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Denials leak information: Simulatable auditing", "paper_id": "WOS:000322557300010"}