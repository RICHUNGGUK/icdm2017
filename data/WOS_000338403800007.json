{"auto_keywords": [{"score": 0.04637570254730582, "phrase": "long-term_dependencies"}, {"score": 0.0367698002611785, "phrase": "smrnn"}, {"score": 0.00481495049065317, "phrase": "segmented-memory_recurrent_neural_networks"}, {"score": 0.0045726187330434025, "phrase": "recurrent_neural_networks"}, {"score": 0.0037834649508375544, "phrase": "increasing_network_size"}, {"score": 0.0034118234596993836, "phrase": "layer-local_unsupervised_pre-training_procedure"}, {"score": 0.003296212583539166, "phrase": "information_latching_problem"}, {"score": 0.0030414140732789186, "phrase": "longer_periods"}, {"score": 0.0028880836335117297, "phrase": "better_generalisation"}, {"score": 0.0025596276616952516, "phrase": "proposed_ebptt_algorithm"}, {"score": 0.0024586427394152196, "phrase": "big_networks"}, {"score": 0.002361632539339945, "phrase": "pre-training_procedure"}, {"score": 0.0022815262950738814, "phrase": "supervised_learning_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Recurrent neural networks", " Segmented-memory recurrent neural network", " Vanishing gradient problem", " Long-term dependencies", " Unsupervised pre-training"], "paper_abstract": "In general, recurrent neural networks have difficulties in learning long-term dependencies. The segmented-memory recurrent neural network (SMRNN) architecture together with the extended real-time recurrent learning (eRTRL) algorithm was proposed to circumvent this problem. Due to its computational complexity eRTRL becomes impractical with increasing network size. Therefore, we introduce the less complex extended backpropagation through time (eBPIT) for SMRNN together with a layer-local unsupervised pre-training procedure. A comparison on the information latching problem showed that eRTRL is better able to handle the latching of information over longer periods of time, even though eBPTT guaranteed a better generalisation when training was successful. Further, pre-training significantly improved the ability to learn long-term dependencies with eBIDTT. Therefore, the proposed eBPTT algorithm is suited for tasks that require big networks where eRTRL is impractical. The pre-training procedure itself is independent of the supervised learning algorithm and can improve learning in SMRNN in general. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Learning long-term dependencies in segmented-memory recurrent neural networks with backpropagation of error", "paper_id": "WOS:000338403800007"}