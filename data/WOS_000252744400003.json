{"auto_keywords": [{"score": 0.031284017462435476, "phrase": "sampling_error"}, {"score": 0.00481495049065317, "phrase": "decision_tree_learning"}, {"score": 0.004720043307726269, "phrase": "good_classification_accuracy"}, {"score": 0.004673290248999786, "phrase": "unseen_examples"}, {"score": 0.004604023526673226, "phrase": "decision_tree"}, {"score": 0.004337028108577593, "phrase": "sufficient_structure"}, {"score": 0.004230382915464492, "phrase": "correct_majority_class"}, {"score": 0.0037913267098534887, "phrase": "percentage_classification_rate"}, {"score": 0.0035360742296288213, "phrase": "bayes"}, {"score": 0.0034317185678527672, "phrase": "error_decomposition"}, {"score": 0.0033306180744375616, "phrase": "relative_contributions"}, {"score": 0.0032164136388931805, "phrase": "incorrect_determination"}, {"score": 0.0031845067058084583, "phrase": "majority_class"}, {"score": 0.0030296432347510687, "phrase": "majority_class_error"}, {"score": 0.00286795254228461, "phrase": "possible_bias"}, {"score": 0.0028113140998735366, "phrase": "attribute_selection_method"}, {"score": 0.002769568481822277, "phrase": "induction_algorithm"}, {"score": 0.0023846534541336326, "phrase": "selection_bias"}, {"score": 0.0021049977753042253, "phrase": "simple_underlying_model"}], "paper_keywords": ["decision tree learning", " error decomposition", " majority classes", " sampling error", " attribute selection bias"], "paper_abstract": "To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model.", "paper_title": "Structure and majority classes in decision tree learning", "paper_id": "WOS:000252744400003"}