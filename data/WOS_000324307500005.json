{"auto_keywords": [{"score": 0.03143394633372254, "phrase": "intermediate_result_size"}, {"score": 0.00481495049065317, "phrase": "recent_proliferation"}, {"score": 0.004763565662615503, "phrase": "social_networks"}, {"score": 0.004712726613162057, "phrase": "mobile_applications"}, {"score": 0.004637478829869904, "phrase": "online_services"}, {"score": 0.004514714250357749, "phrase": "data_gathering"}, {"score": 0.004233119608341877, "phrase": "challenging_issue"}, {"score": 0.004187917910692495, "phrase": "related_works"}, {"score": 0.004011867299107537, "phrase": "efficient_approaches"}, {"score": 0.003947765701573287, "phrase": "single_machine"}, {"score": 0.0038226070564362697, "phrase": "large-scale_dataset"}, {"score": 0.0033602844686125375, "phrase": "duplicate_detection"}, {"score": 0.003324450994910762, "phrase": "mapreduce"}, {"score": 0.0030017207229983385, "phrase": "candidate_record_pairs"}, {"score": 0.002859977408815691, "phrase": "shuffle_cost"}, {"score": 0.0028293966058473476, "phrase": "different_nodes"}, {"score": 0.00266693888510356, "phrase": "new_signature_scheme"}, {"score": 0.0026384167183586015, "phrase": "new_pruning_strategies"}, {"score": 0.0025546627686955656, "phrase": "candidate_pairs"}, {"score": 0.0024868972942701604, "phrase": "proposed_solution"}, {"score": 0.002306543826130521, "phrase": "experimental_results"}, {"score": 0.0022696277881969896, "phrase": "real_and_synthetic_datasets"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["duplicate detection", " MapReduce", " cloud"], "paper_abstract": "As the recent proliferation of social networks, mobile applications, and online services increased the rate of data gathering, to find near-duplicate records efficiently has become a challenging issue. Related works on this problem mainly aim to propose efficient approaches on a single machine. However, when processing large-scale dataset, the performance to identify duplicates is still far from satisfactory. In this paper, we try to handle the problem of duplicate detection applying MapReduce. We argue that the performance of utilizing MapReduce to detect duplicates mainly depends on the number of candidate record pairs and intermediate result size, which is related to the shuffle cost among different nodes in cluster. In this paper, we proposed a new signature scheme with new pruning strategies to minimize the number of candidate pairs and intermediate result size. The proposed solution is an exact one, which assures none duplicate record pair can be lost. The experimental results over both real and synthetic datasets demonstrate that our proposed signature-based method is efficient and scalable. Copyright (c) 2012 John Wiley & Sons, Ltd.", "paper_title": "Efficient and exact duplicate detection on cloud", "paper_id": "WOS:000324307500005"}