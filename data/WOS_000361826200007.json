{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sequential_classifiers"}, {"score": 0.004785022360244881, "phrase": "long_and_noisy_discrete-event_sequences"}, {"score": 0.004638136142210615, "phrase": "information_extraction"}, {"score": 0.004609301940749165, "phrase": "intrusion_detection"}, {"score": 0.004580646169804186, "phrase": "protein_fold_recognition"}, {"score": 0.004467785611058897, "phrase": "discrete_events"}, {"score": 0.004384960761826366, "phrase": "unordered_sets"}, {"score": 0.004237065611401975, "phrase": "order_dependence"}, {"score": 0.004158500036893521, "phrase": "data_instance"}, {"score": 0.004055998173160551, "phrase": "classification_problems"}, {"score": 0.003931402465972151, "phrase": "sequential_interactions"}, {"score": 0.003834476721674575, "phrase": "ordering_relationship"}, {"score": 0.003625009798162227, "phrase": "hidden_markov_models"}, {"score": 0.003557752688779838, "phrase": "frequent_sequences"}, {"score": 0.0034700069041302003, "phrase": "computing_string_kernels"}, {"score": 0.0032701580015554025, "phrase": "long_range_dependencies"}, {"score": 0.0030625949549620475, "phrase": "simple_algorithms"}, {"score": 0.0030340353187232947, "phrase": "highly_effective_sequential_classifiers"}, {"score": 0.002959167753690868, "phrase": "approximately_contiguous_subsequences"}, {"score": 0.0028951712636442906, "phrase": "demand-driven_basis"}, {"score": 0.002859223385284036, "phrase": "lightweight_and_flexible_subsequence_matching_function"}, {"score": 0.0028325548627901004, "phrase": "innovative_subsequence_enumeration_strategy"}, {"score": 0.002711344582144154, "phrase": "noisy_data"}, {"score": 0.002619748321217153, "phrase": "best_trade-off"}, {"score": 0.0025872113569311555, "phrase": "learning_time"}, {"score": 0.002430474136314913, "phrase": "learning_cost"}, {"score": 0.0022198795731382137, "phrase": "existing_solutions"}, {"score": 0.0021381565332182773, "phrase": "significant_accuracy_improvements"}, {"score": 0.0021049977753042253, "phrase": "evaluated_cases"}], "paper_keywords": ["Sequential classifiers", " Efficient learning", " Long range sequences", " Partial matching", " Approximately contiguous sequences"], "paper_abstract": "A variety of applications, such as information extraction, intrusion detection and protein fold recognition, can be expressed as sequences of discrete events or elements (rather than unordered sets of features), that is, there is an order dependence among the elements composing each data instance. These applications may be modeled as classification problems, and in this case the classifier should exploit sequential interactions among the elements, so that the ordering relationship among them is properly captured. Dominant approaches to this problem include: (i) learning Hidden Markov Models, (ii) exploiting frequent sequences extracted from the data and (iii) computing string kernels. Such approaches, however, are computationally hard and vulnerable to noise, especially if the data shows long range dependencies (i.e., long subsequences are necessary in order to model the data). In this paper we provide simple algorithms that build highly effective sequential classifiers. Our algorithms are based on enumerating approximately contiguous subsequences from the training set on a demand-driven basis, exploiting a lightweight and flexible subsequence matching function and an innovative subsequence enumeration strategy called pattern silhouettes, making our learning algorithms fast and the corresponding classifiers robust to noisy data. Our empirical results on a variety of datasets indicate that the best trade-off between accuracy and learning time is usually obtained by limiting the length of the subsequences by a factor of , which leads to a learning cost (where is the length of the sequence being classified). Finally, we show that, in most of the cases, our classifiers are faster than existing solutions (sometimes, by orders of magnitude), also providing significant accuracy improvements in most of the evaluated cases.", "paper_title": "Learning sequential classifiers from long and noisy discrete-event sequences efficiently", "paper_id": "WOS:000361826200007"}