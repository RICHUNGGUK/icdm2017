{"auto_keywords": [{"score": 0.04697668243769892, "phrase": "computer_graphics"}, {"score": 0.027773607799187222, "phrase": "average_shape"}, {"score": 0.00481495049065317, "phrase": "shape_matching"}, {"score": 0.004592905436175987, "phrase": "wide_variety"}, {"score": 0.004468117364177126, "phrase": "computer_vision"}, {"score": 0.004278807087784626, "phrase": "object_recognition"}, {"score": 0.0042452552248398445, "phrase": "medical_diagnosis"}, {"score": 0.004195418010131832, "phrase": "quantitative_morphological_analysis"}, {"score": 0.0041625173012740544, "phrase": "biological_operations"}, {"score": 0.004129873531628877, "phrase": "automatic_shape"}, {"score": 0.0041136473765874815, "phrase": "matching_techniques"}, {"score": 0.003986103192038901, "phrase": "object_surfaces"}, {"score": 0.0038930368103015467, "phrase": "inner_voxels"}, {"score": 0.0037133478610481994, "phrase": "magnetic_resonance_imagery"}, {"score": 0.003626626504149897, "phrase": "confocal_microscopes"}, {"score": 0.003555902292752528, "phrase": "point_correspondences"}, {"score": 0.003445591238183763, "phrase": "human_interactions"}, {"score": 0.003405109584265324, "phrase": "automatic_methods"}, {"score": 0.003378385493203297, "phrase": "matched_objects"}, {"score": 0.00327356326590461, "phrase": "combinatorial_explosions"}, {"score": 0.0030614565460256897, "phrase": "proposed_method"}, {"score": 0.0030254743224136273, "phrase": "dense_point_correspondences"}, {"score": 0.002897109710213723, "phrase": "second_volume"}, {"score": 0.0028293966058473476, "phrase": "first_volume"}, {"score": 0.0027741761880779535, "phrase": "voxel_intensities"}, {"score": 0.002752390133272192, "phrase": "mutiresolutional_pyramids"}, {"score": 0.0026986685585382347, "phrase": "computational_load"}, {"score": 0.00266693888510356, "phrase": "highly_plastic_objects"}, {"score": 0.0025638362414466278, "phrase": "similar_objects"}, {"score": 0.002455009746748708, "phrase": "matching_results"}, {"score": 0.00238816839721704, "phrase": "intermediate_volumes"}, {"score": 0.002163930783583212, "phrase": "brain_cells"}, {"score": 0.0021049977753042253, "phrase": "human_skull"}], "paper_keywords": ["shape matching three-dimensional (3-D) segmentation", " 3-D volume matching", " volume interpolation", " volume morphing"], "paper_abstract": "Recently, shape matching in three dimensions (3-D) has been gaining importance in a wide variety of fields such as computer graphics, computer vision, medicine, and biology, with applications such as object recognition, medical diagnosis, and quantitative morphological analysis of biological operations. Automatic shape matching techniques developed in the field of computer graphics handle object surfaces, but ignore intensities of inner voxels. In biology and medical imaging, voxel intensities obtained by computed tomography (CT), magnetic resonance imagery (MRI), and confocal microscopes are important to determine point correspondences. Nevertheless, most biomedical volume matching techniques require human interactions, and automatic methods assume matched objects to have very similar shapes so as to avoid combinatorial explosions of point. This article is aimed at decreasing the gap between the two fields. The proposed method automatically finds dense point correspondences between two grayscale volumes; i.e., finds a correspondent in the second volume for every voxel in the first volume, based on the voxel intensities. Mutiresolutional pyramids are introduced to reduce computational load and handle highly plastic objects. We calculate the average shape of a set of similar objects and give a measure of plasticity to compare them. Matching results can also be used to generate intermediate volumes for morphing. We use various data to validate the effectiveness of our method: we calculate the average shape and plasticity of a set of fly brain cells, and we also match a human skull and an orangutan", "paper_title": "Automatic 3-D grayscale volume matching and shape analysis", "paper_id": "WOS:000236674400017"}