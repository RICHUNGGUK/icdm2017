{"auto_keywords": [{"score": 0.03259375215011238, "phrase": "target_detection"}, {"score": 0.00481495049065317, "phrase": "center-surround_saliency_mechanisms"}, {"score": 0.004748371393578422, "phrase": "biologically_inspired_discriminant_object_tracker"}, {"score": 0.004575272950516775, "phrase": "discriminant_tracking"}, {"score": 0.0044702911400353535, "phrase": "top-down_tuning"}, {"score": 0.004408456735066782, "phrase": "saliency_mechanisms"}, {"score": 0.00428733103049275, "phrase": "visual_attention"}, {"score": 0.00418892838079239, "phrase": "discriminant_saliency"}, {"score": 0.003943481134959376, "phrase": "center-surround_saliency"}, {"score": 0.003610292749660581, "phrase": "tracking_problem"}, {"score": 0.0035110198699787013, "phrase": "continuous_target-background_classification"}, {"score": 0.003199345207727474, "phrase": "bottom-up_saliency"}, {"score": 0.003140401552000472, "phrase": "maximally_discriminant_set"}, {"score": 0.0029838294169759663, "phrase": "feature-based_attention_mechanism"}, {"score": 0.002942495422585405, "phrase": "target-tuned_top-down_discriminant_saliency_detector"}, {"score": 0.0027698788736535865, "phrase": "discriminant_features"}, {"score": 0.002731500545019039, "phrase": "target_location"}, {"score": 0.00269365253474335, "phrase": "video_frame"}, {"score": 0.0025121156016001864, "phrase": "natural_images"}, {"score": 0.002331909406363425, "phrase": "saliency_formulation"}, {"score": 0.0022571810910697013, "phrase": "unified_framework"}, {"score": 0.002236272403824371, "phrase": "classifier_design"}, {"score": 0.002195033004022173, "phrase": "automatic_tracker_initialization"}, {"score": 0.002164602220179823, "phrase": "scale_adaptation"}, {"score": 0.0021445492649196955, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_discriminant_saliency_tracker"}], "paper_keywords": ["Object tracking", " discriminant tracking", " saliency", " attention", " motion saliency", " automatic target initialization", " scale adaptive tracking", " discriminant center-surround architecture", " video modeling"], "paper_abstract": "A biologically inspired discriminant object tracker is proposed. It is argued that discriminant tracking is a consequence of top-down tuning of the saliency mechanisms that guide the deployment of visual attention. The principle of discriminant saliency is then used to derive a tracker that implements a combination of center-surround saliency, a spatial spotlight of attention, and feature-based attention. In this framework, the tracking problem is formulated as one of continuous target-background classification, implemented in two stages. The first, or learning stage, combines a focus of attention (FoA) Mechanism, and bottom-up saliency to identify a maximally discriminant set of features for target detection. The second, or detection stage, uses a feature-based attention mechanism and a target-tuned top-down discriminant saliency detector to detect the target. Overall, the tracker iterates between learning discriminant features from the target location in a video frame and detecting the location of the target in the next. The statistics of natural images are exploited to derive an implementation which is conceptually simple and computationally efficient. The saliency formulation is also shown to establish a unified framework for classifier design, target detection, automatic tracker initialization, and scale adaptation. Experimental results show that the proposed discriminant saliency tracker outperforms a number of state-of-the-art trackers in the literature.", "paper_title": "Biologically Inspired Object Tracking Using Center-Surround Saliency Mechanisms", "paper_id": "WOS:000314792900003"}