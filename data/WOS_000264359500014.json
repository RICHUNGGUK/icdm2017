{"auto_keywords": [{"score": 0.04381934363111791, "phrase": "based_bayesian_network_paradigm"}, {"score": 0.023398400301607763, "phrase": "flexible_classifiers"}, {"score": 0.00481495049065317, "phrase": "kernel_density_estimation"}, {"score": 0.004644014990871423, "phrase": "bayesian_network_based_classifiers_continuous_variables"}, {"score": 0.004342430078306461, "phrase": "gaussian_distribution"}, {"score": 0.003997891221012094, "phrase": "bayesian_network"}, {"score": 0.003916098498242888, "phrase": "true_density"}, {"score": 0.0038558497470953306, "phrase": "continuous_variables"}, {"score": 0.0037381083634085424, "phrase": "tree-augmented_naive_bayes"}, {"score": 0.0036437276275883015, "phrase": "bayesian"}, {"score": 0.003459191419478298, "phrase": "novel_kernel"}, {"score": 0.003353521531649663, "phrase": "strong_consistency_properties"}, {"score": 0.003301899275278983, "phrase": "presented_classifiers"}, {"score": 0.003168079471049896, "phrase": "mutual_information"}, {"score": 0.0028864239961184364, "phrase": "natural_extension"}, {"score": 0.002841971477743441, "phrase": "flexible_naive_bayes_classifier"}, {"score": 0.0027982266175339246, "phrase": "john"}, {"score": 0.002769397490051997, "phrase": "langley"}, {"score": 0.002740885774408238, "phrase": "g.h._john"}, {"score": 0.0027126684113045756, "phrase": "p._langley"}, {"score": 0.002670884656785558, "phrase": "continuous_distributions"}, {"score": 0.002643386043540555, "phrase": "bayesian_classifiers"}, {"score": 0.0024971144603069006, "phrase": "artificial_intelligence"}, {"score": 0.002310578855290687, "phrase": "flexible_tree-augmented_naive_bayes"}, {"score": 0.0022283519468680475, "phrase": "supervised_classification"}, {"score": 0.0021049977753042253, "phrase": "competitive_errors"}], "paper_keywords": ["Bayesian network", " Kernel density estimation", " Supervised classification", " Flexible naive Bayes"], "paper_abstract": "When learning Bayesian network based classifiers continuous variables are usually handled by discretization, or assumed that they follow a Gaussian distribution. This work introduces the kernel based Bayesian network paradigm for supervised classification. This paradigm is a Bayesian network which estimates the true density of the continuous variables using kernels. Besides, tree-augmented naive Bayes, k-dependence Bayesian classifier and complete graph classifier are adapted to the novel kernel based Bayesian network paradigm. Moreover, the strong consistency properties of the presented classifiers are proved and an estimator of the mutual information based on kernels is presented. The classifiers presented in this work can be seen as the natural extension of the flexible naive Bayes classifier proposed by John and Langley [G.H. John, P. Langley, Estimating continuous distributions in Bayesian classifiers, in: Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, 1995, pp. 338-345], breaking with its strong independence assumption. Flexible tree-augmented naive Bayes seems to have superior behavior for supervised classification among the flexible classifiers. Besides, flexible classifiers presented have obtained competitive errors compared with the state-of-the-art classifiers. (C) 2008 Elsevier Inc. All rights reserved.", "paper_title": "Bayesian classifiers based on kernel density estimation: Flexible classifiers", "paper_id": "WOS:000264359500014"}