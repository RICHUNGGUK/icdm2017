{"auto_keywords": [{"score": 0.040012451990183516, "phrase": "elm"}, {"score": 0.00481495049065317, "phrase": "elm_hidden_nodes"}, {"score": 0.004740476936977635, "phrase": "extreme_leaning_machine"}, {"score": 0.004488734023485018, "phrase": "input_weights"}, {"score": 0.004152018764100109, "phrase": "generalization_performance"}, {"score": 0.003870541730075597, "phrase": "meta-learning_model"}, {"score": 0.0037224647319912293, "phrase": "meta-elm."}, {"score": 0.0033897080229676516, "phrase": "elm."}, {"score": 0.0032855095476183372, "phrase": "meta-elm_learning"}, {"score": 0.0028771413440398614, "phrase": "training_data"}, {"score": 0.002766963729586353, "phrase": "top_elm"}, {"score": 0.0026609940110902666, "phrase": "base_elms"}, {"score": 0.002619748321217153, "phrase": "hidden_nodes"}, {"score": 0.0022760856906838814, "phrase": "proposed_meta-elm_model"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme learning machine", " Meta-learning", " ELM hidden node"], "paper_abstract": "Extreme Leaning Machine (ELM) simply randomly assigns input weights and biases, ineluctably leading to certain stochastic behaviors and reducing generalization performance. In this paper, we propose a meta-learning model of ELM, called Meta-ELM. The Meta-ELM architecture consists of several base ELMs and one top ELM. Therefore, the Meta-ELM learning proceeds in two stages. First, each base ELM is trained on a subset of the training data. Then, the top ELM is learned with the base ELMs as hidden nodes. Theoretical analysis and experimental results on a few artificial and benchmark regression datasets show that the proposed Meta-ELM model is feasible and effective. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Meta-ELM: ELM with ELM hidden nodes", "paper_id": "WOS:000331851700010"}