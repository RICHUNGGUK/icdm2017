{"auto_keywords": [{"score": 0.03788952838179796, "phrase": "gradient_local_search"}, {"score": 0.03291721668032435, "phrase": "proposed_dmgbde"}, {"score": 0.00481495049065317, "phrase": "gradient-based_local_search"}, {"score": 0.004502943680955863, "phrase": "real-parameter_optimization_problems"}, {"score": 0.00445293569103488, "phrase": "nonlinear_and_multimodal_functions"}, {"score": 0.00409508755819924, "phrase": "classical_de_harbors"}, {"score": 0.00400459716272594, "phrase": "limited_local_search_ability"}, {"score": 0.00389427992231721, "phrase": "gradient-based_algorithms"}, {"score": 0.0038510053729959074, "phrase": "powerful_local_search_ability"}, {"score": 0.0037449033166879874, "phrase": "new_algorithm"}, {"score": 0.0036621225944272256, "phrase": "diversity-maintained_de"}, {"score": 0.0035216195857272403, "phrase": "dmgbde"}, {"score": 0.0034245614958572012, "phrase": "approximate_gradient-based_algorithms"}, {"score": 0.003166751241099304, "phrase": "primary_novelties"}, {"score": 0.0029612292670237505, "phrase": "gradient-based_algorithm"}, {"score": 0.002847540262784593, "phrase": "different_manner"}, {"score": 0.0027535633837592597, "phrase": "diversity-maintained_mutation"}, {"score": 0.0026478260192958924, "phrase": "learning_procedure"}, {"score": 0.002603758927691463, "phrase": "searched_best_individual"}, {"score": 0.0025461386232786356, "phrase": "numerical_experiments"}, {"score": 0.002475898613632249, "phrase": "benchmark_problems"}, {"score": 0.0023807981747706376, "phrase": "proposed_dmgbde._simulation_results"}, {"score": 0.0023021895404322767, "phrase": "classical_de"}, {"score": 0.0022261705957258506, "phrase": "diversity-based_mutation"}], "paper_keywords": ["Differential evolution", " Gradient local search", " Diversity-maintained mutation", " Ability to continue searching"], "paper_abstract": "Differential evolution (DE) has been used to solve real-parameter optimization problems with nonlinear and multimodal functions for more than a decade of years. However, it is pointed out that this classical DE harbors restricted efficiency and limited local search ability. Inspired by that gradient-based algorithms have powerful local search ability, we propose a new algorithm, which is diversity-maintained DE based on gradient local search (namely, DMGBDE), by incorporating approximate gradient-based algorithms into the DE search while maintaining the diversity of the population. The primary novelties of the proposed DMGBDE are the following: (1) the gradient-based algorithm is embedded into DE in a different manner and (2) a diversity-maintained mutation is introduced to slow down the learning procedure from the searched best individual. We conduct numerical experiments with a number of benchmark problems to measure the performance of the proposed DMGBDE. Simulation results show that the proposed DMGBDE outperforms classical DE and variant without gradient local search or diversity-based mutation. Moreover, comparison with some other recently reported approaches indicates that our proposed DMGBDE is rather competitive.", "paper_title": "Diversity-maintained differential evolution embedded with gradient-based local search", "paper_id": "WOS:000321644600018"}