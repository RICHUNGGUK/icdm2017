{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "stochastic_demand"}, {"score": 0.004761850836420121, "phrase": "finite_horizons"}, {"score": 0.004309434349688514, "phrase": "q-learning_approach"}, {"score": 0.004054299982508618, "phrase": "optimal_dynamic_packaging_decision"}, {"score": 0.004009555045206973, "phrase": "e-retailing_setting"}, {"score": 0.00392153537973826, "phrase": "practical_application"}, {"score": 0.0038782499819752423, "phrase": "dynamic_packaging"}, {"score": 0.003814212645832246, "phrase": "large_number"}, {"score": 0.003730465168354817, "phrase": "normal_q-learning_approach"}, {"score": 0.003608266640131664, "phrase": "excessively_large_state_space"}, {"score": 0.003451517638449501, "phrase": "tabular_form"}, {"score": 0.003319939715683536, "phrase": "excessive_amount"}, {"score": 0.0030715948036518603, "phrase": "state_space"}, {"score": 0.0029544572689681934, "phrase": "random_exploration"}, {"score": 0.0027031706928674092, "phrase": "event-driven_nature"}, {"score": 0.0026584843838039166, "phrase": "dynamic_packaging_problem"}, {"score": 0.002614534850703364, "phrase": "markov_decision_process_model"}, {"score": 0.002542890176213921, "phrase": "states_generalization_approach"}, {"score": 0.0025008468316682036, "phrase": "distortion_measure"}, {"score": 0.0021763973978248005, "phrase": "simulated_test"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["E-retailing", " Dynamic packaging", " Q-learning"], "paper_abstract": "This paper investigates how intelligent an agent may utilize a Q-learning approach, a simulation-based stochastic technique, to make optimal dynamic packaging decision in e-retailing setting. When the practical application of dynamic packaging involves a large number of products, normal Q-learning approach would encounter two major problems due to excessively large state space. First, learning the Q-values in tabular form may be infeasible because of the excessive amount of memory needed to store the table. Second, rewards in the state space may be so sparse that with random exploration they will only be discovered extremely slowly. This paper first describes the state-dependent and event-driven nature of the dynamic packaging problem with a Markov decision process model, then proposes a states generalization approach based on distortion measure, and finally puts forward a heuristic based exploration/exploitation policy which is used to improve the convergence in Q-learning. We validate our approach in a simulated test. (C) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Dynamic packaging in e-retailing with stochastic demand over finite horizons: A Q-learning approach", "paper_id": "WOS:000264182800048"}