{"auto_keywords": [{"score": 0.04728331461408164, "phrase": "unbiased_estimators"}, {"score": 0.03609280679826321, "phrase": "integral_transform"}, {"score": 0.014165934521712848, "phrase": "lower_bounds"}, {"score": 0.013019827484251326, "phrase": "proposed_class"}, {"score": 0.012145077575561523, "phrase": "hilbert_subspace"}, {"score": 0.011050116263806765, "phrase": "likelihood-ratio_function"}, {"score": 0.00481495049065317, "phrase": "general_classes_of_performance"}, {"score": 0.004719473826242326, "phrase": "parameter_estimation_-_part"}, {"score": 0.004377921202912336, "phrase": "new_class"}, {"score": 0.004269630481719451, "phrase": "mean_square_error"}, {"score": 0.004227161490216867, "phrase": "mse"}, {"score": 0.004122490120202936, "phrase": "deterministic_parameters"}, {"score": 0.003766908559075843, "phrase": "estimation_error"}, {"score": 0.0035648266669315943, "phrase": "linear_transformations"}, {"score": 0.00327356326590461, "phrase": "traditional_derivative_and_sampling_operators"}, {"score": 0.0030211613688439346, "phrase": "cramer-rao"}, {"score": 0.0029909911944871422, "phrase": "bhattacharyya"}, {"score": 0.002946312377831317, "phrase": "mcaulay-seidman"}, {"score": 0.002830410693265704, "phrase": "well-known_lower_bounds"}, {"score": 0.0025473475682739784, "phrase": "new_lower_bound"}, {"score": 0.002411065186457913, "phrase": "fourier"}, {"score": 0.0023040642263522505, "phrase": "proposed_bound"}, {"score": 0.00223570488212345, "phrase": "better_prediction"}, {"score": 0.0022022878843247257, "phrase": "threshold_region"}, {"score": 0.0021693692831601745, "phrase": "maximum-likelihood_estimator"}, {"score": 0.0021049977753042253, "phrase": "single_tone_estimation"}], "paper_keywords": ["Mean square error bounds", " non-Bayesian estimation", " parameter estimation", " performance lower bounds", " threshold SNR", " uniformly unbiased estimators"], "paper_abstract": "In this paper, a new class of lower bounds on the mean square error (MSE) of unbiased estimators of deterministic parameters is proposed. Derivation of the proposed class is performed by projecting each entry of the vector of estimation error on a Hilbert subspace of L(2). This Hilbert subspace contains linear transformations of elements in the domain of an integral transform of the likelihood-ratio function. The integral transform generalizes the traditional derivative and sampling operators, which are applied on the likelihood-ratio function for computation of performance lower bounds, such as Cramer-Rao, Bhattacharyya, and McAulay-Seidman bounds. It is shown that some well-known lower bounds on the MSE of unbiased estimators can be derived from this class by modifying the kernel of the integral transform. A new lower bound is derived from the proposed class using the kernel of the Fourier transform. In comparison with other existing bounds, the proposed bound is computationally manageable and provides better prediction of the threshold region of the maximum-likelihood estimator, in the problem of single tone estimation.", "paper_title": "General Classes of Performance Lower Bounds for Parameter Estimation - Part I: Non-Bayesian Bounds for Unbiased Estimators", "paper_id": "WOS:000283260900019"}