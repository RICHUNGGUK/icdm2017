{"auto_keywords": [{"score": 0.033491012821962436, "phrase": "discriminant_vector"}, {"score": 0.013923632809139585, "phrase": "discriminant_vectors"}, {"score": 0.00481495049065317, "phrase": "kernel_foley-sammon_optimal_discriminant_vectors"}, {"score": 0.004685575787741084, "phrase": "important_role"}, {"score": 0.0046432244185370605, "phrase": "statistical_pattern_recognition"}, {"score": 0.0045804107341714, "phrase": "popular_method"}, {"score": 0.004518442937844318, "phrase": "foley-sammon_optimal_discriminant_vectors"}, {"score": 0.00429828390000229, "phrase": "optimal_set"}, {"score": 0.0041827333568982055, "phrase": "fisher_discriminant_criterion"}, {"score": 0.004126123386192962, "phrase": "orthogonal_constraint"}, {"score": 0.004070276454809864, "phrase": "fsodvs_method"}, {"score": 0.004015182362035361, "phrase": "classic_fisher_linear_discriminant_analysis"}, {"score": 0.003978867121074052, "phrase": "flda"}, {"score": 0.0037336520904376687, "phrase": "kernel_foley-sammon"}, {"score": 0.003584025547693833, "phrase": "nonlinear_extension"}, {"score": 0.003503497128638099, "phrase": "kernel_trick"}, {"score": 0.003424771860049617, "phrase": "current_kfsodvs_algorithm"}, {"score": 0.00334780964309978, "phrase": "heavy_computation_problem"}, {"score": 0.0030988045934942587, "phrase": "cubic_complexity"}, {"score": 0.002753224894944965, "phrase": "fast_algorithm"}, {"score": 0.0026307897899760383, "phrase": "rank-one_update"}, {"score": 0.002491015208328868, "phrase": "square_complexity"}, {"score": 0.002305589818994465, "phrase": "optimally_constrained_generalized_rayleigh_quotient"}, {"score": 0.002193013499219711, "phrase": "extensive_experiments"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithms"}], "paper_keywords": ["Dimensionality reduction", " discriminant analysis", " kernel Foley-Sammon optimal discriminant vectors (KFSODVs)", " principal eigenvector"], "paper_abstract": "Discriminant analysis plays an important role in statistical pattern recognition. A popular method is the Foley-Sammon optimal discriminant vectors (FSODVs) method, which aims to find an optimal set of discriminant vectors that maximize the Fisher discriminant criterion under the orthogonal constraint. The FSODVs method outperforms the classic Fisher linear discriminant analysis (FLDA) method in the sense that it can solve more discriminant vectors for recognition. Kernel Foley-Sammon optimal discriminant vectors (KFSODVs) is a nonlinear extension of FSODVs via the kernel trick. However, the current KFSODVs algorithm may suffer from the heavy computation problem since it involves computing the inverse of matrices when solving each discriminant vector, resulting in a cubic complexity for each discriminant vector. This is costly when the number of discriminant vectors to be computed is large. In this paper, we propose a fast algorithm for solving the KFSODVs, which is based on rank-one update (ROU) of the eigensytems. It only requires a square complexity for each discriminant vector. Moreover, we also generalize our method to efficiently solve a family of optimally constrained generalized Rayleigh quotient (OCGRQ) problems which include many existing dimensionality reduction techniques. We conduct extensive experiments on several real data sets to demonstrate the effectiveness of the proposed algorithms.", "paper_title": "A Rank-One Update Algorithm for Fast Solving Kernel Foley-Sammon Optimal Discriminant Vectors", "paper_id": "WOS:000275040300003"}