{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "classification_rules"}, {"score": 0.04356786662400934, "phrase": "large-scale_and_high-dimensional_data_sets"}, {"score": 0.03104244392514206, "phrase": "multi-threaded_cpu_algorithm"}, {"score": 0.004689321062844586, "phrase": "multiple_instance_learning"}, {"score": 0.004597244847332915, "phrase": "challenging_task"}, {"score": 0.004536862734384202, "phrase": "supervised_learning"}, {"score": 0.004477270137801732, "phrase": "data_mining"}, {"score": 0.003820144626025562, "phrase": "computing_time"}, {"score": 0.0034363735647539267, "phrase": "multiple_instance_problems"}, {"score": 0.0033245147666408157, "phrase": "gpu_model"}, {"score": 0.003216285378512588, "phrase": "multiple_gpus"}, {"score": 0.0028739236927637288, "phrase": "simd"}, {"score": 0.0027437235598539904, "phrase": "data_sets"}, {"score": 0.002707620139970122, "phrase": "experimental_results"}, {"score": 0.002636828502928141, "phrase": "computation_time"}, {"score": 0.002175913488493802, "phrase": "rules_interpreter"}, {"score": 0.002147265537489893, "phrase": "great_efficiency"}], "paper_keywords": ["Multi-instance learning", " Classification", " Parallel computing", " GPU"], "paper_abstract": "Multiple instance learning is a challenging task in supervised learning and data mining. However, algorithm performance becomes slow when learning from large-scale and high-dimensional data sets. Graphics processing units (GPUs) are being used for reducing computing time of algorithms. This paper presents an implementation of the G3P-MI algorithm on GPUs for solving multiple instance problems using classification rules. The GPU model proposed is distributable to multiple GPUs, seeking for its scalability across large-scale and high-dimensional data sets. The proposal is compared to the multi-threaded CPU algorithm with streaming SIMD extensions parallelism over a series of data sets. Experimental results report that the computation time can be significantly reduced and its scalability improved. Specifically, an speedup of up to 149 can be achieved over the multi-threaded CPU algorithm when using four GPUs, and the rules interpreter achieves great efficiency and runs over 108 billion genetic programming operations per second.", "paper_title": "Speeding up multiple instance learning classification rules on GPUs", "paper_id": "WOS:000356297500006"}