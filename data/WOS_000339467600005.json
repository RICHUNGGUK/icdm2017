{"auto_keywords": [{"score": 0.027994900841407006, "phrase": "ccd"}, {"score": 0.00481495049065317, "phrase": "video_copy_detection_benchmark"}, {"score": 0.0041703614123283165, "phrase": "trec_video_retrieval"}, {"score": 0.0038810583785936505, "phrase": "main_contributions"}, {"score": 0.0035156341294017685, "phrase": "evolving_design"}, {"score": 0.0034220655476652683, "phrase": "evaluation_framework"}, {"score": 0.0030997236644076196, "phrase": "high-level_overview"}, {"score": 0.0029901699209266435, "phrase": "best-performing_approaches"}, {"score": 0.0026841368358805407, "phrase": "content-based_copy_detection"}, {"score": 0.00249767641333441, "phrase": "large_collection"}, {"score": 0.002453116398642071, "phrase": "synthetic_queries"}, {"score": 0.002364479992411814, "phrase": "trecvid"}, {"score": 0.00216263228969761, "phrase": "normalized_detection_cost_framework"}, {"score": 0.0021049977753042253, "phrase": "particular_evaluation_design_choices"}], "paper_keywords": ["Algorithms", " Experimentation", " Measurement", " Performance", " Video copy detection", " multimedia", " evaluation", " TRECVID"], "paper_abstract": "This article presents an overview of the video copy detection benchmark which was run over a period of 4 years (2008-2011) as part of the TREC Video Retrieval (TRECVID) workshop series. The main contributions of the article include i) an examination of the evolving design of the evaluation framework and its components (system tasks, data, measures); a high-level overview of results and best-performing approaches; and a discussion of lessons learned over the four years. The content-based copy detection (CCD) benchmark worked with a large collection of synthetic queries, which is atypical for TRECVID, as was the use of a normalized detection cost framework. These particular evaluation design choices are motivated and appraised.", "paper_title": "Content-Based Video Copy Detection Benchmarking at TRECVID", "paper_id": "WOS:000339467600005"}