{"auto_keywords": [{"score": 0.03980713684810354, "phrase": "adaptive_cgf"}, {"score": 0.02169310775802782, "phrase": "cgf"}, {"score": 0.010612387000973441, "phrase": "simulator-based_training"}, {"score": 0.008810723403004508, "phrase": "real_time"}, {"score": 0.008079458561742216, "phrase": "self-organizing_neural_network"}, {"score": 0.007186888701127447, "phrase": "rule-based_cgf."}, {"score": 0.006041468057027329, "phrase": "rule-based_cgf"}, {"score": 0.005349525638716055, "phrase": "human_subjects"}, {"score": 0.0047113960612140335, "phrase": "constant_pursuit"}, {"score": 0.004670597652149685, "phrase": "increasing_level"}, {"score": 0.004530552828603269, "phrase": "doctrine-driven_computer-generated_forces"}, {"score": 0.00428146713705618, "phrase": "doctrine-driven_cgf"}, {"score": 0.0041170581247889654, "phrase": "complex_expert_knowledge"}, {"score": 0.004028452405970376, "phrase": "trainees'_progress"}, {"score": 0.0035662378173267647, "phrase": "air_combat_maneuvering_strategies"}, {"score": 0.0034143003168921114, "phrase": "state_space"}, {"score": 0.003384695931421288, "phrase": "action_space"}, {"score": 0.0031845067058084583, "phrase": "hierarchical_doctrine"}, {"score": 0.0030092017034068666, "phrase": "model_complexity"}, {"score": 0.002918826667294298, "phrase": "first_case_study"}, {"score": 0.002818850283787242, "phrase": "effective_air_combat_maneuvers"}, {"score": 0.002770150086489194, "phrase": "subsequent_case_study"}, {"score": 0.002629026692001082, "phrase": "adaptive_cgf."}, {"score": 0.0025389513338018414, "phrase": "positive_outcome"}, {"score": 0.002347378595930709, "phrase": "better_understanding"}, {"score": 0.0023168823213024856, "phrase": "existing_constraints"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Simulator-based training", " Reinforcement learning", " Self-organizing neural network"], "paper_abstract": "Simulator-based training is in constant pursuit of increasing level of realism. The transition from doctrine-driven computer-generated forces (CGF) to adaptive CGF represents one such effort. The use of doctrine-driven CGF is fraught with challenges such as modeling of complex expert knowledge and adapting to the trainees' progress in real time. Therefore, this paper reports on how the use of adaptive CGF can overcome these challenges. Using a self-organizing neural network to implement the adaptive CGF, air combat maneuvering strategies are learned incrementally and generalized in real time. The state space and action space are extracted from the same hierarchical doctrine used by the rule-based CGF. In addition, this hierarchical doctrine is used to bootstrap the self-organizing neural network to improve learning efficiency and reduce model complexity. Two case studies are conducted. The first case study shows how adaptive CGF can converge to the effective air combat maneuvers against rule-based CGF. The subsequent case study replaces the rule-based CGF with human pilots as the opponent to the adaptive CGF. The results from these two case studies show how positive outcome from learning against rule-based CGF can differ markedly from learning against human subjects for the same tasks. With a better understanding of the existing constraints, an adaptive CGF that performs well against rule-based CGF and human subjects can be designed. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Adaptive computer-generated forces for simulator-based training", "paper_id": "WOS:000324663000019"}