{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "intermediate_semantic_representation"}, {"score": 0.004736434518535051, "phrase": "annotated_auxiliary_dataset"}, {"score": 0.004621043537046209, "phrase": "different_classes"}, {"score": 0.00445318311339027, "phrase": "low-level_feature_space"}, {"score": 0.004398589723893764, "phrase": "semantic_representation_space"}, {"score": 0.00430907756010007, "phrase": "auxiliary_dataset"}, {"score": 0.004186799638538032, "phrase": "target_dataset"}, {"score": 0.003464890914694523, "phrase": "novel_framework"}, {"score": 0.003284361433827464, "phrase": "second_limitation"}, {"score": 0.003244050414264445, "phrase": "prototype_sparsity_problem"}, {"score": 0.0031260524791486347, "phrase": "target_class"}, {"score": 0.0030497740719977835, "phrase": "zero-shot_learning"}, {"score": 0.0030123335878664064, "phrase": "semantic_representation"}, {"score": 0.0029147175927233546, "phrase": "novel_heterogeneous_multi-view_hypergraph_label_propagation_method"}, {"score": 0.002797121441432477, "phrase": "transductive_embedding_space"}, {"score": 0.002728847267539957, "phrase": "complementary_information"}, {"score": 0.0026953359825383624, "phrase": "different_semantic_representations"}, {"score": 0.002629539710839523, "phrase": "manifold_structures"}, {"score": 0.002607965563121546, "phrase": "multiple_representation_spaces"}, {"score": 0.002575934917174136, "phrase": "coherent_manner"}, {"score": 0.002523420125608051, "phrase": "extensive_experiments"}, {"score": 0.002492425259337031, "phrase": "proposed_approach"}, {"score": 0.002431570196486513, "phrase": "projection_shift"}, {"score": 0.0024017007954518065, "phrase": "auxiliary_and_target_domains"}, {"score": 0.002314271047073693, "phrase": "multiple_semantic_representations"}, {"score": 0.0022484715639779153, "phrase": "existing_methods"}, {"score": 0.0022117133940361025, "phrase": "n-shot_recognition"}, {"score": 0.002166607745253829, "phrase": "video_benchmark_datasets"}, {"score": 0.0021049977753042253, "phrase": "novel_cross-view_annotation_tasks"}], "paper_keywords": ["Transducitve learning", " multi-view Learning", " transfer Learning", " zero-shot Learning", " heterogeneous hypergraph"], "paper_abstract": "Most existing zero-shot learning approaches exploit transfer learning via an intermediate semantic representation shared between an annotated auxiliary dataset and a target dataset with different classes and no annotation. A projection from a low-level feature space to the semantic representation space is learned from the auxiliary dataset and applied without adaptation to the target dataset. In this paper we identify two inherent limitations with these approaches. First, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. The second limitation is the prototype sparsity problem which refers to the fact that for each target class, only a single prototype is available for zero-shot learning given a semantic representation. To overcome this problem, a novel heterogeneous multi-view hypergraph label propagation method is formulated for zero-shot learning in the transductive embedding space. It effectively exploits the complementary information offered by different semantic representations and takes advantage of the manifold structures of multiple representation spaces in a coherent manner. We demonstrate through extensive experiments that the proposed approach (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets, and (4) enables novel cross-view annotation tasks.", "paper_title": "Transductive Multi-View Zero-Shot Learning", "paper_id": "WOS:000362411000014"}