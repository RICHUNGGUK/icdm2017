{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "human_motion_capture_data"}, {"score": 0.04208347954551837, "phrase": "motion_states"}, {"score": 0.02460677473921949, "phrase": "motionexplorer"}, {"score": 0.004759507166427077, "phrase": "hierarchical_aggregation"}, {"score": 0.004491678520807451, "phrase": "human_motion"}, {"score": 0.00445712058827745, "phrase": "large_motion"}, {"score": 0.004439941010250576, "phrase": "capture_data_collections"}, {"score": 0.004388796812234033, "phrase": "special_type"}, {"score": 0.004355026964936104, "phrase": "multivariate_time"}, {"score": 0.004338239182984759, "phrase": "series_data"}, {"score": 0.004141742095830515, "phrase": "key_tasks"}, {"score": 0.004078232992955235, "phrase": "motion_data"}, {"score": 0.0036884198249734863, "phrase": "human_motion_data"}, {"score": 0.0036038701178581403, "phrase": "visual_summaries"}, {"score": 0.0035761182400032487, "phrase": "drill-down_functionality"}, {"score": 0.003534889186471058, "phrase": "large_motion_data_collections"}, {"score": 0.0034008496666026585, "phrase": "appropriate_visual_retrieval_and_analysis_support"}, {"score": 0.0032972747743006603, "phrase": "large_motion_data"}, {"score": 0.003147778277286763, "phrase": "domain_experts"}, {"score": 0.003111472132927442, "phrase": "exploratory_search_system"}, {"score": 0.003075583447856341, "phrase": "interactive_aggregation"}, {"score": 0.00297037471252423, "phrase": "data_navigation"}, {"score": 0.0028576796380512157, "phrase": "overview-first_type_visualization"}, {"score": 0.0027706013216537042, "phrase": "interesting_sub-sequences"}, {"score": 0.002707033618304716, "phrase": "query-by-example_metaphor"}, {"score": 0.002554406040896888, "phrase": "close_collaboration"}, {"score": 0.0025249264547573943, "phrase": "targeted_users"}, {"score": 0.0024669814850296146, "phrase": "human_motion_synthesis"}, {"score": 0.002410363090950055, "phrase": "summative_field_study"}, {"score": 0.002345944657572499, "phrase": "laboratory_design_study"}], "paper_keywords": ["Visual analytics", " exploratory search", " multivariate time series", " motion capture data", " data aggregation", " cluster glyph"], "paper_abstract": "We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.", "paper_title": "MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation", "paper_id": "WOS:000325991600031"}