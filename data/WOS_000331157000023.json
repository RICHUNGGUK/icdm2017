{"auto_keywords": [{"score": 0.048603687852811175, "phrase": "kernel_classifiers"}, {"score": 0.04550288640963527, "phrase": "generalization_error"}, {"score": 0.0048150026943728, "phrase": "rademacher"}, {"score": 0.004466519142879166, "phrase": "new_upper_bounds"}, {"score": 0.004169226023596022, "phrase": "misclassification_rate"}, {"score": 0.003990385737207714, "phrase": "new_and_previously_unseen_data"}, {"score": 0.0037480753312398754, "phrase": "error_estimation_topic"}, {"score": 0.003454858296390957, "phrase": "model_selection_purposes"}, {"score": 0.0033482708495261864, "phrase": "derived_bounds"}, {"score": 0.003265365666305185, "phrase": "rademacher_complexity"}, {"score": 0.003047776993499616, "phrase": "unlabeled_samples"}, {"score": 0.002688532127821979, "phrase": "confidence_term"}, {"score": 0.0026384167183586015, "phrase": "conventional_rademacher_complexity"}, {"score": 0.0023864566654035924, "phrase": "unlabeled_examples"}, {"score": 0.0022696277881969896, "phrase": "localized_versions"}, {"score": 0.0022273034991008326, "phrase": "hypothesis_class"}, {"score": 0.0021857667493036786, "phrase": "optimal_classifier"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Support vector machine", " Rademacher complexity", " Structural risk minimization", " Error estimation", " Model selection"], "paper_abstract": "We derive in this work new upper bounds for estimating the generalization error of kernel classifiers, that is the misclassification rate that the models will perform on new and previously unseen data. Though this paper is more targeted towards the error estimation topic, the generalization error can be obviously exploited, in practice, for model selection purposes as well. The derived bounds are based on Rademacher complexity and result to be particularly useful when a set of unlabeled samples are available, in addition to the (labeled) training examples: we will show that, by exploiting further unlabeled patterns, the confidence term of the conventional Rademacher complexity bound can be reduced by a factor of three. Moreover, the availability of unlabeled examples allows also to obtain further improvements by building localized versions of the hypothesis class containing the optimal classifier. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Unlabeled patterns to tighten Rademacher complexity error bounds for kernel classifiers", "paper_id": "WOS:000331157000023"}