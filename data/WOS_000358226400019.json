{"auto_keywords": [{"score": 0.04088899068598468, "phrase": "mammoth"}, {"score": 0.015497250619658854, "phrase": "large-scale_data_processing"}, {"score": 0.008713768289289523, "phrase": "mapreduce_platform"}, {"score": 0.006841867170727466, "phrase": "global_memory_management"}, {"score": 0.005105917858010978, "phrase": "mammoth_system"}, {"score": 0.004480196733208298, "phrase": "common_hardware_configurations"}, {"score": 0.004458722048888919, "phrase": "small-and_medium-size_enterprises"}, {"score": 0.004311241567238011, "phrase": "memory-constrained_systems"}, {"score": 0.004229158339021651, "phrase": "bottleneck_resource"}, {"score": 0.004188702178897384, "phrase": "cpu_power"}, {"score": 0.004011366422179016, "phrase": "memory-constrained_system"}, {"score": 0.003860045973452444, "phrase": "mapreduce"}, {"score": 0.0037956118049364044, "phrase": "mapreduce_performance"}, {"score": 0.003643607569483598, "phrase": "memory_allocation"}, {"score": 0.003608732492631878, "phrase": "execution_units"}, {"score": 0.003480887673830776, "phrase": "holistic_benefits"}, {"score": 0.003422781254306241, "phrase": "memory_unit"}, {"score": 0.003365641528627568, "phrase": "multi-threaded_execution_engine"}, {"score": 0.0032777653153451265, "phrase": "single_jvm"}, {"score": 0.003223038648516361, "phrase": "execution_engine"}, {"score": 0.0031616083063302903, "phrase": "memory_scheduling"}, {"score": 0.003049555059533735, "phrase": "sequential_disk_accessing"}, {"score": 0.0029556502180184113, "phrase": "full_garbage_collection"}, {"score": 0.002934392690655852, "phrase": "jvm."}, {"score": 0.0029062860108533374, "phrase": "extensive_experiments"}, {"score": 0.002864628676257225, "phrase": "native_hadoop_platform"}, {"score": 0.002789797188741905, "phrase": "job_execution_time"}, {"score": 0.002763071761938376, "phrase": "typical_cases"}, {"score": 0.002710384439697906, "phrase": "hadoop_programs"}, {"score": 0.0025337634569533147, "phrase": "pagerank"}, {"score": 0.0024734896597641684, "phrase": "spark"}, {"score": 0.0024556852466581527, "phrase": "better_performance"}, {"score": 0.002432152706473349, "phrase": "interactive_and_iterative_applications"}, {"score": 0.0023685917706615482, "phrase": "batch_processing_applications"}, {"score": 0.0022572421555843722, "phrase": "similar_performance"}, {"score": 0.002198242191529977, "phrase": "growing_importance"}, {"score": 0.002156301969685775, "phrase": "proven_success"}, {"score": 0.0021049977753042253, "phrase": "promising_potential"}], "paper_keywords": ["MapReduce", " data processing", " HPC"], "paper_abstract": "The MapReduce platform has been widely used for large-scale data processing and analysis recently. It works well if the hardware of a cluster is well configured. However, our survey has indicated that common hardware configurations in small-and medium-size enterprises may not be suitable for such tasks. This situation is more challenging for memory-constrained systems, in which the memory is a bottleneck resource compared with the CPU power and thus does not meet the needs of large-scale data processing. The traditional high performance computing (HPC) system is an example of the memory-constrained system according to our survey. In this paper, we have developed Mammoth, a new MapReduce system, which aims to improve MapReduce performance using global memory management. In Mammoth, we design a novel rule-based heuristic to prioritize memory allocation and revocation among execution units (mapper, shuffler, reducer, etc.), to maximize the holistic benefits of the Map/Reduce job when scheduling each memory unit. We have also developed a multi-threaded execution engine, which is based on Hadoop but runs in a single JVM on a node. In the execution engine, we have implemented the algorithm of memory scheduling to realize global memory management, based on which we further developed the techniques such as sequential disk accessing, multi-cache and shuffling from memory, and solved the problem of full garbage collection in the JVM. We have conducted extensive experiments to compare Mammoth against the native Hadoop platform. The results show that the Mammoth system can reduce the job execution time by more than 40 percent in typical cases, without requiring any modifications of the Hadoop programs. When a system is short of memory, Mammoth can improve the performance by up to 5.19 times, as observed for I/O intensive applications, such as PageRank. We also compared Mammoth with Spark. Although Spark can achieve better performance than Mammoth for interactive and iterative applications when the memory is sufficient, our experimental results show that for batch processing applications, Mammoth can adapt better to various memory environments and outperform Spark when the memory is insufficient, and can obtain similar performance as Spark when the memory is sufficient. Given the growing importance of supporting large-scale data processing and analysis and the proven success of the MapReduce platform, the Mammoth system can have a promising potential and impact.", "paper_title": "Mammoth: Gearing Hadoop Towards Memory-Intensive MapReduce Applications", "paper_id": "WOS:000358226400019"}