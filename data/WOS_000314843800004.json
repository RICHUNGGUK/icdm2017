{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "nonconvex_heuristic_recovery"}, {"score": 0.004630150894006861, "phrase": "nonconvex_framework"}, {"score": 0.004550300345169981, "phrase": "essential_low-rank_structure"}, {"score": 0.004413846562278824, "phrase": "traditional_approaches"}, {"score": 0.004318881178805575, "phrase": "convex_norms"}, {"score": 0.0033699898113395328, "phrase": "proposed_optimization"}, {"score": 0.003062091606343783, "phrase": "nonconvex_objective_function"}, {"score": 0.002957222630196853, "phrase": "nonconvex_problem"}, {"score": 0.002893505913503472, "phrase": "general_framework"}, {"score": 0.002868404184756449, "phrase": "reweighed_approaches"}, {"score": 0.0027943943921568456, "phrase": "mm-type_algorithm"}, {"score": 0.0027341763737751467, "phrase": "stationary_point"}, {"score": 0.0027104531025934865, "phrase": "successive_iterations"}, {"score": 0.002675252546913683, "phrase": "proposed_model"}, {"score": 0.0024842242921857705, "phrase": "experimental_results"}, {"score": 0.002462664392109308, "phrase": "low-rank_structure"}, {"score": 0.0022472527731744974, "phrase": "convex-norm-based_method"}, {"score": 0.002132711053003407, "phrase": "higher_rank"}, {"score": 0.0021049977753042253, "phrase": "denser_corruptions"}], "paper_keywords": ["Compressive sensing", " log-sum heuristic", " matrix learning", " nuclear norm minimization", " sparse optimization"], "paper_abstract": "In this paper, we propose a nonconvex framework to learn the essential low-rank structure from corrupted data. Different from traditional approaches, which directly utilizes convex norms to measure the sparseness, our method introduces more reasonable nonconvex measurements to enhance the sparsity in both the intrinsic low-rank structure and the sparse corruptions. We will, respectively, introduce how to combine the widely used l(p) norm (0 < p < 1) and log-sum term into the framework of low-rank structure learning. Although the proposed optimization is no longer convex, it still can be effectively solved by a majorization-minimization (MM)-type algorithm, with which the nonconvex objective function is iteratively replaced by its convex surrogate and the nonconvex problem finally falls into the general framework of reweighed approaches. We prove that the MM-type algorithm can converge to a stationary point after successive iterations. The proposed model is applied to solve two typical problems: robust principal component analysis and low-rank representation. Experimental results on low-rank structure learning demonstrate that our nonconvex heuristic methods, especially the log-sum heuristic recovery algorithm, generally perform much better than the convex-norm-based method (0 < p < 1) for both data with higher rank and with denser corruptions.", "paper_title": "Low-Rank Structure Learning via Nonconvex Heuristic Recovery", "paper_id": "WOS:000314843800004"}