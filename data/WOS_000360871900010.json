{"auto_keywords": [{"score": 0.04613833090468005, "phrase": "rlc"}, {"score": 0.00481495049065317, "phrase": "learning-aided_control"}, {"score": 0.004473982381245469, "phrase": "receding_learning-aided_control_algorithm"}, {"score": 0.004312622975366676, "phrase": "optimization_problems"}, {"score": 0.004260134699483774, "phrase": "general_stochastic_networks"}, {"score": 0.004208282550386197, "phrase": "potentially_non-stationary_system_dynamics"}, {"score": 0.004081385289516914, "phrase": "low-complexity_online_algorithm"}, {"score": 0.004007083686875716, "phrase": "zero_a-priori_statistical_knowledge"}, {"score": 0.003700371064796848, "phrase": "underlying_distribution"}, {"score": 0.0036553070575015344, "phrase": "system_dynamics"}, {"score": 0.0036107898615445797, "phrase": "receding_sampling"}, {"score": 0.003438058620195309, "phrase": "sampled_information"}, {"score": 0.0033548062793973144, "phrase": "lagrange_multiplier"}, {"score": 0.003293688299319582, "phrase": "underlying_optimization_problem"}, {"score": 0.0030601162942933665, "phrase": "online_system_controller"}, {"score": 0.0028958411976400646, "phrase": "near-optimal_utility_delay_tradeoffs"}, {"score": 0.0027572166867780275, "phrase": "efficient_distribution-change_detection"}, {"score": 0.0027069560455629917, "phrase": "fast_convergence_speed"}, {"score": 0.002641360377825228, "phrase": "non-stationary_networks"}, {"score": 0.002499508839669243, "phrase": "general_framework"}, {"score": 0.002453934333239467, "phrase": "joint_detection-learning-control_algorithms"}, {"score": 0.002183938718432764, "phrase": "network_control"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Receding learning-aided control", " Detection", " Network optimization", " Queueing"], "paper_abstract": "In this paper, we develop the Receding Learning-aided Control algorithm (RLC) for solving optimization problems in general stochastic networks with potentially non-stationary system dynamics. RLC is a low-complexity online algorithm that requires zero a-priori statistical knowledge. It has three main functionalities. First, it detects changes of the underlying distribution of system dynamics via receding sampling. Then, it carefully selects the sampled information and estimates a Lagrange multiplier of an underlying optimization problem via dual-learning. Lastly, it incorporates the multiplier into an online system controller via drift-augmentation. We show that RLC achieves near-optimal utility delay tradeoffs for stationary systems, while ensuring an efficient distribution-change detection and a fast convergence speed when applied to non-stationary networks. The results in this paper provide a general framework for designing joint detection-learning-control algorithms and provide new understanding about the role-of-information and the powerof-online-learning in network control. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Receding learning-aided control in stochastic networks", "paper_id": "WOS:000360871900010"}