{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "random_variable"}, {"score": 0.004608284537368354, "phrase": "gaussian_noise"}, {"score": 0.0044104497301196794, "phrase": "mismatched_minimum_mean-square_estimator"}, {"score": 0.003700170452390401, "phrase": "noise_ratios"}, {"score": 0.003541182515693916, "phrase": "excess_mean-square_estimation_error"}, {"score": 0.002970476379751357, "phrase": "relative_entropy"}, {"score": 0.002863652837038965, "phrase": "nonreal-valued_random_variables"}, {"score": 0.0027006464509273806, "phrase": "new_general_representations"}, {"score": 0.0026613618933208467, "phrase": "mutual_information"}, {"score": 0.002584494385587982, "phrase": "conditional_means"}, {"score": 0.0024915167458844914, "phrase": "new_representation"}, {"score": 0.0023496433751182162, "phrase": "free_relative_entropy"}, {"score": 0.0021049977753042253, "phrase": "free_probability"}], "paper_keywords": ["Divergence", " free probability", " minimum mean-square error (MMSE) estimation", " mutual information", " relative entropy", " Shannon theory", " statistics"], "paper_abstract": "A random variable with distribution is observed in Gaussian noise and is estimated by a mismatched minimum mean-square estimator that assumes that the distribution is Q, instead of. This paper shows that the integral over all signal-to-noise ratios (SNRs) of the excess mean-square estimation error incurred by the mismatched estimator is twice the relative entropy D(P parallel to Q) (in nats). This representation of relative entropy can be generalized to nonreal-valued random variables, and can be particularized to give new general representations of mutual information in terms of conditional means. Inspired by the new representation, we also propose a definition of free relative entropy which fills a gap in, and is consistent with, the literature on free probability.", "paper_title": "Mismatched Estimation and Relative Entropy", "paper_id": "WOS:000282001700006"}