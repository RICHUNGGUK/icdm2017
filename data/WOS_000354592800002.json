{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "loop_kernels"}, {"score": 0.03018078613847557, "phrase": "exploration_space"}, {"score": 0.004754828422696271, "phrase": "software_information"}, {"score": 0.004710227709931926, "phrase": "memory_architecture"}, {"score": 0.004593317752310294, "phrase": "today's_compilers"}, {"score": 0.004521720473366683, "phrase": "art_libraries"}, {"score": 0.004423346952963429, "phrase": "compiler_sub-problems"}, {"score": 0.004273047608656344, "phrase": "separate_sub-problems_optimization"}, {"score": 0.0038278398921822404, "phrase": "specific_algorithm's_information"}, {"score": 0.003640011825552854, "phrase": "optimal_solution"}, {"score": 0.0033436458712965272, "phrase": "major_scheduling_sub-problems"}, {"score": 0.003149630099034765, "phrase": "minimum_numbers"}, {"score": 0.003023389555114672, "phrase": "data_cache_accesses"}, {"score": 0.0027596607038845923, "phrase": "algorithm's_information"}, {"score": 0.002599442416948628, "phrase": "constraint_propagation"}, {"score": 0.0025588443155279855, "phrase": "hardware_parameters"}, {"score": 0.0024951987856762646, "phrase": "memory_architecture_parameters"}, {"score": 0.002433132434417479, "phrase": "new_faster_c-code"}, {"score": 0.002350298241921987, "phrase": "existing_compiler_transformations"}, {"score": 0.002328199473006809, "phrase": "original_code"}, {"score": 0.0023063080082656737, "phrase": "proposed_methodology"}, {"score": 0.0022702776603245036, "phrase": "five_well-known_algorithms"}, {"score": 0.002234808936443945, "phrase": "embedded_processors"}, {"score": 0.0021451564984201364, "phrase": "iterative_compilation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Data reuse", " Register allocation", " Optimization", " Memory hierarchy", " Loop tiling", " Data locality", " Diophantine equations"], "paper_abstract": "It is well-known that today's compilers and state of the art libraries have three major drawbacks. First, the compiler sub-problems are optimized separately; this is not efficient because the separate sub-problems optimization gives a different schedule for each sub-problem and these schedules cannot coexist as the refining of one, causes the degradation of another. Second, they take into account only part of the specific algorithm's information. Third, they take into account only a few hardware architecture parameters. These approaches cannot give an optimal solution. In this paper, a new methodology/pre-compiler is introduced, which speeds up loop kernels, by overcoming the above problems. This methodology solves four of the major scheduling sub-problems, together as one problem and not separately; these are the sub-problems of finding the schedules with the minimum numbers of (i) L1 data cache accesses, (ii) L2 data cache accesses, (iii) main memory data accesses, (iv) addressing instructions. First, the exploration space (possible solutions) is found according to the algorithm's information, e.g. array subscripts. Then, the exploration space is decreased by orders of magnitude, by applying constraint propagation to the software and hardware parameters. We take the C-code and the memory architecture parameters as input and we automatically produce a new faster C-code; this code cannot be obtained by applying the existing compiler transformations to the original code. The proposed methodology has been evaluated for five well-known algorithms in both general and embedded processors; it is compared with gcc and clang compilers and also with iterative compilation. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "A methodology for speeding up loop kernels by exploiting the software information and the memory architecture", "paper_id": "WOS:000354592800002"}