{"auto_keywords": [{"score": 0.03905199482347015, "phrase": "sub-optimal_policy"}, {"score": 0.0367698002611785, "phrase": "lower_rewards"}, {"score": 0.033853798163865266, "phrase": "optimal_stopping_problem"}, {"score": 0.00481495049065317, "phrase": "reinforcement_learning_paradigms"}, {"score": 0.004363846362238565, "phrase": "inherently_present_cumulative_or_additivity"}, {"score": 0.004167982855788402, "phrase": "definite_necessity"}, {"score": 0.0038397509389856625, "phrase": "optimal_policy"}, {"score": 0.003584025547693836, "phrase": "slower_convergence"}, {"score": 0.0033233797886807375, "phrase": "second_scenario"}, {"score": 0.003269266730193333, "phrase": "supremum_values"}, {"score": 0.0030614565460256897, "phrase": "cumulative_rewards_paradigm"}, {"score": 0.0029528295737527026, "phrase": "non-cumulative_reward_reinforcement-learning_paradigm"}, {"score": 0.002866817775018445, "phrase": "maximum_reward_criterion"}, {"score": 0.0027833043858224078, "phrase": "resulting_reinforcement-learning_model"}, {"score": 0.0026581913054027663, "phrase": "maximum_reward_reinforcement"}, {"score": 0.00260630351276416, "phrase": "non-cumulative_rewards_problem"}, {"score": 0.002547043222086858, "phrase": "maximum_reward-oriented_behavior"}, {"score": 0.0024325244760735566, "phrase": "sub-optimal_policies"}, {"score": 0.0023928819778638055, "phrase": "learning_paradigm"}, {"score": 0.0023694065215394593, "phrase": "maximum_reward_reinforcement_learning"}, {"score": 0.002323142666222693, "phrase": "fitsk-rl_model"}, {"score": 0.0022113879114507577, "phrase": "non-cumulative_rewards"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["maximum reward", " reinforcement learning", " non-cumulative reward", " FITSK-RL", " optimal stopping problem", " financial derivative pricing", " two-cycle task", " maximum reward-oriented behaviour"], "paper_abstract": "Existing reinforcement learning paradigms proposed in the literature are guided by two performance criteria; namely: the expected cumulative-reward, and the average reward criteria. Both of these criteria assume an inherently present cumulative or additivity of the rewards. However, such inherent cumulative of the rewards is not a definite necessity in some contexts. Two possible scenarios are presented in this paper, and are summarized as follows. The first concerns with learning of an optimal policy that is further away in the existence of a sub-optimal policy that is nearer. The cumulative rewards paradigms suffer from slower convergence due to the influence of accumulating the lower rewards, and take time to fade away the effect of the sub-optimal policy. The second scenario concerns with approximating the supremum values of the payoffs of an optimal stopping problem. The payoffs are non-cumulative in nature, and thus the cumulative rewards paradigm is not applicable to resolve this. Hence, a non-cumulative reward reinforcement-learning paradigm is needed in these application contexts. A maximum reward criterion is proposed in this paper, and the resulting reinforcement-learning model with this learning criterion is termed the maximum reward reinforcement learning. The maximum reward reinforcement learning considers the learning of non-cumulative rewards problem, where the agent exhibits a maximum reward-oriented behavior towards the largest rewards in the state-space. Intermediate lower rewards that lead to sub-optimal policies are ignored in this learning paradigm. The maximum reward reinforcement learning is subsequently modeled with the FITSK-RL model. Finally, the model is applied to an optimal stopping problem with a nature of non-cumulative rewards, and its performance is encouraging when benchmarked against other model. (C) 2005 Elsevier Ltd. All rights reserved.", "paper_title": "Maximum reward reinforcement learning: A non-cumulative reward criterion", "paper_id": "WOS:000237645100016"}