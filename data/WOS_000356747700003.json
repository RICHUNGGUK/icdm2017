{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "nonstationary_data"}, {"score": 0.010374291108884052, "phrase": "concept_drift"}, {"score": 0.008341454608376616, "phrase": "interim_ensembles"}, {"score": 0.006890014412005227, "phrase": "component_classifiers"}, {"score": 0.004600100053651545, "phrase": "machine_learning"}, {"score": 0.004516856326107826, "phrase": "active_topic"}, {"score": 0.004475798524102238, "phrase": "ensemble_learning"}, {"score": 0.004354841111644036, "phrase": "ensemble_methods"}, {"score": 0.004103817210338686, "phrase": "current_batch_growing_ensemble_methods"}, {"score": 0.003867206750827, "phrase": "non-stationary_data"}, {"score": 0.0036944890904337833, "phrase": "useful_information"}, {"score": 0.0036275736378775757, "phrase": "fine-tuned_interim_ensembles"}, {"score": 0.0035133644847622383, "phrase": "comprehensive_hierarchical_approach"}, {"score": 0.0034813964296826973, "phrase": "dynamic_ensemble_of_ensembles"}, {"score": 0.0033872210664940817, "phrase": "novel_method"}, {"score": 0.0032358709481309913, "phrase": "consecutive_batches"}, {"score": 0.002993887707344862, "phrase": "final_ensemble"}, {"score": 0.0028600623410146796, "phrase": "available_experts"}, {"score": 0.0027573101010593863, "phrase": "sparsity_learning"}, {"score": 0.0025862736445290088, "phrase": "dynamic_weighted_majority"}, {"score": 0.0025047659706737215, "phrase": "different_classifiers"}, {"score": 0.002403724423251419, "phrase": "real_nonstationary_environments"}, {"score": 0.002234031373581961, "phrase": "better_performance"}, {"score": 0.002203570941628287, "phrase": "promising_generalization_ability"}, {"score": 0.002163600705898299, "phrase": "nonstationary_environments"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Ensemble of ensembles", " Growing ensemble", " Sparsity learning", " Nonstationary environment", " Concept drift", " Incremental learning"], "paper_abstract": "Learning nonstationary data with concept drift has received much attention in machine learning and been an active topic in ensemble learning. Specifically, batch growing ensemble methods present one important direction for dealing with concept drift involved in nonstationary data. However, current batch growing ensemble methods combine all the available component classifiers only, each trained independently from a batch of non-stationary data. They simply discard interim ensembles and hence may lose useful information obtained from the fine-tuned interim ensembles. Distinctively, we introduce a comprehensive hierarchical approach called Dynamic Ensemble of Ensembles (DE2). The novel method combines classifiers as an ensemble of all the interim ensembles dynamically from consecutive batches of nonstationary data. DE2 includes two key stages: component classifiers and interim ensembles are dynamically trained; and the final ensemble is then learned by exponentially-weighted averaging with available experts, i.e., interim ensembles. Moreover, we engage Sparsity Learning to choose component classifiers selectively and intelligently. We also incorporate the techniques of Dynamic Weighted Majority, and Learn(++).NSE for better integrating different classifiers dynamically. We perform experiments with two benchmark test sets in real nonstationary environments, and compare our DE2 method to other conventional competitive ensemble methods. Experimental results confirm that our approach consistently leads to better performance and has promising generalization ability for learning in nonstationary environments. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "DE2: Dynamic ensemble of ensembles for learning nonstationary data", "paper_id": "WOS:000356747700003"}