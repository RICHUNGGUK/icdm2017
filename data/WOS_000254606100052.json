{"auto_keywords": [{"score": 0.04970018005584556, "phrase": "multiple_feature_streams"}, {"score": 0.034692465572853035, "phrase": "mutual_information"}, {"score": 0.03363896917055258, "phrase": "search_space"}, {"score": 0.024864112625590372, "phrase": "field_reports"}, {"score": 0.00481495049065317, "phrase": "based_dynamic_integration"}, {"score": 0.004739056045644449, "phrase": "robust_real-time_lvcsr."}, {"score": 0.004664352260912174, "phrase": "novel_method"}, {"score": 0.0044826811976193485, "phrase": "different_acoustic_aspects"}, {"score": 0.004429566229017728, "phrase": "robust_speech_recognition"}, {"score": 0.004377077848734575, "phrase": "integration_algorithm"}, {"score": 0.0043080553835735825, "phrase": "frame-wise_stream_weight"}, {"score": 0.004240116703465252, "phrase": "higher_weight"}, {"score": 0.004026598712747509, "phrase": "noisy_environments"}, {"score": 0.0038696708318671446, "phrase": "discriminative_ability"}, {"score": 0.003823791479971016, "phrase": "conventional_method"}, {"score": 0.0037336520904376687, "phrase": "spoken_digits"}, {"score": 0.0036023971109946946, "phrase": "whole_set"}, {"score": 0.003573859492987647, "phrase": "hmm_states"}, {"score": 0.0034895911869254146, "phrase": "dynamic_weighting"}, {"score": 0.003248483521309273, "phrase": "input_stream"}, {"score": 0.003222740482478965, "phrase": "active_hmm_states"}, {"score": 0.0031467250930224126, "phrase": "additional_likelihood_calculation"}, {"score": 0.0029409171055888804, "phrase": "marginal_entropy"}, {"score": 0.002882980094838904, "phrase": "active_states"}, {"score": 0.002737615043512257, "phrase": "auditory_filters"}, {"score": 0.00267301171407926, "phrase": "human_auditory_system's_ability"}, {"score": 0.0026412824985309323, "phrase": "amplitude_and_frequency_modulations"}, {"score": 0.0025382066972107777, "phrase": "amplitude_drift"}, {"score": 0.002508073621696602, "phrase": "resonant_frequency"}, {"score": 0.002391067964886602, "phrase": "complementary_clues"}, {"score": 0.002372103459168231, "phrase": "speech_recognition"}, {"score": 0.0023532890136269986, "phrase": "speech_recognition_experiments"}, {"score": 0.002316105585203396, "phrase": "spontaneous_commentary"}, {"score": 0.0022977342663599042, "phrase": "japanese_broadcast_news"}, {"score": 0.0022614266476954467, "phrase": "proposed_method"}, {"score": 0.0022434880661725493, "phrase": "error_words"}, {"score": 0.0021731423856364003, "phrase": "spontaneous_commentaries"}, {"score": 0.002138799283553239, "phrase": "best_result"}, {"score": 0.0021049977753042253, "phrase": "single_stream"}], "paper_keywords": ["speech recognition", " stream integration", " entropy", " mutual information", " active hypotheses"], "paper_abstract": "We present a novel method of integrating the likelihoods of,multiple feature streams, representing different acoustic aspects, for robust speech recognition. The integration algorithm dynamically calculates a frame-wise stream weight so that a higher weight is given to a stream that is robust to a variety of noisy environments or speaking styles. Such a robust stream is expected to show discriminative ability. A conventional method proposed for the recognition of spoken digits calculates the weights from the entropy of the whole set of HMM states. This paper extends the dynamic weighting to a real-time large-vocabulary continuous speech recognition (LVCSR) system. The proposed weight is calculated in real-time from mutual information between an input stream and active HMM states in a search space without an additional likelihood calculation. Furthermore, the mutual information takes the width of the search space into account by calculating the marginal entropy from the number of active states. In this paper, we integrate three features that are extracted through auditory filters by taking into account the human auditory system's ability to extract amplitude and frequency modulations. Due to this, features representing energy, amplitude drift, and resonant frequency drifts, are integrated. These features are expected to provide complementary clues for speech recognition. Speech recognition experiments on field reports and spontaneous commentary from Japanese broadcast news showed that the proposed method reduced error words by 9.2% in field reports and 4.7% in spontaneous commentaries relative to the best result obtained from a single stream.", "paper_title": "Mutual information based dynamic integration of multiple feature streams for robust real-time LVCSR", "paper_id": "WOS:000254606100052"}