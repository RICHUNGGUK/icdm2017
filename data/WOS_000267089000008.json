{"auto_keywords": [{"score": 0.04726673405217005, "phrase": "cva"}, {"score": 0.00481495049065317, "phrase": "multinomial_kernel_discriminant_analysis"}, {"score": 0.004655924035639928, "phrase": "dimensionality_reduction"}, {"score": 0.004593787100061613, "phrase": "canonical_variate_analysis"}, {"score": 0.004382737590042589, "phrase": "pattern_recognition"}, {"score": 0.003282383509542342, "phrase": "sparse_basis"}, {"score": 0.0028498718992429825, "phrase": "orthogonal_least-squares"}, {"score": 0.002700485592691974, "phrase": "similar_approach"}, {"score": 0.002664375650349975, "phrase": "binomial_problems"}, {"score": 0.002524688179363081, "phrase": "minimum_mahalanobis_distance"}, {"score": 0.0024742098091226203, "phrase": "canonical_variates"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Linear discriminant analysis", " Kernel discriminant analysis", " Multi-class", " Multinomial", " Least-squares", " Optimal scaling", " Sparsity control"], "paper_abstract": "Dimensionality reduction via canonical variate analysis (CVA) is important for pattern recognition and has been extended variously to permit more flexibility, e.g. by \"kernelizing\" the formulation. This can lead to over-fitting, usually ameliorated by regularization. Here, a method for sparse, multinomial kernel discriminant analysis (sMKDA) is proposed, using a sparse basis to control complexity. It is based on the connection between CVA and least-squares, and uses forward selection via orthogonal least-squares to approximate a basis, generalizing a similar approach for binomial problems. Classification can be performed directly via minimum Mahalanobis distance in the canonical variates. sMKDA achieves state-of-the-art performance in terms of accuracy and sparseness on 11 benchmark datasets. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Sparse multinomial kernel discriminant analysis (sMKDA)", "paper_id": "WOS:000267089000008"}