{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "stackelberg_games"}, {"score": 0.04793647319920375, "phrase": "human_adversaries"}, {"score": 0.008597757068673433, "phrase": "limited_observations"}, {"score": 0.008370307901747745, "phrase": "agent_interactions"}, {"score": 0.004776235281195212, "phrase": "bounded_rationality"}, {"score": 0.004725096507471105, "phrase": "human_cognition"}, {"score": 0.004550364809125859, "phrase": "natural_models"}, {"score": 0.004489532034272497, "phrase": "human_interaction"}, {"score": 0.004441449270168749, "phrase": "oligopolistic_markets"}, {"score": 0.0044176004876486175, "phrase": "security_domains"}, {"score": 0.004152335204797172, "phrase": "leader's_commitment"}, {"score": 0.004130032407463255, "phrase": "existing_algorithms"}, {"score": 0.0040747959570067395, "phrase": "optimal_solutions"}, {"score": 0.003830035994772447, "phrase": "human_followers"}, {"score": 0.003728240083633891, "phrase": "limited_observation"}, {"score": 0.0035902386595439146, "phrase": "human_adversaries'_decisions"}, {"score": 0.00345732765439755, "phrase": "likely_deviations"}, {"score": 0.003383591957131607, "phrase": "unacceptable_degradation"}, {"score": 0.0033563469491919777, "phrase": "leader's_reward"}, {"score": 0.003320360087169083, "phrase": "security_applications"}, {"score": 0.0030707057928165654, "phrase": "crucial_problem"}, {"score": 0.0030214377691414724, "phrase": "new_mixed-integer_linear_program"}, {"score": 0.0030051951043225354, "phrase": "milp"}, {"score": 0.002870562356794372, "phrase": "human_perception"}, {"score": 0.0028551244098497103, "phrase": "probability_distributions"}, {"score": 0.00277168377634575, "phrase": "human_imprecision"}, {"score": 0.0027419484454788722, "phrase": "new_approach"}, {"score": 0.00271253125510496, "phrase": "traditional_proofs"}, {"score": 0.0025979767249757852, "phrase": "empirical_validation"}, {"score": 0.0025152389351747627, "phrase": "real_deployed_security_systems"}, {"score": 0.002501707130971532, "phrase": "los_angeles_international_airport"}, {"score": 0.0022824668650009944, "phrase": "final_conclusion"}, {"score": 0.0022038008840831783, "phrase": "statistically_significant_higher_rewards"}, {"score": 0.0021393542556570706, "phrase": "existing_approaches"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Behavioral game theory", " Security", " Stackelberg", " Uncertainty"], "paper_abstract": "How do we build algorithms for agent interactions with human adversaries? Stackelberg games are natural models for many important applications that involve human interaction, such as oligopolistic markets and security domains. In Stackelberg games, one player, the leader, commits to a strategy and the follower makes her decision with knowledge of the leader's commitment. Existing algorithms for Stackelberg games efficiently find optimal solutions (leader strategy), but they critically assume that the follower plays optimally. Unfortunately, in many applications, agents face human followers (adversaries) who - because of their bounded rationality and limited observation of the leader strategy - may deviate from their expected optimal response. In other words, human adversaries' decisions are biased due to their bounded rationality and limited observations. Not taking into account these likely deviations when dealing with human adversaries may cause an unacceptable degradation in the leader's reward, particularly in security applications where these algorithms have seen deployment. The objective of this paper therefore is to investigate how to build algorithms for agent interactions with human adversaries. To address this crucial problem, this paper introduces a new mixed-integer linear program (MILP) for Stackelberg games to consider human adversaries, incorporating: (i) novel anchoring theories on human perception of probability distributions and (ii) robustness approaches for MILPs to address human imprecision. Since this new approach considers human adversaries, traditional proofs of correctness or optimality are insufficient; instead, it is necessary to rely on empirical validation. To that end, this paper considers four settings based on real deployed security systems at Los Angeles International Airport (Pita et al., 2008 1351), and compares 6 different approaches (three based on our new approach and three previous approaches), in 4 different observability conditions, involving 218 human subjects playing 2960 games in total. The final conclusion is that a model which incorporates both the ideas of robustness and anchoring achieves statistically significant higher rewards and also maintains equivalent or faster solution speeds compared to existing approaches. (c) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Robust solutions to Stackelberg games: Addressing bounded rationality and limited observations in human cognition", "paper_id": "WOS:000281987600002"}