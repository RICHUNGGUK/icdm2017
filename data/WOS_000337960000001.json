{"auto_keywords": [{"score": 0.04916281891691642, "phrase": "informative_features"}, {"score": 0.00481495049065317, "phrase": "feature_generating_samplings"}, {"score": 0.004677372885331908, "phrase": "extremely_high-dimensional_problems"}, {"score": 0.004521803560561489, "phrase": "sampling_scheme"}, {"score": 0.004350308086118336, "phrase": "recently_developed_feature_generating_machines"}, {"score": 0.004085333687668526, "phrase": "time_complexity"}, {"score": 0.003855034057431947, "phrase": "entire_computational_cost"}, {"score": 0.0034159525504642656, "phrase": "feature_dimensionality"}, {"score": 0.00328625950643742, "phrase": "selected_feature_subset"}, {"score": 0.003070996252787868, "phrase": "feature_generating_sampling_method"}, {"score": 0.0029830992938695007, "phrase": "computational_complexity"}, {"score": 0.0027341763737751467, "phrase": "feature_buffer"}, {"score": 0.002655894070657161, "phrase": "maximum_number"}, {"score": 0.0025181367444581993, "phrase": "buffer_size"}, {"score": 0.0023644936731468252, "phrase": "birth-death_process"}, {"score": 0.0023303871145705954, "phrase": "random_processes_theory"}, {"score": 0.0022094759531152072, "phrase": "feature_selections"}, {"score": 0.0021881743793875767, "phrase": "empirical_studies"}, {"score": 0.0021670777282518424, "phrase": "real-world_datasets"}, {"score": 0.0021049977753042253, "phrase": "proposed_sampling_method"}], "paper_keywords": ["Extremely high dimensional problem", " feature generating machine", " feature selection", " informative feature"], "paper_abstract": "To select informative features on extremely high-dimensional problems, in this paper, a sampling scheme is proposed to enhance the efficiency of recently developed feature generating machines (FGMs). Note that in FGMs O(m log r) time complexity should be taken to order the features by their scores; the entire computational cost of feature ordering will become unbearable when m is very large, for example, m > 10(11), where m is the feature dimensionality and r is the size of the selected feature subset. To solve this problem, in this paper, we propose a feature generating sampling method, which can reduce this computational complexity to O(G(s) log(G) + G(G+ log(G))) while preserving the most informative features in a feature buffer, where Gs is the maximum number of nonzero features for each instance and G is the buffer size. Moreover, we show that our proposed sampling scheme can be deemed as the birth-death process based on random processes theory, which guarantees to include most of the informative features for feature selections. Empirical studies on real-world datasets show the effectiveness of the proposed sampling method.", "paper_title": "Extremely High-Dimensional Feature Selection via Feature Generating Samplings", "paper_id": "WOS:000337960000001"}