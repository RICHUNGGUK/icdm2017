{"auto_keywords": [{"score": 0.04773866258910522, "phrase": "defect_prediction"}, {"score": 0.041727629088517196, "phrase": "static_code_metrics"}, {"score": 0.029709021816766247, "phrase": "inspection_optimization"}, {"score": 0.00481495049065317, "phrase": "inspection_optimization_policies."}, {"score": 0.004625881588444304, "phrase": "social_metrics"}, {"score": 0.0043560464825349275, "phrase": "social_analysis"}, {"score": 0.003766908559075843, "phrase": "software_systems"}, {"score": 0.0032085525502105836, "phrase": "other's_design"}, {"score": 0.0031765314093444956, "phrase": "implementation_decisions"}, {"score": 0.003051601113971322, "phrase": "individual_developers"}, {"score": 0.0028877818105020434, "phrase": "activity-centric_measures"}, {"score": 0.002665027876903159, "phrase": "fewest_lines"}, {"score": 0.002459414063407306, "phrase": "activity_measures"}, {"score": 0.0021049977753042253, "phrase": "activity-centric_static_code_metrics"}], "paper_keywords": ["Data mining", " defect prediction", " static measures"], "paper_abstract": "Recent research has shown the value of social metrics for defect prediction. Yet many repositories lack the information required for a social analysis. So, what other means exist to infer how developers interact around their code? One option is static code metrics that have already demonstrated their usefulness in analyzing change in evolving software systems. But do they also help in defect prediction? To address this question we selected a set of static code metrics to determine what classes are most \"active\" (i.e., the classes where the developers spend much time interacting with each other's design and implementation decisions) in 33 open-source Java systems that lack details about individual developers. In particular, we assessed the merit of these activity-centric measures in the context of \"inspection optimization\" - a technique that allows for reading the fewest lines of code in order to find the most defects. For the task of inspection optimization these activity measures perform as well as (usually, within 4%) a theoretical upper bound on the performance of any set of measures. As a result, we argue that activity-centric static code metrics are an excellent predictor for defects.", "paper_title": "LEARNING BETTER INSPECTION OPTIMIZATION POLICIES", "paper_id": "WOS:000310082700002"}