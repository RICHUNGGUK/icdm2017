{"auto_keywords": [{"score": 0.03992659287798474, "phrase": "polynomial_time"}, {"score": 0.00481495049065317, "phrase": "tree_belief_networks"}, {"score": 0.004637478829869904, "phrase": "decomposable_priors"}, {"score": 0.004371660146981191, "phrase": "tree_belief_nets"}, {"score": 0.004303010061986095, "phrase": "bayesian"}, {"score": 0.004233119608341877, "phrase": "complete_observations"}, {"score": 0.0035078427258024613, "phrase": "super-exponential_number"}, {"score": 0.003150466780962537, "phrase": "factored_distributions"}, {"score": 0.002969628818856786, "phrase": "closed_form"}, {"score": 0.0028446460307503343, "phrase": "tree_parameters"}, {"score": 0.0026957733674735584, "phrase": "heckerman"}, {"score": 0.002666950396847441, "phrase": "geiger"}, {"score": 0.0026384186819885277, "phrase": "chickering"}, {"score": 0.0025546627686955656, "phrase": "tree_parameter_priors"}, {"score": 0.0025003054002226965, "phrase": "compactly_parametrized_product"}, {"score": 0.002473560912156024, "phrase": "dirichlet_distributions"}, {"score": 0.002407941567598653, "phrase": "exact_bayesian_learning"}, {"score": 0.0022941723937662927, "phrase": "new_class"}, {"score": 0.0022696277881969896, "phrase": "tractable_latent_variable_models"}, {"score": 0.0021857667493036786, "phrase": "data_point"}, {"score": 0.002127765807039172, "phrase": "ensemble_average"}, {"score": 0.0021049977753042253, "phrase": "tree_structures"}], "paper_keywords": ["graphical model", " tree belief network", " spanning tree", " Bayesian", " decomposable prior", " matrix tree theorem"], "paper_abstract": "In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time. Our result is the first where computing the normalization constant and averaging over a super-exponential number of graph structures can be performed in polynomial time. This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to Heckerman, Geiger and Chickering (1995) constrain the tree parameter priors to be a compactly parametrized product of Dirichlet distributions. Besides allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures.", "paper_title": "Tractable Bayesian learning of tree belief networks", "paper_id": "WOS:000235674500007"}