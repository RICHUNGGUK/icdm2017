{"auto_keywords": [{"score": 0.047000142584588775, "phrase": "storage_process"}, {"score": 0.014915350224506626, "phrase": "distributed_storage_systems"}, {"score": 0.014105114659637125, "phrase": "different_storage_nodes"}, {"score": 0.010732136280172949, "phrase": "storage_nodes"}, {"score": 0.00481495049065317, "phrase": "network_redundancy_generation"}, {"score": 0.004783105609960904, "phrase": "opportunistic_speedup"}, {"score": 0.004751470337792407, "phrase": "data_backup"}, {"score": 0.004720043307726269, "phrase": "erasure_coding"}, {"score": 0.004673290248999786, "phrase": "storage-efficient_alternative"}, {"score": 0.004596390584582711, "phrase": "reliable_data_backup"}, {"score": 0.0044759626695474106, "phrase": "traditional_erasure_codes"}, {"score": 0.004431616517786392, "phrase": "unique_source_node"}, {"score": 0.004202387892988509, "phrase": "communication_and_computation_capabilities"}, {"score": 0.004133204817078675, "phrase": "storage_process_throughput"}, {"score": 0.004065156038847781, "phrase": "source_node"}, {"score": 0.0037662262006579023, "phrase": "data_center_setting"}, {"score": 0.003667468902144262, "phrase": "peer-to-peer_setting"}, {"score": 0.0035594478687904743, "phrase": "overall_storage_process"}, {"score": 0.0034545974376846687, "phrase": "\"in-network\"_redundancy_generation_process"}, {"score": 0.003243246027166794, "phrase": "new_redundant_data"}, {"score": 0.0032110737733503807, "phrase": "partial_information"}, {"score": 0.003014576010931602, "phrase": "spare_bandwidth"}, {"score": 0.002994602527726632, "phrase": "computing_resources"}, {"score": 0.00293547027122617, "phrase": "proposed_approach"}, {"score": 0.0028966970007550824, "phrase": "local_repairability_property"}, {"score": 0.002877502286766111, "phrase": "newly_proposed_erasure_codes"}, {"score": 0.0026656724016739905, "phrase": "efficient_usage"}, {"score": 0.0026392143749243914, "phrase": "spare_node_resources"}, {"score": 0.0024693979425773993, "phrase": "availability_traces"}, {"score": 0.002453027723284009, "phrase": "real_peer-to-peer_applications"}, {"score": 0.002420611343093316, "phrase": "google_data_center_availability"}, {"score": 0.002318180580715058, "phrase": "environmental_characteristics"}, {"score": 0.0022274718912558343, "phrase": "data_centers"}, {"score": 0.0021907299375981356, "phrase": "peer-to-peer_settings"}, {"score": 0.0021474369836979048, "phrase": "classical_naive_data_insertion_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Distributed storage systems", " Data insertion", " Locally repairable codes", " Self-repairing codes"], "paper_abstract": "Erasure coding is a storage-efficient alternative to replication for achieving reliable data backup in distributed storage systems. During the storage process, traditional erasure codes require a unique source node to create and upload all the redundant data to the different storage nodes. However, such a source node may have limited communication and computation capabilities, which constrain the storage process throughput. Moreover, the source node and the different storage nodes might not be able to send and receive data simultaneously - e.g., nodes might be busy in a data center setting, or simply be offline in a peer-to-peer setting which can further threaten the efficacy of the overall storage process. In this paper, we propose an \"in-network\" redundancy generation process which distributes the data insertion load among the source and storage nodes by allowing the storage nodes to generate new redundant data by exchanging partial information among themselves, improving the throughput of the storage process. The process is carried out asynchronously, utilizing spare bandwidth and computing resources from the storage nodes. The proposed approach leverages on the local repairability property of newly proposed erasure codes, tailor made for the needs of distributed storage systems. We analytically show that, the performance of this technique relies on an efficient usage of the spare node resources, and we derive a set of scheduling algorithms to maximize the same. We experimentally show, using availability traces from real peer-To-peer applications as well as Google data center availability and workload traces, that, our algorithms can, depending on the environmental characteristics, increase the throughput of the storage process significantly (up to 90% in data centers, and 60% in peer-to-peer settings) with respect to the classical naive data insertion approach. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "In-network redundancy generation for opportunistic speedup of data backup", "paper_id": "WOS:000319235600004"}