{"auto_keywords": [{"score": 0.03859150608353858, "phrase": "umc"}, {"score": 0.00481495049065317, "phrase": "optimization_algorithms"}, {"score": 0.004775531918452571, "phrase": "semi-markov_decision_processes"}, {"score": 0.004736482357541719, "phrase": "cao"}, {"score": 0.004564401713056939, "phrase": "alpha-_dependent_equivalent_infinitesimal_generator"}, {"score": 0.004471530397729526, "phrase": "semi-_markov_decision_process"}, {"score": 0.004326833704481825, "phrase": "discounted-_cost_criteria"}, {"score": 0.004221379262804442, "phrase": "alpha-_equivalent_markov_decision_process"}, {"score": 0.004101579688745222, "phrase": "performance_potential_theory"}, {"score": 0.0038403151693333017, "phrase": "error_bounds"}, {"score": 0.003700709435644458, "phrase": "alpha-_uniformized_markov_chain"}, {"score": 0.003284361433827464, "phrase": "corresponding_poisson_equations"}, {"score": 0.0031389505756342635, "phrase": "smdp"}, {"score": 0.002651291854234327, "phrase": "obtained_results"}, {"score": 0.002575934917174136, "phrase": "special_models"}, {"score": 0.0025547994452342266, "phrase": "i._e."}, {"score": 0.0025338369486914364, "phrase": "continuous-_time_mdps"}, {"score": 0.0025130460190930554, "phrase": "markov_chains"}, {"score": 0.002431570196486513, "phrase": "simulation-_based_optimization_methods"}, {"score": 0.0024017007954518065, "phrase": "reinforcement_learning"}, {"score": 0.0023819914954155905, "phrase": "neuro-_dynamic_programming"}, {"score": 0.0023527296826339225, "phrase": "estimation_errors"}, {"score": 0.002333421318779658, "phrase": "approximation_errors"}, {"score": 0.002314271047073693, "phrase": "common_cases"}, {"score": 0.0022484715639779153, "phrase": "application_example"}, {"score": 0.0021755548389283856, "phrase": "conveyor-_serviced_production_station"}, {"score": 0.0021049977753042253, "phrase": "corresponding_error_bounds"}], "paper_keywords": ["semi-Markov decision processes", " performance potential", " uniformized Markov chain", " error bound"], "paper_abstract": "Cao's work shows that, by defining an alpha- dependent equivalent infinitesimal generator A(alpha), a semi- Markov decision process ( SMDP) with both average- and discounted- cost criteria can be treated as an alpha- equivalent Markov decision process ( MDP), and the performance potential theory can also be developed for SMDPs. In this work, we focus on establishing error bounds for potential and A(alpha)- based iterative optimization methods. First, we introduce an alpha- uniformized Markov chain ( UMC) for a SMDP via A(alpha) and a uniformized parameter, and show their relations. Especially, we obtain that their performance potentials, as solutions of corresponding Poisson equations, are proportional, so that the studies of a SMDP and the alpha- UMC based on potentials are unified. Using these relations, we derive the error bounds for a potential- based policy- iteration algorithm and a value- iteration algorithm, respectively, when there exist various calculation errors. The obtained results can be applied directly to the special models, i. e., continuous- time MDPs and Markov chains, and can be extended to some simulation- based optimization methods such as reinforcement learning and neuro- dynamic programming, where estimation errors or approximation errors are common cases. Finally, we give an application example on the look- ahead control of a conveyor- serviced production station ( CSPS), and show the corresponding error bounds.", "paper_title": "Error bounds of optimization algorithms for semi-Markov decision processes", "paper_id": "WOS:000249695500004"}