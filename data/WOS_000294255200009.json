{"auto_keywords": [{"score": 0.04921457158686006, "phrase": "standard_data"}, {"score": 0.047528888441075466, "phrase": "bibliometric_rankings"}, {"score": 0.00481495049065317, "phrase": "consistent_manner"}, {"score": 0.0029783256772952073, "phrase": "quite_surprising_and_unpleasant_results"}, {"score": 0.002751751956416832, "phrase": "\"best\"_department"}, {"score": 0.0027019561451518746, "phrase": "\"worst\"_scientists"}, {"score": 0.0021049977753042253, "phrase": "consistent_rankings"}], "paper_keywords": [""], "paper_abstract": "The standard data that we use when computing bibliometric rankings of scientists are their publication/ citation records, i.e., so many papers with 0 citation, so many with 1 citation, so many with 2 citations, etc. The standard data for bibliometric rankings of departments have the same structure. It is therefore tempting (and many authors gave in to temptation) to use the same method for computing rankings of scientists and rankings of departments. Depending on the method, this can yield quite surprising and unpleasant results. Indeed, with some methods, it may happen that the \"best\" department contains the \"worst\" scientists, and only them. This problem will not occur if the rankings satisfy a property called consistency, recently introduced in the literature. In this article, we explore the consequences of consistency and we characterize two families of consistent rankings.", "paper_title": "Ranking Scientists and Departments in a Consistent Manner", "paper_id": "WOS:000294255200009"}