{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "-sample_model_selection"}, {"score": 0.004697655699119423, "phrase": "trimmed_hinge_loss_support_vector_machine"}, {"score": 0.004101579688745222, "phrase": "model_selection"}, {"score": 0.004001593118269276, "phrase": "support_vector_classifiers"}, {"score": 0.0039040344230094164, "phrase": "in-sample_methods"}, {"score": 0.0035369291417564606, "phrase": "small-sample_regime"}, {"score": 0.0030876784650745973, "phrase": "trimmed_hinge_loss_function"}, {"score": 0.0029753513692467315, "phrase": "rademacher_complexity"}, {"score": 0.0025338369486914364, "phrase": "selected_classifiers"}, {"score": 0.0022117133940361025, "phrase": "soft_loss_function"}, {"score": 0.0021049977753042253, "phrase": "microarray_data"}], "paper_keywords": ["Support vector machine", " Model selection", " Rademacher complexity", " Maximal discrepancy", " Convex-concave programming"], "paper_abstract": "In this letter, we target the problem of model selection for support vector classifiers through in-sample methods, which are particularly appealing in the small-sample regime. In particular, we describe the application of a trimmed hinge loss function to the Rademacher complexity and maximal discrepancy-based in-sample approaches and show that the selected classifiers outperform the ones obtained with other in-sample model selection techniques, which exploit a soft loss function, in classifying microarray data.", "paper_title": "In-sample Model Selection for Trimmed Hinge Loss Support Vector Machine", "paper_id": "WOS:000310898000005"}