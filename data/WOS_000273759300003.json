{"auto_keywords": [{"score": 0.03034743448212457, "phrase": "metacrawler"}, {"score": 0.00481495049065317, "phrase": "metasearch_engine_performance"}, {"score": 0.00477513738640734, "phrase": "purpose_-_the_purpose"}, {"score": 0.004619134710069517, "phrase": "new_method"}, {"score": 0.004505471345843642, "phrase": "metasearch_engines"}, {"score": 0.004322186889256224, "phrase": "reported_study"}, {"score": 0.004233347974930148, "phrase": "eight_popular_mses"}, {"score": 0.003737234606207431, "phrase": "eight_mses"}, {"score": 0.003615016816532732, "phrase": "closeness_degrees"}, {"score": 0.003258322895587601, "phrase": "mse"}, {"score": 0.0032177835714470027, "phrase": "best_performance"}, {"score": 0.0030867203659106727, "phrase": "ten_different_queries"}, {"score": 0.0030357976028714557, "phrase": "stable_result"}, {"score": 0.002936451109357539, "phrase": "dogpile"}, {"score": 0.0027818812662964766, "phrase": "mamma"}, {"score": 0.002735973337048132, "phrase": "webcrawler"}, {"score": 0.0027133032795773697, "phrase": "almost_the_same_performance"}, {"score": 0.002668524425703961, "phrase": "next_positions"}, {"score": 0.002646412155387084, "phrase": "clusty"}, {"score": 0.002624482632439123, "phrase": "search.com"}, {"score": 0.002404885278896803, "phrase": "mse_designers"}, {"score": 0.002345579707333727, "phrase": "numerous_users"}, {"score": 0.0021944684609599245, "phrase": "novel_method"}, {"score": 0.0021225958499898182, "phrase": "valuable_experimental_results"}, {"score": 0.0021049977753042253, "phrase": "eight_popular_ones"}], "paper_keywords": ["Performance appraisal", " Search engines", " Information retrieval", " Worldwide web"], "paper_abstract": "Purpose - The purpose of this paper is to present a new method for evaluating the performance of metasearch engines (MSEs), which was used in the reported study to investigate which of eight popular MSEs (Clusty, Dogpile, Excite, Mamma, MetaCrawler, Search.com, WebCrawler and Webfetch) is the best. Design/methodology/approach - This research evaluated the performance of eight MSEs. For each MSE the average of closeness degrees between its ranked result list and those of its underlying search engines (SEs) was measured. Next, these measures were compared to each other to determine which MSE gives the best performance. Furthermore the experiment was repeated ten times with ten different queries to reach a stable result. Findings - The findings revealed that Dogpile outperformed all the others, followed by MetaCrawler, Excite, Webfetch and then Mamma. MetaCrawler and WebCrawler had almost the same performance and occupied the next positions. Clusty and Search.com performed poorly in comparison to the others. Practical implications - The findings of this research would be useful for MSE designers as well as helping the numerous users of MSEs to choose a truly effective one. Originality/value - This paper provides a novel method for assessing the performance of MSEs and valuable experimental results on eight popular ones.", "paper_title": "Assessing metasearch engine performance", "paper_id": "WOS:000273759300003"}