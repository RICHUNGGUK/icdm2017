{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "visual-based_human_action_recognition_system"}, {"score": 0.0045581828632009795, "phrase": "known_object"}, {"score": 0.004444317282171449, "phrase": "video_sequence"}, {"score": 0.004296889776409743, "phrase": "whole_scene"}, {"score": 0.004207230913063179, "phrase": "multiple_targets"}, {"score": 0.003883194688426525, "phrase": "visual_tracking_system_framework"}, {"score": 0.003802135020791801, "phrase": "\"near_natural_language\"_description"}, {"score": 0.0036914757355166966, "phrase": "scene_actions"}, {"score": 0.0035389348527719404, "phrase": "feature_extraction"}, {"score": 0.0034944094552943, "phrase": "dynamic_number"}, {"score": 0.0033078284621762817, "phrase": "visual_tracking_system"}, {"score": 0.0032250961348210473, "phrase": "human_knowledge"}, {"score": 0.0031845067058084613, "phrase": "transformed_level"}, {"score": 0.0030657684627749364, "phrase": "raw_videos"}, {"score": 0.0029764767329116875, "phrase": "main_aim"}, {"score": 0.002889778121878728, "phrase": "explicit_the_knowledge_injection"}, {"score": 0.0028293966058473476, "phrase": "low-level_representations"}, {"score": 0.002735392281320413, "phrase": "high-level_semantics"}, {"score": 0.0026445028886931837, "phrase": "visual_tracking_problem"}, {"score": 0.0023996317685956213, "phrase": "representation_spaces"}, {"score": 0.0023694065215394593, "phrase": "memetic_algorithm_particle_filter"}, {"score": 0.0023003498123420237, "phrase": "annotated_scenarios"}, {"score": 0.002261794227418188, "phrase": "video-based_surveillance_applications"}, {"score": 0.0022145052062614514, "phrase": "example_applications"}, {"score": 0.002195866991284673, "phrase": "different_surveillance_scenarios"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Visual tracking problems", " Knowledge representation", " Representation spaces", " Particle filters", " Memetic algorithms"], "paper_abstract": "Visual tracking consists of locating or determining the configuration of a known object at each frame of a video sequence. Usually, the description of the whole scene involves the participation of multiple targets, their movements and interactions, etc., and the scenario particular features. This paper presents a visual tracking system framework oriented to provide a \"near natural language\" description of the involved targets in the scene actions. Our prototype focuses on the detection, tracking and feature extraction of a dynamic number of targets in a scenario along time. The design of any visual tracking system usually needs the injection of human knowledge at each transformed level of description, in order to produce from raw videos a linguistic scene summary. The main aim of this work was to make explicit the knowledge injection needed to link the low-level representations (associated to signals) to the high-level semantics (related to knowledge) in the visual tracking problem. As a result, the emerging semantic necessary at the two transformation level is analysed and presented. We have concentrated on the representation spaces for the memetic algorithm particle filter applied to multiple object tracking in annotated scenarios, oriented to video-based surveillance applications. Finally, some example applications on different surveillance scenarios are presented and discussed. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Representation spaces in a visual-based human action recognition system", "paper_id": "WOS:000263372000024"}