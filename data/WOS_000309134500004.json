{"auto_keywords": [{"score": 0.03800439781238907, "phrase": "trained_and_untrained_annotators"}, {"score": 0.00481495049065317, "phrase": "labeled_word_sense_annotations"}, {"score": 0.004766683200410046, "phrase": "supervised_machine_learning_methods"}, {"score": 0.004695183699894267, "phrase": "word_sense"}, {"score": 0.004601508848655033, "phrase": "human_labelers"}, {"score": 0.004245051445971095, "phrase": "ground_truth_word_sense_labels"}, {"score": 0.00418134346679498, "phrase": "fine-grained_sense_inventory"}, {"score": 0.004139457925808934, "phrase": "wordnet"}, {"score": 0.004016072233242585, "phrase": "sentence_corpus"}, {"score": 0.0038573081851680656, "phrase": "ten_moderately_polysemous_words"}, {"score": 0.0037423521744765075, "phrase": "multiple_sense_labels"}, {"score": 0.0035048567587934254, "phrase": "nuanced_representation"}, {"score": 0.0032989853590079153, "phrase": "assessment_metrics"}, {"score": 0.003073988343893906, "phrase": "sense_distributions"}, {"score": 0.002952356518921907, "phrase": "general_annotation_procedure"}, {"score": 0.002750934994181092, "phrase": "wordnet_sense_labels"}, {"score": 0.0025374674652885354, "phrase": "unsupervised_machine_learning_method"}, {"score": 0.0024993218789131437, "phrase": "ground_truth_labels"}, {"score": 0.0023642842046859274, "phrase": "tentative_support"}], "paper_keywords": ["Word sense annotation", " Multilabel learning", " Inter-annotator reliability"], "paper_abstract": "Supervised machine learning methods to model word sense often rely on human labelers to provide a single, ground truth label for each word in its context. We examine issues in establishing ground truth word sense labels using a fine-grained sense inventory from WordNet. Our data consist of a sentence corpus of 1,000 sentences: 100 for each of ten moderately polysemous words. Each word was given multiple sense labels-or a multilabel-from trained and untrained annotators. The multilabels give a nuanced representation of the degree of agreement on instances. A suite of assessment metrics is used to analyze the sets of multilabels, such as comparisons of sense distributions across annotators. Our assessment indicates that the general annotation procedure is reliable, but that words differ regarding how reliably annotators can assign WordNet sense labels, independent of the number of senses. We also investigate the performance of an unsupervised machine learning method to infer ground truth labels from various combinations of labels from the trained and untrained annotators. We find tentative support for the hypothesis that performance depends on the quality of the set of multilabels, independent of the number of labelers or their training.", "paper_title": "Multiplicity and word sense: evaluating and learning from multiply labeled word sense annotations", "paper_id": "WOS:000309134500004"}