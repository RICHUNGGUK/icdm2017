{"auto_keywords": [{"score": 0.03319210326387996, "phrase": "machine_learning"}, {"score": 0.00481495049065317, "phrase": "graph_cut_clustering"}, {"score": 0.004598933498016774, "phrase": "convex_programming"}, {"score": 0.004044064541716598, "phrase": "combinatorial_problems"}, {"score": 0.004007083686875716, "phrase": "semi-definite_programs"}, {"score": 0.0038802832668554457, "phrase": "considerable_attention"}, {"score": 0.003809628721278519, "phrase": "helmberg"}, {"score": 0.0037402558657576124, "phrase": "de_bie"}, {"score": 0.0037060428445111664, "phrase": "cristianini"}, {"score": 0.0036052633308076933, "phrase": "sdp_problems"}, {"score": 0.0035233696341819437, "phrase": "polynomial_time"}, {"score": 0.0033961782274326948, "phrase": "polynomial_complexity_bounds"}, {"score": 0.0032886455096627324, "phrase": "large_problem_sizes"}, {"score": 0.0031699005399973496, "phrase": "powerful_new_tool"}, {"score": 0.0030135741897058844, "phrase": "new_and_fast_sdp_relaxation"}, {"score": 0.0029722900280316216, "phrase": "normalized_graph_cut_problem"}, {"score": 0.0028914057633212045, "phrase": "unsupervised_and_semi-supervised_learning"}, {"score": 0.002786964045867666, "phrase": "convex_algorithm"}, {"score": 0.0026011706232569316, "phrase": "whole_cascade"}, {"score": 0.00257735013358606, "phrase": "fast_relaxations"}, {"score": 0.0024956789310376635, "phrase": "older_spectral_relaxations"}, {"score": 0.002461472107682778, "phrase": "new_sdp_relaxation"}, {"score": 0.0023834640793589435, "phrase": "computational_cost"}, {"score": 0.002361632539339945, "phrase": "relaxation_accuracy"}, {"score": 0.0021049977753042253, "phrase": "max-cut_problem"}], "paper_keywords": ["convex transduction", " normalized graph cut", " semi-definite programming", " semi-supervised learning", " relaxation", " combinatorial optimization", " max-cut"], "paper_abstract": "The rise of convex programming has changed the face of many research fields in recent years, machine learning being one of the ones that benefitted the most. A very recent developement, the relaxation of combinatorial problems to semi-definite programs ( SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade off computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example.", "paper_title": "Fast SDP relaxations of graph cut clustering, transduction, and other combinatorial problems", "paper_id": "WOS:000245388800011"}