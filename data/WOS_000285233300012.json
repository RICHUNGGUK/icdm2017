{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "spatiotemporal_vocabulary"}, {"score": 0.01360344724429658, "phrase": "global_motions"}, {"score": 0.013491389570122556, "phrase": "concurrent_actions"}, {"score": 0.0047155367844327, "phrase": "human_actions"}, {"score": 0.004579780586951551, "phrase": "semantic_cues"}, {"score": 0.004541712637353326, "phrase": "story_understanding"}, {"score": 0.004447915226010682, "phrase": "novel_search_pattern"}, {"score": 0.004392565073466714, "phrase": "traditional_video_search_scenario"}, {"score": 0.004283913685201902, "phrase": "great_challenges"}, {"score": 0.004230595706156783, "phrase": "action-level_video_search"}, {"score": 0.004074574278425223, "phrase": "actor_appearance_variances"}, {"score": 0.003924284136338428, "phrase": "generalized_action_retrieval_framework"}, {"score": 0.003701401653850141, "phrase": "attention_shift_model"}, {"score": 0.0036248955128033084, "phrase": "human-focused_foreground_actions"}, {"score": 0.003334309053314909, "phrase": "human-focused_action_regions"}, {"score": 0.0030541531847589807, "phrase": "inverted_indexing_structure"}, {"score": 0.003028727936200845, "phrase": "approximate_nearest-neighbor_search"}, {"score": 0.002978508766388785, "phrase": "online_ranking"}, {"score": 0.0029291198314113608, "phrase": "dynamic_time_warping_distance"}, {"score": 0.0028805474861014722, "phrase": "action_duration"}, {"score": 0.0028091907694771613, "phrase": "partial_action_matching"}, {"score": 0.002751075263422475, "phrase": "appearance_hashing_strategy"}, {"score": 0.002682917193736698, "phrase": "performance_degeneration"}, {"score": 0.0026494723416072316, "phrase": "divergent_actor_appearances"}, {"score": 0.0026164433143193015, "phrase": "experimental_validation"}, {"score": 0.0025623050508233078, "phrase": "actor-independent_action_retrieval_framework"}, {"score": 0.002346861891137455, "phrase": "best_performance"}, {"score": 0.00216755493625594, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Video search", " Action retrieval", " Attention Shift", " 3D-SIFT", " Spatiotemporal vocabulary", " Dynamic time warping", " Appearance hashing"], "paper_abstract": "Human actions in movies and sitcoms usually capture semantic cues for story understanding, which offer a novel search pattern beyond the traditional video search scenario. However, there are great challenges to achieve action-level video search, such as global motions, concurrent actions, and actor appearance variances. In this paper, we introduce a generalized action retrieval framework, which achieves fully unsupervised, robust, and actor-independent action search in large-scale database. First, an Attention Shift model is presented to extract human-focused foreground actions from videos containing global motions or concurrent actions. Subsequently, a spatiotemporal vocabulary is built based on 3D-SIFT features extracted from these human-focused action regions. These 3D-SIFT features offer robustness against rotations and viewpoints. And the spatiotemporal vocabulary guarantees our search efficiency, which is achieved by inverted indexing structure with approximate nearest-neighbor search. In the online ranking, we employ dynamic time warping distance to handle the action duration variances, as well as partial action matching. Finally, an appearance hashing strategy is presented to address the performance degeneration caused by divergent actor appearances. For experimental validation, we have deployed actor-independent action retrieval framework in 3-season \"Friends\" sitcoms (over 30 h). In this database, we have reported the best performance (MAP@1 > 0.53) with comparisons to alternative and state-of-the-art approaches. Crown Copyright (C) 2010 Published by Elsevier Ltd. All rights reserved.", "paper_title": "Actor-independent action search using spatiotemporal vocabulary with appearance hashing", "paper_id": "WOS:000285233300012"}