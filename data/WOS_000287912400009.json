{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaussian"}, {"score": 0.004426882270402403, "phrase": "simple_models"}, {"score": 0.004127394241838696, "phrase": "expectation_propagation"}, {"score": 0.00379452936798156, "phrase": "approximate_bayesian_inference"}, {"score": 0.0036641148526656247, "phrase": "ep"}, {"score": 0.0036382660173943393, "phrase": "factorizing_posterior_approximation"}, {"score": 0.0034884152484429207, "phrase": "neural_network_models"}, {"score": 0.0032522091171602557, "phrase": "central_limit_theorem_argument"}, {"score": 0.0023223663501882917, "phrase": "optimal_generalization_performance"}, {"score": 0.0021049977753042253, "phrase": "simple_distribution"}], "paper_keywords": [""], "paper_abstract": "We discuss the expectation propagation (EP) algorithm for approximate Bayesian inference using a factorizing posterior approximation. For neural network models, we use a central limit theorem argument to make EP tractable when the number of parameters is large. For two types of models, we show that EP can achieve optimal generalization performance when data are drawn from a simple distribution.", "paper_title": "Expectation Propagation with Factorizing Distributions: A Gaussian Approximation and Performance Results for Simple Models", "paper_id": "WOS:000287912400009"}