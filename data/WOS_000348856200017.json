{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "stochastic_discrete_time_algorithms"}, {"score": 0.004726382602809228, "phrase": "pca_neural_networks"}, {"score": 0.0046394362688524475, "phrase": "learning_algorithms"}, {"score": 0.0045119929871912405, "phrase": "important_role"}, {"score": 0.004388035083357706, "phrase": "practical_application"}, {"score": 0.004307286039184164, "phrase": "neural_networks"}, {"score": 0.00418892838079239, "phrase": "principal_component_analysis"}, {"score": 0.002997735656669146, "phrase": "stochastic_discrete_time"}, {"score": 0.002731500545019039, "phrase": "original_sdt_algorithms"}, {"score": 0.0025832180231667853, "phrase": "invariant_sets"}, {"score": 0.0023537101705489957, "phrase": "stochastic_environment"}, {"score": 0.0022889103976458437, "phrase": "proper_learning_parameters"}, {"score": 0.0021049977753042253, "phrase": "simulation_examples"}], "paper_keywords": ["Neural networks", " nondivergence", " principal component analysis (PCA)", " stochastic discrete time (SDT) method"], "paper_abstract": "Learning algorithms play an important role in the practical application of neural networks based on principal component analysis, often determining the success, or otherwise, of these applications. These algorithms cannot be divergent, but it is very difficult to directly study their convergence properties, because they are described by stochastic discrete time (SDT) algorithms. This brief analyzes the original SDT algorithms directly, and derives some invariant sets that guarantee the nondivergence of these algorithms in a stochastic environment by selecting proper learning parameters. Our theoretical results are verified by a series of simulation examples.", "paper_title": "Non-Divergence of Stochastic Discrete Time Algorithms for PCA Neural Networks", "paper_id": "WOS:000348856200017"}