{"auto_keywords": [{"score": 0.0314359237087165, "phrase": "representative_features"}, {"score": 0.00481495049065317, "phrase": "regularized_self-representation"}, {"score": 0.004696608876994947, "phrase": "irrelevant_and_redundant_features"}, {"score": 0.004513255056769647, "phrase": "compact_representation"}, {"score": 0.004446349752828045, "phrase": "original_feature"}, {"score": 0.004402295697276237, "phrase": "good_generalization_ability"}, {"score": 0.004272724011826495, "phrase": "unlabeled_data"}, {"score": 0.004230382915464492, "phrase": "unsupervised_feature_selection"}, {"score": 0.0038869467872381957, "phrase": "comprehensive_analysis"}, {"score": 0.0037724857647796813, "phrase": "unlabeled_high_dimensional_data"}, {"score": 0.003661382948704217, "phrase": "low-rank_representation"}, {"score": 0.003281186608301359, "phrase": "linear_combination"}, {"score": 0.003121636289442593, "phrase": "representation_coefficient_matrix"}, {"score": 0.003075296862372287, "phrase": "representation_residual_matrix"}, {"score": 0.003044790972763291, "phrase": "rsr"}, {"score": 0.002569933140999018, "phrase": "significant_row"}, {"score": 0.002544422946205545, "phrase": "representation_coefficients"}, {"score": 0.0024693979425773993, "phrase": "experimental_analysis"}, {"score": 0.002444883254621769, "phrase": "synthetic_and_real-world_data"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Self-representation", " Unsupervised feature selection", " Sparse representation", " Group sparsity"], "paper_abstract": "By removing the irrelevant and redundant features, feature selection aims to find a compact representation of the original feature with good generalization ability. With the prevalence of unlabeled data, unsupervised feature selection has shown to be effective in alleviating the curse of dimensionality, and is essential for comprehensive analysis and understanding of myriads of unlabeled high dimensional data Motivated by the success of low-rank representation in subspace clustering, we propose a regularized self-representation (RSR) model for unsupervised feature selection, where each feature can be represented as the linear combination of its relevant features. By using L-2,L-1-norm to characterize the representation coefficient matrix and the representation residual matrix, RSR is effective to select representative features and ensure the robustness to outliers. If a feature is important, then it will participate in the representation of most of other features, leading to a significant row of representation coefficients, and vice versa. Experimental analysis on synthetic and real-world data demonstrates that the proposed method can effectively identify the representative features, outperforming many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy, redundancy reduction and classification accuracy. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Unsupervised feature selection by regularized self-representation", "paper_id": "WOS:000345809600014"}