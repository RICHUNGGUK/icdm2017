{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "time_scales_calculus"}, {"score": 0.0044356867986444426, "phrase": "mathematical_notion"}, {"score": 0.004358368398332723, "phrase": "ordered_derivative"}, {"score": 0.00408617426304956, "phrase": "ordered_derivatives"}, {"score": 0.004014923642015515, "phrase": "backpropagation_training_algorithm"}, {"score": 0.0039449104968131655, "phrase": "important_emerging_area"}, {"score": 0.0035288860880035985, "phrase": "wide_variety"}, {"score": 0.003487721546598799, "phrase": "inter-disciplinary_problems"}, {"score": 0.0033868916699805224, "phrase": "key_area"}, {"score": 0.003212663446866794, "phrase": "continuous_and_discrete_analysis"}, {"score": 0.002693919110230191, "phrase": "new_multivariate_chain_rule"}, {"score": 0.002555247330102505, "phrase": "time_scales"}, {"score": 0.00249592321829865, "phrase": "key_theorem"}, {"score": 0.0023953929817357882, "phrase": "backpropagation_weight_update_equations"}, {"score": 0.002353554841514325, "phrase": "feedforward_multilayer_neural_network_architecture"}, {"score": 0.002258745901862197, "phrase": "scales_calculus"}, {"score": 0.002193367454830741, "phrase": "neural_network_learning"}, {"score": 0.002129877319760485, "phrase": "first_connection"}], "paper_keywords": ["Backprogagation", " dynamic equations", " neural networks", " ordered derivatives", " time scales"], "paper_abstract": "Backpropagation is the most widely used neural network learning technique. It is based on the mathematical notion of an ordered derivative. In this paper, we present a formulation of ordered derivatives and the backpropagation training algorithm using the important emerging area of mathematics known as the time scales calculus. This calculus, with its potential for application to a wide variety of inter-disciplinary problems, is becoming a key area of mathematics. It is capable of unifying continuous and discrete analysis within one coherent theoretical framework. Using this calculus, we present here a generalization of backpropagation which is appropriate for cases beyond the specifically continuous or discrete. We develop a new multivariate chain rule of this calculus, define ordered derivatives on time scales, prove a key theorem about them, and derive the backpropagation weight update equations for a feedforward multilayer neural network architecture. By drawing together the time scales calculus and the area of neural network learning, we present the first connection of two major fields of research.", "paper_title": "Backpropagation and Ordered Derivatives in the Time Scales Calculus", "paper_id": "WOS:000283422600006"}