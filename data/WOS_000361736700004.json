{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multimodal_behavioral_models"}, {"score": 0.004771196027464407, "phrase": "face-to-face_social_interaction"}, {"score": 0.004558289038745564, "phrase": "multimodal_perception-action_loops"}, {"score": 0.004455409162867219, "phrase": "face-to-face_interactions"}, {"score": 0.00429558848734449, "phrase": "trainable_behavioral_models"}, {"score": 0.0042178318724732005, "phrase": "optimal_actions"}, {"score": 0.004141476915000558, "phrase": "others'_perceived_actions"}, {"score": 0.004085115444878828, "phrase": "joint_goals"}, {"score": 0.00393852508392789, "phrase": "sequential_models"}, {"score": 0.0038849148160362257, "phrase": "particular_discrete_hidden_markov_models"}, {"score": 0.003418327319110734, "phrase": "recurrent_structure"}, {"score": 0.003371773890338284, "phrase": "sensory-motor_states"}, {"score": 0.0032805541677236325, "phrase": "explicit_state_duration"}, {"score": 0.0032358709481309913, "phrase": "discrete_hidden_semi_markov_models"}, {"score": 0.0031627430133087616, "phrase": "prediction_performance"}, {"score": 0.003063123420705719, "phrase": "parallel_speech"}, {"score": 0.0030352396244492604, "phrase": "gaze_data"}, {"score": 0.0027322025124213566, "phrase": "voice_activity"}, {"score": 0.002610043728801011, "phrase": "short-time_viterbi_concept"}, {"score": 0.0025627194819793347, "phrase": "incremental_decoding"}, {"score": 0.002493333098814506, "phrase": "proposed_models"}, {"score": 0.0024593458703463474, "phrase": "objectively_several_properties"}, {"score": 0.0023927516438886445, "phrase": "pure_classification_performance"}, {"score": 0.0023386327847292805, "phrase": "incremental_dhmms"}, {"score": 0.002254571365634257, "phrase": "classic_classifiers"}, {"score": 0.0022136780933978612, "phrase": "incremental_dhsmms"}, {"score": 0.0021537217025910356, "phrase": "later_result"}, {"score": 0.0021049977753042253, "phrase": "state_duration_modeling"}], "paper_keywords": ["Sensory-motor behavior", " Interaction unit recognition", " Gaze prediction", " Hidden Semi-Markov Model"], "paper_abstract": "The aim of this paper is to model multimodal perception-action loops of human behavior in face-to-face interactions. To this end, we propose trainable behavioral models that predict the optimal actions for one specific person given others' perceived actions and the joint goals of the interlocutors. We first compare sequential models-in particular discrete hidden Markov models (DHMMs)-with standard classifiers (SVMs and decision trees). We propose a modification of the initialization of the DHMMs in order to better capture the recurrent structure of the sensory-motor states. We show that the explicit state duration modeling by discrete hidden semi markov models (DHSMMs) improves prediction performance. We applied these models to parallel speech and gaze data collected from interacting dyads. The challenge was to predict the gaze of one subject given the gaze of the interlocutor and the voice activity of both. For both DHMMs and DHSMMs the short-time Viterbi concept is used for incremental decoding and prediction. For the proposed models we evaluated objectively several properties in order to go beyond pure classification performance. Results show that incremental DHMMs (IDHMMs) were more efficient than classic classifiers and superseded by incremental DHSMMs (IDHSMMs). This later result emphasizes the relevance of state duration modeling.", "paper_title": "Learning multimodal behavioral models for face-to-face social interaction", "paper_id": "WOS:000361736700004"}