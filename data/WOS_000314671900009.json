{"auto_keywords": [{"score": 0.04930231023249969, "phrase": "prior_knowledge"}, {"score": 0.010996473325504956, "phrase": "boosting_algorithm"}, {"score": 0.004814951491843901, "phrase": "logitboost"}, {"score": 0.004189698902282913, "phrase": "existing_approaches"}, {"score": 0.004121672175779706, "phrase": "additional_samples"}, {"score": 0.003556873173040169, "phrase": "training_samples"}, {"score": 0.003386301445203071, "phrase": "data_domain"}, {"score": 0.0031199418866663543, "phrase": "different_regularization_parameter"}, {"score": 0.002970260970134645, "phrase": "costly_computation"}, {"score": 0.0026482660253173075, "phrase": "computational_issues"}, {"score": 0.0025211567863488962, "phrase": "mixture_distribution"}, {"score": 0.002303720729387236, "phrase": "numerical_experiments"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Boosting", " Prior knowledge", " Mixture distribution", " Multi-class"], "paper_abstract": "The purpose of this study is to incorporate prior knowledge into a boosting algorithm. Existing approaches require additional samples that represent the prior knowledge. Moreover, in order to adjust the balance between the information in training samples and the prior knowledge in the data domain, one needs to repeat the boosting algorithm with a different regularization parameter. These properties lead to costly computation. In this paper, we propose a boosting algorithm with prior knowledge that avoids computational issues. In our method, the mixture distribution of the estimator and prior knowledge is considered. We describe numerical experiments showing the effectiveness of our approach. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Improving Logitboost with prior knowledge", "paper_id": "WOS:000314671900009"}