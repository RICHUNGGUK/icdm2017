{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "empirical_minimization_algorithm"}, {"score": 0.036229190503611196, "phrase": "target_functions"}, {"score": 0.004195418010131832, "phrase": "simple_argument"}, {"score": 0.0038862297330905836, "phrase": "mild_geometric_assumptions"}, {"score": 0.003711723285313348, "phrase": "class_f"}, {"score": 0.0029951555610569225, "phrase": "uniform_error_rate"}, {"score": 0.0026494723416072316, "phrase": "function_learning_setup"}, {"score": 0.0021049977753042253, "phrase": "slow_uniform_error_rate"}], "paper_keywords": ["empirical minimization", " function learning", " lower bounds", " statistical learning theory"], "paper_abstract": "In this correspondence, we present a simple argument that proves that under mild geometric assumptions on the class F and the set of target functions T, the empirical minimization algorithm cannot yield a uniform error rate that is faster than 1/root k in the function learning setup. This result holds for various loss functionals and the target functions from T that cause the slow uniform error rate are clearly exhibited.", "paper_title": "Lower bounds for the empirical minimization algorithm", "paper_id": "WOS:000257861400041"}