{"auto_keywords": [{"score": 0.04327968208765013, "phrase": "activation_functions"}, {"score": 0.014746090762170478, "phrase": "time_series_forecasting"}, {"score": 0.012738266324262715, "phrase": "activation_function"}, {"score": 0.00481495049065317, "phrase": "asymmetric_activation_function_family"}, {"score": 0.004555499634179394, "phrase": "neural_network_models"}, {"score": 0.004410629943506433, "phrase": "experimental_results"}, {"score": 0.004349955692802962, "phrase": "high_capacity"}, {"score": 0.004309968592440261, "phrase": "function_approximation"}, {"score": 0.004270347498153512, "phrase": "good_accuracy"}, {"score": 0.004096493540086106, "phrase": "fixed_parameters"}, {"score": 0.003787108326100196, "phrase": "neural_network_performance"}, {"score": 0.0037177622939493084, "phrase": "limited_number"}, {"score": 0.003468796799802612, "phrase": "asymmetric_activation_functions_family"}, {"score": 0.003389557201018519, "phrase": "neural_networks"}, {"score": 0.0032968474833776906, "phrase": "activation_functions_family"}, {"score": 0.003177154457204775, "phrase": "universal_approximation_theorem"}, {"score": 0.00309023627455429, "phrase": "global_optimization"}, {"score": 0.0030056887638095883, "phrase": "free_parameter"}, {"score": 0.0029234476524900794, "phrase": "processing_units"}, {"score": 0.002883172383460653, "phrase": "neural_network"}, {"score": 0.002843450389745497, "phrase": "main_idea"}, {"score": 0.0026651772919894534, "phrase": "multilayer_perceptron"}, {"score": 0.0025096359514694523, "phrase": "simulated_annealing"}, {"score": 0.0024865237976623286, "phrase": "tabu_search"}, {"score": 0.0024522530645471065, "phrase": "local_learning_algorithm"}, {"score": 0.002287842738005588, "phrase": "levenberg-marquardt"}, {"score": 0.002225196379204285, "phrase": "overall_purpose"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Neural networks", " Asymmetric activation function", " Free parameter", " Simulated annealing", " Tabu search", " BPM algorithm", " LM algorithm", " Time series"], "paper_abstract": "The use of neural network models for time series forecasting has been motivated by experimental results that indicate high capacity for function approximation with good accuracy. Generally, these models use activation functions with fixed parameters. However, it is known that the choice of activation function strongly influences the complexity and neural network performance and that a limited number of activation functions has been used in general. We describe the use of an asymmetric activation functions family with free parameter for neural networks. We prove that the activation functions family defined, satisfies the requirements of the universal approximation theorem We present a methodology for global optimization of the activation functions family with free parameter and the connections between the processing units of the neural network. The main idea is to optimize, simultaneously, the weights and activation function used in a Multilayer Perceptron (MLP), through an approach that combines the advantages of simulated annealing, tabu search and a local learning algorithm. We have chosen two local learning algorithms: the backpropagation with momentum (BPM) and Levenberg-Marquardt (LM). The overall purpose is to improve performance in time series forecasting. (c) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Optimization of the weights and asymmetric activation function family of neural network for time series forecasting", "paper_id": "WOS:000322857200021"}