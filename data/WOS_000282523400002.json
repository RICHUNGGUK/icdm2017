{"auto_keywords": [{"score": 0.04565647474624857, "phrase": "unlabeled_data"}, {"score": 0.01041037342033881, "phrase": "sparse_multi-view_svms"}, {"score": 0.00481495049065317, "phrase": "sparse_semi-supervised"}, {"score": 0.004791744375656141, "phrase": "learning_using_conjugate_functions"}, {"score": 0.004587835274176236, "phrase": "general_framework"}, {"score": 0.004543708337879809, "phrase": "sparse_semi-supervised_learning"}, {"score": 0.004413846562278824, "phrase": "small_portion"}, {"score": 0.004246428036409373, "phrase": "target_functions"}, {"score": 0.004085333687668526, "phrase": "function_evaluations"}, {"score": 0.003949377440799037, "phrase": "new_example"}, {"score": 0.0038179292024316308, "phrase": "fenchel-legendre"}, {"score": 0.0037267132967205136, "phrase": "convex_insensitive_loss"}, {"score": 0.0034826993555190765, "phrase": "semi-supervised_learning_methods"}, {"score": 0.0034325187732054093, "phrase": "multi-view_co-regularized_least_squares"}, {"score": 0.003399466007862412, "phrase": "single-view_laplacian_support_vector_machines"}, {"score": 0.003115908199991072, "phrase": "squared_epsilon-insensitive_loss"}, {"score": 0.0030121159909273897, "phrase": "inf-sup_problem"}, {"score": 0.002968695624366393, "phrase": "optimal_solutions"}, {"score": 0.0029400959763737364, "phrase": "arguably_saddle-point_properties"}, {"score": 0.002869793011316547, "phrase": "globally_optimal_iterative_algorithm"}, {"score": 0.00269475174341129, "phrase": "generalization_error"}, {"score": 0.002592369213499121, "phrase": "empirical_rademacher_complexity"}, {"score": 0.0025549840900344596, "phrase": "induced_function_class"}, {"score": 0.0025059724686541263, "phrase": "artificial_and_real-world_data"}, {"score": 0.0023990982005125763, "phrase": "sequential_training_approach"}, {"score": 0.0022967713922705, "phrase": "large-scale_problems"}, {"score": 0.002263639474937039, "phrase": "encouraging_experimental_results"}, {"score": 0.0021881743793875767, "phrase": "bound_and_empirical_rademacher_complexity"}, {"score": 0.0021049977753042253, "phrase": "semi-supervised_learning"}], "paper_keywords": ["semi-supervised learning", " Fenchel-Legendre conjugate", " representer theorem", " multiview regularization", " support vector machine", " statistical learning theory"], "paper_abstract": "In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared epsilon-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artificial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efficacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning.", "paper_title": "Sparse Semi-supervised Learning Using Conjugate Functions", "paper_id": "WOS:000282523400002"}