{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multi-document_summarization_of_evaluative"}, {"score": 0.004463958296056236, "phrase": "useful_resources"}, {"score": 0.0044219147356791225, "phrase": "critical_task"}, {"score": 0.004204252026468792, "phrase": "evaluative_documents"}, {"score": 0.004164644263639545, "phrase": "single_entity"}, {"score": 0.004125408095021548, "phrase": "natural_language_summary"}, {"score": 0.003922283019437895, "phrase": "additional_contribution"}, {"score": 0.0037056529033273693, "phrase": "user_preferences"}, {"score": 0.003636125247342626, "phrase": "decision_theory"}, {"score": 0.0032046857274089747, "phrase": "baseline_standard_approach"}, {"score": 0.0031845067058084583, "phrase": "multidocument_summarization"}, {"score": 0.003095257144856771, "phrase": "qualitative_comments"}, {"score": 0.00301801991152303, "phrase": "different_strengths"}, {"score": 0.0029613556844064713, "phrase": "initial_user_study"}, {"score": 0.0027625134766418266, "phrase": "critical_role"}, {"score": 0.002609770322855842, "phrase": "second_user_study"}, {"score": 0.0023887436047824386, "phrase": "evaluative_text_abstraction"}, {"score": 0.002256619934086924, "phrase": "third_user_study"}, {"score": 0.0021049977753042253, "phrase": "untailored_ones"}], "paper_keywords": ["evaluative text", " natural language processing", " summarization", " web intelligence"], "paper_abstract": "In many decision-making scenarios, people can benefit from knowing what other people's opinions are. As more and more evaluative documents are posted on the Web, summarizing these useful resources becomes a critical task for many organizations and individuals. This paper presents a framework for summarizing a corpus of evaluative documents about a single entity by a natural language summary. We propose two summarizers: an extractive summarizer and an abstractive one. As an additional contribution, we show how our abstractive summarizer can be modified to generate summaries tailored to a model of the user preferences that is solidly grounded in decision theory and can be effectively elicited from users. We have tested our framework in three user studies. In the first one, we compared the two summarizers. They performed equally well relative to each other quantitatively, while significantly outperforming a baseline standard approach to multidocument summarization. Trends in the results as well as qualitative comments from participants suggest that the summarizers have different strengths and weaknesses. After this initial user study, we realized that the diversity of opinions expressed in the corpus (i.e., its controversiality) might play a critical role in comparing abstraction versus extraction. To clearly pinpoint the role of controversiality, we ran a second user study in which we controlled for the degree of controversiality of the corpora that were summarized for the participants. The outcome of this study indicates that for evaluative text abstraction tends to be more effective than extraction, particularly when the corpus is controversial. In the third user study we assessed the effectiveness of our user tailoring strategy. The results of this experiment confirm that user tailored summaries are more informative than untailored ones.", "paper_title": "MULTI-DOCUMENT SUMMARIZATION OF EVALUATIVE TEXT", "paper_id": "WOS:000326732400001"}