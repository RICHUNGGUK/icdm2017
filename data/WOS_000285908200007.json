{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "predator-protector-prey_multi-robot_system"}, {"score": 0.004505638402009237, "phrase": "competitive_robotic_systems"}, {"score": 0.004439634831927606, "phrase": "highly_complicated_strategies"}, {"score": 0.004342430078306461, "phrase": "complex_learning_architectures"}, {"score": 0.004310501708408856, "phrase": "analytic_solutions"}, {"score": 0.003945117054416623, "phrase": "complex_phenomena"}, {"score": 0.003802135020791801, "phrase": "reinforcement_learning_problem"}, {"score": 0.0037463983616303786, "phrase": "complex_predator-protector-prey_system"}, {"score": 0.0035445401296650535, "phrase": "brook's_sense"}, {"score": 0.003441352119167824, "phrase": "predator-like_robot"}, {"score": 0.0033411580268394732, "phrase": "reinforcement_learning_capabilities"}, {"score": 0.003304339421246321, "phrase": "pure_bio-mimetic_reactive_prey-like_robot"}, {"score": 0.0030017207229983385, "phrase": "whole_learning_process"}, {"score": 0.0029576827451501956, "phrase": "multi-robot_system"}, {"score": 0.002914288956035066, "phrase": "low-level_point"}, {"score": 0.0027982016314450717, "phrase": "learning_system"}, {"score": 0.002570162055835289, "phrase": "reinforcement_learning_setup"}, {"score": 0.002541817937194794, "phrase": "abstract_actions"}, {"score": 0.0024495702960480076, "phrase": "pure_bio-mimetic_reactive_robot"}, {"score": 0.002404706573379854, "phrase": "experimental_results"}, {"score": 0.002343271162668978, "phrase": "complex_learning_system"}, {"score": 0.0023088708688076666, "phrase": "proposed_reinforcement_learning_setup"}, {"score": 0.002258212866482022, "phrase": "optimal_policy"}, {"score": 0.0022250584824198218, "phrase": "defender_robot"}, {"score": 0.0021522261505574035, "phrase": "predator_robot"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multi-robot systems", " Goal coordination", " Reinforcement learning", " Adaptation", " Autonomous robot navigation"], "paper_abstract": "The area of competitive robotic systems usually leads to highly complicated strategies that must be achieved by complex learning architectures since analytic solutions are unpractical or completely unfeasible. In this work we design an experiment in order to study and validate a model about the complex phenomena of adaptation. In particular, we study a reinforcement learning problem that comprises a complex predator-protector-prey system composed by three different robots: a pure bio-mimetic reactive (in Brook's sense, i.e. without reasoning and representation) predator-like robot, a protector-like robot with reinforcement learning capabilities and a pure bio-mimetic reactive prey-like robot. From the high-level point of view, we are interested in studying whether the Law of Adaptation is useful enough to model and explain the whole learning process occurring in this multi-robot system. From a low-level point of view, our interest is in the design of a learning system capable of solving such a complex competitive predator-protector-prey system optimally. We show how this learning problem can be addressed and solved effectively by means of a reinforcement learning setup that uses abstract actions to select a goal or target towards which a pure bio-mimetic reactive robot must navigate. The experimental results clearly show how the Law of Adaptation fits this complex learning system and that the proposed Reinforcement Learning setup is able to find an optimal policy to control the defender robot in its role of protecting the prey against the predator robot. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Analysis and solution of a predator-protector-prey multi-robot system by a high-level reinforcement learning architecture and the adaptive systems theory", "paper_id": "WOS:000285908200007"}