{"auto_keywords": [{"score": 0.04982105502539461, "phrase": "quantized_nonlinear_discrete-time_systems"}, {"score": 0.04930986456658361, "phrase": "input_constraint"}, {"score": 0.00481495049065317, "phrase": "finite-horizon_near-optimal_output_feedback_neural_network_control_of"}, {"score": 0.004661446705304497, "phrase": "output_feedback-based_near-optimal_regulation"}, {"score": 0.004440277230269978, "phrase": "control_constraint"}, {"score": 0.004392565073466714, "phrase": "finite_horizon"}, {"score": 0.00385832528624328, "phrase": "neural_network"}, {"score": 0.0036553070575015344, "phrase": "control_coefficient_matrix"}, {"score": 0.0035771121569391916, "phrase": "separate_identifier"}, {"score": 0.0034442607586033657, "phrase": "approximate_dynamic_programming-based_actor-critic_framework"}, {"score": 0.003334309053314909, "phrase": "time-varying_solution"}, {"score": 0.003210446135114967, "phrase": "constant_weights"}, {"score": 0.0031759067385562553, "phrase": "time-dependent_activation_functions"}, {"score": 0.003124790943886441, "phrase": "new_error_term"}, {"score": 0.0030086877092638945, "phrase": "nn_update_law"}, {"score": 0.002944284514509205, "phrase": "terminal_constraint_error"}, {"score": 0.002789226820897593, "phrase": "novel_dynamic_quantizer"}, {"score": 0.0027443174539994925, "phrase": "control_inputs"}, {"score": 0.002714779217561487, "phrase": "adaptive_step_size"}, {"score": 0.0026280534600940137, "phrase": "quantization_error_overtime"}, {"score": 0.0025167026017217926, "phrase": "traditional_uniform_quantizer"}, {"score": 0.002410058288617039, "phrase": "forward-in-time_manner"}, {"score": 0.002333044307949414, "phrase": "lyapunov_analysis"}, {"score": 0.002234165013865669, "phrase": "simulation_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["Approximate dynamic programming", " finite horizon", " Hamilton-Jacobi-Bellman (HJB) equation", " neural network (NN)", " optimal regulation", " quantization"], "paper_abstract": "The output feedback-based near-optimal regulation of uncertain and quantized nonlinear discrete-time systems in affine form with control constraint over finite horizon is addressed in this paper. First, the effect of input constraint is handled using a nonquadratic cost functional. Next, a neural network (NN)-based Luenberger observer is proposed to reconstruct both the system states and the control coefficient matrix so that a separate identifier is not needed. Then, approximate dynamic programming-based actor-critic framework is utilized to approximate the time-varying solution of the Hamilton-Jacobi-Bellman using NNs with constant weights and time-dependent activation functions. A new error term is defined and incorporated in the NN update law so that the terminal constraint error is also minimized over time. Finally, a novel dynamic quantizer for the control inputs with adaptive step size is designed to eliminate the quantization error overtime, thus overcoming the drawback of the traditional uniform quantizer. The proposed scheme functions in a forward-in-time manner without offline training phase. Lyapunov analysis is used to investigate the stability. Simulation results are given to show the effectiveness and feasibility of the proposed method.", "paper_title": "Finite-Horizon Near-Optimal Output Feedback Neural Network Control of Quantized Nonlinear Discrete-Time Systems With Input Constraint", "paper_id": "WOS:000358224200016"}