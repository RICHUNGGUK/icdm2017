{"auto_keywords": [{"score": 0.03413764650705368, "phrase": "reconstruction_error"}, {"score": 0.010474805142592943, "phrase": "data_matrix"}, {"score": 0.00848026669531047, "phrase": "concise_representation"}, {"score": 0.00481495049065317, "phrase": "large-scale_data_sets"}, {"score": 0.004760518730536645, "phrase": "today's_information_systems"}, {"score": 0.004671156704912419, "phrase": "massive_amounts"}, {"score": 0.004548848077202625, "phrase": "fast_and_accurate_algorithms"}, {"score": 0.004379631619412168, "phrase": "succinct_format"}, {"score": 0.004313713204732449, "phrase": "big_data_analytics"}, {"score": 0.004232702713825967, "phrase": "representative_instances"}, {"score": 0.004200724305692705, "phrase": "large_and_massively_distributed_data"}, {"score": 0.004075198669927074, "phrase": "column_subset_selection_problem"}, {"score": 0.003953409087444568, "phrase": "data_analysts"}, {"score": 0.003763186299187111, "phrase": "selected_instances"}, {"score": 0.0036784938661226104, "phrase": "data_preprocessing_tasks"}, {"score": 0.0036093693971289754, "phrase": "low-dimensional_embedding"}, {"score": 0.0035685175471526823, "phrase": "data_points"}, {"score": 0.0035147642199945387, "phrase": "low-rank_approximation"}, {"score": 0.0034749794034364197, "phrase": "corresponding_matrix"}, {"score": 0.0033967510986397946, "phrase": "fast_and_accurate_greedy_algorithm"}, {"score": 0.003371066990851953, "phrase": "large-scale_column_subset_selection"}, {"score": 0.0032951702142329357, "phrase": "objective_function"}, {"score": 0.0031246358910995316, "phrase": "selected_columns"}, {"score": 0.003042697264766327, "phrase": "centralized_greedy_algorithm"}, {"score": 0.0030196821790309165, "phrase": "column_subset_selection"}, {"score": 0.002951673109860487, "phrase": "novel_recursive_formula"}, {"score": 0.002798865807872575, "phrase": "mapreduce_algorithm"}, {"score": 0.0025357050267224715, "phrase": "random_projection"}, {"score": 0.0024691719264774165, "phrase": "generalized_column_subset_selection_problem"}, {"score": 0.0021453734588569823, "phrase": "proposed_algorithm"}, {"score": 0.002121056250381627, "phrase": "empirical_evaluation"}, {"score": 0.0021049977753042253, "phrase": "benchmark_data_sets"}], "paper_keywords": ["Column subset selection", " Greedy algorithms", " Distributed computing", " Big data", " MapReduce"], "paper_abstract": "In today's information systems, the availability of massive amounts of data necessitates the development of fast and accurate algorithms to summarize these data and represent them in a succinct format. One crucial problem in big data analytics is the selection of representative instances from large and massively distributed data, which is formally known as the Column Subset Selection problem. The solution to this problem enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix. This paper presents a fast and accurate greedy algorithm for large-scale column subset selection. The algorithm minimizes an objective function, which measures the reconstruction error of the data matrix based on the subset of selected columns. The paper first presents a centralized greedy algorithm for column subset selection, which depends on a novel recursive formula for calculating the reconstruction error of the data matrix. The paper then presents a MapReduce algorithm, which selects a few representative columns from a matrix whose columns are massively distributed across several commodity machines. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets.", "paper_title": "Greedy column subset selection for large-scale data sets", "paper_id": "WOS:000361650600001"}