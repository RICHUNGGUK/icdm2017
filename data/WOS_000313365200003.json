{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "stateful_resources"}, {"score": 0.014830474426625285, "phrase": "action_selection"}, {"score": 0.010221487930460807, "phrase": "limited_resources"}, {"score": 0.009989655673543648, "phrase": "resource_use"}, {"score": 0.00881382289486958, "phrase": "stateless_resources"}, {"score": 0.007076959161670888, "phrase": "knowledge_refinement"}, {"score": 0.006930434909232609, "phrase": "knowledge_corruption"}, {"score": 0.005776487179480016, "phrase": "oe"}, {"score": 0.005019736374932271, "phrase": "task_performance"}, {"score": 0.004734853555630616, "phrase": "multi-agent_systems"}, {"score": 0.004675652357253327, "phrase": "bounded_rationality"}, {"score": 0.004617187934074393, "phrase": "limited_knowledge"}, {"score": 0.004588229170119848, "phrase": "agent_sensing"}, {"score": 0.004502433187097914, "phrase": "agent's_knowledge_refinement"}, {"score": 0.00443680872651669, "phrase": "always_sense"}, {"score": 0.004218956505362619, "phrase": "sensing_actions"}, {"score": 0.004045573463116084, "phrase": "current_literature_addressing_agent"}, {"score": 0.0036120815356279357, "phrase": "sensing_action"}, {"score": 0.0034635519951968907, "phrase": "similar_phenomenon"}, {"score": 0.0034418037573506837, "phrase": "physical_sciences"}, {"score": 0.0032657812947251928, "phrase": "strategic_tradeoff"}, {"score": 0.0031183202790516103, "phrase": "distorted_sensing_outcomes"}, {"score": 0.0030343239259424497, "phrase": "partially_observable_markov_decision_process"}, {"score": 0.0027956306949439478, "phrase": "expected_knowledge_refinement"}, {"score": 0.002635897784526283, "phrase": "side_effects"}, {"score": 0.0025221012517852885, "phrase": "fully_and_partially_observable_agent_mining_simulation"}, {"score": 0.0024852687283132076, "phrase": "resource_state"}, {"score": 0.0024182831453515782, "phrase": "better_knowledge_refinement"}, {"score": 0.002382963149430688, "phrase": "current_and_future_refinement"}, {"score": 0.0022467555474700524, "phrase": "improved_sensing"}, {"score": 0.0022139352210982398, "phrase": "good_knowledge_refinement"}, {"score": 0.002131720106846481, "phrase": "wide_variety"}], "paper_keywords": ["Observer effect", " Stateful resources", " Active perception", " Agent sensing"], "paper_abstract": "In many real-world applications of multi-agent systems, agent reasoning suffers from bounded rationality caused by both limited resources and limited knowledge. When agent sensing to overcome its knowledge limitations also requires resource use, the agent's knowledge refinement is affected due to its inability to always sense when and as accurately as needed, further leading to poor decision making. In this paper, we consider what happens when sensing actions require the use of stateful resources, which we define as resources whose state-dependent behavior changes over time based on usage. Current literature addressing agent sensing with limited resources primarily investigates stateless resources, such as avoiding the use of too much time or energy during sensing. However, sensing itself can change the state of a resource, and thus its behavior, which affects both the information gathered and the resulting knowledge refinement. This produces a phenomenon where the sensing action can and will distort its own outcome (and potentially future outcomes), termed the Observer Effect (OE) after the similar phenomenon in the physical sciences. Under this effect, when deliberating about when and how to perform sensing that requires use of stateful resources, an agent faces a strategic tradeoff between satisfying the need for (1) knowledge refinement to support its reasoning, and (2) avoiding knowledge corruption due to distorted sensing outcomes. To address this tradeoff, we model sensing action selection as a partially observable Markov decision process where an agent optimizes knowledge refinement while considering the (possibly hidden) state of the resources used during sensing. In this model, the agent uses reinforcement learning to learn a controller for action selection, as well as how to predict expected knowledge refinement based on resource use during sensing. Our approach is unique from other bounded rationality and sensing research as we consider how to make decisions about sensing with stateful resources that produce side effects such as the OE, as opposed to simply using stateless resources with no such side effect. We evaluate our approach in a fully and partially observable agent mining simulation. The results demonstrate that considering resource state and the OE during sensing action selection through our approach (1) yielded better knowledge refinement, (2) appropriately balanced current and future refinement to avoid knowledge corruption, and (3) exploited the relationship (i.e., high, positive correlation) between sensing and task performance to boost task performance through improved sensing. Further, our methodology also achieved good knowledge refinement even when the OE is not present, indicating that it can improve sensing performance in a wide variety of environments. Finally, our results also provide insights into the types and configurations of learning algorithms useful for learning within our methodology.", "paper_title": "Observer effect from stateful resources in agent sensing", "paper_id": "WOS:000313365200003"}