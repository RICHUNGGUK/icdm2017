{"auto_keywords": [{"score": 0.050077152391246924, "phrase": "training_methods"}, {"score": 0.043113908496203925, "phrase": "som"}, {"score": 0.04219657017261033, "phrase": "input_-_output_space"}, {"score": 0.04066922039966733, "phrase": "bpa."}, {"score": 0.03812283982785162, "phrase": "wide_variety"}, {"score": 0.033586098978291686, "phrase": "bpa"}, {"score": 0.031468604485623136, "phrase": "rga"}, {"score": 0.028223481403252846, "phrase": "ann_model"}, {"score": 0.004784331242964228, "phrase": "artificial_neural_network_rainfall-runoff_models"}, {"score": 0.004648925234889467, "phrase": "multi-_layer_perceptron"}, {"score": 0.004619418632134153, "phrase": "mlp"}, {"score": 0.004560780566066473, "phrase": "artificial_neural_networks"}, {"score": 0.00446004569562802, "phrase": "rainfall_-_runoff_process"}, {"score": 0.004361526027730043, "phrase": "popular_back-propagation_algorithm"}, {"score": 0.004292484293656852, "phrase": "real-_coded_genetic_algorithm"}, {"score": 0.004197650347643135, "phrase": "self-_organizing_map"}, {"score": 0.0039380388343479384, "phrase": "feed-_forward_mlp_models"}, {"score": 0.0038510053729959074, "phrase": "daily_average_rainfall"}, {"score": 0.003826492478418294, "phrase": "streamflow_data"}, {"score": 0.003777932022044434, "phrase": "existing_catchment"}, {"score": 0.00370624008901701, "phrase": "ann_models"}, {"score": 0.003601235776892337, "phrase": "standard_statistical_performance_evaluation_measures"}, {"score": 0.003230620599529342, "phrase": "separate_ann_models"}, {"score": 0.0032100439293541085, "phrase": "different_classes"}, {"score": 0.0030892986597489485, "phrase": "single_ann_rainfall_-_runoff_model"}, {"score": 0.003040698477714429, "phrase": "ann"}, {"score": 0.002916618252808186, "phrase": "better_generalization"}, {"score": 0.002509770427841966, "phrase": "ann_rainfall_-_runoff_model"}, {"score": 0.0023021895404322767, "phrase": "statistical_performance_indices"}, {"score": 0.002187356046251648, "phrase": "global_error"}, {"score": 0.0021595521075096808, "phrase": "output_layer"}, {"score": 0.002105007806912702, "phrase": "elsevier"}], "paper_keywords": ["artificial neural networks", " rainfall-runoff modelling", " real-coded genetic algorithms", " self-organizing maps", " back-propagation training algorithm"], "paper_abstract": "This paper compares various training methods available for training multi- layer perceptron ( MLP) type of artificial neural networks ( ANNs) for modelling the rainfall - runoff process. The training methods investigated include the popular back-propagation algorithm ( BPA), real- coded genetic algorithm ( RGA), and a self- organizing map ( SOM). A SOM was used to first classify the input - output space into different categories and then develop feed- forward MLP models for each category using BPA. The daily average rainfall and streamflow data derived from an existing catchment were employed to develop all ANN models investigated in this study. A wide variety of standard statistical performance evaluation measures were employed to evaluate the performances of various ANN models developed. The results obtained in this study indicate that the approach of first classifying the input - output space into different categories using SOM and then developing separate ANN models for different classes trained using BPA performs better than the approach of developing a single ANN rainfall - runoff model trained using BPA. The ANN rainfall - runoff model trained using RGA was able to provide a better generalization of the complex, dynamic, non- linear, and fragmented rainfall - runoff process in comparison with the other approaches investigated in this study. It has been found that the RGA trained ANN model significantly outperformed the ANN model trained using BPA, and was also able to overcome certain limitations of the ANN rainfall - runoff model trained using BPA reported by many researchers in the past. It is noted that the performances of various ANN models should to be evaluated using a wide variety of statistical performance indices rather than relying on a few global error statistics normally employed that are similar in nature to the global error minimized at the output layer of an ANN. (C) 2005 Elsevier B. V. All rights reserved.", "paper_title": "A comparative analysis of training methods for artificial neural network rainfall-runoff models", "paper_id": "WOS:000236734100006"}