{"auto_keywords": [{"score": 0.03912269620023401, "phrase": "virtual_agents"}, {"score": 0.015719716506582538, "phrase": "new_set"}, {"score": 0.004774530554776608, "phrase": "dynamic_virtual_reality"}, {"score": 0.0046161999598320486, "phrase": "facial_emotion_recognition_ability"}, {"score": 0.004500891637057374, "phrase": "facial_emotions"}, {"score": 0.004463096299091981, "phrase": "target_behaviour"}, {"score": 0.0043699845927285905, "phrase": "social_impairment"}, {"score": 0.0038996120252338556, "phrase": "dynamic_aspects"}, {"score": 0.0038668462001710314, "phrase": "human_expressions"}, {"score": 0.0036914757355166966, "phrase": "feasible_expressed_emotions"}, {"score": 0.0036450381529042133, "phrase": "main_objective"}, {"score": 0.003599182629885611, "phrase": "present_study"}, {"score": 0.0034944094552943, "phrase": "dynamic_virtual_faces"}, {"score": 0.003465036432323081, "phrase": "high_realism"}, {"score": 0.0033641551050434663, "phrase": "virtual_reality"}, {"score": 0.0031979796189858206, "phrase": "full_repertoire"}, {"score": 0.0031710903727066313, "phrase": "social_skills"}, {"score": 0.003104849179057156, "phrase": "highly_realistic_virtual_faces"}, {"score": 0.003014422649429416, "phrase": "facial_movement_animation"}, {"score": 0.00287759989638073, "phrase": "human_facial_expressions"}, {"score": 0.002735392281320413, "phrase": "facial_emotion_recognition_task"}, {"score": 0.0027009493171069763, "phrase": "natural_faces"}, {"score": 0.0026222551419772867, "phrase": "five_basic_emotions"}, {"score": 0.002545856454402888, "phrase": "anova"}, {"score": 0.0025137856138000014, "phrase": "significant_difference"}, {"score": 0.0024926348205500715, "phrase": "participants'_accuracy"}, {"score": 0.0023395610923736595, "phrase": "vr_images"}, {"score": 0.0022145052062614514, "phrase": "participant's_gender_and_reaction_times"}, {"score": 0.0021049977753042253, "phrase": "realistic_human_expressions"}], "paper_keywords": ["Emotion recognition", " Virtual agents", " Dynamism", " Social skills", " Cyberintervention"], "paper_abstract": "The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (n = 98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants' accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant's gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed.", "paper_title": "Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability", "paper_id": "WOS:000331652400006"}