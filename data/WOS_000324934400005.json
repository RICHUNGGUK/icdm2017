{"auto_keywords": [{"score": 0.048084597108476984, "phrase": "online_learning"}, {"score": 0.00481495049065317, "phrase": "distributed_autonomous_online_learning"}, {"score": 0.0046993598473345395, "phrase": "intrinsic_privacy-preserving"}, {"score": 0.004422325007817267, "phrase": "massive_data"}, {"score": 0.004342430078306461, "phrase": "sequential_nature"}, {"score": 0.004136333933790974, "phrase": "centralized_learner"}, {"score": 0.004012500850311087, "phrase": "update_parameters"}, {"score": 0.003775803846431323, "phrase": "distributed_data_sources"}, {"score": 0.0037075456471849892, "phrase": "autonomous_learners"}, {"score": 0.0036627245728600073, "phrase": "local_parameters"}, {"score": 0.0035965032173588753, "phrase": "local_data_sources"}, {"score": 0.0034465894339710864, "phrase": "small_subset"}, {"score": 0.0033433365160336842, "phrase": "communication_network"}, {"score": 0.0031845067058084583, "phrase": "strongly_convex_functions"}, {"score": 0.003070340620983981, "phrase": "ram_et_al"}, {"score": 0.0029602552900111407, "phrase": "convex_functions"}, {"score": 0.0027854565462509095, "phrase": "intrinsic_privacy-preserving_properties"}, {"score": 0.002685558057339722, "phrase": "sufficient_and_necessary_conditions"}, {"score": 0.002348883765159185, "phrase": "malicious_learner"}, {"score": 0.0022371907579775796, "phrase": "sensitive_raw_data"}, {"score": 0.0021049977753042253, "phrase": "privacy-sensitive_applications"}], "paper_keywords": ["Online learning", " distributed computing", " privacy preservation"], "paper_abstract": "Online learning has become increasingly popular on handling massive data. The sequential nature of online learning, however, requires a centralized learner to store data and update parameters. In this paper, we consider online learning with distributed data sources. The autonomous learners update local parameters based on local data sources and periodically exchange information with a small subset of neighbors in a communication network. We derive the regret bound for strongly convex functions that generalizes the work by Ram et al. [1] for convex functions. More importantly, we show that our algorithm has intrinsic privacy-preserving properties, and we prove the sufficient and necessary conditions for privacy preservation in the network. These conditions imply that for networks with greater-than-one connectivity, a malicious learner cannot reconstruct the subgradients (and sensitive raw data) of other learners, which makes our algorithm appealing in privacy-sensitive applications.", "paper_title": "Distributed Autonomous Online Learning: Regrets and Intrinsic Privacy-Preserving Properties", "paper_id": "WOS:000324934400005"}