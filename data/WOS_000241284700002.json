{"auto_keywords": [{"score": 0.03915514201946942, "phrase": "multiple_outputs"}, {"score": 0.00481495049065317, "phrase": "dimensionality_reduction"}, {"score": 0.004778258911940497, "phrase": "feature_projection"}, {"score": 0.004687742829233727, "phrase": "pattern_recognition"}, {"score": 0.004652016085325825, "phrase": "information_retrieval"}, {"score": 0.004195418010131832, "phrase": "supervised_projection"}, {"score": 0.003961330917648707, "phrase": "target_values"}, {"score": 0.00385658790017119, "phrase": "single-output_setting"}, {"score": 0.0034911335714333507, "phrase": "novel_projection_approach"}, {"score": 0.00346449481422526, "phrase": "multi-output_regularized"}, {"score": 0.003308862957060602, "phrase": "input_features"}, {"score": 0.003018196463874475, "phrase": "latent_variable_model"}, {"score": 0.002983700956478823, "phrase": "joint_input-output_space"}, {"score": 0.002938317647182056, "phrase": "reconstruction_errors"}, {"score": 0.002731971103291716, "phrase": "generalized_eigenvalue_problem"}, {"score": 0.0026596472456182707, "phrase": "nonlinear_mappings"}, {"score": 0.0026393362602995254, "phrase": "prediction_accuracy"}, {"score": 0.0025596276616952516, "phrase": "new_features"}, {"score": 0.0023981301755351607, "phrase": "first_setting"}, {"score": 0.002361632539339945, "phrase": "users'_preferences"}, {"score": 0.0022382002476510573, "phrase": "text_categorization"}, {"score": 0.002145750895912557, "phrase": "multiple_categories"}, {"score": 0.0021212055097236527, "phrase": "proposed_algorithm"}], "paper_keywords": ["dimensionality reduction", " supervised projection", " feature transformation"], "paper_abstract": "Dimensionality reduction by feature projection is widely used in pattern recognition, information retrieval, and statistics. When there are some outputs available (e.g., regression values or classification results), it is often beneficial to consider supervised projection, which is based not only on the inputs, but also on the target values. While this applies to a single-output setting, we are more interested in applications with multiple outputs, where several tasks need to be learned simultaneously. In this paper, we introduce a novel projection approach called Multi-Output Regularized feature Projection (MORP), which preserves the information of input features and, meanwhile, captures the correlations between inputs/outputs and (if applicable) between multiple outputs. This is done by introducing a latent variable model on the joint input-output space and minimizing the reconstruction errors for both inputs and outputs. It turns out that the mappings can be found by solving a generalized eigenvalue problem and are ready to extend to nonlinear mappings. Prediction accuracy can be greatly improved by using the new features since the structure of outputs is explored. We validate our approach in two applications. In the first setting, we predict users' preferences for a set of paintings. The second is concerned with image and text categorization where each image (or document) may belong to multiple categories. The proposed algorithm produces very encouraging results in both settings.", "paper_title": "Multi-output regularized feature projection", "paper_id": "WOS:000241284700002"}