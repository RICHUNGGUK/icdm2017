{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "software_defect_prediction"}, {"score": 0.01282876961763219, "phrase": "simplified_metric_set"}, {"score": 0.012690268720765153, "phrase": "different_scenarios"}, {"score": 0.012350441618882765, "phrase": "training_data"}, {"score": 0.010958918031035696, "phrase": "top-k_metrics"}, {"score": 0.009384541476179526, "phrase": "minimum_metric_subset"}, {"score": 0.008438596066113364, "phrase": "defect_prediction"}, {"score": 0.00465944657565638, "phrase": "crucial_role"}, {"score": 0.004533685795068016, "phrase": "large_number"}, {"score": 0.004459857205489101, "phrase": "prediction_accuracy"}, {"score": 0.0042804804366777865, "phrase": "appropriate_decision"}, {"score": 0.0042338720545681855, "phrase": "cross-project_defect_prediction"}, {"score": 0.003826007045267751, "phrase": "practical_guidelines"}, {"score": 0.0037328656607036447, "phrase": "metric_subset"}, {"score": 0.003622088701900472, "phrase": "six_typical_classifiers"}, {"score": 0.0034009367367442175, "phrase": "acceptable_performance"}, {"score": 0.003309030835969531, "phrase": "statistical_methods"}, {"score": 0.003228434077467506, "phrase": "top-k_metric_subset"}, {"score": 0.0031069334778456633, "phrase": "one-way_anova_tests"}, {"score": 0.0029736479837154843, "phrase": "promise_repository"}, {"score": 0.0028305047329728254, "phrase": "acceptable_result"}, {"score": 0.002807324685396813, "phrase": "benchmark_predictors"}, {"score": 0.002650279521276126, "phrase": "experimental_results"}, {"score": 0.002536534348967784, "phrase": "specific_requirement"}, {"score": 0.00239460114991662, "phrase": "limited_resources"}, {"score": 0.002349073225500705, "phrase": "simple_classifiers"}, {"score": 0.002323446065342625, "phrase": "naive_bayes"}, {"score": 0.002260591903283384, "phrase": "simplified_metric"}, {"score": 0.002128220632893247, "phrase": "general_defect_prediction"}, {"score": 0.002116577422058574, "phrase": "acceptable_loss"}, {"score": 0.0021049977753042253, "phrase": "prediction_precision"}], "paper_keywords": ["Defect prediction", " Software metrics", " Metric set simplification", " Software quality"], "paper_abstract": "Context: Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear. Objective: The objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project. Method: First, based on six typical classifiers, three types of predictors using the size of software metric set were constructed in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way ANOVA tests. Results: The study has been conducted on 34 releases of 10 open-source projects available at the PROMISE repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in Table 12. Conclusion: The experimental results indicate that (1) the choice of training data for defect prediction should depend on the specific requirement of accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Naive Bayes) also tend to perform well when using a simplified metric set for defect prediction; and (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "An empirical study on software defect prediction with a simplified metric set", "paper_id": "WOS:000349427200010"}