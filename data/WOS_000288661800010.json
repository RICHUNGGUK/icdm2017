{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "visual_search_reranking"}, {"score": 0.014566173627834947, "phrase": "auxiliary_knowledge"}, {"score": 0.004781357546476652, "phrase": "pairwise_learning"}, {"score": 0.004665608609734034, "phrase": "visual_documents"}, {"score": 0.004505074966056871, "phrase": "initial_search_results"}, {"score": 0.004395984840790689, "phrase": "search_precision"}, {"score": 0.00436530211426514, "phrase": "conventional_approaches"}, {"score": 0.004274527415193801, "phrase": "\"classification_performance"}, {"score": 0.004215057618794356, "phrase": "optimization_objective"}, {"score": 0.004141877689172017, "phrase": "visual_document"}, {"score": 0.003916098498242888, "phrase": "relevant_documents"}, {"score": 0.003768028741967194, "phrase": "classification_performance"}, {"score": 0.003702581068329137, "phrase": "globally_optimal_ranked_list"}, {"score": 0.003587616378156851, "phrase": "optimization_problem"}, {"score": 0.003525291235622844, "phrase": "ranked_list"}, {"score": 0.0034398445347109396, "phrase": "arbitrary_two_documents"}, {"score": 0.0032522091171602557, "phrase": "existing_approaches"}, {"score": 0.0030747771905143273, "phrase": "optimal_ranked_list"}, {"score": 0.003021334228842501, "phrase": "individual_documents"}, {"score": 0.002989714021608139, "phrase": "document_pairs"}, {"score": 0.0029172107428045964, "phrase": "\"ordinal_relation"}, {"score": 0.0028364942193465194, "phrase": "optimal_document_pairs"}, {"score": 0.00277742178063459, "phrase": "initial_rank_order"}, {"score": 0.0026722904884984348, "phrase": "query_examples"}, {"score": 0.002653606593103781, "phrase": "web_resources"}, {"score": 0.002380120876826732, "phrase": "relevant_relation"}, {"score": 0.0023551955270737215, "phrase": "document_pair"}, {"score": 0.0022660100755784057, "phrase": "final_ranked_list"}, {"score": 0.0022344212744309327, "phrase": "comprehensive_experiments"}, {"score": 0.0022110183535859374, "phrase": "automatic_video_search_task"}, {"score": 0.0021347607855562102, "phrase": "consistent_improvements"}, {"score": 0.0021198271568336063, "phrase": "text_search_baseline"}], "paper_keywords": ["Optimization", " pairwise learning", " search reranking", " visual search"], "paper_abstract": "Visual search reranking is defined as reordering visual documents (images or video clips) based on the initial search results or some auxiliary knowledge to improve the search precision. Conventional approaches to visual search reranking empirically take the \"classification performance\" as the optimization objective, in which each visual document is determined relevant or not, followed by a process of increasing the order of relevant documents. In this paper, we first show that the classification performance fails to produce a globally optimal ranked list, and then we formulate reranking as an optimization problem, in which a ranked list is globally optimal only if any arbitrary two documents in the list are correctly ranked in terms of relevance. This is different from existing approaches which simply classify a document as \"relevant\" or not. To find the optimal ranked list, we convert the individual documents to \"document pairs,\" each represented as a \"ordinal relation.\" Then, we find the optimal document pairs which can maximally preserve the initial rank order while simultaneously keeping the consistency with the auxiliary knowledge mined from query examples and web resources as much as possible. We develop two pairwise reranking methods, difference pairwise reranking (DP-reranking) and exclusion pairwise reranking (EP-reranking), to obtain the relevant relation of each document pair. Finally, a round robin criterion is explored to recover the final ranked list. We conducted comprehensive experiments on an automatic video search task over TRECVID 2005-2007 benchmarks, and showed consistent improvements over text search baseline and other reranking approaches.", "paper_title": "Optimizing Visual Search Reranking via Pairwise Learning", "paper_id": "WOS:000288661800010"}