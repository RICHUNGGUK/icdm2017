{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sparse_models"}, {"score": 0.03318457888521305, "phrase": "optimization_problem"}, {"score": 0.004215057618794356, "phrase": "considerable_attention"}, {"score": 0.004127394241838696, "phrase": "significant_research_efforts"}, {"score": 0.00329813918116714, "phrase": "sparse_learning-to-rank_problem"}, {"score": 0.0029274600162609654, "phrase": "simple_but_efficient_iterative_algorithm"}, {"score": 0.0028068030356445894, "phrase": "experimental_results"}, {"score": 0.002691105581191125, "phrase": "proposed_algorithm"}, {"score": 0.002598333022293788, "phrase": "superior_performance_gain"}, {"score": 0.0023551955270737215, "phrase": "fenchelrank"}, {"score": 0.0022739766507258105, "phrase": "sparse_model"}, {"score": 0.0022110183535859374, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sparse learning-to-rank", " Sparse models", " Information retrieval"], "paper_abstract": "Recently, learning-to-rank has attracted considerable attention. Although significant research efforts have been focused on learning-to-rank, it is not the case for the problem of learning sparse models for ranking. In this paper, we consider the sparse learning-to-rank problem. We formulate it as an optimization problem with the l(1) regularization, and develop a simple but efficient iterative algorithm to solve the optimization problem. Experimental results on four benchmark datasets demonstrate that the proposed algorithm shows (I) superior performance gain compared to several state-of-the-art learning-to-rank algorithms, and (2) very competitive performance compared to FenchelRank that also learns a sparse model for ranking. Crown Copyright (C) 2013 Published by Elsevier B.V. All rights reserved.", "paper_title": "Efficient gradient descent algorithm for sparse models with application in learning-to-rank", "paper_id": "WOS:000322428100018"}