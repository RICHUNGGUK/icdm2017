{"auto_keywords": [{"score": 0.03327942571111727, "phrase": "margin_theory"}, {"score": 0.02199923150723442, "phrase": "adaboost"}, {"score": 0.021840074753775727, "phrase": "decorate"}, {"score": 0.01011829962627862, "phrase": "bagging"}, {"score": 0.009402242626607842, "phrase": "original_training"}, {"score": 0.00481495049065317, "phrase": "decorate_ensemble_learning_algorithm"}, {"score": 0.004709567162738894, "phrase": "successful_ensemble_learning_algorithms"}, {"score": 0.004406994912939125, "phrase": "diversity-based_combination"}, {"score": 0.0043584824527198055, "phrase": "former_two_algorithms"}, {"score": 0.004063348730077936, "phrase": "artificial_training_examples"}, {"score": 0.003989046949801574, "phrase": "stronger_robustness"}, {"score": 0.0039016690475620185, "phrase": "better_resilience"}, {"score": 0.0038729688332870865, "phrase": "missing_features"}, {"score": 0.003531474887552889, "phrase": "bias-variance_theory"}, {"score": 0.003316567137142017, "phrase": "generalization_performance"}, {"score": 0.0029906503184034634, "phrase": "ensemble_learning"}, {"score": 0.0029576827451501956, "phrase": "recent_study"}, {"score": 0.002860938351029155, "phrase": "adaboost_motivated"}, {"score": 0.0024768883044869023, "phrase": "decorate."}, {"score": 0.0022168460543913787, "phrase": "large_extent"}, {"score": 0.002176235362654451, "phrase": "optimization_objective"}, {"score": 0.0021522261505574035, "phrase": "effective_ensemblers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Ensemble learning", " DECORATE", " Bagging", " AdaBoost", " Margin theory", " Bias-variance theory", " Generalization performance"], "paper_abstract": "In the past two decades, some successful ensemble learning algorithms have been proposed, typically as Bagging, AdaBoost, DECORATE, etc. Although all adopting diversity-based combination, the former two algorithms are generated by manipulating the original training set, while the latter one by augmenting the original training set using artificial training examples and has exhibited both stronger robustness to noise than AdaBoost and better resilience to missing features than Bagging and AdaBoost. To better understand the effectiveness of DECORATE, a study has already been conducted from the perspective of the bias-variance theory. However, this theory fails in explaining the effectiveness of AdaBoost which constantly improves the generalization performance by adding more base classifiers but it increases the variance. As known, the margin theory is another important means that can be used to explain the effectiveness of ensemble learning. A recent study has manifested its success in explaining the effectiveness of AdaBoost Motivated by the margin theory, in this paper, we try to empirically analyze the effectiveness of DECORATE from this perspective by conducting experiments on 15 standard data sets, we find that the margin theory can also well explain the effectiveness of DECORATE. The consistency with the bias-variance based analysis states that the margin theory could be used not only as a more general analysis tool for the effectiveness of ensemble learning to a large extent but also as an optimization objective to design effective ensemblers. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "An empirical margin explanation for the effectiveness of DECORATE ensemble learning algorithm", "paper_id": "WOS:000351970300001"}