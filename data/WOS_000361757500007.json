{"auto_keywords": [{"score": 0.039701955977101945, "phrase": "ultra-deep_sequencing_data"}, {"score": 0.00481495049065317, "phrase": "sequencing_data"}, {"score": 0.004519736937243476, "phrase": "dna_sequencing"}, {"score": 0.00443638953122199, "phrase": "computational_biologists"}, {"score": 0.004290201740122912, "phrase": "de_novo_genome_assembly"}, {"score": 0.0039380388343479384, "phrase": "excessive_depth"}, {"score": 0.003587842947046177, "phrase": "bacterial_artificial_chromosome"}, {"score": 0.0034437574674180365, "phrase": "combinatorial_pooling_design"}, {"score": 0.0032686865485199806, "phrase": "de_novo_assembly"}, {"score": 0.0032444113660668743, "phrase": "bac_clones"}, {"score": 0.0032083351151314405, "phrase": "real_ultra-deep_sequencing_data"}, {"score": 0.003102488033561956, "phrase": "sequencing_increases"}, {"score": 0.0030451958332949735, "phrase": "sequencing_errors"}, {"score": 0.0028581775908203683, "phrase": "error-free_data"}, {"score": 0.0026726283357752585, "phrase": "first_problem"}, {"score": 0.0026232532659260033, "phrase": "effective_solution"}, {"score": 0.0024897902536499005, "phrase": "large_dataset"}, {"score": 0.00247128526955136, "phrase": "smaller_samples"}, {"score": 0.0024529174827030787, "phrase": "optimal_size"}, {"score": 0.0023368018449680295, "phrase": "experimental_results"}, {"score": 0.002276566401216226, "phrase": "significant_improvement"}, {"score": 0.0022013916728990564, "phrase": "final_assembly"}, {"score": 0.0021687806803474367, "phrase": "second_problem"}, {"score": 0.0021207658298095845, "phrase": "first_time"}, {"score": 0.0021049977753042253, "phrase": "modern_de_novo_assemblers"}], "paper_keywords": [""], "paper_abstract": "Motivation: As the invention of DNA sequencing in the 70s, computational biologists have had to deal with the problem of de novo genome assembly with limited (or insufficient) depth of sequencing. In this work, we investigate the opposite problem, that is, the challenge of dealing with excessive depth of sequencing. Results: We explore the effect of ultra-deep sequencing data in two domains: (i) the problem of decoding reads to bacterial artificial chromosome (BAC) clones (in the context of the combinatorial pooling design we have recently proposed), and (ii) the problem of de novo assembly of BAC clones. Using real ultra-deep sequencing data, we show that when the depth of sequencing increases over a certain threshold, sequencing errors make these two problems harder and harder (instead of easier, as one would expect with error-free data), and as a consequence the quality of the solution degrades with more and more data. For the first problem, we propose an effective solution based on 'divide and conquer': we 'slice' a large dataset into smaller samples of optimal size, decode each slice independently, and then merge the results. Experimental results on over 15 000 barley BACs and over 4000 cowpea BACs demonstrate a significant improvement in the quality of the decoding and the final assembly. For the second problem, we show for the first time that modern de novo assemblers cannot take advantage of ultra-deep sequencing data.", "paper_title": "When less is more: 'slicing' sequencing data improves read decoding accuracy and de novo assembly quality", "paper_id": "WOS:000361757500007"}