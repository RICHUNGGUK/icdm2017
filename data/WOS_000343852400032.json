{"auto_keywords": [{"score": 0.03714748216598737, "phrase": "precision_matrices"}, {"score": 0.015719716506582538, "phrase": "adaptive_sparsity"}, {"score": 0.015606453716004115, "phrase": "gaussian_mixture_model"}, {"score": 0.014464828424822556, "phrase": "gmm"}, {"score": 0.013650325142952273, "phrase": "observation_data"}, {"score": 0.013070131791493446, "phrase": "bayes"}, {"score": 0.012788168936351154, "phrase": "asgmm"}, {"score": 0.004459101239560164, "phrase": "parameter_estimation"}, {"score": 0.004394350285140955, "phrase": "high_dimensionality"}, {"score": 0.004283280680222718, "phrase": "large_number"}, {"score": 0.004039792863335641, "phrase": "effective_method"}, {"score": 0.0032554458733348103, "phrase": "laplace"}, {"score": 0.002992313172550772, "phrase": "proposed_method"}, {"score": 0.0028741605025505435, "phrase": "adaptive_hierarchical_bayes_model"}, {"score": 0.002780958897911242, "phrase": "jeffrey's_noninformative_hyperprior"}, {"score": 0.002565626044684815, "phrase": "cholesky_decomposition"}, {"score": 0.002500662348528587, "phrase": "positive_definite_property"}, {"score": 0.002437339565360659, "phrase": "expectation_maximization"}, {"score": 0.0023669270875629205, "phrase": "final_estimated_parameters"}, {"score": 0.00234104879370883, "phrase": "gmm._experimental_results"}, {"score": 0.002323953608863511, "phrase": "synthetic_and_real-world_datasets"}, {"score": 0.0022321322970228308, "phrase": "high-dimensional_data"}, {"score": 0.0022158306933307685, "phrase": "small_estimated_error"}, {"score": 0.002175594509651082, "phrase": "better_clustering_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["High-dimensional parameter estimation", " Hierarchical Bayes", " Adaptive sparsity", " GMM"], "paper_abstract": "Gaussian Mixture Model (GMM) has been widely used in statistics for its great flexibility. However, parameter estimation for GMM with high dimensionality is a challenge because of the large number of parameters and the lack of observation data. In this paper, we propose an effective method named hierarchical Bayes based Adaptive Sparsity in Gaussian Mixture Model (ASGMM) to estimate the parameters in a GMM by incorporating a two-layer hierarchical Bayes based adaptive sparsity prior. The prior we impose on the precision matrices can encourage sparsity and hence reduce the dimensionality of the parameters to be estimated. In contrast to the l(1)-norm penalty or Laplace prior, our approach does not involve any hyperparameters that must be tuned, and the sparsity adapts to the observation data. The proposed method is achieved by three steps: first, we formulate an adaptive hierarchical Bayes model of the precision matrices in the GMM with a Jeffrey's noninformative hyperprior, which expresses scale-invariance and, more importantly, is hyperparameter-free and unbiased. Second, we perform a Cholesky decomposition on the precision matrices to impose the positive definite property. Finally, we exploit the expectation maximization (EM) algorithm to obtain the final estimated parameters in the GMM. Experimental results on synthetic and real-world datasets demonstrate that ASGMM cannot only adapt the sparsity of high-dimensional data with small estimated error, but also achieve better clustering performance comparing with several classical methods. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Hierarchical Bayes based Adaptive Sparsity in Gaussian Mixture Model", "paper_id": "WOS:000343852400032"}