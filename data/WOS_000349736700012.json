{"auto_keywords": [{"score": 0.04803238026273219, "phrase": "mst"}, {"score": 0.015719501768337463, "phrase": "image_fusion"}, {"score": 0.015563631740732171, "phrase": "multi-scale_transform"}, {"score": 0.011193013492376221, "phrase": "proposed_fusion_framework"}, {"score": 0.004671588539217021, "phrase": "image_fusion_literature"}, {"score": 0.004563028873351776, "phrase": "sparse_representation"}, {"score": 0.004397487039229447, "phrase": "general_image_fusion_framework"}, {"score": 0.0042521895049768875, "phrase": "inherent_defects"}, {"score": 0.004195418010131832, "phrase": "sr-based_fusion_methods"}, {"score": 0.004002596930301241, "phrase": "pre-registered_source_images"}, {"score": 0.0037930162319613856, "phrase": "sr-based_fusion_approach"}, {"score": 0.0037549546301384336, "phrase": "high-pass_bands"}, {"score": 0.0036923623384423785, "phrase": "absolute_values"}, {"score": 0.0036430378180975667, "phrase": "activity_level_measurement"}, {"score": 0.0036064757492752703, "phrase": "fused_image"}, {"score": 0.003522579870903474, "phrase": "inverse_mst"}, {"score": 0.0034872225047008648, "phrase": "merged_coefficients"}, {"score": 0.0033946556965877873, "phrase": "individual_mst-"}, {"score": 0.0033718991936259038, "phrase": "sr-based_method"}, {"score": 0.003282383509542342, "phrase": "theoretical_point"}, {"score": 0.0030380052848799155, "phrase": "laplacian_pyramid"}, {"score": 0.00300749708357049, "phrase": "low-pass_pyramid"}, {"score": 0.0027648598033559956, "phrase": "cvt"}, {"score": 0.002664375650349975, "phrase": "different_decomposition_levels"}, {"score": 0.002541741549046619, "phrase": "fused_results"}, {"score": 0.002474209809122618, "phrase": "best-performed_fusion_method"}, {"score": 0.0023523750536537102, "phrase": "sliding_window's_step_length"}, {"score": 0.002297585503415527, "phrase": "experimental_results"}, {"score": 0.0021479597698614355, "phrase": "multimodal_images"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Image fusion", " Multi-scale transform", " Sparse representation"], "paper_abstract": "In image fusion literature, multi-scale transform (MST) and sparse representation (SR) are two most widely used signal/image representation theories. This paper presents a general image fusion framework by combining MST and SR to simultaneously overcome the inherent defects of both the MST- and SR-based fusion methods. In our fusion framework, the MST is firstly performed on each of the pre-registered source images to obtain their low-pass and high-pass coefficients. Then, the low-pass bands are merged with a SR-based fusion approach while the high-pass bands are fused using the absolute values of coefficients as activity level measurement. The fused image is finally obtained by performing the inverse MST on the merged coefficients. The advantages of the proposed fusion framework over individual MST- or SR-based method are first exhibited in detail from a theoretical point of view, and then experimentally verified with multi-focus, visible-infrared and medical image fusion. In particular, six popular multi-scale transforms, which are Laplacian pyramid (LP), ratio of low-pass pyramid (RP), discrete wavelet transform (DWT), dual-tree complex wavelet transform (DTCWT), curvelet transform (CVT) and nonsubsampled contourlet transform (NSCT), with different decomposition levels ranging from one to four are tested in our experiments. By comparing the fused results subjectively and objectively, we give the best-performed fusion method under the proposed framework for each category of image fusion. The effect of the sliding window's step length is also investigated. Furthermore, experimental results demonstrate that the proposed fusion framework can obtain state-of-the-art performance, especially for the fusion of multimodal images. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A general framework for image fusion based on multi-scale transform and sparse representation", "paper_id": "WOS:000349736700012"}