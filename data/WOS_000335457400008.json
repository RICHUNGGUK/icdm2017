{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "trajectory_data"}, {"score": 0.04946824888397643, "phrase": "bayesian_optimization"}, {"score": 0.04338327601121217, "phrase": "expected_return"}, {"score": 0.004712988631183401, "phrase": "reinforcement_learning"}, {"score": 0.00443878620915958, "phrase": "parametric_policies"}, {"score": 0.0041984140090404985, "phrase": "bayesian_prior_information"}, {"score": 0.0040223714916651845, "phrase": "new_policies"}, {"score": 0.0039035351575463103, "phrase": "bo_framework"}, {"score": 0.0038702288460744274, "phrase": "policy_search"}, {"score": 0.0038207994186834015, "phrase": "exploration-exploitation_tradeoff"}, {"score": 0.0035830153907286, "phrase": "rl"}, {"score": 0.003522027550662857, "phrase": "sequential_trajectory_information"}, {"score": 0.003477029725200912, "phrase": "rl_agents"}, {"score": 0.003288567185099276, "phrase": "new_gaussian_process"}, {"score": 0.0032606624926016883, "phrase": "gp"}, {"score": 0.0030837288356760973, "phrase": "policy_executions"}, {"score": 0.0029416448256236057, "phrase": "posterior_estimates"}, {"score": 0.0027940789980683, "phrase": "second_contribution"}, {"score": 0.002746549549029177, "phrase": "new_gp_mean_function"}, {"score": 0.0026998264237910884, "phrase": "transition_and_reward_functions"}, {"score": 0.002553382046204689, "phrase": "model-based_approach"}, {"score": 0.0024884913795612707, "phrase": "model_inaccuracies"}, {"score": 0.002467228805628948, "phrase": "good_transition_and_reward_models"}, {"score": 0.0023839754995935184, "phrase": "empirical_results"}, {"score": 0.002353483216544667, "phrase": "standard_set"}, {"score": 0.0023333715029833007, "phrase": "rl_benchmarks"}, {"score": 0.0021049977753042253, "phrase": "synergistic_improvement"}], "paper_keywords": ["reinforcement learning", " Bayesian", " optimization", " policy search", " Markov decision process", " MDP"], "paper_abstract": "Recently, Bayesian Optimization (BO) has been used to successfully optimize parametric policies in several challenging Reinforcement Learning (RL) applications. BO is attractive for this problem because it exploits Bayesian prior information about the expected return and exploits this knowledge to select new policies to execute. Effectively, the BO framework for policy search addresses the exploration-exploitation tradeoff. In this work, we show how to more effectively apply BO to RL by exploiting the sequential trajectory information generated by RL agents. Our contributions can be broken into two distinct, but mutually beneficial, parts. The first is a new Gaussian process (GP) kernel for measuring the similarity between policies using trajectory data generated from policy executions. This kernel can be used in order to improve posterior estimates of the expected return thereby improving the quality of exploration. The second contribution, is a new GP mean function which uses learned transition and reward functions to approximate the surface of the objective. We show that the model-based approach we develop can recover from model inaccuracies when good transition and reward models cannot be learned. We give empirical results in a standard set of RL benchmarks showing that both our model-based and model-free approaches can speed up learning compared to competing methods. Further, we show that our contributions can be combined to yield synergistic improvement in some domains.", "paper_title": "Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning", "paper_id": "WOS:000335457400008"}