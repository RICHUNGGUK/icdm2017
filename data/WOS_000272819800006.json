{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "different_subsets"}, {"score": 0.004347057700479576, "phrase": "representation_spaces"}, {"score": 0.003958049372486465, "phrase": "single_weak_learner"}, {"score": 0.003697100573612401, "phrase": "different_sources"}, {"score": 0.00317095235382607, "phrase": "learning_task"}, {"score": 0.0029617414861784525, "phrase": "different_weak_learners"}, {"score": 0.0027662954811372154, "phrase": "weight_update_stage"}, {"score": 0.002539978170928796, "phrase": "new_weighting_scheme"}, {"score": 0.002433843373780471, "phrase": "theoretical_results"}, {"score": 0.0021049977753042253, "phrase": "independent_boosting_procedures"}], "paper_keywords": ["Machine learning", " boosting", " heterogeneous features", " subsets of features", " convergence proofs"], "paper_abstract": "We focus on the adaptation of boosting to representation spaces composed of different subsets of features. Rather than imposing a single weak learner to handle data that could come from different sources (e. g., images and texts and sounds), we suggest the decomposition of the learning task into several dependent sub-problems of boosting, treated by different weak learners, that will optimally collaborate during the weight update stage. To achieve this task, we introduce a new weighting scheme for which we provide theoretical results. Experiments are carried out and show that our method works significantly better than any combination of independent boosting procedures.", "paper_title": "Boosting Classifiers Built from Different Subsets of Features", "paper_id": "WOS:000272819800006"}