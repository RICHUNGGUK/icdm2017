{"auto_keywords": [{"score": 0.03837949065689616, "phrase": "ghost_copies"}, {"score": 0.037677358159743844, "phrase": "neighborhood_data"}, {"score": 0.00481495049065317, "phrase": "flexible_distributed_mesh_database"}, {"score": 0.004656647251517823, "phrase": "parallel_adaptive_simulations"}, {"score": 0.004612376827433774, "phrase": "parallel_control_functions"}, {"score": 0.004568525347438891, "phrase": "load_balancing"}, {"score": 0.004525088883501587, "phrase": "reduced_inter-process_communication"}, {"score": 0.004376273644295806, "phrase": "distributed_meshes"}, {"score": 0.004212157020917929, "phrase": "computational_purposes"}, {"score": 0.004054169947971483, "phrase": "parallel_performance_degradation"}, {"score": 0.003939565240409143, "phrase": "different_processors"}, {"score": 0.003828187766572941, "phrase": "parallel_algorithm"}, {"score": 0.0037377733149169573, "phrase": "data_copies"}, {"score": 0.0035462807506718578, "phrase": "computation_purposes"}, {"score": 0.00349577373145384, "phrase": "inter-process_communication"}, {"score": 0.0034459835582278746, "phrase": "key_characteristics"}, {"score": 0.0032074366230426727, "phrase": "permissible_topological_order"}, {"score": 0.003043031131345537, "phrase": "selected_adjacencies"}, {"score": 0.002942811795757911, "phrase": "neighborhood_communication_patterns"}, {"score": 0.00290087451159631, "phrase": "ghost_creation_process"}, {"score": 0.0025737890313763407, "phrase": "n_number"}, {"score": 0.002549269480908849, "phrase": "ghost_layers"}, {"score": 0.0024652715040303416, "phrase": "whole_partitioned_mesh"}, {"score": 0.002406968743305622, "phrase": "strong_and_weak_scaling_results"}, {"score": 0.0023276534607900197, "phrase": "cray"}, {"score": 0.0022617389804167943, "phrase": "core_count"}, {"score": 0.0021049977753042253, "phrase": "parallel_super-convergent_patch_recovery_error_estimator"}], "paper_keywords": ["Ghost entities", " mesh database", " neighborhood communication", " massively parallel architectures"], "paper_abstract": "Critical to the scalability of parallel adaptive simulations are parallel control functions including load balancing, reduced inter-process communication and optimal data decomposition. In distributed meshes, many mesh-based applications frequently access neighborhood information for computational purposes which must be transmitted efficiently to avoid parallel performance degradation when the neighbors are on different processors. This article presents a parallel algorithm of creating and deleting data copies, referred to as ghost copies, which localize neighborhood data for computation purposes while minimizing inter-process communication. The key characteristics of the algorithm are: (1) It can create ghost copies of any permissible topological order in a 1D, 2D or 3D mesh based on selected adjacencies. (2) It exploits neighborhood communication patterns during the ghost creation process thus eliminating all-to-all communication. (3) For applications that need neighbors of neighbors, the algorithm can create n number of ghost layers up to a point where the whole partitioned mesh can be ghosted. Strong and weak scaling results are presented for the IBM BG/P and Cray XE6 architectures up to a core count of 32,768 processors. The algorithm also leads to scalable results when used in a parallel super-convergent patch recovery error estimator, an application that frequently accesses neighborhood data to carry out computation.", "paper_title": "A parallel ghosting algorithm for the flexible distributed mesh database", "paper_id": "WOS:000322738200002"}