{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "audio-visual_cues"}, {"score": 0.004598933498016774, "phrase": "uncontrolled_tv_video_material"}, {"score": 0.004437613070853689, "phrase": "huge_intra-class_variability"}, {"score": 0.0042819270655224916, "phrase": "large_differences"}, {"score": 0.00411064950774695, "phrase": "lighting_conditions"}, {"score": 0.004068907083697369, "phrase": "camera_viewpoints"}, {"score": 0.0038862297330905836, "phrase": "existing_small_inter-class_variability"}, {"score": 0.003788296551609616, "phrase": "visual_difference"}, {"score": 0.003563172322934695, "phrase": "previous_works"}, {"score": 0.0034556603183020407, "phrase": "visual_information"}, {"score": 0.0032668821950788533, "phrase": "important_source"}, {"score": 0.0031845067058084613, "phrase": "human_interactions"}, {"score": 0.002831462722290752, "phrase": "audio-visual_bag_of_words"}, {"score": 0.002690406375639144, "phrase": "hir_problem"}, {"score": 0.0026494723416072316, "phrase": "traditional_visual_bag"}, {"score": 0.002466510117718297, "phrase": "combined_use"}, {"score": 0.002416589459425828, "phrase": "audio_information"}, {"score": 0.0023798115577749225, "phrase": "better_classification_results"}, {"score": 0.002249672943553135, "phrase": "challenging_tvhid_dataset"}, {"score": 0.0022041312401435346, "phrase": "proposed_avbow"}, {"score": 0.0021817065300196634, "phrase": "statistically_significant_improvements"}, {"score": 0.0021484955960643167, "phrase": "vbow"}, {"score": 0.0021049977753042253, "phrase": "related_literature"}], "paper_keywords": ["Human interactions", " Audio", " Video", " BOW"], "paper_abstract": "Human Interaction Recognition (HIR) in uncontrolled TV video material is a very challenging problem because of the huge intra-class variability of the classes (due to large differences in the way actions are performed, lighting conditions and camera viewpoints, amongst others) as well as the existing small inter-class variability (e.g., the visual difference between hug and kiss is very subtle). Most of previous works have been focused only on visual information (i.e., image signal), thus missing an important source of information present in human interactions: the audio. So far, such approaches have not shown to be discriminative enough. This work proposes the use of Audio-Visual Bag of Words (AVBOW) as a more powerful mechanism to approach the HIR problem than the traditional Visual Bag of Words (VBOW). We show in this paper that the combined use of video and audio information yields to better classification results than video alone. Our approach has been validated in the challenging TVHID dataset showing that the proposed AVBOW provides statistically significant improvements over the VBOW employed in the related literature.", "paper_title": "Human interaction categorization by using audio-visual cues", "paper_id": "WOS:000330314100006"}