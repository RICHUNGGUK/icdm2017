{"auto_keywords": [{"score": 0.04762108095167326, "phrase": "qim"}, {"score": 0.030125549024480808, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "perceptually_better_data_hiding"}, {"score": 0.0046803650677267146, "phrase": "novel_arrangement"}, {"score": 0.004642604455537883, "phrase": "quantizer_levels"}, {"score": 0.004422325007817267, "phrase": "perceptual_advantages"}, {"score": 0.004386637056030611, "phrase": "logarithmic_quantization"}, {"score": 0.004195418010131832, "phrase": "previous_logarithmic_quantization-based_method"}, {"score": 0.0041112665381266315, "phrase": "compression_function"}, {"score": 0.004078078530972285, "phrase": "mu-law_standard"}, {"score": 0.003932003833923186, "phrase": "host_signal"}, {"score": 0.0038375280115736958, "phrase": "logarithmic_domain"}, {"score": 0.003791141540467091, "phrase": "mu-law_compression_function"}, {"score": 0.0037150687722055727, "phrase": "transformed_data"}, {"score": 0.0035386420095120706, "phrase": "original_domain"}, {"score": 0.003495855671790616, "phrase": "inverse_function"}, {"score": 0.003453584878128719, "phrase": "scalar_method"}, {"score": 0.003384262486955773, "phrase": "vector_quantization"}, {"score": 0.0032629585038738856, "phrase": "host_vector"}, {"score": 0.003133252930886608, "phrase": "logarithmic_radii"}, {"score": 0.003107935254560724, "phrase": "optimum_parameter_mu"}, {"score": 0.0029602552900111407, "phrase": "host_signal_distribution"}, {"score": 0.002877361619893651, "phrase": "secret_key"}, {"score": 0.0027854565462509095, "phrase": "dither_modulation"}, {"score": 0.0025892330566358503, "phrase": "analytical_derivations"}, {"score": 0.002547535564922075, "phrase": "extensive_simulations"}, {"score": 0.002526938673434697, "phrase": "artificial_signals"}, {"score": 0.0024461989110005447, "phrase": "real_images"}, {"score": 0.002387337550919433, "phrase": "previous_scalar_and_vector_quantization-based_methods"}, {"score": 0.002311047934798247, "phrase": "stronger_a_watermark"}, {"score": 0.0022738201751000865, "phrase": "conventional_qim"}, {"score": 0.0022011501095573747, "phrase": "better_performance"}, {"score": 0.0021049977753042253, "phrase": "previously_proposed_logarithmic_quantization_algorithm"}], "paper_keywords": ["Digital watermarking", " generalized Gaussian distribution", " logarithmic quantization", " Quantization Index Modulation (QIM)"], "paper_abstract": "In this paper, a novel arrangement for quantizer levels in the Quantization Index Modulation (QIM) method is proposed. Due to perceptual advantages of logarithmic quantization, and in order to solve the problems of a previous logarithmic quantization-based method, we used the compression function of mu-Law standard for quantization. In this regard, the host signal is first transformed into the logarithmic domain using the mu-Law compression function. Then, the transformed data is quantized uniformly and the result is transformed back to the original domain using the inverse function. The scalar method is then extended to vector quantization. For this, the magnitude of each host vector is quantized on the surface of hyperspheres which follow logarithmic radii. Optimum parameter mu for both scalar and vector cases is calculated according to the host signal distribution. Moreover, inclusion of a secret key in the proposed method, similar to the dither modulation in QIM, is introduced. Performance of the proposed method in both cases is analyzed and the analytical derivations are verified through extensive simulations on artificial signals. The method is also simulated on real images and its performance is compared with previous scalar and vector quantization-based methods. Results show that this method features stronger a watermark in comparison with conventional QIM and, as a result, has better performance while it does not suffer from the drawbacks of a previously proposed logarithmic quantization algorithm.", "paper_title": "A Logarithmic Quantization Index Modulation for Perceptually Better Data Hiding", "paper_id": "WOS:000277773200010"}