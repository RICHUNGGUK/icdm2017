{"auto_keywords": [{"score": 0.04137315141208966, "phrase": "individual_networks"}, {"score": 0.011865083170001466, "phrase": "original_training_and_testing_data_sets"}, {"score": 0.008713768692145006, "phrase": "multiple_neural_networks"}, {"score": 0.00842446140509578, "phrase": "forward_selection"}, {"score": 0.00835363530608493, "phrase": "backward_elimination"}, {"score": 0.005734236542734402, "phrase": "model_generalisation"}, {"score": 0.005404258839277434, "phrase": "aggregated_network_error"}, {"score": 0.00475413908468027, "phrase": "model_prediction"}, {"score": 0.004714023408114857, "phrase": "nonlinear_systems"}, {"score": 0.004581324904827059, "phrase": "proposed_techniques"}, {"score": 0.004404994123834589, "phrase": "neural_network_generalisation"}, {"score": 0.004151188559520802, "phrase": "perfect_single_neural_network"}, {"score": 0.0040298010803060495, "phrase": "aggregated_neural_network_model"}, {"score": 0.0038789122930433305, "phrase": "different_data_sets"}, {"score": 0.0038461532860146535, "phrase": "different_training_algorithms"}, {"score": 0.0030199761431186434, "phrase": "aggregated_network"}, {"score": 0.0024842242921857705, "phrase": "dynamic_nonlinear_process"}, {"score": 0.0024320326084093465, "phrase": "diabetes_database"}, {"score": 0.0024114635128122783, "phrase": "application_results"}, {"score": 0.0022434880661725493, "phrase": "heuristic_selective_combination_method"}, {"score": 0.002205692205167157, "phrase": "better_performance"}, {"score": 0.002177762848946681, "phrase": "training_and_testing_data"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multiple neural networks", " Selective combination of neural networks", " Generalisation", " Forward selection", " Backward elimination"], "paper_abstract": "Combining multiple neural networks appears to be a very promising approach in improving neural network generalisation since it is very difficult, if not impossible, to develop a perfect single neural network. In the building of an aggregated neural network model, a number of individual networks are developed from different data sets and/or different training algorithms. In this paper, individual networks are developed from bootstrap re-samples of the original training and testing data sets. Instead of combining all the developed networks, this paper proposes two selective combination techniques: forward selection and backward elimination. These two techniques essentially combine those individual networks that, when combined, can significantly improve model generalisation. In forward selection, individual networks are gradually added into the aggregated network until the aggregated network error on the original training and testing data sets cannot be further reduced. In backward elimination, all the individual networks are initially aggregated and some of the individual networks are then gradually eliminated until the aggregated network error on the original training and testing data sets cannot be further reduced. The proposed techniques are applied to dynamic nonlinear process modelling and classification of diabetes database. Application results demonstrate that the proposed techniques can significantly improve model generalisation and perform better than aggregating all the individual networks and the heuristic selective combination method where networks with better performance on the training and testing data are selected. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Selective combination of multiple neural networks for improving model prediction in nonlinear systems modelling through forward selection and backward elimination", "paper_id": "WOS:000263372000055"}