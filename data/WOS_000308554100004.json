{"auto_keywords": [{"score": 0.03757753897006022, "phrase": "end-system_resources"}, {"score": 0.00481495049065317, "phrase": "end_data-flow"}, {"score": 0.004751889650408546, "phrase": "throughput_optimization"}, {"score": 0.004710306419574744, "phrase": "high-speed_networks"}, {"score": 0.004527619017250708, "phrase": "large-scale_scientific_applications"}, {"score": 0.004429191773525288, "phrase": "efficient_transfer"}, {"score": 0.00431388720289049, "phrase": "current_optical_networking_technology"}, {"score": 0.0041102032028337366, "phrase": "inefficient_transport_protocols"}, {"score": 0.003847802030433298, "phrase": "high-performance_systems"}, {"score": 0.003780692127133743, "phrase": "parallel_disks"}, {"score": 0.003714748329627737, "phrase": "network_interfaces"}, {"score": 0.00352371416065881, "phrase": "available_network_capacity_results"}, {"score": 0.003447035529485425, "phrase": "network_bandwidth"}, {"score": 0.0032697235411641695, "phrase": "'end-to-end_data-flow_parallelism"}, {"score": 0.0030206579994328975, "phrase": "multiple_parallel_streams"}, {"score": 0.0028526276660691525, "phrase": "end_systems"}, {"score": 0.002790511479398411, "phrase": "optimal_number"}, {"score": 0.002693919110230191, "phrase": "data_transfer_speed"}, {"score": 0.0026121403051734744, "phrase": "gridftp_parallel_samplings"}, {"score": 0.0025665261770229757, "phrase": "optimal_level"}, {"score": 0.002434396049547248, "phrase": "actual_gridftp_transfers"}, {"score": 0.0023192471808039746, "phrase": "close-to-optimal_performances"}, {"score": 0.002258745901862197, "phrase": "minimal_number"}, {"score": 0.0022095328812245852, "phrase": "end-to-end_data_transfer_throughput"}, {"score": 0.002142426911242168, "phrase": "end-system_bottlenecks"}, {"score": 0.0021049977753042253, "phrase": "non-optimized_transfers"}], "paper_keywords": ["End-to-end modeling", " Throughput optimization", " Data-flow parallelism", " Prediction", " Parallel streams", " Striping"], "paper_abstract": "The increase in the data produced by large-scale scientific applications necessitates innovative solutions for efficient transfer of data. Although the current optical networking technology reached theoretical speeds of 100 Gbps, applications still suffer from inefficient transport protocols and bottlenecks on the end-systems (e.g. disk, CPU, NIC). High-performance systems provide us with parallel disks, processors and network interfaces. However the lack of orchestration of these end-system resources with the available network capacity results in underutilization of the network bandwidth. In this study, a model and two algorithms that use 'end-to-end data-flow parallelism' to optimize the use of network and end-system resources are proposed. This is achieved by using multiple parallel streams over the network; and multiple parallel disks and CPUs at the end systems. Our model predicts the optimal number of streams and disk/CPU stripes that maximizes the data transfer speed for any setting. Our algorithms use GridFTP parallel samplings and calculate the optimal level of parallelism based on our prediction model. The experiments conducted by using actual GridFTP transfers show that the predictions performed by our model and algorithms provide close-to-optimal performances with negligible overhead and use minimal number of resources. The end-to-end data transfer throughput is improved dramatically in existence of end-system bottlenecks compared to the non-optimized transfers.", "paper_title": "End-to-End Data-Flow Parallelism for Throughput Optimization in High-Speed Networks", "paper_id": "WOS:000308554100004"}