{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "imbalanced_data"}, {"score": 0.04854911147289717, "phrase": "data_intrinsic_characteristics"}, {"score": 0.0046276016230633495, "phrase": "training_classifiers"}, {"score": 0.004528488165249491, "phrase": "imbalanced_class_distributions"}, {"score": 0.004479726755921316, "phrase": "important_problem"}, {"score": 0.004447509888681154, "phrase": "data_mining"}, {"score": 0.003794297119656863, "phrase": "machine_learning"}, {"score": 0.0036202697222281373, "phrase": "imbalanced_dataset_scenario"}, {"score": 0.0035426543993952184, "phrase": "specific_metrics"}, {"score": 0.0034046409849387365, "phrase": "proposed_solutions"}, {"score": 0.003283830127906969, "phrase": "cost-sensitive_learning"}, {"score": 0.003190265467214105, "phrase": "experimental_study"}, {"score": 0.003088178133049113, "phrase": "inter-family_comparison"}, {"score": 0.003011033940247791, "phrase": "thorough_discussion"}, {"score": 0.0029785631356166594, "phrase": "main_issues"}, {"score": 0.0029041492019217033, "phrase": "classification_problem"}, {"score": 0.002821371992763293, "phrase": "current_models"}, {"score": 0.0027409477035216195, "phrase": "small_disjuncts"}, {"score": 0.0026628098176168682, "phrase": "training_data"}, {"score": 0.0025682543875086934, "phrase": "noisy_data"}, {"score": 0.00251313643823856, "phrase": "borderline_instances"}, {"score": 0.0024770482478966896, "phrase": "dataset_shift"}, {"score": 0.002423882939663573, "phrase": "test_distributions"}, {"score": 0.002230418233881411, "phrase": "experimental_examples"}, {"score": 0.0021825348057711628, "phrase": "learning_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Imbalanced dataset", " Sampling", " Cost-sensitive learning", " Small disjuncts", " Noisy data", " Dataset shift"], "paper_abstract": "Training classifiers with datasets which suffer of imbalanced class distributions is an important problem in data mining. This issue occurs when the number of examples representing the class of interest is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. We shortly review the many issues in machine learning and applications of this problem, by introducing the characteristics of the imbalanced dataset scenario in classification, presenting the specific metrics for evaluating performance in class imbalanced learning and enumerating the proposed solutions. In particular, we will describe preprocessing, cost-sensitive learning and ensemble techniques, carrying out an experimental study to contrast these approaches in an intra and inter-family comparison. We will carry out a thorough discussion on the main issues related to using data intrinsic characteristics in this classification problem. This will help to improve the current models with respect to: the presence of small disjuncts, the lack of density in the training data, the overlapping between classes, the identification of noisy data, the significance of the borderline instances, and the dataset shift between the training and the test distributions. Finally, we introduce several approaches and recommendations to address these problems in conjunction with imbalanced data, and we will show some experimental examples on the behavior of the learning algorithms on data with such intrinsic characteristics. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics", "paper_id": "WOS:000325041400006"}