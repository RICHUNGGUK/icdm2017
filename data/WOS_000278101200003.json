{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "local_adaptivity"}, {"score": 0.0045726187330434025, "phrase": "expectation-maximization_algorithm"}, {"score": 0.004342430078306461, "phrase": "image_segmentation"}, {"score": 0.004123781267600542, "phrase": "key_idea"}, {"score": 0.0038758293658651237, "phrase": "global_statistics"}, {"score": 0.0037188360359702182, "phrase": "gaussian_mixture_model"}, {"score": 0.003459191419478298, "phrase": "geometrical_information"}, {"score": 0.0033190180867511605, "phrase": "local_probability_distribution"}, {"score": 0.0030554300179552415, "phrase": "combined_information"}, {"score": 0.0028715299874549245, "phrase": "adaptive_local_classification_strategy"}, {"score": 0.0025625729433234623, "phrase": "fine_features"}, {"score": 0.002408265305704217, "phrase": "proposed_methodology"}], "paper_keywords": ["expectation-maximization algorithm", " Gaussian mixture model", " posterior probability", " local adaptivity", " image segmentation"], "paper_abstract": "We develop an expectation-maximization algorithm with local adaptivity for image segmentation and classification. The key idea of our approach is to combine global statistics extracted from the Gaussian mixture model or other proper statistical models with local statistics and geometrical information, such as local probability distribution, orientation, and anisotropy. The combined information is used to design an adaptive local classification strategy that improves the robustness of the algorithm and also keeps fine features in the image. The proposed methodology is flexible and can be easily generalized to deal with other inferred information/quantities and statistical methods/models.", "paper_title": "Expectation-Maximization Algorithm with Local Adaptivity", "paper_id": "WOS:000278101200003"}