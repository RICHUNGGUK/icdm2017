{"auto_keywords": [{"score": 0.03813099850370479, "phrase": "short_words"}, {"score": 0.00481495049065317, "phrase": "acoustic_and_linguistic_contexts"}, {"score": 0.004785022360244881, "phrase": "human_and_machine_speech_recognition"}, {"score": 0.004667149874736771, "phrase": "automatic_speech_recognition_system"}, {"score": 0.004623696669036629, "phrase": "-gram_language_models"}, {"score": 0.004594951819987416, "phrase": "hmm_acoustic_models"}, {"score": 0.004426180319550746, "phrase": "word_recognition_performance"}, {"score": 0.004398657974555281, "phrase": "human_subjects"}, {"score": 0.004237065611401975, "phrase": "local_linguistic_context"}, {"score": 0.004081385289516914, "phrase": "speech_recordings"}, {"score": 0.004018212085786494, "phrase": "japanese_narration"}, {"score": 0.003993216345115755, "phrase": "spontaneous_speech_corpora"}, {"score": 0.003919154537171269, "phrase": "isolated_words"}, {"score": 0.003798746478986171, "phrase": "spontaneous_speech"}, {"score": 0.003739931635717522, "phrase": "word-boundary_coarticulation"}, {"score": 0.003602477150247449, "phrase": "japanese"}, {"score": 0.003557752688779838, "phrase": "post-positional_particles"}, {"score": 0.003373868246555499, "phrase": "function_words"}, {"score": 0.0033319964917118204, "phrase": "content_words"}, {"score": 0.003015143226706584, "phrase": "even_more_context"}, {"score": 0.0029870247665469358, "phrase": "human_prediction_performance"}, {"score": 0.002968424524565369, "phrase": "text-only_conditions"}, {"score": 0.002940740466043107, "phrase": "acoustic_signals"}, {"score": 0.0028861422653675283, "phrase": "speech_recognition"}, {"score": 0.0028061343826918357, "phrase": "recognition_experiments"}, {"score": 0.0027799596509593166, "phrase": "automatic_speech"}, {"score": 0.0026279461299595756, "phrase": "acoustic_models"}, {"score": 0.00260342914245402, "phrase": "language_model"}, {"score": 0.00252334167070616, "phrase": "human_recognition_performance"}, {"score": 0.002461044825190643, "phrase": "prediction_performance"}, {"score": 0.0024380811094377006, "phrase": "trigram_language_model"}, {"score": 0.0023927929565329873, "phrase": "human_performance"}, {"score": 0.0023410164207985297, "phrase": "succeeding_word"}, {"score": 0.00223379991739614, "phrase": "automatic_speech_recognizers"}, {"score": 0.0021991611806433634, "phrase": "recognition_performance"}, {"score": 0.0021448505687807796, "phrase": "linguistic_context"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Continuous speech recognition", " Human speech recognition ability", " Acoustic model", " Language model"], "paper_abstract": "We compared the performance of an automatic speech recognition system using is-gram language models, HMM acoustic models, as well as combinations of the two, with the word recognition performance of human subjects who either had access to only acoustic information, had information only about local linguistic context, or had access to a combination of both. All speech recordings used were taken from Japanese narration and spontaneous speech corpora. Humans have difficulty recognizing isolated words taken out of context, especially when taken from spontaneous speech, partly due to word-boundary coarticulation. Our recognition performance improves dramatically when one or two preceding words are added. Short words in Japanese mainly consist of post-positional particles (i.e. wa, go, wo, ni, etc.), which are function words located just after content words such as nouns and verbs. So the predictability of short words is very high within the context of the one or two preceding words, and thus recognition of short words is drastically improved. Providing even more context further improves human prediction performance under text-only conditions (without acoustic signals). It also improves speech recognition, but the improvement is relatively small. Recognition experiments using an automatic speech recognizer were conducted under conditions almost identical to the experiments with humans. The performance of the acoustic models without any language model, or with only a unigram language model, were greatly inferior to human recognition performance with no context. In contrast, prediction performance using a trigram language model was superior or comparable to human performance when given a preceding and a succeeding word. These results suggest that we must improve our acoustic models rather than our language models to make automatic speech recognizers comparable to humans in recognition performance under conditions where the recognizer has limited linguistic context. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Effect of acoustic and linguistic contexts on human and machine speech recognition", "paper_id": "WOS:000332999300004"}