{"auto_keywords": [{"score": 0.046054847363248386, "phrase": "job_scheduler"}, {"score": 0.010612387000973441, "phrase": "parallelism_feedback"}, {"score": 0.0073746047232380995, "phrase": "time_steps"}, {"score": 0.004796654329628275, "phrase": "multiprocessor_scheduling"}, {"score": 0.004769339736682578, "phrase": "shared_multiprogramming_environment"}, {"score": 0.004724158990345732, "phrase": "two-level_scheduling"}, {"score": 0.004688321299229397, "phrase": "kernel-level_job_scheduler"}, {"score": 0.004478907336898875, "phrase": "randomized_work-stealing_thread_scheduler"}, {"score": 0.004461882369408972, "phrase": "fork-join_multithreaded_jobs"}, {"score": 0.0044364656402489915, "phrase": "continual_parallelism_feedback"}, {"score": 0.004303324487596141, "phrase": "large_parallel_servers"}, {"score": 0.004262539473250432, "phrase": "common_multiprocessor_resource"}, {"score": 0.004174162232710988, "phrase": "particular_job"}, {"score": 0.004134596278123439, "phrase": "job's_execution"}, {"score": 0.004018125427175556, "phrase": "job's_thread_scheduler"}, {"score": 0.003942299471749811, "phrase": "near-optimal_time"}, {"score": 0.0038974897804242886, "phrase": "allotted_processors"}, {"score": 0.0038385317554884713, "phrase": "thread_scheduler's_adversary"}, {"score": 0.003809386759508937, "phrase": "thread_scheduler"}, {"score": 0.0037660820930878742, "phrase": "operating_environment"}, {"score": 0.003723267869178867, "phrase": "job_scheduler's_administrative_policies"}, {"score": 0.0036529857560317187, "phrase": "large_number"}, {"score": 0.0035096666710980108, "phrase": "stringent_adversarial_assumption"}, {"score": 0.003476377333903321, "phrase": "new_technique"}, {"score": 0.003352722512047951, "phrase": "small_number"}, {"score": 0.0033209168358553686, "phrase": "near-optimal_behavior"}, {"score": 0.0033019779616246356, "phrase": "vast_majority"}, {"score": 0.0032211456924791364, "phrase": "span_t-infinity"}, {"score": 0.0031845067058084583, "phrase": "p_processors"}, {"score": 0.003130325700934738, "phrase": "expected_duration"}, {"score": 0.0030770636828088734, "phrase": "l_lg"}, {"score": 0.0030017207229983385, "phrase": "scheduling_quantum"}, {"score": 0.002867425368894356, "phrase": "processor_availability"}, {"score": 0.002797201072577733, "phrase": "highest_processor_availability"}, {"score": 0.002775940603086468, "phrase": "job's_parallelism"}, {"score": 0.002760101069870307, "phrase": "trimmed_availability"}, {"score": 0.0026720312935445095, "phrase": "nearly_perfect_linear_speedup"}, {"score": 0.002641621667026613, "phrase": "trimmed_mean"}, {"score": 0.0026115572175623666, "phrase": "asymptotic_running_time"}, {"score": 0.0025867643733794724, "phrase": "nearly_the_length"}, {"score": 0.0025042115651986332, "phrase": "simulated_multiprocessor_system"}, {"score": 0.002494673889294297, "phrase": "synthetic_workloads"}, {"score": 0.002470987929071079, "phrase": "sufficient_parallelism"}, {"score": 0.0024335561337016174, "phrase": "almost_perfect_linear_speedup"}, {"score": 0.0024104490605875925, "phrase": "processor_availability_profiles"}, {"score": 0.0023739321566540682, "phrase": "abp_algorithm"}, {"score": 0.002324620886282579, "phrase": "arora_et_al"}, {"score": 0.002263336340041527, "phrase": "heavily_loaded_machines"}, {"score": 0.0022547140008848912, "phrase": "large_numbers"}, {"score": 0.002191084548611809, "phrase": "abp"}, {"score": 0.0021170858148884014, "phrase": "processor_cycles"}, {"score": 0.0021049977753042253, "phrase": "abp."}], "paper_keywords": ["algorithms", " performance", " theory", " experimentation", " adaptive scheduling", " adversary", " span", " instantaneous parallelism", " job scheduling", " multiprocessing", " multiprogramming", " parallelism feedback", " parallel computation", " processor allocation", " randomized algorithm", " thread scheduling", " two-level scheduling", " space sharing", " trim analysis", " work", " work-stealing"], "paper_abstract": "Multiprocessor scheduling in a shared multiprogramming environment can be structured as two-level scheduling, where a kernel-level job scheduler allots processors to jobs and a user-level thread scheduler schedules the work of a job on its allotted processors. We present a randomized work-stealing thread scheduler for fork-join multithreaded jobs that provides continual parallelism feedback to the job scheduler in the form of requests for processors. Our A-STEAL algorithm is appropriate for large parallel servers where many jobs share a common multiprocessor resource and in which the number of processors available to a particular job may vary during the job's execution. Assuming that the job scheduler never allots a job more processors than requested by the job's thread scheduler, A-STEAL guarantees that the job completes in near-optimal time while utilizing at least a constant fraction of the allotted processors. We model the job scheduler as the thread scheduler's adversary, challenging the thread scheduler to be robust to the operating environment as well as to the job scheduler's administrative policies. For example, the job scheduler might make a large number of processors available exactly when the job has little use for them. To analyze the performance of our adaptive thread scheduler under this stringent adversarial assumption, we introduce a new technique called trim analysis, which allows us to prove that our thread scheduler performs poorly on no more than a small number of time steps, exhibiting near-optimal behavior on the vast majority. More precisely, suppose that a job has work T-1 and span T-infinity. On a machine with P processors, A-STEAL completes the job in an expected duration of O(T-1/(P) over tilde + T-infinity + L lg P) time steps, where L is the length of a scheduling quantum, and (P) over tilde denotes the O(T-infinity + L lg P)-trimmed availability. This quantity is the average of the processor availability over all time steps except the O(T-infinity + L lg P) time steps that have the highest processor availability. When the job's parallelism dominates the trimmed availability, that is, (P) over tilde << T-1/T-infinity, the job achieves nearly perfect linear speedup. Conversely, when the trimmed mean dominates the parallelism, the asymptotic running time of the job is nearly the length of its span, which is optimal. We measured the performance of A-STEAL on a simulated multiprocessor system using synthetic workloads. For jobs with sufficient parallelism, our experiments confirm that A-STEAL provides almost perfect linear speedup across a variety of processor availability profiles. We compared A-STEAL with the ABP algorithm, an adaptive work-stealing thread scheduler developed by Arora et al. [1998] which does not employ parallelism feedback. On moderately to heavily loaded machines with large numbers of processors, A-STEAL typically completed jobs more than twice as quickly as ABP, despite being allotted the same number or fewer processors on every step, while wasting only 10% of the processor cycles wasted by ABP.", "paper_title": "Adaptive work-stealing with parallelism feedback", "paper_id": "WOS:000259432500002"}