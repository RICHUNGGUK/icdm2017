{"auto_keywords": [{"score": 0.025701541287455335, "phrase": "lis"}, {"score": 0.01994480604313202, "phrase": "alt"}, {"score": 0.00481495049065317, "phrase": "universal_transfer_learning"}, {"score": 0.004644329185633612, "phrase": "new_learning_tasks"}, {"score": 0.004495922204117959, "phrase": "related_tasks"}, {"score": 0.0044635893862404385, "phrase": "existing_transfer_learning_methods"}, {"score": 0.004336556638639097, "phrase": "pac_analysis"}, {"score": 0.004182816319761868, "phrase": "key_notion"}, {"score": 0.003794297119656863, "phrase": "ill_the_context"}, {"score": 0.0032134045046357876, "phrase": "solomonoff_induction"}, {"score": 0.0031558679589831287, "phrase": "inductive_inference"}, {"score": 0.003110579040263398, "phrase": "universal_measures"}, {"score": 0.0029785631356166594, "phrase": "universally_optimal_bayesian_transfer_learning_methods"}, {"score": 0.0026532001044943117, "phrase": "kolmogorov_complexity"}, {"score": 0.002634084319025334, "phrase": "probability_measures"}, {"score": 0.0025682543875086934, "phrase": "simple_practical_approximation"}, {"score": 0.00251313643823856, "phrase": "transfer_learning"}, {"score": 0.0022958971655679832, "phrase": "experimental_feat"}, {"score": 0.002151183757599893, "phrase": "practical_transfer_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Transfer learning", " Kolmogorov complexity", " Bayesian machine learning", " Universal prediction"], "paper_abstract": "In transfer learning the aim is to solve new learning tasks using fewer examples by using information gained from solving related tasks. Existing transfer learning methods have been used successfully in practice and PAC analysis of these methods have been developed. But the key notion of relatedness between tasks has not yet been defined clearly, which makes it difficult to understand, let alone answer, questions that naturally arise ill the context of transfer, such as, how much information to transfer, whether to transfer information, and how to transfer information across tasks. In this paper, we look at transfer learning from the perspective of Algorithmic Information Theory/Kolmogorov complexity theory, and formally solve these problems in the same sense Solomonoff Induction solves the problem of inductive inference. We define universal measures of relatedness between tasks, and use these measures to develop universally optimal Bayesian transfer learning methods. We also derive results in AlT that are interesting by themselves. To address a concern that arises from the theory, we also briefly look at the notion of Kolmogorov complexity of probability measures. Finally, we present a simple practical approximation to the theory to do transfer learning and show that even these are quite effective, allowing LIS to transfer across tasks that are superficially unrelated. The latter is an experimental feat which has not been achieved before, and thus shows the theory is also useful in constructing practical transfer algorithms. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "On universal transfer learning", "paper_id": "WOS:000265470900007"}