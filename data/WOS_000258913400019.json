{"auto_keywords": [{"score": 0.03464034545023033, "phrase": "bayes"}, {"score": 0.00481495049065317, "phrase": "statistical_learning"}, {"score": 0.004593787100061613, "phrase": "new_method"}, {"score": 0.004502126088469323, "phrase": "binary_classification_problem"}, {"score": 0.004295269532032448, "phrase": "empirical_minimization"}, {"score": 0.004209539728590977, "phrase": "hinge_risk"}, {"score": 0.004125513927068713, "phrase": "increasing_sequence"}, {"score": 0.00407042677958844, "phrase": "finite-dimensional_spaces"}, {"score": 0.00398916666053909, "phrase": "suitable_dimension"}, {"score": 0.0038314621564116192, "phrase": "regularized_risk"}, {"score": 0.0037297918570822876, "phrase": "regularization_term"}, {"score": 0.0035344448639851827, "phrase": "oracle-type_inequality"}, {"score": 0.003417565253539315, "phrase": "excess_generalization_risk"}, {"score": 0.003110396267535251, "phrase": "adequate_convergence_properties"}, {"score": 0.002907992115184249, "phrase": "considered_sequence"}, {"score": 0.002811770047652563, "phrase": "kernel_principal_components_analysis"}, {"score": 0.002662982819305597, "phrase": "svm."}, {"score": 0.002628747284714371, "phrase": "asymptotical_convergence_rate"}, {"score": 0.0024084679931209514, "phrase": "support_vector_machine"}, {"score": 0.0023287359930308864, "phrase": "exemplary_experiments"}, {"score": 0.0022668507535947976, "phrase": "benchmark_data_sets"}, {"score": 0.0022215160785078797, "phrase": "practical_results"}], "paper_keywords": ["classification", " dimension reduction", " kernel principal component analysis (KPCA)", " regularization", " statistical learning", " support vector machine (SVM)"], "paper_abstract": "In this paper, a new method for the binary classification problem is studied. It relies on empirical minimization of the hinge risk over an increasing sequence of finite-dimensional spaces. A suitable dimension is picked by minimizing the regularized risk, where the regularization term is proportional to the dimension. An oracle-type inequality is established for the excess generalization risk (i.e., regret to Bayes) of the procedure, which ensures adequate convergence properties of the method. We suggest to select the considered sequence of subspaces by applying kernel principal components analysis (KPCA). In this case, the asymptotical convergence rate of the method can be better than what is known for the support vector machine (SVM). Exemplary experiments are presented on benchmark data sets where the practical results of the method are comparable to the SVM.", "paper_title": "Finite-dimensional projection for classification and statistical learning", "paper_id": "WOS:000258913400019"}