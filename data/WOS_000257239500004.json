{"auto_keywords": [{"score": 0.048464010079971774, "phrase": "o.l."}, {"score": 0.022584462516233608, "phrase": "mangasarian"}, {"score": 0.00481495049065317, "phrase": "massive_nonlinear_kernel_classification"}, {"score": 0.004733073726115013, "phrase": "bradley"}, {"score": 0.004547356716604537, "phrase": "linear_support_vector_machines"}, {"score": 0.00425791449539253, "phrase": "thompson"}, {"score": 0.0041378132174467105, "phrase": "unconstrained_support_vector_machines"}, {"score": 0.00409073644076669, "phrase": "optimization_theory"}, {"score": 0.00390770398783664, "phrase": "linear_classifiers"}, {"score": 0.003852196813189941, "phrase": "nonlinear_kernel_classification"}, {"score": 0.0038302144900165284, "phrase": "massive_datasets"}, {"score": 0.0037435277798679722, "phrase": "nonlinear_support_vector_machines"}, {"score": 0.003700920201232327, "phrase": "linear_programming_formulation"}, {"score": 0.003545400775645216, "phrase": "a._smola"}, {"score": 0.003525162969696677, "phrase": "p._bartlett"}, {"score": 0.003505040278555349, "phrase": "b._scholkopf"}, {"score": 0.0034850320520618872, "phrase": "d._schournians"}, {"score": 0.003465156539210313, "phrase": "eds"}, {"score": 0.00342568771304873, "phrase": "large_margin_classifiers"}, {"score": 0.003406145775676857, "phrase": "cambridge"}, {"score": 0.0033673504953700417, "phrase": "mit_press"}, {"score": 0.003179950894323199, "phrase": "completely_unconstrained_minimization_problem"}, {"score": 0.0030549925653548826, "phrase": "unconstrained_convex_differentiable_minimization"}, {"score": 0.0030375455671507374, "phrase": "technical_report"}, {"score": 0.0029943614447192775, "phrase": "data_mining_institute"}, {"score": 0.0029772596754941076, "phrase": "computer_sciences_department"}, {"score": 0.0029602552900111407, "phrase": "university_of_wisconsin"}, {"score": 0.0028520627346893834, "phrase": "machine_learning_research"}, {"score": 0.002724300844023273, "phrase": "chunking_leads"}, {"score": 0.002700988793510088, "phrase": "simple_and_accurate_method"}, {"score": 0.0026778756903447073, "phrase": "nonlinear_classifiers"}, {"score": 0.0026247093410046445, "phrase": "machine_capacity"}, {"score": 0.002550578610483457, "phrase": "incline_village"}, {"score": 0.0025360059886239865, "phrase": "nevada"}, {"score": 0.0024223669182785157, "phrase": "proposed_method"}, {"score": 0.002287441214929297, "phrase": "chicago"}, {"score": 0.002135383131913666, "phrase": "kernel_functions"}, {"score": 0.0021049977753042253, "phrase": "simplified_nonlinear_classifier"}], "paper_keywords": ["classification", " nonlinear kernel", " massive datasets", " linear programming", " dual penalty"], "paper_abstract": "A chunking procedure [Bradley, P.S. and Mangasarian, O.L., 2000, Massive data discrimination via linear Support vector machines. Optimization Methods and Software, 13, 1-10. Available online at: ftp://ftp.es.wise.edu/mathprog/tech-reports/98-05.ps] utilized in [Mangasarian, O.L. and Thompson, M.E., 2006, Massive data classification via unconstrained support vector machines. Journal of Optimization Theory and Applications, 131, 315-325. Available online at: ftp://ftp.cs.wise.edu/pub/dmi/techreports/06-01.pdf for linear classifiers is proposed here for nonlinear kernel classification of massive datasets. A highly accurate algorithm based on nonlinear support vector machines that utilize a linear programming formulation [Mangasarian, O.L., 2000, Generalized support vector machines. In: A. Smola, P. Bartlett, B. Scholkopf and D. Schournians (Eds) Advances in Large Margin Classifiers (Cambridge, MA: MIT Press), pp. 135-146. Available online at: ftp://ftp.cs.wise.edu/math-prog/tech-reports/98-14.ps] is developed here as a Completely unconstrained minimization problem [Mangasarian, O.L., 2005, Exact 1-Norm Support vector machines via unconstrained convex differentiable minimization. Technical Report 05-03, Data Mining Institute, Computer Sciences Department, University of Wisconsin, Madison, Wisconsin. Available online at: ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/05-03.ps. Journal (of Machine Learning Research, 7, 1517-1530, 2006.]. This approach together with chunking leads to a simple and accurate method for generating nonlinear classifiers for a 250,000-point dataset that typically exceeds machine capacity when standard linear programming methods such as CPLEX [ILOG, 2003, ILOG CPLEX 9.0 User's Manual, Incline Village, Nevada. Available online at: http://www.ilog.com/products/cplex/] are used. Because a 1-norm Support vector machine underlies the proposed method, the approach together with a reduced Support vector machine formulation [Lee, Y.-J. and Mangasarian, O.L., 2001, RSVM: reduce(] Support vector machines. Proceedings of the First SIAM International Conference on Data Mining, Chicago, 5-7 April, CD-ROM. Available online at: ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-07.ps] minimizes the number of kernel functions utilized to generate a simplified nonlinear classifier.", "paper_title": "Chunking for massive nonlinear kernel classification", "paper_id": "WOS:000257239500004"}