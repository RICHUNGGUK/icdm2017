{"auto_keywords": [{"score": 0.046206033143893974, "phrase": "relevance_feedback"}, {"score": 0.020517400532134567, "phrase": "semantic_web"}, {"score": 0.009079318268581089, "phrase": "hypertext_web"}, {"score": 0.00481495049065317, "phrase": "hypertext_and_semantic_web_search"}, {"score": 0.004598933498016774, "phrase": "semantic_web_data"}, {"score": 0.0045464538515249085, "phrase": "hypertext_web_search"}, {"score": 0.004359078118356132, "phrase": "'virtuous_cycle"}, {"score": 0.004211504818267892, "phrase": "linked_data"}, {"score": 0.004084510791138056, "phrase": "previous_approaches"}, {"score": 0.0037546039916845947, "phrase": "different_domains"}, {"score": 0.0036553070575015344, "phrase": "information_retrieval_performance"}, {"score": 0.0035045293923775256, "phrase": "single_data-set"}, {"score": 0.003398780792129312, "phrase": "hypertext_web_results"}, {"score": 0.0033599501775954024, "phrase": "semantic_web_search"}, {"score": 0.003208999546225013, "phrase": "hypertext_web_data"}, {"score": 0.0030414140732789186, "phrase": "informational_queries"}, {"score": 0.0028936226331006563, "phrase": "real-life_query_log"}, {"score": 0.0028496055358577512, "phrase": "human_judges"}, {"score": 0.0027741761880779535, "phrase": "wide_range"}, {"score": 0.0026698611202305694, "phrase": "baseline_performance"}, {"score": 0.0026191799769765063, "phrase": "deployed_systems"}, {"score": 0.00254983442025512, "phrase": "semantic_web_search_engine_falcon-s"}, {"score": 0.0025303682040656375, "phrase": "yahoo"}, {"score": 0.00241658945942583, "phrase": "semantic_web_inference"}, {"score": 0.0023435920614377306, "phrase": "pseudo-relevance_feedback"}, {"score": 0.0022382002476510573, "phrase": "actual_relevance_feedback"}, {"score": 0.0021789194955896102, "phrase": "first_rigorous_'cranfield'_evaluation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Semantic search", " Query logs", " Information retrieval", " Relevance feedback", " Evaluation", " Linked Data"], "paper_abstract": "We investigate the possibility of using Semantic Web data to improve hypertext Web search. In particular, we use relevance feedback to create a 'virtuous cycle' between data gathered from the Semantic Web of Linked Data and web-pages gathered from the hypertext Web. Previous approaches have generally considered the searching over the Semantic Web and hypertext Web to be entirely disparate, indexing, and searching over different domains. While relevance feedback has traditionally improved information retrieval performance, relevance feedback is normally used to improve rankings over a single data-set. Our novel approach is to use relevance feedback from hypertext Web results to improve Semantic Web search, and results from the Semantic Web to improve the retrieval of hypertext Web data. In both cases, an evaluation is performed based on certain kinds of informational queries (abstract concepts, people, and places) selected from a real-life query log and checked by human judges. We evaluate our work over a wide range of algorithms and options, and show it improves baseline performance on these queries for deployed systems as well, such as the Semantic Web Search engine FALCON-S and Yahoo! Web search. We further show that the use of Semantic Web inference seems to hurt performance, while the pseudo-relevance feedback increases performance in both cases, although not as much as actual relevance feedback. Lastly, our evaluation is the first rigorous 'Cranfield' evaluation of Semantic Web search. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Relevance feedback between hypertext and Semantic Web search: Frameworks and evaluation", "paper_id": "WOS:000300170600008"}