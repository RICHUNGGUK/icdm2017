{"auto_keywords": [{"score": 0.04883313245975001, "phrase": "adaptive_quantization"}, {"score": 0.04822150491313864, "phrase": "fundamental_frequency"}, {"score": 0.03421787969149637, "phrase": "training_data"}, {"score": 0.029588915133989056, "phrase": "input_speech"}, {"score": 0.004815094223002128, "phrase": "hmm"}, {"score": 0.004507760907425495, "phrase": "speaker-independent_hmm-based_voice_conversion_technique"}, {"score": 0.004128317208406523, "phrase": "hmm-based_conversion"}, {"score": 0.0040385352882334235, "phrase": "input_utterance"}, {"score": 0.003985602342418578, "phrase": "source_speaker"}, {"score": 0.003916098498242888, "phrase": "phonetic_and_prosodic_symbol_sequences"}, {"score": 0.003847802030433298, "phrase": "converted_speech"}, {"score": 0.00376409764683147, "phrase": "decoded_information"}, {"score": 0.003714748329627737, "phrase": "pre-trained_target_speaker's_phonetically"}, {"score": 0.0033277936960179892, "phrase": "global_mean"}, {"score": 0.0031845067058084583, "phrase": "current_study"}, {"score": 0.003142732162296465, "phrase": "statistical_parameters"}, {"score": 0.003007389443770562, "phrase": "adaptive_method"}, {"score": 0.00284009509859163, "phrase": "speaker-independent_model"}, {"score": 0.0027660443371695024, "phrase": "model_adaptation"}, {"score": 0.002717750004930131, "phrase": "target_speaker's_model"}, {"score": 0.002646880721119838, "phrase": "required_amount"}, {"score": 0.002555247330102505, "phrase": "phonetic_transcription"}, {"score": 0.0024776677004255104, "phrase": "objective_and_subjective_experimental_results"}, {"score": 0.00245597227496565, "phrase": "japanese"}, {"score": 0.0024024377855956136, "phrase": "adaptive_quantization_method"}, {"score": 0.0023604768913655463, "phrase": "conversion_performance"}, {"score": 0.0023294867572824147, "phrase": "conventional_one"}, {"score": 0.002238930800075706, "phrase": "target_speaker's_adaptation_data"}, {"score": 0.0022095328812245852, "phrase": "conventional_gmm-based_one"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Voice conversion", " Hidden Markov model (HMM)", " HMM-based speech synthesis", " Speaker-independent model", " Fundamental frequency quantization", " Prosody conversion"], "paper_abstract": "This paper describes a speaker-independent HMM-based voice conversion technique that incorporates context-dependent prosodic symbols obtained using adaptive quantization of the fundamental frequency (F0). In the HMM-based conversion of our previous study, the input utterance of a source speaker is decoded into phonetic and prosodic symbol sequences, and the converted speech is generated using the decoded information from the pre-trained target speaker's phonetically and prosodically context-dependent H M M. In our previous work, we generated the F0 symbol by quantizing the average log F0 value of each phone using the global mean and variance calculated from the training data. In the current study, these statistical parameters are obtained from each utterance itself, and this adaptive method improves the F0 conversion performance of the conventional one. We also introduce a speaker-independent model for decoding the input speech and model adaptation for training the target speaker's model in order to reduce the required amount of training data under a condition where the phonetic transcription is available for the input speech. Objective and subjective experimental results for Japanese speech demonstrate that the adaptive quantization method gives better F0 conversion performance than the conventional one. Moreover, our technique with only ten sentences of the target speaker's adaptation data outperforms the conventional GMM-based one using parallel data of 200 sentences. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Speaker-independent HMM-based voice conversion using adaptive quantization of the fundamental frequency", "paper_id": "WOS:000292675800002"}