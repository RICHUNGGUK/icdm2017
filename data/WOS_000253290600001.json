{"auto_keywords": [{"score": 0.04518838488438968, "phrase": "stereo_vision"}, {"score": 0.03959966252961897, "phrase": "depth_information"}, {"score": 0.00481495049065317, "phrase": "adaptive_multi-modal_stereo_people"}, {"score": 0.00461802698572959, "phrase": "monocular_images"}, {"score": 0.00457688347567955, "phrase": "important_and_difficult_problems"}, {"score": 0.004286109357236311, "phrase": "great_attentions"}, {"score": 0.004123012124092743, "phrase": "multiple-people_detection"}, {"score": 0.003977952748523497, "phrase": "multiple_particle_filtering_approach"}, {"score": 0.003648037996175166, "phrase": "disparity_map"}, {"score": 0.003604725094696445, "phrase": "novel_confidence_measure"}, {"score": 0.0035725770197245392, "phrase": "greater_the_amount"}, {"score": 0.0035513038412674763, "phrase": "disparity_information"}, {"score": 0.003509135412622647, "phrase": "higher_the_degree"}, {"score": 0.0034262895977021854, "phrase": "final_particles_weights"}, {"score": 0.0033755037617077483, "phrase": "worst_case"}, {"score": 0.0032859724984346966, "phrase": "proposed_algorithm"}, {"score": 0.0031046598804456366, "phrase": "pure_colour-based_tracking_algorithm"}, {"score": 0.0030404045763332437, "phrase": "adaboost_classifier"}, {"score": 0.003022290647143636, "phrase": "stereo_information"}, {"score": 0.0028554857218530206, "phrase": "disparity_images"}, {"score": 0.0028131366865158302, "phrase": "complex_situations"}, {"score": 0.002779708854857718, "phrase": "different_distances"}, {"score": 0.0025642012084841886, "phrase": "experimental_results"}, {"score": 0.002379548054048268, "phrase": "camera_image"}, {"score": 0.0023302648796456452, "phrase": "real_time"}, {"score": 0.0022888335017747768, "phrase": "proposed_method"}, {"score": 0.002234732712120406, "phrase": "background_model"}, {"score": 0.0021431085940929783, "phrase": "mobile_devices"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["people tracking", " stereo vision", " particle filtering", " real-time imaging", " confidence measure", " colour processing"], "paper_abstract": "Detecting and tracking persons in the sequences of monocular images are the important and difficult problems in computer vision and have been well studied in these two decades. Recently, the methods based on stereo vision have attracted great attentions since 3D information can be exploited. This paper presents an approach for multiple-people detection and tracking using stereo vision. Tracking is carried out using a multiple particle filtering approach that combines depth, colour and gradient information. We modify the degree of confidence assigned to depth information, according to the amount of it found in the disparity map, using a novel confidence measure. The greater the amount of disparity information found, the higher the degree of confidence assigned to depth information in the final particles weights is. In the worst case (total absence of disparity), the proposed algorithm makes use of the information available (colour and gradient) to track, thus performing as a pure colour-based tracking algorithm. People are detected combining an adaboost classifier with stereo information. In order to test the validity of our proposal, it is evaluated in several sequences of colour and disparity images where people interact in complex situations: walk at different distances, shake hands, cross their paths, jump, run, embrace each other and even swap their positions quickly trying to confuse the system. The experimental results show that the proposal is able to deal with occlusions and to effectively determine both the 3D position of the people being tracked and their 2D head locations in the camera image, and everything is realized in real time. Besides, as the proposed method does not require the use of a background model, it can be considered particularly appropriate for applications that must run on mobile devices. (c) 2007 Elsevier Inc. All rights reserved.", "paper_title": "Adaptive multi-modal stereo people tracking without background modelling", "paper_id": "WOS:000253290600001"}