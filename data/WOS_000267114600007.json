{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "robust_adapted_principal_component_analysis_for_face_recognition."}, {"score": 0.004734853555630616, "phrase": "uncontrolled_pose"}, {"score": 0.004578616481036476, "phrase": "challenging_task"}, {"score": 0.0042455945330942746, "phrase": "existing_techniques"}, {"score": 0.003589404014552974, "phrase": "recognition_method"}, {"score": 0.003559387846675818, "phrase": "adapted_principal_component_analysis"}, {"score": 0.0034130176595901104, "phrase": "large_variations"}, {"score": 0.0033420978105332686, "phrase": "facial_expression"}, {"score": 0.003059925849144792, "phrase": "first_step"}, {"score": 0.00299632091372281, "phrase": "active_appearance_model"}, {"score": 0.002921732652917206, "phrase": "non-frontal_face_image"}, {"score": 0.002873038567884343, "phrase": "synthesized_frontal_face_image"}, {"score": 0.002789763110395331, "phrase": "apca"}, {"score": 0.00267495669519805, "phrase": "proposed_technique"}, {"score": 0.002575665611013556, "phrase": "yale_face"}, {"score": 0.002543392353000551, "phrase": "feret_database_-"}, {"score": 0.002500987934534128, "phrase": "different_lighting_conditions"}, {"score": 0.0024800509241747013, "phrase": "facial_expressions"}, {"score": 0.0024182831453515782, "phrase": "experimental_results"}, {"score": 0.002318983332546964, "phrase": "pca"}, {"score": 0.0022993189959092646, "phrase": "fld"}, {"score": 0.0022800704712616616, "phrase": "prm"}, {"score": 0.002260971608578342, "phrase": "ltp."}, {"score": 0.00217701456832968, "phrase": "aam"}, {"score": 0.002158780936704905, "phrase": "frontal_face_synthesis"}, {"score": 0.0021407025628134586, "phrase": "high_pose_angle"}], "paper_keywords": ["Face recognition", " pose", " illumination and expression", " face subspace", " space rotation"], "paper_abstract": "Recognizing faces with uncontrolled pose, illumination, and expression is a challenging task due to the fact that features insensitive to one variation may be highly sensitive to the other variations. Existing techniques dealing with just one of these variations are very often unable to cope with the other variations. The problem is even more difficult in applications where only one gallery image per person is available. In this paper, we describe a recognition method, Adapted Principal Component Analysis (APCA), that can simultaneously deal with large variations in both illumination and facial expression using only a single gallery image per person. We have now extended this method to handle head pose variations in two steps. The first step is to apply an Active Appearance Model (AAM) to the non-frontal face image to construct a synthesized frontal face image. The second is to use APCA for classification robust to lighting and pose. The proposed technique is evaluated on three public face databases - Asian Face, Yale Face, and FERET Database - with images under different lighting conditions, facial expressions, and head poses. Experimental results show that our method performs much better than other recognition methods including PCA, FLD, PRM and LTP. More specifically, we show that by using AAM for frontal face synthesis from high pose angle faces, the recognition rate of our APCA method increases by up to a factor of 4.", "paper_title": "ROBUST ADAPTED PRINCIPAL COMPONENT ANALYSIS FOR FACE RECOGNITION", "paper_id": "WOS:000267114600007"}