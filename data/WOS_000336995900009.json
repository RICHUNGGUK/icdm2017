{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "affective_content_analysis"}, {"score": 0.04082865113596906, "phrase": "low-level_features"}, {"score": 0.029371897546719632, "phrase": "proposed_framework"}, {"score": 0.02809297839708518, "phrase": "audio_emotional_event"}, {"score": 0.004718314112245913, "phrase": "emotional_factors"}, {"score": 0.004647105813224402, "phrase": "audiences'_attention"}, {"score": 0.00446240364863009, "phrase": "video_affective_content_analysis"}, {"score": 0.004306795240631196, "phrase": "existing_methods"}, {"score": 0.004263336444795021, "phrase": "low-level_affective_features"}, {"score": 0.004135562737140424, "phrase": "machine_learning"}, {"score": 0.0040525061916685924, "phrase": "human_perception_process"}, {"score": 0.0038520615699124123, "phrase": "high-level_human_perception"}, {"score": 0.003587924960158317, "phrase": "three-level_affective_content_analysis_framework"}, {"score": 0.003034558550137019, "phrase": "mid-level_representation"}, {"score": 0.0028551716387011637, "phrase": "high-level_affective_content"}, {"score": 0.002672750634553146, "phrase": "case_studies"}, {"score": 0.0025275050635202878, "phrase": "affective_content_detection"}, {"score": 0.0024766636161181544, "phrase": "multiple_modalities"}, {"score": 0.002426842369975213, "phrase": "affective_analysis"}, {"score": 0.002390133671838151, "phrase": "different_modality"}, {"score": 0.0023066199295447686, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "mid-level_representations"}], "paper_keywords": ["Affective content analysis", " Mid-level representation", " Multiple modality"], "paper_abstract": "Emotional factors directly reflect audiences' attention, evaluation and memory. Recently, video affective content analysis attracts more and more research efforts. Most of the existing methods map low-level affective features directly to emotions by applying machine learning. Compared to human perception process, there is actually a gap between low-level features and high-level human perception of emotion. In order to bridge the gap, we propose a three-level affective content analysis framework by introducing mid-level representation to indicate dialog, audio emotional events (e. g., horror sounds and laughters) and textual concepts (e.g., informative keywords). Mid-level representation is obtained from machine learning on low-level features and used to infer high-level affective content. We further apply the proposed framework and focus on a number of case studies. Audio emotional event, dialog and subtitle are studied to assist affective content detection in different video domains/genres. Multiple modalities are considered for affective analysis, since different modality has its own merit to evoke emotions. Experimental results shows the proposed framework is effective and efficient for affective content analysis. Audio emotional event, dialog and subtitle are promising mid-level representations.", "paper_title": "A three-level framework for affective content analysis and its case studies", "paper_id": "WOS:000336995900009"}