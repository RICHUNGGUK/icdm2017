{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "support_vector_machines"}, {"score": 0.004053362743156545, "phrase": "convex_quadratic_programming"}, {"score": 0.0038713805573219297, "phrase": "randomized_algorithms"}, {"score": 0.0037834649508375544, "phrase": "training_svms"}, {"score": 0.0034911335714333507, "phrase": "random_sampling_techniques"}, {"score": 0.0031845067058084583, "phrase": "similar_problems"}, {"score": 0.0029722900280316216, "phrase": "upper_bound"}, {"score": 0.0028715299874549245, "phrase": "expected_running_time"}, {"score": 0.0025596276616952516, "phrase": "data_points"}, {"score": 0.0021049977753042253, "phrase": "chosen_soft_margin_parameter"}], "paper_keywords": ["support vector machine", " SVM training algorithm", " random sampling technique", " combinatorial dimension"], "paper_abstract": "Support Vector Machines are a family of algorithms for the analysis of data based on convex Quadratic Programming. We derive randomized algorithms for training SVMs, based on a variation of Random Sampling Techniques; these have been successfully used for similar problems. We formally prove an upper bound on the expected running time which is quasilinear with respect to the number of data points and polynomial with respect to the other parameters, i.e., the number of attributes and the inverse of a chosen soft margin parameter.", "paper_title": "Provably fast training algorithms for support vector machines", "paper_id": "WOS:000254920900007"}