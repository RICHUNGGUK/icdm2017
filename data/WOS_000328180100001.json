{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "lexical_variabilities"}, {"score": 0.037482702260810276, "phrase": "verbal_content"}, {"score": 0.004743806810821005, "phrase": "emotion_recognition"}, {"score": 0.00470862827326028, "phrase": "affect_recognition"}, {"score": 0.00465634671242413, "phrase": "crucial_requirement"}, {"score": 0.004621813738791474, "phrase": "future_human_machine_interfaces"}, {"score": 0.004536592536813498, "phrase": "nonverbal_behaviors"}, {"score": 0.00445293569103488, "phrase": "speech_emotion_recognition_systems"}, {"score": 0.004338390136272005, "phrase": "speaker's_emotional_state"}, {"score": 0.004274257668843717, "phrase": "human_voice"}, {"score": 0.00389427992231721, "phrase": "emotion_recognition_system"}, {"score": 0.0038224221344037236, "phrase": "robust_emotional_models"}, {"score": 0.0037940502405699765, "phrase": "careful_considerations"}, {"score": 0.003587842947046177, "phrase": "speaker_characteristics"}, {"score": 0.00353476609156032, "phrase": "expressive_behaviors"}, {"score": 0.0034695189961509625, "phrase": "factorization_technique"}, {"score": 0.003418186564522323, "phrase": "phoneme_level_trajectory_models"}, {"score": 0.0032444113660668743, "phrase": "acoustic_features"}, {"score": 0.003220315882881027, "phrase": "communication_traits"}, {"score": 0.0029889584492584073, "phrase": "mutual_information_framework"}, {"score": 0.002944715025443487, "phrase": "uncertainty_reduction"}, {"score": 0.002911961514073769, "phrase": "trajectory_models"}, {"score": 0.0028053842615436706, "phrase": "important_insights"}, {"score": 0.002712794864739074, "phrase": "aforementioned_factors"}, {"score": 0.002613487968156893, "phrase": "feature_normalization_technique"}, {"score": 0.002574788013427524, "phrase": "whitening_transformation"}, {"score": 0.0024346858809238766, "phrase": "normalization_scheme"}, {"score": 0.002389696110283506, "phrase": "presented_factor_analysis_method"}, {"score": 0.002363101338529545, "phrase": "emotion_recognition_experiments"}, {"score": 0.0023281003994670714, "phrase": "normalization_approach"}, {"score": 0.0022428442879034614, "phrase": "speaker_identity"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Emotion recognition", " Factor analysis", " Feature normalization", " Speaker variability"], "paper_abstract": "Affect recognition is a crucial requirement for future human machine interfaces to effectively respond to nonverbal behaviors of the user. Speech emotion recognition systems analyze acoustic features to deduce the speaker's emotional state. However, human voice conveys a mixture of information including speaker, lexical, cultural, physiological and emotional traits. The presence of these communication aspects introduces variabilities that affect the performance of an emotion recognition system. Therefore, building robust emotional models requires careful considerations to compensate for the effect of these variabilities. This study aims to factorize speaker characteristics, verbal content and expressive behaviors in various acoustic features. The factorization technique consists in building phoneme level trajectory models for the features. We propose a metric to quantify the dependency between acoustic features and communication traits (i.e., speaker, lexical and emotional factors). This metric, which is motivated by the mutual information framework, estimates the uncertainty reduction in the trajectory models when a given trait is considered. The analysis provides important insights on the dependency between the features and the aforementioned factors. Motivated by these results, we propose a feature normalization technique based on the whitening transformation that aims to compensate for speaker and lexical variabilities. The benefit of employing this normalization scheme is validated with the presented factor analysis method. The emotion recognition experiments show that the normalization approach can attenuate the variability imposed by the verbal content and speaker identity, yielding 4.1% and 2.4% relative performance improvements on a selected set of features, respectively. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Compensating for speaker or lexical variabilities in speech for emotion recognition", "paper_id": "WOS:000328180100001"}