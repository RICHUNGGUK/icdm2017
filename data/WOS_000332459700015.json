{"auto_keywords": [{"score": 0.03973129782260974, "phrase": "target_people"}, {"score": 0.00481495049065317, "phrase": "nir_face"}, {"score": 0.004683540676363745, "phrase": "visual_versus"}, {"score": 0.004466519142879166, "phrase": "conventional_vis_face"}, {"score": 0.004293336465549723, "phrase": "nir_face_technology"}, {"score": 0.0042426974826585695, "phrase": "illumination_changes"}, {"score": 0.004209269012699911, "phrase": "low-light_condition"}, {"score": 0.004014135205423942, "phrase": "vis_face_images"}, {"score": 0.003966776222128288, "phrase": "id_card_photos"}, {"score": 0.003935513160201076, "phrase": "existing_vis-nir_techniques"}, {"score": 0.0038737214010638745, "phrase": "classifier_learning"}, {"score": 0.003828012660311739, "phrase": "vis_images"}, {"score": 0.003664963414801933, "phrase": "vis-nir_image_pairs"}, {"score": 0.003241750934419473, "phrase": "transductive_method"}, {"score": 0.0032161835102640372, "phrase": "transductive_heterogeneous_face_matching"}, {"score": 0.003054825135908205, "phrase": "available_image_pairs"}, {"score": 0.0028786467218197245, "phrase": "simple_feature_representation"}, {"score": 0.002855934865505357, "phrase": "effective_vis-nir_matching"}, {"score": 0.0027559226203857316, "phrase": "namely_log-dog"}, {"score": 0.0025969550560855873, "phrase": "vis"}, {"score": 0.0025767060974958116, "phrase": "nir"}, {"score": 0.0025359354516609795, "phrase": "transduction_approach"}, {"score": 0.0024960634897058394, "phrase": "domain_difference"}, {"score": 0.00246657044320953, "phrase": "heterogeneous_data"}, {"score": 0.0024277864377045995, "phrase": "discriminative_model"}, {"score": 0.0022967713922705, "phrase": "first_attempt"}, {"score": 0.0022606512324475584, "phrase": "vis-nir_matching"}, {"score": 0.002207530926563892, "phrase": "generalization_problem"}, {"score": 0.002172811149894415, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "heterogeneous_face"}], "paper_keywords": ["Heterogeneous face recognition", " VIS-NIR face matching", " transductive learning"], "paper_abstract": "Visual versus near infrared (VIS-NIR) face image matching uses an NIR face image as the probe and conventional VIS face images as enrollment. It takes advantage of the NIR face technology in tackling illumination changes and low-light condition and can cater for more applications where the enrollment is done using VIS face images such as ID card photos. Existing VIS-NIR techniques assume that during classifier learning, the VIS images of each target people have their NIR counterparts. However, since corresponding VIS-NIR image pairs of the same people are not always available, which is often the case, so those methods cannot be applied. To address this problem, we propose a transductive method named transductive heterogeneous face matching (THFM) to adapt the VIS-NIR matching learned from training with available image pairs to all people in the target set. In addition, we propose a simple feature representation for effective VIS-NIR matching, which can be computed in three steps, namely Log-DoG filtering, local encoding, and uniform feature normalization, to reduce heterogeneities between VIS and NIR images. The transduction approach can reduce the domain difference due to heterogeneous data and learn the discriminative model for target people simultaneously. To the best of our knowledge, it is the first attempt to formulate the VIS-NIR matching using transduction to address the generalization problem for matching. Experimental results validate the effectiveness of our proposed method on the heterogeneous face biometric databases.", "paper_title": "Matching NIR Face to VIS Face Using Transduction", "paper_id": "WOS:000332459700015"}