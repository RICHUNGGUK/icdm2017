{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cardinality_estimators"}, {"score": 0.004627441072541506, "phrase": "cardinality_estimation_algorithms"}, {"score": 0.004273951631010383, "phrase": "arbitrary_order"}, {"score": 0.004173244935967375, "phrase": "possible_repetitions"}, {"score": 0.003947358102140903, "phrase": "distinct_elements"}, {"score": 0.0037040785566765954, "phrase": "required_storage"}, {"score": 0.0032614317990293695, "phrase": "cardinality_estimation_algorithm"}, {"score": 0.0031592690527974285, "phrase": "extreme_order_statistics"}, {"score": 0.002988101395789399, "phrase": "weighted_version"}, {"score": 0.0026099289298770023, "phrase": "total_sum"}, {"score": 0.002508073621696602, "phrase": "proposed_unified_scheme"}, {"score": 0.0024488738006005133, "phrase": "unweighted_estimator"}, {"score": 0.002208035721125305, "phrase": "beta_distribution"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Algorithms", " Statistical", " Big data processing"], "paper_abstract": "Cardinality estimation algorithms receive a stream of elements that may appear in arbitrary order, with possible repetitions, and return the number of distinct elements. Such algorithms usually seek to minimize the required storage at the price of inaccuracy in their output. This paper shows how to generalize every cardinality estimation algorithm that relies on extreme order statistics (min/max sketches) to a weighted version, where each item is associated with a weight and the goal is to estimate the total sum of weights. The proposed unified scheme uses the unweighted estimator as a black-box, and manipulates the input using properties of the beta distribution. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A unified scheme for generalizing cardinality estimators to sum aggregation", "paper_id": "WOS:000346225300053"}