{"auto_keywords": [{"score": 0.004747429622531595, "phrase": "earlier_studies"}, {"score": 0.004455061431809975, "phrase": "different_analysis-frame_lengths"}, {"score": 0.004300453431933622, "phrase": "multiple_modalities"}, {"score": 0.004007083686875716, "phrase": "different_temporal_lengths"}, {"score": 0.0038679618585498597, "phrase": "underlying_characteristics"}, {"score": 0.003760137090849196, "phrase": "different_emotions"}, {"score": 0.0034057975804778293, "phrase": "emotion_classifiers"}, {"score": 0.003334309053314909, "phrase": "novel_fusion_method"}, {"score": 0.003150900886669123, "phrase": "individual_classifiers"}, {"score": 0.0030414140732789186, "phrase": "multi-dimensional_inputs"}, {"score": 0.002998688867384969, "phrase": "multiple_temporal_lengths"}, {"score": 0.0028944757265206332, "phrase": "iemocap_database"}, {"score": 0.002833689453299307, "phrase": "audiovisual_information"}, {"score": 0.0027546167254869493, "phrase": "dyadic_interaction_settings"}, {"score": 0.0026967596578196325, "phrase": "classification_task"}, {"score": 0.0023408288327263294, "phrase": "multimodal-multitemporal_approach"}, {"score": 0.00229164285862567, "phrase": "significant_improvements"}, {"score": 0.0021049977753042253, "phrase": "unimodal-unitemporal_classifiers"}], "paper_keywords": ["Audio-visual emotion recognition", " classifier fusion", " speech analysis"], "paper_abstract": "Earlier studies have shown that certain emotional characteristics are best observed at different analysis-frame lengths. When features of multiple modalities are extracted, it is reasonable to believe that different temporal lengths would better model the underlying characteristics that result from different emotions. In this study, we examine the use of such differing timescales in constructing emotion classifiers. A novel fusion method is introduced that utilizes the outputs of individual classifiers that are trained using multi-dimensional inputs with multiple temporal lengths. We used the IEMOCAP database which contains audiovisual information of 10 subjects in dyadic interaction settings. The classification task was performed over three emotional dimensions: valence, activation, and dominance. The results demonstrate the utility of the multimodal-multitemporal approach. Statistically significant improvements in accuracy are seen for in all three dimensions when compared with unimodal-unitemporal classifiers.", "paper_title": "Multimodal Affect Classification at Various Temporal Lengths", "paper_id": "WOS:000366027900005"}