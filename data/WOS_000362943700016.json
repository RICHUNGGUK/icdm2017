{"auto_keywords": [{"score": 0.038914316772824434, "phrase": "optimal_subspace"}, {"score": 0.00481495049065317, "phrase": "outlier_suppressing_discriminant_analysis"}, {"score": 0.004723847090183984, "phrase": "data_acquirement_technologies"}, {"score": 0.004132411680987228, "phrase": "real_performance"}, {"score": 0.004034841146538891, "phrase": "data_mining"}, {"score": 0.003996458454723708, "phrase": "pattern_analysis"}, {"score": 0.003580355398511932, "phrase": "importance-sampling-inspired_method"}, {"score": 0.0034790978593553794, "phrase": "feature_extraction"}, {"score": 0.0032382660477699695, "phrase": "graph_laplacian"}, {"score": 0.0031466527290980686, "phrase": "approximated_mean"}, {"score": 0.002928765962664163, "phrase": "scatter_metrics"}, {"score": 0.002859533151305368, "phrase": "maximum_margins"}, {"score": 0.00283229915062791, "phrase": "superior_classification_performance"}, {"score": 0.002791932344920906, "phrase": "supervised_information"}, {"score": 0.00276534038425712, "phrase": "local_data_structure"}, {"score": 0.002739001004969201, "phrase": "respective_contributions"}, {"score": 0.002648768009464535, "phrase": "linear_criterion"}, {"score": 0.0025737890313763407, "phrase": "nonlinear_case"}, {"score": 0.002537097217899195, "phrase": "kernel_trick"}, {"score": 0.002500927169609655, "phrase": "regularization_framework"}, {"score": 0.0024185182010749273, "phrase": "rank-deficient_problem"}, {"score": 0.002327648608736437, "phrase": "small_sample_size"}, {"score": 0.0022725931637071852, "phrase": "competitive_performance"}, {"score": 0.0021976915360493628, "phrase": "extensive_experiments"}, {"score": 0.0021560018573288666, "phrase": "synthetic_and_benchmark_data"}, {"score": 0.0021252533072727707, "phrase": "facial_images"}, {"score": 0.0021049977753042253, "phrase": "gene_micro-array_data"}], "paper_keywords": ["Discriminant analysis", " sample weighting", " importance sampling", " feature extraction", " regularization"], "paper_abstract": "As the data acquirement technologies develop rapidly, both the amount and types of data become larger and larger. However, noise and outliers usually attach to the data and then affect the real performance of leaning algorithms in data mining and pattern analysis. To address this problem, the importance of the sample itself in building the optimal subspace is explored, and then an importance-sampling-inspired method is proposed for outlier suppressing feature extraction. First, we assign each sample a weight, which is estimated by graph Laplacian, and then calculate the approximated mean for each subject. By highlighting the most subject-oriented samples, the weighted average and the scatter metrics can be measured with maximum margins and superior classification performance. The supervised information integrates local data structure with respective contributions to building the optimal subspace. The linear criterion can be extended to a nonlinear case by the kernel trick. A regularization framework is proposed to deal with the rank-deficient problem, which is usually induced by the small sample size of training set. Competitive performance of our algorithm has been validated by extensive experiments performed on the synthetic and benchmark data, including facial images and gene micro-array data.", "paper_title": "Sample Weighting: An Inherent Approach for Outlier Suppressing Discriminant Analysis", "paper_id": "WOS:000362943700016"}