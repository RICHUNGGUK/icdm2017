{"auto_keywords": [{"score": 0.04469413353991945, "phrase": "generalization_performance"}, {"score": 0.04232836278723483, "phrase": "regularization_algorithms"}, {"score": 0.012632840038816947, "phrase": "hypothesis_space"}, {"score": 0.012214314760607288, "phrase": "learning_algorithm"}, {"score": 0.010203146103038739, "phrase": "data_quality"}, {"score": 0.00481495049065317, "phrase": "generalization_bounds"}, {"score": 0.004760765880314089, "phrase": "regularization_algorithms_derived_simultaneously_through"}, {"score": 0.004398094237005956, "phrase": "machine_learning_research"}, {"score": 0.004203343568960231, "phrase": "learning_machine"}, {"score": 0.0037959868107895053, "phrase": "stability_property"}, {"score": 0.003627800727103479, "phrase": "practical_applications"}, {"score": 0.0033511469200347907, "phrase": "unitary_factor"}, {"score": 0.0027480831196871093, "phrase": "hypothesis_space_complexity"}, {"score": 0.0027170943110836425, "phrase": "algorithmic_stability"}, {"score": 0.0026262033625248445, "phrase": "new_bounds"}, {"score": 0.0025819015229390663, "phrase": "learning_rate"}, {"score": 0.0024673738215365104, "phrase": "uniform_stability"}, {"score": 0.0023848164282879885, "phrase": "general_type"}, {"score": 0.0023579143175099324, "phrase": "loss_functions"}, {"score": 0.002279011030331704, "phrase": "generic_results"}, {"score": 0.002215274527909304, "phrase": "learning_rates"}, {"score": 0.002190280748985751, "phrase": "support_vector_machines"}, {"score": 0.0021655683497137234, "phrase": "regularization_networks"}, {"score": 0.0021049977753042253, "phrase": "new_strategy"}], "paper_keywords": ["Learning rate", " regularization algorithm", " algorithmic stability", " hypothesis space", " sample error", " regularization error"], "paper_abstract": "A main issue in machine learning research is to analyze the generalization performance of a learning machine. Most classical results on the generalization performance of regularization algorithms are derived merely with the complexity of hypothesis space or the stability property of a learning algorithm. However, in practical applications, the performance of a learning algorithm is not actually affected only by an unitary factor just like the complexity of hypothesis space, stability of the algorithm and data quality. Therefore, in this paper, we develop a framework of evaluating the generalization performance of regularization algorithms combinatively in terms of hypothesis space complexity, algorithmic stability and data quality. We establish new bounds on the learning rate of regularization algorithms based on the measure of uniform stability and empirical covering number for general type of loss functions. As applications of the generic results, we evaluate the learning rates of support vector machines and regularization networks, and propose a new strategy for regularization parameter setting.", "paper_title": "GENERALIZATION BOUNDS OF REGULARIZATION ALGORITHMS DERIVED SIMULTANEOUSLY THROUGH HYPOTHESIS SPACE COMPLEXITY, ALGORITHMIC STABILITY AND DATA QUALITY", "paper_id": "WOS:000292842200002"}