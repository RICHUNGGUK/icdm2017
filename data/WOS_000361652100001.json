{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "crowd_labeling"}, {"score": 0.04215833196561754, "phrase": "true_labels"}, {"score": 0.004571478018964363, "phrase": "research_projects"}, {"score": 0.004535124351828787, "phrase": "human_computation"}, {"score": 0.004340263267055143, "phrase": "common_type"}, {"score": 0.0035544816252832375, "phrase": "multiple_labels"}, {"score": 0.0034016119212097826, "phrase": "true_label"}, {"score": 0.0033077610063168093, "phrase": "high-quality_labels"}, {"score": 0.002922207118572208, "phrase": "acquired_labels"}, {"score": 0.00288736193515505, "phrase": "paper_surveys_methods"}, {"score": 0.0028529310661977577, "phrase": "redundant_crowd_labels"}, {"score": 0.0027964537059186893, "phrase": "unknown_true_labels"}, {"score": 0.002741091312632307, "phrase": "unified_statistical_latent_model"}, {"score": 0.002686821995335494, "phrase": "popular_methods"}, {"score": 0.002623111480310373, "phrase": "different_choices"}, {"score": 0.0023925401023682717, "phrase": "adaptive_methods"}, {"score": 0.0023171750718791713, "phrase": "previously_collected_labels"}, {"score": 0.0022987068521717765, "phrase": "estimated_models"}, {"score": 0.0021909411820497707, "phrase": "distinguished_methods"}, {"score": 0.002147540520631409, "phrase": "future_work"}, {"score": 0.0021049977753042253, "phrase": "current_open_issues"}], "paper_keywords": ["Crowdsourcing", " Human computation", " Mechanical turk", " Labeling", " Latent model", " Inference"], "paper_abstract": "Recently, there has been a burst in the number of research projects on human computation via crowdsourcing. Multiple-choice (or labeling) questions could be referred to as a common type of problem which is solved by this approach. As an application, crowd labeling is applied to find true labels for large machine learning datasets. Since crowds are not necessarily experts, the labels they provide are rather noisy and erroneous. This challenge is usually resolved by collecting multiple labels for each sample and then aggregating them to estimate the true label. Although the mechanism leads to high-quality labels, it is not actually cost-effective. As a result, efforts are currently made to maximize the accuracy in estimating true labels, while fixing the number of acquired labels. This paper surveys methods to aggregate redundant crowd labels in order to estimate unknown true labels. It presents a unified statistical latent model where the differences among popular methods in the field correspond to different choices for the parameters of the model. Afterward, algorithms to make inference on these models will be surveyed. Moreover, adaptive methods which iteratively collect labels based on the previously collected labels and estimated models will be discussed. In addition, this paper compares the distinguished methods and provides guidelines for future work required to address the current open issues.", "paper_title": "A unified statistical framework for crowd labeling", "paper_id": "WOS:000361652100001"}