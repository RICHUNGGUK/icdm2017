{"auto_keywords": [{"score": 0.04181799126751865, "phrase": "proposed_method"}, {"score": 0.034480259906314134, "phrase": "gp"}, {"score": 0.011127075258444974, "phrase": "maxq_method"}, {"score": 0.00481495049065317, "phrase": "hierarchical_learning_structures"}, {"score": 0.004777392826700429, "phrase": "hierarchical_reinforcement_learning"}, {"score": 0.004630055463858585, "phrase": "standard_rl_algorithms"}, {"score": 0.004504850679320975, "phrase": "hierarchical_rl_algorithms"}, {"score": 0.004365881789903319, "phrase": "task_decomposition"}, {"score": 0.004214637810698362, "phrase": "human_designer"}, {"score": 0.0041328796505374155, "phrase": "lamarckian_evolutionary_approach"}, {"score": 0.004100620365805411, "phrase": "automatic_development"}, {"score": 0.004052701031142039, "phrase": "learning_structure"}, {"score": 0.004021065055449586, "phrase": "hierarchical_rl."}, {"score": 0.003943047070755799, "phrase": "maxq_hierarchical_rl_method"}, {"score": 0.0037915057447460133, "phrase": "maxq_framework"}, {"score": 0.0035332058983966424, "phrase": "learned_policies"}, {"score": 0.0033053807885373905, "phrase": "task_hierarchies"}, {"score": 0.0031782684007251403, "phrase": "appropriate_hierarchies"}, {"score": 0.0029500270981001058, "phrase": "simulation_experiments"}, {"score": 0.002915514089373717, "phrase": "foraging_task"}, {"score": 0.0028365413717574544, "phrase": "strong_interconnection"}, {"score": 0.0028033523242224833, "phrase": "obtained_learning_structures"}, {"score": 0.0027705505273051007, "phrase": "'given_task_environments"}, {"score": 0.002738131487658495, "phrase": "main_conclusion"}, {"score": 0.002622465142275264, "phrase": "minimal_strategy"}, {"score": 0.002511672561475743, "phrase": "primitive_subtasks"}, {"score": 0.0023961265476627166, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "lamarckian_mechanisms"}], "paper_keywords": ["autonomous development", " genetic programming (GP)", " hierarchical reinforcement learning (RL)", " Lamarckian evolution"], "paper_abstract": "Hierarchical reinforcement learning (RL) algorithms can learn a policy faster than standard RL algorithms. However, the applicability of hierarchical RL algorithms is limited by the fact that the task decomposition has to be performed in advance by the human designer. We propose a Lamarckian evolutionary approach for automatic development of the learning structure in hierarchical RL. The proposed method combines the MAXQ hierarchical RL method and genetic programming (GP). In the MAXQ framework, a subtask can optimize the policy independently of its parent task's policy, which makes it possible to reuse learned policies of the subtasks. In the proposed method, the MAXQ method learns the policy based on the task hierarchies obtained by GP, while the GP explores the appropriate hierarchies using the result of the MAXQ method. To show the validity of the proposed method, we have performed simulation experiments for a foraging task in three different environmental settings. The results show strong interconnection between the obtained learning structures and the 'given task environments. The main conclusion of the experiments is that the GP can find a minimal strategy, i.e., a hierarchy that minimizes the number of primitive subtasks that can be executed for each type of situation. The experimental results for the most challenging environment also show that the policies of the subtasks can continue to improve, even after the structure of the hierarchy has been evolutionary stabilized, as an effect of Lamarckian mechanisms.", "paper_title": "Evolutionary development of hierarchical learning structures", "paper_id": "WOS:000245518500008"}