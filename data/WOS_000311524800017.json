{"auto_keywords": [{"score": 0.03303103209185277, "phrase": "training_data"}, {"score": 0.00481495049065317, "phrase": "language_model_interpolation"}, {"score": 0.004743164872278112, "phrase": "language_models"}, {"score": 0.0045855181084455444, "phrase": "multiple_individual_component_models"}, {"score": 0.00450020161335541, "phrase": "context_independent_interpolation_weights"}, {"score": 0.004334280585565239, "phrase": "discriminative_approaches"}, {"score": 0.004174451361776433, "phrase": "particular_task"}, {"score": 0.004035630066743877, "phrase": "dependent_weighting"}, {"score": 0.00396050388346298, "phrase": "test-time_adaptation"}, {"score": 0.0038431888954930083, "phrase": "previous_word_contexts"}, {"score": 0.003800093834008562, "phrase": "discrete_history_weighting_function"}, {"score": 0.003673675864705682, "phrase": "component_model"}, {"score": 0.0034984340449789745, "phrase": "robust_weight_estimation_schemes"}, {"score": 0.003344076203780408, "phrase": "first_approach"}, {"score": 0.0032941471167321408, "phrase": "map_estimation"}, {"score": 0.0032694619052789768, "phrase": "interpolation_weights"}, {"score": 0.0032449610741097992, "phrase": "lower_order_contexts"}, {"score": 0.003148774338287351, "phrase": "second_approach"}, {"score": 0.0030901063954739375, "phrase": "robust_estimation"}, {"score": 0.003066945490483524, "phrase": "lm_interpolation_weights"}, {"score": 0.002942619810718864, "phrase": "map_adaptation"}, {"score": 0.0029095936591723645, "phrase": "normalized_perplexity"}, {"score": 0.0028021527740710508, "phrase": "standard_perplexity_criterion"}, {"score": 0.0027811441123794427, "phrase": "corpus_size"}, {"score": 0.0026986685585382347, "phrase": "weight_information"}, {"score": 0.0026483647033436674, "phrase": "test_data_hypotheses"}, {"score": 0.002560162916190598, "phrase": "dependent_lm_adaptation"}, {"score": 0.0023566987138026285, "phrase": "proposed_technique"}, {"score": 0.0023127544795063263, "phrase": "state-of-the-art_mandarin_chinese_broadcast_speech_transcription_task"}, {"score": 0.0022611052781735383, "phrase": "cer"}, {"score": 0.0021530945641993152, "phrase": "perplexity_improvements"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": [""], "paper_abstract": "Language models (LMs) are often constructed by building multiple individual component models that are combined using context independent interpolation weights. By tuning these weights, using either perplexity or discriminative approaches, it is possible to adapt LMs to a particular task. This paper investigates the use of context dependent weighting in both interpolation and test-time adaptation of language models. Depending on the previous word contexts, a discrete history weighting function is used to adjust the contribution from each component model. As this dramatically increases the number of parameters to estimate, robust weight estimation schemes are required. Several approaches are described in this paper. The first approach is based on MAP estimation where interpolation weights of lower order contexts are used as smoothing priors. The second approach uses training data to ensure robust estimation of LM interpolation weights. This can also serve as a smoothing prior for MAP adaptation. A normalized perplexity metric is proposed to handle the bias of the standard perplexity criterion to corpus size. A range of schemes to combine weight information obtained from training data and test data hypotheses are also proposed to improve robustness during context dependent LM adaptation. In addition, a minimum Bayes' risk (MBR) based discriminative training scheme is also proposed. An efficient weighted finite state transducer (WFST) decoding algorithm for context dependent interpolation is also presented. The proposed technique was evaluated using a state-of-the-art Mandarin Chinese broadcast speech transcription task. Character error rate (CER) reductions up to 7.3% relative were obtained as well as consistent perplexity improvements. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Use of contexts in language model interpolation and adaptation", "paper_id": "WOS:000311524800017"}