{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "gap-weighted_subsequence_kernels"}, {"score": 0.004694092086199672, "phrase": "vector_space_models"}, {"score": 0.004537631845923196, "phrase": "state-of-the-art_phonotactic_method"}, {"score": 0.004133627071344356, "phrase": "super_vector"}, {"score": 0.0040298010803060495, "phrase": "n-gram_tokens"}, {"score": 0.0038953958723049287, "phrase": "long-context_co-occurrence_relations"}, {"score": 0.003797530813314321, "phrase": "gram_order"}, {"score": 0.003593812067062062, "phrase": "frontend_phone_recognizer"}, {"score": 0.003459191419478298, "phrase": "gap-weighted_subsequence_kernel"}, {"score": 0.0031643005824589917, "phrase": "non-contiguous_way"}, {"score": 0.0029691377785333872, "phrase": "long-context_relations"}, {"score": 0.002857849698346116, "phrase": "truncated_gwsk"}, {"score": 0.002809732883291955, "phrase": "context_length"}, {"score": 0.0027159094186967247, "phrase": "remote_tokens"}, {"score": 0.0026701760128830573, "phrase": "computational_cost"}, {"score": 0.002526780881314497, "phrase": "multiple_hypotheses"}, {"score": 0.0024947959375982614, "phrase": "phone_recognizer"}, {"score": 0.0023708445073819277, "phrase": "computational_complexity"}, {"score": 0.0023408288327263316, "phrase": "proposed_methods"}, {"score": 0.002196342933346204, "phrase": "gwsk"}, {"score": 0.0021319959284827896, "phrase": "pr-vsm_approach"}], "paper_keywords": ["Spoken language recognition", " Gap-weighted subsequence kernel (GWSK)", " n-Gram", " Phone recognizer (PR)", " Vector space model (VSM)"], "paper_abstract": "Phone recognizers followed by vector space models (PR-VSM) is a state-of-the-art phonotactic method for spoken language recognition. This method resorts to a bag-of-n-grams, with each dimension of the super vector based on the counts of n-gram tokens. The n-gram cannot capture the long-context co-occurrence relations due to the restriction of gram order. Moreover, it is vulnerable to the errors induced by the frontend phone recognizer. In this paper, we introduce a gap-weighted subsequence kernel (GWSK) method to overcome the drawbacks of n-gram. GWSK counts the co-occurrence of the tokens in a non-contiguous way and thus is not only error-tolerant but also capable of revealing the long-context relations. Beyond this, we further propose a truncated GWSK with constraints on context length in order to remove the interference from remote tokens and lower the computational cost, and extend the idea to lattices to take the advantage of multiple hypotheses from the phone recognizer. In addition, we investigate the optimal parameter setting and computational complexity of the proposed methods. Experiments on NIST 2009 LRE evaluation corpus with several configurations show that the proposed GWSK is consistently more effective than the PR-VSM approach. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Spoken language recognition based on gap-weighted subsequence kernels", "paper_id": "WOS:000335804800001"}