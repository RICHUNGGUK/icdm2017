{"auto_keywords": [{"score": 0.0420960249176514, "phrase": "no-eye-tracking_condition"}, {"score": 0.029632338207217227, "phrase": "real_and_simulated_conditions"}, {"score": 0.00481495049065317, "phrase": "self-avatar_eye_movement"}, {"score": 0.0046477357199085035, "phrase": "novel_technique"}, {"score": 0.004586531354697466, "phrase": "self-avatar_eye_movements"}, {"score": 0.004526129302830441, "phrase": "immersive_virtual_environment"}, {"score": 0.0044272138900038095, "phrase": "eye-tracking_hardware"}, {"score": 0.004143196873246147, "phrase": "confidence_experiment"}, {"score": 0.004070573090297285, "phrase": "simulated-eye-tracking_condition"}, {"score": 0.003964007795174772, "phrase": "real-eye-tracking_condition"}, {"score": 0.003894512634265439, "phrase": "avatar's_eyes"}, {"score": 0.0038093479435377764, "phrase": "eye_tracker"}, {"score": 0.0036932254391253134, "phrase": "tracked_self-avatar"}, {"score": 0.0036124465071873998, "phrase": "virtual-embodiment_scenarios"}, {"score": 0.003456132182564149, "phrase": "virtual_body"}, {"score": 0.0033212215487678854, "phrase": "current_tracking_methods"}, {"score": 0.003191560305761507, "phrase": "participants_eyes"}, {"score": 0.003121719509312159, "phrase": "body-ownership_illusion"}, {"score": 0.0029471818883482688, "phrase": "experimental_conditions"}, {"score": 0.0028699415886017468, "phrase": "eye_behaviors"}, {"score": 0.0026501241093379786, "phrase": "statistical_difference"}, {"score": 0.002490833392316362, "phrase": "eye_movements"}, {"score": 0.002436289099444074, "phrase": "subjective_increase"}, {"score": 0.0023101951336207955, "phrase": "participant's_behavior"}, {"score": 0.0022297712188622293, "phrase": "virtual_embodiment"}, {"score": 0.0021906130276146325, "phrase": "effective_results"}, {"score": 0.0021049977753042253, "phrase": "specialized_eye-tracking_hardware"}], "paper_keywords": ["Virtual embodiment", " eye tracking", " virtual characters", " user studies"], "paper_abstract": "We present a novel technique for animating self-avatar eye movements in an immersive virtual environment without the use of eye-tracking hardware, and evaluate our technique via a two-alternative, forced-choice-with-confidence experiment that compares this simulated-eye-tracking condition to a no-eye-tracking condition and a real-eye-tracking condition in which the avatar's eyes were rotated with an eye tracker. Viewing the reflection of a tracked self-avatar is often used in virtual-embodiment scenarios to induce in the participant the illusion that the virtual body of the self-avatar belongs to them, however current tracking methods do not account for the movements of the participants eyes, potentially lessening this body-ownership illusion. The results of our experiment indicate that, although blind to the experimental conditions, participants noticed differences between eye behaviors, and found that the real and simulated conditions represented their behavior better than the no-eye-tracking condition. Additionally, no statistical difference was found when choosing between the real and simulated conditions. These results suggest that adding eye movements to self-avatars produces a subjective increase in self-identification with the avatar due to a more complete representation of the participant's behavior, which may be beneficial for inducing virtual embodiment, and that effective results can be obtained without the need for any specialized eye-tracking hardware.", "paper_title": "An Evaluation of Self-Avatar Eye Movement for Virtual Embodiment", "paper_id": "WOS:000316409700013"}