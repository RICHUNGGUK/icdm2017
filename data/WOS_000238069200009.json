{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "emotional_content"}, {"score": 0.010185314503510515, "phrase": "system_identification"}, {"score": 0.0037188360359702182, "phrase": "musical_features"}, {"score": 0.0025361866386341774, "phrase": "valid_linear_model_structure"}, {"score": 0.00226322835801988, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "previous_time-series_models"}], "paper_keywords": ["appraisals", " emotion", " information retrieval", " model", " mood", " music", " perception", " system identification"], "paper_abstract": "Research was conducted to develop a methodology to model the emotional content of music as a function of time and musical features. Emotion is quantified using the dimensions valence and arousal, and system-identification techniques are used to create the models. Results demonstrate that system identification provides a means to generalize the emotional content for a genre of music. The average R-2 statistic of a valid linear model structure is 21.9% for valence and 78.4% for arousal. The proposed method of constructing models of emotional content generalizes previous time-series models and removes ambiguity from classifiers of emotion.", "paper_title": "Modeling emotional content of music using system identification", "paper_id": "WOS:000238069200009"}