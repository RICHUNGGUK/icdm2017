{"auto_keywords": [{"score": 0.04386757596151299, "phrase": "information_streams"}, {"score": 0.00481495049065317, "phrase": "semantic_relationships"}, {"score": 0.00476527679607755, "phrase": "pervasive_computing"}, {"score": 0.004732444672509326, "phrase": "sensing_applications"}, {"score": 0.0044469055125323466, "phrase": "rich_sources"}, {"score": 0.0043555900164106934, "phrase": "long_periods"}, {"score": 0.0038456073694665, "phrase": "high_volumes"}, {"score": 0.0035268943257252224, "phrase": "underlying_meaning"}, {"score": 0.0033717094055177995, "phrase": "acceptable_level"}, {"score": 0.0033484464719654105, "phrase": "information_quality"}, {"score": 0.003290986600659723, "phrase": "actual_data"}, {"score": 0.003102875178186416, "phrase": "semantically-aware_source"}, {"score": 0.002865320947289691, "phrase": "preliminary_steps"}, {"score": 0.002825898877873034, "phrase": "source_coding_principles"}, {"score": 0.002806391131610627, "phrase": "classical_information_theory"}, {"score": 0.0027486699590262343, "phrase": "semantic_inference_relations"}, {"score": 0.0027296938800704484, "phrase": "probabilistically_expressed_messages"}, {"score": 0.0027108484507501907, "phrase": "underlying_models"}, {"score": 0.002655087144676306, "phrase": "higher_rate"}, {"score": 0.00254697318939512, "phrase": "traditional_syntactic_compression_methods"}, {"score": 0.002503232182798796, "phrase": "\"semantic_entropy\"_measure"}, {"score": 0.0023847032316716503, "phrase": "mutual_information"}, {"score": 0.0023437424558633013, "phrase": "syntactic_messages"}, {"score": 0.002279660253812632, "phrase": "simple_graph"}, {"score": 0.0022639147412879487, "phrase": "semantic_inference_relationships"}, {"score": 0.0022404996930993413, "phrase": "propositional_logic"}, {"score": 0.002217326282120832, "phrase": "practical_algorithms"}, {"score": 0.0021867999937268084, "phrase": "graph_structure"}, {"score": 0.002164180792169302, "phrase": "shared_knowledge_base"}, {"score": 0.0021417950504965997, "phrase": "lossless_semantic_compression"}], "paper_keywords": ["Semantic information theory", " Inference", " Semantic compression"], "paper_abstract": "In pervasive computing and sensing applications, a multitude of devices such as sensors and processors (that perform fusion) serve as rich sources of data and information over long periods of time. It is often the case that the information streams generated inside an application are not independent of each other; instead, they have certain semantic relationships between them. In order to deal with high volumes of information generated over time, it is sometimes necessary to compress these information streams. However, it is often the case that the underlying meaning or semantics of the information is what is critical for maintaining an acceptable level of information quality, rather than the actual data in its entirety. In this paper, we show how semantic redundancy and ambiguity within a semantically-aware source can be exploited to achieve compression with a goal of being able to recover the meaning underlying its messages. We take the preliminary steps to extend the source coding principles of classical information theory and show that by utilizing semantic inference relations between probabilistically expressed messages and underlying models at the source, a higher rate of compression, albeit lossy, may be achieved compared to traditional syntactic compression methods. We define a \"semantic entropy\" measure for a source and show that it is bounded from above by the mutual information between its models and the syntactic messages it generates. We also consider some simple graph based semantic inference relationships derived from propositional logic and give practical algorithms that exploit the graph structure of a shared knowledge base to facilitate lossless semantic compression. (C) 2013 Elsevier B. V. All rights reserved.", "paper_title": "Preserving quality of information by using semantic relationships", "paper_id": "WOS:000333813800013"}