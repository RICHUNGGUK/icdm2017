{"auto_keywords": [{"score": 0.04003926601049941, "phrase": "gemv"}, {"score": 0.015719716506582538, "phrase": "gpu_architecture"}, {"score": 0.01237704049276577, "phrase": "cublas"}, {"score": 0.012277088695417715, "phrase": "magma"}, {"score": 0.011136730464729738, "phrase": "new_method"}, {"score": 0.00973810615578236, "phrase": "register_blocking_method"}, {"score": 0.004717485406306594, "phrase": "powerful_computing_ability"}, {"score": 0.004659951478263083, "phrase": "data_parallel_applications"}, {"score": 0.0044549056501745074, "phrase": "gpu_system"}, {"score": 0.004364696693320128, "phrase": "even_a_simple_algorithm"}, {"score": 0.0043114466990006334, "phrase": "different_optimization_methods"}, {"score": 0.004259005572528398, "phrase": "gpu"}, {"score": 0.004189698902282913, "phrase": "different_performances"}, {"score": 0.004138575034269399, "phrase": "matrix-vector_multiplication_routine"}, {"score": 0.003972607172518598, "phrase": "important_kernel"}, {"score": 0.0036453427552753533, "phrase": "small_or_fat_matrix"}, {"score": 0.003513443975425428, "phrase": "novel_register_blocking_method"}, {"score": 0.0031327496009959464, "phrase": "vector_y"}, {"score": 0.0030317376771013703, "phrase": "highly_parallel_gpu_architecture"}, {"score": 0.0028744731986533076, "phrase": "off-chip_memory_bandwidth"}, {"score": 0.0028046593475268174, "phrase": "memory_access_order"}, {"score": 0.0026700637696697414, "phrase": "memory_access"}, {"score": 0.0026159017392986595, "phrase": "proposed_optimization_methods"}, {"score": 0.002541910967437926, "phrase": "different_matrix_sizes"}, {"score": 0.002459902948098627, "phrase": "different_block_sizes"}, {"score": 0.002380534385718454, "phrase": "experiment_results"}, {"score": 0.002294294558114909, "phrase": "small_square_matrices"}, {"score": 0.002275557501858879, "phrase": "fat_matrices"}, {"score": 0.0021751997984619585, "phrase": "higher_performance"}, {"score": 0.002157433262687853, "phrase": "large_square_matrices"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["GEMV", " Register blocking", " Data reuse", " Memory bandwidth", " GPU", " Many-core", " Parallelization", " CUDA"], "paper_abstract": "GPUs can provide powerful computing ability especially for data parallel applications, such as video/image processing applications. However, the complexity of GPU system makes the optimization of even a simple algorithm difficult. Different optimization methods on a GPU often lead to different performances. The matrix-vector multiplication routine for general dense matrices (GEMV) is an important kernel in video/image processing applications. We find that the implementations of GEMV in CUBLAS or MAGMA are not efficient, especially for small or fat matrix. In this paper, we propose a novel register blocking method to optimize GEMV on GPU architecture. This new method has three advantages. First, instead of using only one thread, we use a warp to compute an element of vector y so that the method can exploit the highly parallel GPU architecture. Second, the register blocking method is used to reduce the requirement of off-chip memory bandwidth. At last, the memory access order is elaborately arranged for the threads in one warp so that coalesced memory access is ensured. The proposed optimization methods for GEMV are comprehensively evaluated on different matrix sizes. The performance of the register blocking method with different block sizes is also evaluated in the experiment. Experiment results show that the new method can achieve very high speedup for small square matrices and fat matrices compared to CUBLAS or MAGMA, and can also achieve higher performance for large square matrices. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Highly parallel GEMV with register blocking method on GPU architecture", "paper_id": "WOS:000342543100007"}