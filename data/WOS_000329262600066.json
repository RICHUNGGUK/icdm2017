{"auto_keywords": [{"score": 0.027457111799361052, "phrase": "opencl"}, {"score": 0.00481495049065317, "phrase": "hybrid_supercomputations"}, {"score": 0.004778594263962987, "phrase": "incompressible_flows"}, {"score": 0.004600870784042767, "phrase": "efficient_parallel_algorithms"}, {"score": 0.004566123591480369, "phrase": "large-scale_simulations"}, {"score": 0.004480394224548146, "phrase": "hybrid_supercomputers"}, {"score": 0.004429727701202369, "phrase": "massively-parallel_accelerators"}, {"score": 0.004379631619412168, "phrase": "governing_equations"}, {"score": 0.004297388539539564, "phrase": "high-order_finite-volume_scheme"}, {"score": 0.004265197906892771, "phrase": "cartesian"}, {"score": 0.0038207243221070166, "phrase": "new_hybrid_algorithm"}, {"score": 0.0037065115435101887, "phrase": "multi-level_parallel_model"}, {"score": 0.0035820833812158005, "phrase": "modern_hybrid_supercomputer"}, {"score": 0.00350157323713205, "phrase": "mpi"}, {"score": 0.003475023331058516, "phrase": "openmp"}, {"score": 0.003383884759373793, "phrase": "couple_nodes"}, {"score": 0.003245520963944678, "phrase": "computing_accelerators"}, {"score": 0.0031484483754909026, "phrase": "hardware_independent_opencl_computing_standard"}, {"score": 0.0030082398041055003, "phrase": "general_computing_model"}, {"score": 0.002985484575498061, "phrase": "central_processors"}, {"score": 0.0029629009627331355, "phrase": "math_co"}, {"score": 0.0028095140358025, "phrase": "basic_operations"}, {"score": 0.0027358163024467854, "phrase": "graphics_processing_units"}, {"score": 0.0027153230209399793, "phrase": "gpu"}, {"score": 0.0026640466056415298, "phrase": "multi-cpu_communication_scheme"}, {"score": 0.0025069746344846397, "phrase": "relevant_performance_results"}, {"score": 0.0024411935317269705, "phrase": "gpu_accelerator"}, {"score": 0.0023591417077156555, "phrase": "different_cpu_architectures"}, {"score": 0.002279841453843241, "phrase": "particular_application"}, {"score": 0.002245494096535417, "phrase": "amd"}, {"score": 0.0022284756998214557, "phrase": "nvidia"}, {"score": 0.002178270547899467, "phrase": "cuda"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["MPI", " OpenMP", " OpenCL", " GPU", " Parallel CFD", " Turbulence"], "paper_abstract": "The work is devoted to the development of efficient parallel algorithms for large-scale simulations of incompressible flows on hybrid supercomputers based on massively-parallel accelerators. The governing equations are discretized using a high-order finite-volume scheme for Cartesian staggered meshes with the only restriction that, at least, one direction is periodic. Its \"classical\" MPI + OpenMP parallel implementation for CPUs was designed to scale till 100,000 CPU cores. The new hybrid algorithm is developed on a base of a multi-level parallel model that exploits several layers of parallelism of a modern hybrid supercomputer. In this model, MPI and OpenMP are used on the first two levels to couple nodes of a supercomputer and to engage its CPU cores. Then, computing accelerators are further used by means of the hardware independent OpenCL computing standard. In this way, the implementation is adapted to a general computing model with central processors and math co-processors. In this paper the work is focused on adapting the basic operations of the algorithm to architectures of Graphics Processing Units (GPU) without considering the multi-CPU communication scheme. Technology of porting the code to OpenCL is described, certain optimization approaches are presented and relevant performance results obtaining up to 80-90 GFLOPS on a GPU accelerator are demonstrated. Moreover, the experience with different CPU architectures is summarized and a comparison based on the particular application is given for AMD and NVIDIA GPUs as well as for CUDA and OpenCL frameworks. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "A parallel MPI plus OpenMP plus OpenCL algorithm for hybrid supercomputations of incompressible flows", "paper_id": "WOS:000329262600066"}