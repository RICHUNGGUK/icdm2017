{"auto_keywords": [{"score": 0.03956997057006548, "phrase": "tanner_graph"}, {"score": 0.021206750666571295, "phrase": "local_optimality"}, {"score": 0.00918531530616048, "phrase": "proc"}, {"score": 0.0060232095849859155, "phrase": "locally_optimal_codeword"}, {"score": 0.005573873595264256, "phrase": "decoding_guarantee"}, {"score": 0.004967835884317702, "phrase": "lower_bounds"}, {"score": 0.00481495049065317, "phrase": "decoding_irregular_tanner_codes"}, {"score": 0.004790902617513584, "phrase": "local-optimality_guarantees"}, {"score": 0.004719473826242326, "phrase": "binary_linear_tanner_codes"}, {"score": 0.004695900553353526, "phrase": "message-passing_iterative_decoding"}, {"score": 0.004579780586951551, "phrase": "memoryless_binary-input_output-symmetric"}, {"score": 0.004488946121663966, "phrase": "new_certificates"}, {"score": 0.0044219992735047954, "phrase": "combinatorial_characterization"}, {"score": 0.004334280585565239, "phrase": "irregular_tanner_codes"}, {"score": 0.004280338430265144, "phrase": "mbios_channel"}, {"score": 0.0038431888954930083, "phrase": "conical_combination"}, {"score": 0.0038239757564863057, "phrase": "normalized_weighted_subtrees"}, {"score": 0.003795335309109018, "phrase": "computation_trees"}, {"score": 0.003701401653850141, "phrase": "finite_height"}, {"score": 0.003502821629168212, "phrase": "local-code_nodes"}, {"score": 0.0033482708495261864, "phrase": "skinny_trees"}, {"score": 0.00327356326590461, "phrase": "new_characterization"}, {"score": 0.003074646498947256, "phrase": "new_message-passing_iterative_decoding_algorithm"}, {"score": 0.0029984960344766705, "phrase": "nwms_decoding"}, {"score": 0.0029611960115559235, "phrase": "bp"}, {"score": 0.002902304851882577, "phrase": "irregular_binary_tanner_code"}, {"score": 0.0028877818105020434, "phrase": "single_parity-check_local_codes"}, {"score": 0.0025795067398773006, "phrase": "nwms"}, {"score": 0.002502995662011305, "phrase": "nwms_decoding_algorithm"}, {"score": 0.002392453087295389, "phrase": "unique_ml_codeword"}, {"score": 0.0023507916951658455, "phrase": "ml_certificate"}, {"score": 0.0022925278792429553, "phrase": "new_local-optimality_characterization"}, {"score": 0.0022753331320081427, "phrase": "tanner"}, {"score": 0.002230100454211538, "phrase": "noise_thresholds"}, {"score": 0.002218986804770828, "phrase": "lp"}, {"score": 0.0022022878843247257, "phrase": "mbios_channels"}, {"score": 0.0021315842986705485, "phrase": "lp_decoding"}, {"score": 0.0021049977753042253, "phrase": "transmitted_codeword"}], "paper_keywords": ["Error bounds", " factor graphs", " graph cover", " IDPC codes", " linear programming (LP) decoding", " local optimality", " max-product algorithm", " maximum-likelihood (ML) certificate", " memoryless binary-input output-symmetric (MBIOS) channel", " message-passing algorithms", " min-sum algorithm", " Tanner codes", " thresholds"], "paper_abstract": "We consider decoding of binary linear Tanner codes using message-passing iterative decoding and linear-programming (LP) decoding in memoryless binary-input output-symmetric (MBIOS) channels. We present new certificates that are based on a combinatorial characterization for the local optimality of a codeword in irregular Tanner codes with respect to any MBIOS channel. This characterization is a generalization of (Arora et al., Proc. ACM Symp. Theory of Computing, 2009) and (Vontobel, Proc. Inf. Theory and Appl. Workshop, 2010) and is based on a conical combination of normalized weighted subtrees in the computation trees of the Tanner graph. These subtrees may have any finite height (even equal or greater than half of the girth of the Tanner graph). In addition, the degrees of local-code nodes in these subtrees are not restricted to two (i.e., these subtrees are not restricted to skinny trees). We prove that local optimality in this new characterization implies maximum-likelihood (ML) optimality and LP optimality, and show that a certificate can be computed efficiently. We also present a new message-passing iterative decoding algorithm, called normalized weighted min-sum (NWMS). NWMS decoding is a belief-propagation (BP) type algorithm that applies to any irregular binary Tanner code with single parity-check local codes (e. g., low-density and high-density parity-check codes). We prove that if a locally optimal codeword with respect to height parameter exists (whereby notably is not limited by the girth of the Tanner graph), then NWMS decoding finds this codeword in iterations. The decoding guarantee of the NWMS decoding algorithm applies whenever there exists a locally optimal codeword. Because local optimality of a codeword implies that it is the unique ML codeword, the decoding guarantee also provides an ML certificate for this codeword. Finally, we apply the new local-optimality characterization to regular Tanner codes, and prove lower bounds on the noise thresholds of LP decoding in MBIOS channels. When the noise is below these lower bounds, the probability that LP decoding fails to decode the transmitted codeword decays doubly exponentially in the girth of the Tanner graph.", "paper_title": "On Decoding Irregular Tanner Codes With Local-Optimality Guarantees", "paper_id": "WOS:000330282600014"}