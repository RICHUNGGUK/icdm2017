{"auto_keywords": [{"score": 0.04707046420780689, "phrase": "regularization_parameter"}, {"score": 0.00481495049065317, "phrase": "neural_network_approach"}, {"score": 0.004718431349852634, "phrase": "key_problems"}, {"score": 0.0046803650677267146, "phrase": "machine_learning_theory"}, {"score": 0.004567990533989851, "phrase": "correct_value"}, {"score": 0.004386637056030611, "phrase": "kernel_machines"}, {"score": 0.004333642054036135, "phrase": "support_vector_machines"}, {"score": 0.004298666679374568, "phrase": "regularized_least_square"}, {"score": 0.004212453011596424, "phrase": "weight_decay_terms"}, {"score": 0.004161553532799662, "phrase": "known_methods"}, {"score": 0.003567475091038435, "phrase": "tikhonov"}, {"score": 0.00342568771304873, "phrase": "primal_form"}, {"score": 0.003384262486955773, "phrase": "learning_algorithm"}, {"score": 0.003370565320453525, "phrase": "regularized_least_squares"}, {"score": 0.003329804436286295, "phrase": "controlled_environment"}, {"score": 0.0032497506915502876, "phrase": "oracular_notions"}, {"score": 0.003171615440419782, "phrase": "new_methodologies"}, {"score": 0.0029602552900111407, "phrase": "james-stein_theory"}, {"score": 0.0028081546657056948, "phrase": "multiple_kernels"}, {"score": 0.002751751956416832, "phrase": "svd_analysis"}, {"score": 0.0027184540871935284, "phrase": "mean_value_estimator"}, {"score": 0.0026423134393535265, "phrase": "rational_function"}, {"score": 0.0025892330566358503, "phrase": "balanced_neural_network_architecture"}, {"score": 0.002547535564922075, "phrase": "statistical_quantities"}, {"score": 0.0025167026017217926, "phrase": "symmetric_expectations"}, {"score": 0.0024862418814045723, "phrase": "obtained_results"}, {"score": 0.0024461989110005447, "phrase": "non-linear_analysis"}, {"score": 0.002387337550919433, "phrase": "non-linear_estimation"}, {"score": 0.0023298892270500983, "phrase": "neural_networks"}, {"score": 0.0022371907579775796, "phrase": "mean_value_estimations"}, {"score": 0.0021922308351126746, "phrase": "small_number"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Model selection", " Regularization", " Mean problem", " Back-propagation", " Multiple kernel learning", " James-Stein theory", " SVD", " Shrinkage"], "paper_abstract": "One of the key problems in machine learning theory and practice is setting the correct value of the regularization parameter: this is particularly crucial in Kernel Machines such as Support Vector Machines, Regularized Least Square or Neural Networks with Weight Decay terms. Well known methods such as Leave-One-Out (or GCV) and Evidence Maximization offer a way of predicting the regularization parameter. This work points out the failure of these methods for predicting the regularization parameter when coping with the, apparently trivial and here introduced, regularized mean problem; this is the simplest form of Tikhonov regularization, that, in turn, is the primal form of the learning algorithm Regularized Least Squares. This controlled environment gives the possibility to define oracular notions of regularization and to experiment new methodologies for predicting the regularization parameter that can be extended to the more general regression case. The analysis stems from James-Stein theory, shows the equivalence of shrinking and regularization and is carried using multiple kernels learning for regression and SVD analysis; a mean value estimator is built, first via a rational function and secondly via a balanced neural network architecture suitable for estimating statistical quantities and gaining symmetric expectations. The obtained results show that a non-linear analysis of the sample and a non-linear estimation of the mean obtained by neural networks can be profitably used to improve the accuracy of mean value estimations, especially when a small number of realizations is provided. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Learning the mean: A neural network approach", "paper_id": "WOS:000298206400014"}