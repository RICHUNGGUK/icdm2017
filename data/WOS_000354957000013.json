{"auto_keywords": [{"score": 0.04183650826006183, "phrase": "numerical_scores"}, {"score": 0.014389835652222679, "phrase": "linguistic_descriptions"}, {"score": 0.013406295540353986, "phrase": "qualitative_evaluations"}, {"score": 0.00481495049065317, "phrase": "blind_image_quality_assessment"}, {"score": 0.004772137871562584, "phrase": "deep_learning"}, {"score": 0.004563691742821759, "phrase": "visual_quality"}, {"score": 0.004364310576600359, "phrase": "extensive_psychological_evidence"}, {"score": 0.003955679223407243, "phrase": "fairly_benchmark_objective_image_quality_assessment"}, {"score": 0.003920480008575449, "phrase": "iqa"}, {"score": 0.0037827603118271757, "phrase": "learning-based_iqa_models"}, {"score": 0.003601235776892337, "phrase": "numerical_ratings"}, {"score": 0.0035216195857272403, "phrase": "learnt_mapping"}, {"score": 0.0031210475158175432, "phrase": "blind_iqa_model"}, {"score": 0.002984504445973107, "phrase": "general_utilization"}, {"score": 0.002957919048133256, "phrase": "fair_comparison"}, {"score": 0.0028795712616923462, "phrase": "natural_scene_statistics_features"}, {"score": 0.0028411768240129585, "phrase": "discriminative_deep_model"}, {"score": 0.002741269087816162, "phrase": "five_grades"}, {"score": 0.00269263690712392, "phrase": "five_explicit_mental_concepts"}, {"score": 0.002506562885555113, "phrase": "newly_designed_quality_pooling"}, {"score": 0.0024292428060702803, "phrase": "qualitative_labels"}, {"score": 0.0023754752864092437, "phrase": "classification_framework"}, {"score": 0.0022816681646340518, "phrase": "regression-based_models"}, {"score": 0.0022112700457629494, "phrase": "small_sample_size_problem"}, {"score": 0.0021430393104977788, "phrase": "popular_databases"}, {"score": 0.0021049977753042253, "phrase": "model's_effectiveness"}], "paper_keywords": ["Deep learning", " image quality assessment (IQA)", " natural scene statistics (NSS)", " no reference"], "paper_abstract": "This paper investigates how to blindly evaluate the visual quality of an image by learning rules from linguistic descriptions. Extensive psychological evidence shows that humans prefer to conduct evaluations qualitatively rather than numerically. The qualitative evaluations are then converted into the numerical scores to fairly benchmark objective image quality assessment (IQA) metrics. Recently, lots of learning-based IQA models are proposed by analyzing the mapping from the images to numerical ratings. However, the learnt mapping can hardly be accurate enough because some information has been lost in such an irreversible conversion from the linguistic descriptions to numerical scores. In this paper, we propose a blind IQA model, which learns qualitative evaluations directly and outputs numerical scores for general utilization and fair comparison. Images are represented by natural scene statistics features. A discriminative deep model is trained to classify the features into five grades, corresponding to five explicit mental concepts, i.e., excellent, good, fair, poor, and bad. A newly designed quality pooling is then applied to convert the qualitative labels into scores. The classification framework is not only much more natural than the regression-based models, but also robust to the small sample size problem. Thorough experiments are conducted on popular databases to verify the model's effectiveness, efficiency, and robustness.", "paper_title": "Blind Image Quality Assessment via Deep Learning", "paper_id": "WOS:000354957000013"}