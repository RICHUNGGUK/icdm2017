{"auto_keywords": [{"score": 0.047869525215773284, "phrase": "training_data"}, {"score": 0.00481495049065317, "phrase": "domain_invariant_transfer_kernel_learning"}, {"score": 0.004758457292840376, "phrase": "domain_transfer"}, {"score": 0.004647442352794694, "phrase": "learning_model"}, {"score": 0.004459334186889869, "phrase": "different_distributions"}, {"score": 0.00438105507792004, "phrase": "general_principle"}, {"score": 0.004178935413340453, "phrase": "distribution_difference"}, {"score": 0.004081385289516914, "phrase": "testing_data"}, {"score": 0.003986103192038901, "phrase": "generalization_error"}, {"score": 0.00387011040370557, "phrase": "current_methods"}, {"score": 0.0037797419108291227, "phrase": "sample_distributions"}, {"score": 0.003735349093835257, "phrase": "input_feature_space"}, {"score": 0.003626626504149897, "phrase": "nonlinear_feature_mapping"}, {"score": 0.003541923233726252, "phrase": "distribution_discrepancy"}, {"score": 0.0034388111429203222, "phrase": "nonlinear_feature_space"}, {"score": 0.003299460925313939, "phrase": "kernel-based_learning_machines"}, {"score": 0.0030554300179552415, "phrase": "domain-invariant_kernel"}, {"score": 0.002984027623988469, "phrase": "source_and_target_distributions"}, {"score": 0.002931569771634119, "phrase": "reproducing_kernel_hilbert_space"}, {"score": 0.0027307746993761035, "phrase": "spectral_kernels"}, {"score": 0.0026827569390113822, "phrase": "target_eigensystem"}, {"score": 0.002651213849567608, "phrase": "source_samples"}, {"score": 0.002620040660435039, "phrase": "mercer's_theorem"}, {"score": 0.002573965014439041, "phrase": "spectral_kernel"}, {"score": 0.0025286975935774245, "phrase": "approximation_error"}, {"score": 0.0024842242921857705, "phrase": "ground_truth_kernel"}, {"score": 0.0024118292918742967, "phrase": "domain-invariant_kernel_machines"}, {"score": 0.0023139986108451967, "phrase": "large_number"}, {"score": 0.002286781334850213, "phrase": "text_categorization"}, {"score": 0.002259883463607508, "phrase": "image_classification"}, {"score": 0.0022201273900989416, "phrase": "video_event_recognition"}, {"score": 0.0021049977753042253, "phrase": "proposed_tkl_approach"}], "paper_keywords": ["Transfer learning", " kernel learning", " Nystrom method", " text mining", " image classification", " video recognition"], "paper_abstract": "Domain transfer learning generalizes a learning model across training data and testing data with different distributions. A general principle to tackle this problem is reducing the distribution difference between training data and testing data such that the generalization error can be bounded. Current methods typically model the sample distributions in input feature space, which depends on nonlinear feature mapping to embody the distribution discrepancy. However, this nonlinear feature space may not be optimal for the kernel-based learning machines. To this end, we propose a transfer kernel learning (TKL) approach to learn a domain-invariant kernel by directly matching source and target distributions in the reproducing kernel Hilbert space (RKHS). Specifically, we design a family of spectral kernels by extrapolating target eigensystem on source samples with Mercer's theorem. The spectral kernel minimizing the approximation error to the ground truth kernel is selected to construct domain-invariant kernel machines. Comprehensive experimental evidence on a large number of text categorization, image classification, and video event recognition datasets verifies the effectiveness and efficiency of the proposed TKL approach over several state-of-the-art methods.", "paper_title": "Domain Invariant Transfer Kernel Learning", "paper_id": "WOS:000353890600006"}