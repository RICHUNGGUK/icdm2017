{"auto_keywords": [{"score": 0.03207438967850344, "phrase": "new_inputs"}, {"score": 0.00481495049065317, "phrase": "force_control_policies"}, {"score": 0.004725886853744007, "phrase": "constrained_robotic_motion_tasks"}, {"score": 0.004616859702338596, "phrase": "control_policies"}, {"score": 0.004304574865291508, "phrase": "new_contexts"}, {"score": 0.004166129325245761, "phrase": "existing_approaches"}, {"score": 0.004127394241838696, "phrase": "limited_performance"}, {"score": 0.004050996777117918, "phrase": "new_tasks"}, {"score": 0.0037768417338850274, "phrase": "motion-based_force_control_policies"}, {"score": 0.003672427620845944, "phrase": "constrained_motions"}, {"score": 0.003604421041893448, "phrase": "motion-dependent_external_forces"}, {"score": 0.0034884152484429207, "phrase": "proposed_methods"}, {"score": 0.002989714021608139, "phrase": "first_proposed_approach"}, {"score": 0.0029618842673339173, "phrase": "policy_and_policy_derivative_values"}, {"score": 0.0028265625216338875, "phrase": "first-order_taylor-like_polynomial"}, {"score": 0.0027100527874262446, "phrase": "second_approach"}, {"score": 0.002684819249720834, "phrase": "policy_and_policy_difference_data"}, {"score": 0.002562123444678208, "phrase": "superposition_fashion"}, {"score": 0.002456486927713543, "phrase": "policy_differences"}, {"score": 0.0022580713471027996, "phrase": "new_incoming_and_average-demonstrated_inputs"}, {"score": 0.0021750990136123367, "phrase": "real-world_robot"}], "paper_keywords": ["Learning by demonstration", " Force control policies", " Policy learning", " Policy derivative", " Policy generalization"], "paper_abstract": "Although learning of control policies from demonstrations has been thoroughly investigated in the literature, generalization of policies to new contexts still remains a challenge given that existing approaches exhibit limited performance when generalizing to new tasks. In this article, we propose two policy generalization approaches employed for generalizing motion-based force control policies with the view of performing constrained motions in presence of motion-dependent external forces. The key concept of the proposed methods is using, apart from policy values, also policy derivatives or differences which express how the policy varies with respect to variations in its input and combine these two kinds of information to generalize the policy at new inputs. The first proposed approach learns policy and policy derivative values by linear regression and combines these data into a first-order Taylor-like polynomial to estimate the policy at new inputs. The second approach learns policy and policy difference data by locally weighted regression and combines them in a superposition fashion to estimate the policy at new inputs. The policy differences in this approach represent variations of the policy in the direction of minimizing the distance between the new incoming and average-demonstrated inputs. The proposed approaches are evaluated in real-world robot constrained motion tasks by using a linear-actuated, two degrees-of-freedom haptic device.", "paper_title": "Generalization of Force Control Policies from Demonstrations for Constrained Robotic Motion Tasks", "paper_id": "WOS:000368189000009"}