{"auto_keywords": [{"score": 0.03338045802505238, "phrase": "chernoff"}, {"score": 0.008442724144041252, "phrase": "hoeffding"}, {"score": 0.00481495049065317, "phrase": "error_margins"}, {"score": 0.004739056045644449, "phrase": "incremental_learning"}, {"score": 0.004627441072541506, "phrase": "good_approach"}, {"score": 0.004273951631010383, "phrase": "new_examples"}, {"score": 0.0037634608487353153, "phrase": "memory_requirements"}, {"score": 0.00347574024760684, "phrase": "iadem"}, {"score": 0.0029409171055888804, "phrase": "new_algorithm"}, {"score": 0.0026518169452004465, "phrase": "accurate_trees"}, {"score": 0.002508073621696602, "phrase": "estimation_error"}, {"score": 0.0021559025985688255, "phrase": "user_requirements"}, {"score": 0.0021049977753042253, "phrase": "desired_error"}], "paper_keywords": [""], "paper_abstract": "Incremental learning is a good approach for classification when datasets are too large or when new examples can arrive at any time. Forgetting these examples while keeping only the relevant information lets us reduce memory requirements. The algorithm presented in this paper, called IADEM, has been developed using these approaches and other concepts such as Chernoff and Hoeffding bounds. The most relevant features of this new algorithm are: its capability to deal with datasets of any size for inducing accurate trees and its capacity to keep updated the estimation error of the tree that is being induced. This estimation of the error is fundamental to satisfy the user requirements about the desired error in the tree and to detect noise in the datasets.", "paper_title": "Incremental algorithm driven by error margins", "paper_id": "WOS:000241631200042"}