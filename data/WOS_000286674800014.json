{"auto_keywords": [{"score": 0.0482650857509263, "phrase": "neural_networks"}, {"score": 0.04319182969280953, "phrase": "training_dataset"}, {"score": 0.00481495049065317, "phrase": "back-propagation_neural_network"}, {"score": 0.004704699244406775, "phrase": "arbitrarily_partitioned_data"}, {"score": 0.004491678520807451, "phrase": "active_research_area"}, {"score": 0.003674192201248236, "phrase": "existing_cryptographic_approaches"}, {"score": 0.0035899674590142653, "phrase": "secure_scalar_product_protocol"}, {"score": 0.0035076666315744525, "phrase": "secure_way"}, {"score": 0.0030283730114326014, "phrase": "privacy_preserving_algorithm"}, {"score": 0.0029589086332593674, "phrase": "neural_network"}, {"score": 0.002419708657850547, "phrase": "final_weights"}, {"score": 0.0021049977753042253, "phrase": "real_world_data"}], "paper_keywords": ["Privacy", " Arbitrary partitioned data", " Neural network"], "paper_abstract": "Neural networks have been an active research area for decades. However, privacy bothers many when the training dataset for the neural networks is distributed between two parties, which is quite common nowadays. Existing cryptographic approaches such as secure scalar product protocol provide a secure way for neural network learning when the training dataset is vertically partitioned. In this paper, we present a privacy preserving algorithm for the neural network learning when the dataset is arbitrarily partitioned between the two parties. We show that our algorithm is very secure and leaks no knowledge (except the final weights learned by both parties) about other party's data. We demonstrate the efficiency of our algorithm by experiments on real world data.", "paper_title": "Privacy preserving Back-propagation neural network learning over arbitrarily partitioned data", "paper_id": "WOS:000286674800014"}