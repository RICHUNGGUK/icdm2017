{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "generalized_boosting_algorithms"}, {"score": 0.00367967106192446, "phrase": "minimax_paradigm"}, {"score": 0.0034317185678527672, "phrase": "population_version"}, {"score": 0.0033306180744375616, "phrase": "boosting_algorithm"}, {"score": 0.0031372369299648203, "phrase": "bayes_classifier"}, {"score": 0.002955050521440921, "phrase": "general_result"}, {"score": 0.0028966970007550824, "phrase": "gauss-southwell_optimization"}, {"score": 0.0028394925080958205, "phrase": "hilbert_space"}, {"score": 0.0026745503978347143, "phrase": "algorithmic_convergence"}, {"score": 0.002595698433525986, "phrase": "sample_version"}, {"score": 0.0023727863021828547, "phrase": "perfect_separation"}, {"score": 0.0021049977753042253, "phrase": "statistical_optimality"}], "paper_keywords": ["classification", " Gauss-Southwell algorithm", " AdaBoost", " cross-validation", " non-parametric convergence rate"], "paper_abstract": "We give a review of various aspects of boosting, clarifying the issues through a few simple results, and relate our work and that of others to the minimax paradigm of statistics. We consider the population version of the boosting algorithm and prove its convergence to the Bayes classifier as a corollary of a general result about Gauss-Southwell optimization in Hilbert space. We then investigate the algorithmic convergence of the sample version, and give bounds to the time until perfect separation of the sample. We conclude by some results on the statistical optimality of the L-2 boosting.", "paper_title": "Some theory for generalized boosting algorithms", "paper_id": "WOS:000240173400001"}