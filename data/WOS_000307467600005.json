{"auto_keywords": [{"score": 0.049596205816074275, "phrase": "unsupervised_learning"}, {"score": 0.026735366585430978, "phrase": "initial_training"}, {"score": 0.00481495049065317, "phrase": "subjectivity_identification"}, {"score": 0.004553822674247401, "phrase": "unsupervised_generative_learning_methods"}, {"score": 0.0044398358177815305, "phrase": "different_domains"}, {"score": 0.004241771072194993, "phrase": "simple_lexicon_information"}, {"score": 0.00409382470158851, "phrase": "base_naive_bayes_classifier"}, {"score": 0.004011603024990615, "phrase": "unannotated_data"}, {"score": 0.00395101799538683, "phrase": "first_method"}, {"score": 0.003793876828939456, "phrase": "high_confidence"}, {"score": 0.003410695644029149, "phrase": "em"}, {"score": 0.003341839487905525, "phrase": "class_distribution"}, {"score": 0.0032251796659158696, "phrase": "real_data"}, {"score": 0.0028120022457134267, "phrase": "unsupervised_learning_methods"}, {"score": 0.002713788077665689, "phrase": "fully_supervised_setup"}, {"score": 0.0026457367614424756, "phrase": "thorough_analysis"}, {"score": 0.0025533148412439166, "phrase": "self-labeling_accuracy"}, {"score": 0.0024023079401125492, "phrase": "added_examples"}, {"score": 0.002260211578368138, "phrase": "different_methods"}, {"score": 0.0021923400552366756, "phrase": "inherent_differences"}, {"score": 0.0021049977753042253, "phrase": "model_behaviors"}], "paper_keywords": [""], "paper_abstract": "In this study, we investigate using unsupervised generative learning methods for subjectivity detection across different domains. We create an initial training set using simple lexicon information and then evaluate two iterative learning methods with a base naive Bayes classifier to learn from unannotated data. The first method is self-training, which adds instances with high confidence into the training set in each iteration. The second is a calibrated EM (expectation-maximization) method where we calibrate the posterior probabilities from EM such that the class distribution is similar to that in the real data. We evaluate both approaches on three different domains: movie data, news resource, and meeting dialogues, and we found that in some cases the unsupervised learning methods can achieve performance close to the fully supervised setup. We perform a thorough analysis to examine factors, such as self-labeling accuracy of the initial training set in unsupervised learning, the accuracy of the added examples in self-training, and the size of the initial training set in different methods. Our experiments and analysis show inherent differences across domains and impacting factors explaining the model behaviors.", "paper_title": "A cross-corpus study of subjectivity identification using unsupervised learning", "paper_id": "WOS:000307467600005"}