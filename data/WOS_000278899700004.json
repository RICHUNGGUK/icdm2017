{"auto_keywords": [{"score": 0.048004834084269796, "phrase": "predictive_accuracy"}, {"score": 0.043069853910456694, "phrase": "model_jittering_ensembling"}, {"score": 0.00481495049065317, "phrase": "jittered_association_rule_classifiers"}, {"score": 0.004450568197899267, "phrase": "n_classifiers"}, {"score": 0.004299910946753963, "phrase": "n_learning_processes"}, {"score": 0.0036914757355166966, "phrase": "base_classifiers_sets"}, {"score": 0.0036553070575015344, "phrase": "classification_association_rules"}, {"score": 0.003479692058963528, "phrase": "iterative_reordering_ensembling"}, {"score": 0.0032160319265621285, "phrase": "single_run"}, {"score": 0.0031377955308198634, "phrase": "multiple_rule_sets"}, {"score": 0.00307657476125683, "phrase": "empirical_results"}, {"score": 0.0028154895878702633, "phrase": "single_model_association_rule_classifier"}, {"score": 0.0027741761880779535, "phrase": "bias-variance_analysis"}, {"score": 0.002693775822783661, "phrase": "pb"}, {"score": 0.0026148807877309417, "phrase": "variance_component"}, {"score": 0.0025512338217004603, "phrase": "ire"}, {"score": 0.002464709648714996, "phrase": "bias_component"}, {"score": 0.0021682027434117095, "phrase": "art_classifiers"}, {"score": 0.0021049977753042253, "phrase": "computational_efficiency"}], "paper_keywords": ["Ensembles", " Associative classification", " Model jittering"], "paper_abstract": "The ensembling of classifiers tends to improve predictive accuracy. To obtain an ensemble with N classifiers, one typically needs to run N learning processes. In this paper we introduce and explore Model Jittering Ensembling, where one single model is perturbed in order to obtain variants that can be used as an ensemble. We use as base classifiers sets of classification association rules. The two methods of jittering ensembling we propose are Iterative Reordering Ensembling (IRE) and Post Bagging (PB). Both methods start by learning one rule set over a single run, and then produce multiple rule sets without relearning. Empirical results on 36 data sets are positive and show that both strategies tend to reduce error with respect to the single model association rule classifier. A bias-variance analysis reveals that while both IRE and PB are able to reduce the variance component of the error, IRE is particularly effective in reducing the bias component. We show that Model Jittering Ensembling can represent a very good speed-up w.r.t. multiple model learning ensembling. We also compare Model Jittering with various state of the art classifiers in terms of predictive accuracy and computational efficiency.", "paper_title": "Ensembles of jittered association rule classifiers", "paper_id": "WOS:000278899700004"}