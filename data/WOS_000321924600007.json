{"auto_keywords": [{"score": 0.04740170155072888, "phrase": "voe"}, {"score": 0.00481495049065317, "phrase": "automatic_video_object_extraction"}, {"score": 0.0046118021635786315, "phrase": "saliency-based_video_object_extraction"}, {"score": 0.004385555408850022, "phrase": "proposed_framework"}, {"score": 0.004230749292148046, "phrase": "foreground_objects"}, {"score": 0.004081385289516914, "phrase": "user_interaction"}, {"score": 0.003909065355459264, "phrase": "training_data"}, {"score": 0.003690527690610707, "phrase": "particular_type"}, {"score": 0.003509317669345521, "phrase": "foreground_and_background_regions"}, {"score": 0.0034097787109430384, "phrase": "video_frames"}, {"score": 0.00333697550984782, "phrase": "proposed_method"}, {"score": 0.0032893028281789287, "phrase": "visual_and_motion_saliency_information"}, {"score": 0.0031959843979467704, "phrase": "input_video"}, {"score": 0.0031277316566463978, "phrase": "conditional_random_field"}, {"score": 0.0029740732099993706, "phrase": "saliency_induced_features"}, {"score": 0.0028076592725295646, "phrase": "unknown_pose"}, {"score": 0.0027675275077806744, "phrase": "scale_variations"}, {"score": 0.002708399936381754, "phrase": "foreground_object"}, {"score": 0.0024664004864629724, "phrase": "spatial_continuity"}, {"score": 0.0024311346712789553, "phrase": "temporal_consistency"}, {"score": 0.0023791769068748194, "phrase": "proposed_voe_framework"}, {"score": 0.0021049977753042253, "phrase": "qualitatively_satisfactory_voe_results"}], "paper_keywords": ["Conditional random field (CRF)", " video object extraction (VOE)", " visual saliency"], "paper_abstract": "This paper presents a saliency-based video object extraction (VOE) framework. The proposed framework aims to automatically extract foreground objects of interest without any user interaction or the use of any training data (i.e., not limited to any particular type of object). To separate foreground and background regions within and across video frames, the proposed method utilizes visual and motion saliency information extracted from the input video. A conditional random field is applied to effectively combine the saliency induced features, which allows us to deal with unknown pose and scale variations of the foreground object (and its articulated parts). Based on the ability to preserve both spatial continuity and temporal consistency in the proposed VOE framework, experiments on a variety of videos verify that our method is able to produce quantitatively and qualitatively satisfactory VOE results.", "paper_title": "Exploring Visual and Motion Saliency for Automatic Video Object Extraction", "paper_id": "WOS:000321924600007"}