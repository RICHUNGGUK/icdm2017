{"auto_keywords": [{"score": 0.04426734026124492, "phrase": "target_data"}, {"score": 0.006335927654671926, "phrase": "sufficient_data"}, {"score": 0.00481495049065317, "phrase": "high-dimensional_small-sized_data"}, {"score": 0.004747997855358171, "phrase": "effective_dimensionality_reduction_model"}, {"score": 0.0046558180142346264, "phrase": "traditional_dimensionality_reduction_methods"}, {"score": 0.004464244691043022, "phrase": "real_applications"}, {"score": 0.004353088808931991, "phrase": "unsupervised_dimensionality_reduction"}, {"score": 0.004316652646915877, "phrase": "high-dimensional_and_small-sized_data"}, {"score": 0.003903442361498738, "phrase": "proposed_model"}, {"score": 0.0038265753098624877, "phrase": "high-dimensional_and_small-sized_target_data"}, {"score": 0.0036484811192408714, "phrase": "sufficient_external_data"}, {"score": 0.003459191419478298, "phrase": "common_part"}, {"score": 0.0034398445347109396, "phrase": "external_data"}, {"score": 0.0032897795304254345, "phrase": "new_representations"}, {"score": 0.0032796900797817945, "phrase": "learnt_bases"}, {"score": 0.0032430998954652043, "phrase": "novel_joint_graph_sparse_coding_model"}, {"score": 0.0031889761655961345, "phrase": "robust_reconstruction_ability"}, {"score": 0.0031445615154154258, "phrase": "local_structures"}, {"score": 0.0031007635286163875, "phrase": "original_space"}, {"score": 0.003049007968098012, "phrase": "external_knowledge"}, {"score": 0.0029233560054443483, "phrase": "proposed_solver"}, {"score": 0.002850457082930323, "phrase": "objective_function"}, {"score": 0.0027100527874262446, "phrase": "learnt_basis_space"}, {"score": 0.0025477809372958953, "phrase": "sparse_features"}, {"score": 0.0023158555051565415, "phrase": "feature_selection"}, {"score": 0.0022203500288703443, "phrase": "proposed_stdr"}, {"score": 0.0021407635895366564, "phrase": "k-means_clustering_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Dimensionality reduction", " Self-taught learning", " Joint sparse coding", " Manifold learning", " Unsupervised learning"], "paper_abstract": "To build an effective dimensionality reduction model usually requires sufficient data. Otherwise, traditional dimensionality reduction methods might be less effective. However, sufficient data cannot always be guaranteed in real applications. In this paper we focus on performing unsupervised dimensionality reduction on the high-dimensional and small-sized data, in which the dimensionality of target data is high and the number of target data is small. To handle the problem, we propose a novel Self-taught Dimensionality Reduction (STDR) approach, which is able to transfer external knowledge (or information) from freely available external (or auxiliary) data to the high-dimensional and small-sized target data. The proposed STDR consists of three steps: First, the bases are learnt from sufficient external data, which might come from the same \"type\" or \"modality\" of target data. The bases are the common part between external data and target data, i.e., the external knowledge (or information). Second, target data are reconstructed by the learnt bases by proposing a novel joint graph sparse coding model, which not only provides robust reconstruction ability but also preserves the local structures amongst target data in the original space. This process transfers the external knowledge (i.e., the learnt bases) to target data. Moreover, the proposed solver to the proposed model is theoretically guaranteed that the objective function of the proposed model converges to the global optimum. After this, target data are mapped into the learnt basis space, and are sparsely represented by the bases, i.e., represented by parts of the bases. Third, the sparse features (that is, the rows with zero (or small) values) of the new representations of target data are deleted for achieving the effectiveness and the efficiency. That is, this step performs feature selection on the new representations of target data. Finally, experimental results at various types of datasets show the proposed STDR outperforms the state-of-the-art algorithms in terms of k-means clustering performance. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Self-taught dimensionality reduction on the high-dimensional small-sized data", "paper_id": "WOS:000309785000019"}