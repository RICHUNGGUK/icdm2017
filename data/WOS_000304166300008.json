{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "heterogeneous_feature_fusion"}, {"score": 0.013815283247925164, "phrase": "variable_selection"}, {"score": 0.004650330504968421, "phrase": "novel_multiple_kernel_learning"}, {"score": 0.00461007111422625, "phrase": "mkl"}, {"score": 0.0045108906244892165, "phrase": "group_lasso_regularizer"}, {"score": 0.004413846562278824, "phrase": "regularized_mkl"}, {"score": 0.004135010810625326, "phrase": "feature_fusion"}, {"score": 0.004028452405970376, "phrase": "base_kernels"}, {"score": 0.003976203286249396, "phrase": "feature_type"}, {"score": 0.003924629168434799, "phrase": "mkl_framework"}, {"score": 0.0038737214010638745, "phrase": "robust_way"}, {"score": 0.0038401488612789963, "phrase": "fitting_data"}, {"score": 0.0037903327482565097, "phrase": "different_feature_domains"}, {"score": 0.0037249113156851013, "phrase": "mixed_norm_constraint"}, {"score": 0.0033699898113395328, "phrase": "compact_feature"}, {"score": 0.0033262524445013303, "phrase": "recognition_purposes"}, {"score": 0.00321236577162671, "phrase": "optimal_base_kernels"}, {"score": 0.0031568884810273226, "phrase": "associated_weights"}, {"score": 0.003129509028432954, "phrase": "kernel_parameters"}, {"score": 0.003062091606343783, "phrase": "improved_recognition_performance"}, {"score": 0.002918826667294298, "phrase": "heterogeneous_variable_selection_problems"}, {"score": 0.0027943943921568456, "phrase": "compact_set"}, {"score": 0.002675252546913683, "phrase": "comparable_or_improved_performance"}, {"score": 0.0025500402081330394, "phrase": "entire_variable_space"}, {"score": 0.0025279105573394727, "phrase": "prior_sequential-based_variable_selection_methods"}, {"score": 0.0024306740161601625, "phrase": "prior_knowledge"}, {"score": 0.0023990982005125763, "phrase": "optimal_size"}, {"score": 0.0023679315987170857, "phrase": "variable_subset"}], "paper_keywords": ["Feature fusion", " multiple kernel learning", " variable selection"], "paper_abstract": "We propose a novel multiple kernel learning (MKL) algorithm with a group lasso regularizer, called group lasso regularized MKL (GL-MKL), for heterogeneous feature fusion and variable selection. For problems of feature fusion, assigning a group of base kernels for each feature type in an MKL framework provides a robust way in fitting data extracted from different feature domains. Adding a mixed norm constraint (i.e., group lasso) as the regularizer, we can enforce the sparsity at the group/feature level and automatically learn a compact feature set for recognition purposes. More precisely, our GL-MKL determines the optimal base kernels, including the associated weights and kernel parameters, and results in improved recognition performance. Besides, our GL-MKL can also be extended to address heterogeneous variable selection problems. For such problems, we aim to select a compact set of variables (i.e., feature attributes) for comparable or improved performance. Our proposed method does not need to exhaustively search for the entire variable space like prior sequential-based variable selection methods did, and we do not require any prior knowledge on the optimal size of the variable subset either. To verify the effectiveness and robustness of our GL-MKL, we conduct experiments on video and image datasets for heterogeneous feature fusion, and perform variable selection on various UCI datasets.", "paper_title": "A Novel Multiple Kernel Learning Framework for Heterogeneous Feature Fusion and Variable Selection", "paper_id": "WOS:000304166300008"}