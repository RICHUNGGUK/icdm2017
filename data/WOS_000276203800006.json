{"auto_keywords": [{"score": 0.04011318763734578, "phrase": "flutist_robot"}, {"score": 0.03989557473630433, "phrase": "musical_partners"}, {"score": 0.032955020560036165, "phrase": "visual_tracking_module"}, {"score": 0.027947175001911715, "phrase": "musical_parameters"}, {"score": 0.004734175871914184, "phrase": "waseda_university"}, {"score": 0.004487181594905386, "phrase": "flute_playing"}, {"score": 0.004337838898735768, "phrase": "basic_cognitive_capabilities"}, {"score": 0.0042891660310426976, "phrase": "flutist_beginners"}, {"score": 0.004205293427426673, "phrase": "research_efforts"}, {"score": 0.003985781820570781, "phrase": "intermediate_human_player"}, {"score": 0.003853062158993238, "phrase": "interaction_capabilities"}, {"score": 0.003641574955457217, "phrase": "long-term_goal"}, {"score": 0.0034807448314149248, "phrase": "jazz_band"}, {"score": 0.0034222756030918384, "phrase": "musical-based_interaction_system"}, {"score": 0.003317612543400555, "phrase": "process_both_visual_and_aural_cues"}, {"score": 0.0030914555414246407, "phrase": "waseda_flutist_robot_no"}, {"score": 0.0028563595473206555, "phrase": "virtual_buttons"}, {"score": 0.0027301134307516103, "phrase": "motion_gestures"}, {"score": 0.002699431192257011, "phrase": "musical_partner"}, {"score": 0.0024660368298717304, "phrase": "proposed_levels"}, {"score": 0.0023108237893935766, "phrase": "human_player"}, {"score": 0.0022848429198019885, "phrase": "experimental_results"}, {"score": 0.0022527762945938586, "phrase": "physical_constraints"}, {"score": 0.00221488852092224, "phrase": "important_role"}, {"score": 0.0021049977753042253, "phrase": "interaction_experience"}], "paper_keywords": ["Human-robot interaction", " Particle filter", " Music"], "paper_abstract": "Since 1990, at Waseda University the development on the Anthropomorphic Flutist Robot has been focused on mechanically reproducing the physiology of the organs involved during the flute playing (i.e. lungs, lips, etc.) and implementing basic cognitive capabilities to interact with flutist beginners. As a results of the research efforts done until now, the Waseda Flutist Robot is considered to play the flute nearly similar to the performance of a intermediate human player. However, we consider that in order to extend the interaction capabilities of the flutist robot with musical partners, further research efforts should be done. In this paper, we propose as a long-term goal to enable the flutist robot to interact more naturally with musical partners on the context of a Jazz band. For this purpose a Musical-Based Interaction System (MbIS) is proposed to enable the robot the process both visual and aural cues coming throughout the interaction with musicians. In particular, in this paper, the details of the implementation of the visual tracking module on the Waseda Flutist Robot No. 4 Refined IV (WF-4RIV) is presented. The visual tracking module is composed by two levels of interaction: basic (visual interface for the musician based on controlling virtual buttons and faders) and advanced (instrument tracking system so that the robot can process motion gestures performed by the musical partner in real-time which are then directly mapped into musical parameters of the performance of the robot). The experiments carried out were focused in verifying the effectiveness and usability of the proposed levels of interaction. In particular, we focused on determining how well our the WF-4RIV dynamically changes musical parameters while interacting with a human player. From the experimental results we observed that the physical constraints of the robot play an important role during the interaction. Although further improvements should be done to overcome such constrains, we expect that the interaction experience may become more natural.", "paper_title": "Musical-based interaction system for the Waseda Flutist Robot", "paper_id": "WOS:000276203800006"}