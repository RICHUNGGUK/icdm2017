{"auto_keywords": [{"score": 0.02513453169605108, "phrase": "hws_algorithm"}, {"score": 0.00481495049065317, "phrase": "hybrid_working_set_algorithm"}, {"score": 0.004667149874736771, "phrase": "kernel_coprocessor"}, {"score": 0.004618893461411858, "phrase": "fpga._support_vector_machines"}, {"score": 0.004477083974172596, "phrase": "popular_class"}, {"score": 0.004430784015995566, "phrase": "supervised_models"}, {"score": 0.004384960761826366, "phrase": "machine_learning"}, {"score": 0.004272456401437226, "phrase": "intensive_learning_algorithm"}, {"score": 0.004184526032558792, "phrase": "real-time_applications"}, {"score": 0.004055998173160551, "phrase": "fully_scalable_architecture"}, {"score": 0.0038907236419588255, "phrase": "multiple_rows"}, {"score": 0.0038304902197108643, "phrase": "kernel_matrix"}, {"score": 0.0036553070575015344, "phrase": "extended_variant"}, {"score": 0.003598705020845104, "phrase": "popular_decomposition_technique"}, {"score": 0.0031928040608991543, "phrase": "cached_kernel_columns"}, {"score": 0.003143341238802547, "phrase": "parallel_computational_power"}, {"score": 0.002983916676307492, "phrase": "xilinx_virtex"}, {"score": 0.002702887319160532, "phrase": "kernel_computation"}, {"score": 0.002579140290642792, "phrase": "application_speedup"}, {"score": 0.0025259712348798323, "phrase": "software_implementation"}, {"score": 0.0024998003600619685, "phrase": "libsvm"}, {"score": 0.0021493248730171132, "phrase": "optimization_time"}, {"score": 0.0021049977753042253, "phrase": "cache_size"}], "paper_keywords": ["Cache", " FPGA", " hardware-software codesign", " hybrid working set (HWS)", " sequential minimal optimization (SMO)", " support vector machine (SVM)"], "paper_abstract": "Support vector machines (SVM) are a popular class of supervised models in machine learning. The associated compute intensive learning algorithm limits their use in real-time applications. This paper presents a fully scalable architecture of a coprocessor, which can compute multiple rows of the kernel matrix in parallel. Further, we propose an extended variant of the popular decomposition technique, sequential minimal optimization, which we call hybrid working set (HWS) algorithm, to effectively utilize the benefits of cached kernel columns and the parallel computational power of the coprocessor. The coprocessor is implemented on Xilinx Virtex 7 field-programmable gate array-based VC707 board and achieves a speedup of upto 25x for kernel computation over single threaded computation on Intel Core i5. An application speedup of upto 15x over software implementation of LIBSVM and speedup of upto 23x over SVMLight is achieved using the HWS algorithm in unison with the coprocessor. The reduction in the number of iterations and sensitivity of the optimization time to variation in cache size using the HWS algorithm are also shown.", "paper_title": "Hybrid Working Set Algorithm for SVM Learning With a Kernel Coprocessor on FPGA", "paper_id": "WOS:000364208500022"}