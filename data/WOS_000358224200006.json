{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "approximate_n-player"}, {"score": 0.004592905436175987, "phrase": "uncertain_continuous_nonlinear_system"}, {"score": 0.0045122922461952805, "phrase": "approximate_online_equilibrium_solution"}, {"score": 0.00438105507792004, "phrase": "n-player_nonzero-sum_game"}, {"score": 0.004304144138805271, "phrase": "continuous-time_nonlinear_unknown_dynamics_and_an_infinite_horizon_quadratic_cost"}, {"score": 0.004228577663320847, "phrase": "novel_actor-critic-identifier_structure"}, {"score": 0.004081385289516914, "phrase": "robust_dynamic_neural_network"}, {"score": 0.003916098498242888, "phrase": "uncertain_system"}, {"score": 0.00387011040370557, "phrase": "additive_disturbances"}, {"score": 0.003541923233726252, "phrase": "value_functions"}, {"score": 0.003500313767208306, "phrase": "equilibrium_policies"}, {"score": 0.0033984088755183287, "phrase": "weight_update_laws"}, {"score": 0.003338690782620756, "phrase": "actor_neural_networks"}, {"score": 0.0031845067058084583, "phrase": "gradient-descent_method"}, {"score": 0.0031100973266506163, "phrase": "critic_nns"}, {"score": 0.0030374213037172803, "phrase": "least_square_regression"}, {"score": 0.002897109710213723, "phrase": "modified_bellman_error"}, {"score": 0.0027961342030212353, "phrase": "system_dynamics"}, {"score": 0.0027469703311742647, "phrase": "lyapunov-based_stability_analysis"}, {"score": 0.0026986685585382347, "phrase": "uniformly_ultimately_bounded_tracking"}, {"score": 0.0026045914281028473, "phrase": "convergence_analysis"}, {"score": 0.0025436978068459565, "phrase": "approximate_control_policies"}, {"score": 0.0024405312482831646, "phrase": "optimal_solutions"}, {"score": 0.0023277282036119106, "phrase": "identifier_structures"}, {"score": 0.002273292707863821, "phrase": "real_time"}, {"score": 0.0021049977753042253, "phrase": "developed_method"}], "paper_keywords": ["Actor-critic (AC) methods", " adaptive control", " adaptive dynamic programming", " differential games", " optimal control"], "paper_abstract": "An approximate online equilibrium solution is developed for an N-player nonzero-sum game subject to continuous-time nonlinear unknown dynamics and an infinite horizon quadratic cost. A novel actor-critic-identifier structure is used, wherein a robust dynamic neural network is used to asymptotically identify the uncertain system with additive disturbances, and a set of critic and actor NNs are used to approximate the value functions and equilibrium policies, respectively. The weight update laws for the actor neural networks (NNs) are generated using a gradient-descent method, and the critic NNs are generated by least square regression, which are both based on the modified Bellman error that is independent of the system dynamics. A Lyapunov-based stability analysis shows that uniformly ultimately bounded tracking is achieved, and a convergence analysis demonstrates that the approximate control policies converge to a neighborhood of the optimal solutions. The actor, critic, and identifier structures are implemented in real time continuously and simultaneously. Simulations on two and three player games illustrate the performance of the developed method.", "paper_title": "Approximate N-Player Nonzero-Sum Game Solution for an Uncertain Continuous Nonlinear System", "paper_id": "WOS:000358224200006"}