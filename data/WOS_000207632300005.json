{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cross-validation"}, {"score": 0.00470862827326028, "phrase": "em_training"}, {"score": 0.0046390482426405324, "phrase": "robust_parameter_estimation"}, {"score": 0.004536592536813498, "phrase": "new_maximum_likelihood_training_algorithm"}, {"score": 0.004242546312843817, "phrase": "em_algorithm"}, {"score": 0.0041488110506566825, "phrase": "cross-validation_likelihood"}, {"score": 0.0040571383470418085, "phrase": "expectation_step"}, {"score": 0.0037658881407605445, "phrase": "sufficient_statistics"}, {"score": 0.0035744997458371335, "phrase": "training_data"}, {"score": 0.0034695189961509625, "phrase": "parallel_em"}, {"score": 0.0032686865485199806, "phrase": "computational_requirements"}, {"score": 0.0031963987773420068, "phrase": "original_em_algorithm"}, {"score": 0.0027949428646076627, "phrase": "somewhat_higher_cost"}, {"score": 0.0026726283357752585, "phrase": "artificial_data"}, {"score": 0.002613487968156893, "phrase": "proposed_algorithms"}, {"score": 0.0024620842899061614, "phrase": "conventional_em_algorithm"}, {"score": 0.0024256208351516027, "phrase": "large_vocabulary_recognition_experiments_on_mandarin_broadcast_news_data"}, {"score": 0.002319431279824863, "phrase": "better_use"}, {"score": 0.002234491924392104, "phrase": "lower_recognition_error_rates"}, {"score": 0.0022013916728990564, "phrase": "standard_em_training"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["EM training", " Overtraining", " Cross-validation", " Sufficient statistics", " HMM"], "paper_abstract": "A new maximum likelihood training algorithm is proposed that compensates for weaknesses of the EM algorithm by using cross-validation likelihood in the expectation step to avoid overtraining. By using a set of sufficient statistics associated with a partitioning of the training data, as in parallel EM, the algorithm has the same order of computational requirements as the original EM algorithm. Another variation uses an approximation of bagging to reduce variance in the E-step but at a somewhat higher cost. Analyses using GMMs with artificial data show the proposed algorithms are more robust to overtraining than the conventional EM algorithm. Large vocabulary recognition experiments on Mandarin broadcast news data show that the methods make better use of more parameters and give lower recognition error rates than standard EM training. (C) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Cross-validation and aggregated EM training for robust parameter estimation", "paper_id": "WOS:000207632300005"}