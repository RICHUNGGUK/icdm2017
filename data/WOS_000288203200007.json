{"auto_keywords": [{"score": 0.044945871751663834, "phrase": "facial_expressions"}, {"score": 0.00481495049065317, "phrase": "expressive_talking_faces"}, {"score": 0.004518442937844318, "phrase": "real-time_facial_animation_system"}, {"score": 0.004361644989356558, "phrase": "mouth_movements"}, {"score": 0.004180622964109706, "phrase": "five_basic_emotions"}, {"score": 0.004092937456065441, "phrase": "hierarchical_structure"}, {"score": 0.004007083686875716, "phrase": "upper_layer"}, {"score": 0.003950846696972114, "phrase": "emotion_classification"}, {"score": 0.003760137090849196, "phrase": "recognized_emotion_label"}, {"score": 0.003681238363808568, "phrase": "under-layer_classification"}, {"score": 0.003629557750546267, "phrase": "sub-phonemic_level"}, {"score": 0.0034299649219748513, "phrase": "acoustic_features"}, {"score": 0.003334309053314909, "phrase": "audio_labels"}, {"score": 0.003150900886669123, "phrase": "predicted_emotion_labels"}, {"score": 0.0029775511937434797, "phrase": "facial_expression_labels"}, {"score": 0.0028740705307540317, "phrase": "sub-phonemic_labels"}, {"score": 0.002735194789581153, "phrase": "facial_action_units"}, {"score": 0.0026214978443388653, "phrase": "audio-visual_synchronized_animation"}, {"score": 0.0023742031943872464, "phrase": "experimental_results"}, {"score": 0.0023079225317714793, "phrase": "two-layer_structure"}, {"score": 0.002211947103318718, "phrase": "sub-phonemic_classifications"}, {"score": 0.0021501863832966966, "phrase": "synthesized_facial_sequences"}, {"score": 0.0021049977753042253, "phrase": "comparative_convincing_quality"}], "paper_keywords": ["audio-visual mapping", " speech-driven facial animation", " facial action units", " speech emotion recognition"], "paper_abstract": "In this paper, we present a real-time facial animation system in which speech drives mouth movements and facial expressions synchronously. Considering five basic emotions, a hierarchical structure with an upper layer of emotion classification is established. Based on the recognized emotion label, the under-layer classification at sub-phonemic level has been modelled on the relationship between acoustic features of frames and audio labels in phonemes. Using certain constraint, the predicted emotion labels of speech are adjusted to gain the facial expression labels which are combined with sub-phonemic labels. The combinations are mapped into facial action units (FAUs), and audio-visual synchronized animation with mouth movements and facial expressions is generated by morphing between FAUs. The experimental results demonstrate that the two-layer structure succeeds in both emotion and sub-phonemic classifications, and the synthesized facial sequences reach a comparative convincing quality.", "paper_title": "Real-time speech-driven animation of expressive talking faces", "paper_id": "WOS:000288203200007"}