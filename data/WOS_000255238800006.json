{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "learning_networks"}, {"score": 0.00474946741204285, "phrase": "functional_layers"}, {"score": 0.0046848707075530256, "phrase": "laminar_cortex"}, {"score": 0.004496280772084575, "phrase": "general-purpose_in-place_learning_networks"}, {"score": 0.00419861271409079, "phrase": "general_yet_adaptive_high-dimensional_function_approximator"}, {"score": 0.0040479661688863884, "phrase": "biological_concept"}, {"score": 0.003974673997871831, "phrase": "genomic_equivalence_principle"}, {"score": 0.0036275736378775757, "phrase": "external_learner"}, {"score": 0.003449718244515019, "phrase": "learning_network"}, {"score": 0.003356396922662246, "phrase": "ambitious_goal"}, {"score": 0.0032211117623834828, "phrase": "unusually_efficient_learning_algorithms"}, {"score": 0.0031627430133087616, "phrase": "low_computational_complexity"}, {"score": 0.0030491497463386924, "phrase": "typical_conventional_learning_algorithms"}, {"score": 0.0029802289354540507, "phrase": "neuroscience_literature"}, {"score": 0.0028210902750714075, "phrase": "feature_layers"}, {"score": 0.0027073229284666294, "phrase": "unsupervised_learning"}, {"score": 0.0026461080202599694, "phrase": "supervised_learning"}, {"score": 0.0025981315738078793, "phrase": "necessary_requirement"}, {"score": 0.0025744696954056404, "phrase": "autonomous_mental_development"}, {"score": 0.002551022760887132, "phrase": "miln"}, {"score": 0.002527788827701828, "phrase": "invariant_neurons"}, {"score": 0.0025047659706737215, "phrase": "different_layers"}, {"score": 0.0024706232839499546, "phrase": "increasing_invariance"}, {"score": 0.0024258208033731154, "phrase": "later_layers"}, {"score": 0.0023927516438886445, "phrase": "total_invariance"}, {"score": 0.002360132221690264, "phrase": "last_motor_layer"}, {"score": 0.002193509835371438, "phrase": "self-generated_invariant_representation"}, {"score": 0.0021537217025910356, "phrase": "intermediate_representations"}, {"score": 0.0021243539405631866, "phrase": "later_tasks"}, {"score": 0.0021049977753042253, "phrase": "open-ended_development"}], "paper_keywords": ["laminar cortex", " cortex structured", " feature extraction", " regression", " unsupervised learning", " supervised learning", " representation", " abstraction", " multi-task learning"], "paper_abstract": "Currently, there is a lack of general-purpose in-place learning networks that model feature layers in the cortex. By \"general-purpose\" we mean a general yet adaptive high-dimensional function approximator. In-place learning is a biological concept rooted in the genomic equivalence principle, meaning that each neuron is fully responsible for its own learning in its environment and there is no need for an external learner. Presented in this paper is the Multilayer In-place Learning Network (MILN) for this ambitious goal. Computationally, in-place learning provides unusually efficient learning algorithms whose simplicity, low computational complexity, and generality are set apart from typical conventional learning algorithms. Based on the neuroscience literature, we model the layer 4 and layer 2/3 as the feature layers in the 6-layer laminar cortex, with layer 4 using unsupervised learning and layer 2/3 using supervised learning. As a necessary requirement for autonomous mental development, MILN generates invariant neurons in different layers, with increasing invariance from earlier to later layers and the total invariance in the last motor layer. Such self-generated invariant representation is enabled mainly by descending (top-down) connections. The self-generated invariant representation is used as intermediate representations for learning later tasks in open-ended development. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Multilayer in-place learning networks for modeling functional layers in the laminar cortex", "paper_id": "WOS:000255238800006"}