{"auto_keywords": [{"score": 0.041892279565940715, "phrase": "data-generating_distribution"}, {"score": 0.00481495049065317, "phrase": "distribution-dependent_priors"}, {"score": 0.0036411406244705557, "phrase": "sharp_risk_bounds"}, {"score": 0.003557283483525861, "phrase": "stochastic_exponential_weights_algorithms"}, {"score": 0.0033170847930578473, "phrase": "controlling_function_class_complexity"}, {"score": 0.00278497898719426, "phrase": "unknown_geometry"}, {"score": 0.0023932142697579506, "phrase": "new_bounds"}, {"score": 0.0023380300046435187, "phrase": "rkhs_regularization_schemes"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Statistical learning theory", " PAC-Bayes", " Localized bounds", " Semi-supervised learning", " Exponential weights algorithm", " SVM"], "paper_abstract": "We further develop the idea that the PAC-Bayes prior can be informed by the data-generating distribution. We use this framework to prove sharp risk bounds for stochastic exponential weights algorithms, and develop insights into controlling function class complexity in this method. In particular we consider controlling capacity with respect to the unknown geometry defined by the data-generating distribution. We also use the method to obtain new bounds for RKHS regularization schemes such as SVMs. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Tighter PAC-Bayes bounds through distribution-dependent priors", "paper_id": "WOS:000315075300002"}