{"auto_keywords": [{"score": 0.04669935944937693, "phrase": "test_items"}, {"score": 0.007293705178200918, "phrase": "multiple_finite_alphabets"}, {"score": 0.005860387442250602, "phrase": "training_data"}, {"score": 0.005666253217073585, "phrase": "marginal_classifier"}, {"score": 0.0048153887772483615, "phrase": "bayesian"}, {"score": 0.004655285779819366, "phrase": "general_inductive_bayesian_classification_framework"}, {"score": 0.0045262653692203815, "phrase": "simultaneous_predictive_distribution"}, {"score": 0.004044822781630218, "phrase": "joint_posterior_distribution"}, {"score": 0.0038668462001710314, "phrase": "simultaneous_and_marginalized_classifiers"}, {"score": 0.003802135020791801, "phrase": "different_loss_functions"}, {"score": 0.0034944094552943, "phrase": "generating_probability_measures"}, {"score": 0.0032296377835432533, "phrase": "higher_correct_classification_rates"}, {"score": 0.003175556211659149, "phrase": "standard_marginal_predictive_classifier"}, {"score": 0.002951444412262927, "phrase": "supervised_case"}, {"score": 0.0028373742057129126, "phrase": "marginal_classifiers"}, {"score": 0.002727700630869639, "phrase": "generalized_exchangeability"}, {"score": 0.0024786330057813204, "phrase": "asymptotic_approximation"}, {"score": 0.0024370965602952496, "phrase": "simultaneous_classifier"}, {"score": 0.0024097920845706795, "phrase": "finite_sets"}, {"score": 0.0022145052062614514, "phrase": "semi-supervised_setting"}, {"score": 0.0021049977753042253, "phrase": "consistent_approximation"}], "paper_keywords": ["Classification", " Exchangeability", " Inductive learning", " Predictive inference"], "paper_abstract": "A general inductive Bayesian classification framework is considered using a simultaneous predictive distribution for test items. We introduce a principle of generative supervised and semi-supervised classification based on marginalizing the joint posterior distribution of labels for all test items. The simultaneous and marginalized classifiers arise under different loss functions, while both acknowledge jointly all uncertainty about the labels of test items and the generating probability measures of the classes. We illustrate for data from multiple finite alphabets that such classifiers achieve higher correct classification rates than a standard marginal predictive classifier which labels all test items independently, when training data are sparse. In the supervised case for multiple finite alphabets the simultaneous and the marginal classifiers are proven to become equal under generalized exchangeability when the amount of training data increases. Hence, the marginal classifier can be interpreted as an asymptotic approximation to the simultaneous classifier for finite sets of training data. It is also shown that such convergence is not guaranteed in the semi-supervised setting, where the marginal classifier does not provide a consistent approximation.", "paper_title": "Have I seen you before? Principles of Bayesian predictive classification revisited", "paper_id": "WOS:000313731400005"}