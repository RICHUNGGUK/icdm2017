{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "data_stall_time"}, {"score": 0.022455364963448108, "phrase": "data_access_delay"}, {"score": 0.010356088899586186, "phrase": "data_access_concurrency"}, {"score": 0.00561084484848254, "phrase": "data_concurrency"}, {"score": 0.004561386908745206, "phrase": "prominent_performance_bottleneck"}, {"score": 0.004516733575701128, "phrase": "high-end_computing_systems"}, {"score": 0.004342430078306461, "phrase": "system_design"}, {"score": 0.004216112320418982, "phrase": "memory_locality"}, {"score": 0.004013662619667083, "phrase": "modern_memory_systems"}, {"score": 0.003935420482629049, "phrase": "existing_studies"}, {"score": 0.0036914757355166966, "phrase": "memory_concurrency"}, {"score": 0.0036553070575015344, "phrase": "overall_memory_system_performance"}, {"score": 0.003428665921276366, "phrase": "novel_data"}, {"score": 0.0032319109979344184, "phrase": "combined_effort"}, {"score": 0.0031223774204093713, "phrase": "p-m_model"}, {"score": 0.0030464123943266673, "phrase": "pure_miss"}, {"score": 0.0028857130140890787, "phrase": "new_understanding"}, {"score": 0.0028154895878702633, "phrase": "new_directions"}, {"score": 0.0027878797103973313, "phrase": "performance_optimization"}, {"score": 0.0027200305495826797, "phrase": "new_models"}, {"score": 0.002680114113899346, "phrase": "summary_table"}, {"score": 0.0026538282523433684, "phrase": "advanced_cache_optimizations"}, {"score": 0.0024525907271610104, "phrase": "data_locality"}, {"score": 0.0023346234464901978, "phrase": "l-c_and_p-m_models"}, {"score": 0.002157538586313944, "phrase": "future_data-centric_architecture"}, {"score": 0.0021049977753042253, "phrase": "modern_computing_systems"}], "paper_keywords": ["memory wall", " data stall time", " memory concurrency", " concurrent average memory access time (C-AMAT)"], "paper_abstract": "Data access delay has become the prominent performance bottleneck of high-end computing systems. The key to reducing data access delay in system design is to diminish data stall time. Memory locality and concurrency are the two essential factors influencing the performance of modern memory systems. However, existing studies in reducing data stall time rarely focus on utilizing data access concurrency because the impact of memory concurrency on overall memory system performance is not well understood. In this study, a pair of novel data stall time models, the L-C model for the combined effort of locality and concurrency and the P-M model for the effect of pure miss on data stall time, are presented. The models provide a new understanding of data access delay and provide new directions for performance optimization. Based on these new models, a summary table of advanced cache optimizations is presented. It has 38 entries contributed by data concurrency while only has 21 entries contributed by data locality, which shows the value of data concurrency. The L-C and P-M models and their associated results and opportunities introduced in this study are important and necessary for future data-centric architecture and algorithm design of modern computing systems.", "paper_title": "Reevaluating Data Stall Time with the Consideration of Data Access Concurrency", "paper_id": "WOS:000351292400002"}