{"auto_keywords": [{"score": 0.033832255826776125, "phrase": "cluster_centroids"}, {"score": 0.02581590435353387, "phrase": "training_samples"}, {"score": 0.00481495049065317, "phrase": "k-hyperline_clustering_algorithm"}, {"score": 0.004758457292840376, "phrase": "k-hyperline"}, {"score": 0.004647442352794694, "phrase": "iterative_algorithm"}, {"score": 0.004565876339189493, "phrase": "singular_value_decomposition"}, {"score": 0.004355267258397068, "phrase": "sparse_component_analysis"}, {"score": 0.0040097139701644165, "phrase": "locally_optimal_solution"}, {"score": 0.0038930368103015467, "phrase": "training_data"}, {"score": 0.003802135020791801, "phrase": "lloyd's_optimality_conditions"}, {"score": 0.0036914757355166966, "phrase": "local_optimality"}, {"score": 0.003562912424153324, "phrase": "expectation-maximization_procedure"}, {"score": 0.0033984088755183287, "phrase": "sparse_representations"}, {"score": 0.003299460925313939, "phrase": "clustering_algorithm"}, {"score": 0.002984027623988469, "phrase": "convex_voronoi_regions"}, {"score": 0.0027469703311742647, "phrase": "empirical_risk_minimization_procedure"}, {"score": 0.0026986685585382347, "phrase": "function_class"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["K-hyperline clustering", " Optimality", " Stability", " Voronoi", " Empirical risk minimization"], "paper_abstract": "K-hyperline clustering is an iterative algorithm based on singular value decomposition and it has been successfully used in sparse component analysis. In this paper, we prove that the algorithm converges to a locally optimal solution for a given set of training data, based on Lloyd's optimality conditions. Furthermore, the local optimality is shown by developing an Expectation-Maximization procedure for learning dictionaries to be used in sparse representations and by deriving the clustering algorithm as its special case. The cluster centroids obtained from the algorithm are proved to tessellate the space into convex Voronoi regions. The stability of clustering is shown by posing the problem as an empirical risk minimization procedure over a function class. It is proved that, under certain conditions, the cluster centroids learned from two sets of i.i.d. training samples drawn from the same probability space become arbitrarily close to each other, as the number of training samples increase asymptotically. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Optimality and stability of the K-hyperline clustering algorithm", "paper_id": "WOS:000291771100008"}