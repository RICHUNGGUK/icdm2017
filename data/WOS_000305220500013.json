{"auto_keywords": [{"score": 0.04904947763914552, "phrase": "expectation_maximization_algorithm"}, {"score": 0.00481495049065317, "phrase": "gaussian_mixture_models"}, {"score": 0.0047732596509542135, "phrase": "image_segmentation"}, {"score": 0.004550300345169981, "phrase": "maximum_likelihood_estimates"}, {"score": 0.0044718206943536514, "phrase": "probabilistic_models"}, {"score": 0.00443308765153716, "phrase": "unobserved_data"}, {"score": 0.004337710093279712, "phrase": "mixture_models"}, {"score": 0.00428146713705618, "phrase": "key_issue"}, {"score": 0.004135010810625326, "phrase": "model_complexity"}, {"score": 0.0038401488612789963, "phrase": "data_likelihood"}, {"score": 0.0036926236934657864, "phrase": "computational_burden"}, {"score": 0.003504670673093149, "phrase": "clustering_method"}, {"score": 0.0032974089700312423, "phrase": "finite_gaussian_mixture_model"}, {"score": 0.0030223381739019894, "phrase": "careful_initialization"}, {"score": 0.0029443683459283955, "phrase": "single_mixture_component"}, {"score": 0.002906138793910919, "phrase": "whole_data"}, {"score": 0.002806595775343074, "phrase": "expectation_maximization_steps"}, {"score": 0.0027461155463816772, "phrase": "fine_nature"}, {"score": 0.002675252546913683, "phrase": "overall_number"}, {"score": 0.0025059724686541263, "phrase": "image_segmentation_applications"}, {"score": 0.0024842242921857705, "phrase": "computational_time"}, {"score": 0.002257070536352418, "phrase": "state-of-the-art_alternative_technique"}, {"score": 0.0021049977753042253, "phrase": "icub_humanoid_robot"}], "paper_keywords": ["Image processing", " Unsupervised learning", " Self-adapting Gaussians mixtures", " Expectation maximization", " Machine learning", " Clustering"], "paper_abstract": "The expectation maximization algorithm has been classically used to find the maximum likelihood estimates of parameters in probabilistic models with unobserved data, for instance, mixture models. A key issue in such problems is the choice of the model complexity. The higher the number of components in the mixture, the higher will be the data likelihood, but also the higher will be the computational burden and data overfitting. In this work, we propose a clustering method based on the expectation maximization algorithm that adapts online the number of components of a finite Gaussian mixture model from multivariate data or method estimates the number of components and their means and covariances sequentially, without requiring any careful initialization. Our methodology starts from a single mixture component covering the whole data set and sequentially splits it incrementally during expectation maximization steps. The coarse to fine nature of the algorithm reduce the overall number of computations to achieve a solution, which makes the method particularly suited to image segmentation applications whenever computational time is an issue. We show the effectiveness of the method in a series of experiments and compare it with a state-of-the-art alternative technique both with synthetic data and real images, including experiments with images acquired from the iCub humanoid robot.", "paper_title": "Fast estimation of Gaussian mixture models for image segmentation", "paper_id": "WOS:000305220500013"}