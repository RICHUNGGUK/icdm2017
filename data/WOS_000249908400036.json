{"auto_keywords": [{"score": 0.047617154407647785, "phrase": "hidden_neuron_outputs"}, {"score": 0.00481495049065317, "phrase": "neural_classifiers"}, {"score": 0.0047310525657970615, "phrase": "enforced_internal_representation"}, {"score": 0.004648609688625397, "phrase": "standard_bp-networks"}, {"score": 0.004332894955218135, "phrase": "whole_interval"}, {"score": 0.0038309141193444015, "phrase": "efficient_framework"}, {"score": 0.003698442219929622, "phrase": "transparent_internal_knowledge_representation"}, {"score": 0.0034168317227203206, "phrase": "formed_internal_representations"}, {"score": 0.0032697235411641695, "phrase": "different_outputs"}, {"score": 0.0024024377855956136, "phrase": "enforced_internal_representations"}, {"score": 0.0022787359717772976, "phrase": "case_study"}, {"score": 0.0022192891418394514, "phrase": "semantic_image_classification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["neural classifiers", " internal knowledge representation", " generalization", " learning from hints"], "paper_abstract": "standard BP-networks, hidden neuron outputs are usually spread over the whole interval (0, 1). In this paper, we propose an efficient framework to enforce a transparent internal knowledge representation in BP-networks during training. We want the formed internal representations to differ as much as possible for different outputs. At the same time, the hidden neuron outputs will be forced to group around three possible values, namely 1, 0 and 0.5. We will call such an internal representation unambiguous and condensed. The performance of BP-networks with enforced internal representations will be examined in a case study devoted to semantic image classification. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Improved generalization of neural classifiers with enforced internal representation", "paper_id": "WOS:000249908400036"}