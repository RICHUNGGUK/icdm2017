{"auto_keywords": [{"score": 0.04540493651240884, "phrase": "q_functions"}, {"score": 0.04464228929897594, "phrase": "world_model"}, {"score": 0.03445564019371514, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "adaptive_model_learning"}, {"score": 0.004764220976404241, "phrase": "dyna-q_learning"}, {"score": 0.004502622933782367, "phrase": "rl"}, {"score": 0.0044237033788643715, "phrase": "offline_simulations"}, {"score": 0.004392565073466714, "phrase": "action_executions"}, {"score": 0.0042102651938187114, "phrase": "feature_values"}, {"score": 0.004165879874212301, "phrase": "next_state"}, {"score": 0.004121960535174029, "phrase": "reward_function"}, {"score": 0.00388165472334023, "phrase": "policy_learning"}, {"score": 0.003813669883040509, "phrase": "tabular_methods"}, {"score": 0.003746871277361436, "phrase": "dyna-q"}, {"score": 0.0036424097335610492, "phrase": "tabular_model"}, {"score": 0.0034421126037849, "phrase": "adaptive_model_learning_method"}, {"score": 0.0034057975804778293, "phrase": "tree_structures"}, {"score": 0.003346119094502001, "phrase": "sampling_efficiency"}, {"score": 0.0032184697893776052, "phrase": "simulated_experiences"}, {"score": 0.003195787852087248, "phrase": "indirect_learning"}, {"score": 0.0031397776924946526, "phrase": "proposed_agent"}, {"score": 0.0031176485976102688, "phrase": "additional_experience"}, {"score": 0.0029775511937434797, "phrase": "state_transition"}, {"score": 0.0029565620771207003, "phrase": "associated_rewards"}, {"score": 0.0029253546782349875, "phrase": "coarse_coding"}, {"score": 0.0028437313714204087, "phrase": "state_space"}, {"score": 0.002784007794229784, "phrase": "precedent_states"}, {"score": 0.0027255350933549703, "phrase": "reward_and_transition_probabilities"}, {"score": 0.0026494723416072316, "phrase": "resultant_tree"}, {"score": 0.002548331088496964, "phrase": "value_iteration"}, {"score": 0.0024510413243251906, "phrase": "induced_states"}, {"score": 0.002157811371348755, "phrase": "proposed_methods"}, {"score": 0.0021350169717509714, "phrase": "simulation_result"}, {"score": 0.0021049977753042253, "phrase": "training_rate"}], "paper_keywords": ["decision tree", " Dyna-Q agent", " model learning", " reinforcement learning"], "paper_abstract": "Dyna-Q, a well-known model-based reinforcement learning (RL) method, interplays offline simulations and action executions to update Q functions. It creates a world model that predicts the feature values in the next state and the reward function of the domain directly from the data and uses the model to train Q functions to accelerate policy learning. In general, tabular methods are always used in Dyna-Q to establish the model, but a tabular model needs many more samples of experience to approximate the environment concisely. In this article, an adaptive model learning method based on tree structures is presented to enhance sampling efficiency in modeling the world model. The proposed method is to produce simulated experiences for indirect learning. Thus, the proposed agent has additional experience for updating the policy. The agent works backwards from collections of state transition and associated rewards, utilizing coarse coding to learn their definitions for the region of state space that tracks back to the precedent states. The proposed method estimates the reward and transition probabilities between states from past experience. Because the resultant tree is always concise and small, the agent can use value iteration to quickly estimate the Q-values of each action in the induced states and determine a policy. The effectiveness and generality of our method is further demonstrated in two numerical simulations. Two simulations, a mountain car and a mobile robot in a maze, are used to verify the proposed methods. The simulation result demonstrates that the training rate of our method can improve obviously.", "paper_title": "Adaptive Model Learning Based on Dyna-Q Learning", "paper_id": "WOS:000324904000002"}