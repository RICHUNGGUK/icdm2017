{"auto_keywords": [{"score": 0.03538831151886269, "phrase": "objective_labels"}, {"score": 0.02360474585996606, "phrase": "subjective_labels"}, {"score": 0.008312839720121843, "phrase": "unlabeled_videos"}, {"score": 0.00794997007547833, "phrase": "youtube"}, {"score": 0.00481495049065317, "phrase": "objective_ground-truth_labels"}, {"score": 0.004743594110762319, "phrase": "improved_video_classification"}, {"score": 0.004708311581484766, "phrase": "comparative_study"}, {"score": 0.00461549660282073, "phrase": "category_labels"}, {"score": 0.004558414405811025, "phrase": "large_video_dataset"}, {"score": 0.004479678062285735, "phrase": "objectively_labeled_videos"}, {"score": 0.00441326830307403, "phrase": "large_video_databases"}, {"score": 0.004315487055548749, "phrase": "new_video_assign"}, {"score": 0.004230382915464492, "phrase": "prescribed_set"}, {"score": 0.004105850229681625, "phrase": "subjective_biases"}, {"score": 0.0039651694106206245, "phrase": "gold_standard"}, {"score": 0.00392586404888927, "phrase": "multimedia_classification"}, {"score": 0.0037631002998184076, "phrase": "unseen_videos"}, {"score": 0.0037257904871180305, "phrase": "subjective_ground-truth"}, {"score": 0.0036160578937825295, "phrase": "video_classification_performance"}, {"score": 0.0034921002307283003, "phrase": "objectively_labeled_ground-truth"}, {"score": 0.0032405527786547915, "phrase": "objectively-labeled_ground-truth_dataset"}, {"score": 0.0030983802147021263, "phrase": "diverse_individuals"}, {"score": 0.003075296862372287, "phrase": "majority_opinion"}, {"score": 0.002992115137331354, "phrase": "objective_opinion"}, {"score": 0.002918443696891907, "phrase": "multiple_human_annotators"}, {"score": 0.0028608109787377255, "phrase": "ground-truth_dataset"}, {"score": 0.0028183324515019723, "phrase": "tinyvideos"}, {"score": 0.002748928987030436, "phrase": "karpenko"}, {"score": 0.002735253083856282, "phrase": "aarabi"}, {"score": 0.0026812282414768536, "phrase": "fourfold_cross-validation_experiment"}, {"score": 0.002595698433525986, "phrase": "superior_consistency"}, {"score": 0.002544422946205545, "phrase": "video_classification"}, {"score": 0.0024693979425773993, "phrase": "feature_sets"}, {"score": 0.0023609780669713288, "phrase": "label_prediction"}, {"score": 0.0022799458141165587, "phrase": "objective_category_labels"}, {"score": 0.0021852710479765116, "phrase": "video_uploaders"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Video classification", " Large dataset analysis", " Data annotation"], "paper_abstract": "We address the problem of predicting category labels for unlabeled videos in a large video dataset by using a ground-truth set of objectively labeled videos that we have created. Large video databases like YouTube require that a user uploading a new video assign to it a category label from a prescribed set of labels. Such category labeling is likely to be corrupted by the subjective biases of the uploader. Despite their noisy nature, these subjective labels are frequently used as gold standard in algorithms for multimedia classification and retrieval. Our goal in this paper is NOT to propose yet another algorithm that predicts labels for unseen videos based on the subjective ground-truth. On the other hand, our goal is to demonstrate that the video classification performance can be improved if instead of using subjective labels, we first create an objectively labeled ground-truth set of videos and then train a classifier based on such a ground-truth so as to predict objective labels for the set of unlabeled videos. With regard to how we generate the objectively-labeled ground-truth dataset, we base it on the notion that when a video is labeled by a panel of diverse individuals, the majority opinion rendered by the panel may be taken to be the objective opinion. In this manner, using judgments provided by multiple human annotators, we have collected objective labels for a ground-truth dataset consisting of randomly-selected 1000 videos from the TinyVideos database that contains roughly 52,000 videos from YouTube (courtesy of Karpenko and Aarabi [1]). Through a fourfold cross-validation experiment on the ground-truth set, we demonstrate that the objective labels have a superior consistency compared to the subjective labels when used for video classification. We show that this claim is valid for several different kinds of feature sets that one can use to compare videos and with two different types of classifiers that one can use for label prediction. Subsequently, we use the ground-truth dataset of 1000 videos to predict the objective category labels of the remaining 51,000 videos. We compare the objective labels thus determined with the subjective labels provided by the video uploaders and qualitatively argue for the more informative nature of the objective labels. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Using objective ground-truth labels created by multiple annotators for improved video classification: A comparative study", "paper_id": "WOS:000323913300015"}