{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "continuous_pomdps"}, {"score": 0.04653478165971171, "phrase": "continuous_spaces"}, {"score": 0.03647638288732213, "phrase": "continuous_state_spaces"}, {"score": 0.02802846283537619, "phrase": "gaussian_mixtures"}, {"score": 0.004707479285061122, "phrase": "novel_approach"}, {"score": 0.004602395776895036, "phrase": "markov_decisions"}, {"score": 0.004320421465896323, "phrase": "model-based_pomdps"}, {"score": 0.004243064277617193, "phrase": "discrete_states"}, {"score": 0.0036060781902674207, "phrase": "value_function"}, {"score": 0.003415681637907093, "phrase": "piecewise-linear_convex"}, {"score": 0.003369669078274226, "phrase": "particular_case"}, {"score": 0.0033393377529146893, "phrase": "discrete_observations"}, {"score": 0.0031629800646754505, "phrase": "continuous_bellman_backups"}, {"score": 0.0030644247579222333, "phrase": "monotonic_convergence"}, {"score": 0.003036832649117453, "phrase": "value-iteration_algorithms"}, {"score": 0.0029025435402480326, "phrase": "perseus_algorithm"}, {"score": 0.0028376361457661415, "phrase": "discrete_pomdps"}, {"score": 0.0026634937939104177, "phrase": "reward_models"}, {"score": 0.0025456716198597627, "phrase": "particle_sets"}, {"score": 0.0024111277643627154, "phrase": "bellman_backup"}, {"score": 0.0023571835517819124, "phrase": "closed_form"}, {"score": 0.0021926062743982144, "phrase": "perseus"}, {"score": 0.0021531831652035482, "phrase": "continuous_action"}, {"score": 0.0021337783230121286, "phrase": "observation_sets"}, {"score": 0.0021049977753042253, "phrase": "effective_sampling_approaches"}], "paper_keywords": ["planning under uncertainty", " partially observable Markov decision processes", " continuous state space", " continuous action space", " continuous observation space", " point-based value iteration"], "paper_abstract": "We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) defined on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally defined on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the PERSEUS algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend PERSEUS to deal with continuous action and observation sets by designing effective sampling approaches.", "paper_title": "Point-based value iteration for continuous POMDPs", "paper_id": "WOS:000245390700003"}