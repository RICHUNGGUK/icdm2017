{"auto_keywords": [{"score": 0.03820679588870147, "phrase": "target_view"}, {"score": 0.018437515418496357, "phrase": "canonical_correlations"}, {"score": 0.01771210988537723, "phrase": "source_view"}, {"score": 0.016344389620859965, "phrase": "common_feature_space"}, {"score": 0.005959539703544032, "phrase": "multiple_source_views"}, {"score": 0.00481495049065317, "phrase": "cross-view_action_recognition_over_heterogeneous_feature_spaces"}, {"score": 0.004769958800452922, "phrase": "cross-view_action_recognition"}, {"score": 0.004494570360108983, "phrase": "data_distribution"}, {"score": 0.004480522897810809, "phrase": "even_the_feature_space"}, {"score": 0.0042350135188564285, "phrase": "action_models"}, {"score": 0.004104456271230213, "phrase": "different_view"}, {"score": 0.004015458740397976, "phrase": "action_instances"}, {"score": 0.003928383344128392, "phrase": "heterogeneous_features"}, {"score": 0.00389164348807591, "phrase": "novel_learning_method"}, {"score": 0.003689823995992878, "phrase": "discriminative_common_feature_space"}, {"score": 0.00347657813672945, "phrase": "map_data"}, {"score": 0.003296212583539166, "phrase": "interclass_training_data"}, {"score": 0.003234806486611361, "phrase": "intraclass_training_data"}, {"score": 0.0031845067058084583, "phrase": "data_distribution_mismatch"}, {"score": 0.003154701855951328, "phrase": "source_and_target_views"}, {"score": 0.0029909851863915283, "phrase": "common_features"}, {"score": 0.002953711377604328, "phrase": "corresponding_action_instances"}, {"score": 0.002835740601294821, "phrase": "even_no_labeled_samples"}, {"score": 0.002613709503583786, "phrase": "multiple_source"}, {"score": 0.0026055251533586804, "phrase": "views_adaptation"}, {"score": 0.0025730425700096365, "phrase": "action_knowledge"}, {"score": 0.002525074485615743, "phrase": "recognition_task"}, {"score": 0.0024547897375637308, "phrase": "different_source_views"}, {"score": 0.002431797888365548, "phrase": "different_weights"}, {"score": 0.002298288831092204, "phrase": "extensive_experiments"}, {"score": 0.0022767594422780026, "phrase": "ixmas_data"}, {"score": 0.002234302459067969, "phrase": "htdcc"}, {"score": 0.0021926354732269846, "phrase": "heterogeneous_cross-view_action_recognition"}, {"score": 0.0021517438495635634, "phrase": "weighting_learning_framework"}, {"score": 0.0021315842986705485, "phrase": "promising_results"}, {"score": 0.0021049977753042253, "phrase": "multiple_transferred_source-view_knowledge"}], "paper_keywords": ["Cross-view action recognition", " transfer learning", " heterogeneous features", " multiple views adaptation"], "paper_abstract": "In cross-view action recognition, what you saw in one view is different from what you recognize in another view, since the data distribution even the feature space can change from one view to another. In this paper, we address the problem of transferring action models learned in one view (source view) to another different view (target view), where action instances from these two views are represented by heterogeneous features. A novel learning method, called heterogeneous transfer discriminant-analysis of canonical correlations (HTDCC), is proposed to discover a discriminative common feature space for linking source view and target view to transfer knowledge between them. Two projection matrices are learned to, respectively, map data from the source view and the target view into a common feature space via simultaneously minimizing the canonical correlations of interclass training data, maximizing the canonical correlations of intraclass training data, and reducing the data distribution mismatch between the source and target views in the common feature space. In our method, the source view and the target view neither share any common features nor have any corresponding action instances. Moreover, our HTDCC method is capable of handling only a few or even no labeled samples available in the target view, and can also be easily extended to the situation of multiple source views. We additionally propose a weighting learning framework for multiple source views adaptation to effectively leverage action knowledge learned from multiple source views for the recognition task in the target view. Under this framework, different source views are assigned different weights according to their different relevances to the target view. Each weight represents how contributive the corresponding source view is to the target view. Extensive experiments on the IXMAS data set demonstrate the effectiveness of HTDCC on learning the common feature space for heterogeneous cross-view action recognition. In addition, the weighting learning framework can achieve promising results on automatically adapting multiple transferred source-view knowledge to the target view.", "paper_title": "Cross-View Action Recognition Over Heterogeneous Feature Spaces", "paper_id": "WOS:000359563500007"}