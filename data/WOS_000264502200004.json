{"auto_keywords": [{"score": 0.04955111375780366, "phrase": "linear_recurrences"}, {"score": 0.015719716506582538, "phrase": "parallel_computation"}, {"score": 0.011179639406839071, "phrase": "compact_representations"}, {"score": 0.008117707997778624, "phrase": "un-optimized_parallel_computations"}, {"score": 0.004709333998298903, "phrase": "compact_matrix_representations"}, {"score": 0.004555217356295278, "phrase": "novel_method"}, {"score": 0.004238307626560739, "phrase": "resource_requirements"}, {"score": 0.004076858589744185, "phrase": "unique_feature"}, {"score": 0.0038998328617557013, "phrase": "matrix_computations"}, {"score": 0.003668859054928118, "phrase": "general_notion"}, {"score": 0.003588291565092195, "phrase": "matrix_multiplication"}, {"score": 0.003319939715683536, "phrase": "permutation_matrices"}, {"score": 0.0030715948036518603, "phrase": "proposed_method"}, {"score": 0.0028895595049429953, "phrase": "cuda"}, {"score": 0.002857605453835429, "phrase": "nvidia_geforce"}, {"score": 0.00262910358108176, "phrase": "larger_recurrences"}, {"score": 0.002542890176213921, "phrase": "good_speedups"}, {"score": 0.0024054226746197706, "phrase": "memory_usage"}, {"score": 0.002164331818029166, "phrase": "promising_approach"}], "paper_keywords": ["Memory optimization", " Recursion", " Matrix multiplication", " Programmable graphics hardware"], "paper_abstract": "This paper presents a novel method for optimizing the parallel computation of linear recurrences. Our method can help reduce the resource requirements for both memory and computation. A unique feature of our technique is its formulation of linear recurrences as matrix computations, before exploiting their mathematical properties for more compact representations. Based on a general notion of closure for matrix multiplication, we present two classes of matrices that have compact representations. These classes are permutation matrices and matrices whose elements are linearly related to each other. To validate the proposed method, we experiment with solving recurrences whose matrices have compact representations using CUDA on nVidia GeForce 8800 GTX GPU. The advantages of our technique are that it enables the computation of larger recurrences in parallel and it provides good speedups of up to eleven times over the un-optimized parallel computations. Also, the memory usage can be as much as nine times lower than that of the un-optimized parallel computations. Our result confirms a promising approach for the adoption of more advanced parallelization techniques. (C) 2009 Elsevier Inc. All rights reserved.", "paper_title": "Optimizing the parallel computation of linear recurrences using compact matrix representations", "paper_id": "WOS:000264502200004"}