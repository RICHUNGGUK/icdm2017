{"auto_keywords": [{"score": 0.04765243024404618, "phrase": "structured_prediction_problems"}, {"score": 0.03546730795101328, "phrase": "latent_variables"}, {"score": 0.00481495049065317, "phrase": "grammar_learning"}, {"score": 0.0046429592670444945, "phrase": "bayesian_networks"}, {"score": 0.004500414250942443, "phrase": "bayesian_network's_model_structure"}, {"score": 0.004362226376603478, "phrase": "predicted_output_structure"}, {"score": 0.004294724955301414, "phrase": "incremental_sigmoid_belief_networks"}, {"score": 0.0040771431404398855, "phrase": "partial_output_structures"}, {"score": 0.003931402465972151, "phrase": "unboundedly_many_compatible_model_structures"}, {"score": 0.003542976343910816, "phrase": "natural_language_parsing"}, {"score": 0.0034519997658752598, "phrase": "domain's_complex_statistical_dependencies_benefits"}, {"score": 0.00331125524068286, "phrase": "exact_inference"}, {"score": 0.0032430426853131346, "phrase": "large_numbers"}, {"score": 0.002968424524565369, "phrase": "previous_neural_network_parsing_model"}, {"score": 0.0028771413440398614, "phrase": "coarse_mean-field_approximation"}, {"score": 0.0026471739851621143, "phrase": "artificial_experiments"}, {"score": 0.002499798127196099, "phrase": "benchmark_natural_language_parsing_task"}, {"score": 0.0022642601698943687, "phrase": "closer_approximation"}, {"score": 0.0022291564590495015, "phrase": "isbn"}, {"score": 0.002206045784326466, "phrase": "better_parsing_accuracy"}, {"score": 0.002127046102734894, "phrase": "appropriate_abstract_model"}, {"score": 0.0021049977753042253, "phrase": "natural_language_grammar"}], "paper_keywords": ["Bayesian networks", " dynamic Bayesian networks", " grammar learning", " natural language parsing", " neural networks"], "paper_abstract": "We propose a class of Bayesian networks appropriate for structured prediction problems where the Bayesian network's model structure is a function of the predicted output structure. These incremental sigmoid belief networks (ISBNs) make decoding possible because inference with partial output structures does not require summing over the unboundedly many compatible model structures, due to their directed edges and incrementally specified model structure. ISBNs are specifically targeted at challenging structured prediction problems such as natural language parsing, where learning the domain's complex statistical dependencies benefits from large numbers of latent variables. While exact inference in ISBNs with large numbers of latent variables is not tractable, we propose two efficient approximations. First, we demonstrate that a previous neural network parsing model can be viewed as a coarse mean-field approximation to inference with ISBNs. We then derive a more accurate but still tractable variational approximation, which proves effective in artificial experiments. We compare the effectiveness of these models on a benchmark natural language parsing task, where they achieve accuracy competitive with the state-of-the-art. The model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of natural language grammar learning.", "paper_title": "Incremental Sigmoid Belief Networks for Grammar Learning", "paper_id": "WOS:000286637200009"}