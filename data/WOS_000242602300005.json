{"auto_keywords": [{"score": 0.05007621541386648, "phrase": "reinforcement_learning"}, {"score": 0.048970045726729895, "phrase": "rl"}, {"score": 0.0355036627848235, "phrase": "influence_zone_algorithm"}, {"score": 0.00454090611248773, "phrase": "direct_experimentation"}, {"score": 0.0044356867986444426, "phrase": "decision-making_problems"}, {"score": 0.004207734323806386, "phrase": "small_or_medium_size_problems"}, {"score": 0.0040385352882334235, "phrase": "value_function_estimation"}, {"score": 0.003698442219929622, "phrase": "rl_performance"}, {"score": 0.0032505921489759224, "phrase": "topological_rl_agent"}, {"score": 0.003212663446866794, "phrase": "trla"}, {"score": 0.0030295361111006866, "phrase": "requested_interactions"}, {"score": 0.0029075757820516634, "phrase": "topological-preserving_characteristic"}, {"score": 0.0027741761880779535, "phrase": "state-action_pairs"}, {"score": 0.002615977783510631, "phrase": "influence_zone_approach"}, {"score": 0.0025854347094662247, "phrase": "seven_other_rl_algorithms"}, {"score": 0.0025254115243261875, "phrase": "proposed_algorithm"}, {"score": 0.002409503258044183, "phrase": "value_function"}, {"score": 0.002206290313627494, "phrase": "remarkable_flexibility"}, {"score": 0.0021049977753042253, "phrase": "input_space_topology"}], "paper_keywords": ["reinforcement learning", " self-organizing map", " instantaneous topological map", " learning acceleration"], "paper_abstract": "Reinforcement Learning (RL) aims to learn through direct experimentation how to solve decision-making problems. RL algorithms often have their practical applications restricted to small or medium size problems-mainly because of their strategies for value function estimation demanding very high number of interactions. To overcome this difficulty, we propose to enhance RL performance by updating several state (or state-action) values at each interaction. Therefore, the influence zone algorithm, an improvement over the topological RL agent (TRLA) strategy, allows to reduce the number of requested interactions. Such a reduction is based on the topological-preserving characteristic of the mapping between states (or state-action pairs) and value estimates. The comparison of the influence zone approach with seven other RL algorithms suggests that the proposed algorithm is among the fastest to estimate the value function and that it takes less value function updatings to perform such an estimation. The influence zone algorithm also presents a remarkable flexibility in adapting its policy to changes of the input space topology. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Influence zones: A strategy to enhance reinforcement learning", "paper_id": "WOS:000242602300005"}