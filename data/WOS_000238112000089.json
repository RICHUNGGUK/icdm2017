{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "fcmac"}, {"score": 0.004725385523804256, "phrase": "multi-agent_cooperation"}, {"score": 0.004424768776701965, "phrase": "well-defined_quantized_state_spaces"}, {"score": 0.004143196873246147, "phrase": "optimal_policy"}, {"score": 0.0036324746826127997, "phrase": "real_robot_tasks"}, {"score": 0.003531474887552889, "phrase": "poor_performance"}, {"score": 0.0034657012607680203, "phrase": "learned_behavior"}, {"score": 0.0032145922365231093, "phrase": "continuous_state_and_action_spaces"}, {"score": 0.002953711377604328, "phrase": "fuzzy-based_cmac_method"}, {"score": 0.002765490759189057, "phrase": "neighboring_state"}, {"score": 0.0026633568650501873, "phrase": "continuous_action_value"}, {"score": 0.0024014759148123736, "phrase": "momentum_term"}, {"score": 0.0021450029487459403, "phrase": "multi-agent_system"}, {"score": 0.0021049977753042253, "phrase": "real_robot_applications"}], "paper_keywords": [""], "paper_abstract": "In general, Q-learning needs well-defined quantized state spaces and action spaces to obtain an optimal policy for accomplishing a given task. This makes it difficult to be applied to real robot tasks because of poor performance of learned behavior due to the failure of quantization of continuous state and action spaces. In this paper, we proposed a fuzzy-based CMAC method to calculate the contribution of each neighboring state to generate a continuous action value in order to make motion smooth and effective. A momentum term to speed up training has been designed and implemented in a multi-agent system for real robot applications.", "paper_title": "Q-learning with FCMAC in multi-agent cooperation", "paper_id": "WOS:000238112000089"}