{"auto_keywords": [{"score": 0.04967298152704192, "phrase": "mapreduce"}, {"score": 0.012421498939865886, "phrase": "mpi"}, {"score": 0.004575053987970206, "phrase": "de_facto_standard"}, {"score": 0.004536244367689415, "phrase": "parallel_programming"}, {"score": 0.0043842558719324526, "phrase": "increasing_concern"}, {"score": 0.004291848822287492, "phrase": "mm_applications"}, {"score": 0.004112812699639956, "phrase": "parallel_checkpointing"}, {"score": 0.00402610372759447, "phrase": "new_programming_model"}, {"score": 0.003941215575554871, "phrase": "massive_data_processing"}, {"score": 0.00387459052649752, "phrase": "numerous_efforts"}, {"score": 0.0038090874465452214, "phrase": "classical_mpi_based_scientific_applications"}, {"score": 0.0036657110980445416, "phrase": "easy_programming"}, {"score": 0.003634587156159668, "phrase": "automatic_parallelism"}, {"score": 0.0033517342967390065, "phrase": "considerable_overhead"}, {"score": 0.0032950412841674026, "phrase": "failure-free_performance_comparison"}, {"score": 0.0029491324941225356, "phrase": "analytical_approach"}, {"score": 0.0027194778713860715, "phrase": "extensive_numerical_analysis"}, {"score": 0.0026507289852980512, "phrase": "different_parameters"}, {"score": 0.0026282000779095987, "phrase": "fault_tolerance"}, {"score": 0.0025291601927956765, "phrase": "hpc_community"}, {"score": 0.002475755804232436, "phrase": "critical_decisions"}, {"score": 0.0024028742248152425, "phrase": "algorithm_designers"}, {"score": 0.0022062235065459274, "phrase": "programming_model"}, {"score": 0.002178144355363467, "phrase": "better_performance"}], "paper_keywords": ["Fault tolerance", " MPI", " MapReduce", " Checkpoint"], "paper_abstract": "MPI has been the de facto standard of parallel programming for decades. There has been an increasing concern about the reliability of MM applications in recent years, partially due to the inefficiency of parallel checkpointing. MapReduce is a new programming model originally introduced to handle massive data processing. There are numerous efforts recently that transform classical MPI based scientific applications to MapReduce, due to the merits of easy programming, automatic parallelism, and fault tolerance of MapReduce. However, the stricter synchronization primitive supported by MapReduce also imposes considerable overhead. While the failure-free performance comparison between MPI and MapReduce has been investigated, there exists little work in comparing the two programming models under failures. In this paper, we propose an analytical approach to quantifying the capabilities of the two programming models to tolerate failures for a comparison. We also carry out extensive numerical analysis to study the impact of different parameters on fault tolerance. This work can be used by the HPC community for various purposes in making critical decisions. For example, it helps algorithm designers to answer the question such as, at which scale should we give up MPI and use MapReduce as the programming model for a better performance under the presence of failures? (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Performance comparison under failures of MPI and MapReduce: An analytical approach", "paper_id": "WOS:000320635700018"}