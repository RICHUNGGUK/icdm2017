{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "floating-point_data"}, {"score": 0.004774324145784758, "phrase": "large_scale_scientific_simulation_codes"}, {"score": 0.004576253356889219, "phrase": "time_steps"}, {"score": 0.004518442937844318, "phrase": "single_file_system"}, {"score": 0.004461359547152617, "phrase": "data_sets"}, {"score": 0.003995772867224968, "phrase": "simulation_stalls"}, {"score": 0.0038789122930433305, "phrase": "data_compression"}, {"score": 0.0037495207047193034, "phrase": "cpu_cycles"}, {"score": 0.0032184697893776052, "phrase": "floating-point_values"}, {"score": 0.0031777570808834213, "phrase": "uniform_integer_grid"}, {"score": 0.003071682293903262, "phrase": "exact_values"}, {"score": 0.0029691377785333872, "phrase": "simple_scheme"}, {"score": 0.0026929458952693465, "phrase": "data-dependent_prediction"}, {"score": 0.0026140878612307536, "phrase": "wide_variety"}, {"score": 0.0025160740422447837, "phrase": "unstructured_meshes"}, {"score": 0.0024947959375982614, "phrase": "point_sets"}, {"score": 0.0024423826953133844, "phrase": "voxel_grids"}, {"score": 0.0024012441367538434, "phrase": "state-of-the-art_compression_rates"}, {"score": 0.00229164285862567, "phrase": "improved_entropy_coder"}, {"score": 0.002196342933346204, "phrase": "real_simulation_runs"}, {"score": 0.002168531704054597, "phrase": "previous_schemes"}, {"score": 0.0021049977753042253, "phrase": "variable-precision_floating-point"}], "paper_keywords": ["high throughput", " lossless compression", " file compaction for I/O efficiency", " fast entropy coding", " range coder", " predictive coding", " large scale simulation and visualization"], "paper_abstract": "Large scale scientific simulation codes typically run on a cluster of CPUs that write/read time steps to/from a single file system. As data sets are constantly growing in size, this increasingly leads to I/O bottlenecks. When the rate at which data is produced exceeds the available I/O bandwidth, the simulation stalls and the CPUs are idle. Data compression can alleviate this problem by using some CPU cycles to reduce the amount of data needed to be transfered. Most compression schemes, however, are designed to operate offline and seek to maximize compression, not throughput. Furthermore, they often require quantizing floating-point values onto a uniform integer grid, which disqualifies their use in applications where exact values must be retained. We propose a simple scheme for lossless, online compression of floating-point data that transparently integrates into the I/O of many applications. A plug-in scheme for data-dependent prediction makes our scheme applicable to a wide variety of data used in visualization, such as unstructured meshes, point sets, images, and voxel grids. We achieve state-of-the-art compression rates and speeds, the latter in part due to an improved entropy coder. We demonstrate that this significantly accelerates I/O throughput in real simulation runs. Unlike previous schemes, our method also adapts well to variable-precision floating-point and integer data.", "paper_title": "Fast and efficient compression of floating-point data", "paper_id": "WOS:000241383300075"}