{"auto_keywords": [{"score": 0.04574007889452913, "phrase": "scientific_applications"}, {"score": 0.015719716506582538, "phrase": "virtual_machines"}, {"score": 0.004764542405129454, "phrase": "multicore_architectures"}, {"score": 0.004426092397211699, "phrase": "best_performance"}, {"score": 0.004243552607799184, "phrase": "current_memory_management"}, {"score": 0.004177049813981273, "phrase": "handling_techniques"}, {"score": 0.0037793152987497286, "phrase": "multiple_paradigms"}, {"score": 0.0036424917384748024, "phrase": "global_address_spaces"}, {"score": 0.003529149828147388, "phrase": "numa_systems"}, {"score": 0.003492155652400838, "phrase": "current_memory_translation_schemes"}, {"score": 0.0031594291695784286, "phrase": "contemporary_os"}, {"score": 0.0031262984171100856, "phrase": "vm_architectures"}, {"score": 0.0030130446986008277, "phrase": "best_solution"}, {"score": 0.0029657683623403085, "phrase": "memory_locality"}, {"score": 0.002518525107994039, "phrase": "shared_memory"}, {"score": 0.002440071551819324, "phrase": "communication_stack"}, {"score": 0.0021049977753042253, "phrase": "multicore_clusters"}], "paper_keywords": ["Virtual machines", " NUMA systems", " parallel systems", " parallel IO", " HPC applications"], "paper_abstract": "In this paper we argue that partitioning is required for attaining the best performance of scientific applications when running on virtual machines. Current memory management and I/O handling techniques introduce high overhead when running scientific applications. Using KVM, we quantify this impact on applications written in multiple paradigms: message passing, shared memory and partitioned global address spaces. Our analysis shows that on NUMA systems, current memory translation schemes cannot preserve the locality of access and introduce up to 82 percent slowdown. We discuss the interaction between contemporary OS and VM architectures and argue that partitioning is the best solution to enforce memory locality. Current I/O solutions using one assistant task cannot provide the level of I/O parallelism required by scientific applications and we observe an average 7.2x application slowdown on a cluster with 16 cores per node. More specialized solutions that implement shared memory by-pass within the communication stack also do not scale well with cores and we observe an average 2.4x application slowdown. Overall, our results indicate that using partitioning and direct inter-VM shared memory support is enough to provide close to native performance in multicore clusters.", "paper_title": "The Case for Partitioning Virtual Machines on Multicore Architectures", "paper_id": "WOS:000342179600019"}