{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "music_information_retrieval"}, {"score": 0.004542158659382197, "phrase": "rigorous_scientific_evaluations"}, {"score": 0.003906586074865932, "phrase": "evaluation_forums"}, {"score": 0.0036671822784770463, "phrase": "evaluation_frameworks"}, {"score": 0.002917331114260319, "phrase": "good_place"}, {"score": 0.0028333356794133053, "phrase": "text_ir_field"}, {"score": 0.002725081358527507, "phrase": "evaluation_process"}, {"score": 0.0025578967205807843, "phrase": "text_ir"}], "paper_keywords": ["Music Information Retrieval", " Text Information Retrieval", " Evaluation and experimentation", " Survey"], "paper_abstract": "The field of Music Information Retrieval has always acknowledged the need for rigorous scientific evaluations, and several efforts have set out to develop and provide the infrastructure, technology and methodologies needed to carry out these evaluations. The community has enormously gained from these evaluation forums, but we have reached a point where we are stuck with evaluation frameworks that do not allow us to improve as much and as well as we want. The community recently acknowledged this problem and showed interest in addressing it, though it is not clear what to do to improve the situation. We argue that a good place to start is again the Text IR field. Based on a formalization of the evaluation process, this paper presents a survey of past evaluation work in the context of Text IR, from the point of view of validity, reliability and efficiency of the experiments. We show the problems that our community currently has in terms of evaluation, point to several lines of research to improve it and make various proposals in that line.", "paper_title": "Evaluation in Music Information Retrieval", "paper_id": "WOS:000326932800002"}