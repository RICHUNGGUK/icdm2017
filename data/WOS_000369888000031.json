{"auto_keywords": [{"score": 0.03430340038332922, "phrase": "n_samples"}, {"score": 0.00481495049065317, "phrase": "minimax_optimal_rates"}, {"score": 0.004695183699894267, "phrase": "decomposition-based_scalable_approach"}, {"score": 0.004442032546224076, "phrase": "minimax_optimal_convergence_rates"}, {"score": 0.004397487039229447, "phrase": "relatively_mild_conditions"}, {"score": 0.003955787027818305, "phrase": "equal_size"}, {"score": 0.003876806615011266, "phrase": "independent_kernel_ridge_regression_estimator"}, {"score": 0.00376127170364341, "phrase": "careful_choice"}, {"score": 0.003704797078215222, "phrase": "regularization_parameter"}, {"score": 0.0036125439380579626, "phrase": "local_solutions"}, {"score": 0.003558294449586273, "phrase": "global_predictor"}, {"score": 0.0034348484776384643, "phrase": "substantial_reduction"}, {"score": 0.0034003686971964707, "phrase": "computation_time"}, {"score": 0.0033492947290751996, "phrase": "standard_approach"}, {"score": 0.0032989853590079153, "phrase": "kernel_ridge_regression"}, {"score": 0.0031366648074858555, "phrase": "computational_speed-up"}, {"score": 0.002878785759716458, "phrase": "partition-based_estimator"}, {"score": 0.0028355237433457313, "phrase": "statistical_minimax_rate"}, {"score": 0.002682370021834353, "phrase": "concrete_examples"}, {"score": 0.002486734224234698, "phrase": "finite-rank_or_gaussian_kernels"}, {"score": 0.0024125253170522816, "phrase": "sobolev_spaces"}, {"score": 0.0023287359930308864, "phrase": "substantial_reductions"}, {"score": 0.0023053339328456234, "phrase": "computational_cost"}, {"score": 0.002214048743052355, "phrase": "simulated_data"}, {"score": 0.0021807544600219216, "phrase": "music-prediction_task"}, {"score": 0.0021049977753042253, "phrase": "computational_and_statistical_benefits"}], "paper_keywords": ["kernel ridge regression", " divide and conquer", " computation complexity"], "paper_abstract": "We study a decomposition-based scalable approach to kernel ridge regression, and show that it achieves minimax optimal convergence rates under relatively mild conditions. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset using a careful choice of the regularization parameter, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of subsets m may grow nearly linearly for finite-rank or Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach.", "paper_title": "Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates", "paper_id": "WOS:000369888000031"}