{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "vector_perceptions"}, {"score": 0.004565007074039171, "phrase": "support_vector_machines"}, {"score": 0.00421410521089045, "phrase": "pattern_classification_applications"}, {"score": 0.0038900709793274484, "phrase": "standard_sigmoidal_kernel_definition"}, {"score": 0.003404244237372396, "phrase": "improved_svm"}, {"score": 0.003314591878914152, "phrase": "sigmoidal_kernel"}, {"score": 0.0027010719929751, "phrase": "proposed_svp"}, {"score": 0.002470987929071079, "phrase": "maximal_margin_solutions"}, {"score": 0.002220551141101661, "phrase": "classical_multilayer_perceptrons"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["support vector", " sigmoidal", " kernel", " multilayer", " perceptron"], "paper_abstract": "Due to their excellent performance, support vector machines (SVMs) are now used extensively in pattern classification applications. In this paper we show that the standard sigmoidal kernel definition lacks the capability to represent the family of perceptrons, and we propose an improved SVM with a sigmoidal kernel called support vector perceptron (SVP). We show by means of both synthetic and real world data sets that the proposed SVP is able to provide very accurate results in many classification problems, providing maximal margin solutions when classes are separable, and also producing very compact architectures comparable to classical multilayer perceptrons. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Support vector perceptions", "paper_id": "WOS:000244137700054"}