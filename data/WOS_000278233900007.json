{"auto_keywords": [{"score": 0.04691829787430935, "phrase": "top-down_attention"}, {"score": 0.00481495049065317, "phrase": "task-driven_object-based_visual_attention_control"}, {"score": 0.004719359108879602, "phrase": "biologically-motivated_computational_model"}, {"score": 0.004662913362513954, "phrase": "task-driven_and_object-based_visual_attention_control"}, {"score": 0.004303404058571052, "phrase": "desired_object"}, {"score": 0.0041842092128678105, "phrase": "bottom-up_attention"}, {"score": 0.004084662575167727, "phrase": "need-based_and_object-driven_state_representation"}, {"score": 0.003845994935767022, "phrase": "early_visual_processing_layer"}, {"score": 0.003694652993546994, "phrase": "biased_saliency-based_bottom-up_model"}, {"score": 0.003592258384936271, "phrase": "cognitive_component"}, {"score": 0.003549245218355768, "phrase": "higher_visual_processing_layer"}, {"score": 0.0035067452724203124, "phrase": "application_specific_operation"}, {"score": 0.003478694092089197, "phrase": "object_recognition"}, {"score": 0.0032491108935498794, "phrase": "decision_making"}, {"score": 0.003121183062764093, "phrase": "u-tree_algorithm"}, {"score": 0.0030591147406390986, "phrase": "object-based_binary_tree"}, {"score": 0.002915125480162171, "phrase": "specific_object"}, {"score": 0.0028342734482926677, "phrase": "early_vision"}, {"score": 0.0028003107731426322, "phrase": "object_recognition_parts"}, {"score": 0.002711714675297321, "phrase": "action_value_table"}, {"score": 0.0026900059414552823, "phrase": "motor_actions"}, {"score": 0.0025840347813610815, "phrase": "motor_action"}, {"score": 0.0025224615049571427, "phrase": "reinforcement_signal"}, {"score": 0.0023558375480288297, "phrase": "action_selection_policy"}, {"score": 0.002327594516681507, "phrase": "proposed_model"}, {"score": 0.0022904619303011097, "phrase": "visual_navigation_tasks"}, {"score": 0.0022630008787375435, "phrase": "obtained_results"}, {"score": 0.0021738146596180404, "phrase": "developed_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Task-driven attention", " Object-based attention", " Top-down attention", " Saliency-based model", " Reinforcement learning", " State space discretization"], "paper_abstract": "We propose a biologically-motivated computational model for learning task-driven and object-based visual attention control in interactive environments. In this model, top-down attention is learned interactively and is used to search for a desired object in the scene through biasing the bottom-up attention in order to form a need-based and object-driven state representation of the environment. Our model consists of three layers. First, in the early visual processing layer, most salient location of a scene is derived using the biased saliency-based bottom-up model of visual attention. Then a cognitive component in the higher visual processing layer performs an application specific operation like object recognition at the focus of attention. From this information, a state is derived in the decision making and learning layer. Top-down attention is learned by the U-TREE algorithm which successively grows an object-based binary tree. Internal nodes in this tree check the existence of a specific object in the scene by biasing the early vision and the object recognition parts. Its leaves point to states in the action value table. Motor actions are associated with the leaves. After performing a motor action, the agent receives a reinforcement signal from the critic. This signal is alternately used for modifying the tree or updating the action selection policy. The proposed model is evaluated on visual navigation tasks, where obtained results lend support to the applicability and usefulness of the developed method for robotics. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Online learning of task-driven object-based visual attention control", "paper_id": "WOS:000278233900007"}