{"auto_keywords": [{"score": 0.044044107045576514, "phrase": "local_features"}, {"score": 0.00481495049065317, "phrase": "sift_match"}, {"score": 0.004752873752686246, "phrase": "geometric_coding"}, {"score": 0.00471193241675062, "phrase": "large-scale_partial-duplicate_web_image_search"}, {"score": 0.004454237726525018, "phrase": "bag-of-visual-words_model"}, {"score": 0.004358903464536199, "phrase": "traditional_bag-of-visual-words_model"}, {"score": 0.0040672766251859975, "phrase": "important_role"}, {"score": 0.00403221710768334, "phrase": "image_retrieval"}, {"score": 0.003911862375084159, "phrase": "geometric_context"}, {"score": 0.0038613835226952117, "phrase": "visual_words"}, {"score": 0.003795086335272422, "phrase": "efficient_global_geometric_verification_methods"}, {"score": 0.003634262647873338, "phrase": "current_existing_methods"}, {"score": 0.003602922164562919, "phrase": "global_geometric_verification"}, {"score": 0.0034953361767153285, "phrase": "real-time_response"}, {"score": 0.0033182992967343916, "phrase": "preceding_problems"}, {"score": 0.00319141242685261, "phrase": "novel_geometric_coding_algorithm"}, {"score": 0.0031230216871739776, "phrase": "spatial_context"}, {"score": 0.0030693625816617044, "phrase": "large-scale_partial-duplicate_web_image_retrieval"}, {"score": 0.0029776614744264724, "phrase": "geometric_square_coding"}, {"score": 0.0029519664912853938, "phrase": "geometric_fan_coding"}, {"score": 0.002888692115811987, "phrase": "spatial_relationships"}, {"score": 0.002863762666256937, "phrase": "sift_features"}, {"score": 0.002766171878646582, "phrase": "geometrically_inconsistent_sift_matches"}, {"score": 0.0025920406425973368, "phrase": "partial-duplicate_images"}, {"score": 0.0025474814283429213, "phrase": "scale_changes"}, {"score": 0.0024928553725344933, "phrase": "background_clutter"}, {"score": 0.002449997042686432, "phrase": "partial-duplicate_web_image_search"}, {"score": 0.0021982976170703884, "phrase": "ransac_verification"}, {"score": 0.002179313258573377, "phrase": "mean_average_precision"}, {"score": 0.0021233361369222344, "phrase": "comparable_performance"}], "paper_keywords": ["Algorithms", " Experimentation", " Verification", " Image retrieval", " partial duplicate", " large scale", " rotation-invariant", " geometric square coding", " geometric fan coding"], "paper_abstract": "Most large-scale image retrieval systems are based on the bag-of-visual-words model. However, the traditional bag-of-visual-words model does not capture the geometric context among local features in images well, which plays an important role in image retrieval. In order to fully explore geometric context of all visual words in images, efficient global geometric verification methods have been attracting lots of attention. Unfortunately, current existing methods on global geometric verification are either computationally expensive to ensure real-time response, or cannot handle rotation well. To solve the preceding problems, in this article, we propose a novel geometric coding algorithm, to encode the spatial context among local features for large-scale partial-duplicate Web image retrieval. Our geometric coding consists of geometric square coding and geometric fan coding, which describe the spatial relationships of SIFT features into three geo-maps for global verification to remove geometrically inconsistent SIFT matches. Our approach is not only computationally efficient, but also effective in detecting partial-duplicate images with rotation, scale changes, partial-occlusion, and background clutter. Experiments in partial-duplicate Web image search, using two datasets with one million Web images as distractors, reveal that our approach outperforms the baseline bag-of-visual-words approach even following a RANSAC verification in mean average precision. Besides, our approach achieves comparable performance to other state-of-the-art global geometric verification methods, for example, spatial coding scheme, but is more computationally efficient.", "paper_title": "SIFT Match Verification by Geometric Coding for Large-Scale Partial-Duplicate Web Image Search", "paper_id": "WOS:000315457000004"}