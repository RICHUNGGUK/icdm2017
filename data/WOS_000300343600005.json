{"auto_keywords": [{"score": 0.043982673134435366, "phrase": "software_engineering"}, {"score": 0.03342878232776224, "phrase": "dataset_shift"}, {"score": 0.00481495049065317, "phrase": "dataset_shift_problem"}, {"score": 0.004753194996637899, "phrase": "software_engineering_prediction_models"}, {"score": 0.004662036750452837, "phrase": "core_assumption"}, {"score": 0.0045726187330434025, "phrase": "prediction_model"}, {"score": 0.004484908028307205, "phrase": "test_data_distribution"}, {"score": 0.004342430078306461, "phrase": "training_data_distribution"}, {"score": 0.004286708921931472, "phrase": "prediction_models"}, {"score": 0.003767203575060969, "phrase": "inconsistent_and_non-transferrable_observations"}, {"score": 0.0033971539563683174, "phrase": "conclusion_instability"}, {"score": 0.003331915219811852, "phrase": "dataset_shift_concept"}, {"score": 0.003289117944111902, "phrase": "software_effort"}, {"score": 0.0032468685934998335, "phrase": "fault_prediction_perspective"}, {"score": 0.0032051601987130207, "phrase": "different_types"}, {"score": 0.002927780883930371, "phrase": "associated_problems"}, {"score": 0.0028346293790895024, "phrase": "dataset_shifts"}, {"score": 0.0027444334639134217, "phrase": "sample_selection_bias"}, {"score": 0.002709161672473817, "phrase": "imbalanced_data"}, {"score": 0.0026060359255636444, "phrase": "software_engineering_research"}, {"score": 0.0024906610581090223, "phrase": "possible_interpretations"}, {"score": 0.00244278788608393, "phrase": "non-transferable_results"}, {"score": 0.0023346234464901978, "phrase": "software_engineering_community"}, {"score": 0.0021883400011393564, "phrase": "related_issues"}, {"score": 0.0021049977753042253, "phrase": "research_outcomes"}], "paper_keywords": ["Dataset shift", " Prediction models", " Effort estimation", " Defect prediction"], "paper_abstract": "A core assumption of any prediction model is that test data distribution does not differ from training data distribution. Prediction models used in software engineering are no exception. In reality, this assumption can be violated in many ways resulting in inconsistent and non-transferrable observations across different cases. The goal of this paper is to explain the phenomena of conclusion instability through the dataset shift concept from software effort and fault prediction perspective. Different types of dataset shift are explained with examples from software engineering, and techniques for addressing associated problems are discussed. While dataset shifts in the form of sample selection bias and imbalanced data are well-known in software engineering research, understanding other types is relevant for possible interpretations of the non-transferable results across different sites and studies. Software engineering community should be aware of and account for the dataset shift related issues when evaluating the validity of research outcomes.", "paper_title": "On the dataset shift problem in software engineering prediction models", "paper_id": "WOS:000300343600005"}