{"auto_keywords": [{"score": 0.049593242225692294, "phrase": "non-negative_matrix_factorization"}, {"score": 0.047002907981885236, "phrase": "small_number"}, {"score": 0.04432396220512619, "phrase": "latent_vectors"}, {"score": 0.04117972994186605, "phrase": "training_data"}, {"score": 0.036229190503611196, "phrase": "enrollment_data"}, {"score": 0.00481495049065317, "phrase": "latent_speaker_space"}, {"score": 0.0046937237797352515, "phrase": "novel_speaker_adaptation_algorithm"}, {"score": 0.004622450915803531, "phrase": "gaussian_mixture_weight_adaptation"}, {"score": 0.004460309246062286, "phrase": "latent_speaker_vectors"}, {"score": 0.004325933404228238, "phrase": "nmf"}, {"score": 0.004174064076143015, "phrase": "distinctive_systematic_patterns"}, {"score": 0.0041316803676012155, "phrase": "gaussian_usage"}, {"score": 0.004027586827290042, "phrase": "individual_speakers"}, {"score": 0.003807684498435389, "phrase": "gaussian_mixture_weights"}, {"score": 0.0037498152007206815, "phrase": "linear_combination"}, {"score": 0.0033857878144822906, "phrase": "resulting_fast_adaptation_algorithm"}, {"score": 0.0032336801561165113, "phrase": "similar_performance"}, {"score": 0.003025915966640355, "phrase": "richer_gaussian_usage_patterns"}, {"score": 0.002934566917795928, "phrase": "nmf-based_weight_adaptation"}, {"score": 0.002875200380397413, "phrase": "vocal_tract_length_normalization"}, {"score": 0.0028459677232555176, "phrase": "vtln"}, {"score": 0.0027883885425383534, "phrase": "adaptive_training"}, {"score": 0.002676692093594358, "phrase": "simple_gaussian_exponentiation_scheme"}, {"score": 0.002622528664187849, "phrase": "dynamic_range"}, {"score": 0.0025826247025542235, "phrase": "gaussian_likelihoods"}, {"score": 0.0025174594088331853, "phrase": "wall_street_journal_tasks"}, {"score": 0.002392008392892289, "phrase": "speaker_independent_recognition_system"}, {"score": 0.002319758345900309, "phrase": "wer"}, {"score": 0.0022382002476510573, "phrase": "weight_adaptation"}, {"score": 0.0022154296909169826, "phrase": "gaussian_mean_adaptation"}, {"score": 0.0021705796895687864, "phrase": "eigenvoice_speaker_adaptation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speaker adaptation", " NMF", " SAT", " fMLLR", " Eigenvoice"], "paper_abstract": "A novel speaker adaptation algorithm based on Gaussian mixture weight adaptation is described. A small number of latent speaker vectors are estimated with non-negative matrix factorization (NMF). These latent vectors encode the distinctive systematic patterns of Gaussian usage observed when modeling the individual speakers that make up the training data. Expressing the speaker dependent Gaussian mixture weights as a linear combination of a small number of latent vectors reduces the number of parameters that must be estimated from the enrollment data. The resulting fast adaptation algorithm, using 3 s of enrollment data only, achieves similar performance as fMLLR adapting on 100+ s of data. In order to learn richer Gaussian usage patterns from the training data, the NMF-based weight adaptation is combined with vocal tract length normalization (VTLN) and speaker adaptive training (SAT), or with a simple Gaussian exponentiation scheme that lowers the dynamic range of the Gaussian likelihoods. Evaluation on the Wall Street Journal tasks shows a 5% relative word error rate (WER) reduction over the speaker independent recognition system which already incorporates VTLN. The WER can be lowered further by combining weight adaptation with Gaussian mean adaptation by means of eigenvoice speaker adaptation. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Rapid speaker adaptation in latent speaker space with non-negative matrix factorization", "paper_id": "WOS:000322294400002"}