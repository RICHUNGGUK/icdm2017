{"auto_keywords": [{"score": 0.048068785010855715, "phrase": "keyframe_selection_method"}, {"score": 0.035736192837356996, "phrase": "varying-content_translation_patterns"}, {"score": 0.00481495049065317, "phrase": "user-generated_video_search-based_annotation"}, {"score": 0.004667149874736771, "phrase": "temporal_segmentation"}, {"score": 0.004577076689777069, "phrase": "user-generated_video"}, {"score": 0.0044712735637757175, "phrase": "ugv"}, {"score": 0.004333973355873714, "phrase": "user's_interest"}, {"score": 0.004266907250334316, "phrase": "camera_movements"}, {"score": 0.004217286731331736, "phrase": "ugv_temporal_segmentation_system"}, {"score": 0.004103728794623367, "phrase": "video_partition"}, {"score": 0.004040211152784802, "phrase": "camera_motion_classification"}, {"score": 0.004008820473628357, "phrase": "motion-related_mid-level_features"}, {"score": 0.003900853871728297, "phrase": "hierarchical_hidden_markov_model"}, {"score": 0.0037957839728524046, "phrase": "user-meaningful_ugv_temporal_segmentation"}, {"score": 0.0035940277928341265, "phrase": "fixed-content_camera_motion_patterns"}, {"score": 0.0033502512757488433, "phrase": "proposed_video_segmentation_approach"}, {"score": 0.0032727116186194584, "phrase": "state-of-the-art_algorithm"}, {"score": 0.0031721009574803127, "phrase": "segmentation-based_evaluation"}, {"score": 0.0031107911390188055, "phrase": "complete_search-based_ugv_annotation_system"}, {"score": 0.0029916929549256297, "phrase": "proposed_algorithms"}, {"score": 0.002956858061485755, "phrase": "end-user_task"}, {"score": 0.0027240799841720957, "phrase": "considered_camera_motion_types"}, {"score": 0.002559072326933071, "phrase": "desired_performance-complexity_tradeoff"}, {"score": 0.0025292620341476283, "phrase": "keyframe_selection_algorithm"}, {"score": 0.00243237365704391, "phrase": "notable_contribution"}, {"score": 0.002376027109070317, "phrase": "global_ugv_annotation_system"}, {"score": 0.00228499525153, "phrase": "ugv_segmentation_algorithm"}, {"score": 0.0022583704173318123, "phrase": "improved_annotation_results"}, {"score": 0.0022146817879683204, "phrase": "fixed-rate_keyframe_selection_baseline"}, {"score": 0.002154930852763048, "phrase": "frame-level_visual_features"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["User Generated Video", " Video annotation", " Video temporal segmentation", " Camera motion analysis", " Keyframe selection"], "paper_abstract": "In this paper we propose a temporal segmentation and a keyframe selection method for User-Generated Video (UGV). Since UGV is rarely structured in shots and usually user's interest are revealed through camera movements, a UGV temporal segmentation system has been proposed that generates a video partition based on a camera motion classification. Motion-related mid-level features have been suggested to feed a Hierarchical Hidden Markov Model (HHMM) that produces a user-meaningful UGV temporal segmentation. Moreover, a keyframe selection method has been proposed that picks a keyframe for fixed-content camera motion patterns such as zoom, still, or shake and a set of keyframes for varying-content translation patterns. The proposed video segmentation approach has been compared to a state-of-the-art algorithm, achieving 8% performance improvement in a segmentation-based evaluation. Furthermore, a complete search-based UGV annotation system has been developed to assess the influence of the proposed algorithms on an end-user task. To that purpose, two UGV datasets have been developed and made available online. Specifically, the relevance of the considered camera motion types has been analyzed for these two datasets, and some guidelines are given to achieve the desired performance-complexity tradeoff. The keyframe selection algorithm for varying-content translation patterns has also been assessed, revealing a notable contribution to the performance of the global UGV annotation system. Finally, it has been shown that the UGV segmentation algorithm also produces improved annotation results with respect to a fixed-rate keyframe selection baseline or a traditional method relying on frame-level visual features. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Temporal segmentation and keyframe selection methods for user-generated video search-based annotation", "paper_id": "WOS:000344034300040"}