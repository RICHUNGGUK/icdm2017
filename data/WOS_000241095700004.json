{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "data_streams"}, {"score": 0.04360247103779726, "phrase": "basic_classifiers"}, {"score": 0.04325760434994917, "phrase": "training_data"}, {"score": 0.0046392209103157936, "phrase": "growing_number"}, {"score": 0.004601045179623499, "phrase": "emerging_application_domains"}, {"score": 0.0044514440242754815, "phrase": "active_research_area"}, {"score": 0.004360412703519539, "phrase": "typical_approach"}, {"score": 0.004236074477264556, "phrase": "ensemble_learning"}, {"score": 0.004047781270412204, "phrase": "global_predictor"}, {"score": 0.0039813976794342, "phrase": "basic_ones"}, {"score": 0.003560807940070886, "phrase": "sufficient_training_data"}, {"score": 0.0034306925765832633, "phrase": "bias-variance_reduction"}, {"score": 0.0033190180867511605, "phrase": "significant_sensitivity"}, {"score": 0.0031977096047057898, "phrase": "recent_training_data"}, {"score": 0.003171358146412577, "phrase": "up-to-date_individual_classifiers"}, {"score": 0.0028715299874549245, "phrase": "unsatisfactory_classification_accuracy"}, {"score": 0.00276653157720066, "phrase": "ensemble_learning_algorithm"}, {"score": 0.0025999744005191713, "phrase": "up-to-date_data_chunk"}, {"score": 0.0024535580495414783, "phrase": "current_concept"}, {"score": 0.002393353846080347, "phrase": "effective_voting"}, {"score": 0.0023540387251475615, "phrase": "sensible_classifiers"}, {"score": 0.0022962708568492734, "phrase": "sensible_ones"}, {"score": 0.002258546887303777, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "severe_circumstances"}], "paper_keywords": [""], "paper_abstract": "As data streams are gaining prominence in a growing number of emerging application domains, classification on data streams is becoming an active research area. Currently, the typical approach to this problem is based on ensemble learning, which learns basic classifiers from training data stream and forms the global predictor by organizing these basic ones. While this approach seems successful to some extent, its performance usually suffers from two contradictory elements existing naturally within many application scenarios: firstly, the need for gathering sufficient training data for basic classifiers and engaging enough basic learners in voting for bias-variance reduction; and secondly, the requirement for significant sensitivity to concept-drifts, which places emphasis on using recent training data and up-to-date individual classifiers. It results in such a dilemma that some algorithms are not sensitive enough to concept-drifts while others, although sensitive enough, suffer from unsatisfactory classification accuracy. In this paper, we propose an ensemble learning algorithm, which: (1) furnishes training data for basic classifiers, starting from the up-to-date data chunk and searching for complement from past chunks while ruling out the data inconsistent with current concept; (2) provides effective voting by adaptively distinguishing sensible classifiers from the else and engaging sensible ones as voters. Experimental results justify the superiority of this strategy in terms of both accuracy and sensitivity, especially in severe circumstances where training data is extremely insufficient or concepts are evolving frequently and significantly.", "paper_title": "An automatic construction and organization strategy for ensemble learning on data streams", "paper_id": "WOS:000241095700004"}