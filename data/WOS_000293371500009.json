{"auto_keywords": [{"score": 0.041229409204698, "phrase": "pagerank"}, {"score": 0.036299765223803376, "phrase": "specific_topics"}, {"score": 0.00481495049065317, "phrase": "evaluation_metrics"}, {"score": 0.004766388573192505, "phrase": "biomedical_journals"}, {"score": 0.00441738161338662, "phrase": "biomedical_literature"}, {"score": 0.00437281181878921, "phrase": "health-related_websites"}, {"score": 0.004263336444795021, "phrase": "information_retrieval_tasks"}, {"score": 0.004220314322073171, "phrase": "current_commonly_used_methods"}, {"score": 0.004073113268289532, "phrase": "pubmed's_clinical_query_filters"}, {"score": 0.004032002950217453, "phrase": "machine_learning-based_filter_models"}, {"score": 0.0038520615699124123, "phrase": "previous_work"}, {"score": 0.0037555742428544096, "phrase": "average_performance"}, {"score": 0.003410380718649286, "phrase": "focused_searches"}, {"score": 0.003375961352495013, "phrase": "clinicians"}, {"score": 0.0032088487124106936, "phrase": "expected_performance"}, {"score": 0.0030811340338342454, "phrase": "present_work"}, {"score": 0.002884317729022721, "phrase": "impact_factor"}, {"score": 0.0028551716387011637, "phrase": "clinical_query_filters"}, {"score": 0.0027554538693034163, "phrase": "different_topics"}, {"score": 0.002713788077665689, "phrase": "topic-specific_impact_factor_and_machine_learning-based_filter_models"}, {"score": 0.0024023079401125492, "phrase": "narrower_topics"}, {"score": 0.0023780209525032688, "phrase": "topic-adjusted_metrics"}, {"score": 0.0023301793939902015, "phrase": "robust_methods"}, {"score": 0.002226017732566121, "phrase": "traditional_topic-sensitive_metrics"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Information retrieval", " Machine learning", " PageRank", " Journal impact factor", " Topic-sensitivity", " Bibliometrics"], "paper_abstract": "Evaluating the biomedical literature and health-related websites for quality are challenging information retrieval tasks. Current commonly used methods include impact factor for journals, PubMed's clinical query filters and machine learning-based filter models for articles, and PageRank for websites. Previous work has focused on the average performance of these methods without considering the topic, and it is unknown how performance varies for specific topics or focused searches. Clinicians, researchers, and users should be aware when expected performance is not achieved for specific topics. The present work analyzes the behavior of these methods for a variety of topics. Impact factor, clinical query filters, and PageRank vary widely across different topics while a topic-specific impact factor and machine learning-based filter models are more stable. The results demonstrate that a method may perform excellently on average but struggle when used on a number of narrower topics. Topic-adjusted metrics and other topic robust methods have an advantage in such situations. Users of traditional topic-sensitive metrics should be aware of their limitations. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "A comparison of evaluation metrics for biomedical journals, articles, and websites in terms of sensitivity to topic", "paper_id": "WOS:000293371500009"}