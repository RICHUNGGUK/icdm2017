{"auto_keywords": [{"score": 0.0441178968640138, "phrase": "different_views"}, {"score": 0.03222741375320347, "phrase": "incomplete-view_problem"}, {"score": 0.03087319757456138, "phrase": "incomplete_views"}, {"score": 0.02888807132631179, "phrase": "multi-view_learning"}, {"score": 0.00481495049065317, "phrase": "multi-view_learning_with_incomplete_views"}, {"score": 0.004719359108879602, "phrase": "conventional_multi-view_learning_algorithms"}, {"score": 0.004035777314816881, "phrase": "incomplete-view_setting"}, {"score": 0.0033822643082593285, "phrase": "random_missing_variables"}, {"score": 0.0032621876512517398, "phrase": "concentrated_missing_variables"}, {"score": 0.0031845067058084613, "phrase": "missing_views"}, {"score": 0.0029623549392492777, "phrase": "multiple_views"}, {"score": 0.0028115863316685937, "phrase": "complete_views"}, {"score": 0.0027556576784848207, "phrase": "effective_algorithm"}, {"score": 0.0025944417620244924, "phrase": "shared_subspace"}, {"score": 0.00254282157860369, "phrase": "large-scale_problem"}, {"score": 0.002512342528545883, "phrase": "fast_convergence"}, {"score": 0.0024623517958288228, "phrase": "successive_over-relaxation_method"}, {"score": 0.0024230746206056536, "phrase": "objective_function"}, {"score": 0.002374855916752648, "phrase": "optimization_technique"}, {"score": 0.0023182554418193927, "phrase": "experimental_results"}, {"score": 0.002299689297293321, "phrase": "toy_data"}, {"score": 0.0022812715029149216, "phrase": "real-world_data_sets"}, {"score": 0.002165091261117016, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "different_applications"}], "paper_keywords": ["Multi-view learning", " incomplete views"], "paper_abstract": "One underlying assumption of the conventional multi-view learning algorithms is that all examples can be successfully observed on all the views. However, due to various failures or faults in collecting and pre-processing the data on different views, we are more likely to be faced with an incomplete-view setting, where an example could be missing its representation on one view (i.e., missing view) or could be only partially observed on that view (i.e., missing variables). Low-rank assumption used to be effective for recovering the random missing variables of features, but it is disabled by concentrated missing variables and has no effect on missing views. This paper suggests that the key to handling the incomplete-view problem is to exploit the connections between multiple views, enabling the incomplete views to be restored with the help of the complete views. We propose an effective algorithm to accomplish multi-view learning with incomplete views by assuming that different views are generated from a shared subspace. To handle the large-scale problem and obtain fast convergence, we investigate a successive over-relaxation method to solve the objective function. Convergence of the optimization technique is theoretically analyzed. The experimental results on toy data and real-world data sets suggest that studying the incomplete-view problem in multi-view learning is significant and that the proposed algorithm can effectively handle the incomplete views in different applications.", "paper_title": "Multi-View Learning With Incomplete Views", "paper_id": "WOS:000369538200005"}