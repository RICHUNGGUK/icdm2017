{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "large_numerical_data_volumes"}, {"score": 0.0047468516620285525, "phrase": "contradictory_requirements"}, {"score": 0.004701985831087475, "phrase": "data_privacy"}, {"score": 0.004657542074154025, "phrase": "data_analysis"}, {"score": 0.00452670409375785, "phrase": "statistical_disclosure_control_techniques"}, {"score": 0.004195418010131832, "phrase": "good_trade-off"}, {"score": 0.00398177607914381, "phrase": "currently_available_microaggregation_algorithms"}, {"score": 0.0038515000499988673, "phrase": "small_datasets"}, {"score": 0.0037432199549135826, "phrase": "current_databases"}, {"score": 0.0036379729015583975, "phrase": "usual_way"}, {"score": 0.0035022161192936234, "phrase": "large_data_volumes"}, {"score": 0.0034690732089830045, "phrase": "smaller_fragments"}, {"score": 0.003371508176308189, "phrase": "reasonable_time"}, {"score": 0.0033395980635636644, "phrase": "available_algorithms"}, {"score": 0.003036595866130994, "phrase": "computational_needs"}, {"score": 0.0027741761880779535, "phrase": "vantage_point"}, {"score": 0.002708960273193547, "phrase": "k-nearest_neighbors_searches"}, {"score": 0.002657890175649468, "phrase": "new_point"}, {"score": 0.0024984517279573906, "phrase": "search_techniques"}, {"score": 0.0022287497627555895, "phrase": "previous_techniques"}, {"score": 0.002186712881521657, "phrase": "better_balance"}, {"score": 0.0021049977753042253, "phrase": "anonymized_dataset"}], "paper_keywords": ["Microaggregation", " Statistical disclosure control", " Large data volumes", " k-nearest neighbors search"], "paper_abstract": "The contradictory requirements of data privacy and data analysis have fostered the development of statistical disclosure control techniques. In this context, microaggregation is one of the most frequently used methods since it offers a good trade-off between simplicity and quality. Unfortunately, most of the currently available microaggregation algorithms have been devised to work with small datasets, while the size of current databases is constantly increasing. The usual way to tackle this problem is to partition large data volumes into smaller fragments that can be processed in reasonable time by available algorithms. This solution is applied at the cost of losing quality. In this paper, we revisited the computational needs of microaggregation showing that it can be reduced to two steps: sorting the dataset with regard to a vantage point and a set of k-nearest neighbors searches. Considering this new point of view, we propose three new efficient quality-preserving microaggregation algorithms based on k-nearest neighbors search techniques. We present a comparison of our approaches with the most significant strategies presented in the literature using three real very large datasets. Experimental results show that our proposals overcome previous techniques by keeping a better balance between performance and the quality of the anonymized dataset.", "paper_title": "Efficient microaggregation techniques for large numerical data volumes", "paper_id": "WOS:000306682900004"}