{"auto_keywords": [{"score": 0.04775829835465016, "phrase": "delay_conditioning"}, {"score": 0.01332832052798967, "phrase": "cerebral_cortex"}, {"score": 0.011256822185634143, "phrase": "trace_conditioning"}, {"score": 0.009537774812691397, "phrase": "conditioned_stimulus"}, {"score": 0.00481495049065317, "phrase": "long_short-term_memory_networks"}, {"score": 0.004696909870731282, "phrase": "temporal-difference_learning_algorithm"}, {"score": 0.004469400468609653, "phrase": "fixed_temporal_representation"}, {"score": 0.004437807397753662, "phrase": "elapsed_time"}, {"score": 0.004406436661912347, "phrase": "conditioned_stimulus_onset"}, {"score": 0.0043289723873340255, "phrase": "new_model"}, {"score": 0.0041780882724537, "phrase": "long_short-term_memory"}, {"score": 0.004119211418949265, "phrase": "artificial_neural_network"}, {"score": 0.003975609222669783, "phrase": "j_comput_neurosci"}, {"score": 0.003796355951305572, "phrase": "model's_ability"}, {"score": 0.0037295765964056255, "phrase": "relevant_data"}, {"score": 0.0036251554804124066, "phrase": "interesting_new_predictions"}, {"score": 0.003511159930253876, "phrase": "strikingly_different_temporal_representation"}, {"score": 0.003412833120814178, "phrase": "working_memory"}, {"score": 0.003201516237269705, "phrase": "important_difference"}, {"score": 0.0031788569040714434, "phrase": "da_responses"}, {"score": 0.002950374774004939, "phrase": "animal_timing"}, {"score": 0.002827243416398972, "phrase": "classical_conditioning"}, {"score": 0.0026333078412549807, "phrase": "second_reward"}, {"score": 0.0025869338215971536, "phrase": "last_simulation"}, {"score": 0.0024439412207268355, "phrase": "new_delays"}, {"score": 0.0022762383517229957, "phrase": "proposed_architecture"}, {"score": 0.0022441016229967025, "phrase": "discharge_patterns"}, {"score": 0.0021967433763351884, "phrase": "dopaminergic_neurons"}, {"score": 0.0021049977753042253, "phrase": "predictive_cost_function"}], "paper_keywords": ["Time representation learning", " Temporal-difference learning", " Long short-term memory networks", " Dopamine", " Conditioning", " Reinforcement learning"], "paper_abstract": "Dopaminergic models based on the temporal-difference learning algorithm usually do not differentiate trace from delay conditioning. Instead, they use a fixed temporal representation of elapsed time since conditioned stimulus onset. Recently, a new model was proposed in which timing is learned within a long short-term memory (LSTM) artificial neural network representing the cerebral cortex (Rivest et al. in J Comput Neurosci 28(1):107-130, 2010). In this paper, that model's ability to reproduce and explain relevant data, as well as its ability to make interesting new predictions, are evaluated. The model reveals a strikingly different temporal representation between trace and delay conditioning since trace conditioning requires working memory to remember the past conditioned stimulus while delay conditioning does not. On the other hand, the model predicts no important difference in DA responses between those two conditions when trained on one conditioning paradigm and tested on the other. The model predicts that in trace conditioning, animal timing starts with the conditioned stimulus offset as opposed to its onset. In classical conditioning, it predicts that if the conditioned stimulus does not disappear after the reward, the animal may expect a second reward. Finally, the last simulation reveals that the buildup of activity of some units in the networks can adapt to new delays by adjusting their rate of integration. Most importantly, the paper shows that it is possible, with the proposed architecture, to acquire discharge patterns similar to those observed in dopaminergic neurons and in the cerebral cortex on those tasks simply by minimizing a predictive cost function.", "paper_title": "Conditioning and time representation in long short-term memory networks", "paper_id": "WOS:000330962400002"}