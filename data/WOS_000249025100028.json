{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mutual_information"}, {"score": 0.046273829908275756, "phrase": "kolmogorov_distance"}, {"score": 0.003459191419478298, "phrase": "coupling_technique"}, {"score": 0.0025892330566358503, "phrase": "upper_bounds"}, {"score": 0.0021049977753042253, "phrase": "finite_discrete_random_variables"}], "paper_keywords": ["Kolmogorov distance", " mutual information", " Shannon entropy"], "paper_abstract": "By use of a coupling technique, two inequalities are established which set upper bounds to the mutual information of finite discrete random variables in terms of the Kolmogorov distance (variational distance).", "paper_title": "Estimating mutual information via Kolmogorov distance", "paper_id": "WOS:000249025100028"}