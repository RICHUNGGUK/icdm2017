{"auto_keywords": [{"score": 0.005704068871235819, "phrase": "execution_time"}, {"score": 0.004658426838467567, "phrase": "global_or_collective_communication_patterns"}, {"score": 0.004564632672126357, "phrase": "extreme_scale_computers"}, {"score": 0.004506968388996092, "phrase": "grand_challenges"}, {"score": 0.004484106242427055, "phrase": "scientific_computing"}, {"score": 0.00445002931309096, "phrase": "parallel_computers"}, {"score": 0.004416210202299748, "phrase": "distributed_memory"}, {"score": 0.004393806405709151, "phrase": "different_domain_decompositions"}, {"score": 0.004197196915720539, "phrase": "best_approach"}, {"score": 0.004091780775570931, "phrase": "reasonable_data_communication_overhead"}, {"score": 0.004019562422931737, "phrase": "data_communication_approach"}, {"score": 0.003999162770142387, "phrase": "dmitruk_et_al"}, {"score": 0.003829877278269543, "phrase": "thorough_quantitative_analysis"}, {"score": 0.003800753174956217, "phrase": "code_performance"}, {"score": 0.00376226395884938, "phrase": "different_problem_sizes"}, {"score": 0.0036770770719241593, "phrase": "load_balance"}, {"score": 0.003639835714755229, "phrase": "subdomain_configuration"}, {"score": 0.0035213773396043186, "phrase": "fixed_total_number"}, {"score": 0.0034067610079173754, "phrase": "existing_attempts"}, {"score": 0.0032624854102342544, "phrase": "takahashi"}, {"score": 0.003188785207807901, "phrase": "li"}, {"score": 0.0031723676499081374, "phrase": "laizet"}, {"score": 0.0030534855146887047, "phrase": "large_problem_size"}, {"score": 0.0030379738368892096, "phrase": "large_number"}, {"score": 0.0029691377785333872, "phrase": "pekurovski's_scheme"}, {"score": 0.0028288819001337563, "phrase": "nelson_et_al"}, {"score": 0.0025678829712650437, "phrase": "different_processors"}, {"score": 0.002522492702921194, "phrase": "excellent_load_balance"}, {"score": 0.002440309168744176, "phrase": "data_copying"}, {"score": 0.0024155634012715745, "phrase": "transpose_operation"}, {"score": 0.0023789133322920384, "phrase": "relative_percentage"}, {"score": 0.002360796860462937, "phrase": "communication_time"}, {"score": 0.0022955395008397166, "phrase": "theoretical_complexity_analysis"}, {"score": 0.0022264006157232366, "phrase": "complexity_analysis"}, {"score": 0.002148360390412075, "phrase": "scalable_computers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["FFT", " 2D decomposition", " Parallel computing"], "paper_abstract": "3D FFT is computationally intensive and at the same time requires global or collective communication patterns. The efficient implementation of FFT on extreme scale computers is one of the grand challenges in scientific computing. On parallel computers with a distributed memory, different domain decompositions are possible to scale 3D FFT computation. In this paper, we argue that 2D domain decomposition is likely the best approach in terms of using a very large number of processors with reasonable data communication overhead. Specifically, we extend the data communication approach of Dmitruk et al. (2001) [21] previously used for 1D domain decomposition, to 2D domain decomposition. A thorough quantitative analysis of the code performance is undertaken for different problem sizes and numbers of processors, including scalability, load balance, dependence on subdomain configuration (i.e., different numbers of subdomain in the two decomposed directions for a fixed total number of subdomains). We show that our proposed approach is faster than the existing attempts on 2D-decomposition of 3D FFTs by Pekurovsky (2007) [23] (p3dfft), Takahashi (2009) [24], and Li and Laizet (2010) [25] (2decomp.org) especially for the case of large problem size and large number of processors (our strategy is 28% faster than Pekurovski's scheme, its closest competitor). We also show theoretically that our scheme performs better than the approach by Nelson et al. (1993) [22] up to a certain number of processors beyond which latency becomes and issue. We demonstrate that the speedup scales with the number of processors almost linearly before it saturates. The execution time on different processors differ by less than 5%, showing an excellent load balance. We further partitioned the execution time into computation, communication, and data copying related to the transpose operation, to understand how the relative percentage of the communication time increases with the number of processors. Finally, a theoretical complexity analysis is carried out to predict the scalability and its saturation. The complexity analysis indicates that the 2D domain decomposition will make it feasible to run a large 3D FFT on scalable computers with several hundred thousands processors. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Parallel implementation and scalability analysis of 3D Fast Fourier Transform using 2D domain decomposition", "paper_id": "WOS:000315557300004"}