{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "silent_data_corruption"}, {"score": 0.03354203451345424, "phrase": "soft_errors"}, {"score": 0.004758053266234474, "phrase": "data_dynamic_monitoring"}, {"score": 0.004701825207431857, "phrase": "scientific_applications"}, {"score": 0.004646258518859209, "phrase": "parallel_programming"}, {"score": 0.004483450050677258, "phrase": "best_ways"}, {"score": 0.004404188306112211, "phrase": "scientific_models"}, {"score": 0.00430067200637091, "phrase": "wide_range"}, {"score": 0.004249825947483216, "phrase": "natural_phenomena"}, {"score": 0.004174677062711731, "phrase": "complex_parallel_codes"}, {"score": 0.004028326293950714, "phrase": "large-scale_parallel_computers"}, {"score": 0.0038640303886081444, "phrase": "scientific_discovery"}, {"score": 0.0036408354779365643, "phrase": "increasing_number"}, {"score": 0.003513133457324989, "phrase": "higher_failure_rates"}, {"score": 0.0033497809969621267, "phrase": "electronic_components"}, {"score": 0.0032130703855586685, "phrase": "dramatic_rise"}, {"score": 0.0029561107046138136, "phrase": "large_inaccuracies"}, {"score": 0.002921114512115261, "phrase": "wrong_results"}, {"score": 0.0027522340188241446, "phrase": "novel_technique"}, {"score": 0.002655616553414394, "phrase": "data_monitoring"}, {"score": 0.00248718591918233, "phrase": "normal_dynamics"}, {"score": 0.0022745508619577927, "phrase": "synthetic_benchmarks"}, {"score": 0.002142970278347126, "phrase": "injected_errors"}], "paper_keywords": ["Fault Tolerance", " Supercomputers", " Silent Data Corruption", " Soft Errors", " Bit Flips", " Data Entropy"], "paper_abstract": "Parallel programming has become one of the best ways to express scientific models that simulate a wide range of natural phenomena. These complex parallel codes are deployed and executed on large-scale parallel computers, making them important tools for scientific discovery. As supercomputers get faster and larger, the increasing number of components is leading to higher failure rates. In particular, the miniaturization of electronic components is expected to lead to a dramatic rise in soft errors and data corruption. Moreover, soft errors can corrupt data silently and generate large inaccuracies or wrong results at the end of the computation. In this paper we propose a novel technique to detect silent data corruption based on data monitoring. Using this technique, an application can learn the normal dynamics of its datasets, allowing it to quickly spot anomalies. We evaluate our technique with synthetic benchmarks and we show that our technique can detect up to 50% of injected errors while incurring only negligible overhead.", "paper_title": "Detecting Silent Data Corruption through Data Dynamic Monitoring for Scientific Applications", "paper_id": "WOS:000349142100038"}