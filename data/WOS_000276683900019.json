{"auto_keywords": [{"score": 0.049217710328296, "phrase": "nixon"}, {"score": 0.015719985514319433, "phrase": "guo"}, {"score": 0.0046432244185370605, "phrase": "criterion_for_feature_subset_selection"}, {"score": 0.004357245559948974, "phrase": "alternative_options"}, {"score": 0.004088807948906792, "phrase": "feature_selection_method"}, {"score": 0.003600351201298387, "phrase": "feature_vector_x"}, {"score": 0.003535489842686145, "phrase": "class_variable"}, {"score": 0.0027407299369891502, "phrase": "feature_selection"}, {"score": 0.0026188490199688013, "phrase": "guo_and_nixon's_criterion"}, {"score": 0.002502374576203785, "phrase": "joint_probability_distributions"}, {"score": 0.0023479408542211875, "phrase": "second-order_product_distributions"}, {"score": 0.0021049977753042253, "phrase": "computationally_attractive_alternatives"}], "paper_keywords": ["Entropic spanning graphs", " feature selection", " mutual information", " Parzen window"], "paper_abstract": "Guo and Nixon proposed a feature selection method based on maximizing I(x; Y), the multidimensional mutual information between feature vector x and class variable Y. Because computing I(x; Y) can be difficult in practice, Guo and Nixon proposed an approximation of I(x; Y) as the criterion for feature selection. We show that Guo and Nixon's criterion originates from approximating the joint probability distributions in I(x; Y) by second-order product distributions. We remark on the limitations of the approximation and discuss computationally attractive alternatives to compute I(x; Y).", "paper_title": "On Guo and Nixon's Criterion for Feature Subset Selection: Assumptions, Implications, and Alternative Options", "paper_id": "WOS:000276683900019"}