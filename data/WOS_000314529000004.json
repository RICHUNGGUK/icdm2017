{"auto_keywords": [{"score": 0.03168857436324733, "phrase": "objective_function"}, {"score": 0.00481495049065317, "phrase": "machine_learning"}, {"score": 0.004701247803858319, "phrase": "optimization_problem"}, {"score": 0.004590217765551337, "phrase": "convex_objective_function"}, {"score": 0.004527947981009528, "phrase": "efficient_convex_optimizers"}, {"score": 0.004497129150551236, "phrase": "nice_guarantees"}, {"score": 0.004436116555817464, "phrase": "local_optima"}, {"score": 0.003949693102823493, "phrase": "modeling_expressivity"}, {"score": 0.0038431888954930083, "phrase": "convex_optimization"}, {"score": 0.0037910145286595386, "phrase": "non-convex_optimization_algorithms"}, {"score": 0.00365114510725007, "phrase": "efficient_and_scalable_algorithms"}, {"score": 0.003626272370749206, "phrase": "non-convex_optimization"}, {"score": 0.0035648266669315943, "phrase": "regularized_unconstrained_optimization_problems"}, {"score": 0.003516417950483685, "phrase": "large_number"}, {"score": 0.0034924598001567944, "phrase": "modern_machine_learning_problems"}, {"score": 0.003456827226847432, "phrase": "logistic_regression"}, {"score": 0.0034332736827447654, "phrase": "conditional_random_fields"}, {"score": 0.003409880074659807, "phrase": "large_margin_estimation"}, {"score": 0.00332924643541238, "phrase": "novel_algorithm"}, {"score": 0.003284026247389735, "phrase": "regularized_objective"}, {"score": 0.003025277245730244, "phrase": "regularization_term"}, {"score": 0.0029135770621039926, "phrase": "limited_memory_extension"}, {"score": 0.002883833572486452, "phrase": "regularized_bundle_methods"}, {"score": 0.002825251837733693, "phrase": "non_convex_risks"}, {"score": 0.0026747708876640377, "phrase": "stationary_solution"}, {"score": 0.0026565317818265394, "phrase": "accuracy_epsilon"}, {"score": 0.0025584115892805384, "phrase": "regularization_parameter"}, {"score": 0.002480826434008026, "phrase": "lipschitz_empirical_risk"}, {"score": 0.002340623433237658, "phrase": "stronger_and_more_disputable_assumption"}, {"score": 0.002300912218086077, "phrase": "experimental_results"}, {"score": 0.002285216591220303, "phrase": "artificial_test_problems"}, {"score": 0.002254145085288829, "phrase": "five_standard_and_difficult_machine_learning_problems"}, {"score": 0.0022007808283071133, "phrase": "non-convex_optimization_problems"}, {"score": 0.0021049977753042253, "phrase": "art_optimization_algorithms"}], "paper_keywords": ["optimization", " non-convex", " non-smooth", " cutting plane", " bundle method", " regularized risk"], "paper_abstract": "Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efficient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efficient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random fields, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy epsilon with a rate O(1/lambda epsilon) where lambda is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difficult and requires a stronger and more disputable assumption. Yet we provide experimental results on artificial test problems, and on five standard and difficult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms.", "paper_title": "Regularized Bundle Methods for Convex and Non-Convex Risks", "paper_id": "WOS:000314529000004"}