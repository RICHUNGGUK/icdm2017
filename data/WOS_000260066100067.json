{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "local_minima_problem"}, {"score": 0.04858132305104682, "phrase": "hidden_nodes"}, {"score": 0.015474611133624772, "phrase": "large_number"}, {"score": 0.004590820632326585, "phrase": "two-layered_feed-forward_artificial_neural_networks"}, {"score": 0.0041074234667687875, "phrase": "multi-layered_feed-forward_networks"}, {"score": 0.003645753655312165, "phrase": "bp"}, {"score": 0.0034482025128821548, "phrase": "exceeding_number"}, {"score": 0.003313741359811683, "phrase": "corresponding_network"}, {"score": 0.003036040409062437, "phrase": "stable_performance"}, {"score": 0.002279508335630797, "phrase": "benchmark_problems"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["Backpropagation", " Local minima", " Hidden nodes", " Target values", " Separate learning"], "paper_abstract": "The gradient descent algorithms like backpropagation (BP) or its variations on multi-layered feed-forward networks are widely used in many applications. However, the most serious problem associated with the BP is local minima problem. Especially, an exceeding number of hidden nodes make the corresponding network deepen the local minima problem. We propose an algorithm which shows stable performance on training despite of the large number of hidden nodes. This algorithm is called separate learning algorithm in which hidden-to-output and input-to-hidden separately trained. Simulations on some benchmark problems have been performed to demonstrate the validity of the proposed method. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Solving local minima problem with large number of hidden nodes on two-layered feed-forward artificial neural networks", "paper_id": "WOS:000260066100067"}