{"auto_keywords": [{"score": 0.03376529746472917, "phrase": "target_data"}, {"score": 0.00481495049065317, "phrase": "nontransductive_semisupervised_learning_and_transfer_learning"}, {"score": 0.004777917315737992, "phrase": "unsupervised_models"}, {"score": 0.004722898398399427, "phrase": "supplementary_soft_constraints"}, {"score": 0.004650519511749445, "phrase": "new_\"target\"_data"}, {"score": 0.004288261454002681, "phrase": "possible_differences"}, {"score": 0.004078232992955235, "phrase": "concept_drift"}, {"score": 0.0039694170761766226, "phrase": "transfer_learning_settings"}, {"score": 0.003878451139312637, "phrase": "general_optimization_framework"}, {"score": 0.0038189638340666936, "phrase": "input_class_membership_estimates"}, {"score": 0.0034672240102960644, "phrase": "cluster_ensemble"}, {"score": 0.0032341432727280707, "phrase": "consensus_labeling"}, {"score": 0.003123527559542239, "phrase": "application_settings"}, {"score": 0.0030401074516310184, "phrase": "learning_scenarios"}, {"score": 0.003005039427642664, "phrase": "training_data"}, {"score": 0.0027492483892113, "phrase": "wide_range"}, {"score": 0.002728059572916527, "phrase": "loss_functions"}, {"score": 0.002644920521740564, "phrase": "bregman_divergences"}, {"score": 0.0026043035241045394, "phrase": "legendre_duality"}, {"score": 0.0025643086619242814, "phrase": "principled_and_scalable_approach"}, {"score": 0.002476546030310661, "phrase": "proposed_framework"}, {"score": 0.0023278569001363263, "phrase": "original_task"}, {"score": 0.0022222152430170254, "phrase": "proposed_approach"}, {"score": 0.0021378442381693847, "phrase": "better_results"}, {"score": 0.0021049977753042253, "phrase": "popular_transductive_learning_techniques"}], "paper_keywords": ["Classification", " clustering", " ensembles", " transductive learning", " semisupervised learning", " transfer learning"], "paper_abstract": "Unsupervised models can provide supplementary soft constraints to help classify new \"target\" data because similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This article describes a general optimization framework that takes as input class membership estimates from existing classifiers learned on previously encountered \"source\" (or training) data, as well as a similarity matrix from a cluster ensemble operating solely on the target (or test) data to be classified, and yields a consensus labeling of the target data. More precisely, the application settings considered are nontransductive semisupervised and transfer learning scenarios where the training data are used only to build an ensemble of classifiers and are subsequently discarded before classifying the target data. The framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by naively applying classifiers learned on the original task to the target data. In addition, we show that the proposed approach, even not being conceptually transductive, can provide better results compared to some popular transductive learning techniques.", "paper_title": "An Optimization Framework for Combining Ensembles of Classifiers and Clusterers with Applications to Nontransductive Semisupervised Learning and Transfer Learning", "paper_id": "WOS:000344353300001"}