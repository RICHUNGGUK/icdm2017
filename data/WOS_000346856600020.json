{"auto_keywords": [{"score": 0.04431077255656771, "phrase": "neural_networks"}, {"score": 0.034316372449548334, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "speaker_identification"}, {"score": 0.0046848707075530256, "phrase": "combined_method"}, {"score": 0.004537525607322823, "phrase": "neural_network_classifiers"}, {"score": 0.004414907432703781, "phrase": "new_method"}, {"score": 0.004374772264092067, "phrase": "speaker_feature_extraction"}, {"score": 0.004276016541197123, "phrase": "wavelet_entropy"}, {"score": 0.004179480763003705, "phrase": "fwenn."}, {"score": 0.004122604238556912, "phrase": "first_stage"}, {"score": 0.004085115444878828, "phrase": "five_formants"}, {"score": 0.0040479661688863884, "phrase": "seven_shannon_entropy_wavelet_packet"}, {"score": 0.003956558420993431, "phrase": "speakers'_signals"}, {"score": 0.0039027036493682887, "phrase": "speaker_feature_vector"}, {"score": 0.003832031470970347, "phrase": "second_stage"}, {"score": 0.0035945701438887282, "phrase": "probabilistic_neural_network"}, {"score": 0.003433987036569199, "phrase": "conventional_speaker_recognition_methods"}, {"score": 0.0028863414975346512, "phrase": "deaf-mute_persons"}, {"score": 0.002782647772219627, "phrase": "experimental_results"}, {"score": 0.002682669289913153, "phrase": "speaker_verification"}, {"score": 0.0026582395558737855, "phrase": "identification_tasks"}, {"score": 0.002634031703695684, "phrase": "high_classification_rate"}, {"score": 0.0025627194819793347, "phrase": "minimum_amount"}, {"score": 0.002370955769074803, "phrase": "major_contribution"}, {"score": 0.002254571365634257, "phrase": "well-known_classical_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speaker verification and identification", " Wavelet packet", " Neural networks", " Formants"], "paper_abstract": "This paper proposes a new method for speaker feature extraction based on Formants, Wavelet Entropy and Neural Networks denoted as FWENN. In the first stage, five formants and seven Shannon entropy wavelet packet are extracted from the speakers' signals as the speaker feature vector. In the second stage, these 12 feature extraction coefficients are used as inputs to feed-forward neural networks. Probabilistic neural network is also proposed for comparison. In contrast to conventional speaker recognition methods that extract features from sentences (or words), the proposed method extracts the features from vowels. Advantages of using vowels include the ability to recognize speakers when only partially-recorded words are available. This may be useful for deaf-mute persons or when the recordings are damaged. Experimental results show that the proposed method succeeds in the speaker verification and identification tasks with high classification rate. This is accomplished with minimum amount of information, using only 12 coefficient features (i.e. vector length) and only one vowel signal, which is the major contribution of this work. The results are further compared to well-known classical algorithms for speaker recognition and are found to be superior. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Speaker identification using vowels features through a combined method of formants, wavelets, and neural network classifiers", "paper_id": "WOS:000346856600020"}