{"auto_keywords": [{"score": 0.030731841359398184, "phrase": "auxiliary_tag"}, {"score": 0.015719716506582538, "phrase": "image_annotation"}, {"score": 0.010872852845511469, "phrase": "auxiliary_tags"}, {"score": 0.00809791904348293, "phrase": "sadata"}, {"score": 0.004588389551682397, "phrase": "image_search"}, {"score": 0.004268293864953256, "phrase": "multi-label_learning_problem"}, {"score": 0.004195418010131832, "phrase": "semi-automatic_image_annotation_system"}, {"score": 0.004109600767747015, "phrase": "proper_words"}, {"score": 0.0038492126993451337, "phrase": "user's_feedback"}, {"score": 0.003757480190200645, "phrase": "novel_multi-label_learning_framework"}, {"score": 0.0037188360359702182, "phrase": "semi-automatic_dynamic_auxiliary-tag-aided"}, {"score": 0.0036177098964897364, "phrase": "classification_result"}, {"score": 0.003471135453226184, "phrase": "classification_results"}, {"score": 0.00327356326590461, "phrase": "strong_correlations"}, {"score": 0.0032398798149196432, "phrase": "target_tag"}, {"score": 0.003151736708029235, "phrase": "normalized_mutual_information"}, {"score": 0.0029722900280316216, "phrase": "auxiliary_set"}, {"score": 0.00276462370914216, "phrase": "probabilistic_model"}, {"score": 0.0026986685585382347, "phrase": "input_image"}, {"score": 0.0025625729433234644, "phrase": "user_feedback"}, {"score": 0.0024842242921857705, "phrase": "auxiliary_classifiers"}, {"score": 0.0023670716500937667, "phrase": "large_collection"}, {"score": 0.0023507916951658455, "phrase": "corel_images"}, {"score": 0.0023265809733919547, "phrase": "experimental_results"}, {"score": 0.0022168460543913787, "phrase": "user_feedbacks"}, {"score": 0.0021864526544323184, "phrase": "annotation_procedure"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Semi-automatic image annotation", " Multi-label learning", " Normalized mutual information", " User feedback"], "paper_abstract": "Image annotation is the foundation for many real-world applications. In the age of Web 2.0, image search and browsing are largely based on the tags of images. In this paper, we formulate image annotation as a multi-label learning problem, and develop a semi-automatic image annotation system. The presented system chooses proper words from a vocabulary as tags for a given image, and refines the tags with the help of the user's feedback. The refinement amounts to a novel multi-label learning framework, named Semi-Automatic Dynamic Auxiliary-Tag-Aided (SADATA), in which the classification result for one certain tag (target tag) can be boosted by the classification results of a subset of the other tags (auxiliary tags). The auxiliary tags, which have strong correlations with the target tag, are determined in terms of the normalized mutual information. We only select those tags whose correlations exceed a threshold as the auxiliary tags, so the auxiliary set is sparse. How much an auxiliary tag can contribute is dependent on the image, so we also build a probabilistic model conditioned on the auxiliary tag and the input image to adjust the weight of the auxiliary tag dynamically. For an given image, the user feedback on the tags corrects the outputs of the auxiliary classifiers and SADATA will recommend more proper tags next round. SADATA is evaluated on a large collection of Corel images. The experimental results validate the effectiveness of our dynamic auxiliary-tag-aided method. Furthermore, the performance also benefits from user feedbacks such that the annotation procedure can be significantly speeded up. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Semi-automatic dynamic auxiliary-tag-aided image annotation", "paper_id": "WOS:000271145000005"}