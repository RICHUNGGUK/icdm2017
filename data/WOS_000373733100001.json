{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayesian_methods"}, {"score": 0.00991939726336753, "phrase": "prior_information"}, {"score": 0.008714174188982153, "phrase": "rl"}, {"score": 0.004760765880314089, "phrase": "machine_learning"}, {"score": 0.004575854901681043, "phrase": "principled_methods"}, {"score": 0.004448169552315403, "phrase": "inference_algorithms"}, {"score": 0.00399449659126893, "phrase": "reinforcement_learning"}, {"score": 0.0038392331564122387, "phrase": "major_incentives"}, {"score": 0.0037745459666402915, "phrase": "bayesian_reasoning"}, {"score": 0.0035666632046674153, "phrase": "elegant_approach"}, {"score": 0.0031131208154718867, "phrase": "prior_knowledge"}, {"score": 0.0029083942467546305, "phrase": "bayesian_inference"}, {"score": 0.002859345731792013, "phrase": "simple_single-step_bandit_model"}, {"score": 0.002763709443278514, "phrase": "extensive_recent_literature"}, {"score": 0.0027017308406240563, "phrase": "model-based_rl"}, {"score": 0.0025239898026543964, "phrase": "markov_model"}, {"score": 0.0024257447861185813, "phrase": "model-free_rl"}, {"score": 0.002318127769195112, "phrase": "value_function"}, {"score": 0.0022919762167777427, "phrase": "policy_class"}, {"score": 0.002153316680437517, "phrase": "comprehensive_survey"}, {"score": 0.002129020447793464, "phrase": "bayesian_rl_algorithms"}], "paper_keywords": [""], "paper_abstract": "Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.", "paper_title": "Introduction", "paper_id": "WOS:000373733100001"}