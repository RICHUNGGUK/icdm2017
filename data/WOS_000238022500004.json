{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "pab"}, {"score": 0.008181109076146706, "phrase": "main_memory"}, {"score": 0.004603244077348199, "phrase": "aggressive_prefetching_mechanisms"}, {"score": 0.004475659507646842, "phrase": "important_applications"}, {"score": 0.004351595642056769, "phrase": "bus_traffic"}, {"score": 0.004230956171850591, "phrase": "cache_tag_arrays"}, {"score": 0.003823584649521733, "phrase": "\"feedback\"_mechanism"}, {"score": 0.0037595947142663997, "phrase": "prefetcher_assessment_buffer"}, {"score": 0.003266201217790818, "phrase": "aggressive_prefetching"}, {"score": 0.00305285084207725, "phrase": "different_configurations"}, {"score": 0.00266693888510356, "phrase": "non-selective_concurrent_use"}, {"score": 0.0021050049855519328, "phrase": "ipc"}], "paper_keywords": ["prefetching", " cache tag pressure", " memory wall"], "paper_abstract": "Aggressive prefetching mechanisms improve performance of some important applications, but substantially increase bus traffic and \"pressure\" on cache tag arrays. They may even reduce performance of applications that are not memory bounded. We introduce a \"feedback\" mechanism, termed Prefetcher Assessment Buffer (PAB), which filters out requests that are unlikely to be useful. With this, applications that cannot benefit from aggressive prefetching will not suffer from their side-effects. The PAB is evaluated with different configurations, e.g., \"all L1 accesses trigger prefetches\" and \"only misses to L1 trigger prefetches\". When compared with the non-selective concurrent use of multiple prefetchers, the PAB's application to prefetching from main memory to the L2 cache can reduce the number of loads from main memory by up to 25% without losing performance. Application of more sophisticated techniques to prefetches between the L2- and L1-cache can increase IPC by 4% while reducing the traffic between the caches 8-fold.", "paper_title": "A PAB-based multi-prefetcher mechanism", "paper_id": "WOS:000238022500004"}