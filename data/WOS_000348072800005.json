{"auto_keywords": [{"score": 0.048877326221094704, "phrase": "depth_camera"}, {"score": 0.015719716506582538, "phrase": "human_action_recognition"}, {"score": 0.013347074786027204, "phrase": "inertial_body_sensor"}, {"score": 0.01097634946574912, "phrase": "feature-level_fusion"}, {"score": 0.010838568908017187, "phrase": "decision-level_fusion"}, {"score": 0.00443308765153716, "phrase": "fusion_approach"}, {"score": 0.0040298010803060495, "phrase": "computationally_efficient_action_features"}, {"score": 0.003928572641266562, "phrase": "depth_images"}, {"score": 0.003757480190200645, "phrase": "accelerometer_signals"}, {"score": 0.003525861581971337, "phrase": "depth_motion_maps"}, {"score": 0.0034812740368914455, "phrase": "statistical_signal_attributes"}, {"score": 0.0034154440860550564, "phrase": "action_recognition"}, {"score": 0.0031845067058084583, "phrase": "collaborative_representation_classifier"}, {"score": 0.002768293851511292, "phrase": "dempster-shafer_theory"}, {"score": 0.002664513608074113, "phrase": "classification_outcomes"}, {"score": 0.0025000985765154028, "phrase": "introduced_fusion_framework"}, {"score": 0.002421726275597666, "phrase": "berkeley_multimodal_human_action_database"}, {"score": 0.002286781334850213, "phrase": "complementary_aspect"}], "paper_keywords": ["Depth motion map (DMM)", " fusion of depth camera and inertial sensor", " human action recognition", " wearable inertial sensor"], "paper_abstract": "This paper presents a fusion approach for improving human action recognition based on two differing modality sensors consisting of a depth camera and an inertial body sensor. Computationally efficient action features are extracted from depth images provided by the depth camera and from accelerometer signals provided by the inertial body sensor. These features consist of depth motion maps and statistical signal attributes. For action recognition, both feature-level fusion and decision-level fusion are examined by using a collaborative representation classifier. In the feature-level fusion, features generated from the two differing modality sensors are merged before classification, while in the decision-level fusion, the Dempster-Shafer theory is used to combine the classification outcomes from two classifiers, each corresponding to one sensor. The introduced fusion framework is evaluated using the Berkeley multimodal human action database. The results indicate that because of the complementary aspect of the data from these sensors, the introduced fusion approaches lead to 2% to 23% recognition rate improvements depending on the action over the situations when each sensor is used individually.", "paper_title": "Improving Human Action Recognition Using Fusion of Depth Camera and Inertial Sensors", "paper_id": "WOS:000348072800005"}