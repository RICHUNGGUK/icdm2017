{"auto_keywords": [{"score": 0.048661882068602086, "phrase": "classification_trees"}, {"score": 0.043367051214551706, "phrase": "penalized_criterion"}, {"score": 0.004822599963738126, "phrase": "bounds"}, {"score": 0.004743164872278112, "phrase": "embedded_variable_selection"}, {"score": 0.003701401653850141, "phrase": "risk_bound_inequality"}, {"score": 0.0031606404966938568, "phrase": "pruning_step"}, {"score": 0.0030901063954739375, "phrase": "cart_algorithm"}, {"score": 0.0028233196851239753, "phrase": "specific_margin_assumptions"}, {"score": 0.0026986685585382347, "phrase": "tuning_parameter"}, {"score": 0.0026384167183586015, "phrase": "cart_penalty"}, {"score": 0.0024287485834314027, "phrase": "simulation_study"}, {"score": 0.0022696277881969896, "phrase": "theoretical_penalized_criterion"}, {"score": 0.0021049977753042253, "phrase": "regularization_parameter"}], "paper_keywords": ["Classification Tree", " Variable Selection", " Statistical Learning Theory"], "paper_abstract": "The problems of model and variable selections for classification trees are jointly considered. A penalized criterion is proposed which explicitly takes into account the number of variables, and a risk bound inequality is provided for the tree classifier minimizing this criterion. This penalized criterion is compared to the one used during the pruning step of the CART algorithm. It is shown that the two criteria are similar under some specific margin assumptions. In practice, the tuning parameter of the CART penalty has to be calibrated by hold-out or cross-validation. A simulation study is performed to compare the form of the theoretical penalized criterion we propose with the form obtained after tuning the regularization parameter via cross-validation.", "paper_title": "Risk Bounds for Embedded Variable Selection in Classification Trees", "paper_id": "WOS:000331902400025"}