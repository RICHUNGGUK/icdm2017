{"auto_keywords": [{"score": 0.03785838340094737, "phrase": "rmpi"}, {"score": 0.00481495049065317, "phrase": "robust_dynamic_programming"}, {"score": 0.004702747762588659, "phrase": "dp"}, {"score": 0.004415675459182871, "phrase": "transition_probabilities"}, {"score": 0.0042452552248398445, "phrase": "markov_decision_problems"}, {"score": 0.0040176151419992956, "phrase": "robust_dp_solutions"}, {"score": 0.0036553070575015344, "phrase": "robust_modified_policy_iteration"}, {"score": 0.003299460925313939, "phrase": "previously_known_algorithms"}, {"score": 0.003247868216333148, "phrase": "robust_value_iteration"}, {"score": 0.0031970796597571367, "phrase": "robust_policy_iteration"}, {"score": 0.0030494153169181334, "phrase": "exact_rmpi"}, {"score": 0.002931569771634119, "phrase": "\"inner_problem"}, {"score": 0.002752390133272192, "phrase": "inexact_rmpi"}, {"score": 0.002645992747675361, "phrase": "inner_problem"}, {"score": 0.0025237171554924128, "phrase": "specified_tolerance"}, {"score": 0.002426137929553722, "phrase": "new_stopping_criteria"}, {"score": 0.0023507916951658455, "phrase": "span_seminorm"}, {"score": 0.0022070310530794097, "phrase": "numerical_studies"}, {"score": 0.0021049977753042253, "phrase": "computation_time"}], "paper_keywords": ["control processes", " Markov processes", " optimization", " dynamic programming", " robust optimization"], "paper_abstract": "Robust dynamic programming (robust DP) mitigates the effects of ambiguity in transition probabilities on the solutions of Markov decision problems. We consider the computation of robust DP solutions for discrete-stage, infinite-horizon, discounted problems with finite state and action spaces. We present robust modified policy iteration (RMPI) and demonstrate its convergence. RMPI encompasses both of the previously known algorithms, robust value iteration and robust policy iteration. In addition to proposing exact RMPI, in which the \"inner problem\" is solved precisely, we propose inexact RMPI, in which the inner problem is solved to within a specified tolerance. We also introduce new stopping criteria based on the span seminorm. Finally, we demonstrate through some numerical studies that RMPI can significantly reduce computation time.", "paper_title": "Robust Modified Policy Iteration", "paper_id": "WOS:000322424000002"}