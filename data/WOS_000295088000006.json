{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "novel_algorithm"}, {"score": 0.004741845603430361, "phrase": "multi-task_learning"}, {"score": 0.004669845451682529, "phrase": "boosted_decision_trees"}, {"score": 0.004426308135681733, "phrase": "joint_model"}, {"score": 0.004227653047394939, "phrase": "shared_parameters"}, {"score": 0.0041001740903338834, "phrase": "task-specific_ones"}, {"score": 0.003976523732243618, "phrase": "implicit_data_sharing"}, {"score": 0.002949598537993663, "phrase": "data_sets"}, {"score": 0.00290473242917264, "phrase": "different_countries"}, {"score": 0.002690406375639144, "phrase": "editorial_judgments"}, {"score": 0.0025892330566358503, "phrase": "proposed_method"}, {"score": 0.0023798115577749225, "phrase": "publicly_available_multi-task_dataset"}, {"score": 0.0021705796895687864, "phrase": "significant_improvements"}, {"score": 0.0021049977753042253, "phrase": "surprising_reliability"}], "paper_keywords": ["Multi-task learning", " Boosting", " Decision trees", " Web search", " Ranking"], "paper_abstract": "In this paper we propose a novel algorithm for multi-task learning with boosted decision trees. We learn several different learning tasks with a joint model, explicitly addressing their commonalities through shared parameters and their differences with task-specific ones. This enables implicit data sharing and regularization. Our algorithm is derived using the relationship between a\"\" (1)-regularization and boosting. We evaluate our learning method on web-search ranking data sets from several countries. Here, multi-task learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. Further, the proposed method obtains state-of-the-art results on a publicly available multi-task dataset. Our experiments validate that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.", "paper_title": "Boosted multi-task learning", "paper_id": "WOS:000295088000006"}