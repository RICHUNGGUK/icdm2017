{"auto_keywords": [{"score": 0.036123960464141965, "phrase": "reconstruction_error"}, {"score": 0.007453707245224921, "phrase": "pseudo-inverse_sample_covariance_matrix"}, {"score": 0.00481495049065317, "phrase": "pseudo-inverse_covariance_learning-a_random_matrix_theory_analysis"}, {"score": 0.004623608221548592, "phrase": "inverse_population_covariance"}, {"score": 0.00441738161338662, "phrase": "sample_covariance_matrix"}, {"score": 0.004328689762198895, "phrase": "modern_scientific_data_sets"}, {"score": 0.004220314322073171, "phrase": "sample_points"}, {"score": 0.003991305899593133, "phrase": "sample_covariance"}, {"score": 0.0038131738052230254, "phrase": "moore-penrose_pseudo-inverse_sample_covariance_matrix"}, {"score": 0.0036614948691735105, "phrase": "nonzero_sample_covariance_eigenvalues"}, {"score": 0.0034803228231621687, "phrase": "inverse_population_covariance_matrix"}, {"score": 0.003308085469148074, "phrase": "true_inverse_covariance"}, {"score": 0.0032088487124106936, "phrase": "frobenius_norm"}, {"score": 0.002988685006809336, "phrase": "smallest_nonzero_sample_covariance_eigenvalues"}, {"score": 0.0029137604783116065, "phrase": "sample_size"}, {"score": 0.002769483748975581, "phrase": "high-dimensional_data"}, {"score": 0.002713788077665689, "phrase": "random_matrix_theory_techniques"}, {"score": 0.002592523232809191, "phrase": "wide_class"}, {"score": 0.0025663181430010686, "phrase": "population_covariance_matrices"}, {"score": 0.0024516267507109753, "phrase": "subspace_methods"}, {"score": 0.0021049977753042253, "phrase": "simulated_and_benchmark_data_sets"}], "paper_keywords": ["Pseudo-inverse", " linear discriminants", " peaking phenomenon", " random matrix theory", " bagging", " random subspace method"], "paper_abstract": "For many learning problems, estimates of the inverse population covariance are required and often obtained by inverting the sample covariance matrix. Increasingly for modern scientific data sets, the number of sample points is less than the number of features and so the sample covariance is not invertible. In such circumstances, the Moore-Penrose pseudo-inverse sample covariance matrix, constructed from the eigenvectors corresponding to nonzero sample covariance eigenvalues, is often used as an approximation to the inverse population covariance matrix. The reconstruction error of the pseudo-inverse sample covariance matrix in estimating the true inverse covariance can be quantified via the Frobenius norm of the difference between the two. The reconstruction error is dominated by the smallest nonzero sample covariance eigenvalues and diverges as the sample size becomes comparable to the number of features. For high-dimensional data, we use random matrix theory techniques and results to study the reconstruction error for a wide class of population covariance matrices. We also show how bagging and random subspace methods can result in a reduction in the reconstruction error and can be combined to improve the accuracy of classifiers that utilize the pseudo-inverse sample covariance matrix. We test our analysis on both simulated and benchmark data sets.", "paper_title": "Accuracy of Pseudo-Inverse Covariance Learning-A Random Matrix Theory Analysis", "paper_id": "WOS:000290574000014"}