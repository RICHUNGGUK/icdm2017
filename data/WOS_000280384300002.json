{"auto_keywords": [{"score": 0.04011238133214689, "phrase": "ut"}, {"score": 0.039875737560621155, "phrase": "di"}, {"score": 0.007740397969376538, "phrase": "virtual_environments"}, {"score": 0.005959334678964139, "phrase": "ei."}, {"score": 0.005108669456078021, "phrase": "usability_problems"}, {"score": 0.0044853402274804815, "phrase": "expert_inspection"}, {"score": 0.004335739075947899, "phrase": "twenty-nine_individuals"}, {"score": 0.003880017219275826, "phrase": "comparison_show"}, {"score": 0.0036030390333022244, "phrase": "di-"}, {"score": 0.0035808664384880213, "phrase": "ut-based_diagnoses"}, {"score": 0.0034827622221683756, "phrase": "identified_problems"}, {"score": 0.0032240802202623316, "phrase": "identification_impact"}, {"score": 0.003194354458513131, "phrase": "whole_set"}, {"score": 0.0026133424923929227, "phrase": "particular_state"}, {"score": 0.0024190798397126924, "phrase": "basic_usability"}, {"score": 0.0021246078976621093, "phrase": "adequate_road"}], "paper_keywords": [""], "paper_abstract": "This article describes an experiment comparing three Usability Evaluation Methods: User Testing (UT), Document-based Inspection (DI), and Expert Inspection (EI) for evaluating Virtual Environments (VEs). Twenty-nine individuals (10 end-users and 19 junior usability experts) participated during 1 hr each in the evaluation of two VEs (a training VE and a 3D map). Quantitative results of the comparison show that the effectiveness of UT and DI is significantly better than the effectiveness of EI. For each method, results show their problem coverage: DI- and UT-based diagnoses lead to more problem diversity than EI. The overlap of identified problems amounts to 22% between UT and DI, 20% between DI and EI, and 12% between EI and UT for both virtual environments. The identification impact of the whole set of usability problems is 60% for DI, 57% for UT, and only 36% for EI for both virtual environments. Also reliability of UT and DI is significantly better than reliability of EI. In addition, a qualitative analysis identified 35 classes describing the profile of usability problems found with each method. It shows that UT seems particularly efficient for the diagnosis of problems that require a particular state of interaction to be detectable. On the other hand, DI supports the identification of problems directly observable, often related to learnability and basic usability. This study shows that DI could be viewed as a o4-wheel drive SUV evaluation typeo (less powerful under certain conditions but able to go everywhere, with any driver), whereas UT could be viewed as a oFormula 1 car evaluation typeo (more powerful but requiring adequate road and a very skilled driver). EI is found (considering all metrics) to be not efficient enough to evaluate usability of VEs.", "paper_title": "Comparing Inspections and User Testing for the Evaluation of Virtual Environments", "paper_id": "WOS:000280384300002"}