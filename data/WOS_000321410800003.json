{"auto_keywords": [{"score": 0.04138828611141582, "phrase": "sensor_fusion"}, {"score": 0.00481495049065317, "phrase": "visual_maps"}, {"score": 0.004777569299319633, "phrase": "interior_space"}, {"score": 0.004722038375317162, "phrase": "new_hierarchical_sensor_fusion_architecture"}, {"score": 0.004523865550323285, "phrase": "best_approach"}, {"score": 0.004471270242168067, "phrase": "accurate_construction"}, {"score": 0.004436545340717729, "phrase": "environment_maps"}, {"score": 0.004384960761826366, "phrase": "sensor-equipped_mobile_robot"}, {"score": 0.004266907250334316, "phrase": "range_sensor"}, {"score": 0.004184526032558792, "phrase": "reflectance_data"}, {"score": 0.003916098498242888, "phrase": "hierarchical_approaches"}, {"score": 0.0037516215763875225, "phrase": "lowest_level"}, {"score": 0.003483573659272443, "phrase": "higher_levels"}, {"score": 0.0032472646337850042, "phrase": "higher_level_representations"}, {"score": 0.0032094634756436595, "phrase": "interior_map"}, {"score": 0.0030506626796257077, "phrase": "data_elements"}, {"score": 0.00300339527261653, "phrase": "different_sensors"}, {"score": 0.0027886573969044042, "phrase": "interval-based_representation"}, {"score": 0.0027347383772384102, "phrase": "sensor_data"}, {"score": 0.0024514505056251316, "phrase": "loop_closure_results"}, {"score": 0.0022672107906933714, "phrase": "map_construction"}, {"score": 0.0021803388613879896, "phrase": "large_loops"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sensor fusion", " Hierarchical map building", " SLAM", " Dense visual map"], "paper_abstract": "It is now generally recognized that sensor-fusion is the best approach to the accurate construction of environment maps by a sensor-equipped mobile robot. Typically, range data collected with a range sensor is combined with the reflectance data obtained from one or more cameras mounted on the robot. In much of the past work on sensor fusion in hierarchical approaches to map construction, the fusion was carried out only at the lowest level of the hierarchy. As a result, in those approaches, only the fused data was made available to the higher levels in the hierarchy. This implied that any errors caused by sensor fusion would propagate upwards into the higher level representations of an interior map. Our work, on the other hand, checks for consistency between the data elements produced by the different sensors at all levels of the hierarchy. This consistency checking is carried out with the help of an interval-based representation of uncertainties in the sensor data. In addition to demonstrating that our approach to the fusion of range and image data results in dense 3D maps of the interior space, we also provide validation of our overall framework by presenting a set of loop closure results. These results demonstrate that our overall errors in the maps remain small (within 0.91% of the distance traveled for map construction) even when the robot has to traverse over large loops inside a building. (c) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Building 3D visual maps of interior space with a new hierarchical sensor fusion architecture", "paper_id": "WOS:000321410800003"}