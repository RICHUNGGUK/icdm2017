{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "instance-based_learning"}, {"score": 0.004710050705871926, "phrase": "ibl"}, {"score": 0.0045820747375409435, "phrase": "memory-based_reasoning"}, {"score": 0.004408729661992651, "phrase": "commonly_used_nonparametric_learning_algorithm"}, {"score": 0.004360412703519539, "phrase": "k-nearest_neighbor"}, {"score": 0.004103944614334696, "phrase": "ibl."}, {"score": 0.0038838512128685884, "phrase": "successfully"}, {"score": 0.00379914165379475, "phrase": "wide_range"}, {"score": 0.003536346954940087, "phrase": "important_model_parameters"}, {"score": 0.0031150035760241705, "phrase": "structured_ways"}, {"score": 0.0029805015010906013, "phrase": "locally_linear_reconstruction"}, {"score": 0.002851790448644913, "phrase": "sequential_minimal_optimization"}, {"score": 0.002820495799974456, "phrase": "smo"}, {"score": 0.002758907973783665, "phrase": "quadratic_programming_step"}, {"score": 0.0026252106881752067, "phrase": "computational_complexity"}, {"score": 0.0025963890234079333, "phrase": "experimental_results"}, {"score": 0.002539689093411356, "phrase": "eight_regression_tasks"}, {"score": 0.0023900527169254744, "phrase": "llr_outperform"}, {"score": 0.002363806685815014, "phrase": "conventional_weight_allocation_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["instance-based learning", " memory-based reasoning", " k-nearest neighbor", " weight allocation", " local reconstruction"], "paper_abstract": "Instance-based learning (IBL), so called memory-based reasoning (MBR), is a commonly used nonparametric learning algorithm. k-nearest neighbor (k-NN) learning is the most popular realization of IBL. Due to its usability and adaptability, k-NN has been Successfully applied to a wide range of applications. However, in practice, one has to set important model parameters only empirically: the number of neighbors (k) and weights to those neighbors. In this paper, we propose structured ways to set these parameters, based on locally linear reconstruction (LLR). We then employed sequential minimal optimization (SMO) for solving quadratic programming step involved in LLR for classification to reduce the computational complexity. Experimental results from 11 classification and eight regression tasks were Promising enough to merit further investigation: not only did LLR Outperform the conventional weight allocation methods without much additional computational cost, but also LLR Was found to be robust to the change of k. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Locally linear reconstruction for instance-based learning", "paper_id": "WOS:000258817800022"}