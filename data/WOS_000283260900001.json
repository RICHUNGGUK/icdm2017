{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "dmt_optimality"}, {"score": 0.004776297980616768, "phrase": "lr-aided_linear_decoders"}, {"score": 0.004718897469501381, "phrase": "general_class"}, {"score": 0.004318684367622946, "phrase": "mimo"}, {"score": 0.0041144374213500715, "phrase": "diversity-multiplexing_tradeoff"}, {"score": 0.00398380712846857, "phrase": "computationally_expensive_maximum-likelihood"}, {"score": 0.003719773848735395, "phrase": "regularized_lattice_decoders"}, {"score": 0.0036308352555610448, "phrase": "dmt"}, {"score": 0.003431384878608803, "phrase": "channel_statistics"}, {"score": 0.0033764382987428497, "phrase": "channel_dimensions"}, {"score": 0.0032559935867182035, "phrase": "particular_lattice-code"}, {"score": 0.0031909403420958752, "phrase": "special_case"}, {"score": 0.0031020367403802773, "phrase": "lll-based_lr-aided_linear_implementation"}, {"score": 0.003064748749580867, "phrase": "mmse"}, {"score": 0.0029913511336659327, "phrase": "dmt_optimal_decoding"}, {"score": 0.0029553380263320195, "phrase": "lattice_code"}, {"score": 0.0029197572138773237, "phrase": "worst-case_complexity"}, {"score": 0.002826949449085849, "phrase": "data_rate"}, {"score": 0.0027704444816096484, "phrase": "fundamental_reduction"}, {"score": 0.0027370835853738626, "phrase": "decoding_complexity"}, {"score": 0.0026933223597258543, "phrase": "ml"}, {"score": 0.0025761937402886954, "phrase": "results'_generality"}, {"score": 0.002494279207996077, "phrase": "pertinent_communication_scenarios"}, {"score": 0.0024642355970277497, "phrase": "quasi-static_mimo"}, {"score": 0.002444409707292031, "phrase": "mimo-ofdm"}, {"score": 0.0023762535034673017, "phrase": "mimo-arq"}, {"score": 0.002272964710550345, "phrase": "lr-aided_linear_decoder"}, {"score": 0.002227508035618043, "phrase": "adopted_approach"}, {"score": 0.0021479597698614373, "phrase": "joint_transceiver_designs"}, {"score": 0.0021220787047953093, "phrase": "improved_snr_gap"}, {"score": 0.0021049977753042253, "phrase": "ml_decoding"}], "paper_keywords": ["Diversity-multiplexing tradeoff", " lattice decoding", " lattice reduction", " linear decoding", " multiple-input multiple-output (MIMO)", " regularization", " space-time coders-decoders"], "paper_abstract": "This paper identifies the first general, explicit, and nonrandom MIMO encoder-decoder structures that guarantee optimality with respect to the diversity-multiplexing tradeoff (DMT), without employing a computationally expensive maximum-likelihood (ML) receiver. Specifically, the work establishes the DMT optimality of a class of regularized lattice decoders, and more importantly the DMT optimality of their lattice-reduction (LR)-aided linear counterparts. The results hold for all channel statistics, for all channel dimensions, and most interestingly, irrespective of the particular lattice-code applied. As a special case, it is established that the LLL-based LR-aided linear implementation of the MMSE-GDFE lattice decoder facilitates DMT optimal decoding of any lattice code at a worst-case complexity that grows at most linearly in the data rate. This represents a fundamental reduction in the decoding complexity when compared to ML decoding whose complexity is generally exponential in the rate. The results' generality lends them applicable to a plethora of pertinent communication scenarios such as quasi-static MIMO, MIMO-OFDM, ISI, cooperative-relaying, and MIMO-ARQ channels, in all of which the DMT optimality of the LR-aided linear decoder is guaranteed. The adopted approach yields insight, and motivates further study, into joint transceiver designs with an improved SNR gap to ML decoding.", "paper_title": "DMT Optimality of LR-Aided Linear Decoders for a General Class of Channels, Lattice Designs, and System Models", "paper_id": "WOS:000283260900001"}