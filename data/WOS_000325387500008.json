{"auto_keywords": [{"score": 0.03894306429535568, "phrase": "loc"}, {"score": 0.006759644184564367, "phrase": "good_measure"}, {"score": 0.0065269354315952545, "phrase": "effort-aware_bug_prediction_models"}, {"score": 0.004838042633336923, "phrase": "effort-aware_models"}, {"score": 0.004549524321051407, "phrase": "buggy_software_locations"}, {"score": 0.00436890075128947, "phrase": "current_bugs"}, {"score": 0.003964007795174772, "phrase": "predicted_files"}, {"score": 0.00378602209193448, "phrase": "prediction_models"}, {"score": 0.0037352048741373816, "phrase": "empirical_evidence"}, {"score": 0.003665200409053223, "phrase": "good_approximation"}, {"score": 0.003379690612201994, "phrase": "empirical_study"}, {"score": 0.003307372169859112, "phrase": "explicitly-recorded_effort_data"}, {"score": 0.0030250067691601967, "phrase": "churn_metrics"}, {"score": 0.0030005611233848324, "phrase": "better_measure"}, {"score": 0.002865710140187219, "phrase": "previous_effort-aware_bug_prediction_work"}, {"score": 0.002436289099444074, "phrase": "effort-aware_bug_prediction"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Effort-aware prediction", " Prediction models", " Defect prediction"], "paper_abstract": "Context: Effort-aware models, e.g., effort-aware bug prediction models aim to help practitioners identify and prioritize buggy software locations according to the effort involved with fixing the bugs. Since the effort of current bugs is not yet known and the effort of past bugs is typically not explicitly recorded, effort-aware bug prediction models are forced to use approximations, such as the number of lines of code (LOC) of the predicted files. Objective: Although the choice of these approximations is critical for the performance of the prediction models, there is no empirical evidence on whether LOC is actually a good approximation. Therefore, in this paper, we investigate the question: is LOC a good measure of effort for use in effort-aware models? Method: We perform an empirical study on four open source projects, for which we obtain explicitly-recorded effort data, and compare the use of LOC to various complexity, size and churn metrics as measures of effort. Results: We find that using a combination of complexity, size and churn metrics are a better measure of effort than using LOC alone. Furthermore, we examine the impact of our findings on previous effort-aware bug prediction work and find that using LOC as a measure for effort does not significantly affect the list of files being flagged, however, using LOC under-estimates the amount of effort required compared to our best effort predictor by approximately 66%. Conclusion: Studies using effort-aware models should not assume that LOC is a good measure of effort. For the case of effort-aware bug prediction, using LOC provides results that are similar to combining complexity, churn, size and LOC as a proxy for effort when prioritizing the most risky files. However, we find that for the purpose of effort-estimation, using LOC may under-estimate the amount of effort required. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Is lines of code a good measure of effort in effort-aware models?", "paper_id": "WOS:000325387500008"}