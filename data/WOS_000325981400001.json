{"auto_keywords": [{"score": 0.0498439927358868, "phrase": "unknown_constrained-input_systems"}, {"score": 0.033002348931367075, "phrase": "identifier_weights"}, {"score": 0.00481495049065317, "phrase": "adaptive_optimal_control_of"}, {"score": 0.004674952036988355, "phrase": "neural_networks"}, {"score": 0.004539005582217309, "phrase": "online_policy_iteration"}, {"score": 0.004495644175093536, "phrase": "pi"}, {"score": 0.004363846362238565, "phrase": "continuous-time_optimal_control_solution"}, {"score": 0.004257806363128601, "phrase": "proposed_pi_algorithm"}, {"score": 0.004154332348800334, "phrase": "actor-critic_structure"}, {"score": 0.003877737556673229, "phrase": "optimal_bounded_control_policy"}, {"score": 0.0037834649508375544, "phrase": "complete_knowledge"}, {"score": 0.003728000957725218, "phrase": "system_dynamics"}, {"score": 0.0036194914688460656, "phrase": "novel_nn_identifier"}, {"score": 0.003496868378204657, "phrase": "critic_nns"}, {"score": 0.003378385493203297, "phrase": "identifier_weights_estimation_error"}, {"score": 0.003263907362490323, "phrase": "nn."}, {"score": 0.0032319109979344184, "phrase": "novel_learning_rule"}, {"score": 0.00307657476125683, "phrase": "small_neighborhoods"}, {"score": 0.002943147037670826, "phrase": "easy-to-check_persistence"}, {"score": 0.0028574164693918433, "phrase": "experience_replay_technique"}, {"score": 0.002680114113899346, "phrase": "current_data"}, {"score": 0.002538687900036229, "phrase": "whole_system"}, {"score": 0.0023928819778638055, "phrase": "system_identifier"}, {"score": 0.002255431275670053, "phrase": "near-optimal_control_law"}, {"score": 0.002157538586313944, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "simulation_example"}], "paper_keywords": ["Input constraints", " neural networks", " optimal control", " reinforcement learning", " unknown dynamics"], "paper_abstract": "This paper presents an online policy iteration (PI) algorithm to learn the continuous-time optimal control solution for unknown constrained-input systems. The proposed PI algorithm is implemented on an actor-critic structure where two neural networks (NNs) are tuned online and simultaneously to generate the optimal bounded control policy. The requirement of complete knowledge of the system dynamics is obviated by employing a novel NN identifier in conjunction with the actor and critic NNs. It is shown how the identifier weights estimation error affects the convergence of the critic NN. A novel learning rule is developed to guarantee that the identifier weights converge to small neighborhoods of their ideal values exponentially fast. To provide an easy-to-check persistence of excitation condition, the experience replay technique is used. That is, recorded past experiences are used simultaneously with current data for the adaptation of the identifier weights. Stability of the whole system consisting of the actor, critic, system state, and system identifier is guaranteed while all three networks undergo adaptation. Convergence to a near-optimal control law is also shown. The effectiveness of the proposed method is illustrated with a simulation example.", "paper_title": "Adaptive Optimal Control of Unknown Constrained-Input Systems Using Policy Iteration and Neural Networks", "paper_id": "WOS:000325981400001"}