{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "support_vector_machines"}, {"score": 0.004388035083357706, "phrase": "quadratic_programming_problem"}, {"score": 0.00418892838079239, "phrase": "large_computational_complexity"}, {"score": 0.003747020072136732, "phrase": "geometrical_point"}, {"score": 0.003414467360488913, "phrase": "homogeneous_hyperplanes"}, {"score": 0.0030257422786589723, "phrase": "batch_mode"}, {"score": 0.002942495422585405, "phrase": "linear_complexity"}, {"score": 0.002331909406363425, "phrase": "support_vectors"}, {"score": 0.002267708463966758, "phrase": "effective_examples"}, {"score": 0.0021049977753042253, "phrase": "lower_performance"}], "paper_keywords": ["support vector machines", " incremental learning", " learning curves", " admissible region"], "paper_abstract": "Support vector machines (SVMs) are known to result in a quadratic programming problem, that requires a large computational complexity. To reduce it, this paper considers, from the geometrical point of view, two incremental or iterative SVMs with homogeneous hyperplanes. One method is shown to produce the same solution as an SVM in batch mode with the linear complexity on average, utilizing the fact that only effective examples are necessary and sufficient for the solution. The other, which stores the set of support vectors instead of effective examples, is quantitatively shown to have a lower performance although implementation is rather easy. (C) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Incremental support vector machines and their geometrical analyses", "paper_id": "WOS:000247745000034"}