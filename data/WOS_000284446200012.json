{"auto_keywords": [{"score": 0.03710138713708482, "phrase": "crr"}, {"score": 0.00481495049065317, "phrase": "linear_discriminant_analysis_methods"}, {"score": 0.004772818068176176, "phrase": "optimal_parameter_selection"}, {"score": 0.004628223381645729, "phrase": "classical_linear_discriminant_analysis"}, {"score": 0.004588190904737843, "phrase": "lda"}, {"score": 0.004429191773525288, "phrase": "under-sampled_problem"}, {"score": 0.0041102032028337366, "phrase": "eigenvalue_decomposition"}, {"score": 0.00376409764683147, "phrase": "lda_variants"}, {"score": 0.003698442219929622, "phrase": "principal_component_analysis"}, {"score": 0.0036664458222786193, "phrase": "pca"}, {"score": 0.0036179751816682454, "phrase": "constrained_ridge_regression"}, {"score": 0.0031845067058084583, "phrase": "class_centers"}, {"score": 0.00307431840409833, "phrase": "within-class_distances"}, {"score": 0.002994178996584381, "phrase": "transform_norm"}, {"score": 0.0029548937818509656, "phrase": "ridge_regression"}, {"score": 0.002790511479398411, "phrase": "particular_regularization_numbers"}, {"score": 0.0027660443371695024, "phrase": "class_indicators"}, {"score": 0.002682081928174866, "phrase": "best_lda_method"}, {"score": 0.0026352496813713292, "phrase": "best_member"}, {"score": 0.002600661525224505, "phrase": "crr_family"}, {"score": 0.0023918783117166326, "phrase": "similar_computations"}, {"score": 0.0023604768913655463, "phrase": "training_process"}, {"score": 0.002288797099841088, "phrase": "loo_errors"}, {"score": 0.0021613897794439227, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Linear discriminant analysis", " Model selection", " Under-sampled problem", " Face recognition", " Principal component analysis", " Constrained ridge regression"], "paper_abstract": "In the last decade, many variants of classical linear discriminant analysis (LDA) have been developed to tackle the under-sampled problem in face recognition. However, choosing the variants is not easy since these methods involve eigenvalue decomposition that makes cross-validation computationally expensive. In this paper, we propose to solve this problem by unifying these LDA variants in one framework: principal component analysis (PCA) plus constrained ridge regression (CRR). In CRR, one selects the target (also called class indicator) for each class, and finds a projection to locate the class centers at their class targets and the transform minimizes the within-class distances with a penalty on the transform norm as in ridge regression. Under this framework, many existing LDA methods can be viewed as PCA+CRR with particular regularization numbers and class indicators and we propose to choose the best LDA method as choosing the best member from the CRR family. The latter can be done by comparing their leave-one-out (LOO) errors and we present an efficient algorithm, which requires similar computations to the training process of CRR, to evaluate the LOO errors. Experiments on Yale Face B, Extended Yale B and CMU-PIE databases are conducted to demonstrate the effectiveness of the proposed methods. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Unified formulation of linear discriminant analysis methods and optimal parameter selection", "paper_id": "WOS:000284446200012"}