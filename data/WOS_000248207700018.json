{"auto_keywords": [{"score": 0.03837736982958032, "phrase": "sufficient_statistics"}, {"score": 0.00481495049065317, "phrase": "large_data_sets"}, {"score": 0.004767546015485477, "phrase": "mixture_models"}, {"score": 0.004720606036240263, "phrase": "sufficient_em."}, {"score": 0.004537401559396022, "phrase": "mixture_model_approach"}, {"score": 0.004492717291734985, "phrase": "two-step_strategy"}, {"score": 0.0042336638794732255, "phrase": "first_step_data"}, {"score": 0.004130165601475531, "phrase": "compression_techniques"}, {"score": 0.004089475312474684, "phrase": "data_compression"}, {"score": 0.003989487818395848, "phrase": "single_observations"}, {"score": 0.00393066798511457, "phrase": "medium_number"}, {"score": 0.003685604033479918, "phrase": "i.e._a_triple"}, {"score": 0.003371220210416809, "phrase": "second_step"}, {"score": 0.0032402142645120958, "phrase": "adapted_em_algorithm"}, {"score": 0.003068327486877065, "phrase": "compressed_data"}, {"score": 0.003023047748772801, "phrase": "estimated_mixture"}, {"score": 0.0027925723953701083, "phrase": "sufficient_em"}, {"score": 0.002737745536992088, "phrase": "real_data"}, {"score": 0.0026839922027139967, "phrase": "web-usage_mining_application"}, {"score": 0.00263129148531588, "phrase": "standard_em"}, {"score": 0.0025164594326820113, "phrase": "spss."}, {"score": 0.002442693254693416, "phrase": "algorithmic_efficiency"}, {"score": 0.00240662380079115, "phrase": "sufficient_em_algorithm"}, {"score": 0.002278856202878812, "phrase": "twostep_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["mixture models", " EM algorithm", " data compression", " unsupervised classification", " web-usage mining"], "paper_abstract": "For the classification of very large data sets with a mixture model approach a two-step strategy for the estimation of the mixture is proposed. In the first step data are scaled down using compression techniques. Data compression consists of clustering the single observations into a medium number of groups and the representation of each group by a prototype, i.e. a triple of sufficient statistics (mean vector, covariance matrix, number of observations compressed). In the second step the mixture is estimated by applying an adapted EM algorithm (called sufficient EM) to the sufficient statistics of the compressed data. The estimated mixture allows the classification of observations according to their maximum posterior probability of component membership. The performance of sufficient EM in clustering a real data set from a web-usage mining application is compared to standard EM and the TwoStep clustering algorithm as implemented in SPSS. It turns out that the algorithmic efficiency of the sufficient EM algorithm is much more higher than for standard EM. While the TwoStep algorithm is even faster the results show a lack of stability. (C) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Classification of large data sets with mixture models via sufficient EM", "paper_id": "WOS:000248207700018"}