{"auto_keywords": [{"score": 0.029617278108292247, "phrase": "teragrid"}, {"score": 0.00481495049065317, "phrase": "workflow-based_applications"}, {"score": 0.004504193207111593, "phrase": "large-scale_national_cyberinfrastructure"}, {"score": 0.004235095881928715, "phrase": "large-scale_data"}, {"score": 0.00368673793612343, "phrase": "computational_workflows_increases"}, {"score": 0.0034485432592912917, "phrase": "individual_tasks"}, {"score": 0.00325901518290063, "phrase": "workflow_description"}, {"score": 0.0030017207229983385, "phrase": "workflow_management"}, {"score": 0.002971016979295967, "phrase": "performance_analysis_systems"}, {"score": 0.002895620643297197, "phrase": "earthquake_science_application"}, {"score": 0.0028659990472580154, "phrase": "cybershake"}, {"score": 0.0027505037762786087, "phrase": "scientific_goal"}, {"score": 0.002708399936381754, "phrase": "scec_cybershake_project"}, {"score": 0.0026532596852754525, "phrase": "probabilistic_seismic_hazard_curves"}, {"score": 0.0025992391074270097, "phrase": "southern_california"}, {"score": 0.00248167018931442, "phrase": "cybershake_platform"}, {"score": 0.0022161333611134806, "phrase": "scalability_challenges"}, {"score": 0.0021709933368474223, "phrase": "log_mining_systems"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Scientific workflows", " Distributed applications", " Workflow scalability"], "paper_abstract": "Scientific applications, often expressed as workflows are making use of large-scale national cyberinfrastructure to explore the behavior of systems, search for phenomena in large-scale data, and to conduct many other scientific endeavors As the complexity of the systems being studied grows and as the data set sizes Increase, the scale of the computational workflows increases as well. In some cases, workflows now have hundreds of thousands of individual tasks Managing such scale is difficult from the point of view of workflow description, execution, and analysis In this paper, we describe the challenges faced by workflow management and performance analysis systems when dealing with an earthquake science application. CyberShake, executing on the TeraGrid. The scientific goal of the SCEC CyberShake project is to calculate probabilistic seismic hazard curves for sites in Southern California. For each site of interest, the CyberShake platform includes two large-scale MPI calculations and approximately 840,000 embarrassingly parallel post-processing jobs. In this paper, we show how we approach the scalability challenges in our workflow management and log mining systems. (C) 2009 Elsevier Inc. All rights reserved.", "paper_title": "Scaling up workflow-based applications", "paper_id": "WOS:000278701700005"}