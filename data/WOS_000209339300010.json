{"auto_keywords": [{"score": 0.04838919478929022, "phrase": "dram_errors"}, {"score": 0.009920124272001474, "phrase": "production_systems"}, {"score": 0.008567286759526362, "phrase": "hard_errors"}, {"score": 0.00481495049065317, "phrase": "cosmic_rays"}, {"score": 0.0046175914612292404, "phrase": "detailed_analytical_study"}, {"score": 0.004562362497952548, "phrase": "system_design"}, {"score": 0.004529696178827392, "phrase": "main_memory"}, {"score": 0.004449045430179066, "phrase": "leading_hardware_causes"}, {"score": 0.00441718685609235, "phrase": "machine_crashes"}, {"score": 0.004385555408850022, "phrase": "today's_datacenters"}, {"score": 0.0042004472903194616, "phrase": "memory_errors"}, {"score": 0.004155399041702324, "phrase": "good_understanding"}, {"score": 0.004110831924517065, "phrase": "underlying_characteristics"}, {"score": 0.003509317669345521, "phrase": "open_questions"}, {"score": 0.0033490008882413103, "phrase": "soft_errors"}, {"score": 0.0032423089921154503, "phrase": "typical_patterns"}, {"score": 0.0030609320300175953, "phrase": "diverse_range"}, {"score": 0.0029210388852074208, "phrase": "first_contribution"}, {"score": 0.00284837132359445, "phrase": "dram_error_characteristics"}, {"score": 0.0027575842698911173, "phrase": "large_fraction"}, {"score": 0.0025568135952982345, "phrase": "second_contribution"}, {"score": 0.00247529638625566, "phrase": "measurement_study"}, {"score": 0.0024223970976460173, "phrase": "promising_directions"}, {"score": 0.0023451553255837317, "phrase": "different_protection_mechanisms"}, {"score": 0.0023033100935570755, "phrase": "realistic_error_patterns"}, {"score": 0.002245975340642921, "phrase": "simple_page_retirement_policies"}, {"score": 0.002190064650401852, "phrase": "large_number"}, {"score": 0.0021049977753042253, "phrase": "total_dram"}], "paper_keywords": ["Reliability", " Measurement"], "paper_abstract": "Main memory is one of the leading hardware causes for machine crashes in today's datacenters. Designing, evaluating and modeling systems that are resilient against memory errors requires a good understanding of the underlying characteristics of errors in DRAM in the field. While there have recently been a few first studies on DRAM errors in production systems, these have been too limited in either the size of the data set or the granularity of the data to conclusively answer many of the open questions on DRAM errors. Such questions include, for example, the prevalence of soft errors compared to hard errors, or the analysis of typical patterns of hard errors. In this paper, we study data on DRAM errors collected on a diverse range of production systems in total covering nearly 300 terabyte-years of main memory. As a first contribution, we provide a detailed analytical study of DRAM error characteristics, including both hard and soft errors. We find that a large fraction of DRAM errors in the field can be attributed to hard errors and we provide a detailed analytical study of their characteristics. As a second contribution, the paper uses the results from the measurement study to identify a number of promising directions for designing more resilient systems and evaluates the potential of different protection mechanisms in the light of realistic error patterns. One of our findings is that simple page retirement policies might be able to mask a large number of DRAM errors in production systems, while sacrificing only a negligible fraction of the total DRAM in the system.", "paper_title": "Cosmic Rays Don't Strike Twice: Understanding the Nature of DRAM Errors and the Implications for System Design", "paper_id": "WOS:000209339300010"}