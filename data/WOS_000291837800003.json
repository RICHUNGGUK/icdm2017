{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "non-stationary_environments"}, {"score": 0.03209147612227859, "phrase": "forgetting_function"}, {"score": 0.0047707093928203, "phrase": "recent_machine_learning_challenges"}, {"score": 0.004451546786101008, "phrase": "new_algorithms"}, {"score": 0.0042506731642425275, "phrase": "underlying_problem"}, {"score": 0.004040123129261597, "phrase": "trend_changes"}, {"score": 0.004002972723329302, "phrase": "abrupt_changes"}, {"score": 0.003966162568548304, "phrase": "recurring_contexts"}, {"score": 0.003683565150278339, "phrase": "algorithms_exhibit_difficulties"}, {"score": 0.0034528025626422154, "phrase": "variable_length_windowing"}, {"score": 0.0032215231177304513, "phrase": "new_method"}, {"score": 0.003177154457204775, "phrase": "single-layer_neural_networks"}, {"score": 0.002991823455621222, "phrase": "incremental_online_learning_algorithm"}, {"score": 0.0028566300478906916, "phrase": "new_data"}, {"score": 0.002765636101369536, "phrase": "incremental_learning"}, {"score": 0.00274017282519193, "phrase": "increasing_importance_assignment"}, {"score": 0.0025802660239538353, "phrase": "stable_behavior"}, {"score": 0.0023091093387071593, "phrase": "previous_works"}, {"score": 0.0022772782536385717, "phrase": "proposed_algorithm"}, {"score": 0.0022458849724980904, "phrase": "high_adaptation"}, {"score": 0.0021843879253500894, "phrase": "low_consumption"}, {"score": 0.002164264600379492, "phrase": "computational_resources"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Incremental learning", " Concept drift", " Online learning", " Neural networks"], "paper_abstract": "Recent machine learning challenges require the capability of learning in non-stationary environments. These challenges imply the development of new algorithms that are able to deal with changes in the underlying problem to be learnt. These changes can be gradual or trend changes, abrupt changes and recurring contexts. As the dynamics of the changes can be very different, existing machine learning algorithms exhibit difficulties to cope with them. Several methods using, for instance, ensembles or variable length windowing have been proposed to approach this task. In this work we propose a new method, for single-layer neural networks, that is based on the introduction of a forgetting function in an incremental online learning algorithm. This forgetting function gives a monotonically increasing importance to new data. Due to the combination of incremental learning and increasing importance assignment the network forgets rapidly in the presence of changes while maintaining a stable behavior when the context is stationary. The performance of the method has been tested over several regression and classification problems and its results compared with those of previous works. The proposed algorithm has demonstrated high adaptation to changes while maintaining a low consumption of computational resources. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A robust incremental learning method for non-stationary environments", "paper_id": "WOS:000291837800003"}