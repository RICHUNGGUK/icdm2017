{"auto_keywords": [{"score": 0.049643973872019624, "phrase": "embodied_conversational_agents"}, {"score": 0.04315995617089535, "phrase": "facial_expressions"}, {"score": 0.00481495049065317, "phrase": "richer_emotions"}, {"score": 0.004516856326107826, "phrase": "effective_facial_emotional_expressivity"}, {"score": 0.00429558848734449, "phrase": "valence-arousal-dominance_representation"}, {"score": 0.0040479661688863884, "phrase": "complex_emotional_feelings"}, {"score": 0.003956558420993431, "phrase": "hidden_emotions"}, {"score": 0.0039027036493682887, "phrase": "social_conventions"}, {"score": 0.0037284065559949064, "phrase": "generic_way"}, {"score": 0.0036441884295335502, "phrase": "new_model"}, {"score": 0.0035781807623238905, "phrase": "valence-arousal-dominance_emotion_model"}, {"score": 0.0033872210664940817, "phrase": "antagonist_action_units"}, {"score": 0.003310684108128973, "phrase": "proposed_linear_model"}, {"score": 0.0032358709481309913, "phrase": "large_number"}, {"score": 0.0032064196784562017, "phrase": "autonomous_virtual_humans"}, {"score": 0.003148316319108045, "phrase": "interactive_design"}, {"score": 0.003119659433566212, "phrase": "complex_facial_expressions"}, {"score": 0.002993887707344862, "phrase": "symmetric_facial_expressions"}, {"score": 0.0028863414975346512, "phrase": "emotional_spectrum"}, {"score": 0.002769950045729965, "phrase": "differing_emotions"}, {"score": 0.002551022760887132, "phrase": "static_images"}, {"score": 0.0024706232839499546, "phrase": "expressive_power"}, {"score": 0.002360132221690264, "phrase": "eight_basic_and_complex_emotions"}, {"score": 0.0022649119108225564, "phrase": "basic_emotions"}, {"score": 0.002234031373581961, "phrase": "significant_improvement"}, {"score": 0.0021735249249471614, "phrase": "ambivalent_feelings"}], "paper_keywords": ["asymmetric facial expression", " VAD emotional model", " real-time application", " evaluation study", " embodied agent", " linear model"], "paper_abstract": "In this paper, we propose a method to achieve effective facial emotional expressivity for embodied conversational agents by considering two types of asymmetry when exploiting the valence-arousal-dominance representation of emotions. Indeed, the asymmetry of facial expressions helps to convey complex emotional feelings such as conflicting and/or hidden emotions due to social conventions. To achieve such a higher degree of facial expression in a generic way, we propose a new model for mapping the valence-arousal-dominance emotion model onto a set of 12 scalar facial part actions built mostly by combining pairs of antagonist action units from the Facial Action Coding System. The proposed linear model can automatically drive a large number of autonomous virtual humans or support the interactive design of complex facial expressions over time. By design, our approach produces symmetric facial expressions, as expected for most of the emotional spectrum. However, more complex ambivalent feelings can be produced when differing emotions are applied on the left and right sides of the face. We conducted an experiment on static images produced by our approach to compare the expressive power of symmetric and asymmetric facial expressions for a set of eight basic and complex emotions. Results confirm both the pertinence of our general mapping for expressing basic emotions and the significant improvement brought by asymmetry for expressing ambivalent feelings. Copyright (c) 2013 John Wiley & Sons, Ltd.", "paper_title": "Asymmetric facial expressions: revealing richer emotions for embodied conversational agents", "paper_id": "WOS:000328551100003"}