{"auto_keywords": [{"score": 0.03605781665678908, "phrase": "test_cases"}, {"score": 0.00481495049065317, "phrase": "bounded_model_checking"}, {"score": 0.004556901872357751, "phrase": "automatic_rating"}, {"score": 0.0038624981944755813, "phrase": "programming_exercise"}, {"score": 0.003778253716938922, "phrase": "state-of-the-art_rating_methods"}, {"score": 0.0034975571855450343, "phrase": "exhaustive_set"}, {"score": 0.002713604305939145, "phrase": "automatic_generation"}, {"score": 0.0025678829712650437, "phrase": "experimental_evaluation"}, {"score": 0.00240328454980006, "phrase": "substantial_increase"}, {"score": 0.0021520139248317333, "phrase": "moderate_increase"}, {"score": 0.0021049977753042253, "phrase": "computation_resources"}], "paper_keywords": ["Bounded model checking", " Test case generation", " Program equivalence checking", " Automatic rating"], "paper_abstract": "In this paper we focus on the task of rating solutions to a programming exercise. State-of-the-art rating methods generally examine each solution against an exhaustive set of test cases, typically designed manually. Hence an issue of completeness arises. We propose the application of bounded model checking to the automatic generation of test cases. The experimental evaluation we have performed reveals a substantial increase in accuracy of ratings at a cost of a moderate increase in computation resources needed. Most importantly, application of model checking leads to the finding of errors in solutions that would previously have been classified as correct.", "paper_title": "Incremental test case generation using bounded model checking: an application to automatic rating", "paper_id": "WOS:000355682600007"}