{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dopamine_ramps"}, {"score": 0.004513957235071457, "phrase": "reward_prediction_errors"}, {"score": 0.004398872331144531, "phrase": "temporal_difference"}, {"score": 0.0040186038781533946, "phrase": "phasic_levels"}, {"score": 0.003767203575060969, "phrase": "reward_prediction_error"}, {"score": 0.003310447652300423, "phrase": "recent_observations"}, {"score": 0.0031435968684569112, "phrase": "stratal_dopamine_levels"}, {"score": 0.0026570998719084153, "phrase": "temporal_difference_learning_models"}, {"score": 0.0024586427394152196, "phrase": "key_idea"}, {"score": 0.002304606411453748, "phrase": "quadratic_transformation"}, {"score": 0.0021049977753042253, "phrase": "approximately_linear_ramping"}], "paper_keywords": [""], "paper_abstract": "Temporal difference learning models of dopamine assert that phasic levels of dopamine encode a reward prediction error. However, this hypothesis has been challenged by recent observations of gradually ramping stratal dopamine levels as a goal is approached. This note describes conditions under which temporal difference learning models predict dopamine ramping. The key idea is representational: a quadratic transformation of proximity to the goal implies approximately linear ramping, as observed experimentally.", "paper_title": "Dopamine Ramps Are a Consequence of Reward Prediction Errors", "paper_id": "WOS:000331268400001"}