{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "constructive_algorithm"}, {"score": 0.004706895428234964, "phrase": "feedforward_neural_networks"}, {"score": 0.004457309741979921, "phrase": "arbitrarily_connected_feedforward_neural_network_architectures"}, {"score": 0.004240116703465252, "phrase": "neural_network"}, {"score": 0.004126123386192962, "phrase": "learning_procedure"}, {"score": 0.004070276454809864, "phrase": "main_purpose"}, {"score": 0.003978866233891427, "phrase": "parsimonious_neural_network"}, {"score": 0.0036998728605395384, "phrase": "high_levels"}, {"score": 0.003503497128638099, "phrase": "global_optimization_algorithm"}, {"score": 0.0034403745992576808, "phrase": "population-based_metaheuristics"}, {"score": 0.0033937776228549557, "phrase": "constructive_approach"}, {"score": 0.0033175095965171674, "phrase": "premature_convergence"}, {"score": 0.0031129269142224194, "phrase": "relaxation_strategy"}, {"score": 0.0030707512751243214, "phrase": "learning_error"}, {"score": 0.0030291453183733897, "phrase": "synaptic_weights"}, {"score": 0.002988101395789399, "phrase": "neural_networks"}, {"score": 0.002934237411711622, "phrase": "constructive_mechanism"}, {"score": 0.0028682668647074397, "phrase": "quasi-newton_method"}, {"score": 0.0027407299369891502, "phrase": "current_network"}, {"score": 0.0026790983342162887, "phrase": "mutual_information_criterion"}, {"score": 0.0026188490199688013, "phrase": "benchmark_experiments"}, {"score": 0.0025833500717948343, "phrase": "artificial_and_real_datasets"}, {"score": 0.002525248554951453, "phrase": "new_proposal"}, {"score": 0.002491015208328868, "phrase": "favorable_performance"}, {"score": 0.0024460897937643032, "phrase": "alternative_approaches"}, {"score": 0.0023694065215394593, "phrase": "traditional_mlp"}, {"score": 0.002326669201839705, "phrase": "heterogeneous_experts"}, {"score": 0.002305589818994465, "phrase": "cascade_correlation_networks"}, {"score": 0.0022743274702883456, "phrase": "evolutionary_programming_system"}, {"score": 0.002213065910356615, "phrase": "classification_accuracy"}, {"score": 0.0021632743048552536, "phrase": "obtained_classifier"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural networks", " Constructive learning", " Arbitrary architectures", " Classification"], "paper_abstract": "In this work we present a constructive algorithm capable of producing arbitrarily connected feedforward neural network architectures for classification problems. Architecture and synaptic weights of the neural network should be defined by the learning procedure. The main purpose is to obtain a parsimonious neural network, in the form of a hybrid and dedicate linear/nonlinear classification model, which can guide to high levels of performance in terms of generalization. Though not being a global optimization algorithm, nor a population-based metaheuristics, the constructive approach has mechanisms to avoid premature convergence, by mixing growing and pruning processes, and also by implementing a relaxation strategy for the learning error. The synaptic weights of the neural networks produced by the constructive mechanism are adjusted by a quasi-Newton method, and the decision to grow or prune the current network is based on a mutual information criterion. A set of benchmark experiments, including artificial and real datasets, indicates that the new proposal presents a favorable performance when compared with alternative approaches in the literature, such as traditional MLP, mixture of heterogeneous experts, cascade correlation networks and an evolutionary programming system, in terms of both classification accuracy and parsimony of the obtained classifier. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A constructive algorithm to synthesize arbitrarily connected feedforward neural networks", "paper_id": "WOS:000297830900003"}