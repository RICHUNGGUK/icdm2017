{"auto_keywords": [{"score": 0.049146612112995945, "phrase": "multiple_hardware_accelerators"}, {"score": 0.00481495049065317, "phrase": "dense_linear_systems"}, {"score": 0.00462919213721188, "phrase": "previous_ppopp_paper"}, {"score": 0.004516733575701128, "phrase": "flame_methodology"}, {"score": 0.004406994912939125, "phrase": "supermatrix_runtime_system"}, {"score": 0.004321118442104989, "phrase": "simple_yet_powerful_solution"}, {"score": 0.004257806363128601, "phrase": "dense_linear_algebra_operations"}, {"score": 0.003935420482629049, "phrase": "programmability_problem"}, {"score": 0.003709693561373117, "phrase": "multicore_processor"}, {"score": 0.003601715012418258, "phrase": "cell_b._e."}, {"score": 0.003296212583539166, "phrase": "heterogeneous_distributed-memory_system"}, {"score": 0.0031532895341013297, "phrase": "flame_programming_model"}, {"score": 0.0031070348334684356, "phrase": "new_situation"}, {"score": 0.0030314419455512013, "phrase": "significant_change"}, {"score": 0.0028154895878702633, "phrase": "supermatrix_runtime_scheduling_mechanism"}, {"score": 0.0027605398374811667, "phrase": "software_implementations"}, {"score": 0.002464709648714996, "phrase": "peak_performances"}, {"score": 0.0023117182131967523, "phrase": "matrix-matrix_product"}, {"score": 0.0022777800735119405, "phrase": "cholesky_factorization"}, {"score": 0.0021682027434117095, "phrase": "best_performance_numbers"}, {"score": 0.002125859121671525, "phrase": "new_architecture"}], "paper_keywords": ["Algorithms", " Performance", " GPUs", " algorithms-by-blocks", " dependency analysis", " dynamic scheduling", " out-of-order execution"], "paper_abstract": "In a previous PPoPP paper we showed how the FLAME methodology, combined with the SuperMatrix runtime system, yields a simple yet powerful solution for programming dense linear algebra operations on multicore platforms. In this paper we provide further evidence that this approach solves the programmability problem for this domain by targeting a more complex architecture, composed of a multicore processor and multiple hardware accelerators (GPUs, Cell B. E., etc.), each with its own local memory, resulting in a platform more reminiscent of a heterogeneous distributed-memory system. In particular, we show that the FLAME programming model accommodates this new situation effortlessly so that no significant change needs to be made to the codebase. All complexity is hidden inside the SuperMatrix runtime scheduling mechanism, which incorporates software implementations of standard cache/memory coherence techniques in computer architecture to improve the performance. Our experimental evaluation on a Intel Xeon 8-core host linked to an NVIDIA Tesla S870 platform with four GPUs delivers peak performances around 550 and 450 (single-precision) GFLOPS for the matrix-matrix product and the Cholesky factorization, respectively, which we believe to be the best performance numbers posted on this new architecture for such operations.", "paper_title": "Solving Dense Linear Systems on Platforms with Multiple Hardware Accelerators", "paper_id": "WOS:000272014600015"}