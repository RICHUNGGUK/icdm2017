{"auto_keywords": [{"score": 0.035719248733257394, "phrase": "mt"}, {"score": 0.03409538235988779, "phrase": "motion_energy_model"}, {"score": 0.00481495049065317, "phrase": "neural_network_model"}, {"score": 0.004776669953449352, "phrase": "pattern_motion_selectivity"}, {"score": 0.0047386923107107645, "phrase": "visual_cortex"}, {"score": 0.004682288586254123, "phrase": "large-scale_models"}, {"score": 0.004645057834614705, "phrase": "biological_motion_perception"}, {"score": 0.004517055580214742, "phrase": "required_memory"}, {"score": 0.004445494322321316, "phrase": "network_structure"}, {"score": 0.004392565073466714, "phrase": "computational_power"}, {"score": 0.004288581530198804, "phrase": "neuronal_dynamics"}, {"score": 0.004237512575106522, "phrase": "low-cost_yet_high-performance_approach"}, {"score": 0.004187049198138107, "phrase": "large-scale_neural_network_models"}, {"score": 0.004039221742250308, "phrase": "parallel_processing_capability"}, {"score": 0.004007083686875716, "phrase": "graphics_processing_units"}, {"score": 0.0038043055953253047, "phrase": "two-stage_model"}, {"score": 0.0037740295668681014, "phrase": "visual_area_mt"}, {"score": 0.003669941438286456, "phrase": "first_large-scale_spiking_network"}, {"score": 0.0034290166884431072, "phrase": "cds"}, {"score": 0.0032814236411444022, "phrase": "spatiotemporal_receptive_fields"}, {"score": 0.003203660045180957, "phrase": "simoncelli"}, {"score": 0.0031781522289304255, "phrase": "heeger"}, {"score": 0.003053598205821532, "phrase": "mt_cds_cells"}, {"score": 0.0030171910207671205, "phrase": "wide_range"}, {"score": 0.0029931603112397084, "phrase": "preferred_directions"}, {"score": 0.002898930717812433, "phrase": "electrophysiological_results"}, {"score": 0.00287583918696114, "phrase": "grating_and_plaid_stimuli"}, {"score": 0.0028302049070944944, "phrase": "speed_tuning"}, {"score": 0.0027964537059186893, "phrase": "behavioral_response"}, {"score": 0.002730150696072947, "phrase": "motion_discrimination_task"}, {"score": 0.0026760974046034854, "phrase": "psychophysical_data"}, {"score": 0.0026022112631368223, "phrase": "previous_implementation"}, {"score": 0.0024901940672239806, "phrase": "computational_speed"}, {"score": 0.0024703502756471514, "phrase": "memory_usage"}, {"score": 0.0024408799645640345, "phrase": "full_network"}, {"score": 0.0021997634417824517, "phrase": "gpu."}, {"score": 0.0021049977753042253, "phrase": "computer_vision_researchers"}], "paper_keywords": ["Pattern motion selectivity", " Spiking neural network", " MT", " GPU", " Real-time", " CARLsim"], "paper_abstract": "Simulating large-scale models of biological motion perception is challenging, due to the required memory to store the network structure and the computational power needed to quickly solve the neuronal dynamics. A low-cost yet high-performance approach to simulating large-scale neural network models in real-time is to leverage the parallel processing capability of graphics processing units (GPUs). Based on this approach, we present a two-stage model of visual area MT that we believe to be the first large-scale spiking network to demonstrate pattern direction selectivity. In this model, component-direction-selective (CDS) cells in MT linearly combine inputs from V1 cells that have spatiotemporal receptive fields according to the motion energy model of Simoncelli and Heeger. Pattern-direction-selective (PDS) cells in MT are constructed by pooling over MT CDS cells with a wide range of preferred directions. Responses of our model neurons are comparable to electrophysiological results for grating and plaid stimuli as well as speed tuning. The behavioral response of the network in a motion discrimination task is in agreement with psychophysical data. Moreover, our implementation outperforms a previous implementation of the motion energy model by orders of magnitude in terms of computational speed and memory usage. The full network, which comprises 153,216 neurons and approximately 40 million synapses, processes 20 frames per second of a 40 x 40 input video in real-time using a single off-the-shelf GPU. To promote the use of this algorithm among neuroscientists and computer vision researchers, the source code for the simulator, the network, and analysis scripts are publicly available.", "paper_title": "Efficient Spiking Neural Network Model of Pattern Motion Selectivity in Visual Cortex", "paper_id": "WOS:000339801300008"}