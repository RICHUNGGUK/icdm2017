{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "large_state_space"}, {"score": 0.04827337430415872, "phrase": "reinforcement_learning"}, {"score": 0.044027087409931595, "phrase": "dcs-srl"}, {"score": 0.004746065479362881, "phrase": "main_challenge"}, {"score": 0.004523455475492876, "phrase": "larger_and_more_complex_problems"}, {"score": 0.004416079619543104, "phrase": "scaling_problem"}, {"score": 0.004050116768809519, "phrase": "divide-and-conquer_strategy"}, {"score": 0.0038047474331318974, "phrase": "learning_problem"}, {"score": 0.0037322713756404295, "phrase": "continuous_state_space"}, {"score": 0.0036611708297251645, "phrase": "multiple_smaller_subproblems"}, {"score": 0.0035914198812657897, "phrase": "specific_learning_algorithm"}, {"score": 0.0034392837410790293, "phrase": "limited_available_resources"}, {"score": 0.00334144503360907, "phrase": "component_solutions"}, {"score": 0.0032308006242867224, "phrase": "desired_result"}, {"score": 0.0030203487665054806, "phrase": "weighted_priority_scheduling_algorithm"}, {"score": 0.002948547333717581, "phrase": "scheduling_algorithm"}, {"score": 0.0028100102841877835, "phrase": "problem_space"}, {"score": 0.0026523079380037706, "phrase": "learning_process"}, {"score": 0.0026142814606853037, "phrase": "new_parallel_method"}, {"score": 0.0025767987652419054, "phrase": "dcs-sprl"}, {"score": 0.002467536552997677, "phrase": "parallel_scheduling_architecture"}, {"score": 0.00242047093248845, "phrase": "dcs-sprl_method"}, {"score": 0.002208853811052348, "phrase": "experimental_results"}, {"score": 0.002125371638869951, "phrase": "fast_convergence_speed"}, {"score": 0.0021049977753042253, "phrase": "good_scalability"}], "paper_keywords": ["divide-and-conquer strategy", " parallel schedule", " scalability", " large state space", " continuous state space"], "paper_abstract": "The main challenge in the area of reinforcement learning is scaling up to larger and more complex problems. Aiming at the scaling problem of reinforcement learning, a scalable reinforcement learning method, DCS-SRL, is proposed on the basis of divide-and-conquer strategy, and its convergence is proved. In this method, the learning problem in large state space or continuous state space is decomposed into multiple smaller subproblems. Given a specific learning algorithm, each subproblem can be solved independently with limited available resources. In the end, component solutions can be recombined to obtain the desired result. To address the question of prioritizing subproblems in the scheduler, a weighted priority scheduling algorithm is proposed. This scheduling algorithm ensures that computation is focused on regions of the problem space which are expected to be maximally productive. To expedite the learning process, a new parallel method, called DCS-SPRL, is derived from combining DCS-SRL with a parallel scheduling architecture. In the DCS-SPRL method, the subproblems will be distributed among processors that have the capacity to work in parallel. The experimental results show that learning based on DCS-SPRL has fast convergence speed and good scalability.", "paper_title": "A parallel scheduling algorithm for reinforcement learning in large state space", "paper_id": "WOS:000312193200001"}