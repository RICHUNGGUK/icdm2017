{"auto_keywords": [{"score": 0.04668260212856778, "phrase": "essential_content"}, {"score": 0.04635531096465833, "phrase": "see_data"}, {"score": 0.03455018268948868, "phrase": "reduced_data"}, {"score": 0.00481495049065317, "phrase": "software_effort_estimation_data"}, {"score": 0.004675565200598801, "phrase": "complex_methods"}, {"score": 0.004641351004344458, "phrase": "software_effort_estimation"}, {"score": 0.003977741661623133, "phrase": "contained_information"}, {"score": 0.0038061298318563925, "phrase": "complex_learning_schemes"}, {"score": 0.003668768542734106, "phrase": "euclidean_distance"}, {"score": 0.0031786562062819327, "phrase": "simple_learner"}, {"score": 0.002888747754856871, "phrase": "hold-out_experiments"}, {"score": 0.002825681384008326, "phrase": "mean_and_median_mre"}, {"score": 0.002753837418890651, "phrase": "mbre"}, {"score": 0.002733646947011804, "phrase": "mibre"}, {"score": 0.0027036380093304747, "phrase": "mmer."}, {"score": 0.00264460592618947, "phrase": "quick"}, {"score": 0.00257735013358606, "phrase": "training_data"}, {"score": 0.00240328454980006, "phrase": "cart's_predictions"}, {"score": 0.0022909940601580675, "phrase": "see_datasets"}, {"score": 0.0022492129111567824, "phrase": "complex_estimation_methods"}], "paper_keywords": ["Software cost estimation", " active learning", " analogy", " k-NN"], "paper_abstract": "Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (features) of SEE data, then prunes synonyms (similar features) and outliers (distant instances), then assesses the reduced data by comparing predictions from 1) a simple learner using the reduced data and 2) a state-of-the-art learner (CART) using all data. Performance is measured using hold-out experiments and expressed in terms of mean and median MRE, MAR, PRED(25), MBRE, MIBRE, or MMER. Results: For 18 datasets, QUICK pruned 69 to 96 percent of the training data (median = 89 percent). K 1 nearest neighbor predictions (in the reduced data) performed as well as CART's predictions (using all data). Conclusion: The essential content of some SEE datasets is very small. Complex estimation methods may be overelaborate for such datasets and can be simplified. We offer QUICK as an example of such a simpler SEE method.", "paper_title": "Active Learning and Effort Estimation: Finding the Essential Content of Software Effort Estimation Data", "paper_id": "WOS:000322388400001"}