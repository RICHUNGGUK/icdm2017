{"auto_keywords": [{"score": 0.04107868567004354, "phrase": "base_learners"}, {"score": 0.00481495049065317, "phrase": "extreme_learning_machine_ensembles"}, {"score": 0.004484908028307205, "phrase": "generalization_power"}, {"score": 0.004342430078306461, "phrase": "learner_models"}, {"score": 0.004286708921931472, "phrase": "sampling_and_optimization_techniques"}, {"score": 0.003967021436047945, "phrase": "selective_collection"}, {"score": 0.003767203575060969, "phrase": "effective_implementation"}, {"score": 0.0035543706594585076, "phrase": "open_problem"}, {"score": 0.0034191818674096453, "phrase": "evolutionary_approach"}, {"score": 0.0032051601987130207, "phrase": "model_diversity"}, {"score": 0.003163985877966426, "phrase": "fitness_function"}, {"score": 0.0029467742734417255, "phrase": "optimal_solution"}, {"score": 0.0029089095588612007, "phrase": "ensemble_size_control"}, {"score": 0.0028530201960995896, "phrase": "comprehensive_comparison"}, {"score": 0.002726740692380339, "phrase": "basic_elm"}, {"score": 0.0025892330566358503, "phrase": "neural_networks"}, {"score": 0.0023958326775913165, "phrase": "proposed_method"}, {"score": 0.002319566486828135, "phrase": "simple_average"}], "paper_keywords": ["Learner model ensembles", " Extreme learning machines", " Evolutionary computation", " Generalization capability", " Robustness"], "paper_abstract": "Ensemble learning aims to improve the generalization power and the reliability of learner models through sampling and optimization techniques. It has been shown that an ensemble constructed by a selective collection of base learners outperforms favorably. However, effective implementation of such an ensemble from a given learner pool is still an open problem. This paper presents an evolutionary approach for constituting extreme learning machine (ELM) ensembles. Our proposed algorithm employs the model diversity as fitness function to direct the selection of base learners, and produces an optimal solution with ensemble size control. A comprehensive comparison is carried out, where the basic ELM is used to generate a set of neural networks and 12 benchmarked regression datasets are employed in simulations. Our reporting results demonstrate that the proposed method outperforms other ensembling techniques, including simple average, bagging and adaboost, in terms of both effectiveness and efficiency. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Evolutionary extreme learning machine ensembles with size control", "paper_id": "WOS:000313761500012"}