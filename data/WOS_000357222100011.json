{"auto_keywords": [{"score": 0.04152945160027967, "phrase": "h-o_actions"}, {"score": 0.00790133773734513, "phrase": "haptic_signals"}, {"score": 0.0077114392462153975, "phrase": "pressure_sensors"}, {"score": 0.00481495049065317, "phrase": "haptic-ostensive_actions"}, {"score": 0.004781498797706604, "phrase": "collaborative_multimodal_human-human_dialogues"}, {"score": 0.004731754442072492, "phrase": "robohelper"}, {"score": 0.0046176782475198085, "phrase": "assistive_robots"}, {"score": 0.004459445258869177, "phrase": "multimodal_dialogue_architecture"}, {"score": 0.004413036532585712, "phrase": "collaborative_task-oriented_human-human_dialogue"}, {"score": 0.004217425501814589, "phrase": "specific_type"}, {"score": 0.0041589926969138585, "phrase": "haptic-ostensive"}, {"score": 0.003988487841970088, "phrase": "collaborative_dialogue"}, {"score": 0.0038249462699142733, "phrase": "referring_function"}, {"score": 0.0036425727071563965, "phrase": "realistic_setting"}, {"score": 0.0035176353680157367, "phrase": "unobtrusive_sensory_glove"}, {"score": 0.003468864442215017, "phrase": "multiple_annotations"}, {"score": 0.0033851312604425516, "phrase": "find_corpus"}, {"score": 0.0033615796302502547, "phrase": "supervised_machine_learning"}, {"score": 0.0032462487283717546, "phrase": "reference_resolution"}, {"score": 0.0032012284065895537, "phrase": "classification_modules"}, {"score": 0.0031678720736808574, "phrase": "corpus_analysis"}, {"score": 0.0030591665967282886, "phrase": "crucial_role"}, {"score": 0.0028527866590859967, "phrase": "true_human-robot_interaction"}, {"score": 0.0027452574218375593, "phrase": "real_time"}, {"score": 0.002678944072162297, "phrase": "annotated_categories"}, {"score": 0.0025333084852052147, "phrase": "actual_human_robot_interaction"}, {"score": 0.002506894277338543, "phrase": "last_past"}, {"score": 0.0024721021985994115, "phrase": "additional_experiments"}, {"score": 0.002387214876538434, "phrase": "sensory_glove"}, {"score": 0.0022105600003496225, "phrase": "classification_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Haptic-Ostensive actions", " Multimodal dialogues", " Reference resolution", " Dialogue act classification"], "paper_abstract": "The RoboHelper project has the goal of developing assistive robots for the elderly. One crucial component of such a robot is a multimodal dialogue architecture, since collaborative task-oriented human-human dialogue is inherently multimodal. In this paper, we focus on a specific type of interaction, Haptic-Ostensive (H-O) actions, that are pervasive in collaborative dialogue. H-O actions manipulate objects, but they also often perform a referring function. We collected 20 collaborative task-oriented human-human dialogues between a helper and an elderly person in a realistic setting. To collect the haptic signals, we developed an unobtrusive sensory glove with pressure sensors. Multiple annotations were then conducted to build the Find corpus. Supervised machine learning was applied to these annotations in order to develop reference resolution and dialogue act classification modules. Both corpus analysis, and these two modules show that H-O actions play a crucial role in interaction: models that include H-O actions, and other extra-linguistic information such as pointing gestures, perform better. For true human-robot interaction, all communicative intentions must of course be recognized in real time, not on the basis of annotated categories. To demonstrate that our corpus analysis is not an end in itself, but can inform actual human robot interaction, the last past of our paper presents additional experiments on recognizing H-O actions from the haptic signals measured through the sensory glove. We show that even though pressure sensors are relatively imprecise and the data provided by the glove is noisy, the classification algorithms can successfully identify actions of interest within subjects. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "The roles and recognition of Haptic-Ostensive actions in collaborative multimodal human-human dialogues", "paper_id": "WOS:000357222100011"}