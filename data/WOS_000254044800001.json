{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "recent_studies"}, {"score": 0.004734175871914184, "phrase": "highly_associative_caches"}, {"score": 0.004694295735569168, "phrase": "performance_gap"}, {"score": 0.004550908278609338, "phrase": "theoretical_optimal_replacement_algorithms"}, {"score": 0.0044493728074453, "phrase": "alternative_replacement_algorithms"}, {"score": 0.004411881183253102, "phrase": "cache_performance"}, {"score": 0.004374704085463403, "phrase": "lru_replacement"}, {"score": 0.004205293427426673, "phrase": "long_time"}, {"score": 0.004146386253535282, "phrase": "lru_line"}, {"score": 0.00407678138571414, "phrase": "cache_capacity"}, {"score": 0.003974550062454033, "phrase": "multilevel_caches"}, {"score": 0.003952181032417829, "phrase": "temporal_reuse_patterns"}, {"score": 0.0037990701686283672, "phrase": "filtering_effect"}, {"score": 0.0033552975070941416, "phrase": "new_counter-based_approach"}, {"score": 0.003271099035362546, "phrase": "former_problem"}, {"score": 0.0031001995131925896, "phrase": "latter_problem"}, {"score": 0.0030653708657869755, "phrase": "never-reaccessed_lines"}, {"score": 0.0028969927933916676, "phrase": "single_counter-based_mechanism"}, {"score": 0.0027767900889698713, "phrase": "event_counter"}, {"score": 0.0025439032008217704, "phrase": "line's_threshold"}, {"score": 0.002343716482198893, "phrase": "aip"}, {"score": 0.002330502853933639, "phrase": "lvp"}, {"score": 0.002147071287303209, "phrase": "average_speedups"}], "paper_keywords": ["caches", " counter-based algorithms", " cache replacement algorithms", " cache bypassing", " cache misses"], "paper_abstract": "Recent studies have shown that, in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, motivating the design of alternative replacement algorithms to improve cache performance. In LRU replacement, a line, after its last use, remains in the cache for a long time until it becomes the LRU line. Such deadlines unnecessarily reduce the cache capacity available for other lines. In addition, in multilevel caches, temporal reuse patterns are often inverted, showing in the L1 cache but, due to the filtering effect of the L1 cache, not showing in the L2 cache. At the L2, these lines appear to be brought in the cache but are never reaccessed until they are replaced. These lines unnecessarily pollute the L2 cache. This paper proposes a new counter-based approach to deal with the above problems. For the former problem, we predict lines that have become dead and replace them early from the L2 cache. For the latter problem, we identify never-reaccessed lines, bypass the L2 cache, and place them directly in the L1 cache. Both techniques are achieved through a single counter-based mechanism. In our approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest such as certain cache accesses occurs. When the counter reaches a threshold, the line \"expires\" and becomes replaceable. Each line's threshold is unique and is dynamically learned. We propose and evaluate two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 capacity-constrained SPEC2000 benchmarks by up to 48 percent and 15 percent on average (7 percent on average for the whole 21 Spec2000 benchmarks). Cache bypassing further reduces L2 cache pollution and improves the average speedups to 17 percent (8 percent for the whole 21 Spec2000 benchmarks).", "paper_title": "Counter-based cache replacement and bypassing algorithms", "paper_id": "WOS:000254044800001"}