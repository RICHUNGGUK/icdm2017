{"auto_keywords": [{"score": 0.026858423457009232, "phrase": "ar"}, {"score": 0.00481495049065317, "phrase": "robust_face_recognition"}, {"score": 0.004782587436296574, "phrase": "sparse_coding"}, {"score": 0.004546666837896004, "phrase": "face_recognition"}, {"score": 0.0045160988337296605, "phrase": "sparse_representation"}, {"score": 0.004425616928975799, "phrase": "considerable_attention"}, {"score": 0.00430777589162606, "phrase": "holistic_descriptors"}, {"score": 0.004278807087784626, "phrase": "closed-set_identification_applications"}, {"score": 0.004067635935640407, "phrase": "sufficient_samples"}, {"score": 0.003751143724152193, "phrase": "face_verification_scenario"}, {"score": 0.003344332868248692, "phrase": "alternative_approach"}, {"score": 0.0033218213825698417, "phrase": "sr-based_face_verification"}, {"score": 0.0032332756920184177, "phrase": "local_image_patches"}, {"score": 0.0031898890682164212, "phrase": "entire_face"}, {"score": 0.0031577303835881964, "phrase": "obtained_sparse_signals"}, {"score": 0.0030839447259626215, "phrase": "multiple_region_descriptors"}, {"score": 0.0030220695120265974, "phrase": "overall_face_descriptor"}, {"score": 0.0029714533918949798, "phrase": "deliberate_loss"}, {"score": 0.002951444412262927, "phrase": "spatial_relations"}, {"score": 0.002843772431469196, "phrase": "resulting_descriptor"}, {"score": 0.0027307746993761035, "phrase": "proposed_framework"}, {"score": 0.0026605885630213723, "phrase": "sann"}, {"score": 0.0026579417954765137, "phrase": "sparse_autoencoder_neural_network"}, {"score": 0.002629061651920931, "phrase": "feret"}, {"score": 0.0025609496151442696, "phrase": "gaussian_mixture_models"}, {"score": 0.0025436978068459565, "phrase": "thorough_experiments"}, {"score": 0.0024822506218979963, "phrase": "banca"}, {"score": 0.002478307955771211, "phrase": "chokepoint"}, {"score": 0.0024179512279236207, "phrase": "local_sr_approach"}, {"score": 0.0021700361584578633, "phrase": "considerably_higher_computational_cost"}, {"score": 0.0021408862709571615, "phrase": "sann-based_and_probabilistic_encoding"}, {"score": 0.0021049977753042253, "phrase": "higher_recognition_rates"}], "paper_keywords": [""], "paper_abstract": "In the field of face recognition, sparse representation (SR) has received considerable attention during the past few years, with a focus on holistic descriptors in closed-set identification applications. The underlying assumption in such SR-based methods is that each class in the gallery has sufficient samples and the query lies on the subspace spanned by the gallery of the same class. Unfortunately, such an assumption is easily violated in the face verification scenario, where the task is to determine if two faces (where one or both have not been seen before) belong to the same person. In this study, the authors propose an alternative approach to SR-based face verification, where SR encoding is performed on local image patches rather than the entire face. The obtained sparse signals are pooled via averaging to form multiple region descriptors, which then form an overall face descriptor. Owing to the deliberate loss of spatial relations within each region (caused by averaging), the resulting descriptor is robust to misalignment and various image deformations. Within the proposed framework, they evaluate several SR encoding techniques: l(1)-minimisation, Sparse Autoencoder Neural Network (SANN) and an implicit probabilistic technique based on Gaussian mixture models. Thorough experiments on AR, FERET, exYaleB, BANCA and ChokePoint datasets show that the local SR approach obtains considerably better and more robust performance than several previous state-of-the-art holistic SR methods, on both the traditional closed-set identification task and the more applicable face verification task. The experiments also show that l(1)-minimisation-based encoding has a considerably higher computational cost when compared with SANN-based and probabilistic encoding, but leads to higher recognition rates.", "paper_title": "On robust face recognition via sparse coding: the good, the bad and the ugly", "paper_id": "WOS:000346562800002"}