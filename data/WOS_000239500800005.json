{"auto_keywords": [{"score": 0.04957466661795797, "phrase": "self-organizing_distinctive-state_abstraction"}, {"score": 0.03363896917055258, "phrase": "soda_agent"}, {"score": 0.00481495049065317, "phrase": "navigation_behavior"}, {"score": 0.004615202066192795, "phrase": "reinforcement_learning_research"}, {"score": 0.004021267064832957, "phrase": "robot_navigation"}, {"score": 0.003978866233891427, "phrase": "high-resolution_sensors"}, {"score": 0.003895458412283213, "phrase": "soda"}, {"score": 0.003674738405118697, "phrase": "continuous_world"}, {"score": 0.0034482025128821548, "phrase": "high-level_features"}, {"score": 0.0033937776228549557, "phrase": "temporally_extended_actions"}, {"score": 0.003304965389155549, "phrase": "distinctive_states"}, {"score": 0.0031176485976102688, "phrase": "self-organizing_feature_map"}, {"score": 0.0030199761431186434, "phrase": "high-level_perceptual_features"}, {"score": 0.0027889366191837504, "phrase": "high-level_actions"}, {"score": 0.0026307897899760383, "phrase": "control_laws"}, {"score": 0.0025214217930287003, "phrase": "local_maxima"}, {"score": 0.0024947959375982614, "phrase": "feature_activations"}, {"score": 0.0024037949232582462, "phrase": "simulated_robot_navigation_task"}, {"score": 0.0021049977753042253, "phrase": "local_actions"}], "paper_keywords": ["developmental robotics", " self-organization", " reinforcement learning", " robot navigation"], "paper_abstract": "A major challenge in reinforcement learning research is to extend the methods that have worked well on discrete, short-range, low-dimensional problems to continuous, high-diameter, high-dimensional problems, such as robot navigation using high-resolution sensors. Self-organizing distinctive-state abstraction (SODA) is a new, generic method by which a robot in a continuous world can better learn to navigate, by learning a set of high-level features and building temporally extended actions to carry it between distinctive states based on those features. A SODA agent first uses a self-organizing feature map to develop a set of high-level perceptual features while exploring the environment with primitive, local actions. The agent then builds a set of high-level actions composed of generic trajectory-following and hill-climbing control laws that carry it between the states at local maxima of feature activations. In an experiment on a simulated robot navigation task, the SODA agent learns to perform a task requiring 300 small-scale, local actions using as few as nine new, temporally extended actions, significantly improving learning time over navigating with the local actions.", "paper_title": "Developing navigation behavior through self-organizing distinctive-state abstraction", "paper_id": "WOS:000239500800005"}