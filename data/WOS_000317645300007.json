{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "limited_training_data"}, {"score": 0.02583247883688493, "phrase": "new_method"}, {"score": 0.004684157900507474, "phrase": "novel_method"}, {"score": 0.004641351004344458, "phrase": "audio-visual_feature-level_fusion"}, {"score": 0.004494570360108983, "phrase": "facial_modalities"}, {"score": 0.004273196830970426, "phrase": "prior_knowledge"}, {"score": 0.004062682236016889, "phrase": "limited_amount"}, {"score": 0.004025531820226182, "phrase": "training_data"}, {"score": 0.0038802832668554457, "phrase": "short_training_speech_segment"}, {"score": 0.0038271712887391015, "phrase": "single_training_facial_image"}, {"score": 0.0037060428445111694, "phrase": "new_multimodal_feature_representation"}, {"score": 0.0036553070575015344, "phrase": "modified_cosine_similarity"}, {"score": 0.0035395987202335223, "phrase": "bimodal_features"}, {"score": 0.003427540517249369, "phrase": "vastly_differing_data_rates"}, {"score": 0.003349670229755675, "phrase": "optimal_feature_selection"}, {"score": 0.0033190180867511605, "phrase": "multicondition_training"}, {"score": 0.0030554300179552415, "phrase": "unknown_bimodal_corruption"}, {"score": 0.002931569771634119, "phrase": "bimodal_dataset"}, {"score": 0.002878140062594849, "phrase": "spidre_speaker_recognition_database"}, {"score": 0.002851790448644913, "phrase": "ar_face_recognition_database"}, {"score": 0.002825681384008326, "phrase": "variable_noise_corruption"}, {"score": 0.0027361626349903744, "phrase": "face_images"}, {"score": 0.0026986685585382347, "phrase": "system's_speaker_identification_performance"}, {"score": 0.0026616868981501006, "phrase": "spidre"}, {"score": 0.002613163083407071, "phrase": "facial_identification_performance"}, {"score": 0.00257735013358606, "phrase": "ar_database"}, {"score": 0.002405496949144104, "phrase": "multimodal_fusion"}, {"score": 0.0023725232579531273, "phrase": "significantly_improved_accuracy"}, {"score": 0.0023400004981080818, "phrase": "unimodal_systems"}, {"score": 0.002194011800331242, "phrase": "improved_identification_accuracy"}, {"score": 0.0021539955106204354, "phrase": "bimodal_systems"}, {"score": 0.002124461969404541, "phrase": "multicondition_model_training"}, {"score": 0.0021049977753042253, "phrase": "missing-feature_decoding"}], "paper_keywords": ["Limited training data", " multimodal fusion", " noisy speech", " occluded face", " person identification", " robustness"], "paper_abstract": "This paper presents a novel method of audio-visual feature-level fusion for person identification where both the speech and facial modalities may be corrupted, and there is a lack of prior knowledge about the corruption. Furthermore, we assume there are limited amount of training data for each modality (e.g., a short training speech segment and a single training facial image for each person). A new multimodal feature representation and a modified cosine similarity are introduced to combine and compare bimodal features with limited training data, as well as vastly differing data rates and feature sizes. Optimal feature selection and multicondition training are used to reduce the mismatch between training and testing, thereby making the system robust to unknown bimodal corruption. Experiments have been carried out on a bimodal dataset created from the SPIDRE speaker recognition database and AR face recognition database with variable noise corruption of speech and occlusion in the face images. The system's speaker identification performance on the SPIDRE database, and facial identification performance on the AR database, is comparable with the literature. Combining both modalities using the new method of multimodal fusion leads to significantly improved accuracy over the unimodal systems, even when both modalities have been corrupted. The new method also shows improved identification accuracy compared with the bimodal systems based on multicondition model training or missing-feature decoding alone.", "paper_title": "Robust Multimodal Person Identification With Limited Training Data", "paper_id": "WOS:000317645300007"}