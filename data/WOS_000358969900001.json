{"auto_keywords": [{"score": 0.04151111040011756, "phrase": "latent_variables"}, {"score": 0.009902952514198086, "phrase": "unsupervised_learning_tasks"}, {"score": 0.00481495049065317, "phrase": "latent-variable_estimation"}, {"score": 0.0047732596509542135, "phrase": "bayesian_semi-supervised_learning"}, {"score": 0.004610058444728656, "phrase": "gaussian_mixture_models"}, {"score": 0.004375613460102577, "phrase": "observable_and_latent_variables"}, {"score": 0.00428146713705618, "phrase": "observable_data"}, {"score": 0.004225950334493496, "phrase": "underlying_data-generation_process"}, {"score": 0.004081385289516914, "phrase": "cluster_analysis"}, {"score": 0.0038906171474520756, "phrase": "observable_ones"}, {"score": 0.0032830808525570903, "phrase": "labeled_data"}, {"score": 0.003170667618004276, "phrase": "sufficient_theoretical_analysis"}, {"score": 0.0030092017034068666, "phrase": "previous_study"}, {"score": 0.0029701328654852246, "phrase": "distribution-based_error_function"}, {"score": 0.002855934865505357, "phrase": "unsupervised_learning"}, {"score": 0.002831158147480665, "phrase": "generative_models"}, {"score": 0.002652039172817654, "phrase": "bayes_method"}, {"score": 0.002583597507349819, "phrase": "maximum-likelihood_method"}, {"score": 0.0025500402081330394, "phrase": "present_paper"}, {"score": 0.002516917671182532, "phrase": "asymptotic_forms"}, {"score": 0.0024842242921857705, "phrase": "error_function"}, {"score": 0.0024628890889690164, "phrase": "bayesian"}, {"score": 0.0023886641270884973, "phrase": "discriminative_and_generative_models"}, {"score": 0.0023168823213024856, "phrase": "generative_model"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Latent-variable estimation", " Generative and discriminative models", " Bayes statistics"], "paper_abstract": "Hierarchical probabilistic models, such as Gaussian mixture models, are widely used for unsupervised learning tasks. These models consist of observable and latent variables, which represent the observable data and the underlying data-generation process, respectively. Unsupervised learning tasks, such as cluster analysis, are regarded as estimations of latent variables based on the observable ones. The estimation of latent variables in semi-supervised learning, where some labels are observed, will be more precise than that in unsupervised, and one of the concerns is to clarify the effect of the labeled data. However, there has not been sufficient theoretical analysis of the accuracy of the estimation of latent variables. In a previous study, a distribution-based error function was formulated, and its asymptotic form was calculated for unsupervised learning with generative models. It has been shown that, for the estimation of latent variables, the Bayes method is more accurate than the maximum-likelihood method. The present paper reveals the asymptotic forms of the error function in Bayesian semi-supervised learning for both discriminative and generative models. The results show that the generative model, which uses all of the given data, performs better when the model is well specified. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Accuracy of latent-variable estimation in Bayesian semi-supervised learning", "paper_id": "WOS:000358969900001"}