{"auto_keywords": [{"score": 0.04918051837039986, "phrase": "goodness_measures"}, {"score": 0.02549498893980503, "phrase": "error_reduction"}, {"score": 0.00481495049065317, "phrase": "unsupervised_and_supervised_word_segmentation"}, {"score": 0.004566123591480369, "phrase": "unsupervised_and_supervised_segmentation"}, {"score": 0.004531758803009421, "phrase": "chinese"}, {"score": 0.004412965940679449, "phrase": "present_state-of-the_art"}, {"score": 0.003983512316698646, "phrase": "raw_texts"}, {"score": 0.0036784938661226104, "phrase": "pre-segmented_corpus"}, {"score": 0.0035957006001349915, "phrase": "known_words"}, {"score": 0.003501452436803395, "phrase": "unseen_ones"}, {"score": 0.0034226302212225206, "phrase": "character_tagging"}, {"score": 0.0032209766681045365, "phrase": "description_length_gain"}, {"score": 0.0031845067058084583, "phrase": "word_discovery"}, {"score": 0.0031484483754909026, "phrase": "simple_strategies"}, {"score": 0.002951673109860487, "phrase": "accessor_variety"}, {"score": 0.002798865807872575, "phrase": "supervised_learning"}, {"score": 0.0027358163024467854, "phrase": "conditional_random_fields_model"}, {"score": 0.0027048247303526583, "phrase": "goodness_scores"}, {"score": 0.0026640466056415298, "phrase": "feature_values"}, {"score": 0.002488001663771768, "phrase": "benchmark_data_sets"}, {"score": 0.002350196395175188, "phrase": "best_performance"}, {"score": 0.00233240715947344, "phrase": "closed_test"}, {"score": 0.0022625835448873495, "phrase": "five_closed_test_tracks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Chinese word segmentation", " Unsupervised segmentation", " Unknown word detection", " Conditional random fields", " Character tagging", " Description length gain", " Accessor variety", " Boundary entropy"], "paper_abstract": "This study explores the feasibility of integrating unsupervised and supervised segmentation of Chinese texts for enhancing performance beyond the present state-of-the art, focusing on the critical role of the former in enhancing the latter. Following only a pre-defined goodness measure, unsupervised segmentation has the advantage of discovering many new words in raw texts, but it has the disadvantage of inevitably corrupting many known. By contrast, supervised segmentation conventionally trained only on a pre-segmented corpus is particularly good at identifying known words but possesses little intrinsic mechanism to deal with unseen ones until it is formulated as character tagging. To combine their strengths, we empirically evaluate a set of goodness measures, among which description length gain excels in word discovery, but simple strategies like word candidate pruning and assemble segmentation can further improve it. Interestingly, however, accessor variety and boundary entropy, two other goodness measures, are found more effective in enhancing the supervised learning of character tagging with the conditional random fields model. All goodness scores are discretized into feature values to enrich this model. The success of this approach has been verified by our experiments on the benchmark data sets of the last two Bakeoffs: on average, it achieves an error reduction of 6.39% over the best performance of closed test in Bakeoff-3 and ranks first in all five closed test tracks in Bakeoff-4, outperforming other participants significantly and consistently by an error reduction of 8.96%. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Integrating unsupervised and supervised word segmentation: The role of goodness measures", "paper_id": "WOS:000284511100011"}