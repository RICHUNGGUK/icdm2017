{"auto_keywords": [{"score": 0.04228756662661977, "phrase": "near-optimal_policy"}, {"score": 0.00481495049065317, "phrase": "large_markov_decision_processes"}, {"score": 0.004513957235071457, "phrase": "markov_decision"}, {"score": 0.004379977556546879, "phrase": "complexity_point"}, {"score": 0.004106063237069123, "phrase": "efficient_approximation_method"}, {"score": 0.0039841418890102925, "phrase": "planning_problem"}, {"score": 0.003949974260471287, "phrase": "discounted_mdps"}, {"score": 0.003899269293218826, "phrase": "satisfaction_probabilities"}, {"score": 0.003865826699096508, "phrase": "interesting_properties"}, {"score": 0.0037188360359702182, "phrase": "markov_chain"}, {"score": 0.003426556090707852, "phrase": "first_one"}, {"score": 0.0033680032592969633, "phrase": "sparse_sampling"}, {"score": 0.0032398798149196432, "phrase": "multiplicative_weights"}, {"score": 0.0032259472087775138, "phrase": "update_algorithm"}, {"score": 0.0031435968684569112, "phrase": "first_approximation_method"}, {"score": 0.003037047249598515, "phrase": "state_space"}, {"score": 0.002959515743428509, "phrase": "mdp."}, {"score": 0.0029089095588612007, "phrase": "complete_analysis"}, {"score": 0.0027503564589458837, "phrase": "targeted_quality"}, {"score": 0.002680114113899346, "phrase": "second_approach"}, {"score": 0.0023346234464901978, "phrase": "lassaigne"}, {"score": 0.0023145690608400425, "phrase": "peyronnet"}, {"score": 0.002255431275670053, "phrase": "acm_symposium"}, {"score": 0.002236055625099805, "phrase": "applied_computing"}], "paper_keywords": ["Probabilistic verification", " Machine learning", " Markov decision processes"], "paper_abstract": "We focus on the planning and verification problems for very large probabilistic systems, such as Markov decision processes (MDPs), from a complexity point of view. More precisely, we deal with the problem of designing an efficient approximation method to compute a near-optimal policy for the planning problem in discounted MDPs and the satisfaction probabilities of interesting properties, like reachability or safety, over the Markov chain obtained by restricting the MDP to the near-optimal policy. In this paper, we present two different approaches. The first one is based on sparse sampling while the second uses a variant of the multiplicative weights update algorithm. The complexity of the first approximation method is independent of the size of the state space and uses only a probabilistic generator of the MDP. We give a complete analysis of this approach, for which the control parameter is mainly the targeted quality of the approximation. The second approach is more prospective and is different in the sense that the method can be controlled dynamically by observing its speed of convergence. Parts of this paper have already been presented in Lassaigne and Peyronnet (in Proceedings of the ACM Symposium on applied computing, SAC 2012, pp 1314-1319, ACM 2012), by the same authors.", "paper_title": "Approximate planning and verification for large Markov decision processes", "paper_id": "WOS:000357485500006"}