{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "naive_bayes_classifier"}, {"score": 0.02677355608546669, "phrase": "decision_regions"}, {"score": 0.008172017212670437, "phrase": "decision_boundaries"}, {"score": 0.007869084865567448, "phrase": "nbtree_algorithm"}, {"score": 0.007463610173485331, "phrase": "classifier_tree"}, {"score": 0.004600870784042767, "phrase": "data_space"}, {"score": 0.004426454679871459, "phrase": "sd_algorithm"}, {"score": 0.004396267325338022, "phrase": "decision_tree_algorithms"}, {"score": 0.00421668332357505, "phrase": "decision_tree"}, {"score": 0.0039986494357811715, "phrase": "non-root_node"}, {"score": 0.0038498211283780484, "phrase": "parent_node"}, {"score": 0.0035820833812158005, "phrase": "root_and_leaf_classifiers"}, {"score": 0.0034618177773967015, "phrase": "naive_bayes_classifiers"}, {"score": 0.0033329032920236994, "phrase": "second_division"}, {"score": 0.002996840657035727, "phrase": "two-level_classifier_trees"}, {"score": 0.002940487679224863, "phrase": "root_classifiers"}, {"score": 0.0025357050267224715, "phrase": "uc_irvine"}, {"score": 0.0024785689583828796, "phrase": "experiment_results"}, {"score": 0.002413531395475291, "phrase": "better_generalization_abilities"}, {"score": 0.0023861822265800795, "phrase": "nbtree"}, {"score": 0.0023591417077156555, "phrase": "averaged_one-dependence_estimators"}, {"score": 0.0022711961447468114, "phrase": "support_vector_machine"}, {"score": 0.0022284577808744316, "phrase": "leaf_classifiers"}, {"score": 0.0021049977753042253, "phrase": "argument_values"}], "paper_keywords": ["Naive Bayes classifier", " Decision region", " NBTree", " C4.5 algorithm", " Support vector machine (SVM)"], "paper_abstract": "Classification can be regarded as dividing the data space into decision regions separated by decision boundaries. In this paper we analyze decision tree algorithms and the NBTree algorithm from this perspective. Thus, a decision tree can be regarded as a classifier tree, in which each classifier on a non-root node is trained in decision regions of the classifier on the parent node. Meanwhile, the NBTree algorithm, which generates a classifier tree with the C4.5 algorithm and the naive Bayes classifier as the root and leaf classifiers respectively, can also be regarded as training naive Bayes classifiers in decision regions of the C4.5 algorithm. We propose a second division (SD) algorithm and three soft second division (SD-soft) algorithms to train classifiers in decision regions of the naive Bayes classifier. These four novel algorithms all generate two-level classifier trees with the naive Bayes classifier as root classifiers. The SD and three SD-soft algorithms can make good use of both the information contained in instances near decision boundaries, and those that may be ignored by the naive Bayes classifier. Finally, we conduct experiments on 30 data sets from the UC Irvine (UCI) repository. Experiment results show that the SD algorithm can obtain better generalization abilities than the NBTree and the averaged one-dependence estimators (AODE) algorithms when using the C4.5 algorithm and support vector machine (SVM) as leaf classifiers. Further experiments indicate that our three SD-soft algorithms can achieve better generalization abilities than the SD algorithm when argument values are selected appropriately.", "paper_title": "Improving naive Bayes classifier by dividing its decision regions", "paper_id": "WOS:000294084100004"}