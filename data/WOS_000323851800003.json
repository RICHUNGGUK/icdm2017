{"auto_keywords": [{"score": 0.04881990207677595, "phrase": "cross-media_retrieval"}, {"score": 0.00481495049065317, "phrase": "inherent_and_external_knowledge"}, {"score": 0.004638602907882106, "phrase": "cross-media"}, {"score": 0.00451659023910662, "phrase": "multimedia_data"}, {"score": 0.004468684611798047, "phrase": "different_modalities"}, {"score": 0.004421284843189731, "phrase": "content-based_methods"}, {"score": 0.004169394481182493, "phrase": "multimedia_retrieval"}, {"score": 0.0041251561573501455, "phrase": "single_modality"}, {"score": 0.004038076978795421, "phrase": "image_retrieval"}, {"score": 0.003995226376780718, "phrase": "audio_retrieval"}, {"score": 0.0035150223642728437, "phrase": "boosted_retrieval_performance"}, {"score": 0.003261934055562238, "phrase": "novel_cross-media_retrieval_approach"}, {"score": 0.002994859407965545, "phrase": "audio_samples"}, {"score": 0.0029159568444629053, "phrase": "isomorphic_feature_subspace"}, {"score": 0.002884979280317279, "phrase": "kernel-based_method"}, {"score": 0.002824005136794786, "phrase": "multimedia_semantics"}, {"score": 0.0027643161113894018, "phrase": "inherent_feature_correlation"}, {"score": 0.0027349450784503273, "phrase": "local_linear_regression"}, {"score": 0.0026771333996253783, "phrase": "graph_model"}, {"score": 0.00260657976055413, "phrase": "external_knowledge"}, {"score": 0.002578880352208335, "phrase": "relevance_feedback"}, {"score": 0.002497531381123025, "phrase": "unified_objective_function"}, {"score": 0.002470987929071079, "phrase": "inherent_and_external_learning_results"}, {"score": 0.002393034183276193, "phrase": "objective_function"}, {"score": 0.0023424328921363585, "phrase": "multimodal_semantic_space"}, {"score": 0.002220551141101661, "phrase": "extensive_experiments"}, {"score": 0.0021735890994119757, "phrase": "proposed_methods"}], "paper_keywords": ["Cross-media retrieval", " Nonlinear learning", " Relevance feedback"], "paper_abstract": "Cross-media retrieval focuses on searching multimedia data of different modalities with content-based methods. However, most of those methods are designed for multimedia retrieval in single modality, such as image retrieval and audio retrieval. Though a few work has focused on cross-media retrieval, the performance is yet to be satisfactory and the potential of using cross-media retrieval for boosted retrieval performance remains largely unexplored. Hence, in this paper, we propose a novel cross-media retrieval approach for general multimedia data, such as image and audio. First, image and audio samples are mapped into an isomorphic feature subspace with kernel-based method; second, multimedia semantics is learned from inherent feature correlation by local linear regression; also a graph model is constructed to utilize external knowledge from relevance feedback; then we build a unified objective function integrating inherent and external learning results, and by solving the objective function we calculate a multimodal semantic space where cross-media retrieval among image and audio is enabled. Extensive experiments have validated the proposed methods with encouraging results. (c) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Fusing inherent and external knowledge with nonlinear learning for cross-media retrieval", "paper_id": "WOS:000323851800003"}