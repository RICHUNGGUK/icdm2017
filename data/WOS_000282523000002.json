{"auto_keywords": [{"score": 0.04749178419281339, "phrase": "pca"}, {"score": 0.010611873228285324, "phrase": "principal_component_analysis"}, {"score": 0.01029153981186116, "phrase": "missing_values"}, {"score": 0.004444037594955185, "phrase": "classical_data_analysis_technique"}, {"score": 0.004362564831572539, "phrase": "linear_transformations"}, {"score": 0.004204054047427536, "phrase": "maximal_amount"}, {"score": 0.0039040344230094164, "phrase": "data_values"}, {"score": 0.0035369291417564606, "phrase": "nonlinear_models"}, {"score": 0.0034083145672246067, "phrase": "bad_locally_optimal_solutions"}, {"score": 0.0033457659530207306, "phrase": "probabilistic_formulation"}, {"score": 0.003244050414264445, "phrase": "good_foundation"}, {"score": 0.002938821840519756, "phrase": "high_dimensional_and_very_sparse_data"}, {"score": 0.0028494425496120228, "phrase": "severe_problem"}, {"score": 0.002814454488050123, "phrase": "traditional_algorithms"}, {"score": 0.0026458370516131255, "phrase": "novel_fast_algorithm"}, {"score": 0.002565345470805401, "phrase": "variational_bayesian_learning"}, {"score": 0.0025338369486914364, "phrase": "different_versions"}, {"score": 0.0024416087858984644, "phrase": "artificial_experiments"}, {"score": 0.002295277579105177, "phrase": "posterior_variance"}, {"score": 0.0022117133940361025, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "netflix_problem"}], "paper_keywords": ["principal component analysis", " missing values", " overfitting", " regularization", " variational Bayes"], "paper_abstract": "Principal component analysis (PCA) is a classical data analysis technique that finds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overfitting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overfitting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artificial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the Netflix problem.", "paper_title": "Practical Approaches to Principal Component Analysis in the Presence of Missing Values", "paper_id": "WOS:000282523000002"}