{"auto_keywords": [{"score": 0.040975329739854946, "phrase": "extrinsic_semantic_context"}, {"score": 0.04036040340270114, "phrase": "video_subtitles"}, {"score": 0.03955424170127063, "phrase": "candidate_annotation_concepts"}, {"score": 0.037416159671967784, "phrase": "key_terms"}, {"score": 0.00481495049065317, "phrase": "news_video_annotation"}, {"score": 0.004764859786310636, "phrase": "automatic_video_annotation"}, {"score": 0.004641890423057823, "phrase": "semantic_gap"}, {"score": 0.004545793387330536, "phrase": "based_video_retrieval"}, {"score": 0.004475022550508119, "phrase": "high_level_concepts"}, {"score": 0.004428452434981208, "phrase": "video_data"}, {"score": 0.004314127287770832, "phrase": "context_information"}, {"score": 0.004202741104973709, "phrase": "important_direction"}, {"score": 0.00396767023935381, "phrase": "novel_video_annotation_refinement_approach"}, {"score": 0.0038049789989278463, "phrase": "intrinsic_context"}, {"score": 0.0034628156490003775, "phrase": "semantic_similarity"}, {"score": 0.003251920460276645, "phrase": "initial_annotation_results"}, {"score": 0.0031678720736808574, "phrase": "textual_information"}, {"score": 0.003118486096035156, "phrase": "similarity_measurements"}, {"score": 0.003085989256759819, "phrase": "google_distance"}, {"score": 0.0030538300202383106, "phrase": "wordnet_distance"}, {"score": 0.0028527866590859967, "phrase": "semantic_relationship"}, {"score": 0.002678944072162297, "phrase": "individual_terms"}, {"score": 0.002476424729269258, "phrase": "final_refinement"}, {"score": 0.0024377918036784336, "phrase": "annotation_results"}, {"score": 0.002362320344521919, "phrase": "annotation_concepts"}, {"score": 0.002337684811187166, "phrase": "comprehensive_experiments"}, {"score": 0.002183684303511462, "phrase": "proposed_annotation_approach"}], "paper_keywords": ["Video annotation", " News video", " Semantic context", " Semantic similarity", " Random walk"], "paper_abstract": "Automatic video annotation is to bridge the semantic gap and facilitate concept based video retrieval by detecting high level concepts from video data. Recently, utilizing context information has emerged as an important direction in such domain. In this paper, we present a novel video annotation refinement approach by utilizing extrinsic semantic context extracted from video subtitles and intrinsic context among candidate annotation concepts. The extrinsic semantic context is formed by identifying a set of key terms from video subtitles. The semantic similarity between those key terms and the candidate annotation concepts is then exploited to refine initial annotation results, while most existing approaches utilize textual information heuristically. Similarity measurements including Google distance and WordNet distance have been investigated for such a refinement purpose, which is different with approaches deriving semantic relationship among concepts from given training datasets. Visualness is also utilized to discriminate individual terms for further refinement. In addition, Random Walk with Restarts (RWR) technique is employed to perform final refinement of the annotation results by exploring the inter-relationship among annotation concepts. Comprehensive experiments on TRECVID 2005 dataset have been conducted to demonstrate the effectiveness of the proposed annotation approach and to investigate the impact of various factors.", "paper_title": "Semantic context based refinement for news video annotation", "paper_id": "WOS:000323750900005"}