{"auto_keywords": [{"score": 0.042309036047671555, "phrase": "video_annotation"}, {"score": 0.015150061092814705, "phrase": "video_annotation_accuracy"}, {"score": 0.00481495049065317, "phrase": "new_method"}, {"score": 0.004713379149462482, "phrase": "temporal_context"}, {"score": 0.004327981760540394, "phrase": "continuous_shots"}, {"score": 0.004191690483929023, "phrase": "relevant_content"}, {"score": 0.003869373085652601, "phrase": "temporal_dependency"}, {"score": 0.0035908627899097407, "phrase": "temporal_context_model"}, {"score": 0.0035150223642728437, "phrase": "redundant_information"}, {"score": 0.0033680968918579717, "phrase": "conditional_random_field"}, {"score": 0.003108911488613549, "phrase": "refined_probability"}, {"score": 0.002884979280317279, "phrase": "temporal_context_information"}, {"score": 0.0028543298631502107, "phrase": "initial_output"}, {"score": 0.0027643161113894018, "phrase": "existing_methods"}, {"score": 0.0027349450784503273, "phrase": "temporal_context_mining"}, {"score": 0.0026345758986796703, "phrase": "different_kinds"}, {"score": 0.00260657976055413, "phrase": "shot_dependency"}, {"score": 0.0025243592424524764, "phrase": "video_annotation_performance"}, {"score": 0.0022806798371116698, "phrase": "large_scale_data"}, {"score": 0.0022324489895237504, "phrase": "extensive_experimental_results"}, {"score": 0.0021735897467873005, "phrase": "trecvid"}], "paper_keywords": ["video annotation", " temporal context", " conditional random field", " video retrieval"], "paper_abstract": "In this paper, we propose a new method to model the temporal context for boosting video annotation accuracy. The motivation of our idea mainly comes from the fact that temporally continuous shots in video are generally with relevant content, so that the performance of video annotation could be comparably boosted by mining the temporal dependency between shots in video. Based on this consideration, we propose a temporal context model to mine the redundant information between shots. By connecting our model with conditional random field and borrowing the learning and inference approaches from it, we could obtain the refined probability of a concept occurring in the shot, which is the leverage of temporal context information and initial output of video annotation. Comparing with existing methods for temporal context mining of video annotation, our model could capture different kinds of shot dependency more accurately to improve the video annotation performance. Furthermore, our model is relatively simple and efficient, which is important for the applications which have large scale data to process. Extensive experimental results on the widely used TRECVID datasets exhibit the effectiveness of our method for improving video annotation accuracy.", "paper_title": "A temporal context model for boosting video annotation", "paper_id": "WOS:000329537800008"}