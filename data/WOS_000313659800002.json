{"auto_keywords": [{"score": 0.03784144720028532, "phrase": "global_address_space"}, {"score": 0.004558748120167778, "phrase": "global_address_spaces"}, {"score": 0.004449280529563265, "phrase": "hardware_parallelism"}, {"score": 0.004289966839994858, "phrase": "language_designers"}, {"score": 0.004136333933790974, "phrase": "massive_parallelism"}, {"score": 0.004012500850311087, "phrase": "task-based_programming_models"}, {"score": 0.003916098498242888, "phrase": "runtime_systems"}, {"score": 0.00386876598761639, "phrase": "dependency_analysis"}, {"score": 0.0034676182423067307, "phrase": "parallel_memory_allocator"}, {"score": 0.0034049123167891955, "phrase": "myrmics_runtime_system"}, {"score": 0.0033230588808076267, "phrase": "multiple_allocator_instances"}, {"score": 0.0032431667821391044, "phrase": "tree_hierarchy"}, {"score": 0.003107935254560724, "phrase": "dynamic_region_support"}, {"score": 0.003070340620983981, "phrase": "distributed_memory_machines"}, {"score": 0.0030147970456037274, "phrase": "myrmics_hierarchical_memory_allocator"}, {"score": 0.0029422942175866057, "phrase": "improved_productivity"}, {"score": 0.0028715299874549245, "phrase": "parallel_programming"}, {"score": 0.0027184540871935284, "phrase": "dynamic_regions"}, {"score": 0.0025892330566358503, "phrase": "convenient_shared_memory_abstraction"}, {"score": 0.0025578967205807843, "phrase": "dynamic_and_irregular_data_structures"}, {"score": 0.002436289099444074, "phrase": "many-core_systems"}, {"score": 0.002406799309495983, "phrase": "system-wide_cache_coherency"}, {"score": 0.0023346234464901978, "phrase": "stand-alone_allocator"}, {"score": 0.0021049977753042253, "phrase": "unified_parallel_c"}], "paper_keywords": ["Parallel Memory Allocator", " GAS"], "paper_abstract": "Constantly increasing hardware parallelism poses more and more challenges to programmers and language designers. One approach to harness the massive parallelism is to move to task-based programming models that rely on runtime systems for dependency analysis and scheduling. Such models generally benefit from the existence of a global address space. This paper presents the parallel memory allocator of the Myrmics runtime system, in which multiple allocator instances organized in a tree hierarchy cooperate to implement a global address space with dynamic region support on distributed memory machines. The Myrmics hierarchical memory allocator is step towards improved productivity and performance in parallel programming. Productivity is improved through the use of dynamic regions in a global address space, which provide a convenient shared memory abstraction for dynamic and irregular data structures. Performance is improved through scaling on many-core systems without system-wide cache coherency. We evaluate the stand-alone allocator on an MPI-based x86 cluster and find that it scales well for up to 512 worker cores, while it can outperform Unified Parallel C by a factor of 3.7-10.7x.", "paper_title": "The Myrmics Memory Allocator: Hierarchical, Message-Passing Allocation for Global Address Spaces", "paper_id": "WOS:000313659800002"}