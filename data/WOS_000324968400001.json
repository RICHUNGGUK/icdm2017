{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "kernel_ridge_regression"}, {"score": 0.03429532035147689, "phrase": "tuning_parameters"}, {"score": 0.0046269981365987915, "phrase": "ridge_regression"}, {"score": 0.004558414405811025, "phrase": "potentially_infinite_number"}, {"score": 0.004513255056769647, "phrase": "nonlinear_transformations"}, {"score": 0.004446349752828045, "phrase": "independent_variables"}, {"score": 0.004209369306023422, "phrase": "data-rich_nonlinear_forecasting_tool"}, {"score": 0.0037724857647796813, "phrase": "forecast_accuracy"}, {"score": 0.0036070601923916196, "phrase": "polynomial_kernels"}, {"score": 0.003553540530701019, "phrase": "gaussian_kernel"}, {"score": 0.0034834197333776656, "phrase": "sinc"}, {"score": 0.0034146585649631692, "phrase": "latter_two_kernels"}, {"score": 0.0031372369299648203, "phrase": "smoothness_measures"}, {"score": 0.0030906666356950887, "phrase": "prediction_function"}, {"score": 0.0027420820738468577, "phrase": "small_grids"}, {"score": 0.0026745503978347143, "phrase": "monte_carlo_study"}, {"score": 0.002634830238219976, "phrase": "practical_usefulness"}, {"score": 0.0025066304722422463, "phrase": "flexible_and_smooth_functional_forms"}, {"score": 0.002457110101089137, "phrase": "gaussian_and_sinc_kernels"}, {"score": 0.0022913497040895586, "phrase": "popular_polynomial_kernels"}, {"score": 0.002268598551466928, "phrase": "general_settings"}, {"score": 0.0021907299375981356, "phrase": "data-generating_process"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Nonlinear forecasting", " Shrinkage estimation", " Kernel methods", " High dimensionality"], "paper_abstract": "Kernel ridge regression is a technique to perform ridge regression with a potentially infinite number of nonlinear transformations of the independent variables as regressors. This method is gaining popularity as a data-rich nonlinear forecasting tool, which is applicable in many different contexts. The influence of the choice of kernel and the setting of tuning parameters on forecast accuracy is investigated. Several popular kernels are reviewed, including polynomial kernels, the Gaussian kernel, and the Sinc kernel. The latter two kernels are interpreted in terms of their smoothing properties, and the tuning parameters associated to all these kernels are related to smoothness measures of the prediction function and to the signal-to-noise ratio. Based on these interpretations, guidelines are provided for selecting the tuning parameters from small grids using cross-validation. A Monte Carlo study confirms the practical usefulness of these rules of thumb. Finally, the flexible and smooth functional forms provided by the Gaussian and Sinc kernels make them widely applicable. Therefore, their use is recommended instead of the popular polynomial kernels in general settings, where no information on the data-generating process is available. (c) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Model selection in kernel ridge regression", "paper_id": "WOS:000324968400001"}