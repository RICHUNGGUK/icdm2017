{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "conditional_gabor"}, {"score": 0.00462919213721188, "phrase": "facial_tracking"}, {"score": 0.004583878086896926, "phrase": "person-specific_facial_action_recognition"}, {"score": 0.004406994912939125, "phrase": "affective_computing_research_field"}, {"score": 0.004257806363128601, "phrase": "big_challenge"}, {"score": 0.004195418010131832, "phrase": "automated_systems"}, {"score": 0.004133940019630129, "phrase": "human_emotions"}, {"score": 0.004093453936415085, "phrase": "video_sequences"}, {"score": 0.003954837422946628, "phrase": "facial_feature_localization"}, {"score": 0.0036733470451885465, "phrase": "coarse-to-fine_paradigm"}, {"score": 0.003566422652043389, "phrase": "facial_fiducial_points"}, {"score": 0.003479692058963528, "phrase": "gabor_filters"}, {"score": 0.0034118234596993836, "phrase": "first_face_image"}, {"score": 0.0033288399506044763, "phrase": "coarsest_level"}, {"score": 0.0032319109979344184, "phrase": "rough_position"}, {"score": 0.0031845067058084583, "phrase": "facial_feature"}, {"score": 0.0030464123943266673, "phrase": "fine_displacement_refinement"}, {"score": 0.0030017207229983385, "phrase": "image_pyramid"}, {"score": 0.002843372121209168, "phrase": "subsequent_frames"}, {"score": 0.0027605398374811667, "phrase": "fast_gabor-phase"}, {"score": 0.0026277995163948263, "phrase": "confidence_measure"}, {"score": 0.002576503437739865, "phrase": "iterative_conditional_disparity_estimation_procedure"}, {"score": 0.0025137856138000014, "phrase": "proposed_tracking_process"}, {"score": 0.002464709648714996, "phrase": "personalized_feature-based_facial_action_recognition_framework"}, {"score": 0.0022890371899188466, "phrase": "experimental_results"}, {"score": 0.0022443390559699974, "phrase": "facial_feature_points"}, {"score": 0.0021896891011033105, "phrase": "high_accuracy"}, {"score": 0.002146926767291475, "phrase": "sufficient_precision"}, {"score": 0.0021049977753042253, "phrase": "better_facial_action_recognition_performance"}], "paper_keywords": ["Gabor wavelets", " Feature extraction", " Tracking", " Facial expression"], "paper_abstract": "Within the affective computing research field, researchers are still facing a big challenge to establish automated systems to recognize human emotions from video sequences. Performances are quite dependent on facial feature localization and tracking. In this paper, we present a method based on a coarse-to-fine paradigm to characterize a set of facial fiducial points using a bank of Gabor filters. When the first face image is captured, the coarsest level is used to estimate a rough position for each facial feature. Afterward, a coarse-to-fine displacement refinement on an image pyramid is performed. The positions are then tracked over the subsequent frames using a modification of a fast Gabor-phase based technique. This includes a redefinition of the confidence measure and introduces an iterative conditional disparity estimation procedure. We used the proposed tracking process to implement a personalized feature-based facial action recognition framework, motivated by the fact that the same facial expression may vary differently across humans. Experimental results show that the facial feature points can be localized with high accuracy and tracked with sufficient precision leading to a better facial action recognition performance.", "paper_title": "Conditional Gabor phase-based disparity estimation applied to facial tracking for person-specific facial action recognition: a preliminary study", "paper_id": "WOS:000360071800024"}