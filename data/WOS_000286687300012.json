{"auto_keywords": [{"score": 0.04777760737094776, "phrase": "inex"}, {"score": 0.03719614042404607, "phrase": "inex_collections"}, {"score": 0.03486576940272885, "phrase": "fixed_amount"}, {"score": 0.0346288127133061, "phrase": "assessment_effort"}, {"score": 0.00481495049065317, "phrase": "evaluation_effort"}, {"score": 0.004714448392640623, "phrase": "xml_retrieval"}, {"score": 0.004519665344659923, "phrase": "trec-like_platform"}, {"score": 0.004472234553892357, "phrase": "content-oriented_xml_retrieval_systems"}, {"score": 0.004287415750101536, "phrase": "precision-recall_based_metrics"}, {"score": 0.0040957690050613185, "phrase": "focused_retrieval_measures"}, {"score": 0.004024351756560517, "phrase": "inex_pooling_method"}, {"score": 0.0037773674108196376, "phrase": "query_sets"}, {"score": 0.003459191419478298, "phrase": "\"new\"_systems"}, {"score": 0.0033749889869455407, "phrase": "pooling_process"}, {"score": 0.0030366573271799406, "phrase": "authors'_findings"}, {"score": 0.002994178996584381, "phrase": "precision-recall-based_metrics"}, {"score": 0.0029212625534156063, "phrase": "early_precision_measures"}, {"score": 0.0027905114793984084, "phrase": "incomplete_judgments"}, {"score": 0.002770920590799572, "phrase": "small_topic-set_sizes"}, {"score": 0.0027129670661026964, "phrase": "system_rankings"}, {"score": 0.0024929933447362554, "phrase": "nonparticipating_systems"}, {"score": 0.0023729878025466836, "phrase": "shallow_pools"}, {"score": 0.0023151639346874883, "phrase": "deep_pools"}, {"score": 0.002290814637620121, "phrase": "smaller_set"}], "paper_keywords": [""], "paper_abstract": "The Initiative for the Evaluation of XML retrieval (INEX) provides a TREC-like platform for evaluating content-oriented XML retrieval systems. Since 2007, INEX has been using a set of precision-recall based metrics for its ad hoc tasks. The authors investigate the reliability and robustness of these focused retrieval measures, and of the INEX pooling method. They explore four specific questions: How reliable are the metrics when assessments are incomplete, or when query sets are small? What is the minimum pool/query-set size that can be used to reliably evaluate systems? Can the INEX collections be used to fairly evaluate \"new\" systems that did not participate in the pooling process? And, for a fixed amount of assessment effort, would this effort be better spent in thoroughly judging a few queries, or in judging many queries relatively superficially? The authors' findings validate properties of precision-recall-based metrics observed in document retrieval settings. Early precision measures are found to be more error-prone and less stable under incomplete judgments and small topic-set sizes. They also find that system rankings remain largely unaffected even when assessment effort is substantially (but systematically) reduced, and confirm that the INEX collections remain usable when evaluating nonparticipating systems. Finally, they observe that for a fixed amount of effort, judging shallow pools for many queries is better than judging deep pools for a smaller set of queries. However, when judging only a random sample of a pool, it is better to completely judge fewer topics than to partially judge many topics. This result confirms the effectiveness of pooling methods.", "paper_title": "Evaluation Effort, Reliability and Reusability in XML Retrieval", "paper_id": "WOS:000286687300012"}