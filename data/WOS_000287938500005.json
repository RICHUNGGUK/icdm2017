{"auto_keywords": [{"score": 0.03976570402861194, "phrase": "regression_vector"}, {"score": 0.00481495049065317, "phrase": "bayesian_generalized_kernel_mixed_models"}, {"score": 0.004675930527604869, "phrase": "fully_bayesian_methodology"}, {"score": 0.004621447740470602, "phrase": "generalized_kernel_mixed_models"}, {"score": 0.00438399094249306, "phrase": "generalized_linear"}, {"score": 0.004358368398332723, "phrase": "mixed_models"}, {"score": 0.004282391936827106, "phrase": "feature_space"}, {"score": 0.004183137595671171, "phrase": "reproducing_kernel"}, {"score": 0.003991449490087454, "phrase": "point-mass_distribution"}, {"score": 0.003944945681746315, "phrase": "silverman"}, {"score": 0.0037201996003549246, "phrase": "generalized_kernel_model"}, {"score": 0.0031938648476635225, "phrase": "sparse_modeling"}, {"score": 0.003101503914610488, "phrase": "bayesian_computation"}, {"score": 0.0029766550209309127, "phrase": "data_augmentation_methodology"}, {"score": 0.0029075757820516634, "phrase": "markov_chain"}, {"score": 0.002890557197908352, "phrase": "monte_carlo"}, {"score": 0.0027579362569638945, "phrase": "reversible_jump_method"}, {"score": 0.002693919110230191, "phrase": "model_selection"}, {"score": 0.002646880721119838, "phrase": "bayesian_model_averaging_method"}, {"score": 0.0025854347094662247, "phrase": "posterior_prediction"}, {"score": 0.0025254115243261875, "phrase": "feature_basis_expansion"}, {"score": 0.0024813081174066653, "phrase": "reproducing_kernel_hilbert_space"}, {"score": 0.002409503258044183, "phrase": "stochastic_process"}, {"score": 0.0022854384756395116, "phrase": "karhunen-loeve_expansion"}, {"score": 0.0022455164874830777, "phrase": "gaussian_process"}, {"score": 0.002219406116302408, "phrase": "gp"}, {"score": 0.0021049977753042253, "phrase": "flexible_approximation_method"}], "paper_keywords": ["reproducing kernel Hilbert spaces", " generalized kernel models", " Silverman's g-prior", " Bayesian model averaging", " Gaussian processes"], "paper_abstract": "We propose a fully Bayesian methodology for generalized kernel mixed models (GKMMs), which are extensions of generalized linear mixed models in the feature space induced by a reproducing kernel. We place a mixture of a point-mass distribution and Silverman's g-prior on the regression vector of a generalized kernel model (GKM). This mixture prior allows a fraction of the components of the regression vector to be zero. Thus, it serves for sparse modeling and is useful for Bayesian computation. In particular, we exploit data augmentation methodology to develop a Markov chain Monte Carlo (MCMC) algorithm in which the reversible jump method is used for model selection and a Bayesian model averaging method is used for posterior prediction. When the feature basis expansion in the reproducing kernel Hilbert space is treated as a stochastic process, this approach can be related to the Karhunen-Loeve expansion of a Gaussian process (GP). Thus, our sparse modeling framework leads to a flexible approximation method for GPs.", "paper_title": "Bayesian Generalized Kernel Mixed Models", "paper_id": "WOS:000287938500005"}