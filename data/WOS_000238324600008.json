{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "low-rank_approximation"}, {"score": 0.03690585342209339, "phrase": "c_columns"}, {"score": 0.034035374432812834, "phrase": "matrix_a"}, {"score": 0.027897405505063148, "phrase": "additional_error"}, {"score": 0.00473516593179706, "phrase": "data_consist"}, {"score": 0.004445454438267843, "phrase": "specified_rank"}, {"score": 0.004355556168010664, "phrase": "n._methods"}, {"score": 0.0043233166079199034, "phrase": "singular_value_decomposition"}, {"score": 0.004173394025968985, "phrase": "well-defined_sense"}, {"score": 0.004013707008510311, "phrase": "data_sets"}, {"score": 0.003888905257972882, "phrase": "x_n_matrix_a"}, {"score": 0.0037609850995414364, "phrase": "svd."}, {"score": 0.0037331183605766936, "phrase": "provable_bounds"}, {"score": 0.0035175525976782075, "phrase": "first_algorithm"}, {"score": 0.0033891699702987186, "phrase": "appropriate_rescaling"}, {"score": 0.0033143930864301683, "phrase": "top_singular_values"}, {"score": 0.0033020911317806186, "phrase": "corresponding_singular_vectors"}, {"score": 0.0032593915869553714, "phrase": "computed_singular_vectors"}, {"score": 0.0032472931149420707, "phrase": "description_d"}, {"score": 0.0029755165646053903, "phrase": "high_probability"}, {"score": 0.002877545278459206, "phrase": "random_access_memory"}, {"score": 0.002798364302821266, "phrase": "external_memory"}, {"score": 0.002757034515426803, "phrase": "ram."}, {"score": 0.0027467878267542684, "phrase": "second_algorithm"}, {"score": 0.0027011800626129574, "phrase": "matrix_c"}, {"score": 0.0026811531172346676, "phrase": "r_rows"}, {"score": 0.002651390020663135, "phrase": "r_x_c_matrix_w."}, {"score": 0.0025261781704411686, "phrase": "best_rank"}, {"score": 0.0023580946625560172, "phrase": "failure_probability"}, {"score": 0.0023362533830006125, "phrase": "time_linear"}, {"score": 0.0022592826575460606, "phrase": "previously_published_results"}, {"score": 0.0022383545467736074, "phrase": "rank_parameter_k"}, {"score": 0.0022217514131966753, "phrase": "spectral_norms"}, {"score": 0.0021848422732893926, "phrase": "error_bounds"}, {"score": 0.0021726756927135118, "phrase": "novel_method"}, {"score": 0.002160576716634114, "phrase": "important_use"}, {"score": 0.0021525481058490285, "phrase": "matrix_perturbation_theory"}, {"score": 0.002140560973788298, "phrase": "probability_distribution"}, {"score": 0.0021049977753042253, "phrase": "crucial_features"}], "paper_keywords": ["randomized algorithms", " Monte Carlo methods", " massive data sets", " singular value decomposition"], "paper_abstract": "In many applications, the data consist of ( or may be naturally formulated as) an m x n matrix A. It is often of interest to find a low-rank approximation to A, i.e., an approximation D to the matrix A of rank not greater than a specified rank k, where k is much smaller than m and n. Methods such as the singular value decomposition (SVD) may be used to find an approximation to A which is the best in a well-defined sense. These methods require memory and time which are superlinear in m and n; for many applications in which the data sets are very large this is prohibitive. Two simple and intuitive algorithms are presented which, when given an m x n matrix A, compute a description of a low-rank approximation D* to A, and which are qualitatively faster than the SVD. Both algorithms have provable bounds for the error matrix A - D*. For any matrix X, let parallel to X parallel to(F) and parallel to X parallel to(2) denote its Frobenius norm and its spectral norm, respectively. In the first algorithm, c columns of A are randomly chosen. If the m x c matrix C consists of those c columns of A ( after appropriate rescaling), then it is shown that from (CC)-C-T approximations to the top singular values and corresponding singular vectors may be computed. From the computed singular vectors a description D* of the matrix A may be computed such that rank(D*) <= k and such that parallel to A - D*parallel to(2)(xi) <= min (D: rank(D) <= k) parallel to A - D parallel to(2)(xi) + poly(k, 1/ c)parallel to A parallel to(2)(F) holds with high probability for both xi = 2, F. This algorithm may be implemented without storing the matrix A in random access memory ( RAM), provided it can make two passes over the matrix stored in external memory and use O(cm + c(2)) additional RAM. The second algorithm is similar except that it further approximates the matrix C by randomly sampling r rows of C to form a r x c matrix W. Thus, it has additional error, but it can be implemented in three passes over the matrix using only constant additional RAM. To achieve an additional error ( beyond the best rank k approximation) that is at most epsilon parallel to A parallel to(2)(F), both algorithms take time which is polynomial in k, 1/epsilon, and log(1/delta), where delta > 0 is a failure probability; the first takes time linear in max(m, n) and the second takes time independent of m and n. Our bounds improve previously published results with respect to the rank parameter k for both the Frobenius and spectral norms. In addition, the proofs for the error bounds use a novel method that makes important use of matrix perturbation theory. The probability distribution over columns of A and the rescaling are crucial features of the algorithms which must be chosen judiciously.", "paper_title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix", "paper_id": "WOS:000238324600008"}