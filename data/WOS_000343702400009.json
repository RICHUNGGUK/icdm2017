{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high_dimensional_data"}, {"score": 0.004602085616330634, "phrase": "good_performance"}, {"score": 0.004273782634394231, "phrase": "nearest_neighbor_matches"}, {"score": 0.004238775572746022, "phrase": "high_dimensional_vectors"}, {"score": 0.0041696157492057, "phrase": "training_data"}, {"score": 0.004101579688745222, "phrase": "new_algorithms"}, {"score": 0.004067977399245212, "phrase": "approximate_nearest_neighbor_matching"}, {"score": 0.003952513974962462, "phrase": "previous_algorithms"}, {"score": 0.00388800655625083, "phrase": "high_dimensional_features"}, {"score": 0.003479180627368156, "phrase": "priority_search"}, {"score": 0.0033388871136347704, "phrase": "new_algorithm"}, {"score": 0.0032979090806670493, "phrase": "binary_features"}, {"score": 0.003257432325797907, "phrase": "multiple_hierarchical_clustering_trees"}, {"score": 0.0030497740719977835, "phrase": "optimal_nearest_neighbor_algorithm"}, {"score": 0.002890810475014827, "phrase": "automated_configuration_procedure"}, {"score": 0.002843581231499026, "phrase": "best_algorithm"}, {"score": 0.002797121441432477, "phrase": "particular_data"}, {"score": 0.0025865679621853667, "phrase": "single_machine"}, {"score": 0.0025338369486914364, "phrase": "distributed_nearest_neighbor_matching_framework"}, {"score": 0.002323826502178581, "phrase": "open_source_library"}, {"score": 0.002276439636865258, "phrase": "approximate_nearest_neighbors"}, {"score": 0.0021049977753042253, "phrase": "nearest_neighbor_matching"}], "paper_keywords": ["Nearest neighbor search", " big data", " approximate search", " algorithm configuration"], "paper_abstract": "For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.", "paper_title": "Scalable Nearest Neighbor Algorithms for High Dimensional Data", "paper_id": "WOS:000343702400009"}