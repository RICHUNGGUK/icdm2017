{"auto_keywords": [{"score": 0.04353040808935283, "phrase": "cdn"}, {"score": 0.00481495049065317, "phrase": "large-scale_latency_estimation_system"}, {"score": 0.004652016085325825, "phrase": "google_content_delivery_network"}, {"score": 0.004552981066668676, "phrase": "standard_features"}, {"score": 0.004513957235071457, "phrase": "contemporary_web_clients"}, {"score": 0.004342430078306461, "phrase": "latency_measurements"}, {"score": 0.004286708921931472, "phrase": "scalable_centralized_scheduler"}, {"score": 0.003949974260471287, "phrase": "large-scale_latency_estimation"}, {"score": 0.0038326698261576023, "phrase": "coordinate_stability"}, {"score": 0.003382547312072468, "phrase": "daily_re-computations"}, {"score": 0.0028715299874549245, "phrase": "measured_latency"}, {"score": 0.002448061472257196, "phrase": "huge_volume"}, {"score": 0.002427035025924384, "phrase": "latency_data"}, {"score": 0.0023958326775913165, "phrase": "clustering_techniques"}, {"score": 0.0023245746777044766, "phrase": "globally_distributed_internet_hosts"}, {"score": 0.0021696370171307615, "phrase": "google"}, {"score": 0.002132420504803564, "phrase": "public_interface"}, {"score": 0.0021049977753042253, "phrase": "latency_estimates"}], "paper_keywords": ["large-scale distributed systems", " network modeling", " latency estimation"], "paper_abstract": "We present the implementation of a large-scale latency estimation system based on GNP and incorporated into the Google content delivery network. Our implementation employs standard features of contemporary Web clients, and carefully controls the overhead incurred by latency measurements using a scalable centralized scheduler. It also requires only a small number of CDN modifications, which makes it attractive for any CDN interested in large-scale latency estimation. We investigate the issue of coordinate stability over time and show that coordinates drift away from their initial values with time, so that 25% of node coordinates become inaccurate by more than 33 ins after one week. However, daily re-computations make 75% of the coordinates stay within 6 ms of their initial values. Furthermore, we demonstrate that using coordinates to decide on client-to-replica re-direction leads to selecting replicas closest in term of measured latency in 86% of all cases. In another 10% of all cases, clients are re-directed to replicas offering latencies that are at most two times longer than optimal. Finally, collecting a huge volume of latency data and using clustering techniques enable us to estimate latencies between globally distributed Internet hosts that have not participated in our measurements at all. The results are sufficiently promising that Google may offer a public interface to the latency estimates in the future. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Practical large-scale latency estimation", "paper_id": "WOS:000255706200001"}