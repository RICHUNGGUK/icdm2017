{"auto_keywords": [{"score": 0.037771328677087865, "phrase": "structure_outputs"}, {"score": 0.019298255614865235, "phrase": "active_shape_parameters"}, {"score": 0.01727409404465675, "phrase": "human_parsing"}, {"score": 0.01049831138592189, "phrase": "active_template_regression"}, {"score": 0.00481495049065317, "phrase": "deep_human"}, {"score": 0.004643594260262632, "phrase": "human_parsing_task"}, {"score": 0.00456020621016909, "phrase": "human_image"}, {"score": 0.004397880205653141, "phrase": "atr"}, {"score": 0.004303252595782034, "phrase": "normalized_mask"}, {"score": 0.004180234323211199, "phrase": "linear_combination"}, {"score": 0.004135010810625326, "phrase": "learned_mask_templates"}, {"score": 0.0038457240885563146, "phrase": "semantic_region"}, {"score": 0.003804105622385469, "phrase": "mask_template"}, {"score": 0.003681923235979914, "phrase": "human_parsing_results"}, {"score": 0.003525074461418428, "phrase": "deep_convolutional_neural_network"}, {"score": 0.003499649026388717, "phrase": "cnn"}, {"score": 0.0034118234596993836, "phrase": "end-to-end_relation"}, {"score": 0.0031386091093713706, "phrase": "first_cnn_network"}, {"score": 0.0029830992938695007, "phrase": "label_mask"}, {"score": 0.0029400959763737364, "phrase": "second_cnn_network"}, {"score": 0.0028249975318149468, "phrase": "mask_position"}, {"score": 0.0027341763737751467, "phrase": "new_image"}, {"score": 0.0025334249463305875, "phrase": "super-pixel_smoothing"}, {"score": 0.002469830206270585, "phrase": "human_parsing_result"}, {"score": 0.0024519545383399773, "phrase": "comprehensive_evaluations"}, {"score": 0.0024253827365474734, "phrase": "large_dataset"}, {"score": 0.0023903999901926224, "phrase": "significant_superiority"}, {"score": 0.0023644936731468252, "phrase": "atr_framework"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_algorithm"}], "paper_keywords": ["Active template regression", " CNN", " human parsing", " active template network", " active shape network"], "paper_abstract": "In this work, the human parsing task, namely decomposing a human image into semantic fashion/body regions, is formulated as an active template regression (ATR) problem, where the normalized mask of each fashion/body item is expressed as the linear combination of the learned mask templates, and then morphed to a more precise mask with the active shape parameters, including position, scale and visibility of each semantic region. The mask template coefficients and the active shape parameters together can generate the human parsing results, and are thus called the structure outputs for human parsing. The deep Convolutional Neural Network (CNN) is utilized to build the end-to-end relation between the input human image and the structure outputs for human parsing. More specifically, the structure outputs are predicted by two separate networks. The first CNN network is with max-pooling, and designed to predict the template coefficients for each label mask, while the second CNN network is without max-pooling to preserve sensitivity to label mask position and accurately predict the active shape parameters. For a new image, the structure outputs of the two networks are fused to generate the probability of each label for each pixel, and super-pixel smoothing is finally used to refine the human parsing result. Comprehensive evaluations on a large dataset well demonstrate the significant superiority of the ATR framework over other state-of-the-arts for human parsing. In particular, the F1-score reaches 64: 38 percent by our ATR framework, significantly higher than 44: 76 percent based on the state-of-the-art algorithm [28].", "paper_title": "Deep Human Parsing with Active Template Regression", "paper_id": "WOS:000364831700004"}