{"auto_keywords": [{"score": 0.048951593908953754, "phrase": "virtual_agent"}, {"score": 0.04784874064207587, "phrase": "gaze-based_interface"}, {"score": 0.00481495049065317, "phrase": "shared_attention"}, {"score": 0.004173244935967375, "phrase": "simple_shared_attention_behaviours"}, {"score": 0.004074901471704325, "phrase": "interaction_scenario"}, {"score": 0.003559675269411366, "phrase": "standard_web-camera"}, {"score": 0.0034208822062692127, "phrase": "users'_head_directions"}, {"score": 0.002964415652865295, "phrase": "user_perception"}, {"score": 0.0028944757265206332, "phrase": "agent's_behaviour"}, {"score": 0.002826181226538412, "phrase": "shared_attention_scenario"}, {"score": 0.002694375425481759, "phrase": "important_factors"}, {"score": 0.0025687008721685454, "phrase": "engagement_models"}, {"score": 0.0021049977753042253, "phrase": "shared_attention_situations"}], "paper_keywords": ["Shared attention", " Gaze detection", " Embodied agents", " Social behaviour"], "paper_abstract": "This paper investigates the use of a gaze-based interface for testing simple shared attention behaviours during an interaction scenario with a virtual agent. The interface is non-intrusive, operating in real-time using a standard web-camera for input, monitoring users' head directions and processing them in real-time for resolution to screen coordinates. We use the interface to investigate user perception of the agent's behaviour during a shared attention scenario. Our aim is to elaborate important factors to be considered when constructing engagement models that must account not only for behaviour in isolation, but also for the context of the interaction, as is the case during shared attention situations.", "paper_title": "Investigating shared attention with a virtual agent using a gaze-based interface", "paper_id": "WOS:000208480100011"}