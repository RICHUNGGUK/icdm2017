{"auto_keywords": [{"score": 0.028040201871001355, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "diverse_artificial_neural_networks"}, {"score": 0.004619027561771169, "phrase": "neural_network_architectures"}, {"score": 0.0045765783794477505, "phrase": "evolutionary_algorithms"}, {"score": 0.004370087541308677, "phrase": "last_generation"}, {"score": 0.0038577428353448596, "phrase": "higher_accuracy"}, {"score": 0.0037177622939493084, "phrase": "best_classifier"}, {"score": 0.0035994372026264478, "phrase": "major_factors"}, {"score": 0.0035663245542174224, "phrase": "optimum_accuracy"}, {"score": 0.0032665093519968083, "phrase": "diverse_evolutionary_neural_networks"}, {"score": 0.0031045562469735307, "phrase": "behavior_knowledge_space_method"}, {"score": 0.0028698706191651155, "phrase": "sharing_radius"}, {"score": 0.0028303313661029597, "phrase": "representative_speciation_method"}, {"score": 0.0027784560880013886, "phrase": "diverse_results"}, {"score": 0.0027528751037412128, "phrase": "standard_evolutionary_algorithms"}, {"score": 0.0025447065763362984, "phrase": "average_output"}, {"score": 0.0025212722607043546, "phrase": "pearson_correlation"}, {"score": 0.002486545267297852, "phrase": "kullback-leibler"}, {"score": 0.0024409345168575833, "phrase": "fitness_sharing_performance"}, {"score": 0.0023851187337905412, "phrase": "australian_credit_card_assessment"}, {"score": 0.0022878396720586044, "phrase": "uci_database"}, {"score": 0.002164264600379492, "phrase": "previously_published_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["evolutionary neural networks ensemble", " fitness sharing", " Kullback-Leibler entropy", " majority voting", " UCI benchmark dataset"], "paper_abstract": "Recently, many researchers have designed neural network architectures with evolutionary algorithms but most of them have used only the fittest solution of the last generation. To better exploit information, an ensemble of individuals is a more promising choice because information that is derived from combining a set of classifiers might produce higher accuracy than merely using the information from the best classifier among them. One of the major factors for optimum accuracy is the diversity of the classifier set. In this paper, we present a method of generating diverse evolutionary neural networks through fitness sharing and then combining these networks by the behavior knowledge space method. Fitness sharing that shares resources if the distance between the individuals is smaller than the sharing radius is a representative speciation method, which produces diverse results than standard evolutionary algorithms that converge to only one solution. Especially, the proposed method calculates the distance between the individuals using average output, Pearson correlation and modified Kullback-Leibler entropy to enhance fitness sharing performance. In experiments with Australian credit card assessment, breast cancer, and diabetes in the UCI database, the proposed method performed better than not only the non-speciation method but also better than previously published methods. (C) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Evolutionary ensemble of diverse artificial neural networks using speciation", "paper_id": "WOS:000255239200043"}