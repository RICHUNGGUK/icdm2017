{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "approximated_gradient_method"}, {"score": 0.04047293105117697, "phrase": "error_function"}, {"score": 0.03149278777849825, "phrase": "weights_sequence"}, {"score": 0.004697655699119423, "phrase": "elman_network"}, {"score": 0.004362564831572539, "phrase": "elman_networks"}, {"score": 0.00405127920820053, "phrase": "finite_sample"}, {"score": 0.0034935290662899488, "phrase": "training_process"}, {"score": 0.0033251716926666437, "phrase": "approximated_gradient"}, {"score": 0.0026622351303038885, "phrase": "moderate_condition"}, {"score": 0.0022670785913397637, "phrase": "numerical_example"}, {"score": 0.0021049977753042253, "phrase": "theoretical_findings"}], "paper_keywords": ["Elman network", " approximated gradient method", " convergence"], "paper_abstract": "An approximated gradient method for training Elman networks is considered. For the finite sample set, the error function is proved to be monotone in the training process, and the approximated gradient of the error function tends to zero if the weights sequence is bounded. Furthermore, after adding a moderate condition, the weights sequence itself is also proved to be convergent. A numerical example is given to support the theoretical findings.", "paper_title": "Convergence of approximated gradient method for Elman network", "paper_id": "WOS:000259042700001"}