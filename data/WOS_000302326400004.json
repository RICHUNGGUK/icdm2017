{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "computer_experiments"}, {"score": 0.004291280781713637, "phrase": "gaussian"}, {"score": 0.00417692318756945, "phrase": "gp"}, {"score": 0.003114449258490065, "phrase": "measurement_error"}, {"score": 0.0027989800906333784, "phrase": "computer_experiment"}, {"score": 0.0027252240573281163, "phrase": "statistically_inefficient_way"}, {"score": 0.0023424328921363585, "phrase": "better_statistical_properties"}, {"score": 0.0022604585778195152, "phrase": "predictive_accuracy"}, {"score": 0.0021049977753042253, "phrase": "common_situations"}], "paper_keywords": ["Computer simulator", " Surrogate model", " Gaussian process", " Interpolation", " Smoothing"], "paper_abstract": "Most surrogate models for computer experiments are interpolators, and the most common interpolator is a Gaussian process (GP) that deliberately omits a small-scale (measurement) error term called the nugget. The explanation is that computer experiments are, by definition, \"deterministic\", and so there is no measurement error. We think this is too narrow a focus for a computer experiment and a statistically inefficient way to model them. We show that estimating a (non-zero) nugget can lead to surrogate models with better statistical properties, such as predictive accuracy and coverage, in a variety of common situations.", "paper_title": "Cases for the nugget in modeling computer experiments", "paper_id": "WOS:000302326400004"}