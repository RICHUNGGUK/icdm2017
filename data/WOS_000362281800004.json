{"auto_keywords": [{"score": 0.0327937590180093, "phrase": "datastore_server's_local_cache"}, {"score": 0.00481495049065317, "phrase": "distributed_key-value_datastores"}, {"score": 0.004718897469501381, "phrase": "vital_maintenance_mechanism"}, {"score": 0.0045876189888390895, "phrase": "log-structured_merge-tree"}, {"score": 0.00445997627516653, "phrase": "data_files"}, {"score": 0.004424160689087029, "phrase": "update-intensive_workloads"}, {"score": 0.004318422955036175, "phrase": "read_latencies"}, {"score": 0.004232232993872917, "phrase": "long_run"}, {"score": 0.0040978781692373005, "phrase": "significantly_degraded_read_performance"}, {"score": 0.0038417798012011155, "phrase": "in-depth_analysis"}, {"score": 0.003810909603178837, "phrase": "compaction-related_performance_overheads"}, {"score": 0.003587125711973247, "phrase": "dedicated_compaction_server"}, {"score": 0.0035296940954990964, "phrase": "datastore_server"}, {"score": 0.003417565253539315, "phrase": "actual_workload"}, {"score": 0.003335804608791029, "phrase": "newly_compacted_data"}, {"score": 0.0032559935867182035, "phrase": "compaction_server's_main_memory"}, {"score": 0.003040050100579337, "phrase": "performance_penalty"}, {"score": 0.0028498718992429825, "phrase": "compacted_data"}, {"score": 0.0028155572612699976, "phrase": "remote_cache"}, {"score": 0.0026932247685847246, "phrase": "local_cache"}, {"score": 0.002597088221737922, "phrase": "smarter_warmup_algorithm"}, {"score": 0.002545165985686468, "phrase": "incoming_read_requests"}, {"score": 0.002366676414323734, "phrase": "hbase"}, {"score": 0.002319346861878611, "phrase": "ycsb"}, {"score": 0.002300682437457228, "phrase": "tpc-c"}, {"score": 0.002218526133566788, "phrase": "compaction-related_performance_problems"}, {"score": 0.0021049977753042253, "phrase": "multiple_compaction_servers"}], "paper_keywords": [""], "paper_abstract": "Compactions are a vital maintenance mechanism used by datastores based on the log-structured merge-tree to counter the continuous buildup of data files under update-intensive workloads. While compactions help keep read latencies in check over the long run, this comes at the cost of significantly degraded read performance over the course of the compaction itself. In this paper, we offer an in-depth analysis of compaction-related performance overheads and propose techniques for their mitigation. We offload large, expensive compactions to a dedicated compaction server to allow the datastore server to better utilize its resources towards serving the actual workload. Moreover, since the newly compacted data is already cached in the compaction server's main memory, we fetch this data over the network directly into the datastore server's local cache, thereby avoiding the performance penalty of reading it back from the filesystem. In fact, pre-fetching the compacted data from the remote cache prior to switching the workload over to it can eliminate local cache misses altogether. Therefore, we implement a smarter warmup algorithm that ensures that all incoming read requests are served from the datastore server's local cache even as it is warming up. We have integrated our solution into HBase, and using the YCSB and TPC-C benchmarks, we show that our approach significantly mitigates compaction-related performance problems. We also demonstrate the scalability of our solution by distributing compactions across multiple compaction servers.", "paper_title": "Compaction management in distributed key-value datastores", "paper_id": "WOS:000362281800004"}