{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "local_dependencies"}, {"score": 0.004620093797504386, "phrase": "large_margin_classifiers"}, {"score": 0.004115267130977749, "phrase": "unknown_data_generating_distribution"}, {"score": 0.0032376465741070274, "phrase": "internal_correlations"}, {"score": 0.0030808211830129304, "phrase": "different_batches"}, {"score": 0.0025257081555100556, "phrase": "probabilistic_formulation"}, {"score": 0.00240328454980006, "phrase": "mathematical_programming_analysis"}, {"score": 0.0021049977753042253, "phrase": "naive_support_vector_machine"}], "paper_keywords": ["batch-wise classification", " support vector machine", " linear programming", " machine learning", " statistical methods", " unconstrained optimization"], "paper_abstract": "Most classification methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this assumption, we consider the case where batches or groups of samples may have internal correlations, whereas the samples from different batches may be considered to be uncorrelated. This paper introduces three algorithms for classifying all the samples in a batch jointly: one based on a probabilistic formulation, and two based on mathematical programming analysis. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are significantly more accurate than a naive support vector machine which ignores the correlations among the samples.", "paper_title": "Using Local Dependencies within Batches to Improve Large Margin Classifiers", "paper_id": "WOS:000270824200004"}