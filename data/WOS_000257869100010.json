{"auto_keywords": [{"score": 0.038899833657713954, "phrase": "grad"}, {"score": 0.02163336802137927, "phrase": "srad"}, {"score": 0.0071912329467288165, "phrase": "os_allocator"}, {"score": 0.005933154011878313, "phrase": "greedy_thread_scheduler"}, {"score": 0.005760069689884212, "phrase": "work-stealing_thread_scheduler"}, {"score": 0.00481495049065317, "phrase": "multiprocessor_scheduling"}, {"score": 0.004761732436126133, "phrase": "shared_multiprogramming_environment"}, {"score": 0.004605565335682916, "phrase": "kernel-level_os_allocator"}, {"score": 0.004372712391262538, "phrase": "ready_threads"}, {"score": 0.004276541976450121, "phrase": "allotted_processors"}, {"score": 0.004167001961564886, "phrase": "thread_scheduler"}, {"score": 0.004060256280654948, "phrase": "upcoming_quantum"}, {"score": 0.0037981086150309885, "phrase": "next_quantum"}, {"score": 0.0035135084960705816, "phrase": "processor_allotments"}, {"score": 0.003461758868675955, "phrase": "fair_allocation"}, {"score": 0.0030970990565391183, "phrase": "centralized_scheduling"}, {"score": 0.002995361424576051, "phrase": "distributed_settings"}, {"score": 0.0028331500847209984, "phrase": "advance_knowledge"}, {"score": 0.0028017745468000587, "phrase": "job's_parallelism"}, {"score": 0.0027097117582440687, "phrase": "effective_control"}, {"score": 0.0026401983633087267, "phrase": "efficient_utilization"}, {"score": 0.0024786956187352327, "phrase": "optimal_clairvoyant_scheduler"}, {"score": 0.0023184226291364097, "phrase": "arbitrary_release_times"}, {"score": 0.002267329954548889, "phrase": "mean_response_time"}, {"score": 0.0022009497295481678, "phrase": "offline_optimal_scheduler"}, {"score": 0.0021846599115561832, "phrase": "arbitrary_batched_jobs"}, {"score": 0.002128587147609537, "phrase": "first_nonclairvoyant_scheduling_algorithms"}, {"score": 0.0021049977753042253, "phrase": "provable_efficiency"}], "paper_keywords": ["online nonclairvoyant scheduling", " competitive analysis", " makespan", " mean response time", " parallel processors", " two-level scheduling", " request allotment protocol"], "paper_abstract": "Multiprocessor scheduling in a shared multiprogramming environment can be structured in two levels, where a kernel-level OS allocator allots processors to jobs and a user-level thread scheduler maps the ready threads of a job onto the allotted processors. Between scheduling quanta, each thread scheduler computes its desire for processors in the upcoming quantum and feeds back to the OS allocator. The OS allocator then adjusts the allotment of processors for the next quantum. We present two provably efficient two-level scheduling schemes, called GRAD and SRAD, respectively. Both schemes use the same OS allocator RAD for the processor allotments, which ensures fair allocation under all levels of workload. In GRAD, RAD is combined with a greedy thread scheduler; in SRAD, RAD is combined with a work-stealing thread scheduler. The greedy thread scheduler is suitable for centralized scheduling, whereas the work-stealing thread scheduler is more suitable for distributed settings. Both GRAD and SRAD are nonclairvoyant, i.e., they do not require advance knowledge about the job's parallelism and arrival time. Moreover, they provide effective control over the scheduling overhead and ensure efficient utilization of processors. We analyze the competitiveness of both GRAD and SRAD with respect to an optimal clairvoyant scheduler. In terms of makespan, both schemes can achieve O(-)-competitiveness for any set of jobs with arbitrary release times. In terms of the mean response time, both schemes are O(1)-competitive against the offline optimal scheduler for arbitrary batched jobs. GRAD and SRAD are the first nonclairvoyant scheduling algorithms that guarantee provable efficiency, fairness, and minimal overhead simultaneously.", "paper_title": "Provably efficient online nonclairvoyant adaptive scheduling", "paper_id": "WOS:000257869100010"}