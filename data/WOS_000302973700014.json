{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "fisher's_linear_discriminant"}, {"score": 0.004759635663291431, "phrase": "randomly_projected_data_spaces"}, {"score": 0.004518442937844318, "phrase": "non-adaptive_dimensionality_reduction"}, {"score": 0.004240116703465252, "phrase": "classification_error"}, {"score": 0.004191377834552202, "phrase": "fisher's_linear_discriminant_classifier"}, {"score": 0.004001938457469328, "phrase": "randomly_projected_versions"}, {"score": 0.003755307753574745, "phrase": "random_projection"}, {"score": 0.0034233569308012982, "phrase": "simple_class_structure"}, {"score": 0.0032309093154028663, "phrase": "non-trivial_performance"}, {"score": 0.0030847461428168614, "phrase": "low-dimensional_structure_restrictions"}, {"score": 0.0027634902834910184, "phrase": "potential_issue"}, {"score": 0.0026846436189970446, "phrase": "finite_sample"}, {"score": 0.002548331088496964, "phrase": "existing_bounds"}, {"score": 0.0024899850926034567, "phrase": "randomly_projected_data"}, {"score": 0.002363532907211947, "phrase": "training_data_increases"}, {"score": 0.0023228223366301226, "phrase": "preliminary_version"}, {"score": 0.0022434880661725493, "phrase": "ibm_best_student_paper_award"}, {"score": 0.0022048406330100697, "phrase": "international_conference"}, {"score": 0.0021794454466797382, "phrase": "pattern_recognition"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Fisher discriminant analysis", " Random projection", " Dimensionality reduction", " Curse of dimensionality", " Performance bounds"], "paper_abstract": "We consider the problem of classification in non-adaptive dimensionality reduction. Specifically, we give an average-case bound on the classification error of Fisher's linear discriminant classifier when the classifier only has access to randomly projected versions of a given training set. By considering the system of random projection and classifier together as a whole, we are able to take advantage of the simple class structure inherent in the problem, and so derive a non-trivial performance bound without imposing any sparsity or underlying low-dimensional structure restrictions on the data. Our analysis also reveals and quantifies the effect of class 'flipping' - a potential issue when randomly projecting a finite sample. Our bound is reasonably tight, and unlike existing bounds on learning from randomly projected data, it becomes tighter as the quantity of training data increases. A preliminary version of this work received an IBM Best Student Paper Award at the 20th International Conference on Pattern Recognition. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A tight bound on the performance of Fisher's linear discriminant in randomly projected data spaces", "paper_id": "WOS:000302973700014"}