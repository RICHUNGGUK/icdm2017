{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "attentive_objects"}, {"score": 0.023022725783098625, "phrase": "concept_association_network"}, {"score": 0.005425292093287223, "phrase": "linear_system"}, {"score": 0.004506968388996092, "phrase": "efficient_and_effective_image_retrieval"}, {"score": 0.004453116532937849, "phrase": "text-based_approach"}, {"score": 0.004413148455148519, "phrase": "automatic_image_annotation"}, {"score": 0.004373537527118041, "phrase": "critical_task"}, {"score": 0.004321273059270048, "phrase": "important_issue"}, {"score": 0.003984391456005223, "phrase": "habitual_way"}, {"score": 0.003854762905177511, "phrase": "segmentation_algorithm"}, {"score": 0.003785836059995376, "phrase": "segmentation_process"}, {"score": 0.003469613024006653, "phrase": "attention-driven_image_interpretation_method"}, {"score": 0.003397313647699294, "phrase": "over-segmented_image"}, {"score": 0.003267003547703286, "phrase": "basic_unit"}, {"score": 0.0031606404966938568, "phrase": "visual_classifiers"}, {"score": 0.0030485414351981766, "phrase": "object_recognition"}, {"score": 0.0029670762930358394, "phrase": "concept_nodes"}, {"score": 0.0029052181875265995, "phrase": "trained_neural_network"}, {"score": 0.002836096284133166, "phrase": "single_object"}, {"score": 0.0024992301032831983, "phrase": "loopy_propagations"}, {"score": 0.002311013816721592, "phrase": "annotation_problem"}, {"score": 0.002262802370005939, "phrase": "node_combination"}, {"score": 0.0022424487483398876, "phrase": "maximum_response"}, {"score": 0.0022289812518790824, "phrase": "annotation_experiments"}, {"score": 0.0022089311733832985, "phrase": "better_accuracy"}, {"score": 0.0021433882705962034, "phrase": "annotation_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Image annotation", " Concept association network (CAN)", " Attentive objects", " Visual classifier", " Neural network"], "paper_abstract": "With the advancement of imaging techniques and IT technologies, image retrieval has become a bottle neck. The key for efficient and effective image retrieval is by a text-based approach in which automatic image annotation is a critical task. As an important issue, the metadata of the annotation, i.e., the basic unit of an image to be labeled, has not been fully studied. A habitual way is to label the segments which are produced by a segmentation algorithm. However, after a segmentation process an object has often been broken into pieces, which not only produces noise for annotation but also increases the complexity of the model. We adopt an attention-driven image interpretation method to extract attentive objects from an over-segmented image and use the attentive objects for annotation. By such doing, the basic unit of annotation has been upgraded from segments to attentive objects. Visual classifiers are trained and a concept association network (CAN) is constructed for object recognition. A CAN consists of a number of concept nodes in which each node is a trained neural network (visual classifier) to recognize a single object. The nodes are connected through their correlation links forming a network. Given that an image contains several unknown attentive objects, all the nodes in CAN generate their own responses which propagate to other nodes through the network simultaneously. For a combination of nodes under investigation, these loopy propagations can be characterized by a linear system. The response of a combination of nodes can be obtained by solving the linear system. Therefore, the annotation problem is converted into finding out the node combination with the maximum response. Annotation experiments show a better accuracy of attentive objects over segments and that the concept association network improves annotation performance. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Recognition of attentive objects with a concept association network for image annotation", "paper_id": "WOS:000280006700030"}