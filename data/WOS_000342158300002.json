{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "minimal_norm-like_solutions"}, {"score": 0.0043875257506180865, "phrase": "general_class"}, {"score": 0.004297795905522757, "phrase": "convex_optimization_problems"}, {"score": 0.003956784357357341, "phrase": "strongly_convex_function"}, {"score": 0.003835972725191194, "phrase": "closed_and_convex_set"}, {"score": 0.0036052633308076933, "phrase": "optimal_set"}, {"score": 0.0034951469511975346, "phrase": "convex_problem"}, {"score": 0.0033190180867511605, "phrase": "gradient-based_method"}, {"score": 0.0022399174232611853, "phrase": "function_values"}], "paper_keywords": ["Bilevel optimization", " Complexity analysis", " Convex minimization", " Minimal norm solution"], "paper_abstract": "We consider a general class of convex optimization problems in which one seeks to minimize a strongly convex function over a closed and convex set which is by itself an optimal set of another convex problem. We introduce a gradient-based method, called the minimal norm gradient method, for solving this class of problems, and establish the convergence of the sequence generated by the algorithm as well as a rate of convergence of the sequence of function values. The paper ends with several illustrating numerical examples.", "paper_title": "A first order method for finding minimal norm-like solutions of convex optimization problems", "paper_id": "WOS:000342158300002"}