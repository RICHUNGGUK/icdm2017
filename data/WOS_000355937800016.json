{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "max-margin"}, {"score": 0.0047057084749407485, "phrase": "data_augmentation"}, {"score": 0.0044688500725247726, "phrase": "new_max-margin_discriminant_projection_method"}, {"score": 0.0042928648367867835, "phrase": "latent_variable_representation"}, {"score": 0.004243862930012258, "phrase": "support_vector_machine"}, {"score": 0.0041001740903338834, "phrase": "classification_criterion"}, {"score": 0.0039841418890102925, "phrase": "proposed_model"}, {"score": 0.0038936755823756226, "phrase": "discriminative_subspace"}, {"score": 0.0037834649508375544, "phrase": "bayesian_framework"}, {"score": 0.003697538418750941, "phrase": "augmented_variables"}, {"score": 0.003592859432554987, "phrase": "extended_nonlinear_model"}, {"score": 0.003471135453226184, "phrase": "kernel_trick"}, {"score": 0.0033922781038328844, "phrase": "similar_model"}, {"score": 0.003094306770612843, "phrase": "kernel_expansion"}, {"score": 0.0028715299874549245, "phrase": "basis_vectors"}, {"score": 0.0027741761880779535, "phrase": "corresponding_candidates"}, {"score": 0.002726740692380339, "phrase": "existing_methods"}, {"score": 0.0026041635822373265, "phrase": "original_feature_space"}, {"score": 0.0024586427394152196, "phrase": "final_classification_task"}, {"score": 0.0023889533502525527, "phrase": "conditionally_conjugate_property"}, {"score": 0.0022424956516376073, "phrase": "simple_and_efficient_gibbs_sampler"}, {"score": 0.0021416404045850224, "phrase": "synthesized_and_real-world_data"}, {"score": 0.0021049977753042253, "phrase": "large-scale_data_sets"}], "paper_keywords": ["Max-margin", " Gibbs sampling", " feature extraction", " kernel methods", " radar automatic target recognition (RATR)"], "paper_abstract": "In this paper, we introduce a new max-margin discriminant projection method, which takes advantage of the latent variable representation for support vector machine (SVM) as the classification criterion. Specifically, the proposed model jointly learns the discriminative subspace and classifier in a Bayesian framework by conditioning on augmented variables. Moreover, an extended nonlinear model is developed based on the kernel trick, where the similar model can be used in this setting with few modifications. To explore the sparsity in the kernel expansion, we use the spike-and-slab prior to seek basis vectors (BVs) from the corresponding candidates. Unlike existing methods, which employ BVs to approximate the original feature space, in our method BVs are sought to associate the final classification task. Thanks to the conditionally conjugate property, the parameters in our models can be inferred via the simple and efficient Gibbs sampler. Finally, we test our methods on synthesized and real-world data, including large-scale data sets to demonstrate their efficiency and effectiveness.", "paper_title": "Max-Margin Discriminant Projection via Data Augmentation", "paper_id": "WOS:000355937800016"}