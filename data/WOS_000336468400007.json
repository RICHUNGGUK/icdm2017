{"auto_keywords": [{"score": 0.04977793821547785, "phrase": "robot_grasp_synthesis"}, {"score": 0.00481495049065317, "phrase": "human_grasp_demonstrations"}, {"score": 0.004680220363692397, "phrase": "everyday_manipulation_tasks"}, {"score": 0.004592491583367684, "phrase": "new_skills"}, {"score": 0.004506399797123487, "phrase": "different_complex_environments"}, {"score": 0.0044219147356791225, "phrase": "lifelong_learning"}, {"score": 0.004284596323560538, "phrase": "similar_dexterity"}, {"score": 0.0042576464476131205, "phrase": "robotic_hands"}, {"score": 0.0042308653646339405, "phrase": "cognitive_capacity"}, {"score": 0.004099455381916337, "phrase": "relevant_multi-sensor_information"}, {"score": 0.003972110690317813, "phrase": "previous_grasping_tasks"}, {"score": 0.0038730779753083787, "phrase": "different_contexts"}, {"score": 0.003693973656103879, "phrase": "human_experiences"}, {"score": 0.003579179502431823, "phrase": "unknown_objects"}, {"score": 0.0034899090274046014, "phrase": "artificial_system"}, {"score": 0.0034244153725855, "phrase": "previous_human_object_grasping_demonstrations"}, {"score": 0.0033284645391157266, "phrase": "probabilistic_distributions"}, {"score": 0.003235193484261443, "phrase": "preliminary_knowledge"}, {"score": 0.0031544754107802413, "phrase": "point_cloud"}, {"score": 0.0031247264362609614, "phrase": "unknown_object"}, {"score": 0.002999012736727594, "phrase": "twofold_process"}, {"score": 0.002980124909321296, "phrase": "object_decomposition"}, {"score": 0.0029613556844064713, "phrase": "grasp_synthesis"}, {"score": 0.0028065024340850615, "phrase": "new_unknown_objects"}, {"score": 0.0027106334164712057, "phrase": "defined_object_primitives"}, {"score": 0.002676587673012276, "phrase": "feasible_object_regions"}, {"score": 0.0025047222344966678, "phrase": "selected_grasp"}, {"score": 0.0024421847638964947, "phrase": "real_robotic_platform"}, {"score": 0.002256619934086924, "phrase": "previous_learning"}, {"score": 0.0022353193959268767, "phrase": "proposed_approach"}, {"score": 0.0022212306049833397, "phrase": "suitable_grasps"}, {"score": 0.0021726139494880653, "phrase": "analytical_and_geometrical_approaches"}, {"score": 0.0021453112313233554, "phrase": "autonomous_grasping"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Robot grasp synthesis", " Human grasp demonstrations", " Object shape representation", " Probabilistic inference"], "paper_abstract": "Humans excel when dealing with everyday manipulation tasks, being able to learn new skills, and to adapt to different complex environments. This results from a lifelong learning, and also observation of other skilled humans. To obtain similar dexterity with robotic hands, cognitive capacity is needed to deal with uncertainty. By extracting relevant multi-sensor information from the environment (objects), knowledge from previous grasping tasks can be generalized to be applied within different contexts. Based on this strategy, we show in this paper that learning from human experiences is a way to accomplish our goal of robot grasp synthesis for unknown objects. In this article we address an artificial system that relies on knowledge from previous human object grasping demonstrations. A learning process is adopted to quantify probabilistic distributions and uncertainty. These distributions are combined with preliminary knowledge towards inference of proper grasps given a point cloud of an unknown object. In this article, we designed a method that comprises a twofold process: object decomposition and grasp synthesis. The decomposition of objects into primitives is used, across which similarities between past observations and new unknown objects can be made. The grasps are associated with the defined object primitives, so that feasible object regions for grasping can be determined. The hand pose relative to the object is computed for the pre-grasp and the selected grasp. We have validated our approach on a real robotic platform a dexterous robotic hand. Results show that the segmentation of the object into primitives allows to identify the most suitable regions for grasping based on previous learning. The proposed approach provides suitable grasps, better than more time consuming analytical and geometrical approaches, contributing for autonomous grasping. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Knowledge-based reasoning from human grasp demonstrations for robot grasp synthesis", "paper_id": "WOS:000336468400007"}