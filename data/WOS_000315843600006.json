{"auto_keywords": [{"score": 0.05007570437152931, "phrase": "facial_expressions"}, {"score": 0.04954756810706401, "phrase": "eeg"}, {"score": 0.00470862827326028, "phrase": "implicit_affective_tagging"}, {"score": 0.004428139418750855, "phrase": "strong_need"}, {"score": 0.004378958522416482, "phrase": "efficient_search"}, {"score": 0.004187614134377688, "phrase": "predominant_method"}, {"score": 0.004141093507753515, "phrase": "content-based_tagging"}, {"score": 0.003829548069657859, "phrase": "intensive_research"}, {"score": 0.0034824716857663114, "phrase": "foreseeable_future"}, {"score": 0.003256526411907174, "phrase": "implicit_tagging"}, {"score": 0.003202361402737197, "phrase": "users'_responses"}, {"score": 0.0030967107893195246, "phrase": "multimedia_content"}, {"score": 0.002977835845437357, "phrase": "descriptive_tags"}, {"score": 0.0028635111041882956, "phrase": "multi-modal_approach"}, {"score": 0.0026330549554033876, "phrase": "affective_tags"}, {"score": 0.0025037596405075866, "phrase": "valence-arousal_space"}, {"score": 0.0023941575109692336, "phrase": "decision-level_fusion"}, {"score": 0.002176887956041267, "phrase": "complementary_information"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Emotion classification", " EEG", " Facial expressions", " Signal processing", " Pattern classification", " Affective computing"], "paper_abstract": "The explosion of user-generated, untagged multimedia data in recent years, generates a strong need for efficient search and retrieval of this data. The predominant method for content-based tagging is through slow, labor-intensive manual annotation. Consequently, automatic tagging is currently a subject of intensive research. However, it is clear that the process will not be fully automated in the foreseeable future. We propose to involve the user and investigate methods for implicit tagging, wherein users' responses to the interaction with the multimedia content are analyzed in order to generate descriptive tags. Here, we present a multi-modal approach that analyses both facial expressions and electroencephalography (EEG) signals for the generation of affective tags. We perform classification and regression in the valence-arousal space and present results for both feature-level and decision-level fusion. We demonstrate improvement in the results when using both modalities, suggesting the modalities contain complementary information. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Fusion of facial expressions and EEG for implicit affective tagging", "paper_id": "WOS:000315843600006"}