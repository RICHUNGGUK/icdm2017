{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "peptide-mhc_class"}, {"score": 0.0046221918332842995, "phrase": "silico_methods"}, {"score": 0.004518442937844318, "phrase": "major_histocompatibility_complex"}, {"score": 0.004259418153299952, "phrase": "available_prediction_tools"}, {"score": 0.003871869188502677, "phrase": "solid_basis"}, {"score": 0.0037848978618663684, "phrase": "different_prediction_tools"}, {"score": 0.0036332251283046997, "phrase": "automated_benchmarking"}, {"score": 0.003503497128638099, "phrase": "weekly_benchmarks"}, {"score": 0.0033630629374009576, "phrase": "immune_epitope_database"}, {"score": 0.003257727050827128, "phrase": "public_access"}, {"score": 0.0031990183472675377, "phrase": "up-to-date_performance_evaluations"}, {"score": 0.0030847461428168614, "phrase": "potential_selection_bias"}, {"score": 0.0029881018404103368, "phrase": "iedb"}, {"score": 0.0028037753646260937, "phrase": "different_prediction_methods"}, {"score": 0.002778385486855157, "phrase": "divergent_predictions"}, {"score": 0.0027035833504191233, "phrase": "experimental_binding_validation"}, {"score": 0.0026307897899760383, "phrase": "benchmark_study"}, {"score": 0.002305589818994465, "phrase": "educated_selections"}, {"score": 0.0022847009758393405, "phrase": "participating_tools"}, {"score": 0.0022231606165229235, "phrase": "netmhcpan"}, {"score": 0.00215370902252004, "phrase": "ann"}, {"score": 0.0021339861577599887, "phrase": "smm"}, {"score": 0.0021049977753042253, "phrase": "arb."}], "paper_keywords": [""], "paper_abstract": "Motivation: Numerous in silico methods predicting peptide binding to major histocompatibility complex (MHC) class I molecules have been developed over the last decades. However, the multitude of available prediction tools makes it non-trivial for the end-user to select which tool to use for a given task. To provide a solid basis on which to compare different prediction tools, we here describe a framework for the automated benchmarking of peptide-MHC class I binding prediction tools. The framework runs weekly benchmarks on data that are newly entered into the Immune Epitope Database (IEDB), giving the public access to frequent, up-to-date performance evaluations of all participating tools. To overcome potential selection bias in the data included in the IEDB, a strategy was implemented that suggests a set of peptides for which different prediction methods give divergent predictions as to their binding capability. Upon experimental binding validation, these peptides entered the benchmark study. Results: The benchmark has run for 15 weeks and includes evaluation of 44 datasets covering 17 MHC alleles and more than 4000 peptide-MHC binding measurements. Inspection of the results allows the end-user to make educated selections between participating tools. Of the four participating servers, NetMHCpan performed the best, followed by ANN, SMM and finally ARB.", "paper_title": "Automated benchmarking of peptide-MHC class I binding predictions", "paper_id": "WOS:000357425800015"}