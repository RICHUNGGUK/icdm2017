{"auto_keywords": [{"score": 0.043528350819261416, "phrase": "measurement_data"}, {"score": 0.034182875683624124, "phrase": "appropriate_classification_model"}, {"score": 0.00481495049065317, "phrase": "single-_and_multi-cycled_software_classification_models"}, {"score": 0.004592905436175987, "phrase": "essential_support_tool"}, {"score": 0.004528301408585875, "phrase": "measurement_and_analysis_processes"}, {"score": 0.004422632150619634, "phrase": "established_models"}, {"score": 0.004278807087784626, "phrase": "model_usage_stage"}, {"score": 0.003930000775297499, "phrase": "unseen_case"}, {"score": 0.003802135020791801, "phrase": "multi-cycled_model"}, {"score": 0.003410479630045025, "phrase": "better_classification_efficiency"}, {"score": 0.003378385493203297, "phrase": "poorer_classification_accuracy"}, {"score": 0.0033465923623290034, "phrase": "software_project_managers"}, {"score": 0.0030446120560494155, "phrase": "important_topic"}, {"score": 0.0029594318330247614, "phrase": "software_measurement"}, {"score": 0.002931569771634119, "phrase": "analysis_literature"}, {"score": 0.0028630535573013686, "phrase": "industrial_software_measurement_dataset"}, {"score": 0.002836201791468494, "phrase": "nasa"}, {"score": 0.0027437235598539904, "phrase": "quantitative_performance_comparisons"}, {"score": 0.0027050595279115015, "phrase": "classification_accuracy"}, {"score": 0.002641823250595624, "phrase": "discriminant_analysis"}, {"score": 0.0025922974876210194, "phrase": "logistic_regression"}, {"score": 0.00240328454980006, "phrase": "echaid"}, {"score": 0.0023030731312573246, "phrase": "experimental_results"}, {"score": 0.002259883463607508, "phrase": "re-appraisal_cost"}, {"score": 0.002175913488493802, "phrase": "software_failure_cost"}, {"score": 0.002155412043665635, "phrase": "type_ii_mr"}, {"score": 0.002125020720324262, "phrase": "data_collection_cost"}, {"score": 0.0021049977753042253, "phrase": "software_measurements"}], "paper_keywords": ["Software measurement and analysis", " Software classification model", " Single-cycle", " Multi-cycle", " Classification accuracy and efficiency"], "paper_abstract": "Software classification models have been regarded as an essential support tool in performing measurement and analysis processes. Most of the established models are single-cycled in the model usage stage, and thus require the measurement data of all the model's variables to be simultaneously collected and utilized for classifying an unseen case within only a single decision cycle. Conversely, the multi-cycled model allows the measurement data of all the model's variables to be gradually collected and utilized for such a classification within more than one decision cycle, and thus intuitively seems to have better classification efficiency but poorer classification accuracy. Software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs. However, this important topic is not adequately explored in software measurement and analysis literature. By using an industrial software measurement dataset of NASA KC2, this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis (DA)- and logistic regression (LR)-based single-cycled models and the decision tree (DT)-based (C4.5 and ECHAID algorithms) multi-cycled models. The experimental results Suggest that the re-appraisal cost of the Type I MR, the software failure cost of Type II MR and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Accuracy and efficiency comparisons of single- and multi-cycled software classification models", "paper_id": "WOS:000262077500016"}