{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "smo_algorithm"}, {"score": 0.015445870108017248, "phrase": "support_vector_regression"}, {"score": 0.013776976196942011, "phrase": "svr"}, {"score": 0.004646852519870358, "phrase": "global_convergence"}, {"score": 0.004524624065803229, "phrase": "sequential_minimal_optimization"}, {"score": 0.0038900709793274484, "phrase": "l_training_samples"}, {"score": 0.0036553070575015344, "phrase": "convex_quadratic_programming"}, {"score": 0.003465350958971198, "phrase": "l_pairs"}, {"score": 0.0030868583963299698, "phrase": "optimality_condition"}, {"score": 0.002515384834484012, "phrase": "finite_number"}, {"score": 0.0023845255359681143, "phrase": "optimal_solution"}, {"score": 0.0023010815714140467, "phrase": "efficient_implementation_techniques"}], "paper_keywords": ["convergence", " quadratic programming (QP)", " sequential minimal optimization (SMO)", " support vector regression (SVR)"], "paper_abstract": "Global convergence of the sequential minimal optimization (SMO) algorithm for support vector regression (SVR) is studied in this paper. Given l training samples, SVR is formulated as a convex quadratic programming (QP) problem with l pairs of variables. We prove that if two pairs of variables violating the optimality condition are chosen for update in each step and subproblems are solved in a certain way, then the SMO algorithm always stops within a finite number of iterations after finding an optimal solution. Also, efficient implementation techniques for the SMO algorithm are presented and compared experimentally with other SMO algorithms.", "paper_title": "Global convergence of SMO algorithm for support vector regression", "paper_id": "WOS:000256670500005"}