{"auto_keywords": [{"score": 0.04096369961791017, "phrase": "loss_functions"}, {"score": 0.010659099726991377, "phrase": "query-level_loss_functions"}, {"score": 0.01017702958648332, "phrase": "query-level_loss_function"}, {"score": 0.00481495049065317, "phrase": "-level_loss_functions"}, {"score": 0.004667833113864304, "phrase": "support_vector_machines"}, {"score": 0.004578165674080246, "phrase": "neural_networks"}, {"score": 0.00447282529270722, "phrase": "ranking_problem"}, {"score": 0.004438250823243621, "phrase": "information_retrieval"}, {"score": 0.003770633265791941, "phrase": "document_pairs"}, {"score": 0.0036411406244705557, "phrase": "evaluation_criteria"}, {"score": 0.0033821367649595254, "phrase": "ranking_performances"}, {"score": 0.0032031180198375283, "phrase": "ranking_functions"}, {"score": 0.003141498438356646, "phrase": "basic_properties"}, {"score": 0.0029867619862229853, "phrase": "cosine_similarity"}, {"score": 0.002952146666681891, "phrase": "ranking_list"}, {"score": 0.002917931350071647, "phrase": "corresponding_ground_truth"}, {"score": 0.002850682396853918, "phrase": "coordinate_descent_algorithm"}, {"score": 0.0027958237350835607, "phrase": "rankcosine"}, {"score": 0.002742017872865778, "phrase": "proposed_loss_function"}, {"score": 0.0026997176829763746, "phrase": "generalized_additive_ranking_model"}, {"score": 0.0026069066266394118, "phrase": "existing_ranking_algorithms"}, {"score": 0.002536926192232391, "phrase": "experimental_results"}, {"score": 0.0024880904032564583, "phrase": "trec_web_track"}, {"score": 0.002468820230936371, "phrase": "ohsumed"}, {"score": 0.0023289568500915207, "phrase": "proposed_query-level_loss_function"}, {"score": 0.0022841153001549193, "phrase": "ranking_accuracies"}, {"score": 0.0021715177915174375, "phrase": "document-level_loss_functions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["information retrieval", " learning to rank", " query-level loss function", " RankCosine"], "paper_abstract": "Many machine learning technologies such as support vector machines, boosting, and neural networks have been applied to the ranking problem in information retrieval. However, since originally the methods were not developed for this task, their loss functions do not directly link to the criteria used in the evaluation of ranking. Specifically, the loss functions are defined on the level of documents or document pairs, in contrast to the fact that the evaluation criteria are defined on the level of queries. Therefore, minimizing the loss functions does not necessarily imply enhancing ranking performances. To solve this problem.. we propose using query-level loss functions in learning of ranking functions. We discuss the basic properties that a query-level loss function should have and propose a query-level loss function based on the cosine similarity between a ranking list and the corresponding ground truth. We further design a coordinate descent algorithm, referred to as RankCosine, which utilizes the proposed loss function to create a generalized additive ranking model. We also discuss whether the loss functions of existing ranking algorithms can be extended to query-level. Experimental results on the datasets of TREC web track, OHSUMED, and a commercial web search engine show that with the use of the proposed query-level loss function we can significantly improve ranking accuracies. Furthermore, we found that it is difficult to extend the document-level loss functions to query-level loss functions. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Query-level loss functions for information retrieval", "paper_id": "WOS:000254813200024"}