{"auto_keywords": [{"score": 0.04660802533264657, "phrase": "virtual_character"}, {"score": 0.015719716506582538, "phrase": "reinforcement_learning"}, {"score": 0.014208152739889365, "phrase": "virtual_environment"}, {"score": 0.011811661133478224, "phrase": "intimate_condition"}, {"score": 0.011464462014881042, "phrase": "social_condition"}, {"score": 0.011193982009813074, "phrase": "random_condition"}, {"score": 0.0047431764968244436, "phrase": "avatar"}, {"score": 0.004616624583337195, "phrase": "immersive_virtual_reality"}, {"score": 0.004386701682320949, "phrase": "specified_location"}, {"score": 0.004256816146106783, "phrase": "wide_field-of-view_head-tracked_stereo_head-mounted_display"}, {"score": 0.0040569185796103845, "phrase": "personal_or_intimate_distance"}, {"score": 0.0032571885279065126, "phrase": "rl_method"}, {"score": 0.0028964869166407054, "phrase": "significant_differences"}, {"score": 0.0024917159298274297, "phrase": "applied_presence_theory"}, {"score": 0.0023888534328463337, "phrase": "immersive_virtual_environments"}, {"score": 0.0021433882705962034, "phrase": "human_participants"}, {"score": 0.0021049977753042253, "phrase": "particular_goals"}], "paper_keywords": ["Experimentation", " Human Factors", " Human-computer interaction", " proxemics", " virtual characters", " avatars"], "paper_abstract": "A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals.", "paper_title": "Reinforcement Learning Utilizes Proxemics: An Avatar Learns to Manipulate the Position of People in Immersive Virtual Reality", "paper_id": "WOS:000304614400003"}