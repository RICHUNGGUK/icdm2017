{"auto_keywords": [{"score": 0.040012451990183495, "phrase": "control_policy_table"}, {"score": 0.014110364056376676, "phrase": "control_signals"}, {"score": 0.009386148777906608, "phrase": "tabular_control_policy"}, {"score": 0.00481495049065317, "phrase": "interactive_games"}, {"score": 0.004765173195920904, "phrase": "fragment-based_character_animation"}, {"score": 0.004547438494330851, "phrase": "appropriate_motion_capture_fragments"}, {"score": 0.004184526032558792, "phrase": "realistic_character_motions"}, {"score": 0.003712796038920873, "phrase": "next_motion_fragment"}, {"score": 0.003598705020845104, "phrase": "current_user's_input"}, {"score": 0.003542976343910816, "phrase": "previous_motion_fragment"}, {"score": 0.003345895878114341, "phrase": "similar_fragments"}, {"score": 0.0032430426853131346, "phrase": "dynamic_programming"}, {"score": 0.003143341238802547, "phrase": "training_samples"}, {"score": 0.0029224275921964724, "phrase": "supervised_learning_routine"}, {"score": 0.002619748321217153, "phrase": "optimal_controller"}, {"score": 0.0024868131939261716, "phrase": "reinforcement_learning_algorithm"}, {"score": 0.002448260697722081, "phrase": "value_iteration"}, {"score": 0.0021049977753042253, "phrase": "interactive_character_games"}], "paper_keywords": ["Motion fragment", " Character control", " Motion graph", " Responsive character animation"], "paper_abstract": "Fragment-based character animation has become popular in recent years. By stringing appropriate motion capture fragments together, the system drives characters responding to the control signals of the user and generates realistic character motions. In this paper, we propose a novel, straightforward and fast method to build the control policy table, which selects the next motion fragment to play based on the current user's input and the previous motion fragment. During the synthesis of the control policy table, we cluster similar fragments together to create several fragment classes. Dynamic programming is employed to generate the training samples based on the control signals of the user. Finally, we use a supervised learning routine to create the tabular control policy. We demonstrate the efficacy of our method by comparing the motions generated by our controller to the optimal controller and other previous controllers. The results indicate that although a reinforcement learning algorithm known as value iteration also creates the tabular control policy, it is more complex and requires more expensive space-time cost in synthesis of the control policy table. Our approach is simple but efficient, and is practical for interactive character games.", "paper_title": "Fragment-based responsive character motion for interactive games", "paper_id": "WOS:000265539300012"}