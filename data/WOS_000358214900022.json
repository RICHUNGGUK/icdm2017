{"auto_keywords": [{"score": 0.04924934083174297, "phrase": "vsm"}, {"score": 0.03189870680658262, "phrase": "replacement_speech"}, {"score": 0.00481495049065317, "phrase": "voice_disorder"}, {"score": 0.004773040022691547, "phrase": "video_self_modeling"}, {"score": 0.004649474569983707, "phrase": "behavioral_intervention_technique"}, {"score": 0.00450933377534525, "phrase": "target_behavior"}, {"score": 0.004223032986655664, "phrase": "speech_language_pathology"}, {"score": 0.003920395745880907, "phrase": "autism"}, {"score": 0.0038355531523269217, "phrase": "fluency_disorder"}, {"score": 0.003769006950727621, "phrase": "technical_challenges"}, {"score": 0.003703611021999795, "phrase": "vsm_contents"}, {"score": 0.0036553070575015344, "phrase": "previously_unseen_behaviors"}, {"score": 0.0035141291835319682, "phrase": "novel_system"}, {"score": 0.003468287913322255, "phrase": "new_video_sequences"}, {"score": 0.003438058620195309, "phrase": "vsm_treatment"}, {"score": 0.003378385493203297, "phrase": "voice_disorders"}, {"score": 0.003305243635051068, "phrase": "video_recording"}, {"score": 0.0032621183241997777, "phrase": "voice-disorder_patient"}, {"score": 0.0032195538751433527, "phrase": "proposed_system"}, {"score": 0.0031775430407770026, "phrase": "coarse_speech"}, {"score": 0.0030547611342112693, "phrase": "patient's_original_voice"}, {"score": 0.0028731024405707277, "phrase": "clean_speeches"}, {"score": 0.002823207272381655, "phrase": "voice_similarity_metric"}, {"score": 0.002725994306480105, "phrase": "original_video"}, {"score": 0.0026321198895843173, "phrase": "audio_segmentation"}, {"score": 0.0026091594819333654, "phrase": "lip-state_detection"}, {"score": 0.002552628669352939, "phrase": "corresponding_time_markers"}, {"score": 0.0025192983141887285, "phrase": "audio_and_video_tracks"}, {"score": 0.0024113009621042677, "phrase": "adaptive_video"}, {"score": 0.0023902621773166963, "phrase": "-sampling_scheme"}, {"score": 0.0022777800735119405, "phrase": "spatial_sharpness"}, {"score": 0.002228412982799273, "phrase": "objective_measurements"}, {"score": 0.0022089663761854657, "phrase": "subjective_evaluations"}, {"score": 0.0021049977753042253, "phrase": "proposed_techniques"}], "paper_keywords": ["Video self modeling", " Positive feedforward", " Voice disorder", " Computational multimedia", " Frame interpolation", " Voice imitation", " Audio segmentation", " Lip reading"], "paper_abstract": "Video self modeling (VSM) is a behavioral intervention technique in which a learner models a target behavior by watching a video of him-or herself. In the field of speech language pathology, the approach of VSM has been successfully used for treatment of language in children with Autism and in individuals with fluency disorder of stuttering. Technical challenges remain in creating VSM contents that depict previously unseen behaviors. In this paper, we propose a novel system that synthesizes new video sequences for VSM treatment of patients with voice disorders. Starting with a video recording of a voice-disorder patient, the proposed system replaces the coarse speech with a clean, healthier speech that bears resemblance to the patient's original voice. The replacement speech is synthesized using either a text-to-speech engine or selecting from a database of clean speeches based on a voice similarity metric. To realign the replacement speech with the original video, a novel audiovisual algorithm that combines audio segmentation with lip-state detection is proposed to identify corresponding time markers in the audio and video tracks. Lip synchronization is then accomplished by using an adaptive video re-sampling scheme that minimizes the amount of motion jitter and preserves the spatial sharpness. Results of both objective measurements and subjective evaluations on a dataset with 31 subjects demonstrate the effectiveness of the proposed techniques.", "paper_title": "Automatic video self modeling for voice disorder", "paper_id": "WOS:000358214900022"}