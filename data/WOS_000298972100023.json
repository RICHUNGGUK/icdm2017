{"auto_keywords": [{"score": 0.04077697487285128, "phrase": "example_images"}, {"score": 0.04017990702934365, "phrase": "corresponding_annotations"}, {"score": 0.00481495049065317, "phrase": "multi-directional_search"}, {"score": 0.004741845603430361, "phrase": "image_annotation_propagation"}, {"score": 0.0046937237797352515, "phrase": "image_annotation"}, {"score": 0.004460309246062286, "phrase": "image_understanding"}, {"score": 0.0044150318724093226, "phrase": "search_areas"}, {"score": 0.004216880725292, "phrase": "novel_multi-directional_search_framework"}, {"score": 0.004174064076143015, "phrase": "semi-automatic_annotation_propagation"}, {"score": 0.003769006950727621, "phrase": "annotation_propagation_process"}, {"score": 0.003351381374975648, "phrase": "local_neighborhood"}, {"score": 0.003025915966640355, "phrase": "user_marks"}, {"score": 0.002949598537993663, "phrase": "annotation_process"}, {"score": 0.00290473242917264, "phrase": "multiple_directions"}, {"score": 0.002860546817196148, "phrase": "feature_space"}, {"score": 0.0028170314398963704, "phrase": "query_movements"}, {"score": 0.0027459678326877744, "phrase": "multiple_path_navigation"}, {"score": 0.0026091594819333654, "phrase": "user's_input"}, {"score": 0.0024918548761791435, "phrase": "accurate_annotation_assistance"}, {"score": 0.002453934333239467, "phrase": "user_-_images"}, {"score": 0.002392008392892289, "phrase": "different_visual_characteristics"}, {"score": 0.0023079225317714815, "phrase": "comprehensive_experiments"}, {"score": 0.002284444272376043, "phrase": "corel_and_u._of_washington_image_databases"}, {"score": 0.0021705796895687864, "phrase": "annotating_image_databases"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Image annotation", " Annotation propagation", " Image retrieval", " Relevance feedback", " Bridge semantic gap", " Multi-directional search", " Dynamic clustering", " Relevance history"], "paper_abstract": "Image annotation has attracted lots of attention due to its importance in image understanding and search areas. In this paper, we propose a novel Multi-Directional Search framework for semi-automatic annotation propagation. In this system, the user interacts with the system to provide example images and the corresponding annotations during the annotation propagation process. In each iteration, the example images are clustered and the corresponding annotations are propagated separately to each cluster: images in the local neighborhood are annotated. Furthermore, some of those images are returned to the user for further annotation. As the user marks more images, the annotation process goes into multiple directions in the feature space. The query movements can be treated as multiple path navigation. Each path could be further split based on the user's input. In this manner, the system provides accurate annotation assistance to the user - images with the same semantic meaning but different visual characteristics can be handled effectively. From comprehensive experiments on Corel and U. of Washington image databases, the proposed technique shows accuracy and efficiency on annotating image databases. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "A Multi-Directional Search technique for image annotation propagation", "paper_id": "WOS:000298972100023"}