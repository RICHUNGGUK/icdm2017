{"auto_keywords": [{"score": 0.03325918553268762, "phrase": "global_model"}, {"score": 0.00481495049065317, "phrase": "natural_images"}, {"score": 0.004722038375317162, "phrase": "key_step"}, {"score": 0.004685375117376838, "phrase": "perceptual_organization"}, {"score": 0.004402088928254543, "phrase": "high-level_processing"}, {"score": 0.004333973355873714, "phrase": "non-accidental_shapes"}, {"score": 0.004135859558144883, "phrase": "computational_model"}, {"score": 0.004071846628708832, "phrase": "complex_natural_scenes"}, {"score": 0.003993216345115755, "phrase": "large_dataset"}, {"score": 0.003916098498242888, "phrase": "human-marked_segmentations"}, {"score": 0.0038106195546165574, "phrase": "quantitative_evaluation"}, {"score": 0.0037079710789139305, "phrase": "familiar_configuration"}, {"score": 0.0036648265911110164, "phrase": "prototypical_local_shapes"}, {"score": 0.003580032420161922, "phrase": "image_data"}, {"score": 0.0035108659422301985, "phrase": "mid-level_visual_cues"}, {"score": 0.0033372020049592726, "phrase": "shapeme_representation"}, {"score": 0.0032727116186194584, "phrase": "logistic_classifier"}, {"score": 0.0031107911390188055, "phrase": "conditional_random_field"}, {"score": 0.002968424524565369, "phrase": "loopy_belief_propagation"}, {"score": 0.0029338597685088603, "phrase": "approximate_inference"}, {"score": 0.002865929543375884, "phrase": "maximum_likelihood_parameters"}, {"score": 0.0028436364707396613, "phrase": "ground-truth_labels"}, {"score": 0.00277778944472107, "phrase": "local_shapeme_model"}, {"score": 0.002681859084438076, "phrase": "correct_figural_assignment"}, {"score": 0.002619748321217153, "phrase": "previous_studies"}, {"score": 0.0024418934977061876, "phrase": "low-level_edge_detector"}, {"score": 0.002366763442497185, "phrase": "human_segmentations"}, {"score": 0.0023391880766516285, "phrase": "global_crf_model"}, {"score": 0.0022760856906838814, "phrase": "local_model"}, {"score": 0.002223351523626989, "phrase": "human-marked_boundaries"}, {"score": 0.002171836484200008, "phrase": "promising_experimental_results"}, {"score": 0.0021215125099937663, "phrase": "feasible_approach"}], "paper_keywords": [""], "paper_abstract": "Figure/ground assignment is a key step in perceptual organization which assigns contours to one of the two abutting regions, providing information about occlusion and allowing high-level processing to focus on non-accidental shapes of figural regions. In this paper, we develop a computational model for figure/ground assignment in complex natural scenes. We utilize a large dataset of images annotated with human-marked segmentations and figure/ground labels for training and quantitative evaluation. We operationalize the concept of familiar configuration by constructing prototypical local shapes, i.e. shapemes, from image data. Shapemes automatically encode mid-level visual cues to figure/ground assignment such as convexity and parallelism. Based on the shapeme representation, we train a logistic classifier to locally predict figure/ground labels. We also consider a global model using a conditional random field (CRF) to enforce global figure/ground consistency at T-junctions. We use loopy belief propagation to perform approximate inference on this model and learn maximum likelihood parameters from ground-truth labels. We find that the local shapeme model achieves an accuracy of 64% in predicting the correct figural assignment. This compares favorably to previous studies using classical figure/ground cues [1]. We evaluate the global model using either a set of contours extracted from a low-level edge detector or the set of contours given by human segmentations. The global CRF model significantly improves the performance over the local model, most notably when using human-marked boundaries (78%). These promising experimental results show that this is a feasible approach to bottom-up figure/ground assignment in natural images.", "paper_title": "Figure/ground assignment in natural images", "paper_id": "WOS:000237555200047"}