{"auto_keywords": [{"score": 0.03328570139681654, "phrase": "proposed_algorithm"}, {"score": 0.007582641708866281, "phrase": "average_response_time"}, {"score": 0.00481495049065317, "phrase": "distributed_computing-a_reinforcement_learning_approach"}, {"score": 0.004635187610736025, "phrase": "grid_computing"}, {"score": 0.0045850648862091085, "phrase": "online_users"}, {"score": 0.004413846562278824, "phrase": "dynamic_resources"}, {"score": 0.004366106816225457, "phrase": "task_arrival_and_execution_processes"}, {"score": 0.004157561334726201, "phrase": "consequent_uncertainties"}, {"score": 0.003980531478122155, "phrase": "response_time"}, {"score": 0.0038948525410795517, "phrase": "main_concern"}, {"score": 0.0038527044625533574, "phrase": "dynamic_scheduling"}, {"score": 0.0037493114715625784, "phrase": "decision_theory"}, {"score": 0.0036091892088392775, "phrase": "markov_decision_process"}, {"score": 0.003362660648932247, "phrase": "machine_learning"}, {"score": 0.003272375502853897, "phrase": "task_arrival"}, {"score": 0.003032228032253871, "phrase": "aforehand_modeling"}, {"score": 0.0028715299874549245, "phrase": "forthcoming_tasks"}, {"score": 0.002719325582159337, "phrase": "min-min"}, {"score": 0.0025892394160604167, "phrase": "ect"}, {"score": 0.0022718774783143203, "phrase": "smaller_variance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Distributed computing", " Markov decision process", " Queueing model", " Reinforcement learning", " Task scheduling"], "paper_abstract": "In distributed computing such as grid computing, online users submit their tasks anytime and anywhere to dynamic resources. Task arrival and execution processes are stochastic. How to adapt to the consequent uncertainties, as well as scheduling overhead and response time, are the main concern in dynamic scheduling. Based on the decision theory, scheduling is formulated as a Markov decision process (MDP). To address this problem, an approach from machine learning is used to learn task arrival and execution patterns online. The proposed algorithm can automatically acquire such knowledge without any aforehand modeling, and proactively allocate tasks on account of the forthcoming tasks and their execution dynamics. Under comparison with four classic algorithms such as Min-Min, Min-Max, Suffrage, and ECT, the proposed algorithm has much less scheduling overhead. The experiments over both synthetic and practical environments reveal that the proposed algorithm outperforms other algorithms in terms of the average response time. The smaller variance of average response time further validates the robustness of our algorithm. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Proactive scheduling in distributed computing-A reinforcement learning approach", "paper_id": "WOS:000337261000012"}