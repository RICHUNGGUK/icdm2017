{"auto_keywords": [{"score": 0.04722154241900733, "phrase": "video_clip"}, {"score": 0.00481495049065317, "phrase": "grounding_language_inference"}, {"score": 0.00448179813172021, "phrase": "entire_natural-language_sentence"}, {"score": 0.00422899036681456, "phrase": "entire_sentences"}, {"score": 0.003963211180578005, "phrase": "predicate-argument_relations"}, {"score": 0.0033635683594830158, "phrase": "spatial_relations"}, {"score": 0.002748984715897838, "phrase": "multiple_similar_simultaneous_events"}, {"score": 0.0024387423375226507, "phrase": "video_clips"}, {"score": 0.0021049977753042253, "phrase": "learned_meaning_representations"}], "paper_keywords": [""], "paper_abstract": "We present an approach to simultaneously reasoning about a video clip and an entire natural-language sentence. The compositional nature of language is exploited to construct models which represent the meanings of entire sentences composed out of the meanings of the words in those sentences mediated by a grammar that encodes the predicate-argument relations. We demonstrate that these models faithfully represent the meanings of sentences and are sensitive to how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) affect the meaning of a sentence and how it is grounded in video. We exploit this methodology in three ways. In the first, a video clip along with a sentence are taken as input and the participants in the event described by the sentence are highlighted, even when the clip depicts multiple similar simultaneous events. In the second, a video clip is taken as input without a sentence and a sentence is generated that describes an event in that clip. In the third, a corpus of video clips is paired with sentences which describe some of the events in those clips and the meanings of the words in those sentences are learned. We learn these meanings without needing to specify which attribute of the video clips each word in a given sentence refers to. The learned meaning representations are shown to be intelligible to humans.", "paper_title": "A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video", "paper_id": "WOS:000354603900003"}