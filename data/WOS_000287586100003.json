{"auto_keywords": [{"score": 0.04861823021267309, "phrase": "probabilistic_inference_queries"}, {"score": 0.015719716506582538, "phrase": "frequent_probabilistic_inference_queries"}, {"score": 0.010817195764231935, "phrase": "materialized_views"}, {"score": 0.0047657508931351865, "phrase": "databases"}, {"score": 0.004716520738331953, "phrase": "existing_solutions"}, {"score": 0.004525629293824217, "phrase": "single_inference_query"}, {"score": 0.004188198923320451, "phrase": "frequent_queries"}, {"score": 0.0037965243835456214, "phrase": "computation_caching"}, {"score": 0.0036616109902631293, "phrase": "inference_queries"}, {"score": 0.0035681790580031998, "phrase": "clique_tree_propagation"}, {"score": 0.003234299589505403, "phrase": "intermediate_results"}, {"score": 0.0031845067058084583, "phrase": "previous_inference_queries"}, {"score": 0.0029620573986902416, "phrase": "time_cost"}, {"score": 0.0028567126120064546, "phrase": "query_workload"}, {"score": 0.002769395841379498, "phrase": "frequently_queried_variables"}, {"score": 0.0026847499540354858, "phrase": "ctp"}, {"score": 0.0026161698049580804, "phrase": "frequent_query_variables"}, {"score": 0.0024333239726872604, "phrase": "different_query_plans"}, {"score": 0.002298649369775623, "phrase": "optimal_query_plan"}, {"score": 0.0022168460543913787, "phrase": "experimental_evaluation"}, {"score": 0.002194011800331242, "phrase": "relational_databases"}], "paper_keywords": ["Probabilistic inference", " variable elimination", " clique tree propagation"], "paper_abstract": "Existing solutions for probabilistic inference queries mainly focus on answering a single inference query, but seldom address the issues of efficiently returning results for a sequence of frequent queries, which is more popular and practical in many real applications. In this paper, we mainly study the computation caching and sharing among a sequence of inference queries in databases. The clique tree propagation (CTP) algorithm is first introduced in databases for probabilistic inference queries. We use the materialized views to cache the intermediate results of the previous inference queries, which might be shared with the following queries, and consequently reduce the time cost. Moreover, we take the query workload into account to identify the frequently queried variables. To optimize probabilistic inference queries with CTP, we cache these frequent query variables into the materialized views to maximize the reuse. Due to the existence of different query plans, we present heuristics to estimate costs and select the optimal query plan. Finally, we present the experimental evaluation in relational databases to illustrate the validity and superiority of our approaches in answering frequent probabilistic inference queries.", "paper_title": "Answering Frequent Probabilistic Inference Queries in Databases", "paper_id": "WOS:000287586100003"}