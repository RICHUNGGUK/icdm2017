{"auto_keywords": [{"score": 0.04970198387475844, "phrase": "semantic_analysis"}, {"score": 0.041590199587873876, "phrase": "concept_database"}, {"score": 0.04096150193558667, "phrase": "textual_query"}, {"score": 0.00481495049065317, "phrase": "based_image_retrieval"}, {"score": 0.004683540676363745, "phrase": "new_text"}, {"score": 0.004573748810476845, "phrase": "-ranking_approach"}, {"score": 0.00450198136261303, "phrase": "relevancy_rate"}, {"score": 0.0043103495043308755, "phrase": "fundamental_semantic_gap"}, {"score": 0.004225950334493496, "phrase": "low-level_visual_features"}, {"score": 0.004143196873246147, "phrase": "high-level_textual_queries"}, {"score": 0.004062057297453209, "phrase": "connected_hierarchy"}, {"score": 0.0037978390554858766, "phrase": "popular_search_engines"}, {"score": 0.003753022329975749, "phrase": "initial_retrieval"}, {"score": 0.0035931556794840027, "phrase": "higher_level_concepts"}, {"score": 0.00344007531836105, "phrase": "two-layer_scoring_system"}, {"score": 0.0032034752726003025, "phrase": "image_feature_vectors"}, {"score": 0.0030913302090179967, "phrase": "related_concept"}, {"score": 0.0028334016912935165, "phrase": "second_feature"}, {"score": 0.0027126012542151015, "phrase": "query_accuracy"}, {"score": 0.002576442680766742, "phrase": "users'_queries"}, {"score": 0.002408623172021922, "phrase": "user_queries"}, {"score": 0.0023427323356117365, "phrase": "human_labor"}, {"score": 0.0023058910418682676, "phrase": "sophisticated_initial_concept_database"}, {"score": 0.0022517100995552443, "phrase": "complex_queries"}, {"score": 0.0022162970216404927, "phrase": "five_scenarios"}, {"score": 0.0021471293746360026, "phrase": "significant_improvement"}, {"score": 0.0021049977753042253, "phrase": "current_state-of-the-art_image_search_engines"}], "paper_keywords": ["Re-ranking", " Semantic gap", " CBIR", " Image retrieval", " Semantic mapping scheme"], "paper_abstract": "We present a new text-to-image re-ranking approach for improving the relevancy rate in searches. In particular, we focus on the fundamental semantic gap that exists between the low-level visual features of the image and high-level textual queries by dynamically maintaining a connected hierarchy in the form of a concept database. For each textual query, we take the results from popular search engines as an initial retrieval, followed by a semantic analysis to map the textual query to higher level concepts. In order to do this, we design a two-layer scoring system which can identify the relationship between the query and the concepts automatically. We then calculate the image feature vectors and compare them with the classifier for each related concept. An image is relevant only when it is related to the query both semantically and content-wise. The second feature of this work is that we loosen the requirement for query accuracy from the user, which makes it possible to perform well on users' queries containing less relevant information. Thirdly, the concept database can be dynamically maintained to satisfy the variations in user queries, which eliminates the need for human labor in building a sophisticated initial concept database. We designed our experiment using complex queries (based on five scenarios) to demonstrate how our retrieval results are a significant improvement over those obtained from current state-of-the-art image search engines.", "paper_title": "Automatic content based image retrieval using semantic analysis", "paper_id": "WOS:000342150600003"}