{"auto_keywords": [{"score": 0.04708471221683657, "phrase": "input_points"}, {"score": 0.00481495049065317, "phrase": "strategyproof_classification"}, {"score": 0.004757087682427765, "phrase": "strategyproof_classification_problem"}, {"score": 0.004624751696958501, "phrase": "decision_maker"}, {"score": 0.004477992162329249, "phrase": "binary_labels"}, {"score": 0.004388631450145507, "phrase": "expected_error"}, {"score": 0.00421520166010848, "phrase": "self-interested_agents"}, {"score": 0.003734810936407523, "phrase": "truthful_mechanisms"}, {"score": 0.0036898803847106023, "phrase": "false_reports"}, {"score": 0.003587125711973247, "phrase": "strategyproof_mechanisms"}, {"score": 0.0035439655241057207, "phrase": "classification_problem"}, {"score": 0.0032559935867182035, "phrase": "shared_set"}, {"score": 0.003152531962718477, "phrase": "plausible_assumptions"}, {"score": 0.003114584448820957, "phrase": "strong_positive_results"}, {"score": 0.0029792984108210525, "phrase": "random_dictator_mechanism"}, {"score": 0.002884603537502399, "phrase": "approximately_optimal_outcomes"}, {"score": 0.002639385610609024, "phrase": "best_possible_approximation_ratio"}, {"score": 0.0025658094607383646, "phrase": "truthful_mechanism"}, {"score": 0.0024247382451294255, "phrase": "sampled_data"}, {"score": 0.002385872451981323, "phrase": "pac-style_generalization_bounds"}, {"score": 0.002174155795026218, "phrase": "facility_location"}, {"score": 0.0021566565951738658, "phrase": "judgment_aggregation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Mechanism design", " Classification", " Game theory", " Approximation"], "paper_abstract": "The strategyproof classification problem deals with a setting where a decision maker must classify a set of input points with binary labels, while minimizing the expected error. The labels of the input points are reported by self-interested agents, who might lie in order to obtain a classifier that more closely matches their own labels, thereby creating a bias in the data; this motivates the design of truthful mechanisms that discourage false reports. In this paper we give strategyproof mechanisms for the classification problem in two restricted settings: (i) there are only two classifiers, and (ii) all agents are interested in a shared set of input points. We show that these plausible assumptions lead to strong positive results. In particular, we demonstrate that variations of a random dictator mechanism. that are truthful, can guarantee approximately optimal outcomes with respect to any family of classifiers. Moreover, these results are tight in the sense that they match the best possible approximation ratio that can be guaranteed by any truthful mechanism. We further show how our mechanisms can be used for learning classifiers from sampled data, and provide PAC-style generalization bounds on their expected error. Interestingly, our results can be applied to problems in the context of various fields beyond classification, including facility location and judgment aggregation. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Algorithms for strategyproof classification", "paper_id": "WOS:000304737300004"}