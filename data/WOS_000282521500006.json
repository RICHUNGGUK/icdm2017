{"auto_keywords": [{"score": 0.0496586067150974, "phrase": "kernel_function"}, {"score": 0.03906211445148561, "phrase": "kernel_learning_problem"}, {"score": 0.026026892183866696, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "appropriate_selection"}, {"score": 0.004689210758225518, "phrase": "feature_space"}, {"score": 0.004593677275091001, "phrase": "crucial_role"}, {"score": 0.0045266275871436975, "phrase": "kernel_methods"}, {"score": 0.004280605517385785, "phrase": "invariant_kernels"}, {"score": 0.004218106086234951, "phrase": "binary_classification"}, {"score": 0.004181043455992058, "phrase": "learning_capacity"}, {"score": 0.003930529468459255, "phrase": "radial_kernels"}, {"score": 0.003850392835818635, "phrase": "invariant_kernel_functions"}, {"score": 0.0037830010278885437, "phrase": "nested_set"}, {"score": 0.00357724980284755, "phrase": "appropriate_sub-class"}, {"score": 0.0034328142696903345, "phrase": "lanckriet_et_al"}, {"score": 0.0033529044784726477, "phrase": "functional_formulation"}, {"score": 0.0032460470691682074, "phrase": "optimal_kernel"}, {"score": 0.0032174978938185736, "phrase": "finite_mixture"}, {"score": 0.0031986042942694255, "phrase": "cosine_functions"}, {"score": 0.003114942418729772, "phrase": "semi-infinite_programming"}, {"score": 0.0030067770042809354, "phrase": "quadratically_constrained_quadratic_programming"}, {"score": 0.0028938199455860836, "phrase": "cosine_kernel"}, {"score": 0.0027933114255870063, "phrase": "qcqp_sub-problem"}, {"score": 0.002744370297766629, "phrase": "kernel_matrices"}, {"score": 0.002649038691536882, "phrase": "large-scale_problems"}, {"score": 0.0025345056783823206, "phrase": "individual_kernels"}, {"score": 0.002519612567209806, "phrase": "isotropic_gaussian_kernels"}, {"score": 0.0024900877185228647, "phrase": "learning_process"}, {"score": 0.0024681707866449376, "phrase": "interesting_feature"}, {"score": 0.002417776723734214, "phrase": "optimal_classifier"}, {"score": 0.0023475606648681742, "phrase": "cosine_kernels"}, {"score": 0.002279379132030189, "phrase": "remarkable_speedup"}, {"score": 0.002193688457453366, "phrase": "kernel_trick"}, {"score": 0.002180793657565869, "phrase": "complex-valued_kernel_functions"}, {"score": 0.0021488865854453073, "phrase": "artificial_and_real-world_benchmark_data_sets"}, {"score": 0.0021236985566587614, "phrase": "usps"}, {"score": 0.0021049977753042253, "phrase": "mnist_digit_recognition_data_sets"}], "paper_keywords": ["kernel learning", " translation invariant kernels", " capacity control", " support vector machines", " classification", " semi-infinite programming"], "paper_abstract": "Appropriate selection of the kernel function, which implicitly defines the feature space of an algorithm, has a crucial role in the success of kernel methods. In this paper, we consider the problem of optimizing a kernel function over the class of translation invariant kernels for the task of binary classification. The learning capacity of this class is invariant with respect to rotation and scaling of the features and it encompasses the set of radial kernels. We show that how translation invariant kernel functions can be embedded in a nested set of sub-classes and consider the kernel learning problem over one of these sub-classes. This allows the choice of an appropriate sub-class based on the problem at hand. We use the criterion proposed by Lanckriet et al. (2004) to obtain a functional formulation for the problem. It will be proven that the optimal kernel is a finite mixture of cosine functions. The kernel learning problem is then formulated as a semi-infinite programming (SIP) problem which is solved by a sequence of quadratically constrained quadratic programming (QCQP) sub-problems. Using the fact that the cosine kernel is of rank two, we propose a formulation of a QCQP sub-problem which does not require the kernel matrices to be loaded into memory, making the method applicable to large-scale problems. We also address the issue of including other classes of kernels, such as individual kernels and isotropic Gaussian kernels, in the learning process. Another interesting feature of the proposed method is that the optimal classifier has an expansion in terms of the number of cosine kernels, instead of support vectors, leading to a remarkable speedup at run-time. As a by-product, we also generalize the kernel trick to complex-valued kernel functions. Our experiments on artificial and real-world benchmark data sets, including the USPS and the MNIST digit recognition data sets, show the usefulness of the proposed method.", "paper_title": "Learning Translation Invariant Kernels for Classification", "paper_id": "WOS:000282521500006"}