{"auto_keywords": [{"score": 0.029796678827189137, "phrase": "fuzzy_balancer"}, {"score": 0.02695501214392698, "phrase": "efsl"}, {"score": 0.00481495049065317, "phrase": "fuzzy_reinforcement_learning"}, {"score": 0.004675768396728975, "phrase": "fuzzy_balance_management_scheme"}, {"score": 0.004409304640026443, "phrase": "critic-only_fuzzy_reinforcement_learning_method"}, {"score": 0.004198839649063847, "phrase": "newly_developed_continuous_reinforcement_learning_method"}, {"score": 0.004117482754216607, "phrase": "sarsa"}, {"score": 0.0037888687452790953, "phrase": "action_value_function_approximation"}, {"score": 0.0036792371574197826, "phrase": "overfitting_problem"}, {"score": 0.0036256123915754303, "phrase": "action_value_function"}, {"score": 0.0035902958977673313, "phrase": "continuous_reinforcement_learning_algorithms"}, {"score": 0.0034863899423238707, "phrase": "new_adaptive_learning_rate"}, {"score": 0.003303617297414837, "phrase": "learning_rate"}, {"score": 0.0032237268289353983, "phrase": "\"fuzzy_visit_value"}, {"score": 0.0031612033120065747, "phrase": "current_state"}, {"score": 0.00311510526624626, "phrase": "training_data"}, {"score": 0.003024909947609294, "phrase": "uniform_effect"}, {"score": 0.0029807934480637855, "phrase": "weight_parameters"}, {"score": 0.002663208600887875, "phrase": "suitable_temperature_factor"}, {"score": 0.0026243534128245886, "phrase": "softmax_formula"}, {"score": 0.0025608471909800076, "phrase": "enhanced_fsl"}, {"score": 0.0024503741901918527, "phrase": "proposed_adaptive_learning_rate"}, {"score": 0.002391067964886602, "phrase": "fsl._simulation_results"}, {"score": 0.0021678232353129472, "phrase": "action_quality"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Decision analysis", " Fuzzy control", " Exploration", " Exploitation"], "paper_abstract": "This paper offers a fuzzy balance management scheme between exploration and exploitation, which can be implemented in any critic-only fuzzy reinforcement learning method. The paper, however. focuses on a newly developed continuous reinforcement learning method, called fuzzy Sarsa learning (FSL) due to its advantages. Establishing balance greatly depends on the accuracy of action value function approximation. At first. the overfitting problem in approximating action value function in continuous reinforcement learning algorithms is discussed, and a new adaptive learning rate is proposed to prevent this problem. By relating the learning rate to the inverse of \"fuzzy visit value\" of the current state, the training data set is forced to have uniform effect on the weight parameters of the approximator and hence overfitting is resolved. Then. a fuzzy balancer is introduced to balance exploration vs. exploitation by generating a suitable temperature factor for the Softmax formula. Finally, an enhanced FSL (EFSL) is offered by integrating the proposed adaptive learning rate and the fuzzy balancer into FSL. Simulation results show that EFSL eliminates overfitting, well manages balance. and outperforms FSL in terms of learning speed and action quality. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Exploration and exploitation balance management in fuzzy reinforcement learning", "paper_id": "WOS:000274284600006"}