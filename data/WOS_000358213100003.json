{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "automated_temporal_abstraction"}, {"score": 0.004713162564945179, "phrase": "observable_reinforcement_learning"}, {"score": 0.004548253531664032, "phrase": "reinforcement_learning"}, {"score": 0.004357927940322685, "phrase": "learning_time"}, {"score": 0.004205395707787349, "phrase": "repeated_sub-policy_patterns"}, {"score": 0.004029359223675788, "phrase": "automatic_extraction"}, {"score": 0.003916098498242888, "phrase": "rl_process"}, {"score": 0.0032766780632972363, "phrase": "problem_domain"}, {"score": 0.0031393948330209224, "phrase": "learning_agent"}, {"score": 0.003094920033010818, "phrase": "learning_abstractions"}, {"score": 0.003051073364818384, "phrase": "partially_observable_rl"}, {"score": 0.002986461855862617, "phrase": "relatively_less_explored_area"}, {"score": 0.0028006988338192375, "phrase": "existing_automatic_abstraction_method"}, {"score": 0.0027610089418453614, "phrase": "namely_extended_sequence_tree"}, {"score": 0.002664221051021009, "phrase": "fully_observable_problems"}, {"score": 0.0026077803487003, "phrase": "modified_method"}, {"score": 0.002516350372379076, "phrase": "model-based_partially_observable_rl_settings"}, {"score": 0.0024281181691350085, "phrase": "belief_state_discretization_methods"}, {"score": 0.002309754152767196, "phrase": "new_abstraction_mechanism"}, {"score": 0.002212892265364764, "phrase": "proposed_abstraction_method"}, {"score": 0.0021049977753042253, "phrase": "well-known_benchmark_problems"}], "paper_keywords": ["Learning abstractions", " partially observable Markov decision process (POMDP)", " reinforcement learning (RL)"], "paper_abstract": "Temporal abstraction for reinforcement learning (RL) aims to decrease learning time by making use of repeated sub-policy patterns in the learning task. Automatic extraction of abstractions during RL process is difficult but has many challenges such as dealing with the curse of dimensionality. Various studies have explored the subject under the assumption that the problem domain is fully observable by the learning agent. Learning abstractions for partially observable RL is a relatively less explored area. In this paper, we adapt an existing automatic abstraction method, namely extended sequence tree, originally designed for fully observable problems. The modified method covers a certain family of model-based partially observable RL settings. We also introduce belief state discretization methods that can be used with this new abstraction mechanism. The effectiveness of the proposed abstraction method is shown empirically by experimenting on well-known benchmark problems.", "paper_title": "Toward Generalization of Automated Temporal Abstraction to Partially Observable Reinforcement Learning", "paper_id": "WOS:000358213100003"}