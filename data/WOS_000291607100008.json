{"auto_keywords": [{"score": 0.049783551527037576, "phrase": "parameterized_learning_problems"}, {"score": 0.041048078735704104, "phrase": "learning_methods"}, {"score": 0.00481495049065317, "phrase": "reinforcement_learning_methods"}, {"score": 0.004711779740073973, "phrase": "reinforcement_learning"}, {"score": 0.004582358363660175, "phrase": "elegant_theoretical_results"}, {"score": 0.004374473000977313, "phrase": "optimal_policies"}, {"score": 0.004307286039184164, "phrase": "practical_problems"}, {"score": 0.004073809704022463, "phrase": "rl_algorithms"}, {"score": 0.0038055011575934936, "phrase": "different_problems"}, {"score": 0.0037586442197443375, "phrase": "account_factors"}, {"score": 0.003723879111071547, "phrase": "state_estimation"}, {"score": 0.003678023470672862, "phrase": "function_approximation"}, {"score": 0.0033515736929172644, "phrase": "targeted_studies"}, {"score": 0.0031795757989482765, "phrase": "optimal_behavior"}, {"score": 0.0030634906292640947, "phrase": "existing_rl_applications"}, {"score": 0.002942495422585405, "phrase": "\"first_order\"_factors"}, {"score": 0.0028438444033180303, "phrase": "appropriate_parameterized_learning_problem"}, {"score": 0.0025832180231667853, "phrase": "classes_project"}, {"score": 0.002465806683872287, "phrase": "cma-es"}, {"score": 0.002339153851589379, "phrase": "relevant_problem_instances"}, {"score": 0.0022889103976458437, "phrase": "problem_space"}, {"score": 0.002212123058024591, "phrase": "extensive_search_procedure"}, {"score": 0.0021916308348851428, "phrase": "additional_insights"}, {"score": 0.002164602220179823, "phrase": "method-specific_parameters"}, {"score": 0.0021379062257004698, "phrase": "eligibility_traces"}, {"score": 0.0021246816854197732, "phrase": "initial_weights"}, {"score": 0.0021049977753042253, "phrase": "population_sizes"}], "paper_keywords": ["Reinforcement learning", " Empirical evaluation", " Partial observability", " Function approximation", " Parameterized learning problem"], "paper_abstract": "The field of reinforcement learning (RL) has been energized in the past few decades by elegant theoretical results indicating under what conditions, and how quickly, certain algorithms are guaranteed to converge to optimal policies. However, in practical problems, these conditions are seldom met. When we cannot achieve optimality, the performance of RL algorithms must be measured empirically. Consequently, in order to meaningfully differentiate learning methods, it becomes necessary to characterize their performance on different problems, taking into account factors such as state estimation, exploration, function approximation, and constraints on computation and memory. To this end, we propose parameterized learning problems, in which such factors can be controlled systematically and their effects on learning methods characterized through targeted studies. Apart from providing very precise control of the parameters that affect learning, our parameterized learning problems enable benchmarking against optimal behavior; their relatively small sizes facilitate extensive experimentation. Based on a survey of existing RL applications, in this article, we focus our attention on two predominant, \"first order\" factors: partial observability and function approximation. We design an appropriate parameterized learning problem, through which we compare two qualitatively distinct classes of algorithms: on-line value function-based methods and policy search methods. Empirical comparisons among various methods within each of these classes project Sarsa(lambda) and Q-learning(lambda) as winners among the former, and CMA-ES as the winner in the latter. Comparing Sarsa(lambda) and CMA-ES further on relevant problem instances, our study highlights regions of the problem space favoring their contrasting approaches. Short run-times for our experiments allow for an extensive search procedure that provides additional insights on relationships between method-specific parameters-such as eligibility traces, initial weights, and population sizes-and problem instances.", "paper_title": "Characterizing reinforcement learning methods through parameterized learning problems", "paper_id": "WOS:000291607100008"}