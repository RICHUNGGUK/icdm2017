{"auto_keywords": [{"score": 0.047785680182713765, "phrase": "eda"}, {"score": 0.04413109051936228, "phrase": "amalgam"}, {"score": 0.015719645500980213, "phrase": "parameter-free_amalgam"}, {"score": 0.0046192222897398685, "phrase": "parameter-free_estimation-of-distribution_algorithm"}, {"score": 0.004449822473607577, "phrase": "gaussian"}, {"score": 0.004140354220982935, "phrase": "numerical_optimization"}, {"score": 0.0037674258625824113, "phrase": "covariance_matrix"}, {"score": 0.0037249853525936428, "phrase": "gaussian_distribution"}, {"score": 0.003440945901489461, "phrase": "noisy_bbob_problems"}, {"score": 0.0032884384364324395, "phrase": "experimental_evidence"}, {"score": 0.0032026066791743866, "phrase": "wide_range"}, {"score": 0.003142668996712078, "phrase": "perceived_polynomial_scalability"}, {"score": 0.0031072444973406586, "phrase": "multimodal_problems"}, {"score": 0.0030606302942851027, "phrase": "best_or_near-best_results"}, {"score": 0.002902903244616154, "phrase": "katsuuras"}, {"score": 0.002795228413298423, "phrase": "time_limit"}, {"score": 0.002763709443278514, "phrase": "rastrigin-bueche"}, {"score": 0.0027325449069867222, "phrase": "lunacek_bi-rastrigin"}, {"score": 0.0027119634978446895, "phrase": "higher_dimensions"}, {"score": 0.002552781867125423, "phrase": "larger_required_population_size"}, {"score": 0.002402921062033032, "phrase": "search_space"}, {"score": 0.002340147988348115, "phrase": "noise_averaging"}, {"score": 0.0022618378709785172, "phrase": "direct_application"}, {"score": 0.002137088643982437, "phrase": "best_performing_algorithms"}, {"score": 0.0021049977753042253, "phrase": "bbob_workshop"}], "paper_keywords": ["Estimation-of-distribution algorithms", " Gaussian distribution", " maximum likelihood", " adaptive computation", " optimization", " black box optimization", " benchmarking"], "paper_abstract": "We describe a parameter-free estimation-of-distribution algorithm (EDA) called the adapted maximum-likelihood Gaussian model iterated density-estimation evolutionary algorithm (AMaLGaM-IDEA, or AMaLGaM for short) for numerical optimization. AMaLGaM is benchmarked within the 2009 black box optimization benchmarking (BBOB) framework and compared to a variant with incremental model building (iAMaLGaM). We study the implications of factorizing the covariance matrix in the Gaussian distribution, to use only a few or no covariances. Further, AMaLGaM and iAMaLGaM are also evaluated on the noisy BBOB problems and we assess how well multiple evaluations per solution can average out noise. Experimental evidence suggests that parameter-free AMaLGaM can solve a wide range of problems efficiently with perceived polynomial scalability, including multimodal problems, obtaining the best or near-best results among all algorithms tested in 2009 on functions such as the step-ellipsoid and Katsuuras, but failing to locate the optimum within the time limit on skew Rastrigin-Bueche separable and Lunacek bi-Rastrigin in higher dimensions. AMaLGaM is found to be more robust to noise than iAMaLGaM due to the larger required population size. Using few or no covariances hinders the EDA from dealing with rotations of the search space. Finally, the use of noise averaging is found to be less efficient than the direct application of the EDA unless the noise is uniformly distributed. AMaLGaM was among the best performing algorithms submitted to the BBOB workshop in 2009.", "paper_title": "Benchmarking Parameter-Free AMaLGaM on Functions With and Without Noise", "paper_id": "WOS:000330360000004"}