{"auto_keywords": [{"score": 0.04828362906607965, "phrase": "new_method"}, {"score": 0.009422134346301715, "phrase": "evaluation_function"}, {"score": 0.00481495049065317, "phrase": "evaluation_functions"}, {"score": 0.004769182299692333, "phrase": "minimax_search"}, {"score": 0.00459039896577789, "phrase": "minimax_tree"}, {"score": 0.004397230847865867, "phrase": "heuristic_evaluation_function"}, {"score": 0.004334657104697421, "phrase": "practical_alpha-beta_search_program"}, {"score": 0.004132411680987228, "phrase": "non-linear_combination"}, {"score": 0.003791763445590174, "phrase": "search_results"}, {"score": 0.0037199473428123175, "phrase": "move_decisions"}, {"score": 0.0036494864560524735, "phrase": "game_records"}, {"score": 0.0036147562669032957, "phrase": "human_experts"}, {"score": 0.0035632774896594524, "phrase": "well-modeled_objective_function"}, {"score": 0.0033969001271637934, "phrase": "numerical_iterative_method"}, {"score": 0.0033166379485150507, "phrase": "local_minima"}, {"score": 0.00326939082861424, "phrase": "objective_function"}, {"score": 0.003131637033429892, "phrase": "small_number"}, {"score": 0.0031018195951638882, "phrase": "hyper_parameters"}, {"score": 0.0029569247903654477, "phrase": "major_variant"}, {"score": 0.0028187792953857957, "phrase": "larger_state_space"}, {"score": 0.002752139271586703, "phrase": "experimental_results"}, {"score": 0.002699960312990493, "phrase": "large-scale_optimization"}, {"score": 0.0026235360404084137, "phrase": "playing_strength"}, {"score": 0.002598543802992702, "phrase": "shogi_programs"}, {"score": 0.002372649454733251, "phrase": "b_o"}, {"score": 0.00230546852838578, "phrase": "made_substantial_contributions"}, {"score": 0.0022725931637071852, "phrase": "program's_first-place"}, {"score": 0.002166349705365602, "phrase": "preliminary_evidence"}, {"score": 0.002145703330831668, "phrase": "broader_applicability"}], "paper_keywords": [""], "paper_abstract": "This paper presents a new method, Minimax Tree Optimization (MMTO), to learn a heuristic evaluation function of a practical alpha-beta search program. The evaluation function may be a linear or non-linear combination of weighted features, and the weights are the parameters to be optimized. To control the search results so that the move decisions agree with the game records of human experts, a well-modeled objective function to be minimized is designed. Moreover, a numerical iterative method is used to find local minima of the objective function, and more than forty million parameters are adjusted by using a small number of hyper parameters. This method was applied to shogi, a major variant of chess in which the evaluation function must handle a larger state space than in chess. Experimental results show that the large-scale optimization of the evaluation function improves the playing strength of shogi programs, and the new method performs significantly better than other methods. Implementation of the new method in our shogi program B o n a n z a made substantial contributions to the program's first-place finish in the 2013 World Computer Shogi Championship. Additionally, we present preliminary evidence of broader applicability of our method to other two-player games such as chess.", "paper_title": "Large-Scale Optimization for Evaluation Functions with Minimax Search", "paper_id": "WOS:000333554100001"}