{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "penalized_likelihood"}, {"score": 0.04876404494641556, "phrase": "objective_function"}, {"score": 0.025291683923675968, "phrase": "proposed_method"}, {"score": 0.004748371393578422, "phrase": "general_approach"}, {"score": 0.004532988904965752, "phrase": "log_likelihood"}, {"score": 0.004367707617503722, "phrase": "non-smooth_solutions"}, {"score": 0.0036951530348821116, "phrase": "regression_problem"}, {"score": 0.003543804852018315, "phrase": "classification_problem"}, {"score": 0.003414467360488913, "phrase": "new_classification_method"}, {"score": 0.0031845067058084583, "phrase": "novel_penalty_term"}, {"score": 0.003125835572347772, "phrase": "k-nearest_neighbors"}, {"score": 0.003096905353118796, "phrase": "simple_analytical_derivations"}, {"score": 0.0025355968118803956, "phrase": "k-nearest_neighbor_contributions"}, {"score": 0.0024658019116387845, "phrase": "unbalanced_class_patterns_situation"}, {"score": 0.002420339995088838, "phrase": "extensive_experiments"}, {"score": 0.0022258906408344973, "phrase": "top_ranks"}, {"score": 0.0022052711621931144, "phrase": "classification_performance"}, {"score": 0.002164602220179823, "phrase": "fairly_small_computation_time"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["K-nearest neighbor", " Penalized likelihood", " Pattern classification", " Posterior probability", " Class balancing", " Weighted KNN"], "paper_abstract": "Penalized likelihood is a general approach whereby an objective function is defined, consisting of the log likelihood of the data minus some term penalizing non-smooth solutions. Subsequently, this objective function is maximized, yielding a solution that achieves Some sort of trade-off between the faithfulness and the smoothness of the fit. Most work on that topic focused on the regression problem, and there has been little work on the classification problem. In this paper we propose a new classification method using the concept of penalized likelihood (for the two class case). By proposing a novel penalty term based on the K-nearest neighbors, simple analytical derivations have led to an algorithm that is proved to converge to the global optimum. Moreover, this algorithm is very simple to implement and converges typically in two or three iterations. We also introduced two variants of the method by distance-weighting the K-nearest neighbor contributions, and by tackling the unbalanced class patterns situation. We performed extensive experiments to compare the proposed method to several well-known classification methods. These simulations reveal that the proposed method achieves one of the top ranks in classification performance and with a fairly small computation time. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "A penalized likelihood based pattern classification algorithm", "paper_id": "WOS:000268453800031"}