{"auto_keywords": [{"score": 0.03833590842663952, "phrase": "hfn_framework"}, {"score": 0.009375741846188937, "phrase": "learning_process"}, {"score": 0.007880805964900798, "phrase": "heuristic_functions"}, {"score": 0.007815846120983558, "phrase": "hfn"}, {"score": 0.00481495049065317, "phrase": "heuristic_function_negotiation_for"}, {"score": 0.004795002816566936, "phrase": "markov_decision_process"}, {"score": 0.004716031208088975, "phrase": "uav_simulation"}, {"score": 0.004657653216262924, "phrase": "traditional_reinforcement_learning"}, {"score": 0.004580933283492972, "phrase": "markov_decision_processes"}, {"score": 0.004431246962597134, "phrase": "learning_methods"}, {"score": 0.004322186889256224, "phrase": "priori_knowledge"}, {"score": 0.004146327465630697, "phrase": "optimal_policy"}, {"score": 0.0039446780026489905, "phrase": "specific_issues"}, {"score": 0.003784120134537663, "phrase": "heuristic_function_negotiation"}, {"score": 0.003690927840780375, "phrase": "online_learning_framework"}, {"score": 0.0034967818541774844, "phrase": "state-action_dual_layer_structure"}, {"score": 0.0034534720319134562, "phrase": "rl"}, {"score": 0.0034106416719768035, "phrase": "triple_layer_structure"}, {"score": 0.003354393040689453, "phrase": "multiple_heuristic_functions"}, {"score": 0.0031125001790084936, "phrase": "different_algorithms"}, {"score": 0.0029981559422442693, "phrase": "appropriate_action"}, {"score": 0.0027934768366077397, "phrase": "domain_knowledge"}, {"score": 0.0026135859312619875, "phrase": "user_preferences"}, {"score": 0.00247596871869742, "phrase": "rl."}, {"score": 0.0023948981440134478, "phrase": "reasonable_heuristic_functions"}, {"score": 0.0023651843133643768, "phrase": "learning_results"}, {"score": 0.0022877332844249065, "phrase": "traditional_rl."}, {"score": 0.0022220423638260015, "phrase": "air_combat_simulation"}, {"score": 0.0022036216160494925, "phrase": "unmanned_aerial_vehicles"}, {"score": 0.0021314498639048085, "phrase": "different_function_settings"}, {"score": 0.0021049977753042253, "phrase": "different_combat_behaviors"}], "paper_keywords": ["Markov decision processes", " heuristic function", " reinforcement learning", " UAV"], "paper_abstract": "The traditional reinforcement learning (RL) methods can solve Markov Decision Processes (MDPs) online, but these learning methods cannot effectively use a priori knowledge to guide the learning process. The exploration of the optimal policy is time-consuming and does not employ the information about specific issues. To tackle the problem, this paper proposes heuristic function negotiation (HFN) as an online learning framework. The HFN framework extends MDPs and introduces heuristic functions. HFN changes the state-action dual layer structure of traditional RL to the triple layer structure, in which multiple heuristic functions can be set to meet the needs required to solve the problem. The HFN framework can use different algorithms to let the functions negotiate to determine the appropriate action, and adjust the impact of each function according to the rewards. The HFN framework introduces domain knowledge by setting heuristic functions and thus speeds up the problem solving of MDPs. Furthermore, user preferences can be reflected in the learning process, which improves the flexibility of RL. The experiments show that, by setting reasonable heuristic functions, the learning results of the HFN framework are more efficient than traditional RL. We also apply HFN to the air combat simulation of unmanned aerial vehicles (UAVs), which shows that different function settings lead to different combat behaviors.", "paper_title": "Heuristic Function Negotiation for Markov Decision Process and Its Application in UAV Simulation", "paper_id": "WOS:000330553700009"}