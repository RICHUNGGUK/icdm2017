{"auto_keywords": [{"score": 0.04586846484671965, "phrase": "peaking_phenomenon"}, {"score": 0.00481495049065317, "phrase": "fixed_sample_size"}, {"score": 0.004755054742217193, "phrase": "common_phenomenon"}, {"score": 0.004618166136069052, "phrase": "designed_classifier_decreases"}, {"score": 0.00421297029755273, "phrase": "classification_rule"}, {"score": 0.00417793854311445, "phrase": "feature-label_distribution"}, {"score": 0.0039737570920809215, "phrase": "fixed_ordering"}, {"score": 0.0038271712887391015, "phrase": "strongest_individual_feature"}, {"score": 0.0037168948492715386, "phrase": "individual_classification_capability"}, {"score": 0.0036097844103129043, "phrase": "account_feature-selection"}, {"score": 0.0035204268675160257, "phrase": "high-dimensional_and_small_sample_settings"}, {"score": 0.0033204052817782464, "phrase": "massive_simulation"}, {"score": 0.003279039724748895, "phrase": "high-performance_computing_environment"}, {"score": 0.0031845067058084583, "phrase": "feature-label_models"}, {"score": 0.0031579997262619758, "phrase": "feature-selection_algorithms"}, {"score": 0.0031186510842824626, "phrase": "classifier_models"}, {"score": 0.003066945490483524, "phrase": "large_library"}, {"score": 0.0030160945545795494, "phrase": "feature_size_curves"}, {"score": 0.00290473242917264, "phrase": "genomic_classification"}, {"score": 0.0028446460307503343, "phrase": "gene-expression-based_classification"}, {"score": 0.0028209599545587745, "phrase": "breast-cancer_patient_prognosis"}, {"score": 0.002660574167091781, "phrase": "error_curves"}, {"score": 0.002447101794397243, "phrase": "long_range"}, {"score": 0.00227914162528171, "phrase": "peaking_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["classification", " feature-selection", " peaking phenomenon"], "paper_abstract": "For a fixed sample size, a common phenomenon is that the error of a designed classifier decreases and then increases as the number of features grows. This peaking phenomenon has been recognized for forty years and depends on the classification rule and feature-label distribution. Historically, the peaking phenomenon has been treated by assuming a fixed Ordering of the features, usually beginning with the strongest individual feature and proceeding with features of decreasing individual classification capability. This does not take into account feature-selection, which is commonplace in high-dimensional and small sample settings. This paper revisits the peaking phenomenon in the presence of feature-selection. Using massive simulation in a high-performance computing environment, the paper considers various combinations of feature-label models, feature-selection algorithms, and classifier models to produce a large library of error versus feature size curves. Owing to the prevalence of feature-selection in genomic classification, we also consider gene-expression-based classification of breast-cancer patient prognosis. Results vary widely and are strongly dependent on the combination. The error curves tend to fall into three categories: peaking, settling into a plateau, or falling very slowly over a long range of feature set sizes. It can be concluded that one should be wary of applying peaking results found in the absence of feature-selection to settings in which feature-selection is employed. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "The peaking phenomenon in the presence of feature-selection", "paper_id": "WOS:000258177000008"}