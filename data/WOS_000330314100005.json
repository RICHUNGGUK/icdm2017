{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multimedia"}, {"score": 0.004707913053935566, "phrase": "multimodal_feature_fusion"}, {"score": 0.004655285779819366, "phrase": "temporal_concept_localization"}, {"score": 0.004475659507646842, "phrase": "multimedia_event_detection"}, {"score": 0.004400804926266097, "phrase": "developed_system"}, {"score": 0.004351595642056769, "phrase": "complex_multimedia_events"}, {"score": 0.004254814649568355, "phrase": "large_array"}, {"score": 0.004207230913063179, "phrase": "multimodal_features"}, {"score": 0.0041136473765874815, "phrase": "unseen_videos"}, {"score": 0.004022137057452406, "phrase": "diverse_responses"}, {"score": 0.003802135020791801, "phrase": "novel_visual_and_audio_features"}, {"score": 0.0035539019235553897, "phrase": "unsupervised_manner"}, {"score": 0.0035141291835319682, "phrase": "mid-level_and_high-level_features"}, {"score": 0.0033594249301033604, "phrase": "semantic_understanding"}, {"score": 0.0032296377835432533, "phrase": "novel_latent_svm_model"}, {"score": 0.003140004304719746, "phrase": "discriminative_high-level_concepts"}, {"score": 0.0029681091946289757, "phrase": "detection_accuracy"}, {"score": 0.00293487292076612, "phrase": "existing_approaches"}, {"score": 0.002853396755102163, "phrase": "unique_summary"}, {"score": 0.002743105561254975, "phrase": "high-level_concepts"}, {"score": 0.0027123819773059127, "phrase": "temporal_evidence_localization"}, {"score": 0.00266693888510356, "phrase": "resulting_summary"}, {"score": 0.0023962545023307937, "phrase": "novel_fusion_learning_algorithms"}, {"score": 0.0022777800735119405, "phrase": "limited_training_data_condition"}, {"score": 0.0021049977753042253, "phrase": "presented_system"}], "paper_keywords": ["Multimedia", " Classification", " Machine learning", " Fusion"], "paper_abstract": "We present a system for multimedia event detection. The developed system characterizes complex multimedia events based on a large array of multimodal features, and classifies unseen videos by effectively fusing diverse responses. We present three major technical innovations. First, we explore novel visual and audio features across multiple semantic granularities, including building, often in an unsupervised manner, mid-level and high-level features upon low-level features to enable semantic understanding. Second, we show a novel Latent SVM model which learns and localizes discriminative high-level concepts in cluttered video sequences. In addition to improving detection accuracy beyond existing approaches, it enables a unique summary for every retrieval by its use of high-level concepts and temporal evidence localization. The resulting summary provides some transparency into why the system classified the video as it did. Finally, we present novel fusion learning algorithms and our methodology to improve fusion learning under limited training data condition. Thorough evaluation on a large TRECVID MED 2011 dataset showcases the benefits of the presented system.", "paper_title": "Multimedia event detection with multimodal feature fusion and temporal concept localization", "paper_id": "WOS:000330314100005"}