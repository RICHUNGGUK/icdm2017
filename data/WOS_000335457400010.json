{"auto_keywords": [{"score": 0.04957786067961377, "phrase": "eligibility_traces"}, {"score": 0.00481495049065317, "phrase": "policy_learning"}, {"score": 0.004544127927346964, "phrase": "markov_decision_processes"}, {"score": 0.004177049813981273, "phrase": "linear_approximation"}, {"score": 0.004111584914709491, "phrase": "value_function"}, {"score": 0.004047141845635495, "phrase": "fixed_policy"}, {"score": 0.003799275066001196, "phrase": "on-policy_learning_algorithms"}, {"score": 0.0034555479218891638, "phrase": "unified_algorithmic_view"}, {"score": 0.003330406613112555, "phrase": "systematic_approach"}, {"score": 0.002828331235643921, "phrase": "new_extensions"}, {"score": 0.0025586903829811296, "phrase": "comprehensive_algorithmic_derivation"}, {"score": 0.0024789887640568093, "phrase": "recursive_and_memory-efficent_form"}, {"score": 0.0023765719803077673, "phrase": "garnet"}, {"score": 0.002172701452347454, "phrase": "feature_space_dimension"}, {"score": 0.0021049977753042253, "phrase": "least-squares_approach"}], "paper_keywords": ["reinforcement learning", " value function estimation", " off-policy learning", " eligibility traces"], "paper_abstract": "In the framework of Markov Decision Processes, we consider linear off-policy learning, that is the problem of learning a linear approximation of the value function of some fixed policy from one trajectory possibly generated by some other policy. We briefly review on-policy learning algorithms of the literature (gradient-based and least-squares-based), adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting them to off-policy learning with eligibility traces. This leads to some known algorithms-off-policy LSTD(lambda), LSPE(lambda), TD(lambda), TDC/GQ(lambda)-and suggests new extensions-off-policy FPKF(lambda), BRM(lambda), gBRM(lambda), GTD2(lambda). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their known convergence properties and illustrate their relative empirical behavior on Garnet problems. Our experiments suggest that the most standard algorithms on and off-policy LSTD(lambda)/LSPE(lambda)-and TD(lambda) if the feature space dimension is too large for a least-squares approach-perform the best.", "paper_title": "Off-policy Learning With Eligibility Traces: A Survey", "paper_id": "WOS:000335457400010"}