{"auto_keywords": [{"score": 0.04556577424655626, "phrase": "off-chip_memory"}, {"score": 0.03522627194855989, "phrase": "power_consumption"}, {"score": 0.00481495049065317, "phrase": "c-pack"}, {"score": 0.004625164125506718, "phrase": "microprocessor_designers"}, {"score": 0.004541377162789232, "phrase": "tight_constraints"}, {"score": 0.004459101239560164, "phrase": "on-chip_cache_memory"}, {"score": 0.004298974960001275, "phrase": "dynamic_random_access_memory"}, {"score": 0.00385214804128572, "phrase": "computer_systems"}, {"score": 0.0038240605592708083, "phrase": "microarchitecture_researchers"}, {"score": 0.0037684961730030235, "phrase": "hardware_data_compression_units"}, {"score": 0.0037273513742154237, "phrase": "memory_hierarchies"}, {"score": 0.0036065830625093883, "phrase": "energy_efficiency"}, {"score": 0.0034516024325468653, "phrase": "cache_compression"}, {"score": 0.003401431460601017, "phrase": "unsubstantiated_assumptions"}, {"score": 0.003303259489028193, "phrase": "area_overheads"}, {"score": 0.0032671775301001483, "phrase": "proposed_compression_algorithms"}, {"score": 0.003081272473508291, "phrase": "memory_hierarchy"}, {"score": 0.002863652837038965, "phrase": "raw_compression_ratio"}, {"score": 0.0027006464509273806, "phrase": "lossless_compression_algorithm"}, {"score": 0.0026419338604879404, "phrase": "fast_on-line_data_compression"}, {"score": 0.0025098414374397308, "phrase": "novel_features"}, {"score": 0.002419542935774618, "phrase": "compressed_lines"}, {"score": 0.0023756164484983874, "phrase": "parallel_compression"}, {"score": 0.0023582694348219233, "phrase": "multiple_words"}, {"score": 0.002323953608863511, "phrase": "single_dictionary"}, {"score": 0.0022817585639886883, "phrase": "compression_ratio"}, {"score": 0.00224032791638295, "phrase": "proposed_algorithm"}, {"score": 0.0022158306933307685, "phrase": "register_transfer_level_hardware_design"}, {"score": 0.0021518035978153878, "phrase": "area_estimation"}, {"score": 0.0021049977753042253, "phrase": "previous_work"}], "paper_keywords": ["Cache compression", " effective system-wide compression ratio", " hardware implementation", " pair matching", " parallel compression"], "paper_abstract": "Microprocessor designers have been torn between tight constraints on the amount of on-chip cache memory and the high latency of off-chip memory, such as dynamic random access memory. Accessing off-chip memory generally takes an order of magnitude more time than accessing on-chip cache, and two orders of magnitude more time than executing an instruction. Computer systems and microarchitecture researchers have proposed using hardware data compression units within the memory hierarchies of microprocessors in order to improve performance, energy efficiency, and functionality. However, most past work, and all work on cache compression, has made unsubstantiated assumptions about the performance, power consumption, and area overheads of the proposed compression algorithms and hardware. It is not possible to determine whether compression at levels of the memory hierarchy closest to the processor is beneficial without understanding its costs. Furthermore, as we show in this paper, raw compression ratio is not always the most important metric. In this work, we present a lossless compression algorithm that has been designed for fast on-line data compression, and cache compression in particular. The algorithm has a number of novel features tailored for this application, including combining pairs of compressed lines into one cache line and allowing parallel compression of multiple words while using a single dictionary and without degradation in compression ratio. We reduced the proposed algorithm to a register transfer level hardware design, permitting performance, power consumption, and area estimation. Experiments comparing our work to previous work are described.", "paper_title": "C-Pack: A High-Performance Microprocessor Cache Compression Algorithm", "paper_id": "WOS:000282380600005"}