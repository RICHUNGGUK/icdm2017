{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "least_squares_superposition_codes_of_moderate_dictionary"}, {"score": 0.004284755353695709, "phrase": "additive_white_gaussian_noise_channel"}, {"score": 0.004202223763361474, "phrase": "average_codeword_power_constraint"}, {"score": 0.003775803846431323, "phrase": "sparse_superpositions"}, {"score": 0.0033271045779740683, "phrase": "possible_messages"}, {"score": 0.0029892206557359836, "phrase": "least_squares"}, {"score": 0.0027383842769094354, "phrase": "assumed_form"}, {"score": 0.0026337482179592422, "phrase": "linear_combinations"}, {"score": 0.002297949552105756, "phrase": "error_probability"}, {"score": 0.0021049977753042253, "phrase": "shannon_capacity"}], "paper_keywords": ["Achieving capacity", " compressed sensing", " exponential error bounds", " Gaussian channel", " maximum likelihood estimation", " subset selection"], "paper_abstract": "For the additive white Gaussian noise channel with average codeword power constraint, coding methods are analyzed in which the codewords are sparse superpositions, that is, linear combinations of subsets of vectors from a given design, with the possible messages indexed by the choice of subset. Decoding is by least squares (maximum likelihood), tailored to the assumed form of codewords being linear combinations of elements of the design. Communication is shown to be reliable with error probability exponentially small for all rates up to the Shannon capacity.", "paper_title": "Least Squares Superposition Codes of Moderate Dictionary Size Are Reliable at Rates up to Capacity", "paper_id": "WOS:000303204900001"}