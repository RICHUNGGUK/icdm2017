{"auto_keywords": [{"score": 0.03253253627295768, "phrase": "gaussian_assumption"}, {"score": 0.015719716506582538, "phrase": "artificial_metaplasticity_learning_algorithm"}, {"score": 0.010258322068494039, "phrase": "training_patterns"}, {"score": 0.00467207889925365, "phrase": "biological_metaplasticity_property"}, {"score": 0.004592469289385798, "phrase": "shannon"}, {"score": 0.004398872331144531, "phrase": "bio-inspired_hypothesis"}, {"score": 0.0041773934798743405, "phrase": "biological_learning"}, {"score": 0.004123781267600542, "phrase": "unfrequent_patterns"}, {"score": 0.0040708542958968605, "phrase": "common_ones"}, {"score": 0.003751011827704279, "phrase": "artificial_metaplasticity"}, {"score": 0.003623949225126944, "phrase": "regular_backpropagation_algorithm"}, {"score": 0.0035620354052144656, "phrase": "variable_learning_rate"}, {"score": 0.003486123304074186, "phrase": "iteration_step"}, {"score": 0.003426556090707852, "phrase": "heterosynaptic_plasticity"}, {"score": 0.0033971539563683174, "phrase": "biological_neurons"}, {"score": 0.003353521531649663, "phrase": "variable_rate"}, {"score": 0.003324743991757282, "phrase": "statistical_inference"}, {"score": 0.0032820385246266773, "phrase": "training_set"}, {"score": 0.003170811465728631, "phrase": "gaussian_distribution"}, {"score": 0.0030109772626153797, "phrase": "real_one"}, {"score": 0.0029722900280316216, "phrase": "statistical_information"}, {"score": 0.0027861630854233693, "phrase": "significative_variations"}, {"score": 0.0027150087516999047, "phrase": "input_sets"}, {"score": 0.002680114113899346, "phrase": "different_probability_distributions"}, {"score": 0.00253400005142168, "phrase": "general_algorithm"}, {"score": 0.002427035025924384, "phrase": "inherent_statistical_inference"}, {"score": 0.0023346234464901978, "phrase": "input_patterns"}, {"score": 0.002236055625099805, "phrase": "last_algorithm"}, {"score": 0.0022073031056287886, "phrase": "input_distribution"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Synapse", " Plasticity", " Metaplasticity", " Backpropagation", " Artificial metaplasticity", " Learning", " Information entropy"], "paper_abstract": "Artificial metaplasticity learning algorithm is inspired by the biological metaplasticity property of neurons and Shannon's information theory. It is based on the bio-inspired hypothesis that neurons do not learn in the same amount (metaplasticity of biological learning) from unfrequent patterns than from common ones, as the former are expected to contain more information than the latter (information entropy concept). On MLPs, the artificial metaplasticity can be formulated as an improvement in regular backpropagation algorithm by using a variable learning rate affecting all the weights in each iteration step and so resembling heterosynaptic plasticity of biological neurons. The variable rate involves statistical inference on the training set and it is common to successfully assume Gaussian distribution for the training patterns. Nevertheless, Gaussian assumption may diverge from the real one and using statistical information extracted from the training patterns may be necessary. In this research, robustness to significative variations on Gaussian assumption is evaluated using input sets generated with different probability distributions. For the cases where Gaussian assumption shows to degrade learning, a general algorithm is applied. This algorithm takes advantage of the inherent statistical inference performed by the MLP through the a posteriori probabilities of input patterns estimation provided by its outputs. The generality of this last algorithm for any input distribution is then demonstrated. (C) 2014 Published by Elsevier B.V.", "paper_title": "Robustness of artificial metaplasticity learning algorithm", "paper_id": "WOS:000347753400005"}