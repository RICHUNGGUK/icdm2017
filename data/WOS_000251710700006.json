{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "evolutionary_and_hybrid_algorithms"}, {"score": 0.004776297980616768, "phrase": "real-valued_optimisation"}, {"score": 0.0047379542803396915, "phrase": "randomised_population-based_algorithms"}, {"score": 0.004550783060775205, "phrase": "traditional_search_techniques"}, {"score": 0.004266501496765747, "phrase": "readily_applicable_nature"}, {"score": 0.003749908582313067, "phrase": "effective_comparative_evaluations"}, {"score": 0.003515479934694842, "phrase": "better_performance"}, {"score": 0.0033628392198449134, "phrase": "evolutionary_and_allied_algorithms"}, {"score": 0.0031909403420958752, "phrase": "benchmarking_test"}, {"score": 0.0031398318568608505, "phrase": "fair_comparison"}, {"score": 0.0027816546451972725, "phrase": "pairwise_comparison"}, {"score": 0.0027041233221381756, "phrase": "first_problem"}, {"score": 0.0025554669315362424, "phrase": "commonly_used_benchmarking_functions"}, {"score": 0.0024742098091226203, "phrase": "test_problems"}, {"score": 0.0024247382451294255, "phrase": "self-similar_or_fractal_landscapes"}, {"score": 0.002254672021120091, "phrase": "web_services"}, {"score": 0.0021049977753042253, "phrase": "central_benchmarking_repository"}], "paper_keywords": ["real-valued optimisation", " evolutionary algorithms", " hybrid algorithms", " benchmarking", " fractal landscapes", " composite recursive functions", " web services"], "paper_abstract": "Randomised population-based algorithms, such as evolutionary, genetic and swarm-based algorithms, and their hybrids with traditional search techniques, have proven successful and robust on many difficult real-valued optimisation problems. This success, along with the readily applicable nature of these techniques, has led to an explosion in the number of algorithms and variants proposed. In order for the field to advance it is necessary to carry out effective comparative evaluations of these algorithms, and thereby better identify and understand those properties that lead to better performance. This paper discusses the difficulties of providing benchmarking of evolutionary and allied algorithms that is both meaningful and logistically viable. To be meaningful the benchmarking test must give a fair comparison that is free, as far as possible, from biases that favour one style of algorithm over another. To be logistically viable it must overcome the need for pairwise comparison between all the proposed algorithms. To address the first problem, we begin by attempting to identify the biases that are inherent in commonly used benchmarking functions. We then describe a suite of test problems, generated recursively as self-similar or fractal landscapes, designed to overcome these biases. For the second, we describe a server that uses web services to allow researchers to 'plug in' their algorithms, running on their local machines, to a central benchmarking repository.", "paper_title": "Towards unbiased benchmarking of evolutionary and hybrid algorithms for real-valued optimisation", "paper_id": "WOS:000251710700006"}