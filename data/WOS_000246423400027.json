{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "validation-based_learning"}, {"score": 0.004383406949799696, "phrase": "tight_convex_relaxation"}, {"score": 0.0038431888954930083, "phrase": "based_criterion"}, {"score": 0.003531474887552889, "phrase": "ridge_regression"}, {"score": 0.0034657012607680203, "phrase": "regularization_networks"}, {"score": 0.0032145922365231093, "phrase": "support_vector_machines"}, {"score": 0.0029260602766930065, "phrase": "convex_approach"}, {"score": 0.0027916287357898544, "phrase": "reliable_and_efficient_tools"}, {"score": 0.002688532127821979, "phrase": "computational_cost"}, {"score": 0.002564985233317994, "phrase": "learning_method"}, {"score": 0.0021049977753042253, "phrase": "weighted_ls-svm."}], "paper_keywords": ["convex optimization", " model selection", " regularization"], "paper_abstract": "This letter investigates a tight convex relaxation to the problem of tuning the regularization constant with respect to a validation based criterion. A number of algorithms is covered including ridge regression, regularization networks, smoothing splines, and least squares support vector machines (LS-SVMs) for regression. This convex approach allows the application of reliable and efficient tools, thereby improving computational cost and automatization of the learning method. It is shown that all solutions of the relaxation allow an interpretation in terms of a solution to a weighted LS-SVM.", "paper_title": "A convex approach to validation-based learning of the regularization constant", "paper_id": "WOS:000246423400027"}