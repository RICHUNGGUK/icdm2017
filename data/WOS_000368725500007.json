{"auto_keywords": [{"score": 0.040642263753439135, "phrase": "speculative_execution"}, {"score": 0.014590221314593529, "phrase": "pse"}, {"score": 0.010612387000973441, "phrase": "mapreduce_performance"}, {"score": 0.004776297980616768, "phrase": "partial_speculative_execution"}, {"score": 0.004718897469501381, "phrase": "mapreduce_framework"}, {"score": 0.00464343008299973, "phrase": "de_facto_standard"}, {"score": 0.004283739213556286, "phrase": "multiple_tasks"}, {"score": 0.00421520166010848, "phrase": "task_execution"}, {"score": 0.004164516065972361, "phrase": "large_cluster"}, {"score": 0.004131063310728304, "phrase": "commodity_machines"}, {"score": 0.004081385289516914, "phrase": "increasing_heterogeneity"}, {"score": 0.0040485975929323, "phrase": "distributed_environments"}, {"score": 0.0039042694651106884, "phrase": "job_completion"}, {"score": 0.003719773848735395, "phrase": "existing_speculative_execution_mechanism"}, {"score": 0.0030278017562213265, "phrase": "original_tasks"}, {"score": 0.002826949449085849, "phrase": "processed_data"}, {"score": 0.0027592929218287344, "phrase": "hadoop"}, {"score": 0.0026715589067557526, "phrase": "job_completion_time"}, {"score": 0.0025761937402886954, "phrase": "classical_workloads"}, {"score": 0.0025554669315362424, "phrase": "experimental_results"}, {"score": 0.002504374718975704, "phrase": "heterogeneous_environments"}, {"score": 0.0023099968745243066, "phrase": "late"}, {"score": 0.0022006705963260433, "phrase": "average_pse"}, {"score": 0.0021049977753042253, "phrase": "late."}], "paper_keywords": ["Speculative execution", " MapReduce performance", " Straggler mitigation"], "paper_abstract": "The MapReduce framework has become the de facto standard for big data processing due to its attractive features and abilities. One is that it automatically parallelizes a job into multiple tasks and transparently handles task execution on a large cluster of commodity machines. The increasing heterogeneity of distributed environments may result in a few straggling tasks, which prolong job completion. Speculative execution is proposed to mitigate stragglers. However, the existing speculative execution mechanism could not work efficiently as many speculative tasks are still slower than their original tasks. In this paper, we explore an approach to increase the efficiency of speculative execution, and further improve MapReduce performance. We propose the Partial Speculative Execution (PSE) strategy to make speculative tasks start from the checkpoint. By leveraging the checkpoint of original tasks, PSE can eliminate the costs of re-reading, re-copying, and re-computing the processed data. We implement PSE in Hadoop, and evaluate its performance in terms of job completion time and the efficiency of speculative execution under several kinds of classical workloads. Experimental results show that, in heterogeneous environments with stragglers, PSE completes jobs 56 % faster than that with no speculation and 12 % faster than that with LATE, an improved speculative execution algorithm. In addition, on average PSE can improve the efficiency of speculative execution by 24 % compared to LATE.", "paper_title": "Improving MapReduce Performance with Partial Speculative Execution", "paper_id": "WOS:000368725500007"}