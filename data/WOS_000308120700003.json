{"auto_keywords": [{"score": 0.047916548037392724, "phrase": "openmp"}, {"score": 0.005441388338980815, "phrase": "mpi."}, {"score": 0.00481495049065317, "phrase": "fluid_and_fluid-particulate_systems"}, {"score": 0.004527371799277529, "phrase": "large_scale_multi-physics_applications"}, {"score": 0.004322967486451877, "phrase": "efficient_computation"}, {"score": 0.004085579055228594, "phrase": "openmp_codes"}, {"score": 0.003961613484271729, "phrase": "large_core_counts"}, {"score": 0.003802135020791801, "phrase": "fine_grained_openmp_parallelism"}, {"score": 0.00374399358035391, "phrase": "large_cfd_code"}, {"score": 0.0034485432592912917, "phrase": "performance_optimization"}, {"score": 0.003292685392413911, "phrase": "weak_and_strong_scaling_studies"}, {"score": 0.0030640804704319255, "phrase": "sgi_altix_systems"}, {"score": 0.0028807718824400697, "phrase": "key_components"}, {"score": 0.002836679611901583, "phrase": "good_scalability"}, {"score": 0.002708399936381754, "phrase": "hybrid_implementation"}, {"score": 0.00266693888510356, "phrase": "dual_core_system"}, {"score": 0.0025992391074270097, "phrase": "standalone_mpi"}, {"score": 0.002468938912850425, "phrase": "irregular_multi-physics_applications"}, {"score": 0.0022856052476424344, "phrase": "tightly_coupled_fluid-particulate_systems"}, {"score": 0.0021821915946356168, "phrase": "big_performance_advantage"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Computational fluid dynamics (CFD)", " OpenMP", " MPI", " Hybrid parallelization", " Performance tools", " Multiphase flows"], "paper_abstract": "In order to exploit the flexibility of OpenMP in parallelizing large scale multi-physics applications where different modes of parallelism are needed for efficient computation, it is first necessary to be able to scale OpenMP codes as well as MPI on large core counts. In this research we have implemented fine grained OpenMP parallelism for a large CFD code Gen-IDLEST and investigated the performance from 1 to 256 cores using a variety of performance optimization and measurement tools. It is shown through weak and strong scaling studies that OpenMP performance can be made to match that of MPI on the SGI Altix systems for up to 256 cores. Data placement and locality were established to be key components in obtaining good scalability with OpenMP. It is also shown that a hybrid implementation on a dual core system gives the same performance as standalone MPI or OpenMP. Finally, it is shown that in irregular multi-physics applications which do not adhere solely to the SPMD (Single Process, Multiple Data) mode of computation, as encountered in tightly coupled fluid-particulate systems, the flexibility of OpenMP can have a big performance advantage over MPI. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "OpenMP parallelism for fluid and fluid-particulate systems", "paper_id": "WOS:000308120700003"}