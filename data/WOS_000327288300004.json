{"auto_keywords": [{"score": 0.049437714378803975, "phrase": "uncertain_knowledge"}, {"score": 0.00481495049065317, "phrase": "expressive_logic"}, {"score": 0.004782587436296574, "phrase": "automated_reasoning"}, {"score": 0.004546666837896004, "phrase": "completely_satisfactory_integration"}, {"score": 0.0043663006236597975, "phrase": "expressive_languages"}, {"score": 0.004336939968686303, "phrase": "higher-order_logic"}, {"score": 0.004207230913063179, "phrase": "structured_knowledge"}, {"score": 0.0040951809276235495, "phrase": "graded_probabilities"}, {"score": 0.004053932711987671, "phrase": "binary_truth_values"}, {"score": 0.004013098289856805, "phrase": "main_technical_problem"}, {"score": 0.0035900807334877218, "phrase": "natural_wish-list"}, {"score": 0.0034944094552943, "phrase": "probability_distribution"}, {"score": 0.003401278997237025, "phrase": "knowledge_base"}, {"score": 0.0033106223384515546, "phrase": "consistent_inference_procedure"}, {"score": 0.0032115093493071366, "phrase": "deductive_logic"}, {"score": 0.002815093247670994, "phrase": "technical_requirements"}, {"score": 0.0027867024825194834, "phrase": "prior_probability"}, {"score": 0.0026759663480185047, "phrase": "explicit_constructions"}, {"score": 0.002442594378453517, "phrase": "finitely_many_sentences"}, {"score": 0.002426137929553722, "phrase": "suitable_probabilities"}, {"score": 0.0022906498741695094, "phrase": "developed_theory"}, {"score": 0.0022370795358896784, "phrase": "autonomous_reasoning_agents"}, {"score": 0.0021773853011112882, "phrase": "globally_consistent_and_empirically_satisfactory_unification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Higher-order logic", " Probability on sentences", " Gaifman", " Cournot", " Induction", " Confirmation", " Learning", " Prior", " Knowledge", " Entropy"], "paper_abstract": "Automated reasoning about uncertain knowledge has many applications. One difficulty when developing such systems is the lack of a completely satisfactory integration of logic and probability. We address this problem directly. Expressive languages like higher-order logic are ideally suited for representing and reasoning about structured knowledge. Uncertain knowledge can be modeled by using graded probabilities rather than binary truth values. The main technical problem studied in this paper is the following: Given a set of sentences, each having some probability of being true, what probability should be ascribed to other (query) sentences? A natural wish-list, among others, is that the probability distribution (i) is consistent with the knowledge base, (ii) allows for a consistent inference procedure and in particular (iii) reduces to deductive logic in the limit of probabilities being 0 and 1, (iv) allows (Bayesian) inductive reasoning and (v) learning in the limit and in particular (vi) allows confirmation of universally quantified hypotheses/sentences. We translate this wish-list into technical requirements for a prior probability and show that probabilities satisfying all our criteria exist. We also give explicit constructions and several general characterizations of probabilities that satisfy some or all of the criteria and various (counter)examples. We also derive necessary and sufficient conditions for extending beliefs about finitely many sentences to suitable probabilities over all sentences, and in particular least dogmatic or least biased ones. We conclude with a brief outlook on how the developed theory might be used and approximated in autonomous reasoning agents. Our theory is a step towards a globally consistent and empirically satisfactory unification of probability and logic. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Probabilities on Sentences in an Expressive Logic", "paper_id": "WOS:000327288300004"}