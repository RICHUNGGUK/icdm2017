{"auto_keywords": [{"score": 0.027994900841407006, "phrase": "vapnik"}, {"score": 0.00481495049065317, "phrase": "vc_dimensions_of_principal_component_analysis"}, {"score": 0.004645057834614705, "phrase": "statistical_learning"}, {"score": 0.004481132747216135, "phrase": "principal_component_analysis"}, {"score": 0.003677280504320636, "phrase": "k-dimensional_affine_subspace"}, {"score": 0.0034841647157189985, "phrase": "vc_dimension"}, {"score": 0.0032132782979796895, "phrase": "constant_factor"}, {"score": 0.002833035775182696, "phrase": "data_covariance_matrix"}, {"score": 0.0026841368358805407, "phrase": "vc_dimensions"}, {"score": 0.002387758958649797, "phrase": "upper_bound_proof"}, {"score": 0.002282667669149476, "phrase": "simple_proof"}, {"score": 0.0022419349154472806, "phrase": "warren's_bound"}, {"score": 0.002143247921132179, "phrase": "sign_sequences"}, {"score": 0.0021049977753042253, "phrase": "real_polynomials"}], "paper_keywords": ["VC dimensions", " Principal component analysis", " Warren's bound"], "paper_abstract": "Motivated by statistical learning theoretic treatment of principal component analysis, we are concerned with the set of points in a\"e (d) that are within a certain distance from a k-dimensional affine subspace. We prove that the VC dimension of the class of such sets is within a constant factor of (k+1)(d-k+1), and then discuss the distribution of eigenvalues of a data covariance matrix by using our bounds of the VC dimensions and Vapnik's statistical learning theory. In the course of the upper bound proof, we provide a simple proof of Warren's bound of the number of sign sequences of real polynomials.", "paper_title": "VC Dimensions of Principal Component Analysis", "paper_id": "WOS:000280905600006"}