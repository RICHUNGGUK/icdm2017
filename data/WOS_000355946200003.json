{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "large-scale_text_data"}, {"score": 0.004476399615305783, "phrase": "novel_online_algorithm"}, {"score": 0.004263972370406983, "phrase": "average_stochastic_variational_inference"}, {"score": 0.0037301606780397456, "phrase": "previous_iterations"}, {"score": 0.0035530197642369464, "phrase": "noisy_natural_gradients"}, {"score": 0.0033433365160336842, "phrase": "convergence_property"}, {"score": 0.003223494720348768, "phrase": "proposed_algorithm"}, {"score": 0.002685558057339722, "phrase": "experimental_results"}, {"score": 0.002436289099444074, "phrase": "'stochastic_variational_inference"}, {"score": 0.002320449502337321, "phrase": "sgrld"}, {"score": 0.0021569129153653777, "phrase": "faster_convergence_rate"}, {"score": 0.0021049977753042253, "phrase": "better_performance"}], "paper_keywords": ["Latent Dirichlet allocation (LDA)", " Topic modeling", " Online learning", " Moving average"], "paper_abstract": "This paper develops a novel online algorithm, namely moving average stochastic variational inference (MASVI), which applies the results obtained by previous iterations to smooth out noisy natural gradients. We analyze the convergence property of the proposed algorithm and conduct a set of experiments on two large-scale collections that contain millions of documents. Experimental results indicate that in contrast to algorithms named 'stochastic variational inference' and 'SGRLD', our algorithm achieves a faster convergence rate and better performance.", "paper_title": "Topic modeling for large-scale text data", "paper_id": "WOS:000355946200003"}