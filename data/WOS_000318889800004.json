{"auto_keywords": [{"score": 0.048097298277172844, "phrase": "cfknnc"}, {"score": 0.04278379952839438, "phrase": "test_sample"}, {"score": 0.017199801820715982, "phrase": "nearest_neighbors"}, {"score": 0.009355291122023811, "phrase": "training_samples"}, {"score": 0.008362389501918505, "phrase": "cknnc"}, {"score": 0.008130949170543258, "phrase": "\"representation-based_distances"}, {"score": 0.00481495049065317, "phrase": "fine_k_nearest_neighbor"}, {"score": 0.004529881123652277, "phrase": "conventional_knn_classifier"}, {"score": 0.004281678802569817, "phrase": "small_number"}, {"score": 0.0039716833701918365, "phrase": "k_nearest_neighbors"}, {"score": 0.003861288537079326, "phrase": "main_difference"}, {"score": 0.0036327839715330867, "phrase": "euclidean"}, {"score": 0.0031399038722901788, "phrase": "dependent_relationship"}, {"score": 0.003110515172439764, "phrase": "different_training_samples"}, {"score": 0.0029816229984366374, "phrase": "proposed_method"}, {"score": 0.0026384167183586015, "phrase": "cknnc."}, {"score": 0.002613709503583786, "phrase": "experimental_results"}, {"score": 0.0024014759148123736, "phrase": "nearest_feature_line"}, {"score": 0.0023789880034765748, "phrase": "nfl"}, {"score": 0.0022803353468979046, "phrase": "nfs"}, {"score": 0.0022273036096572543, "phrase": "nnlc"}, {"score": 0.002196077907454079, "phrase": "center-based_nearest_neighbor_classifier"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Nearest neighbor classifier", " Classification", " Biometrics", " Face recognition", " Palmprint recognition"], "paper_abstract": "In this paper, we propose a coarse to fine K nearest neighbor (KNN) classifier (CFKNNC). CFKNNC differs from the conventional KNN classifier (CKNNC) as follows: CFKNNC first coarsely determines a small number of training samples that are \"close\" to the test sample and then finely identifies the K nearest neighbors of the test sample. The main difference between CFKNNC and CKNNC is that they exploit the \"representation-based distances\" and Euclidean distances to determine the nearest neighbors of the test sample from the set of training samples, respectively. The analysis shows that the \"representation-based distances\" are able to take into account the dependent relationship between different training samples. Actually, the nearest neighbors determined by the proposed method are optimal from the point of view of representing the test sample. Moreover, the nearest neighbors obtained using our method contain less redundant information than those obtained using CKNNC. The experimental results show that CFKNNC can classify much more accurately than CKNNC and various improvements to CKNNC such as the nearest feature line (NFL) classifier, the nearest feature space (NFS) classifier, nearest neighbor line classifier (NNLC) and center-based nearest neighbor classifier (CBNNC). (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Coarse to fine K nearest neighbor classifier", "paper_id": "WOS:000318889800004"}