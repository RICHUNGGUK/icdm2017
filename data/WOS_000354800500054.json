{"auto_keywords": [{"score": 0.004814958357143643, "phrase": "crowd-sourcing"}, {"score": 0.0045318666714939905, "phrase": "program_synthesis"}, {"score": 0.003948613642680196, "phrase": "good_if_not_perfect_solutions"}, {"score": 0.0039053194919285725, "phrase": "inherently_tricky_programming_tasks"}, {"score": 0.003820144626025562, "phrase": "even_expert_developers"}, {"score": 0.00373682045166829, "phrase": "easy-to-formalize_specification"}, {"score": 0.003478321590910367, "phrase": "crowd-sourcing_imperfect_solutions"}, {"score": 0.003421245054271364, "phrase": "difficult_programming_problem"}, {"score": 0.002996992362027833, "phrase": "crowdboost"}, {"score": 0.0028993944292191433, "phrase": "interesting_and_highly_non-trivial_tasks"}, {"score": 0.002758907973783665, "phrase": "email_addresses"}, {"score": 0.0025537472239953807, "phrase": "crowd-sourced_results"}, {"score": 0.002337848196418384, "phrase": "starting_programs"}, {"score": 0.002261666793030715, "phrase": "consistent_boosts"}, {"score": 0.0021049977753042253, "phrase": "relatively_modest_monetary_cost"}], "paper_keywords": ["Program Synthesis", " Crowd-sourcing", " Symbolic Automata", " Regular Expressions"], "paper_abstract": "In this paper, we investigate an approach to program synthesis that is based on crowd-sourcing. With the help of crowd-sourcing, we aim to capture the \"wisdom of the crowds\" to find good if not perfect solutions to inherently tricky programming tasks, which elude even expert developers and lack an easy-to-formalize specification. We propose an approach we call program boosting, which involves crowd-sourcing imperfect solutions to a difficult programming problem from developers and then blending these programs together in a way that improves their correctness. We implement this approach in a system called CROWDBOOST and show in our experiments that interesting and highly non-trivial tasks such as writing regular expressions for URLs or email addresses can be effectively crowd-sourced. We demonstrate that carefully blending the crowd-sourced results together consistently produces a boost, yielding results that are better than any of the starting programs. Our experiments on 465 program pairs show consistent boosts in accuracy and demonstrate that program boosting can be performed at a relatively modest monetary cost", "paper_title": "Program Boosting: Program Synthesis via Crowd-Sourcing", "paper_id": "WOS:000354800500054"}