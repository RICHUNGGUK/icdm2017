{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gesture_segmentation"}, {"score": 0.01079870609582324, "phrase": "observation_probability"}, {"score": 0.004687742829233727, "phrase": "accumulative_hmms"}, {"score": 0.004652016085325825, "phrase": "existing_gesture_segmentations"}, {"score": 0.004598933498016774, "phrase": "backward_spotting_scheme"}, {"score": 0.004511799033574768, "phrase": "end_point"}, {"score": 0.004392565073466714, "phrase": "start_point"}, {"score": 0.0043258453436258405, "phrase": "extracted_gesture_segment"}, {"score": 0.004276468590041119, "phrase": "hidden_markov_model"}, {"score": 0.004243926308223413, "phrase": "hmm"}, {"score": 0.004195418010131832, "phrase": "gesture_recognition"}, {"score": 0.00411589720697736, "phrase": "inevitable_time_delay"}, {"score": 0.003946195919158241, "phrase": "continuous_gesture_recognition"}, {"score": 0.003797978200741566, "phrase": "forward_spotting_scheme"}, {"score": 0.003669330508576073, "phrase": "start_and_end_points"}, {"score": 0.003585986761045987, "phrase": "zero_crossing"}, {"score": 0.003398780792129312, "phrase": "competitive_differential_observation_probability"}, {"score": 0.0032585499655442404, "phrase": "maximal_gesture"}, {"score": 0.0031481164109237636, "phrase": "sliding_window"}, {"score": 0.002983700956478823, "phrase": "incomplete_feature_extraction"}, {"score": 0.00288255520607591, "phrase": "gesture_recognition_rate"}, {"score": 0.0028278480051602355, "phrase": "accumulated_gesture_segments"}, {"score": 0.0027215202678634517, "phrase": "gesture_type"}, {"score": 0.002690406375639144, "phrase": "majority_vote"}, {"score": 0.0026596472456182707, "phrase": "intermediate_recognition_results"}, {"score": 0.0026091594819333654, "phrase": "predetermined_association_mapping"}, {"score": 0.0025206783724309252, "phrase": "feature_extraction_time"}, {"score": 0.0024633601573822114, "phrase": "proposed_simultaneous_gesture_segmentation"}, {"score": 0.0023981301755351607, "phrase": "upper-body_gestures"}, {"score": 0.002316788811533867, "phrase": "smart_home_environment"}, {"score": 0.0022990901047984197, "phrase": "experimental_results"}, {"score": 0.0022640963589007457, "phrase": "proposed_method"}, {"score": 0.0022382002476510573, "phrase": "good_recognition_rate"}, {"score": 0.0022041312401435346, "phrase": "continuously_changing_gestures"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["gesture segmentation", " gesture recognition", " hidden Markov model", " forward spotting", " accumulative HMM", " competitive differential observation probability", " association mapping", " smart home control"], "paper_abstract": "Existing gesture segmentations use the backward spotting scheme that first detects the end point, then traces back to the start point and sends the extracted gesture segment to the hidden Markov model (HMM) for gesture recognition. This makes an inevitable time delay between the gesture segmentation and recognition and is not appropriate for continuous gesture recognition. To solve this problem, we propose a forward spotting scheme that executes gesture segmentation and recognition simultaneously. The start and end points of gestures are determined by zero crossing from negative to positive (or from positive to negative) of a competitive differential observation probability that is defined by the difference of observation probability between the maximal gesture and the non-gesture. We also propose the sliding window and accumulative HMMs. The former is used to alleviate the effect of incomplete feature extraction on the observation probability and the latter improves the gesture recognition rate greatly by accepting all accumulated gesture segments between the start and end points and deciding the gesture type by a majority vote of all intermediate recognition results. We use the predetermined association mapping to determine the 3D articulation data, which reduces the feature extraction time greatly. We apply the proposed simultaneous gesture segmentation and recognition method to recognize the upper-body gestures for controlling the curtains and lights in a smart home environment. Experimental results show that the proposed method has a good recognition rate of 95.42% for continuously changing gestures. (c) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Simultaneous gesture segmentation and recognition based on forward spotting accumulative HMMs", "paper_id": "WOS:000248468800013"}