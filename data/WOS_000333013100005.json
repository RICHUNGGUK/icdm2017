{"auto_keywords": [{"score": 0.03789642122028191, "phrase": "original_experiment"}, {"score": 0.018913581779633988, "phrase": "bt"}, {"score": 0.010612387000973441, "phrase": "testing_techniques"}, {"score": 0.008581238887314938, "phrase": "ep"}, {"score": 0.006336564009520806, "phrase": "inscope_faults"}, {"score": 0.004737364301132865, "phrase": "verification_and_validation_activity"}, {"score": 0.004704495156558224, "phrase": "fundamental_role"}, {"score": 0.00467185299669322, "phrase": "software_quality"}, {"score": 0.004491093879657087, "phrase": "experimental_software_engineering_researchers"}, {"score": 0.004408456735066782, "phrase": "controlled_experiment"}, {"score": 0.00421821065416333, "phrase": "control-flow_structural_testing_technique"}, {"score": 0.004083281377007922, "phrase": "literal_replication"}, {"score": 0.004064359912083564, "phrase": "juristo_et_al"}, {"score": 0.0037383254707271905, "phrase": "outscope"}, {"score": 0.003422411319890915, "phrase": "studied_techniques"}, {"score": 0.00332828739329299, "phrase": "experimental_groups"}, {"score": 0.0032974896408709377, "phrase": "stratified_randomization"}, {"score": 0.003251824787506553, "phrase": "programming_experience"}, {"score": 0.003199345207727474, "phrase": "experimental_materials"}, {"score": 0.0031477098951101005, "phrase": "training_duration"}, {"score": 0.0030897146022309517, "phrase": "escuela_polit"}, {"score": 0.0030398432591176357, "phrase": "sede_latacunga"}, {"score": 0.0029838294169759663, "phrase": "software_verification"}, {"score": 0.0029699874949715367, "phrase": "validation_course"}, {"score": 0.0029493445611623106, "phrase": "experimental_subjects"}, {"score": 0.002815333717108928, "phrase": "significant_effects"}, {"score": 0.0027506229601034576, "phrase": "outscope_faults"}, {"score": 0.0027188259793109264, "phrase": "group_variables"}, {"score": 0.002529706064459843, "phrase": "group_factor"}, {"score": 0.002488851299416018, "phrase": "small_sample_effects"}, {"score": 0.0023537101705489957, "phrase": "fatigue_effect"}, {"score": 0.002337340638007271, "phrase": "technique_x_program_interaction"}, {"score": 0.0022889103976458437, "phrase": "main_effects"}], "paper_keywords": ["Replication", " Experiment", " Unit testing", " Reporting guidelines"], "paper_abstract": "The verification and validation activity plays a fundamental role in improving software quality. Determining which the most effective techniques for carrying out this activity are has been an aspiration of experimental software engineering researchers for years. This paper reports a controlled experiment evaluating the effectiveness of two unit testing techniques (the functional testing technique known as equivalence partitioning (EP) and the control-flow structural testing technique known as branch testing (BT)). This experiment is a literal replication of Juristo et al. (2013). Both experiments serve the purpose of determining whether the effectiveness of BT and EP varies depending on whether or not the faults are visible for the technique (InScope or OutScope, respectively). We have used the materials, design and procedures of the original experiment, but in order to adapt the experiment to the context we have: (1) reduced the number of studied techniques from 3 to 2; (2) assigned subjects to experimental groups by means of stratified randomization to balance the influence of programming experience; (3) localized the experimental materials and (4) adapted the training duration. We ran the replication at the Escuela Polit,cnica del Ej,rcito Sede Latacunga (ESPEL) as part of a software verification & validation course. The experimental subjects were 23 master's degree students. EP is more effective than BT at detecting InScope faults. The session/program and group variables are found to have significant effects. BT is more effective than EP at detecting OutScope faults. The session/program and group variables have no effect in this case. The results of the replication and the original experiment are similar with respect to testing techniques. There are some inconsistencies with respect to the group factor. They can be explained by small sample effects. The results for the session/program factor are inconsistent for InScope faults. We believe that these differences are due to a combination of the fatigue effect and a technique x program interaction. Although we were able to reproduce the main effects, the changes to the design of the original experiment make it impossible to identify the causes of the discrepancies for sure. We believe that further replications closely resembling the original experiment should be conducted to improve our understanding of the phenomena under study.", "paper_title": "Effectiveness for detecting faults within and outside the scope of testing techniques: an independent replication", "paper_id": "WOS:000333013100005"}