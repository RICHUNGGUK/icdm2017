{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "fp-hadoop"}, {"score": 0.006097124341385576, "phrase": "intermediate_values"}, {"score": 0.004721162131188487, "phrase": "parallel_jobs_over_skewed_data"}, {"score": 0.004684157900507474, "phrase": "big_data_parallel_frameworks"}, {"score": 0.004611040654812415, "phrase": "mapreduce"}, {"score": 0.004574879613132892, "phrase": "spark"}, {"score": 0.00438105507792004, "phrase": "poor_performance"}, {"score": 0.0042119653427658025, "phrase": "important_cases"}, {"score": 0.0041625173012740544, "phrase": "high_percentage"}, {"score": 0.004081385289516914, "phrase": "reduce_side"}, {"score": 0.0036553070575015344, "phrase": "mapreduce_jobs"}, {"score": 0.003541923233726252, "phrase": "new_phase"}, {"score": 0.003260690513595527, "phrase": "intermediate_reduce_workers"}, {"score": 0.0031719830393831115, "phrase": "scheduling_strategy"}, {"score": 0.0031223774204093713, "phrase": "ir_phase"}, {"score": 0.0029899137403982027, "phrase": "main_part"}, {"score": 0.0029547698888028697, "phrase": "reducing_work"}, {"score": 0.0028630535573013686, "phrase": "computing_resources"}, {"score": 0.0028293966058473476, "phrase": "available_workers"}, {"score": 0.0027093285574752457, "phrase": "extensive_experiments"}, {"score": 0.002688050388891007, "phrase": "synthetic_and_real_datasets"}, {"score": 0.0026459927476753585, "phrase": "excellent_performance_gains"}, {"score": 0.0026148807877309417, "phrase": "native_hadoop"}, {"score": 0.0025436978068459565, "phrase": "reduce_time"}, {"score": 0.002494039334114861, "phrase": "total_execution_time"}, {"score": 0.0023507916951658455, "phrase": "job_executions"}, {"score": 0.0022688141624966967, "phrase": "general_information"}, {"score": 0.0021049977753042253, "phrase": "different_configurations"}], "paper_keywords": [""], "paper_abstract": "Big data parallel frameworks, such as MapReduce or Spark have been praised for their high scalability and performance, but show poor performance in the case of data skew. There are important cases where a high percentage of processing in the reduce side ends up being done by only one node. In this demonstration, we illustrate the use of FP-Hadoop, a system that efficiently deals with data skew in MapReduce jobs. In FP-Hadoop, there is a new phase, called intermediate reduce (IR), in which blocks of intermediate values, constructed dynamically, are processed by intermediate reduce workers in parallel, by using a scheduling strategy. Within the IR phase, even if all intermediate values belong to only one key, the main part of the reducing work can be done in parallel using the computing resources of all available workers. We implemented a prototype of FP-Hadoop, and conducted extensive experiments over synthetic and real datasets. We achieve excellent performance gains compared to native Hadoop, e.g. more than 10 times in reduce time and 5 times in total execution time. During our demonstration, we give the users the possibility to execute and compare job executions in FP-Hadoop and Hadoop. They can retrieve general information about the job and the tasks and a summary of the phases. They can also visually compare different configurations to explore the difference between the approaches.", "paper_title": "FP-Hadoop: Efficient Execution of Parallel Jobs Over Skewed Data", "paper_id": "WOS:000386424800050"}