{"auto_keywords": [{"score": 0.05007753572495548, "phrase": "proper_validation"}, {"score": 0.034337812857620324, "phrase": "ppv"}, {"score": 0.004644629973090153, "phrase": "exemplar_context"}, {"score": 0.00459568664065473, "phrase": "phenomenological_analysis"}, {"score": 0.004576253356889219, "phrase": "common_validation_practices"}, {"score": 0.004556901872357751, "phrase": "data_analysis"}, {"score": 0.004461359547152617, "phrase": "generic_principles"}, {"score": 0.004133627071344356, "phrase": "full_knowledge"}, {"score": 0.0041074234667687875, "phrase": "underlying_definitions"}, {"score": 0.003945266418163835, "phrase": "proper_validation_objectives"}, {"score": 0.00378145978325955, "phrase": "validation_myths"}, {"score": 0.0036786370068876023, "phrase": "monolithic_application"}, {"score": 0.0036553070575015344, "phrase": "one-for-all_procedure"}, {"score": 0.003586198032407676, "phrase": "data_sets"}, {"score": 0.0034960737684745454, "phrase": "sub-optimal_simulation"}, {"score": 0.0034812740368914455, "phrase": "test_set_validation"}, {"score": 0.0034445472801689046, "phrase": "critical_sampling_variance_omission"}, {"score": 0.0030457198309616694, "phrase": "key_element"}, {"score": 0.0029377980304705425, "phrase": "variance_generating_factors"}, {"score": 0.0028037753646260937, "phrase": "fatal_inconstant_sampling_bias"}, {"score": 0.0027741761880779535, "phrase": "statistical_correction"}, {"score": 0.0026140878612307536, "phrase": "sampling_errors"}, {"score": 0.0025919830205975215, "phrase": "'future'_situations"}, {"score": 0.002570064617803693, "phrase": "validated_model"}, {"score": 0.0024947959375982614, "phrase": "sampling_approaches"}, {"score": 0.002360796860462937, "phrase": "full_scientific_understanding"}, {"score": 0.0023013968622710847, "phrase": "emphatic_call"}, {"score": 0.002281930100792185, "phrase": "stringent_commitment"}, {"score": 0.0022674379203603224, "phrase": "set_validation"}, {"score": 0.0022530375696567136, "phrase": "graphical_inspection"}, {"score": 0.0022434880661725493, "phrase": "pertinent_t-u_plots"}, {"score": 0.002215081186105616, "phrase": "x-y_interrelationships"}, {"score": 0.0022010126160835024, "phrase": "validation_guidance"}, {"score": 0.002177762848946681, "phrase": "partial_exemption"}, {"score": 0.002163930783583212, "phrase": "present_test"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["Principles of Proper Validation (PPV)", " future performance assessment", " test set validation", " cross-validation", " re-sampling", " predictive regression", " Theory of Sampling (TOS)"], "paper_abstract": "Validation in chemometrics is presented using the exemplar context of multivariate calibration/prediction. A phenomenological analysis of common validation practices in data analysis and chemometrics leads to formulation of a set of generic Principles of Proper Validation (PPV), which is based on a set of characterizing distinctions: (i) Validation cannot be understood by focusing on the methods of validation only; validation must be based on full knowledge of the underlying definitions, objectives, methods, effects and consequences which are all outlined and discussed here. (ii) Analysis of proper validation objectives implies that there is one valid paradigm only: test set validation. (iii) Contrary to much contemporary chemometric practices (and validation myths), cross-validation is shown to be unjustified in the form of monolithic application of a one-for-all procedure (segmented cross-validation) on all data sets. Within its own design and scope, cross-validation is in reality a sub-optimal simulation of test set validation, crippled by a critical sampling variance omission, as it manifestly is based on one data set only (training data set). Other re-sampling validation methods are shown to suffer from the same deficiencies. The PPV are universal and can be applied to all situations in which the assessment of performance is desired: prediction-, classification-, time series forecasting-, modeling validation. The key element of PPV is the Theory of Sampling (TOS), which allow insight into all variance generating factors, especially the so-called incorrect sampling errors, which, if not properly eliminated, are responsible for a fatal inconstant sampling bias, for which no statistical correction is possible. In the light of TOS it is shown how a second data set (test set, validation set) is critically necessary for the inclusion of the sampling errors incurred in all 'future' situations in which the validated model must perform. Logically, therefore, all one data set re-sampling approaches for validation, especially cross-validation and leverage-corrected validation, should be terminated, or at the very least used only with full scientific understanding and disclosure of their detrimental variance omissions and consequences. Regarding PLS-regression, an emphatic call is made for stringent commitment to test set validation based on graphical inspection of pertinent t-u plots for optimal understanding of the X-Y interrelationships and for validation guidance. OSAR/QSAP forms a partial exemption from the present test set imperative with no generalization potential. Copyright (C) 2010 John Wiley & Sons, Ltd.", "paper_title": "Principles of Proper Validation: use and abuse of re-sampling for validation", "paper_id": "WOS:000277343000009"}