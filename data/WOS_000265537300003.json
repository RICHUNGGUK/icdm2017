{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "case-based_explanation"}, {"score": 0.04399383761203485, "phrase": "case-based_explanations"}, {"score": 0.004768649568826951, "phrase": "traditional_explanation_strategies"}, {"score": 0.004722791767189843, "phrase": "machine_learning"}, {"score": 0.004610058444728656, "phrase": "rule_and_decision_tree_based_approaches"}, {"score": 0.004435230698216773, "phrase": "alternative_approach"}, {"score": 0.0043713857434347254, "phrase": "inherent_advantages"}, {"score": 0.004246428036409373, "phrase": "user_acceptability"}, {"score": 0.004046020290832207, "phrase": "similar_past_examples"}, {"score": 0.0038179283509963695, "phrase": "traditional_approach"}, {"score": 0.0036730297909962142, "phrase": "nearest_neighbour"}, {"score": 0.0033182152378613767, "phrase": "useful_explanations"}, {"score": 0.0031922210879462513, "phrase": "explanation_case"}, {"score": 0.0030858947334838145, "phrase": "end_user"}, {"score": 0.002997572637726648, "phrase": "domain_knowledge"}, {"score": 0.0027741761880779535, "phrase": "knowledge-light_approach"}, {"score": 0.0026430660986106955, "phrase": "explanation_utility"}, {"score": 0.002542642270762272, "phrase": "feature-value_differences"}, {"score": 0.0023303871145705954, "phrase": "explanation_oriented_retrieval"}, {"score": 0.0021670777282518424, "phrase": "knowledge-light_explanation_framework"}, {"score": 0.0021049977753042253, "phrase": "local_logistic_regression"}], "paper_keywords": ["Case-based explanation"], "paper_abstract": "Traditional explanation strategies in machine learning have been dominated by rule and decision tree based approaches. Case-based explanations represent an alternative approach which has inherent advantages in terms of transparency and user acceptability. Case-based explanations are based on a strategy of presenting similar past examples in support of and as justification for recommendations made. The traditional approach to such explanations, of simply supplying the nearest neighbour as an explanation, has been found to have shortcomings. Cases should be selected based on their utility in forming useful explanations. However, the relevance of the explanation case may not be clear to the end user as it is retrieved using domain knowledge which they themselves may not have. In this paper the focus is on a knowledge-light approach to case-based explanations that works by selecting cases based on explanation utility and offering insights into the effects of feature-value differences. In this paper we examine to two such a knowledge-light frameworks for case-based explanation. We look at explanation oriented retrieval (EOR) a strategy which explicitly models explanation utility and also at the knowledge-light explanation framework (KLEF) that uses local logistic regression to support case-based explanation.", "paper_title": "Gaining insight through case-based explanation", "paper_id": "WOS:000265537300003"}