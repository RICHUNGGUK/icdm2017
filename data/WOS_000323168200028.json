{"auto_keywords": [{"score": 0.050071049385739454, "phrase": "supervised_word_sense_disambiguation"}, {"score": 0.048421161238975864, "phrase": "active_learning_strategies"}, {"score": 0.045614719753412414, "phrase": "annotated_samples"}, {"score": 0.04410485411803989, "phrase": "disambiguation_models"}, {"score": 0.036965051388441375, "phrase": "learning_curve"}, {"score": 0.030366548063439067, "phrase": "pl"}, {"score": 0.004777569299319633, "phrase": "medline._objectives"}, {"score": 0.004523882351605302, "phrase": "wsd"}, {"score": 0.004135859558144883, "phrase": "support_vector_machine"}, {"score": 0.003946766004066276, "phrase": "msh_wsd_collection"}, {"score": 0.0038407045475615607, "phrase": "svm"}, {"score": 0.00373701482510911, "phrase": "passive_learner"}, {"score": 0.0036505565334886227, "phrase": "random_sampling"}, {"score": 0.0035940277928341265, "phrase": "ambiguous_term"}, {"score": 0.003552204377093386, "phrase": "learning_algorithm"}, {"score": 0.0031108036342147624, "phrase": "alc"}, {"score": 0.0030387767523076528, "phrase": "primary_metric"}, {"score": 0.0029453365339797933, "phrase": "active_learners"}, {"score": 0.0028325548627901004, "phrase": "better_performance"}, {"score": 0.0027134630181926854, "phrase": "average_accuracy"}, {"score": 0.002519402443228795, "phrase": "annotation_effort"}, {"score": 0.0024418934977061876, "phrase": "active_learning_algorithms"}, {"score": 0.0024040357029838774, "phrase": "superior_performance"}, {"score": 0.0023300677028248776, "phrase": "early_learning_stage"}, {"score": 0.002223351523626989, "phrase": "future_improvements"}, {"score": 0.0021381565332182773, "phrase": "supervised_wsd_methods"}, {"score": 0.0021049977753042253, "phrase": "annotation_cost"}], "paper_keywords": ["Active Learning", " Word Sense Disambiguation", " Natural Language Processing", " Machine Learning", " Uncertainty Sampling", " Annotation"], "paper_abstract": "Objectives This study was to assess whether active learning strategies can be integrated with supervised word sense disambiguation (WSD) methods, thus reducing the number of annotated samples, while keeping or improving the quality of disambiguation models. Methods We developed support vector machine (SVM) classifiers to disambiguate 197 ambiguous terms and abbreviations in the MSH WSD collection. Three different uncertainty sampling-based active learning algorithms were implemented with the SVM classifiers and were compared with a passive learner (PL) based on random sampling. For each ambiguous term and each learning algorithm, a learning curve that plots the accuracy computed from the test set as a function of the number of annotated samples used in the model was generated. The area under the learning curve (ALC) was used as the primary metric for evaluation. Results Our experiments demonstrated that active learners (ALs) significantly outperformed the PL, showing better performance for 177 out of 197 (89.8%) WSD tasks. Further analysis showed that to achieve an average accuracy of 90%, the PL needed 38 annotated samples, while the ALs needed only 24, a 37% reduction in annotation effort. Moreover, we analyzed cases where active learning algorithms did not achieve superior performance and identified three causes: (1) poor models in the early learning stage; (2) easy WSD cases; and (3) difficult WSD cases, which provide useful insight for future improvements. Conclusions This study demonstrated that integrating active learning strategies with supervised WSD methods could effectively reduce annotation cost and improve the disambiguation models.", "paper_title": "Applying active learning to supervised word sense disambiguation in MEDLINE", "paper_id": "WOS:000323168200028"}