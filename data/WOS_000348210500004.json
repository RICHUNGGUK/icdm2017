{"auto_keywords": [{"score": 0.02981798630874746, "phrase": "noise_variance"}, {"score": 0.00481495049065317, "phrase": "adaptive_particle_filtering"}, {"score": 0.004707672504464783, "phrase": "multiple_moving_speakers"}, {"score": 0.004672444473700524, "phrase": "indoor_environments"}, {"score": 0.004466519142879166, "phrase": "single_modality"}, {"score": 0.004285702427563952, "phrase": "multi-modal_information"}, {"score": 0.004143196873246147, "phrase": "tracking_performance"}, {"score": 0.003990385737207714, "phrase": "challenging_situations"}, {"score": 0.003901407184706592, "phrase": "limited_field"}, {"score": 0.0037153426267756452, "phrase": "fusion_algorithms"}, {"score": 0.0036188435232325337, "phrase": "sensor_measurements"}, {"score": 0.003578255090799811, "phrase": "non-negligible_detection_errors"}, {"score": 0.003511613254403125, "phrase": "novel_approach"}, {"score": 0.0034722233102219466, "phrase": "audio_and_visual_data"}, {"score": 0.003356675879109443, "phrase": "arrival_angles"}, {"score": 0.0033190180867511605, "phrase": "audio_sources"}, {"score": 0.0032694619052789768, "phrase": "typical_gaussian_noise_distribution"}, {"score": 0.0032085525502105836, "phrase": "propagation_step"}, {"score": 0.003148774338287351, "phrase": "observation_model"}, {"score": 0.003113441660343867, "phrase": "measurement_step"}, {"score": 0.0029984960344766705, "phrase": "typical_problem"}, {"score": 0.0029537606017516027, "phrase": "pf"}, {"score": 0.0027811441123794427, "phrase": "state_estimation"}, {"score": 0.0027602925235474317, "phrase": "particle_propagation"}, {"score": 0.0026384167183586015, "phrase": "regular_pf_implementation"}, {"score": 0.0021775526153883355, "phrase": "baseline_pf_method"}, {"score": 0.0021530945641993152, "phrase": "existing_adaptive_pf_algorithm"}, {"score": 0.0021289106372625252, "phrase": "occluded_speakers"}, {"score": 0.0021049977753042253, "phrase": "significantly_reduced_number"}], "paper_keywords": ["Adaptive particle filter", " audio-visual speaker tracking", " particle filter"], "paper_abstract": "The problem of tracking multiple moving speakers in indoor environments has received much attention. Earlier techniques were based purely on a single modality, e.g., vision. Recently, the fusion of multi-modal information has been shown to be instrumental in improving tracking performance, as well as robustness in the case of challenging situations like occlusions (by the limited field of view of cameras or by other speakers). However, data fusion algorithms often suffer from noise corrupting the sensor measurements which cause non-negligible detection errors. Here, a novel approach to combining audio and visual data is proposed. We employ the direction of arrival angles of the audio sources to reshape the typical Gaussian noise distribution of particles in the propagation step and to weight the observation model in the measurement step. This approach is further improved by solving a typical problem associated with the PF, whose efficiency and accuracy usually depend on the number of particles and noise variance used in state estimation and particle propagation. Both parameters are specified beforehand and kept fixed in the regular PF implementation which makes the tracker unstable in practice. To address these problems, we design an algorithm which adapts both the number of particles and noise variance based on tracking error and the area occupied by the particles in the image. Experiments on the AV16.3 dataset show the advantage of our proposed methods over the baseline PF method and an existing adaptive PF algorithm for tracking occluded speakers with a significantly reduced number of particles.", "paper_title": "Audio Assisted Robust Visual Tracking With Adaptive Particle Filtering", "paper_id": "WOS:000348210500004"}