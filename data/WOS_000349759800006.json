{"auto_keywords": [{"score": 0.024921618130673173, "phrase": "overall_accuracy"}, {"score": 0.00481495049065317, "phrase": "conditional_random_fields"}, {"score": 0.0047787596516662, "phrase": "first-person_activity_recognition"}, {"score": 0.004707188147442449, "phrase": "disabled_patients"}, {"score": 0.0046192222897398685, "phrase": "novel_pervasive_system"}, {"score": 0.004567230180529598, "phrase": "human_daily_activities"}, {"score": 0.0045158206186680224, "phrase": "wearable_device"}, {"score": 0.004124754315882517, "phrase": "first-person_view_camera"}, {"score": 0.0040171817997972335, "phrase": "subject's_activities"}, {"score": 0.0039869632444468036, "phrase": "daily_living"}, {"score": 0.003853757393357659, "phrase": "ego-activity_recognition_system"}, {"score": 0.0037249853525936428, "phrase": "specific_person"}, {"score": 0.0036691380448651443, "phrase": "disabled_patient"}, {"score": 0.003641528016135388, "phrase": "elderly_people"}, {"score": 0.003480161521018509, "phrase": "cognitive_impairments"}, {"score": 0.0034539685987677376, "phrase": "hazardous_situations"}, {"score": 0.003049086085585051, "phrase": "small_motions"}, {"score": 0.002913895605074143, "phrase": "structured_classification"}, {"score": 0.0026915366885986586, "phrase": "elder_adls_datasets"}, {"score": 0.0026511422582578027, "phrase": "patient_adls_datasets"}, {"score": 0.002514464610980629, "phrase": "system_performance"}, {"score": 0.0024487847651989128, "phrase": "conventional_classification_approaches"}, {"score": 0.0022110892229209407, "phrase": "different_disabilities"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Activity recognition", " Feature classification", " Graphical model", " First-person", " Feature extraction", " Computer vision"], "paper_abstract": "We propose a novel pervasive system to recognise human daily activities from a wearable device. The system is designed in a form of reading glasses, named 'Smart Glasses', integrating a 3-axis accelerometer and a first-person view camera. Our aim is to classify subject's activities of daily living (ADLs) based on their vision and head motion data. This ego-activity recognition system not only allows caretakers to track on a specific person (such as disabled patient or elderly people), but also has the potential to remind/warn people with cognitive impairments of hazardous situations. We present the following contributions: a feature extraction method from accelerometer and video; a classification algorithm integrating both locomotive (body motions) and stationary activities (without or with small motions); a novel multi-scale dynamic graphical model for structured classification over time. In this paper, we collect, train and validate our system on two large datasets: 20 h of elder ADLs datasets and 40 h of patient ADLs datasets, containing 12 and 14 different activities separately. The results show that our method efficiently improves the system performance (F-Measure) over conventional classification approaches by an average of 20%-40% up to 84.45%, with an overall accuracy of 90.04% for elders. Furthermore, we also validate our method on 30 patients with different disabilities, achieving an overall accuracy up to 77.07%. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Multi-scale Conditional Random Fields for first-person activity recognition on elders and disabled patients", "paper_id": "WOS:000349759800006"}