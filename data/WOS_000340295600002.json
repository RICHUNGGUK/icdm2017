{"auto_keywords": [{"score": 0.040691162217611467, "phrase": "music_information"}, {"score": 0.0344514253688494, "phrase": "audio_codewords"}, {"score": 0.00481495049065317, "phrase": "systematic_evaluation_of"}, {"score": 0.00462919213721188, "phrase": "music_information_retrieval"}, {"score": 0.004494570360108983, "phrase": "increasing_attention"}, {"score": 0.004428728205625129, "phrase": "feature_representations"}, {"score": 0.0041136473765874815, "phrase": "unsupervised_feature_learning_techniques"}, {"score": 0.0040334640350288, "phrase": "sparse_coding"}, {"score": 0.003993958025223994, "phrase": "deep_belief_networks"}, {"score": 0.003820896867623235, "phrase": "term-document_structure"}, {"score": 0.00376488621224566, "phrase": "elementary_audio_codewords"}, {"score": 0.0036914757355166966, "phrase": "widespread_use"}, {"score": 0.003479692058963528, "phrase": "different_component_settings"}, {"score": 0.0033288399506044763, "phrase": "text_retrieval_community"}, {"score": 0.0031070348334684356, "phrase": "bof_model"}, {"score": 0.002986969236641105, "phrase": "comprehensive_evaluation"}, {"score": 0.002928682556953258, "phrase": "large_number"}, {"score": 0.002899965889623787, "phrase": "bof_variants"}, {"score": 0.0028154895878702633, "phrase": "different_ways"}, {"score": 0.0027878797103973313, "phrase": "low-level_feature_representation"}, {"score": 0.0027200305495826797, "phrase": "codeword_assignment"}, {"score": 0.00266693888510356, "phrase": "song-level_feature_pooling"}, {"score": 0.002640781900605126, "phrase": "tf-idf_term_weighting"}, {"score": 0.0025638362414466278, "phrase": "dimension_reduction"}, {"score": 0.002323142666222693, "phrase": "difficult_tasks"}, {"score": 0.0022890371899188466, "phrase": "predominant_instrument_recognition"}, {"score": 0.0022113879114507577, "phrase": "system_performance"}, {"score": 0.0021363670305843403, "phrase": "modeling_methods"}, {"score": 0.0021049977753042253, "phrase": "latent_dirichlet_allocation"}], "paper_keywords": ["Bag-of-frames model", " music information retrieval", " sparse coding", " unsupervised feature learning"], "paper_abstract": "There has been an increasing attention on learning feature representations from the complex, high-dimensional audio data applied in various music information retrieval (MIR) problems. Unsupervised feature learning techniques, such as sparse coding and deep belief networks have been utilized to represent music information as a term-document structure comprising of elementary audio codewords. Despite the widespread use of such bag-of-frames (BoF) model, few attempts have been made to systematically compare different component settings. Moreover, whether techniques developed in the text retrieval community are applicable to audio codewords is poorly understood. To further our understanding of the BoF model, we present in this paper a comprehensive evaluation that compares a large number of BoF variants on three different MIR tasks, by considering different ways of low-level feature representation, codebook construction, codeword assignment, segment-level and song-level feature pooling, tf-idf term weighting, power normalization, and dimension reduction. Our evaluations lead to the following findings: 1) modeling music information by two levels of abstraction improves the result for difficult tasks such as predominant instrument recognition, 2) tf-idf weighting and power normalization improve system performance in general, 3) topic modeling methods such as latent Dirichlet allocation does not work for audio codewords.", "paper_title": "A Systematic Evaluation of the Bag-of-Frames Representation for Music Information Retrieval", "paper_id": "WOS:000340295600002"}