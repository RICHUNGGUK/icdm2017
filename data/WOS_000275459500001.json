{"auto_keywords": [{"score": 0.05007827403782804, "phrase": "different_types"}, {"score": 0.04871069003649857, "phrase": "learning_techniques"}, {"score": 0.036752610332553164, "phrase": "smo"}, {"score": 0.004718230196581402, "phrase": "supervised_learning_techniques"}, {"score": 0.004596711717456095, "phrase": "noisy_data"}, {"score": 0.004478308876320956, "phrase": "resulting_data_models"}, {"score": 0.004350308086118336, "phrase": "key_aspect"}, {"score": 0.004325148424301224, "phrase": "supervised_learning"}, {"score": 0.004287680323820891, "phrase": "reliable_models"}, {"score": 0.0041170581247889654, "phrase": "particular_learners"}, {"score": 0.004046020290832207, "phrase": "different_learners"}, {"score": 0.0038624981944755813, "phrase": "different_degrees"}, {"score": 0.003773870856027139, "phrase": "different_paradigms"}, {"score": 0.003350480663660326, "phrase": "different_learning_paradigms"}, {"score": 0.0032451751960451214, "phrase": "top_ten_algorithms"}, {"score": 0.0032263862712733934, "phrase": "data_mining"}, {"score": 0.0030799267141887058, "phrase": "data_sets"}, {"score": 0.0030092017034068666, "phrase": "input_attributes"}, {"score": 0.0029658232220207254, "phrase": "output_class"}, {"score": 0.002931569771634119, "phrase": "initial_hypothesis"}, {"score": 0.0027341763737751467, "phrase": "first_group"}, {"score": 0.002640507929183363, "phrase": "key_observations"}, {"score": 0.0023168823213024856, "phrase": "underlying_empirical_behavior"}, {"score": 0.002230984431056064, "phrase": "noise_type"}, {"score": 0.0022116174573590434, "phrase": "specific_data"}, {"score": 0.002148264328397543, "phrase": "training_data"}], "paper_keywords": ["Attribute noise", " Class noise", " Machine learning techniques", " Noise impacts"], "paper_abstract": "Machine learning techniques often have to deal with noisy data, which may affect the accuracy of the resulting data models. Therefore, effectively dealing with noise is a key aspect in supervised learning to obtain reliable models from data. Although several authors have studied the effect of noise for some particular learners, comparisons of its effect among different learners are lacking. In this paper, we address this issue by systematically comparing how different degrees of noise affect four supervised learners that belong to different paradigms. Specifically, we consider the Na < ve Bayes probabilistic classifier, the C4.5 decision tree, the IBk instance-based learner and the SMO support vector machine. We have selected four methods which enable us to contrast different learning paradigms, and which are considered to be four of the top ten algorithms in data mining (Yu et al. 2007). We test them on a collection of data sets that are perturbed with noise in the input attributes and noise in the output class. As an initial hypothesis, we assign the techniques to two groups, NB with C4.5 and IBk with SMO, based on their proposed sensitivity to noise, the first group being the least sensitive. The analysis enables us to extract key observations about the effect of different types and degrees of noise on these learning techniques. In general, we find that Na < ve Bayes appears as the most robust algorithm, and SMO the least, relative to the other two techniques. However, we find that the underlying empirical behavior of the techniques is more complex, and varies depending on the noise type and the specific data set being processed. In general, noise in the training data set is found to give the most difficulty to the learners.", "paper_title": "A study of the effect of different types of noise on the precision of supervised learning techniques", "paper_id": "WOS:000275459500001"}