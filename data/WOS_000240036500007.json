{"auto_keywords": [{"score": 0.04613130232163521, "phrase": "hardware_implementation"}, {"score": 0.00481495049065317, "phrase": "optimal_implementation"}, {"score": 0.004324985524216987, "phrase": "partially_connected_neural_network"}, {"score": 0.0037213333316218522, "phrase": "partially_connected_neural_networks"}, {"score": 0.0035648266669315943, "phrase": "higher_performance"}, {"score": 0.002937879003671346, "phrase": "complete_study"}, {"score": 0.00266693888510356, "phrase": "partially_connected_version"}, {"score": 0.0023950275815082297, "phrase": "hardware_resources"}, {"score": 0.0023440589015485077, "phrase": "performance_cost"}, {"score": 0.002174041708084857, "phrase": "different_memory_management_strategies"}, {"score": 0.0021049977753042253, "phrase": "connectivity_patterns"}], "paper_keywords": [""], "paper_abstract": "We present the hardware implementation of partially connected neural network that is defined as an extended of the Multi-Layer Perceptron (MLP) model. We demonstrate that partially connected neural networks lead to a higher performance in terms of computing speed (requiring less memory and computing resources). This work addresses a complete study that compares the hardware implementation of MLP and a partially connected version (XMLP) in terms of computing speed, hardware resources and performance cost. Furthermore, we study also different memory management strategies for the connectivity patterns.", "paper_title": "Towards an optimal implementation of MLP in FPGA", "paper_id": "WOS:000240036500007"}