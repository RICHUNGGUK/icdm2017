{"auto_keywords": [{"score": 0.04900761970662832, "phrase": "finite_beta_mixtures"}, {"score": 0.004816265469549797, "phrase": "bayesian"}, {"score": 0.00474283953918251, "phrase": "reversible_jump_mcmc"}, {"score": 0.004584495834677719, "phrase": "mixture_models"}, {"score": 0.0045158206186680224, "phrase": "signal_processing"}, {"score": 0.004414723336530117, "phrase": "considerable_interest"}, {"score": 0.004332199142914195, "phrase": "theoretical_development"}, {"score": 0.004156012879026324, "phrase": "mixture_estimation_and_selection_problem"}, {"score": 0.004109212945115815, "phrase": "model_complex_datasets"}, {"score": 0.004062937864702115, "phrase": "different_techniques"}, {"score": 0.0038683363637006902, "phrase": "full_bayesian_approaches"}, {"score": 0.0036969566358089644, "phrase": "bayesian_learning"}, {"score": 0.003627800727103479, "phrase": "prior_knowledge"}, {"score": 0.0035869274480340727, "phrase": "formal_coherent_way"}, {"score": 0.003546513036085932, "phrase": "overfitting_problems"}, {"score": 0.0034279721355412285, "phrase": "fully_bayesian_approach"}, {"score": 0.0031665087019813244, "phrase": "optimal_number"}, {"score": 0.002870173628745123, "phrase": "missing_values"}, {"score": 0.002827105825163379, "phrase": "random_variables"}, {"score": 0.0027119634978446895, "phrase": "rjmcmc."}, {"score": 0.002631172352670227, "phrase": "beta_mixtures"}, {"score": 0.002572158343938064, "phrase": "unknown_distributional_shape"}, {"score": 0.002495521662819317, "phrase": "useful_class"}, {"score": 0.002476721068743341, "phrase": "flexible_models"}, {"score": 0.0023758152068732025, "phrase": "well-known_marked_deviation"}, {"score": 0.0023507641780170984, "phrase": "gaussian"}, {"score": 0.0022876463319883634, "phrase": "proposed_approach"}, {"score": 0.002253299769886694, "phrase": "synthetic_mixture_data"}, {"score": 0.0022363199197710385, "phrase": "real_data"}, {"score": 0.0021944267528581094, "phrase": "interesting_application"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Beta distribution", " Mixture modeling", " Bayesian analysis", " MCMC", " Reversible jump", " Gibbs sampling", " Metropolis-Hastings", " Texture classification", " Retrieval"], "paper_abstract": "The use of mixture models in image and signal processing has proved to be of considerable interest in terms of both theoretical development and in their usefulness in several applications. Researchers have approached the mixture estimation and selection problem, to model complex datasets, with different techniques in the last few years. In theory, it is well-known that full Bayesian approaches, to handle this problem, are fully optimal. The Bayesian learning allows the incorporation of prior knowledge in a formal coherent way that avoids overfitting problems. In this paper, we propose a fully Bayesian approach for finite Beta mixtures learning using a reversible jump Markov chain Monte Carlo (RJMCMC) technique which simultaneously allows cluster assignments, parameters estimation, and the selection of the optimal number of clusters. The adverb \"fully\" is justified by the fact that all parameters of interest in our model including number of clusters and missing values are considered as random variables for which priors are specified and posteriors are approximated using RJMCMC. Our work is motivated by the fact that Beta mixtures are able to fit any unknown distributional shape and then can be considered as a useful class of flexible models to address several problems and applications involving measurements and features having well-known marked deviation from the Gaussian shape. The usefulness of the proposed approach is confirmed using synthetic mixture data, real data, and through an interesting application namely texture classification and retrieval. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "A fully Bayesian model based on reversible jump MCMC and finite Beta mixtures for clustering", "paper_id": "WOS:000301155300134"}