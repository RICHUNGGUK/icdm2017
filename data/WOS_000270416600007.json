{"auto_keywords": [{"score": 0.04465012940866221, "phrase": "rca"}, {"score": 0.014751986122880464, "phrase": "causality_boundaries"}, {"score": 0.014627476342450863, "phrase": "causality_expressions"}, {"score": 0.013726086177275143, "phrase": "engineering_domains"}, {"score": 0.008744367202449522, "phrase": "proposed_approach"}, {"score": 0.008063707675705654, "phrase": "nlp_analysis"}, {"score": 0.00481495049065317, "phrase": "root_cause_analysis_reports"}, {"score": 0.004573842508130535, "phrase": "automatic_approach"}, {"score": 0.004363401436068803, "phrase": "explicitly_expressed_causalities"}, {"score": 0.004307700591183651, "phrase": "root_cause_analysis"}, {"score": 0.004127094600862448, "phrase": "cause_and_effect_pairs"}, {"score": 0.00409188863874203, "phrase": "multiple_expressions"}, {"score": 0.004005176784823524, "phrase": "single_sentence"}, {"score": 0.0039035351575463103, "phrase": "text_fragments"}, {"score": 0.0037398106866046972, "phrase": "corresponding_effects"}, {"score": 0.003629291866667764, "phrase": "natural_language_processing"}, {"score": 0.0035983502148981485, "phrase": "nlp"}, {"score": 0.003522027550662857, "phrase": "current_off-the-shelf_nlp_tools"}, {"score": 0.0034179225740833055, "phrase": "language_models"}, {"score": 0.003388745581055085, "phrase": "general-purpose_texts"}, {"score": 0.0030705342386533083, "phrase": "comparable_analysis_accuracy"}, {"score": 0.003044313656896057, "phrase": "new_domains"}, {"score": 0.0029165216647792924, "phrase": "rare_and_unpredictable_behaviours"}, {"score": 0.002854645802851, "phrase": "closed_domains"}, {"score": 0.0028302635493019867, "phrase": "ill-formed_sentences"}, {"score": 0.0027583557819362034, "phrase": "common_words"}, {"score": 0.002586457551025025, "phrase": "probability-based_method"}, {"score": 0.0025424508991051483, "phrase": "probability_distribution"}, {"score": 0.002404522326122611, "phrase": "local_contexts"}, {"score": 0.002373767863833987, "phrase": "language_conventions"}, {"score": 0.002216251512062482, "phrase": "aerospace_company"}, {"score": 0.0021049977753042253, "phrase": "baseline_approach"}], "paper_keywords": ["Causality boundary identification", " Root cause analysis", " Natural language processing"], "paper_abstract": "This paper presents the results of developing and evaluating an automatic approach that identifies causality boundaries from causality expressions. This approach focuses on explicitly expressed causalities extracted from Root Cause Analysis (RCA) reports in engineering domains. Causality expressions contain Cause and Effect pairs and multiple expressions can occur in a single sentence. Causality boundaries are semantically annotated text fragments explicitly indicating which parts of a fragment denote Causes and corresponding Effects. To identify these, linguistic analysis using natural language processing (NLP) is required. Current off-the-shelf NLP tools are mostly developed based on the language models of general-purpose texts, e. g. newspapers. The lack of portability of these tools to engineering domains has been identified as a barrier to achieving comparable analysis accuracy in new domains. One of the reasons for this is the rare and unpredictable behaviours of certain words in closed domains. Ill-formed sentences, abbreviations and capitalization of common words also contribute to the difficulty. The proposed approach addresses this problem by using a probability-based method that learns the probability distribution of the boundaries not only from the NLP analysis but also from the local contexts that exploit language conventions occurred in the RCA reports. Using a collection of RCA reports obtained from an aerospace company, a test showed that the proposed approach achieved 86% accuracy outperforming a baseline approach that relied only on the NLP analysis.", "paper_title": "Towards automatic causality boundary identification from root cause analysis reports", "paper_id": "WOS:000270416600007"}