{"auto_keywords": [{"score": 0.029540288343682004, "phrase": "hq_minimization"}, {"score": 0.014714816497320565, "phrase": "cost_function"}, {"score": 0.00481495049065317, "phrase": "half-quadratic_minimization"}, {"score": 0.004755054742217193, "phrase": "gradient_linearization_iteration"}, {"score": 0.004695900553353526, "phrase": "popular_way"}, {"score": 0.004447915226010682, "phrase": "quadratic_data-fidelity_term"}, {"score": 0.00421297029755273, "phrase": "latter_term"}, {"score": 0.003940713019060992, "phrase": "hq"}, {"score": 0.0037795183561431705, "phrase": "geman"}, {"score": 0.003748185933404487, "phrase": "reynolds"}, {"score": 0.0036097844103129043, "phrase": "computational_task"}, {"score": 0.003535165233011893, "phrase": "image_reconstruction"}, {"score": 0.003505749731180509, "phrase": "nonconvex_regularization"}, {"score": 0.003404703106153948, "phrase": "locally_homogeneous_image_models"}, {"score": 0.0033622909109253616, "phrase": "continuous-valued_line_process"}, {"score": 0.003292770855568091, "phrase": "optimization_problem"}, {"score": 0.0032246835734680377, "phrase": "augmented_cost_function"}, {"score": 0.003028727936200845, "phrase": "line_process"}, {"score": 0.002856563331151566, "phrase": "large_amount"}, {"score": 0.002762601646452813, "phrase": "important_results"}, {"score": 0.0026941587813581252, "phrase": "convex_regularization"}, {"score": 0.002327310963701377, "phrase": "iteration_step"}, {"score": 0.0022507188208913394, "phrase": "exactly_the_same_iterations"}, {"score": 0.0021315842986705485, "phrase": "quasi-newton_method"}, {"score": 0.0021049977753042253, "phrase": "generalized_weiszfeld's_method"}], "paper_keywords": ["gradient linearization", " half-quadratic (HQ) regularization", " inverse problems", " optimization", " signal and image restoration", " variational methods"], "paper_abstract": "A popular way to restore images comprising edges is to minimize a cost function combining a quadratic data-fidelity term and an edge-preserving (possibly nonconvex) regularization term. Mainly because of the latter term, the calculation of the solution is slow and cumbersome. Half-quadratic (HQ) minimization (multiplicative form) was pioneered by Geman and Reynolds (1992) in order to alleviate the computational task in the context of image reconstruction with nonconvex regularization. By promoting the idea of locally homogeneous image models with a continuous-valued line process, they reformulated the optimization problem in terms of an augmented cost function which is quadratic with respect to the image and separable with respect to the line process, hence the name \"half quadratic.\" Since then, a large amount of papers were dedicated to HQ minimization and important results-including edge-preservation along with convex regularization and convergence-have been obtained. In this paper, we show that HQ minimization (multiplicative form) is equivalent to the most simple and basic method where the gradient of the cost function is linearized at each iteration step. In fact, both methods give exactly the same iterations. Furthermore, connections of HQ minimization with other methods, such as the quasi-Newton method and the generalized Weiszfeld's method, are straightforward.", "paper_title": "The equivalence of half-quadratic minimization and the gradient linearization iteration", "paper_id": "WOS:000246641600012"}