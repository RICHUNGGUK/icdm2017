{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "dynamic_prices"}, {"score": 0.03550339016594857, "phrase": "inventory_levels"}, {"score": 0.015413924893243265, "phrase": "price_sensitive_customers"}, {"score": 0.01529325437613964, "phrase": "stochastic_demands"}, {"score": 0.01511399042451168, "phrase": "inventory_replenishments"}, {"score": 0.012171806335082108, "phrase": "rl"}, {"score": 0.01111586181596151, "phrase": "customer_queue_levels"}, {"score": 0.004757917303420435, "phrase": "electronic_retail_markets"}, {"score": 0.004325208730208653, "phrase": "price_dynamics"}, {"score": 0.004273951631010383, "phrase": "electronic_retail_market"}, {"score": 0.004173244935967375, "phrase": "price_sensitive_and_lead_time_sensitive_customers"}, {"score": 0.00396308097989371, "phrase": "stochastically_arriving_demands"}, {"score": 0.00385431710959668, "phrase": "standard_inventory_control_and_replenishment_policies"}, {"score": 0.0030239842475439814, "phrase": "automated_pricing_agents"}, {"score": 0.0029409171055888804, "phrase": "rl-based_pricing_algorithms"}, {"score": 0.0028715299874549245, "phrase": "random_intervals"}, {"score": 0.0027815466400771768, "phrase": "back_orders"}, {"score": 0.002726740692380339, "phrase": "replenishment_lead_times"}, {"score": 0.0026518169452004465, "phrase": "discounted_cumulative_profit"}, {"score": 0.0024586427394152196, "phrase": "derivative_following"}, {"score": 0.002391067964886602, "phrase": "partial_information_case"}, {"score": 0.002316105585203396, "phrase": "markovian_game"}, {"score": 0.002147333949483224, "phrase": "new_and_promising_way"}, {"score": 0.0021049977753042253, "phrase": "multiseller_environments"}], "paper_keywords": ["dynamic pricing", " inventory replenishments", " Markovian game", " multi-agent learning", " online retail markets", " price sensitive customers", " reinforcement learning (RL)", " stochastic demands"], "paper_abstract": "In this paper, we use reinforcement learning (RL) as a tool to study price dynamics in an electronic retail market consisting of two competing sellers, and price sensitive and lead time sensitive customers. Sellers, offering identical products, compete on price to satisfy stochastically arriving demands (customers), and follow standard inventory control and replenishment policies to manage their inventories. In such a generalized setting, RL techniques have not previously been applied. We consider two representative cases: 1) no information case, were none of the sellers has any information about customer queue levels, inventory levels, or prices at the competitors; and 2) partial information case, where every seller has information about the customer queue levels and inventory levels of the competitors. Sellers employ automated pricing agents, or pricebots, which use RL-based pricing algorithms to reset the prices at random intervals based on factors such as number of back orders, inventory levels, and replenishment lead times, with the objective of maximizing discounted cumulative profit. In the no information case, we show that a seller who uses Q-learning outperforms a seller who uses derivative following (DF). In the partial information case, we model the problem as a Markovian game and use actor-critic based RL to learn dynamic prices. We believe our approach to solving these problems is a new and promising way of setting dynamic prices in multiseller environments with stochastic demands, price sensitive customers, and inventory replenishments.", "paper_title": "Learning dynamic prices in multiseller electronic retail markets with price sensitive customers, stochastic demands, and inventory replenishments", "paper_id": "WOS:000235899000009"}