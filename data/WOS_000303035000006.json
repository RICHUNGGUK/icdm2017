{"auto_keywords": [{"score": 0.03269580021341535, "phrase": "decomposable_model"}, {"score": 0.014598012736099423, "phrase": "covariance_matrix"}, {"score": 0.013555247506549956, "phrase": "inverse_covariance_matrix"}, {"score": 0.011647585278826308, "phrase": "true_model"}, {"score": 0.008621084396574646, "phrase": "sample_size"}, {"score": 0.00481495049065317, "phrase": "decomposable_gaussian_graphical_models"}, {"score": 0.00473727773195392, "phrase": "graphical_models"}, {"score": 0.004691273968935458, "phrase": "powerful_tool"}, {"score": 0.004615587054831451, "phrase": "conditional_independence"}, {"score": 0.004296747096249343, "phrase": "gaussian_setting"}, {"score": 0.004186325123152709, "phrase": "non-zero_elements"}, {"score": 0.003834099867584972, "phrase": "high-dimensional_bayesian"}, {"score": 0.003747705541406206, "phrase": "model_selection_procedures"}, {"score": 0.0037233796053514918, "phrase": "decomposable_models"}, {"score": 0.0033658327434624457, "phrase": "decomposable_graphical_model"}, {"score": 0.0032053233533168362, "phrase": "true_matrix"}, {"score": 0.003143276831449432, "phrase": "maximum_likelihood"}, {"score": 0.0030227528484104756, "phrase": "true_non-decomposable_model"}, {"score": 0.002925843578799596, "phrase": "particular_elements"}, {"score": 0.002336527289457915, "phrase": "dramatic_increase"}, {"score": 0.00214659338413584, "phrase": "models_increases"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Graphical model", " Covariance selection", " Decomposable models", " Regularization", " Small-sample inference"], "paper_abstract": "Graphical models are a powerful tool for describing patterns of conditional independence, and can also be used to regularize the covariance matrix. Vertices in the graph represent variables, and in the Gaussian setting, edges between vertices are equivalent to non-zero elements in the inverse covariance matrix. Models that can be represented as a decomposable (triangulated) graph are more computationally tractable; in fact, in the high-dimensional Bayesian setting it is common to restrict model selection procedures to decomposable models. We consider estimation of the covariance and inverse covariance matrix where the true model forms a cycle, but estimation is performed supposing that the pattern of zeros is a decomposable graphical model, where the elements restricted to zero are a subset of those in the true matrix. The variance of the maximum likelihood estimator based on the decomposable model is demonstrably larger than for the true non-decomposable model, and which decomposable model is selected affects the variance of particular elements of the matrix. When estimating the inverse covariance matrix the cost in terms of accuracy for using the decomposable model is fairly small, even when the difference in sparsity is large and the sample size is fairly small (e.g., the true model is a cycle of size 50, and the sample size is 51). However, when estimating the covariance matrix, the estimators for most elements had a dramatic increase in variance (200-fold in some cases) when a decomposable model was substituted. These increases become more pronounced as the difference in sparsity between models increases. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "The cost of using decomposable Gaussian graphical models for computational convenience", "paper_id": "WOS:000303035000006"}