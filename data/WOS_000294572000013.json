{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "custom_software"}, {"score": 0.004635701867952543, "phrase": "promising_empirical_performance"}, {"score": 0.0038996120252338556, "phrase": "example'_approach"}, {"score": 0.003818210967854446, "phrase": "open-source_nlp_pipelines"}, {"score": 0.0036144035798673967, "phrase": "top-performing_configurations"}, {"score": 0.003378385493203297, "phrase": "top_f-measure_scores"}, {"score": 0.0032800186243040663, "phrase": "medical_problems"}, {"score": 0.002735392281320413, "phrase": "end-user_time"}, {"score": 0.002633355582628613, "phrase": "average_f-measure"}, {"score": 0.0025458479274370832, "phrase": "mean_f-measure"}, {"score": 0.002461241017147324, "phrase": "strong_precision_scores"}, {"score": 0.002280990744438186, "phrase": "iterative_approach"}, {"score": 0.002261794227418188, "phrase": "model_creation"}, {"score": 0.002242758902593467, "phrase": "conclusion_acceptable_levels"}, {"score": 0.0021773853011112882, "phrase": "fully_automated_and_generalizable_approaches"}, {"score": 0.0021590588269751816, "phrase": "concept-level_information_extraction"}, {"score": 0.0021049977753042253, "phrase": "related_documentation"}], "paper_keywords": [""], "paper_abstract": "Objective Despite at least 40 years of promising empirical performance, very few clinical natural language processing (NLP) or information extraction systems currently contribute to medical science or care. The authors address this gap by reducing the need for custom software and rules development with a graphical user interface-driven, highly generalizable approach to concept-level retrieval. Materials and methods A 'learn by example' approach combines features derived from open-source NLP pipelines with open-source machine learning classifiers to automatically and iteratively evaluate top-performing configurations. The Fourth i2b2/VA Shared Task Challenge's concept extraction task provided the data sets and metrics used to evaluate performance. Results Top F-measure scores for each of the tasks were medical problems (0.83), treatments (0.82), and tests (0.83). Recall lagged precision in all experiments. Precision was near or above 0.90 in all tasks. Discussion With no customization for the tasks and less than 5 min of end-user time to configure and launch each experiment, the average F-measure was 0.83, one point behind the mean F-measure of the 22 entrants in the competition. Strong precision scores indicate the potential of applying the approach for more specific clinical information extraction tasks. There was not one best configuration, supporting an iterative approach to model creation. Conclusion Acceptable levels of performance can be achieved using fully automated and generalizable approaches to concept-level information extraction. The described implementation and related documentation is available for download.", "paper_title": "Automated concept-level information extraction to reduce the need for custom software and rules development", "paper_id": "WOS:000294572000013"}