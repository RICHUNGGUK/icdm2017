{"auto_keywords": [{"score": 0.04286666008487537, "phrase": "brain's_responses"}, {"score": 0.010944828285218516, "phrase": "brain_networks"}, {"score": 0.010096548630967829, "phrase": "benchmark_attention_curves"}, {"score": 0.00481495049065317, "phrase": "fmri-driven_visual_attention_model"}, {"score": 0.004709567162738894, "phrase": "digital_video_data"}, {"score": 0.004657739551687038, "phrase": "profound_challenge"}, {"score": 0.004522291357556591, "phrase": "quickly-evolving_research_topic"}, {"score": 0.00424734451992536, "phrase": "human_brain"}, {"score": 0.00388729255686046, "phrase": "novel_video_abstraction_paradigm"}, {"score": 0.003844478919573134, "phrase": "functional_magnetic_resonance_imaging"}, {"score": 0.0036914757355166966, "phrase": "video_stimuli"}, {"score": 0.0035445401296650535, "phrase": "visually_informative_segments"}, {"score": 0.003428665921276366, "phrase": "video_perception"}, {"score": 0.0031845067058084613, "phrase": "spectral_graph_theory"}, {"score": 0.003035177667751495, "phrase": "fmri-measured_brain_responses"}, {"score": 0.0029796206196589115, "phrase": "training_video_streams"}, {"score": 0.0028085616610192456, "phrase": "low-level_visual_features"}, {"score": 0.0027673496368158545, "phrase": "bayesian_surprise_model"}, {"score": 0.0026966744704447275, "phrase": "training_stage"}, {"score": 0.00266693888510356, "phrase": "optimization_objective"}, {"score": 0.0026084451971464867, "phrase": "learned_attentional_model"}, {"score": 0.0024768883044869023, "phrase": "video_contents"}, {"score": 0.0024405312482831646, "phrase": "application_stage"}, {"score": 0.0023003498123420237, "phrase": "effective_benchmark"}, {"score": 0.0022834017104377525, "phrase": "abstract_testing_videos"}, {"score": 0.0022250584824198218, "phrase": "video_sequences"}, {"score": 0.0022005118194652704, "phrase": "trecvid_database"}, {"score": 0.0021522261505574035, "phrase": "proposed_framework"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Video abstraction", " Visual attention", " Functional magnetic resonance imaging", " Propensity for synchronization", " Bayesian surprise model"], "paper_abstract": "The explosive growth of digital video data renders a profound challenge to succinct, informative, and human-centric representations of video contents. This quickly-evolving research topic is typically called 'video abstraction'. We are motivated by the facts that the human brain is the end-evaluator of multimedia content and that the brain's responses can quantitatively reveal its attentional engagement in the comprehension of video. We propose a novel video abstraction paradigm which leverages functional magnetic resonance imaging (FMRI) to monitor and quantify the brain's responses to video stimuli. These responses are used to guide the extraction of visually informative segments from videos. Specifically, most relevant brain regions involved in video perception and cognition are identified to form brain networks. Then, the propensity for synchronization (PFS) derived from spectral graph theory is utilized over the brain networks to yield the benchmark attention curves based on the fMRI-measured brain responses to a number of training video streams. These benchmark attention curves are applied to guide and optimize the combinations of a variety of low-level visual features created by the Bayesian surprise model. In particular, in the training stage, the optimization objective is to ensure that the learned attentional model correlates well with the brain's responses and reflects the attention that viewers pay to video contents. In the application stage, the attention curves predicted by the learned and optimized attentional model serve as an effective benchmark to abstract testing videos. Evaluations on a set of video sequences from the TRECVID database demonstrate the effectiveness of the proposed framework. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Video abstraction based on fMRI-driven visual attention model", "paper_id": "WOS:000340315600055"}