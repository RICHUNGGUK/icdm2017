{"auto_keywords": [{"score": 0.04949240338189186, "phrase": "online_linear_regression"}, {"score": 0.03882201257057183, "phrase": "stochastic_setting"}, {"score": 0.00481495049065317, "phrase": "individual_sequences"}, {"score": 0.0044991925146184025, "phrase": "arbitrary_deterministic_sequences"}, {"score": 0.004416713079266374, "phrase": "ambient_dimension"}, {"score": 0.004152502093643003, "phrase": "time_rounds"}, {"score": 0.003976978140059542, "phrase": "sparsity_regret"}, {"score": 0.003832423454781517, "phrase": "deterministic_online_counterpart"}, {"score": 0.0037854112749311844, "phrase": "recent_risk_bounds"}, {"score": 0.003625347557965931, "phrase": "sparsity_scenario"}, {"score": 0.0034720284451902083, "phrase": "online-learning_algorithm"}, {"score": 0.003429422506430291, "phrase": "seqsew"}, {"score": 0.0033457659530207306, "phrase": "exponential_weighting"}, {"score": 0.0033047037762521984, "phrase": "data-driven_truncation"}, {"score": 0.0032240802202623316, "phrase": "second_part"}, {"score": 0.003145417398038261, "phrase": "parameter-free_version"}, {"score": 0.002938821840519756, "phrase": "random_design"}, {"score": 0.0028494425496120228, "phrase": "risk_bounds"}, {"score": 0.002745760444779018, "phrase": "dalalyan"}, {"score": 0.0027120517119619326, "phrase": "tsybakov"}, {"score": 0.0023819914954155905, "phrase": "logarithmic_factor"}, {"score": 0.002323826502178581, "phrase": "unknown_variance"}, {"score": 0.002213361127372731, "phrase": "gaussian"}, {"score": 0.0021311850321045767, "phrase": "regression_model"}, {"score": 0.0021049977753042253, "phrase": "fixed_design"}], "paper_keywords": ["sparsity", " online linear regression", " individual sequences", " adaptive regret bounds"], "paper_abstract": "We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T. We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same flavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with fixed design.", "paper_title": "Sparsity Regret Bounds for Individual Sequences in Online Linear Regression", "paper_id": "WOS:000317461700001"}