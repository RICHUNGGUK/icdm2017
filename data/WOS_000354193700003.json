{"auto_keywords": [{"score": 0.040841239240267654, "phrase": "rank-order_tournaments"}, {"score": 0.00481495049065317, "phrase": "incent_crowd_workers_payment_schemes"}, {"score": 0.004738398266229738, "phrase": "crowdsourcing"}, {"score": 0.004607139608615063, "phrase": "digital_work_places"}, {"score": 0.00455202992136173, "phrase": "amazon_mechanical_turk"}, {"score": 0.004479570469116531, "phrase": "clickworker"}, {"score": 0.004286171444202565, "phrase": "human_work"}, {"score": 0.004150761431291174, "phrase": "crowdsourcing_settings"}, {"score": 0.004035777314816881, "phrase": "worker_effort"}, {"score": 0.003955593400628455, "phrase": "common_incentive_schemes"}, {"score": 0.003923965929636472, "phrase": "piece_rate_payments"}, {"score": 0.0038306524202144955, "phrase": "tournaments"}, {"score": 0.0037244390953401533, "phrase": "worker's_current_competitive_position"}, {"score": 0.0036212219870826725, "phrase": "exploratory_approach"}, {"score": 0.0035208552733545463, "phrase": "worker_performance"}, {"score": 0.00340954056723008, "phrase": "real_effort_studies"}, {"score": 0.0033822643082593285, "phrase": "experimental_techniques"}, {"score": 0.0033417571465258, "phrase": "online_labor_market"}, {"score": 0.003236086384959101, "phrase": "dyadic_tournaments"}, {"score": 0.00319732428387928, "phrase": "rate_payments"}, {"score": 0.003121183062764093, "phrase": "average_dyadic_tournaments"}, {"score": 0.003022465992278672, "phrase": "simple_piece_rate"}, {"score": 0.0029982770062678926, "phrase": "simple_and_short_crowdsourcing_tasks"}, {"score": 0.0029034358982402346, "phrase": "competitive_position"}, {"score": 0.0028003107731426322, "phrase": "workers'_performance"}, {"score": 0.002711714675297321, "phrase": "task_completion"}, {"score": 0.0025840347813610815, "phrase": "strong_competitors"}, {"score": 0.0024230746206056536, "phrase": "lower_performance"}, {"score": 0.0022721178679337025, "phrase": "reduced_effort"}, {"score": 0.0022358683283304533, "phrase": "individual_piece_rate_payments"}], "paper_keywords": ["Crowdsourcing", " Online labor", " Incentives", " Exploratory study", " Experimental techniques", " Real effort task", " Rank-order tournament", " Piece rate", " Feedback"], "paper_abstract": "Crowdsourcing gains momentum: In digital work places such as Amazon Mechanical Turk, oDesk, Clickworker, 99designs, or InnoCentive it is easy to distribute human work to hundreds or thousands of freelancers. In these crowdsourcing settings, one challenge is to properly incent worker effort to create value. Common incentive schemes are piece rate payments and rank-order tournaments among workers. Tournaments might or might not disclose a worker's current competitive position via a leaderboard. Following an exploratory approach, we derive a model on worker performance in rank-order tournaments and present a series of real effort studies using experimental techniques on an online labor market to test the model and to compare dyadic tournaments to piece rate payments. Data suggests that on average dyadic tournaments do not improve performance compared to a simple piece rate for simple and short crowdsourcing tasks. Furthermore, giving feedback on the competitive position in such tournaments tends to be negatively related to workers' performance. This relation is partially mediated by task completion and moderated by the provision of feedback: When playing against strong competitors, feedback is associated with workers quitting the task altogether and, thus, showing lower performance. When the competitors are weak, workers tend to complete the task but with reduced effort. Overall, individual piece rate payments are most simple to communicate and implement while incenting performance is on par with more complex dyadic tournaments.", "paper_title": "How (not) to Incent Crowd Workers Payment Schemes and Feedback in Crowdsourcing", "paper_id": "WOS:000354193700003"}