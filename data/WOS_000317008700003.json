{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cheat_robustness"}, {"score": 0.004738325072560588, "phrase": "crowdsourcing_tasks"}, {"score": 0.00444377150105556, "phrase": "widely_used_means"}, {"score": 0.004338076511024884, "phrase": "large-scale_scientific_corpora"}, {"score": 0.004101088357088207, "phrase": "information_retrieval"}, {"score": 0.003939748053452577, "phrase": "novel_way"}, {"score": 0.003876996400581697, "phrase": "data_acquisition"}, {"score": 0.003549245218355768, "phrase": "significant_share"}, {"score": 0.003301733505307784, "phrase": "quick_generic_answers"}, {"score": 0.0032231139180761183, "phrase": "correct_ones"}, {"score": 0.0027667539384678814, "phrase": "numerous_sophisticated_schemes"}, {"score": 0.0025326211341475903, "phrase": "additional_resources"}, {"score": 0.002472269923319476, "phrase": "artificial_limitations"}, {"score": 0.002244876254113051, "phrase": "different_approach"}, {"score": 0.0021049977753042253, "phrase": "crowdsourced_tasks"}], "paper_keywords": ["Crowdsourcing", " User experiments", " Stability", " Human factors"], "paper_abstract": "Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.", "paper_title": "Increasing cheat robustness of crowdsourcing tasks", "paper_id": "WOS:000317008700003"}