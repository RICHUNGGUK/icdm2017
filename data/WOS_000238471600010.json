{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "optimum_choice"}, {"score": 0.0045959609346817535, "phrase": "nearest_neighbor_classification"}, {"score": 0.004438250823243621, "phrase": "major_issue"}, {"score": 0.004336115760817672, "phrase": "k-nearest_neighbor_classification"}, {"score": 0.004043541331944951, "phrase": "optimum_value"}, {"score": 0.003859500298805362, "phrase": "parameter_k._popular_cross-validation_techniques"}, {"score": 0.0032031180198375283, "phrase": "multiple_minimizers"}, {"score": 0.003093054709625219, "phrase": "estimated_misclassification_rate"}, {"score": 0.002884111441153185, "phrase": "bayesian_method"}, {"score": 0.002566685388128598, "phrase": "multiple_optimizers"}, {"score": 0.0023932142697579506, "phrase": "proposed_method"}, {"score": 0.0022576249214220187, "phrase": "benchmark_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["accuracy index", " Bayesian strength function", " cross-validation", " misclassification rate", " neighborhood parameter", " non-informative prior", " optimal Bayes risk", " posterior probability"], "paper_abstract": "A major issue in k-nearest neighbor classification is how to choose the optimum value of the neighborhood parameter k. Popular cross-validation techniques often fail to guide us well in selecting k mainly due to the presence of multiple minimizers of the estimated misclassification rate. This article investigates a Bayesian method in this connection, which solves the problem of multiple optimizers. The utility of the proposed method is illustrated using some benchmark data sets. (C) 2005 Elsevier B.V. All rights reserved.", "paper_title": "On optimum choice of k in nearest neighbor classification", "paper_id": "WOS:000238471600010"}