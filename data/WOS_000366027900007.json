{"auto_keywords": [{"score": 0.03657649623565426, "phrase": "umeme"}, {"score": 0.007625250936540795, "phrase": "emotion_perception"}, {"score": 0.004725886853744007, "phrase": "cto_communication"}, {"score": 0.004552648949390462, "phrase": "social_interactions"}, {"score": 0.004510336430422002, "phrase": "emotion_expression"}, {"score": 0.004344965512058521, "phrase": "vocal_behavior"}, {"score": 0.00428451995524549, "phrase": "body_gestures"}, {"score": 0.004146716743362567, "phrase": "multimodal_information"}, {"score": 0.003812299519486331, "phrase": "multimodal_classification_algorithms"}, {"score": 0.0037768417338850274, "phrase": "affective_interfaces"}, {"score": 0.0037242701420971062, "phrase": "even_mental_health_assessment"}, {"score": 0.0036382660173943393, "phrase": "novel_data_set"}, {"score": 0.0035211747962710246, "phrase": "emotion_perception_process"}, {"score": 0.0033761303704612734, "phrase": "critical_feature"}, {"score": 0.00329813918116714, "phrase": "currently_existing_data_sets"}, {"score": 0.0028799352389969443, "phrase": "emotionally_complex_and_dynamic_stimuli"}, {"score": 0.002735522834514124, "phrase": "emotion_content"}, {"score": 0.0026722904884984348, "phrase": "emotional_incongruence"}, {"score": 0.00263505298504827, "phrase": "emotional_noise"}, {"score": 0.002514625755809045, "phrase": "statistical_properties"}, {"score": 0.002456486927713543, "phrase": "present_evidence"}, {"score": 0.002344201345813952, "phrase": "specific_types"}, {"score": 0.002237036807966899, "phrase": "consistent_patterns"}, {"score": 0.0021049977753042253, "phrase": "important_new_tool"}], "paper_keywords": ["Emotion perception", " McGurk effect", " multimodal", " unimodal", " affect"], "paper_abstract": "Emotion is cto communication; it colors our interpretation of events and social interactions. Emotion expression is generally multimodal, modulating our facial movement, vocal behavior, and body gestures. The method through which this multimodal information is integrated and perceived is not well understood. This knowledge has implications for the design of multimodal classification algorithms, affective interfaces, and even mental health assessment. We present a novel data set designed to support research into the emotion perception process, the University of Michigan Emotional McGurk Effect Data set (UMEME). UMEME has a critical feature that differentiates it from currently existing data sets; it contains not only emotionally congruent stimuli (emotionally matched faces and voices), but also emotionally incongruent stimuli (emotionally mismatched faces and voices). The inclusion of emotionally complex and dynamic stimuli provides an opportunity to study how individuals make assessments of emotion content in the presence of emotional incongruence, or emotional noise. We describe the collection, annotation, and statistical properties of the data and present evidence illustrating how audio and video interact to result in specific types of emotion perception. The results demonstrate that there exist consistent patterns underlying emotion evaluation, even given incongruence, positioning UMEME as an important new tool for understanding emotion perception.", "paper_title": "UMEME: University of Michigan Emotional McGurk Effect Data Set", "paper_id": "WOS:000366027900007"}