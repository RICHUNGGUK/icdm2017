{"auto_keywords": [{"score": 0.036171595291928434, "phrase": "esa"}, {"score": 0.010612229798637263, "phrase": "machine_learning"}, {"score": 0.0070932148714528765, "phrase": "probabilistic_framework"}, {"score": 0.005130431274912892, "phrase": "evolutionary_sampling_approach"}, {"score": 0.00464449788953748, "phrase": "training_data"}, {"score": 0.004542603228075638, "phrase": "sequential_markov_chains"}, {"score": 0.004517479191423587, "phrase": "particle_filters"}, {"score": 0.004369613583314372, "phrase": "intractable_optimization_problems"}, {"score": 0.00434544213389516, "phrase": "classical_learning_methods"}, {"score": 0.004076858589744185, "phrase": "key_feature"}, {"score": 0.00404306734103577, "phrase": "sampling_process"}, {"score": 0.004009555045206973, "phrase": "representative_samples"}, {"score": 0.003987367359722647, "phrase": "original_data"}, {"score": 0.0039106691249360965, "phrase": "probability_distribution"}, {"score": 0.0038460985534832237, "phrase": "reliable_samples"}, {"score": 0.0038036426379851915, "phrase": "easy_task"}, {"score": 0.0037721073763702486, "phrase": "arbitrary_probability_distribution"}, {"score": 0.00370981613076245, "phrase": "evolutionary_computation"}, {"score": 0.0036384371764476386, "phrase": "novel_sampling_strategy"}, {"score": 0.003490056936165004, "phrase": "machine_learning_method"}, {"score": 0.0032741692702522463, "phrase": "support_sample_model"}, {"score": 0.0031405975280029913, "phrase": "original_density_function"}, {"score": 0.003097291905274541, "phrase": "concrete_implementation"}, {"score": 0.0029957718120653238, "phrase": "optimal_model_parameters"}, {"score": 0.0029709154333362586, "phrase": "ssm."}, {"score": 0.002921815042028607, "phrase": "rejection_sampling"}, {"score": 0.002905629026254221, "phrase": "evolutionary_searching"}, {"score": 0.002833891290347501, "phrase": "optimal_solution"}, {"score": 0.0028025773884888727, "phrase": "total_variation_distance"}, {"score": 0.0027486060614330043, "phrase": "high_computational_efficiency"}, {"score": 0.0027106909015979026, "phrase": "normalized_factor"}, {"score": 0.0026881926384093088, "phrase": "density_function"}, {"score": 0.002643753276264565, "phrase": "high_precision"}, {"score": 0.0026218096435054556, "phrase": "esa."}, {"score": 0.002542890176213921, "phrase": "machine_learning_problems"}, {"score": 0.0025008468316682036, "phrase": "density_function_approximation_problems"}, {"score": 0.0024255601098027227, "phrase": "rejection_sampling_strategy"}, {"score": 0.0023788319661581696, "phrase": "online_learning_abilities"}, {"score": 0.002359081558986726, "phrase": "large-scale_data_stream_processing_tasks"}, {"score": 0.0022068557886223673, "phrase": "novel_way"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Evolutionary sampling", " Support sample model", " Monte Carlo Markov chain", " Rejection sampling", " Online learning", " Particle swarm optimization"], "paper_abstract": "In many traditional machine learning methods, sampling is only a process of acquiring training data. However, some studies (on sequential Markov chains and particle filters) have demonstrated that sampling can be used for solving some intractable optimization problems in classical learning methods. Along this line of thinking, the relationships between sampling and learning are theoretically exploited in this paper, wherein the key feature of the sampling process is selecting representative samples from original data that can be modeled by a probability distribution. In theory, acquiring reliable samples is not an easy task for an arbitrary probability distribution. Motivated by approaches in evolutionary computation, rejection sampling and function approximation, a novel sampling strategy, called the evolutionary sampling, is proposed in this paper, and a machine learning method, called the evolutionary sampling approach (ESA), is put forward afterwards. Within ESA, a computing model, called the support sample model (SSM), is presented as well and is used to approximate an original density function. Accordingly, a concrete implementation of an evolutionary sampling approach (ESA) is proposed to seek the optimal model parameters of the SSM. Benefiting from the combination of rejection sampling and evolutionary searching, the ESA can theoretically converge to the optimal solution by minimizing the total variation distance, and can do this with high computational efficiency.. Moreover, the normalized factor of a density function can be automatically estimated with high precision within the ESA. As a result, the ESA may be suitable for machine learning problems that could be transformed into density function approximation problems within a probabilistic framework. In addition, derived from the rejection sampling strategy, the ESA can also have online learning abilities required by large-scale data stream processing tasks. Theoretical analyses and application studies are carried out in this paper, and the results demonstrate that the ESA, as a novel way of machine learning, has several prominent merits aspired by past researches in machine learning. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Evolutionary sampling: A novel way of machine learning within a probabilistic framework", "paper_id": "WOS:000349728200016"}