{"auto_keywords": [{"score": 0.04412469500707909, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "iteration_complexity_of"}, {"score": 0.004762202897268383, "phrase": "feasible_descent_methods"}, {"score": 0.004658426838467567, "phrase": "convex_optimization"}, {"score": 0.004312622975366676, "phrase": "dual_form"}, {"score": 0.004081385289516914, "phrase": "objective_function"}, {"score": 0.0032376465741070274, "phrase": "commonly_used_optimization_algorithms"}, {"score": 0.002931569771634119, "phrase": "global_linear_convergence"}, {"score": 0.0028360962841331634, "phrase": "wide_range"}, {"score": 0.0025678829712650437, "phrase": "non-strongly_convex_problems"}, {"score": 0.0021049977753042253, "phrase": "support_vector_classification"}], "paper_keywords": ["convergence rate", " convex optimization", " iteration complexity", " feasible descent", " methods"], "paper_abstract": "In many machine learning problems such as the dual form of SVM, the objective function to be minimized is convex but not strongly convex. This fact causes difficulties in obtaining the complexity of some commonly used optimization algorithms. In this paper, we proved the global linear convergence on a wide range of algorithms when they are applied to some non-strongly convex problems. In particular, we are the first to prove O(log(1/is an element of)) time complexity of cyclic coordinate descent methods on dual problems of support vector classification and regression.", "paper_title": "Iteration Complexity of Feasible Descent Methods for Convex Optimization", "paper_id": "WOS:000338420000011"}