{"auto_keywords": [{"score": 0.04954756810706401, "phrase": "collaborative_polling"}, {"score": 0.00481495049065317, "phrase": "mpi_communication_overlap"}, {"score": 0.0046046429665581555, "phrase": "parallel_applications_complexity"}, {"score": 0.004428139418750855, "phrase": "computational_power"}, {"score": 0.004306205029641728, "phrase": "recent_trends"}, {"score": 0.004258372598568333, "phrase": "high-performance_computing"}, {"score": 0.004049590638590415, "phrase": "single-core_performance"}, {"score": 0.003808209866576388, "phrase": "exascale_machine"}, {"score": 0.003703282519292336, "phrase": "enormous_growth"}, {"score": 0.0034437574674180365, "phrase": "data_volume"}, {"score": 0.0033864888703414903, "phrase": "compute_nodes"}, {"score": 0.0032565300451219133, "phrase": "exascale"}, {"score": 0.003202361402737197, "phrase": "communication_layer"}, {"score": 0.0030451958332949735, "phrase": "network_messages"}, {"score": 0.0029447150254434842, "phrase": "message_progression"}, {"score": 0.0028316582436089064, "phrase": "efficient_auto-adaptive_overlapping"}, {"score": 0.002475898613632249, "phrase": "threaded_message_progression"}, {"score": 0.0023941596496958374, "phrase": "infiniband"}, {"score": 0.002354302193867031, "phrase": "thread-based_mpi_runtime"}, {"score": 0.0023281024793218577, "phrase": "mpc."}, {"score": 0.0022137465420508785, "phrase": "nas_parallel_benchmarks"}, {"score": 0.002128694000627911, "phrase": "significant_improvements"}, {"score": 0.0021049977753042253, "phrase": "communication_times"}], "paper_keywords": ["HPC", " Overlap", " MPI", " High-speed network", " Polling"], "paper_abstract": "With the rise of parallel applications complexity, the needs in term of computational power are continually growing. Recent trends in High-Performance Computing (HPC) have shown that improvements in single-core performance will not be sufficient to face the challenges of an exascale machine: we expect an enormous growth of the number of cores as well as a multiplication of the data volume exchanged across compute nodes. To scale applications up to Exascale, the communication layer has to minimize the time while waiting for network messages. This paper presents a message progression based on Collaborative Polling which allows an efficient auto-adaptive overlapping of communication phases by performing computing. This approach is new as it increases the application overlap potential without introducing overheads of a threaded message progression. We designed our approch for Infiniband into a thread-based MPI runtime called MPC. We evaluate the gain from Collaborative Polling on the NAS Parallel Benchmarks and three scientific applications, where we show significant improvements in communication times up to a factor of 2.", "paper_title": "Improving MPI communication overlap with collaborative polling", "paper_id": "WOS:000333023600002"}