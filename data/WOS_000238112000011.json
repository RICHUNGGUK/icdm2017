{"auto_keywords": [{"score": 0.04913548507380238, "phrase": "trigonometric_hidden_layer_units"}, {"score": 0.04509655915119651, "phrase": "feedforward_neural_networks"}, {"score": 0.03380191138272064, "phrase": "essential_approximation_ability"}, {"score": 0.00481495049065317, "phrase": "neural_networks"}, {"score": 0.004402295697276237, "phrase": "approximation_ability"}, {"score": 0.004188459638572375, "phrase": "existing_studies"}, {"score": 0.0037537380967196123, "phrase": "upper_bound_estimation"}, {"score": 0.0036070601923916196, "phrase": "multivariate_function"}, {"score": 0.0026745503978347143, "phrase": "bound_estimations"}, {"score": 0.0026217213613095322, "phrase": "approximation_order"}, {"score": 0.0022127018991294047, "phrase": "second_order"}, {"score": 0.0021049977753042253, "phrase": "approximated_function"}], "paper_keywords": [""], "paper_abstract": "There have been various studies on approximation ability of feedforward neural networks. The existing studies are, however, only concerned with the density or upper bound estimation on how a multivariate function can be approximated by the networks, and consequently, the essential approximation ability of networks cannot be revealed. In this paper, by establishing both upper and lower bound estimations on approximation order, the essential approximation ability of a class of feedforward neural networks with trigonometric hidden layer units is clarified in terms of the second order modulus of smoothness of approximated function.", "paper_title": "The essential approximation order for neural networks with trigonometric hidden layer units", "paper_id": "WOS:000238112000011"}