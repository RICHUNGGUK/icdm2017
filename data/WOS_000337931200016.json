{"auto_keywords": [{"score": 0.049459317599017866, "phrase": "data-intensive_applications"}, {"score": 0.03074451962491464, "phrase": "data_chunks"}, {"score": 0.015719716506582538, "phrase": "workload_partition_factor"}, {"score": 0.011033132253473733, "phrase": "processing_nodes"}, {"score": 0.004777451835750203, "phrase": "resource_utilization"}, {"score": 0.004715598857504476, "phrase": "recent_data_deluge"}, {"score": 0.00460626523683981, "phrase": "major_challenges"}, {"score": 0.004570384351035464, "phrase": "computational_field"}, {"score": 0.004464402502195634, "phrase": "specially-designed_applications"}, {"score": 0.004315622626764739, "phrase": "parallel_execution"}, {"score": 0.004293175778663009, "phrase": "data-intensive_applications_input_data"}, {"score": 0.004248630241580105, "phrase": "smaller_data_chunks"}, {"score": 0.004085645237320135, "phrase": "severe_performance_problems"}, {"score": 0.004032712685822676, "phrase": "load_imbalance"}, {"score": 0.003980463173701481, "phrase": "available_resources"}, {"score": 0.0038277259893677576, "phrase": "performance_problems"}, {"score": 0.003778122295696292, "phrase": "dynamic_behavior"}, {"score": 0.003493665137212979, "phrase": "data_partitions"}, {"score": 0.0034573864329600413, "phrase": "overall_execution_time"}, {"score": 0.0033333422518227796, "phrase": "efficient_execution"}, {"score": 0.003272993685529221, "phrase": "application_behavior"}, {"score": 0.003197000180225272, "phrase": "gathered_data"}, {"score": 0.0030742330223615974, "phrase": "single_execution"}, {"score": 0.0030582231820028057, "phrase": "multiple_related_queries"}, {"score": 0.002925452606050736, "phrase": "initial_size"}, {"score": 0.0028575052964897223, "phrase": "scheduling_policy"}, {"score": 0.0028352077213099204, "phrase": "first_data_chunks"}, {"score": 0.002820439143557402, "phrase": "large_processing_times"}, {"score": 0.002762128010045399, "phrase": "biggest_associated_computation_times"}, {"score": 0.0027120928631675857, "phrase": "small_computation_times"}, {"score": 0.0026215563658729757, "phrase": "chunks'_associated_execution_time"}, {"score": 0.0025473072427198862, "phrase": "processing_elements"}, {"score": 0.0025011537516782104, "phrase": "resources_utilization"}, {"score": 0.002468698571619528, "phrase": "dynamic_evaluation"}, {"score": 0.0024494274654206505, "phrase": "application_performance"}, {"score": 0.0022647811364366263, "phrase": "synthetic_data-intensive_application"}, {"score": 0.002172050647768612, "phrase": "encouraging_results"}, {"score": 0.0021607289655029057, "phrase": "total_execution_times"}, {"score": 0.002149466169695525, "phrase": "efficient_use"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Load balancing", " Dynamic tuning", " Data-intensive applications", " Divisible Load Theory (DLT)"], "paper_abstract": "The recent data deluge needing to be processed represents one of the major challenges in the computational field. This fact led to the growth of specially-designed applications known as data-intensive applications. In general, in order to ease the parallel execution of data-intensive applications input data is divided into smaller data chunks that can be processed separately. However, in many cases, these applications show severe performance problems mainly due to the load imbalance, inefficient use of available resources, and improper data partition policies. In addition, the impact of these performance problems can depend on the dynamic behavior of the application. This work proposes a methodology to dynamically improve the performance of data-intensive applications based on: (i) adapting the size and the number of data partitions to reduce the overall execution time; and (ii) adapting the number of processing nodes to achieve an efficient execution. We propose to monitor the application behavior for each exploration (query) and use gathered data to dynamically tune the performance of the application. The methodology assumes that a single execution includes multiple related queries on the same partitioned workload. The adaptation of the workload partition factor is addressed through the definition of the initial size for the data chunks; the modification of the scheduling policy to send first data chunks with large processing times; dividing of the data chunks with the biggest associated computation times; and joining of data chunks with small computation times. The criteria for dividing or gathering chunks are based on the chunks' associated execution time (average and standard deviation) and the number of processing elements being used. Additionally, the resources utilization is addressed through the dynamic evaluation of the application performance and the estimation and modification of the number of processing nodes that can be efficiently used. We have evaluated our strategy using as cases of study a real and a synthetic data-intensive application. Analytical expressions have been analyzed through simulation. Applying our methodology, we have obtained encouraging results reducing total execution times and efficient use of resources. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Dynamic tuning of the workload partition factor and the resource utilization in data-intensive applications", "paper_id": "WOS:000337931200016"}