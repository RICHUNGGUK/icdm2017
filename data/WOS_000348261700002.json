{"auto_keywords": [{"score": 0.036202361760999516, "phrase": "linear"}, {"score": 0.00481495049065317, "phrase": "synthesized_speech"}, {"score": 0.004716520738331953, "phrase": "perceptual_quality_dimensions"}, {"score": 0.004652016085325825, "phrase": "instrumental_speech-quality_prediction"}, {"score": 0.004463723624910583, "phrase": "speech_signals"}, {"score": 0.004312622975366676, "phrase": "twofold_manner"}, {"score": 0.0041666157975446564, "phrase": "perceptual_quality_space"}, {"score": 0.003809628721278519, "phrase": "multiple_auditory_tests"}, {"score": 0.0037060428445111694, "phrase": "quality-prediction_models"}, {"score": 0.003483120583663528, "phrase": "mfcc-based_measurands"}, {"score": 0.0033883824438527316, "phrase": "nonlinear_model_types"}, {"score": 0.003296212583539166, "phrase": "cross-validation_restrictions"}, {"score": 0.0032287289440915187, "phrase": "detailed_insight"}, {"score": 0.0031845067058084583, "phrase": "model-generalizability_aspects"}, {"score": 0.0031193028729331667, "phrase": "regularized_properties"}, {"score": 0.0030344301563232944, "phrase": "quality_elements"}, {"score": 0.002851790448644913, "phrase": "quality-indicative_effect"}, {"score": 0.0028127163745564777, "phrase": "individual_signal_characteristics"}, {"score": 0.0026986685585382347, "phrase": "perceptual_model_reference"}, {"score": 0.0025892330566358503, "phrase": "semi-supervised_fashion"}, {"score": 0.0025537472239953807, "phrase": "natural_and_synthetic_speech"}, {"score": 0.002416589459425828, "phrase": "instrumental_quality_prediction"}, {"score": 0.0023834640793589435, "phrase": "tts_signals"}, {"score": 0.0023346234464901978, "phrase": "broad_training_material"}, {"score": 0.0022710523337245337, "phrase": "high_prediction_accuracy"}, {"score": 0.0021940118003312397, "phrase": "nonlinear_model_structures"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speech-quality prediction", " Synthetic speech perception", " Text-to-speech", " Non-intrusive quality assessment", " Perceptual regularization"], "paper_abstract": "Instrumental speech-quality prediction for text-to-speech signals is explored in a twofold manner. First, the perceptual quality space of TTS is structured by means of three perceptual quality dimensions which are derived from multiple auditory tests. Second, quality-prediction models are evaluated for each dimension using prosodic and MFCC-based measurands. Linear and nonlinear model types are compared under cross-validation restrictions, giving detailed insight into model-generalizability aspects. Perceptually regularized properties, denoted as quality elements, are introduced in order to encode the quality-indicative effect of individual signal characteristics. These elements integrate a perceptual model reference which is derived in a semi-supervised fashion from natural and synthetic speech. The results highlight the feasibility of instrumental quality prediction for TTS signals provided that broad training material is employed. High prediction accuracy, however, requires nonlinear model structures. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Quality prediction of synthesized speech based on perceptual quality dimensions", "paper_id": "WOS:000348261700002"}