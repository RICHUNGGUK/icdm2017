{"auto_keywords": [{"score": 0.042948433778622036, "phrase": "viewing_conditions"}, {"score": 0.00481495049065317, "phrase": "unrestricted_transformations"}, {"score": 0.00471700544744796, "phrase": "novel_method"}, {"score": 0.004678384952779725, "phrase": "class-based_feature_matching"}, {"score": 0.004221379262804442, "phrase": "similar_part"}, {"score": 0.0039362878637371574, "phrase": "object_images"}, {"score": 0.0036553070575015344, "phrase": "feature's_appearance"}, {"score": 0.00327086925741257, "phrase": "transformed_feature"}, {"score": 0.003204232566468624, "phrase": "approximately_the_same_set"}, {"score": 0.0031132086055691214, "phrase": "consistency_requirement"}, {"score": 0.0030876784650745973, "phrase": "corresponding_features"}, {"score": 0.0029753513692467315, "phrase": "candidate_matches"}, {"score": 0.002938821840519756, "phrase": "previous_approaches"}, {"score": 0.0029027394930867902, "phrase": "proposed_scheme"}, {"score": 0.002878930338561009, "phrase": "feature_appearances"}, {"score": 0.002843581231499026, "phrase": "similar_viewing_conditions"}, {"score": 0.0027856252557826467, "phrase": "different_viewing_conditions"}, {"score": 0.002607965563121546, "phrase": "affine_transformations"}, {"score": 0.0025027144548626975, "phrase": "correct_matches"}, {"score": 0.0024116163689723354, "phrase": "proposed_method"}, {"score": 0.0023819914954155905, "phrase": "dense_set"}, {"score": 0.0023624435546097658, "phrase": "accurate_correspondences"}, {"score": 0.002314271047073693, "phrase": "experimental_comparisons"}, {"score": 0.0022300169122921906, "phrase": "previous_schemes"}, {"score": 0.0021049977753042253, "phrase": "invariant_object_recognition"}], "paper_keywords": ["feature matching", " invariant recognition", " parts"], "paper_abstract": "We develop a novel method for class-based feature matching across large changes in viewing conditions. The method (called MBE) is based on the property that, when objects share a similar part, the similarity is preserved across viewing conditions. Given a feature and a training set of object images, we first identify the subset of objects that share this feature. The transformation of the feature's appearance across viewing conditions is determined mainly by the properties of the feature, rather than of the object in which it is embedded. Therefore, the transformed feature will be shared by approximately the same set of objects. Based on this consistency requirement, corresponding features can be reliably identified from a set of candidate matches. Unlike previous approaches, the proposed scheme compares feature appearances only in similar viewing conditions, rather than across different viewing conditions. As a result, the scheme is not restricted to locally planar objects or affine transformations. The approach also does not require examples of correct matches. We show that, by using the proposed method, a dense set of accurate correspondences can be obtained. Experimental comparisons demonstrate that matching accuracy is significantly improved over previous schemes. Finally, we show that the scheme can be successfully used for invariant object recognition.", "paper_title": "Class-based feature matching across unrestricted transformations", "paper_id": "WOS:000257504400009"}