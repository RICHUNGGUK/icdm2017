{"auto_keywords": [{"score": 0.03487218608518814, "phrase": "brc"}, {"score": 0.01439362157044895, "phrase": "thread_divergence"}, {"score": 0.011598777833104212, "phrase": "input_matrix"}, {"score": 0.00481495049065317, "phrase": "balanced_sparse_matrix-vector_multiplication"}, {"score": 0.004720606036240263, "phrase": "sparse_matrix-vector"}, {"score": 0.004515004397496303, "phrase": "key_operations"}, {"score": 0.004470539706227421, "phrase": "linear_algebra"}, {"score": 0.004361276005140776, "phrase": "load_imbalance"}, {"score": 0.0043183186953102805, "phrase": "un-coalesced_and_indirect_memory_access"}, {"score": 0.003853583126988071, "phrase": "two-dimensional_blocking_mechanism"}, {"score": 0.003507504253087326, "phrase": "nearly_equal_number"}, {"score": 0.003472927015404464, "phrase": "non-zero_elements"}, {"score": 0.0032724821705972357, "phrase": "load_balance"}, {"score": 0.0031453010852936334, "phrase": "constant_number"}, {"score": 0.003068327486877065, "phrase": "different_warps"}, {"score": 0.002862648127577722, "phrase": "brc_performance"}, {"score": 0.002834409847642238, "phrase": "judicious_selection"}, {"score": 0.002806449337145999, "phrase": "block_size"}, {"score": 0.0027650234530228923, "phrase": "sparsity_characteristics"}, {"score": 0.0026839922027139967, "phrase": "cuda_implementation"}, {"score": 0.00263129148531588, "phrase": "nvidia_cusp"}, {"score": 0.002504011301105577, "phrase": "unstructured_sparse_matrices"}, {"score": 0.0024793019587009035, "phrase": "multiple_application_domains"}, {"score": 0.002442693254693416, "phrase": "brc_format"}, {"score": 0.0023245146355917626, "phrase": "petsc's_solvers"}, {"score": 0.0021902063413831545, "phrase": "linear_speedup"}, {"score": 0.002168586864007222, "phrase": "multiple_gpus"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["SpMV", " GPU", " CUDA"], "paper_abstract": "Sparse Matrix-Vector multiplication (SpMV) is one of the key operations in linear algebra. Overcoming thread divergence, load imbalance and un-coalesced and indirect memory access due to sparsity and irregularity are challenges to optimizing SpMV on GPUs. In this paper we present a new Blocked Row-Column (BRC) storage format with a two-dimensional blocking mechanism that addresses these challenges effectively. It reduces thread divergence by reordering and blocking rows of the input matrix with nearly equal number of non-zero elements onto the same execution units (i.e., warps). BRC improves load balance by partitioning rows into blocks with a constant number of non-zeros such that different warps perform the same amount of work. We also present an approach to optimize BRC performance by judicious selection of block size based on sparsity characteristics of the matrix. A CUDA implementation of BRC outperforms NVIDIA CUSP and cuSPARSE libraries and other stateof-the-art SpMV formats on a range of unstructured sparse matrices from multiple application domains. The BRC format has been integrated with PETSc, enabling its use in PETSc's solvers. Furthermore, when partitioning the input matrix, BRC achieves near linear speedup on multiple GPUs. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "A model-driven blocking strategy for load balanced sparse matrix-vector multiplication on GPUs", "paper_id": "WOS:000352117800002"}