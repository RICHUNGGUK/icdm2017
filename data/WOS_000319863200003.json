{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "orchestral_conductor's_baton_movements_using_machine_learning"}, {"score": 0.0047707093928203, "phrase": "telematic_musical_performance"}, {"score": 0.0045765783794477505, "phrase": "networked_audio"}, {"score": 0.004349955692802962, "phrase": "extreme_case"}, {"score": 0.004077616905584829, "phrase": "steadily_decreasing_tempo"}, {"score": 0.0038046452381676967, "phrase": "conductor_video"}, {"score": 0.0037177622939493084, "phrase": "network_latencies"}, {"score": 0.0035663245542174224, "phrase": "alternative_approach"}, {"score": 0.0034368818187904744, "phrase": "distributed_orchestral_performances"}, {"score": 0.0033274664434575136, "phrase": "predicted_version"}, {"score": 0.003281643474564305, "phrase": "conductor's_baton_trajectory"}, {"score": 0.0032364494905485677, "phrase": "prediction_step"}, {"score": 0.0030196181347744372, "phrase": "conventional_machine_learning_techniques"}, {"score": 0.0029234476524900794, "phrase": "particle_filter"}, {"score": 0.00288317238346065, "phrase": "extended_kalman_filter"}, {"score": 0.0027784560880013886, "phrase": "baton's_tip"}, {"score": 0.0027275289970185015, "phrase": "multiple_beats"}, {"score": 0.0026042472337367015, "phrase": "conventional_feature-based_method"}, {"score": 0.0025329623864528317, "phrase": "generic_two-part_framework"}, {"score": 0.0024636239664210433, "phrase": "rehearsal_data"}, {"score": 0.002429668083484833, "phrase": "probabilistic_model"}, {"score": 0.0023522423291484212, "phrase": "live_performance"}, {"score": 0.0022563011263787847, "phrase": "experimental_methodology"}, {"score": 0.002225196379204285, "phrase": "perceptually_based_metrics"}, {"score": 0.002204697943520258, "phrase": "predicted_baton_paths"}, {"score": 0.002154272413276506, "phrase": "perceptual_efficacy"}, {"score": 0.0021245712108461227, "phrase": "presented_methods"}, {"score": 0.0021049977753042253, "phrase": "experimental_confirmation"}], "paper_keywords": [""], "paper_abstract": "Telematic musical performance, in which performers at two or more sites collaborate via networked audio and video, suffers significantly from latency. In the extreme case, performers at all sites slow to match their delayed counterparts, resulting in a steadily decreasing tempo. Introducing video of a conductor does not immediately solve the problem, as conductor video is also subjected to network latencies. This article lays the groundwork for an alternative approach to mitigating the effects of latency in distributed orchestral performances, based on generation of a predicted version of the conductor's baton trajectory. The prediction step is the most fundamental problem in this scheme, for which we propose the use of conventional machine learning techniques. Specifically, we demonstrate a particle filter and an extended Kalman filter that each track the location of the baton's tip and predict it multiple beats into the future; we compare these with a conventional feature-based method. We also describe a generic two-part framework that prescribes the incorporation of rehearsal data into a probabilistic model, which is then adapted during live performance. Finally, we suggest a framework and experimental methodology for establishing perceptually based metrics for predicted baton paths. Note that the perceptual efficacy of the presented methods requires experimental confirmation beyond the scope of this article.", "paper_title": "Predicting an Orchestral Conductor's Baton Movements Using Machine Learning", "paper_id": "WOS:000319863200003"}