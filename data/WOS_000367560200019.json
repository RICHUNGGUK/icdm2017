{"auto_keywords": [{"score": 0.04335738919692907, "phrase": "data_augmentation"}, {"score": 0.022995917257219195, "phrase": "original_training_data"}, {"score": 0.009067799912277375, "phrase": "predictive_power"}, {"score": 0.00481495049065317, "phrase": "modeling_through_data_augmentation"}, {"score": 0.004741845603430361, "phrase": "simulated_unobserved_data_points"}, {"score": 0.004437613070853689, "phrase": "response_variable"}, {"score": 0.004403784208967898, "phrase": "diverse_areas"}, {"score": 0.004336894811284795, "phrase": "spectroscopic_data"}, {"score": 0.0042819270655224916, "phrase": "conserved_domains"}, {"score": 0.004260134699483774, "phrase": "protein_sequences"}, {"score": 0.004089725258435817, "phrase": "qsar_models"}, {"score": 0.004037877545494505, "phrase": "rmse_values"}, {"score": 0.004007083686875716, "phrase": "test_set"}, {"score": 0.003916098692296838, "phrase": "chembl"}, {"score": 0.0034822313837838768, "phrase": "gaussian_noise"}, {"score": 0.002972352083274451, "phrase": "svm"}, {"score": 0.0029121708113952157, "phrase": "morgan"}, {"score": 0.0028026735035370206, "phrase": "factor_levels"}, {"score": 0.002767097123546759, "phrase": "balanced_fixed-effect_full-factorial_experiment"}, {"score": 0.0027041907335656782, "phrase": "increased_predictive_power"}, {"score": 0.002530359917257626, "phrase": "highest_drop"}, {"score": 0.002435190484324197, "phrase": "maximum_increase"}, {"score": 0.002373736426377474, "phrase": "training_data"}, {"score": 0.0022786120364641674, "phrase": "reasonable_trade-off"}, {"score": 0.002261204313587616, "phrase": "increased_performance"}, {"score": 0.0022267859286766553, "phrase": "computational_cost"}, {"score": 0.0022041312401435346, "phrase": "namely_increase"}, {"score": 0.0021049977753042253, "phrase": "training_examples"}], "paper_keywords": [""], "paper_abstract": "Extending the original training data with simulated unobserved data points has proven powerful to increase both the generalization ability of predictive models and their robustness against changes in the structure of data (e.g., systematic drifts in the response variable) in diverse areas such as the analysis of spectroscopic data or the detection of conserved domains in protein sequences. In this contribution, we explore the effect of data augmentation in the predictive power of QSAR models, quantified by the RMSE values on the test set. We collected 8 diverse data sets from the literature and ChEMBL version 19 reporting compound activity as pIC(50) values. The original training data were replicated (i.e., augmented) N times (N is an element of 0, 1, 2, 4, 6, 8, 10), and these replications were perturbed with Gaussian noise (mu = 0, sigma = sigma(noise)) on either (i) the pIC(50) values, (ii) the compound descriptors, (iii) both the compound descriptors and the pIC(50) values, or (iv) none of them. The effect of data augmentation was evaluated across three different algorithms (RE, GBM, and SVM radial) and two descriptor types (Morgan fingerprints and physicochemical-property-based descriptors). The influence of all factor levels was analyzed with a balanced fixed-effect full-factorial experiment. Overall, data augmentation constantly led to increased predictive power on the test set by 10-15%. Injecting noise on (i) compound descriptors or on 60 both compound descriptors and pIC(50) values led to the highest drop of RMSEtest, values (from 0.67-0.72 to 0.60-0.63 pIC(50) units). The maximum increase in predictive power provided by data augmentation is reached when the training data is replicated one time. Therefore, extending the original training data with one perturbed repetition thereof represents a reasonable trade-off between the increased performance of the models and the computational cost of data augmentation, namely increase of (i) model complexity due to the need for optimizing sigma(noise) and (ii) the number of training examples.", "paper_title": "Improved Chemical Structure-Activity Modeling Through Data Augmentation", "paper_id": "WOS:000367560200019"}