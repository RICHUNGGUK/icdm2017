{"auto_keywords": [{"score": 0.034243830874721354, "phrase": "pbl-mcrbfn"}, {"score": 0.014311087003637054, "phrase": "proposed_approach"}, {"score": 0.007836708625301638, "phrase": "subject-independent_study"}, {"score": 0.00481495049065317, "phrase": "independent_human_action_recognition"}, {"score": 0.00478557489890886, "phrase": "spatio-depth_information"}, {"score": 0.0047563776685277314, "phrase": "meta-cognitive_rbf_network"}, {"score": 0.004627168743964783, "phrase": "machine_learning_approach"}, {"score": 0.004598933498016774, "phrase": "subject_independent_human_action_recognition"}, {"score": 0.004339108074589307, "phrase": "flow_information"}, {"score": 0.004044064541716598, "phrase": "depth_image"}, {"score": 0.00399483173553482, "phrase": "depth_flow"}, {"score": 0.0038981499110532307, "phrase": "obtained_flow"}, {"score": 0.003803799017548693, "phrase": "space_time"}, {"score": 0.0037805689375225117, "phrase": "feature_vectors"}, {"score": 0.0035997452281985465, "phrase": "hierarchical_fashion"}, {"score": 0.0035668128900149814, "phrase": "hierarchical_fine"}, {"score": 0.003545024951402818, "phrase": "coarse_windows"}, {"score": 0.003512591457930393, "phrase": "motion_dynamics"}, {"score": 0.003427540517249369, "phrase": "extracted_features"}, {"score": 0.0033651019362620866, "phrase": "meta-cognitive_radial_basis_function_network"}, {"score": 0.0031747618267940155, "phrase": "zero_hidden_neurons"}, {"score": 0.003097865379563127, "phrase": "best_human_learning_strategy"}, {"score": 0.0030043508031729277, "phrase": "meta-cognitive_environment"}, {"score": 0.002913650880082273, "phrase": "pblmcrbfn"}, {"score": 0.002886977102894829, "phrase": "sample_overlapping_conditions"}, {"score": 0.002834357813044087, "phrase": "learning_algorithm"}, {"score": 0.002673957615079041, "phrase": "support_vector_machine"}, {"score": 0.002633273185094898, "phrase": "extreme_learning_machine"}, {"score": 0.0025226117209113737, "phrase": "training_and_testing_datasets"}, {"score": 0.002358013394257883, "phrase": "leave-one-subject-out_strategy"}, {"score": 0.0022519745140925475, "phrase": "mcrbfn"}, {"score": 0.0021441057555396013, "phrase": "video_analytics_lab"}, {"score": 0.0021049977753042253, "phrase": "berkeley_multimodal_human_action_database"}], "paper_keywords": ["Action recognition", " 3-D optical flow", " Kinect depth sensor", " Projection based learning", " Meta-cognition and self-regulated learning"], "paper_abstract": "In this paper, we present a machine learning approach for subject independent human action recognition using depth camera, emphasizing the importance of depth in recognition of actions. The proposed approach uses the flow information of all 3 dimensions to classify an action. In our approach, we have obtained the 2-D optical flow and used it along with the depth image to obtain the depth flow (Z motion vectors). The obtained flow captures the dynamics of the actions in space time. Feature vectors are obtained by averaging the 3-D motion over a grid laid over the silhouette in a hierarchical fashion. These hierarchical fine to coarse windows capture the motion dynamics of the object at various scales. The extracted features are used to train a Meta-cognitive Radial Basis Function Network (McRBFN) that uses a Projection Based Learning (PBL) algorithm, referred to as PBL-McRBFN, henceforth. PBL-McRBFN begins with zero hidden neurons and builds the network based on the best human learning strategy, namely, self-regulated learning in a meta-cognitive environment. When a sample is used for learning, PBLMcRBFN uses the sample overlapping conditions, and a projection based learning algorithm to estimate the parameters of the network. The performance of PBL-McRBFN is compared to that of a Support Vector Machine (SVM) and Extreme Learning Machine (ELM) classifiers with representation of every person and action in the training and testing datasets. Performance study shows that PBL-McRBFN outperforms these classifiers in recognizing actions in 3-D. Further, a subject-independent study is conducted by leave-one-subject-out strategy and its generalization performance is tested. It is observed from the subject-independent study that McRBFN is capable of generalizing actions accurately. The performance of the proposed approach is benchmarked with Video Analytics Lab (VAL) dataset and Berkeley Multimodal Human Action Database (MHAD). (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Subject independent human action recognition using spatio-depth information and meta-cognitive RBF network", "paper_id": "WOS:000325237800002"}