{"auto_keywords": [{"score": 0.039328825554405, "phrase": "iterative_q_function"}, {"score": 0.00481495049065317, "phrase": "neuro-optimal_tracking_control"}, {"score": 0.004762550320451225, "phrase": "stable_iterative_q-learning_algorithm"}, {"score": 0.004608730887154718, "phrase": "new_policy_iteration_q-learning_algorithm"}, {"score": 0.004508942069742167, "phrase": "infinite_horizon_optimal_tracking_problems"}, {"score": 0.004387225575447685, "phrase": "discrete-time_nonlinear_systems"}, {"score": 0.004199247711875386, "phrase": "iterative_adaptive_dynamic_programming"}, {"score": 0.0041535297178886595, "phrase": "adp"}, {"score": 0.0040192915527630995, "phrase": "iterative_tracking_control_law"}, {"score": 0.0038470175104007524, "phrase": "desired_state_trajectory"}, {"score": 0.0036419834462626125, "phrase": "optimal_tracking_problem"}, {"score": 0.0035435867326548665, "phrase": "optimal_regulation_problem"}, {"score": 0.0034478392374444177, "phrase": "q-learning_algorithm"}, {"score": 0.003318108914777263, "phrase": "optimal_control_law"}, {"score": 0.003264010423431402, "phrase": "regulation_system"}, {"score": 0.003175792987190496, "phrase": "arbitrary_admissible_control_law"}, {"score": 0.0031240075437664314, "phrase": "convergence_property"}, {"score": 0.0028150302583265655, "phrase": "optimal_q_function"}, {"score": 0.002679501909303057, "phrase": "iterative_control_laws"}, {"score": 0.0026213749921120623, "phrase": "transformed_nonlinear_system"}, {"score": 0.002454432810862053, "phrase": "iterative_control_law"}, {"score": 0.0023362245884046176, "phrase": "policy_iteration_q-learning_algorithm"}, {"score": 0.002175436093794315, "phrase": "developed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Adaptive dynamic programming", " Approximate", " Dynamic programming", " Q-learning", " Optimal tracking control", " Neural networks"], "paper_abstract": "This paper discusses a new policy iteration Q-learning algorithm to solve the infinite horizon optimal tracking problems for a class of discrete-time nonlinear systems. The idea is to use an iterative adaptive dynamic programming (ADP) technique to construct the iterative tracking control law which makes the system state track the desired state trajectory and simultaneously minimizes the iterative Q function. Via system transformation, the optimal tracking problem is transformed into an optimal regulation problem. The policy iteration Q-learning algorithm is then developed to obtain the optimal control law for the regulation system. Initialized by an arbitrary admissible control law, the convergence property is analyzed. It is shown that the iterative Q function is monotonically non-increasing and converges to the optimal Q function. It is proven that any of the iterative control laws can stabilize the transformed nonlinear system. Two neural networks are used to approximate the iterative Q function and compute the iterative control law, respectively, for facilitating the implementation of policy iteration Q-learning algorithm. Finally, two simulation examples are presented to illustrate the performance of the developed algorithm. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Nonlinear neuro-optimal tracking control via stable iterative Q-learning algorithm", "paper_id": "WOS:000359165000050"}