{"auto_keywords": [{"score": 0.04317179544664411, "phrase": "classification_problems"}, {"score": 0.041851968963554545, "phrase": "boostfs"}, {"score": 0.037413203084667356, "phrase": "decision_stump"}, {"score": 0.03626275334087338, "phrase": "sample_distribution"}, {"score": 0.00481495049065317, "phrase": "learning_process"}, {"score": 0.004662036750452837, "phrase": "fundamental_role"}, {"score": 0.004398872331144531, "phrase": "boosting-based_feature_selection_algorithm"}, {"score": 0.004259165896932762, "phrase": "adaboost"}, {"score": 0.003865826699096508, "phrase": "training_samples"}, {"score": 0.0037188360359702182, "phrase": "uniform_distribution"}, {"score": 0.0031435968684569112, "phrase": "classification_results"}, {"score": 0.0025230952890994236, "phrase": "experimental_results"}, {"score": 0.0024906610581090223, "phrase": "synthetic_datasets"}, {"score": 0.0024586427394152196, "phrase": "five_uci_datasets"}, {"score": 0.002411383506019635, "phrase": "real_malware_detection_dataset"}, {"score": 0.0022897425998475362, "phrase": "boostfs_help"}, {"score": 0.0022457226013538343, "phrase": "learning_algorithms"}, {"score": 0.0021462652605453163, "phrase": "original_feature"}, {"score": 0.0021049977753042253, "phrase": "redundant_features"}], "paper_keywords": ["Feature selection", " AdaBoost", " decision stump"], "paper_abstract": "In a learning process, features play a fundamental role. In this paper, we propose a Boosting-based feature selection algorithm called BoostFS. It extends AdaBoost which is designed for classification problems to feature selection. BoostFS maintains a distribution over training samples which is initialized from the uniform distribution. In each iteration, a decision stump is trained under the sample distribution and then the sample distribution is adjusted so that it is orthogonal to the classification results of all the generated stumps. Because a decision stump can also be regarded as one selected feature, BoostFS is capable to select a subset of features that are irrelevant to each other as much as possible. Experimental results on synthetic datasets, five UCI datasets and a real malware detection dataset all show that the features selected by BoostFS help to improve learning algorithms in classification problems, especially when the original feature set contains redundant features.", "paper_title": "BoostFS: A Boosting-Based Irrelevant Feature Selection Algorithm", "paper_id": "WOS:000361918600004"}