{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mathematical_analysis"}, {"score": 0.004670896353428476, "phrase": "hebbian_learning_rules"}, {"score": 0.00411129159104145, "phrase": "hebbian"}, {"score": 0.004036968488559984, "phrase": "random_recurrent_neural_networks"}, {"score": 0.003939980614608309, "phrase": "generic_hebbian_learning_rule"}, {"score": 0.00386876598761639, "phrase": "passive_forgetting"}, {"score": 0.003822003373331997, "phrase": "different_timescales"}, {"score": 0.0037529131348732715, "phrase": "neuronal_activity"}, {"score": 0.003640516896245813, "phrase": "previous_numerical_work"}, {"score": 0.0035530197642369464, "phrase": "hebbian_learning"}, {"score": 0.003384262486955773, "phrase": "steady_state"}, {"score": 0.0029602552900111407, "phrase": "complex_coupling"}, {"score": 0.0029244418026475832, "phrase": "neuronal_dynamics"}, {"score": 0.0028890603352364273, "phrase": "synaptic_graph_structure"}, {"score": 0.0027854565462509095, "phrase": "jacobian_matrices"}, {"score": 0.0025578967205807843, "phrase": "neural_network_evolution"}, {"score": 0.002406799309495983, "phrase": "learned_pattern"}, {"score": 0.0023346234464901978, "phrase": "largest_lyapunov_exponent"}, {"score": 0.0022101055924375725, "phrase": "neural_networks"}, {"score": 0.0021049977753042253, "phrase": "high_functional_interest"}], "paper_keywords": [""], "paper_abstract": "We present a mathematical analysis of the effects of Hebbian learning in random recurrent neural networks, with a generic Hebbian learning rule, including passive forgetting and different timescales, for neuronal activity and learning dynamics. Previous numerical work has reported that Hebbian learning drives the system from chaos to a steady state through a sequence of bifurcations. Here, we interpret these results mathematically and show that these effects, involving a complex coupling between neuronal dynamics and synaptic graph structure, can be analyzed using Jacobian matrices, which introduce both a structural and a dynamical point of view on neural network evolution. Furthermore, we show that sensitivity to a learned pattern is maximal when the largest Lyapunov exponent is close to 0. We discuss how neural networks may take advantage of this regime of high functional interest.", "paper_title": "A Mathematical Analysis of the Effects of Hebbian Learning Rules on the Dynamics and Structure of Discrete-Time Random Recurrent Neural Networks", "paper_id": "WOS:000260825000003"}