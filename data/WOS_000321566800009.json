{"auto_keywords": [{"score": 0.028719460092936126, "phrase": "sweep-h"}, {"score": 0.010612387000973441, "phrase": "parallel_s"}, {"score": 0.004714023408114857, "phrase": "unstructured_grids"}, {"score": 0.004680851157467745, "phrase": "multi-core_clusters"}, {"score": 0.004631527916092452, "phrase": "particle_transport_simulations"}, {"score": 0.004598933498016774, "phrase": "radiation_effects"}, {"score": 0.004502513902377422, "phrase": "discrete_ordinates"}, {"score": 0.004361644989356558, "phrase": "boltzmann_equation"}, {"score": 0.004300453431933622, "phrase": "ordinate_direction"}, {"score": 0.004165879874212301, "phrase": "radiation_flux"}, {"score": 0.003992950134435965, "phrase": "unstructured_grid"}, {"score": 0.003909185474360582, "phrase": "topological_traversal"}, {"score": 0.0038679618585498597, "phrase": "equivalent_directed_acyclic_graph"}, {"score": 0.003760137090849196, "phrase": "data-driven_algorithm"}, {"score": 0.003681238363808568, "phrase": "mpi_model_results"}, {"score": 0.0036553070575015344, "phrase": "irregular_communication"}, {"score": 0.003629557750546267, "phrase": "massive_short_messages"}, {"score": 0.0035283552859817764, "phrase": "mpi_runtime"}, {"score": 0.003466536738232506, "phrase": "high-end_hpc_cluster_systems"}, {"score": 0.0033937776228549557, "phrase": "standard_processor_configuration"}, {"score": 0.0033579708256295847, "phrase": "single_node"}, {"score": 0.003322540556404882, "phrase": "traditional_data-driven_algorithm"}, {"score": 0.003207108833360533, "phrase": "potential_advantages"}, {"score": 0.0031397776924946526, "phrase": "shared_memory"}, {"score": 0.0029775511937434797, "phrase": "elegant_solution"}, {"score": 0.0029253546782349875, "phrase": "previous_mpi-only_design"}, {"score": 0.002833689453299307, "phrase": "new_design"}, {"score": 0.00281371157349156, "phrase": "data-driven_parallel_s"}, {"score": 0.0027546167254869493, "phrase": "hybrid_mpi"}, {"score": 0.002735194789581153, "phrase": "pthread_programming"}, {"score": 0.0026682872124177233, "phrase": "hierarchical_parallelism"}, {"score": 0.002603012037901889, "phrase": "special_multi-threading_techniques"}, {"score": 0.0025125151618810523, "phrase": "better_load_balance"}, {"score": 0.0024597305738431226, "phrase": "analytical_performance_model"}, {"score": 0.002357457094026928, "phrase": "former_mpi_counterpart"}, {"score": 0.002219790619936024, "phrase": "nearly_linear_scalability"}, {"score": 0.0022041312401435346, "phrase": "moderate_problem_sizes"}, {"score": 0.0021425882815624803, "phrase": "previous_mpi_algorithm"}], "paper_keywords": [""], "paper_abstract": "In particle transport simulations, radiation effects are often described by the discrete ordinates (S (n) ) form of Boltzmann equation. In each ordinate direction, the solution is computed by sweeping the radiation flux across the grid. Parallel S (n) sweep on an unstructured grid can be explicitly modeled as topological traversal through an equivalent directed acyclic graph (DAG), which is a data-driven algorithm. Its traditional design using MPI model results in irregular communication of massive short messages which cannot be efficiently handled by MPI runtime. Meanwhile, in high-end HPC cluster systems, multicore has become the standard processor configuration of a single node. The traditional data-driven algorithm of S (n) sweeps has not exploited potential advantages of multi-threading of multicore on shared memory. These advantages, however, as we shall demonstrate, could provide an elegant solution resolving problems in the previous MPI-only design. In this paper, we give a new design of data-driven parallel S (n) sweeps using hybrid MPI and Pthread programming, namely Sweep-H, to exploit hierarchical parallelism of processes and threads. With special multi-threading techniques and vertex schedule policy, Sweep-H gets more efficient communication and better load balance. We further present an analytical performance model for Sweep-H to reveal why and when it is advantageous over former MPI counterpart. On a 64-node multicore cluster system with 12 cores per node, 768 cores in total, Sweep-H achieves nearly linear scalability for moderate problem sizes, and better absolute performance than the previous MPI algorithm on more than 16 nodes (by up to two times speedup on 64 nodes).", "paper_title": "Optimizing Parallel S (n) Sweeps on Unstructured Grids for Multi-Core Clusters", "paper_id": "WOS:000321566800009"}