{"auto_keywords": [{"score": 0.0473555705346302, "phrase": "logid"}, {"score": 0.04144193814003033, "phrase": "base_classifiers"}, {"score": 0.014412380761034637, "phrase": "dynamic_selection"}, {"score": 0.010099499100349884, "phrase": "new_members"}, {"score": 0.00481495049065317, "phrase": "local_and_global_incremental_learning"}, {"score": 0.0043305354943056875, "phrase": "main_goal"}, {"score": 0.004236539316738291, "phrase": "hidden_markov_model-based_pattern_recognition_systems"}, {"score": 0.004054598617367474, "phrase": "baseline_system"}, {"score": 0.0036731870109004993, "phrase": "test_sample"}, {"score": 0.0035802796531956413, "phrase": "proposed_k-nearest_output_profiles_algorithm"}, {"score": 0.0033397385936467204, "phrase": "previously_unobserved_data"}, {"score": 0.0032552377213294604, "phrase": "incremental_learning"}, {"score": 0.0031845067058084583, "phrase": "local_incremental_learning"}, {"score": 0.0029380187414193653, "phrase": "global_incremental_learning"}, {"score": 0.0028220038404484196, "phrase": "output_profiles"}, {"score": 0.00276066021951467, "phrase": "proposed_framework"}, {"score": 0.0027006464509273806, "phrase": "diversified_set"}, {"score": 0.0025468951021098717, "phrase": "recognition_rates"}, {"score": 0.0025098414374397308, "phrase": "proposed_method"}, {"score": 0.0023496433751182162, "phrase": "batch_learning"}, {"score": 0.002306982970571251, "phrase": "simulated_incremental_learning_setting"}, {"score": 0.0021996478802839316, "phrase": "small_training_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Adaptive systems", " Ensembles of classifiers", " Incremental learning", " Dynamic selection", " Hidden Markov models"], "paper_abstract": "In this work, we propose the LoGID (Local and Global Incremental Learning for Dynamic Selection) framework, the main goal of which is to adapt hidden Markov model-based pattern recognition systems during both the generalization and learning phases. Given that the baseline system is composed of a pool of base classifiers, adaptation during generalization is performed through the dynamic selection of the members of this pool that best recognize each test sample. This is achieved by the proposed K-nearest output profiles algorithm, while adaptation during learning consists of gradually updating the knowledge embedded in the base classifiers, by processing previously unobserved data. This phase employs two types of incremental learning: local and global. Local incremental learning involves updating the pool of base classifiers by adding new members to this set. The new members are created with the Learn++ algorithm. Global incremental learning, in contrast, consists of updating the set of output profiles used during generalization. The proposed framework has been evaluated on a diversified set of databases. The results indicate that LoGID is promising. For most databases, the recognition rates achieved by the proposed method are higher than those achieved by other state-of-the-art approaches, such as batch learning. Furthermore, the simulated incremental learning setting demonstrates that LoGID can effectively improve the performance of systems created with small training sets as more data are observed over time. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "LoGID: An adaptive framework combining local and global incremental learning for dynamic selection of ensembles of HMMs", "paper_id": "WOS:000306091900043"}