{"auto_keywords": [{"score": 0.04854498731652113, "phrase": "sparse_representation"}, {"score": 0.030049536764850712, "phrase": "schg"}, {"score": 0.00481518645811847, "phrase": "hypergraph"}, {"score": 0.00462291551375938, "phrase": "transductive_learning_image_retrieval"}, {"score": 0.004585433265422727, "phrase": "content-based_image_retrieval"}, {"score": 0.0044385053213394175, "phrase": "so-called_semantic_gap"}, {"score": 0.004141663280057689, "phrase": "cbir_systems"}, {"score": 0.00407474190126995, "phrase": "current_qbme_methods"}, {"score": 0.003912112659326054, "phrase": "query_results"}, {"score": 0.003802135020791801, "phrase": "computational_time"}, {"score": 0.003710323357333052, "phrase": "growing_number"}, {"score": 0.003680212979775528, "phrase": "query_examples"}, {"score": 0.0035477010181860376, "phrase": "novel_qbme_method"}, {"score": 0.0035189057079817285, "phrase": "fast_image_retrieval"}, {"score": 0.0034761488481944657, "phrase": "transductive_learning_framework"}, {"score": 0.0033783859957006116, "phrase": "qbme"}, {"score": 0.0032302647179383915, "phrase": "semantic_correlations"}, {"score": 0.0032040376301815544, "phrase": "image_data"}, {"score": 0.0031650944584992726, "phrase": "training_process"}, {"score": 0.002677837900473396, "phrase": "ranking_values"}, {"score": 0.002613103872300205, "phrase": "pre-learned_semantic_correlations"}, {"score": 0.002539550974361945, "phrase": "multiple_probing_strategy"}, {"score": 0.0024781515864648242, "phrase": "multiple_query_examples"}, {"score": 0.002438043578514396, "phrase": "traditional_qbme_methods"}, {"score": 0.0022561982694417116, "phrase": "experimental_results"}, {"score": 0.00220163445285539, "phrase": "proposed_method"}, {"score": 0.0021748483128942687, "phrase": "retrieval_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Content-based image retrieval", " Hypergraph", " Sparse representation", " Query by multiple examples", " Ranking algorithm"], "paper_abstract": "Content-based image retrieval (CBIR) always suffers from the so-called semantic gap. Query-By-Multiple-Examples (QBME) is introduced to bridge it and applied in a lot of CBIR systems. However, current QBME methods usually query with each example separately and combine the query results. In this way, the computational time will increase linearly with the growing number of query examples. In this paper, we propose a novel QBME method for fast image retrieval based on transductive learning framework. To improve the speed of QBME, we introduce two improvements. First, we explore the semantic correlations of image data in the training process. These correlations are learned using sparse representation. With the semantic correlations, semantic correlation hypergraph (SCHG) is constructed to model the images and their correlations. The construction of SCHG is free of any parameter. After constructing SCHG, we predict the ranking values of images by using the pre-learned semantic correlations. Second, we propose a multiple probing strategy to rank the images with multiple query examples. Different from traditional QBME methods which accept one input example at a time, all the input examples are processed at the same time in this strategy. The experimental results demonstrate the effectiveness of the proposed method on both retrieval performance and speed. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Hypergraph-based multi-example ranking with sparse representation for transductive learning image retrieval", "paper_id": "WOS:000312171100012"}