{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "context-centric_speech-based_human-computer_interaction"}, {"score": 0.004616560363089716, "phrase": "dialog_management"}, {"score": 0.004460309246062286, "phrase": "quantitative_method"}, {"score": 0.004359078118356132, "phrase": "contextual_cues"}, {"score": 0.0042928648367867835, "phrase": "speech-based_human-computer_interactions"}, {"score": 0.004084510791138056, "phrase": "human_input"}, {"score": 0.00402245126752142, "phrase": "spoken_speech"}, {"score": 0.003961330917648707, "phrase": "machine's_understanding"}, {"score": 0.003916098498242888, "phrase": "user's_intent"}, {"score": 0.0038125469099459905, "phrase": "adequate_reaction"}, {"score": 0.0036553070575015344, "phrase": "context-centric_approach"}, {"score": 0.003531474887552889, "phrase": "primary_basis"}, {"score": 0.0034118234596993836, "phrase": "embodied_conversation_agent"}, {"score": 0.0033599501775954024, "phrase": "seamless_engagement"}, {"score": 0.0033215617190376565, "phrase": "speech-based_information-deployment_entity"}, {"score": 0.003221316290858497, "phrase": "dialog_manager"}, {"score": 0.002949598537993663, "phrase": "text_translations"}, {"score": 0.002860546817196148, "phrase": "asr_systems"}, {"score": 0.0028278480051602355, "phrase": "recognition_rate"}, {"score": 0.002700738055025905, "phrase": "contextual_assistance"}, {"score": 0.0026698611202305715, "phrase": "dialog_system"}, {"score": 0.0026191799769765085, "phrase": "speech-based_embodied_conversation_agent_platform"}, {"score": 0.0023435920614377306, "phrase": "emphasized_use"}, {"score": 0.0022382002476510573, "phrase": "empirical_evidence"}, {"score": 0.0021956951486095805, "phrase": "conversational_context"}, {"score": 0.0021789194955896124, "phrase": "speech-based_human-computer_interaction"}, {"score": 0.0021539955106204354, "phrase": "field-tested_context-centric_dialog_manager"}, {"score": 0.0021049977753042253, "phrase": "wiley_periodicals"}], "paper_keywords": [""], "paper_abstract": "This paper describes research that addresses the problem of dialog management from a strong, context-centric approach. We further present a quantitative method of measuring the importance of contextual cues when dealing with speech-based human-computer interactions. It is generally accepted that using context in conjunction with a human input, such as spoken speech, enhances a machine's understanding of the user's intent as a means to pinpoint an adequate reaction. For this work, however, we present a context-centric approach in which the use of context is the primary basis for understanding and not merely an auxiliary process. We employ an embodied conversation agent that facilitates the seamless engagement of a speech-based information-deployment entity by its human end user. This dialog manager emphasizes the use of context to drive its mixed-initiative discourse model. Atypical, modern automatic speech recognizer (ASR) was incorporated to handle the speech-to-text translations. As is the nature of these ASR systems, the recognition rate is consistently less than perfect, thus emphasizing the need for contextual assistance. The dialog system was encapsulated into a speech-based embodied conversation agent platform for prototyping and testing purposes. Experiments were performed to evaluate the robustness of its performance, namely through measures of naturalness and usefulness, with respect to the emphasized use of context. The contribution of this work is to provide empirical evidence of the importance of conversational context in speech-based human-computer interaction using a field-tested context-centric dialog manager. (C) 2013 Wiley Periodicals, Inc.", "paper_title": "Context-Centric Speech-Based Human-Computer Interaction", "paper_id": "WOS:000322579900005"}