{"auto_keywords": [{"score": 0.04219097367734989, "phrase": "generalization_error"}, {"score": 0.009739344801982353, "phrase": "model_parameters"}, {"score": 0.005600156545888985, "phrase": "optimization_quality"}, {"score": 0.005201025336905623, "phrase": "computational_cost"}, {"score": 0.00481495049065317, "phrase": "shrinkage_parameters"}, {"score": 0.0047267100475487595, "phrase": "regularized_subspace_information_criterion"}, {"score": 0.004583205112737672, "phrase": "higher_level"}, {"score": 0.004527024978801684, "phrase": "generalization_capability"}, {"score": 0.004471530397729526, "phrase": "supervised_learning"}, {"score": 0.0033251716926666437, "phrase": "standard_procedure"}, {"score": 0.0032641439000713306, "phrase": "parameter_optimization"}, {"score": 0.003145417398038261, "phrase": "finite_set"}, {"score": 0.0030686679245324837, "phrase": "model_parameter_values"}, {"score": 0.0021710766932077972, "phrase": "optimal_model_parameter_value"}, {"score": 0.0021049977753042253, "phrase": "infinitely_many_candidates"}], "paper_keywords": ["supervised learning", " generalization capability", " model selection", " shrinkage estimator", " regularized subspace information criterion"], "paper_abstract": "For obtaining a higher level of generalization capability in supervised learning, model parameters should be optimized, i.e., they should be determined in such a way that the generalization error is minimized. However, since the generalization error is inaccessible in practice, model parameters are usually determined in such a way that an estimate, of the generalization error is minimized. A standard procedure for model parameter optimization is to first prepare a finite set of candidates of model parameter values, estimate the generalization error for each candidate, and then choose the best one from the candidates. If the number of candidates is increased in this procedure, the optimization quality may be improved. However, this in turn increases the computational cost. In this paper, we give methods for analytically finding the optimal model parameter value from a set of infinitely many candidates. This maximally enhances the optimization quality while the computational cost is kept reasonable.", "paper_title": "Analytic optimization of shrinkage parameters based on regularized subspace information criterion", "paper_id": "WOS:000239577400015"}