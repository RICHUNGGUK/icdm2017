{"auto_keywords": [{"score": 0.05007852962010534, "phrase": "average_precision"}, {"score": 0.011220317856561213, "phrase": "incomplete_judgments"}, {"score": 0.009090528600995032, "phrase": "ap"}, {"score": 0.007380223013078608, "phrase": "incomplete_relevance_judgments"}, {"score": 0.005375247001658961, "phrase": "relevance_judgments"}, {"score": 0.004621671570727774, "phrase": "retrieval_systems"}, {"score": 0.004527949864573469, "phrase": "buckley"}, {"score": 0.004497130085954818, "phrase": "voorhees"}, {"score": 0.0044512920090463105, "phrase": "standard_measures"}, {"score": 0.0044209926093642235, "phrase": "retrieval_performance"}, {"score": 0.0042579795015449005, "phrase": "new_measure"}, {"score": 0.004048809915975478, "phrase": "complete_relevance_judgments"}, {"score": 0.0032728171827367068, "phrase": "statistical_estimates"}, {"score": 0.003195414285736136, "phrase": "random_subset"}, {"score": 0.003173636240475185, "phrase": "complete_judgments"}, {"score": 0.0031305230217636495, "phrase": "natural_scenarios"}, {"score": 0.0030985717040741875, "phrase": "highly_incomplete_judgments"}, {"score": 0.003066945490483524, "phrase": "random_judgment_sets"}, {"score": 0.002777341265639153, "phrase": "trec_data"}, {"score": 0.002364777610365689, "phrase": "inferred_ap"}, {"score": 0.0022007808283071133, "phrase": "detailed_analysis"}], "paper_keywords": ["evaluation", " incomplete judgments", " robustness", " average precision", " bpref", " infAP"], "paper_abstract": "We consider the problem of evaluating retrieval systems with incomplete relevance judgments. Recently, Buckley and Voorhees showed that standard measures of retrieval performance are not robust to incomplete judgments, and they proposed a new measure, bpref, that is much more robust to incomplete judgments. Although bpref is highly correlated with average precision when the judgments are effectively complete, the value of bpref deviates from average precision and from its own value as the judgment set degrades, especially at very low levels of assessment. In this work, we propose three new evaluation measures induced AP, subcollection AP, and inferred AP that are equivalent to average precision when the relevance judgments are complete and that are statistical estimates of average precision when relevance judgments are a random subset of complete judgments. We consider natural scenarios which yield highly incomplete judgments such as random judgment sets or very shallow depth pools. We compare and contrast the robustness of the three measures proposed in this work with bpref for both of these scenarios. Through the use of TREC data, we demonstrate that these measures are more robust to incomplete relevance judgments than bpref, both in terms of how well the measures estimate average precision (as measured with complete relevance judgments) and how well they estimate themselves (as measured with complete relevance judgments). Finally, since inferred AP is the most accurate approximation to average precision and the most robust measure in the presence of incomplete judgments, we provide a detailed analysis of this measure, both in terms of its behavior in theory and its implementation in practice.", "paper_title": "Estimating average precision when judgments are incomplete", "paper_id": "WOS:000258064200003"}