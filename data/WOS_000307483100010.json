{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "regression_function"}, {"score": 0.003961330917648707, "phrase": "joint_penalized_optimization_criterion"}, {"score": 0.0038271712887391015, "phrase": "log-spline_density_estimation"}, {"score": 0.0037402558657576124, "phrase": "spline-based_regression_methods"}, {"score": 0.0031121406603482112, "phrase": "high_dimensional_covariate_space"}, {"score": 0.00283870598459543, "phrase": "robustification_effect"}, {"score": 0.0023889533502525527, "phrase": "practical_examples"}, {"score": 0.002255431275670053, "phrase": "appropriate_evaluation_criterion"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Boosting", " Density estimation", " Functional data analysis", " Robust estimates"], "paper_abstract": "This paper considers a problem of jointly estimating a regression function and the distribution of residuals when both are specified non-parametrically. We present a joint penalized optimization criterion that combines log-spline density estimation with spline-based regression methods. We also examine the use of boosting methodology to estimate a regression function over a high dimensional covariate space. We demonstrate that our method has a robustification effect, and show its usefulness in diagnosing problems in data. We illustrate our methods with practical examples when likelihood is an appropriate evaluation criterion. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Learned-loss boosting", "paper_id": "WOS:000307483100010"}