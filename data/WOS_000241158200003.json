{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "linear_hash_file"}, {"score": 0.004600596158672458, "phrase": "bulk_loading"}, {"score": 0.004282787299993434, "phrase": "good_hash_function"}, {"score": 0.004092026656771699, "phrase": "random_locations"}, {"score": 0.003859146369720225, "phrase": "random_disk_access"}, {"score": 0.0033658327434624457, "phrase": "bulk_loading_algorithm"}, {"score": 0.0032792555200980783, "phrase": "random_disk_accesses"}, {"score": 0.0032157824854214313, "phrase": "multiple_accesses"}, {"score": 0.0030924870122337905, "phrase": "single_access"}, {"score": 0.0022177498761722773, "phrase": "berkeley_db_load_utility"}, {"score": 0.00214659338413584, "phrase": "running_time"}], "paper_keywords": [""], "paper_abstract": "We study the problem of bulk loading a linear hash file; the problem is that a good hash function is able to distribute records into random locations in the file; however, performing a random disk access for each record can be costly and this cost increases with the size of the file. We propose a bulk loading algorithm that can avoid random disk accesses by reducing multiple accesses to the same location into a single access and reordering the accesses such that the pages are accessed sequentially. Our analysis shows that our algorithm is near-optimal with a cost roughly equal to the cost of sorting the dataset, thus the algorithm can scale up to very large datasets. Our experiments show that our method can improve upon the Berkeley DB load utility, in terms of running time, by two orders of magnitude and the improvements scale up well with the size of the dataset.", "paper_title": "Bulk loading a linear hash file", "paper_id": "WOS:000241158200003"}