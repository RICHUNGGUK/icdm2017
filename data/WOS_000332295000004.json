{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "classifications_problems"}, {"score": 0.00399881984083004, "phrase": "strong_experimental_results"}, {"score": 0.003712362078915926, "phrase": "effective_and_open_framework"}, {"score": 0.003199345207727474, "phrase": "lower_classification_error"}, {"score": 0.0031113369425402287, "phrase": "original_versions"}, {"score": 0.0025121156016001886, "phrase": "major_theoretical_issues"}, {"score": 0.0023537101705489957, "phrase": "main_branches"}, {"score": 0.0021049977753042253, "phrase": "interesting_research_directions"}], "paper_keywords": [""], "paper_abstract": "Bagging and boosting are two of the most well-known ensemble learning methods due to their theoretical performance guarantees and strong experimental results. Since bagging and boosting are an effective and open framework, several researchers have proposed their variants, some of which have turned out to have lower classification error than the original versions. This paper tried to summarize these variants and categorize them into groups. We hope that the references cited cover the major theoretical issues, and provide access to the main branches of the literature dealing with such methods, guiding the researcher in interesting research directions.", "paper_title": "Bagging and boosting variants for handling classifications problems: a survey", "paper_id": "WOS:000332295000004"}