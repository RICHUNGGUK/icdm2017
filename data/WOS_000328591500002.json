{"auto_keywords": [{"score": 0.044010507984638604, "phrase": "web_sources"}, {"score": 0.013870057296031014, "phrase": "action_database"}, {"score": 0.00481495049065317, "phrase": "relevant_video_shots"}, {"score": 0.004774530554776608, "phrase": "specific_actions"}, {"score": 0.004734448316824488, "phrase": "web_data"}, {"score": 0.004694700982410345, "phrase": "video_sharing_websites"}, {"score": 0.004596779715291598, "phrase": "tremendous_video_source"}, {"score": 0.004296889776409743, "phrase": "action_recognition_field"}, {"score": 0.003965975628742746, "phrase": "recognition_database"}, {"score": 0.003818210967854446, "phrase": "extensive_human_efforts"}, {"score": 0.0037861265016041813, "phrase": "manual_selection"}, {"score": 0.0037543106253910313, "phrase": "video_parts"}, {"score": 0.0037070855410818986, "phrase": "specified_actions"}, {"score": 0.003568932067237836, "phrase": "novel_method"}, {"score": 0.0035091888798162176, "phrase": "video_shots"}, {"score": 0.0034359094594153304, "phrase": "web_videos"}, {"score": 0.0033641551050434663, "phrase": "visual_features"}, {"score": 0.0032800186243040663, "phrase": "relevant_videos"}, {"score": 0.0032524418285459324, "phrase": "tagged_web_videos"}, {"score": 0.003078740573398959, "phrase": "selected_videos"}, {"score": 0.002735392281320413, "phrase": "web_images"}, {"score": 0.0027123819773059127, "phrase": "human_pose_matching_method"}, {"score": 0.0026782279715171866, "phrase": "ranking_step"}, {"score": 0.0025137856138000014, "phrase": "unsupervised_method"}, {"score": 0.0024302416436559867, "phrase": "action_keywords"}, {"score": 0.002389514188569084, "phrase": "surf_wave"}, {"score": 0.0023594160789762227, "phrase": "\"bake_bread"}, {"score": 0.002271372252250557, "phrase": "large-scale_experiments"}, {"score": 0.0022238834225283594, "phrase": "human_actions"}, {"score": 0.0021866066626507028, "phrase": "non-human_actions"}, {"score": 0.0021590588269751816, "phrase": "promising_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Web video", " Web image", " Tag relevance", " Visual Rank", " Spatio-temporal feature", " Pose estimation"], "paper_abstract": "Video sharing websites have recently become a tremendous video source, which is easily accessible without any costs. This has encouraged researchers in the action recognition field to construct action database exploiting Web sources. However Web sources are generally too noisy to be used directly as a recognition database. Thus building action database from Web sources has required extensive human efforts on manual selection of video parts related to specified actions. In this paper, we introduce a novel method to automatically extract video shots related to given action keywords from Web videos according to their metadata and visual features. First, we select relevant videos among tagged Web videos based on the relevance between their tags and the given keyword. After segmenting selected videos into shots, we rank these shots exploiting their visual features in order to obtain shots of interest as top ranked shots. Especially, we propose to adopt Web images and human pose matching method in shot ranking step and show that this application helps to boost more relevant shots to the top. This unsupervised method of ours only requires the provision of action keywords such as \"surf wave\" or \"bake bread\" at the beginning. We have made large-scale experiments on various kinds of human actions as well as non-human actions and obtained promising results. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Automatic extraction of relevant video shots of specific actions exploiting Web data", "paper_id": "WOS:000328591500002"}