{"auto_keywords": [{"score": 0.03008382743364908, "phrase": "mid-level_features"}, {"score": 0.02160081916953188, "phrase": "video_words"}, {"score": 0.01025641493251647, "phrase": "human_actions"}, {"score": 0.007344960725378478, "phrase": "spatiotemporal_features"}, {"score": 0.006931546876292521, "phrase": "video_word"}, {"score": 0.00481495049065317, "phrase": "semantic_features"}, {"score": 0.004789599930639501, "phrase": "action_recognition"}, {"score": 0.004764382201533623, "phrase": "diffusion_maps"}, {"score": 0.004739296614044257, "phrase": "efficient_modeling"}, {"score": 0.004519360688525621, "phrase": "spatiotemporal_interest_points"}, {"score": 0.0038877510526435437, "phrase": "data_points"}, {"score": 0.0038672631978909256, "phrase": "different_levels"}, {"score": 0.0037071854953981435, "phrase": "learning_framework"}, {"score": 0.003629639728243997, "phrase": "principled_approach"}, {"score": 0.0035914750611347164, "phrase": "semantic_vocabulary"}, {"score": 0.0035631141969666782, "phrase": "large_amount"}, {"score": 0.003525646552423009, "phrase": "diffusion_maps_embedding"}, {"score": 0.0034793636230383367, "phrase": "flat_vocabularies"}, {"score": 0.003451885004220209, "phrase": "traditional_methods"}, {"score": 0.0033886063545402216, "phrase": "hierarchical_nature"}, {"score": 0.0033707400151191877, "phrase": "feature_vocabularies"}, {"score": 0.0033002085989689875, "phrase": "interest_points"}, {"score": 0.003256875433971246, "phrase": "lowest_level"}, {"score": 0.0030728486951690876, "phrase": "pointwise_mutual_information"}, {"score": 0.0030085317219673046, "phrase": "training_video_clips"}, {"score": 0.0029533563706836954, "phrase": "mid-level_feature"}, {"score": 0.0029222817819040195, "phrase": "highest_level"}, {"score": 0.0028086279821223008, "phrase": "semantically_meaningful_distance_measures"}, {"score": 0.002728093290659738, "phrase": "similar_video_sources"}, {"score": 0.002526643766431107, "phrase": "diffusion_distance"}, {"score": 0.0024606763022893214, "phrase": "underlying_idea"}, {"score": 0.002409141581517314, "phrase": "lower-dimensional_space"}, {"score": 0.0023215332766056526, "phrase": "supervised_vocabulary_construction_approaches"}, {"score": 0.0023031776036503324, "phrase": "unsupervised_methods"}, {"score": 0.002272984416250363, "phrase": "lda"}, {"score": 0.0022371036986856777, "phrase": "local_relationship"}, {"score": 0.002161450631294569, "phrase": "diverse_datasets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Action recognition", " Bag of video words", " Semantic visual vocabulary", " Diffusion Maps", " Pointwise Mutual Information"], "paper_abstract": "Efficient modeling of actions is critical for recognizing human actions. Recently, bag of video words (BoVW) representation, in which features computed around spatiotemporal interest points are quantized into video words based on their appearance similarity, has been widely and successfully explored. The performance of this representation however, is highly sensitive to two main factors: the granularity, and therefore, the size of vocabulary, and the space in which features and words are clustered, i.e., the distance measure between data points at different levels of the hierarchy. The goal of this paper is to propose a representation and learning framework that addresses both these limitations. We present a principled approach to learning a semantic vocabulary from a large amount of video words using Diffusion Maps embedding. As opposed to flat vocabularies used in traditional methods, we propose to exploit the hierarchical nature of feature vocabularies representative of human actions. Spatiotemporal features computed around interest points in videos form the lowest level of representation. Video words are then obtained by clustering those spatiotemporal features. Each video word is then represented by a vector of Pointwise Mutual Information (PMI) between that video word and training video clips, and is treated as a mid-level feature. At the highest level of the hierarchy, our goal is to further cluster the mid-level features, while exploiting semantically meaningful distance measures between them. We conjecture that the mid-level features produced by similar video sources (action classes) must lie on a certain manifold. To capture the relationship between these features, and retain it during clustering, we propose to use diffusion distance as a measure of similarity between them. The underlying idea is to embed the mid-level features into a lower-dimensional space, so as to construct a compact yet discriminative, high level vocabulary. Unlike some of the supervised vocabulary construction approaches and the unsupervised methods such as pLSA and LDA, Diffusion Maps can capture local relationship between the mid-level features on the manifold. We have tested our approach on diverse datasets and have obtained very promising results. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "Learning semantic features for action recognition via diffusion maps", "paper_id": "WOS:000299800000006"}