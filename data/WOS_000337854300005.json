{"auto_keywords": [{"score": 0.04893638421315736, "phrase": "process_capability"}, {"score": 0.022151029227794137, "phrase": "vision_calibration"}, {"score": 0.009593685532601062, "phrase": "remote_site"}, {"score": 0.004815005551074787, "phrase": "integrated"}, {"score": 0.004185289462328125, "phrase": "robotic_assembly_operations"}, {"score": 0.0038924989779389424, "phrase": "robot_positioning_accuracy"}, {"score": 0.0037267132967205136, "phrase": "vision_camera"}, {"score": 0.0035679633126097115, "phrase": "calibration_accuracy"}, {"score": 0.0035336109352157763, "phrase": "vision-guided_robot_system"}, {"score": 0.0034159525504642656, "phrase": "high_precision_assembly_robots"}, {"score": 0.0033667304423940893, "phrase": "excellent_positioning_accuracies"}, {"score": 0.003334309053314909, "phrase": "normal_working_conditions"}, {"score": 0.00328625950643742, "phrase": "imperfect_mathematical_conversion"}, {"score": 0.00320770578058916, "phrase": "robot_coordinates"}, {"score": 0.0031614748814351823, "phrase": "accuracy_problems"}, {"score": 0.003056169480391946, "phrase": "novel_vision_calibration_method"}, {"score": 0.002954361295811154, "phrase": "inherent_complications"}, {"score": 0.0029117710453690827, "phrase": "lens_distortions"}, {"score": 0.002801166393333128, "phrase": "lens_distortion"}, {"score": 0.0027209713296448296, "phrase": "vision_field"}, {"score": 0.002630299922281955, "phrase": "non-uniform_distortion"}, {"score": 0.002592369213499121, "phrase": "single_mathematical_equation"}, {"score": 0.00249386680748172, "phrase": "proposed_methodology"}, {"score": 0.002446024708601971, "phrase": "positioning_accuracy"}, {"score": 0.002241816883550397, "phrase": "today's_global_manufacturing_companies"}, {"score": 0.0022094759531152072, "phrase": "fast_product_cycles"}, {"score": 0.0021881743793875767, "phrase": "geographically_distributed_production_lines"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["New calibration methods", " Vision guidance", " Remote control", " Process capability", " Networked robotics"], "paper_abstract": "This study aims at jointly controlling two critical process parameters from a remote site, of which include the process capability of robotic assembly operations and the accuracy of vision calibration. The process capability is regarded as the indication of robot positioning accuracy. When the robot is driven by the vision camera, the process capability becomes mainly dependent on the calibration accuracy of vision-guided robot system. Even though newly commissioned, high precision assembly robots typically display excellent positioning accuracies under normal working conditions, the imperfect mathematical conversion of vision coordinates into robot coordinates imparts the accuracy problems. In this study, a novel vision calibration method is proposed that effectively rectifies the inherent complications associated with lens distortions. Our analysis shows that the degree of lens distortion appears very differently along the vision field of view. Because of this non-uniform distortion, a single mathematical equation for vision calibration is deemed ineffective. The proposed methodology significantly improves the positioning accuracy, which can be performed over the network from a remote site. This is better suited for today's global manufacturing companies, where fast product cycles and geographically distributed production lines dictate more efficient and effective quality control strategies. (c) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Integrated remote control of the process capability and the accuracy of vision calibration", "paper_id": "WOS:000337854300005"}