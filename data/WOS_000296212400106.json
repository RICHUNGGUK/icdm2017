{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "extreme_learning_machine"}, {"score": 0.03329471463625431, "phrase": "hidden_nodes"}, {"score": 0.004747429622531595, "phrase": "regression_problems"}, {"score": 0.0046152575988437465, "phrase": "elm"}, {"score": 0.004486640769041646, "phrase": "new_learning_algorithm"}, {"score": 0.0044237033788643715, "phrase": "single-hidden_layer_feedforward_networks"}, {"score": 0.004240116703465252, "phrase": "huang_et_al"}, {"score": 0.003978866233891427, "phrase": "lower_computational_cost"}, {"score": 0.003629557750546267, "phrase": "high-dimensional_space"}, {"score": 0.0034057975804778293, "phrase": "elm_networks"}, {"score": 0.003334309053314909, "phrase": "regularized_regression_methods"}, {"score": 0.0032184697893776052, "phrase": "suitable_number"}, {"score": 0.0030847461428168614, "phrase": "network_architecture"}, {"score": 0.0029775511937434797, "phrase": "initial_large_number"}, {"score": 0.0028944757265206332, "phrase": "irrelevant_nodes"}, {"score": 0.002793874145713024, "phrase": "ridge_regression"}, {"score": 0.0027546167254869493, "phrase": "elastic_net"}, {"score": 0.002603012037901889, "phrase": "architectural_design"}, {"score": 0.0025664295665392203, "phrase": "elm_network"}, {"score": 0.0024772013657695896, "phrase": "empirical_studies"}, {"score": 0.0023079225317714793, "phrase": "compact_networks"}, {"score": 0.002259426425685765, "phrase": "competitive_results"}, {"score": 0.002196342933346204, "phrase": "elm_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Regularization", " Extreme learning machine", " Regression", " Artificial neural networks"], "paper_abstract": "Extreme learning machine (ELM) is a new learning algorithm for single-hidden layer feedforward networks (SLFNs) proposed by Huang et al. [1]. Its main advantage is the lower computational cost, which is especially relevant when dealing with many patterns defined in a high-dimensional space. This paper proposes an algorithm for pruning ELM networks by using regularized regression methods, thus obtaining a suitable number of the hidden nodes in the network architecture. Beginning from an initial large number of hidden nodes, irrelevant nodes are then pruned using ridge regression, elastic net and lasso methods; hence, the architectural design of ELM network can be automated. Empirical studies on several commonly used regression benchmark problems show that the proposed approach leads to compact networks that generate competitive results compared with the ELM algorithm. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Regularized extreme learning machine for regression problems", "paper_id": "WOS:000296212400106"}