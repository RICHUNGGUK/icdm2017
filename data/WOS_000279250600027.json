{"auto_keywords": [{"score": 0.04557122035267361, "phrase": "visual_words"}, {"score": 0.015574423535694304, "phrase": "scene_classification"}, {"score": 0.00481495049065317, "phrase": "semantic_visual_words"}, {"score": 0.004301834610818624, "phrase": "unsupervised_manner"}, {"score": 0.004104456271230213, "phrase": "similar_appearances"}, {"score": 0.004047020753789236, "phrase": "distinct_semantic_concepts"}, {"score": 0.003934540154108188, "phrase": "novel_supervised_learning_framework"}, {"score": 0.003825173769489887, "phrase": "full_advantage"}, {"score": 0.003789395499639015, "phrase": "label_information"}, {"score": 0.0036324746826127997, "phrase": "gaussian_mixture"}, {"score": 0.0034657012607680173, "phrase": "\"semantic_interpretation"}, {"score": 0.0033851984021896287, "phrase": "scene_labels"}, {"score": 0.003291051334991867, "phrase": "probability_density"}, {"score": 0.0032449610741097992, "phrase": "low-level_visual_features_space"}, {"score": 0.003110515172439764, "phrase": "posterior_scene"}, {"score": 0.003095923814666088, "phrase": "semantic_concepts_probabilities"}, {"score": 0.0030239842475439814, "phrase": "information_bottleneck"}, {"score": 0.0028715299874549245, "phrase": "\"visual_words"}, {"score": 0.002818013056816282, "phrase": "supervised_manner"}, {"score": 0.0027395968397212053, "phrase": "semantic_interpretations"}, {"score": 0.0026633568650501873, "phrase": "semantic_information"}, {"score": 0.0024818877663889813, "phrase": "corresponding_visual_words"}, {"score": 0.002301896758256577, "phrase": "scene_categorization_task"}, {"score": 0.002259042870959357, "phrase": "svm"}, {"score": 0.0021857667493036786, "phrase": "challenging_dataset_show"}, {"score": 0.0021551222226685648, "phrase": "proposed_visual_words"}, {"score": 0.0021249064184347658, "phrase": "scene_classification_task"}], "paper_keywords": ["scene classification", " information bottleneck", " Gaussian mixture modeling", " semantic visual words", " semantic interpretation"], "paper_abstract": "Bag-of-Visual-Words representation has recently become popular for scene classification. However, learning the visual words in an unsupervised manner suffers from the problem when faced these patches with similar appearances corresponding to distinct semantic concepts. This paper proposes a novel supervised learning framework, which aims at taking full advantage of label information to address the problem. Specifically, the Gaussian Mixture Modeling (GMM) is firstly applied to obtain \"semantic interpretation\" of patches using scene labels. Each scene induces a probability density on the low-level visual features space, and patches are represented as vectors of posterior scene semantic concepts probabilities. And then the Information Bottleneck (IB) algorithm is introduce to cluster the patches into \"visual words\" via a supervised manner, from the perspective of semantic interpretations. Such operation can maximize the semantic information of the visual words. Once obtained the visual words, the appearing frequency of the corresponding visual words in a given image forms a histogram, which can be subsequently used in the scene categorization task via the Support Vector Machine (SVM) classifier. Experiments on a challenging dataset show that the proposed visual words better perform scene classification task than most existing methods.", "paper_title": "Discriminating Semantic Visual Words for Scene Classification", "paper_id": "WOS:000279250600027"}