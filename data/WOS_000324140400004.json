{"auto_keywords": [{"score": 0.0426444043108344, "phrase": "relevance_labels"}, {"score": 0.00481495049065317, "phrase": "calibrated_multi-class_classifiers"}, {"score": 0.0045843245695654875, "phrase": "ranking_function"}, {"score": 0.004509919457793852, "phrase": "gold_standard_partial_ordering"}, {"score": 0.004138575034269399, "phrase": "partial_ordering"}, {"score": 0.0038603905208013482, "phrase": "absolute_scale"}, {"score": 0.0036903962975894634, "phrase": "standard_multi-class_classifiers"}, {"score": 0.003372460634140967, "phrase": "multi-class_classifiers"}, {"score": 0.003290591812800382, "phrase": "regression_losses"}, {"score": 0.003210704011271791, "phrase": "bayes-scoring_function"}, {"score": 0.003158522476399068, "phrase": "normalized_discounted_cumulative_gain"}, {"score": 0.0030692309090066166, "phrase": "third_step"}, {"score": 0.002994701080666466, "phrase": "best_multi-class_hyperparameters"}, {"score": 0.0028627183712016894, "phrase": "simple_ensemble_scheme"}, {"score": 0.0027931892160452513, "phrase": "substantial_contribution"}, {"score": 0.0027141975147496264, "phrase": "existing_learning"}, {"score": 0.002605201547405916, "phrase": "available_large-scale_benchmark_data_sets"}, {"score": 0.0025315126622525424, "phrase": "ndcg_score"}, {"score": 0.0024398167078355224, "phrase": "conceptually_more_complex_listwise"}, {"score": 0.0023322317017275803, "phrase": "data_size"}, {"score": 0.0022849068677290836, "phrase": "technical_contribution"}, {"score": 0.002220257443705344, "phrase": "confusing_results"}, {"score": 0.002157433262687853, "phrase": "evaluation_tools"}, {"score": 0.0021049977753042253, "phrase": "future_studies"}], "paper_keywords": ["Learning-to-rank", " Multi-class classification", " Class Probability Calibration", " Regression Based Calibration", " Ensemble methods"], "paper_abstract": "In subset ranking, the goal is to learn a ranking function that approximates a gold standard partial ordering of a set of objects (in our case, a set of documents retrieved for the same query). The partial ordering is given by relevance labels representing the relevance of documents with respect to the query on an absolute scale. Our approach consists of three simple steps. First, we train standard multi-class classifiers (AdaBoost.MH and multi-class SVM) to discriminate between the relevance labels. Second, the posteriors of multi-class classifiers are calibrated using probabilistic and regression losses in order to estimate the Bayes-scoring function which optimizes the Normalized Discounted Cumulative Gain (NDCG). In the third step, instead of selecting the best multi-class hyperparameters and the best calibration, we mix all the learned models in a simple ensemble scheme. Our extensive experimental study is itself a substantial contribution. We compare most of the existing learning-to-rank techniques on all of the available large-scale benchmark data sets using a standardized implementation of the NDCG score. We show that our approach is competitive with conceptually more complex listwise and pairwise methods, and clearly outperforms them as the data size grows. As a technical contribution, we clarify some of the confusing results related to the ambiguities of the evaluation tools, and propose guidelines for future studies.", "paper_title": "Tune and mix: learning to rank using ensembles of calibrated multi-class classifiers", "paper_id": "WOS:000324140400004"}