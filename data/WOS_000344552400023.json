{"auto_keywords": [{"score": 0.02649012607673133, "phrase": "gpu"}, {"score": 0.008409270648525482, "phrase": "springer"}, {"score": 0.00481495049065317, "phrase": "heterogeneous_computing_platform"}, {"score": 0.004526793300190607, "phrase": "real-time_applications"}, {"score": 0.004404296965915598, "phrase": "large_matrices"}, {"score": 0.003826441783705964, "phrase": "tenth_international_symposium"}, {"score": 0.003800271653332246, "phrase": "wireless_communication_systems"}, {"score": 0.00368998915705484, "phrase": "wetzel"}, {"score": 0.00365949103351756, "phrase": "block-reduction_concept"}, {"score": 0.003523907167337643, "phrase": "new_york"}, {"score": 0.0033241495608238884, "phrase": "cost-reduced_mb-lll"}, {"score": 0.0031464653712924436, "phrase": "computational_complexity"}, {"score": 0.0030402538087882015, "phrase": "first_lll_condition"}, {"score": 0.0028975361811534265, "phrase": "gram-schmidt_coefficients_update"}, {"score": 0.002828702522575436, "phrase": "boundary_checks"}, {"score": 0.0027805362553564336, "phrase": "complexity_reduction"}, {"score": 0.002761499542632865, "phrase": "implementation_details"}, {"score": 0.002508239347479441, "phrase": "dynamic_parallelism"}, {"score": 0.002465516335722086, "phrase": "multi-core_cpu."}, {"score": 0.002390436017272046, "phrase": "dynamic_scheduling"}, {"score": 0.002239340134646663, "phrase": "execution_time"}, {"score": 0.0022163699144426155, "phrase": "cr-mb-lll_algorithm"}, {"score": 0.002163682878218085, "phrase": "multi-core_cpu"}, {"score": 0.0021049977753042253, "phrase": "cr-as-lll_algorithm"}], "paper_keywords": ["Lattice reduction", " LLL", " GPU", " CUDA", " OpenMP"], "paper_abstract": "The lattice reduction (LR) technique has become very important in many engineering fields. However, its high complexity makes difficult its use in real-time applications, especially in applications that deal with large matrices. As a solution, the modified block LLL (MB-LLL) algorithm was introduced, where several levels of parallelism were exploited: (a) fine-grained parallelism was achieved through the cost-reduced all-swap LLL (CR-AS-LLL) algorithm introduced together with the MB-LLL by Jzsa et al. (Proceedings of the tenth international symposium on wireless communication systems, 2013) and (b) coarse-grained parallelism was achieved by applying the block-reduction concept presented by Wetzel (Algorithmic number theory. Springer, New York, pp 323-337, 1998). In this paper, we present the cost-reduced MB-LLL (CR-MB-LLL) algorithm, which allows to significantly reduce the computational complexity of the MB-LLL by allowing the relaxation of the first LLL condition while executing the LR of submatrices, resulting in the delay of the Gram-Schmidt coefficients update and by using less costly procedures during the boundary checks. The effects of complexity reduction and implementation details are analyzed and discussed for several architectures. A mapping of the CR-MB-LLL on a heterogeneous platform is proposed and it is compared with implementations running on a dynamic parallelism enabled GPU and a multi-core CPU. The mapping on the architecture proposed allows a dynamic scheduling of kernels where the overhead introduced is hidden by the use of several CUDA streams. Results show that the execution time of the CR-MB-LLL algorithm on the heterogeneous platform outperforms the multi-core CPU and it is more efficient than the CR-AS-LLL algorithm in case of large matrices.", "paper_title": "High performance lattice reduction on heterogeneous computing platform", "paper_id": "WOS:000344552400023"}