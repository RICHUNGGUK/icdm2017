{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "adaptive_pruning_algorithm"}, {"score": 0.03756373022282025, "phrase": "proposed_algorithm"}, {"score": 0.004772137871562584, "phrase": "least_squares"}, {"score": 0.004729704116022601, "phrase": "vector_machine_classifier"}, {"score": 0.004645959941491617, "phrase": "new_version"}, {"score": 0.0046046429665581555, "phrase": "support_vector_machine"}, {"score": 0.004563977133706135, "phrase": "svm"}, {"score": 0.004325487452170154, "phrase": "equality_instead_of_inequality_constraints_and_works_with_a_least_squares_cost_function"}, {"score": 0.004154995396099332, "phrase": "ls-svm_applications"}, {"score": 0.0037490909410974166, "phrase": "top_strategy"}, {"score": 0.0035216195857272403, "phrase": "incremental_and_decremental_learning_procedures"}, {"score": 0.0034283921404628975, "phrase": "small_support_vector_set"}, {"score": 0.00303839203670705, "phrase": "final_classifier"}, {"score": 0.0028795712616923462, "phrase": "support_vector_set"}, {"score": 0.002729029534060092, "phrase": "sparse_solution"}, {"score": 0.0025178072071268534, "phrase": "eight_uci_datasets"}, {"score": 0.0024620842899061614, "phrase": "experimental_results"}, {"score": 0.002418393046947528, "phrase": "presented_algorithm"}, {"score": 0.002364865100579589, "phrase": "sparse_solutions"}, {"score": 0.002291905937251069, "phrase": "classification_problems"}, {"score": 0.002162316557411786, "phrase": "sequential_minimal_optimization_algorithm"}, {"score": 0.0021049977753042253, "phrase": "large-scale_classification_problems"}], "paper_keywords": ["Support vector machine", " Least squares support vector machine", " Pruning", " Incremental learning", " Decremental learning", " Adaptive"], "paper_abstract": "As a new version of support vector machine (SVM), least squares SVM (LS-SVM) involves equality instead of inequality constraints and works with a least squares cost function. A well-known drawback in the LS-SVM applications is that the sparseness is lost. In this paper, we develop an adaptive pruning algorithm based on the bottom-to-top strategy, which can deal with this drawback. In the proposed algorithm, the incremental and decremental learning procedures are used alternately and a small support vector set, which can cover most of the information in the training set, can be formed adaptively. Using this set, one can construct the final classifier. In general, the number of the elements in the support vector set is much smaller than that in the training set and a sparse solution is obtained. In order to test the efficiency of the proposed algorithm, we apply it to eight UCI datasets and one benchmarking dataset. The experimental results show that the presented algorithm can obtain adaptively the sparse solutions with losing a little generalization performance for the classification problems with no-noises or noises, and its training speed is much faster than sequential minimal optimization algorithm (SMO) for the large-scale classification problems with no-noises.", "paper_title": "Adaptive pruning algorithm for least squares support vector machine classifier", "paper_id": "WOS:000274404400001"}