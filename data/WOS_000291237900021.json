{"auto_keywords": [{"score": 0.04576021885069074, "phrase": "data-intensive_applications"}, {"score": 0.033189123614918076, "phrase": "splitcache"}, {"score": 0.00481495049065317, "phrase": "map_reduce_performance_by_exploiting_input_redundancy"}, {"score": 0.004657393640152439, "phrase": "parallel_programming"}, {"score": 0.004606023493393774, "phrase": "large_clusters"}, {"score": 0.004504969093304219, "phrase": "new_research_avenue"}, {"score": 0.004430629452265786, "phrase": "numerous_types"}, {"score": 0.004309434349688514, "phrase": "feasible_plan"}, {"score": 0.004009555045206973, "phrase": "nontrivial_amount"}, {"score": 0.0037931018103829427, "phrase": "redundancy_problem"}, {"score": 0.00370981613076245, "phrase": "emerging_issue"}, {"score": 0.0036485497734171294, "phrase": "recent_literature"}, {"score": 0.003608266640131664, "phrase": "even_the_locality-aware_scheduling_policy"}, {"score": 0.0034134024569847264, "phrase": "cluster_environment"}, {"score": 0.0033757067564871494, "phrase": "storage_nodes"}, {"score": 0.003283272764794988, "phrase": "computation_service"}, {"score": 0.0030376623293574905, "phrase": "data-intensive_olap-style_applications"}, {"score": 0.002921815042028607, "phrase": "mapreduce_framework"}, {"score": 0.002873524721802591, "phrase": "key_strategy"}, {"score": 0.0027031706928674092, "phrase": "different_applications"}, {"score": 0.0026732973536990373, "phrase": "common_input_data"}, {"score": 0.00262910358108176, "phrase": "overlapped_time_period"}, {"score": 0.0025570607111118793, "phrase": "first_input_stream"}, {"score": 0.00251478366610296, "phrase": "computing_nodes"}, {"score": 0.002445865715873089, "phrase": "future_demands"}, {"score": 0.0023656467839555458, "phrase": "cache-aware_task_scheduler"}, {"score": 0.0023136311468961125, "phrase": "important_role"}, {"score": 0.002275369652100353, "phrase": "high_cache_utilization"}, {"score": 0.0022007302738063566, "phrase": "tpc-h_benchmark"}, {"score": 0.0021049977753042253, "phrase": "network_traffic"}], "paper_keywords": ["mapreduce", " I/O redundancy", " task scheduling", " distributed system", " cloud computing"], "paper_abstract": "The proliferation of data parallel programming on large clusters has set a new research avenue: accommodating numerous types of data-intensive applications with a feasible plan. Behind the many research efforts, we can observe that there exists a nontrivial amount of redundant I/O in the execution of data-intensive applications. This redundancy problem arises as an emerging issue in the recent literature because even the locality-aware scheduling policy in a MapReduce framework is not effective in a cluster environment where storage nodes cannot provide a computation service. In this article, we introduce SplitCache for improving the performance of data-intensive OLAP-style applications by reducing redundant I/O in a MapReduce framework. The key strategy to achieve the goal is to eliminate such I/O redundancy especially when different applications read common input data within an overlapped time period; SplitCache caches the first input stream in the computing nodes and reuses them for future demands. We also design a cache-aware task scheduler that plays an important role in achieving high cache utilization. In execution of the TPC-H benchmark, we achieved 64.3% faster execution and 83.48% reduction in network traffic in average.", "paper_title": "Improving Map Reduce Performance by Exploiting Input Redundancy", "paper_id": "WOS:000291237900021"}