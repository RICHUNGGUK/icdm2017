{"auto_keywords": [{"score": 0.026259771168731624, "phrase": "knn"}, {"score": 0.00481495049065317, "phrase": "new_observations"}, {"score": 0.004688989566904871, "phrase": "non-linear_manifold_learning"}, {"score": 0.0046477357199085035, "phrase": "non-linear_dimensionality_reduction_techniques"}, {"score": 0.004388252997694986, "phrase": "adjacency_graphs"}, {"score": 0.0040347380310668994, "phrase": "first_aspect"}, {"score": 0.0039815737219022675, "phrase": "proposed_solutions"}, {"score": 0.0037925389501636975, "phrase": "second_aspect"}, {"score": 0.003660700128133065, "phrase": "accurate_mapping"}, {"score": 0.0036124465071873998, "phrase": "unseen_data_samples"}, {"score": 0.0035648266669315943, "phrase": "existing_manifold"}, {"score": 0.003350740737336966, "phrase": "optimal_performance"}, {"score": 0.0032629585038738856, "phrase": "suitable_parameter_choice"}, {"score": 0.0030399188412691914, "phrase": "sparse_representation_theory"}, {"score": 0.0029733865110222785, "phrase": "automatic_graph_construction"}, {"score": 0.0029212075318449616, "phrase": "recent_works"}, {"score": 0.0028446460307503343, "phrase": "accurate_alternative"}, {"score": 0.0028195727965553367, "phrase": "out-of-sample_embedding"}, {"score": 0.0027578496677536373, "phrase": "case_study"}, {"score": 0.002661883310629088, "phrase": "face_recognition_problem"}, {"score": 0.0025692477422626678, "phrase": "proposed_out-of-sample_embedding"}, {"score": 0.002490833392316362, "phrase": "k-nearest_neighbor"}, {"score": 0.002436289099444074, "phrase": "kernel_support_vector_machines"}, {"score": 0.0023724065150232897, "phrase": "six_public_face_datasets"}, {"score": 0.002341094670523097, "phrase": "experimental_results"}, {"score": 0.0022999859758286423, "phrase": "proposed_model"}, {"score": 0.0022496114364599328, "phrase": "high_categorization_effectiveness"}, {"score": 0.0022101055924375725, "phrase": "high_consistency"}, {"score": 0.002161695357601592, "phrase": "batch_modes"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Non-linear manifold learning", " Out-of-sample embedding", " Sparse representation", " Face recognition"], "paper_abstract": "Non-linear dimensionality reduction techniques are affected by two critical aspects: (i) the design of the adjacency graphs, and (ii) the embedding of new test data-the out-of-sample problem. For the first aspect, the proposed solutions, in general, were heuristically driven. For the second aspect, the difficulty resides in finding an accurate mapping that transfers unseen data samples into an existing manifold. Past works addressing these two aspects were heavily parametric in the sense that the optimal performance is only achieved for a suitable parameter choice that should be known in advance. In this paper, we demonstrate that the sparse representation theory not only serves for automatic graph construction as shown in recent works, but also represents an accurate alternative for out-of-sample embedding. Considering for a case study the Laplacian Eigenmaps, we applied our method to the face recognition problem. To evaluate the effectiveness of the proposed out-of-sample embedding, experiments are conducted using the K-nearest neighbor (KNN) and Kernel Support Vector Machines (KSVM) classifiers on six public face datasets. The experimental results show that the proposed model is able to achieve high categorization effectiveness as well as high consistency with non-linear embeddings/manifolds obtained in batch modes. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Embedding new observations via sparse-coding for non-linear manifold learning", "paper_id": "WOS:000326903500038"}