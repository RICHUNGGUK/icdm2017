{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sequential_sampling"}, {"score": 0.0046993598473345395, "phrase": "sequential_sampling_policy"}, {"score": 0.004653900688892822, "phrase": "noisy_discrete_global_optimization"}, {"score": 0.0043477114072437316, "phrase": "finite_set"}, {"score": 0.0039447744176901054, "phrase": "multidimensional_vector"}, {"score": 0.003906586074865932, "phrase": "categorical_and_numerical_attributes"}, {"score": 0.0038499929374478125, "phrase": "independent_normal_rewards"}, {"score": 0.003757480190200645, "phrase": "bayesian_probability_model"}, {"score": 0.0037030390595465673, "phrase": "unknown_reward"}, {"score": 0.003579046565851265, "phrase": "fully_sequential_sampling_policy"}, {"score": 0.003359647377874098, "phrase": "expected_increment"}, {"score": 0.003278878522165815, "phrase": "sampling_information"}, {"score": 0.00323134923196813, "phrase": "time_period"}, {"score": 0.0031536550157613974, "phrase": "hierarchical_aggregation_technique"}, {"score": 0.003092842645798083, "phrase": "common_features"}, {"score": 0.0029602552900111407, "phrase": "even_a_single_measurement"}, {"score": 0.002861062731782219, "phrase": "measurement_effort"}, {"score": 0.0027651847105654363, "phrase": "prior_knowledge"}, {"score": 0.002608218347502229, "phrase": "aggregation_function"}, {"score": 0.0025829353063908256, "phrase": "computational_issues"}, {"score": 0.002297949552105756, "phrase": "globally_optimal_alternative"}], "paper_keywords": ["sequential experimental design", " ranking and selection", " adaptive learning", " hierarchical statistics", " Bayesian statistics"], "paper_abstract": "We propose a sequential sampling policy for noisy discrete global optimization and ranking and selection, in which we aim to efficiently explore a finite set of alternatives before selecting an alternative as best when exploration stops. Each alternative may be characterized by a multidimensional vector of categorical and numerical attributes and has independent normal rewards. We use a Bayesian probability model for the unknown reward of each alternative and follow a fully sequential sampling policy called the knowledge-gradient policy. This policy myopically optimizes the expected increment in the value of sampling information in each time period. We propose a hierarchical aggregation technique that uses the common features shared by alternatives to learn about many alternatives from even a single measurement. This approach greatly reduces the measurement effort required, but it requires some prior knowledge on the smoothness of the function in the form of an aggregation function and computational issues limit the number of alternatives that can be easily considered to the thousands. We prove that our policy is consistent, finding a globally optimal alternative when given enough measurements, and show through simulations that it performs competitively with or significantly better than other policies.", "paper_title": "Hierarchical Knowledge Gradient for Sequential Sampling", "paper_id": "WOS:000298103200008"}