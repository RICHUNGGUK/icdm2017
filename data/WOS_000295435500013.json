{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "resource_usage"}, {"score": 0.0047810714349158165, "phrase": "planetary-scale_shared_infrastructures"}, {"score": 0.004747429622531595, "phrase": "planetlab's_case_study"}, {"score": 0.004680851157467745, "phrase": "different_applications"}, {"score": 0.004647911247507267, "phrase": "different_resource_requirements"}, {"score": 0.004582722012454012, "phrase": "shared_infrastructure"}, {"score": 0.004439354841093929, "phrase": "resource_allocation"}, {"score": 0.00422516469733364, "phrase": "well_known_planetary_scale_experimental_facility"}, {"score": 0.004180622964109706, "phrase": "case_study_-_planetlab"}, {"score": 0.004078502325220629, "phrase": "workload_characterization"}, {"score": 0.004035500442527029, "phrase": "node_point"}, {"score": 0.0036553070575015344, "phrase": "underlying_nature"}, {"score": 0.003603989174919527, "phrase": "distributed_applications"}, {"score": 0.003417859963947459, "phrase": "mutual_influence"}, {"score": 0.0032991276152705934, "phrase": "overall_consumption"}, {"score": 0.0029357204796552653, "phrase": "network_usage"}, {"score": 0.00290473242917264, "phrase": "experimental_applications"}, {"score": 0.0028639258341034025, "phrase": "planetlab"}, {"score": 0.002744888626958772, "phrase": "new_scheduling"}, {"score": 0.002687235334499192, "phrase": "experimental_facility"}, {"score": 0.0026214978443388653, "phrase": "future_advances"}, {"score": 0.002603012037901889, "phrase": "resource_scheduling"}, {"score": 0.002575526825728026, "phrase": "future_internet"}, {"score": 0.0025125151618810523, "phrase": "publicly_available_traces"}, {"score": 0.00225144317399471, "phrase": "good_matches"}, {"score": 0.0021501863832966966, "phrase": "similar_statistical_characteristics"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Statistical analysis", " Modeling of systems", " Large-scale networked infrastructures", " Resource allocation"], "paper_abstract": "Understanding how different applications with different resource requirements interact in a shared infrastructure like Internet is crucial in the design of resource allocation and regulation policies. In this paper, our focus is on analyzing a well known planetary scale experimental facility as a case study - PlanetLab. Previous analysis focused on the workload characterization from the node point of view, while little attention has been paid to the processes leading to such workload - i.e. distributed applications. In particular, our aim is twofold: (i) characterize the underlying nature of interactions among distributed applications in a collaborative and shared infrastructure. In other words, understand the interference or mutual influence of one slice on other slices and the effect on the overall consumption of slices over the infrastructure, and (ii) characterize two important aspects of applications' resource usage - i.e. the short-term distribution and temporal dynamics of CPU, memory and network usage made by experimental applications currently running in PlanetLab. Understanding these interactions is a prerequisite for the design of new scheduling and regulation mechanisms for the experimental facility and serve as a testbed for future advances in resource scheduling facing the Future Internet. Based on the analysis of publicly available traces, we find that the distribution of resource usage is highly skewed - with a few applications producing most of the resource usage. We use our findings to develop a model that produces good matches in the metrics studied, allowing us to generate a workload with similar statistical characteristics. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Modeling resource usage in planetary-scale shared infrastructures: PlanetLab's case study", "paper_id": "WOS:000295435500013"}