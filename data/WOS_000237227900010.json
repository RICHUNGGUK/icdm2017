{"auto_keywords": [{"score": 0.027403442866390584, "phrase": "gp"}, {"score": 0.00481495049065317, "phrase": "fitness_functions"}, {"score": 0.004703671193348433, "phrase": "test_cases"}, {"score": 0.004523865550323285, "phrase": "genetic_programming"}, {"score": 0.004119762981892093, "phrase": "learning_task"}, {"score": 0.0038404642251010797, "phrase": "limited_number"}, {"score": 0.0034162643819889054, "phrase": "gp-based_learning"}, {"score": 0.0031107911390188055, "phrase": "run_individuals"}, {"score": 0.002854761308379149, "phrase": "parsimony_pressure"}, {"score": 0.002519402443228795, "phrase": "binary_classification_setup"}, {"score": 0.002366763442497185, "phrase": "test_sets"}, {"score": 0.002223351523626989, "phrase": "baseline_results"}, {"score": 0.002171836484200008, "phrase": "mean_tree_size"}, {"score": 0.0021049977753042253, "phrase": "tested_methods"}], "paper_keywords": [""], "paper_abstract": "Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.", "paper_title": "Genetic programming, validation sets, and parsimony pressure", "paper_id": "WOS:000237227900010"}