{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "statistical_machine_translation"}, {"score": 0.042078324571925355, "phrase": "standard_n-gram_language_models"}, {"score": 0.004685575787741084, "phrase": "language_model"}, {"score": 0.0035677736570659813, "phrase": "conventional_forward_language_model"}, {"score": 0.0032577270508271305, "phrase": "long-distance_dependencies"}, {"score": 0.0027407299369891502, "phrase": "state-of-the-art_phrase-based_decoders"}, {"score": 0.0022847009758393405, "phrase": "translation_quality"}, {"score": 0.002203027437847318, "phrase": "bleu"}, {"score": 0.002163283969413678, "phrase": "meteor"}, {"score": 0.0021049977753042253, "phrase": "competitive_baseline"}], "paper_keywords": [""], "paper_abstract": "The language model is one of the most important knowledge sources for statistical machine translation. In this article, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We introduce algorithms to integrate the two proposed models into two kinds of state-of-the-art phrase-based decoders. Our experimental results on Chinese/Spanish/Vietnamese-to-English show that both models are able to significantly improve translation quality in terms of BLEU and METEOR over a competitive baseline.", "paper_title": "Backward and trigger-based language models for statistical machine translation", "paper_id": "WOS:000351756500002"}