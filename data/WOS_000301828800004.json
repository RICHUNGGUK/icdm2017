{"auto_keywords": [{"score": 0.029817986308747473, "phrase": "nearest_neighbors"}, {"score": 0.00481495049065317, "phrase": "mutual_k-nearest_neighbor"}, {"score": 0.004668059458774745, "phrase": "nearest_neighbor"}, {"score": 0.004525629293824217, "phrase": "effective_and_powerful_lazy_learning_algorithm"}, {"score": 0.004102528738941734, "phrase": "training_data"}, {"score": 0.003816197777388722, "phrase": "large_scale_databases"}, {"score": 0.003301899275278983, "phrase": "new_anomaly_removal"}, {"score": 0.0031354779851351287, "phrase": "primary_characteristic"}, {"score": 0.0029467742734417255, "phrase": "class_labels"}, {"score": 0.0029164434911787187, "phrase": "unseen_instances"}, {"score": 0.0028864239961184364, "phrase": "mutual_nearest_neighbors"}, {"score": 0.0025361866386341796, "phrase": "prediction_process"}, {"score": 0.002471400498582011, "phrase": "final_learning_result"}, {"score": 0.0023958326775913165, "phrase": "extensive_comparative_experimental_analysis"}, {"score": 0.0023467391904246834, "phrase": "uci_datasets"}, {"score": 0.0023225701080913388, "phrase": "empirical_evidence"}, {"score": 0.002251542790734307, "phrase": "proposed_method"}, {"score": 0.0021714122348552747, "phrase": "k-nn_rule"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Data mining", " Pattern classification", " kNN", " Mutual nearest neighbor", " Data reduction"], "paper_abstract": "k nearest neighbor (kNN) is an effective and powerful lazy learning algorithm, notwithstanding its easy-to-implement. However, its performance heavily relies on the quality of training data. Due to many complex real-applications, noises coming from various possible sources are often prevalent in large scale databases. How to eliminate anomalies and improve the quality of data is still a challenge. To alleviate this problem, in this paper we propose a new anomaly removal and learning algorithm under the framework of kNN. The primary characteristic of our method is that the evidence of removing anomalies and predicting class labels of unseen instances is mutual nearest neighbors, rather than k nearest neighbors. The advantage is that pseudo nearest neighbors can be identified and will not be taken into account during the prediction process. Consequently, the final learning result is more creditable. An extensive comparative experimental analysis carried out on UCI datasets provided empirical evidence of the effectiveness of the proposed method for enhancing the performance of the k-NN rule. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "Noisy data elimination using mutual k-nearest neighbor for classification mining", "paper_id": "WOS:000301828800004"}