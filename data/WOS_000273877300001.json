{"auto_keywords": [{"score": 0.04191165799288371, "phrase": "isotropic_log-concave_distributions"}, {"score": 0.015719716506582538, "phrase": "malicious_noise"}, {"score": 0.011552359762376447, "phrase": "malicious_noise_rate"}, {"score": 0.01051434841345406, "phrase": "isotropic_log-concave_distribution"}, {"score": 0.004742511243894382, "phrase": "new_algorithms"}, {"score": 0.004635881163634672, "phrase": "challenging_malicious_noise_model"}, {"score": 0.0044634416051962545, "phrase": "underlying_distribution"}, {"score": 0.004346547997061901, "phrase": "malicious_noise_rates"}, {"score": 0.004281125387210667, "phrase": "previous_work"}, {"score": 0.0040444053843446326, "phrase": "fairly_broad_class"}, {"score": 0.0035957006001349915, "phrase": "uniform_distribution"}, {"score": 0.0035550029054401016, "phrase": "unit_ball"}, {"score": 0.0034096661914349577, "phrase": "best_previous_result"}, {"score": 0.0030311679447397725, "phrase": "first_efficient_algorithm"}, {"score": 0.002798865807872575, "phrase": "origin-centered_halfspaces"}, {"score": 0.0026843584448921565, "phrase": "adversarial_label_noise"}, {"score": 0.002574523719093119, "phrase": "adversarial_label_noise_setting"}, {"score": 0.0024043803470189455, "phrase": "previous_results"}, {"score": 0.0022972306948217548, "phrase": "unspecified_function"}, {"score": 0.0022115879730677, "phrase": "anti-concentration_properties"}, {"score": 0.0021453734588569823, "phrase": "iterative_outlier_removal_procedure"}, {"score": 0.0021291312714904957, "phrase": "principal_component_analysis"}, {"score": 0.0021049977753042253, "phrase": "\"smooth\"_boosting"}], "paper_keywords": ["PAC learning", " noise tolerance", " malicious noise", " agnostic learning", " label noise", " halfspace learning", " linear classifiers"], "paper_abstract": "We give new algorithms for learning halfspaces in the challenging malicious noise model, where an adversary may corrupt both the labels and the underlying distribution of examples. Our algorithms can tolerate malicious noise rates exponentially larger than previous work in terms of the dependence on the dimension n, and succeed for the fairly broad class of all isotropic log-concave distributions. We give poly(n, 1/epsilon)-time algorithms for solving the following problems to accuracy epsilon: Learning origin-centered halfspaces in R(n) with respect to the uniform distribution on the unit ball with malicious noise rate eta = Omega(epsilon(2)/log(n/epsilon)). (The best previous result was Omega(epsilon/(nlog(n/epsilon))(1/4)).) Learning origin-centered halfspaces with respect to any isotropic log-concave distribution on R(n) with malicious noise rate eta = Omega(epsilon(3)/log(2)(n/epsilon)). This is the first efficient algorithm for learning under isotropic log-concave distributions in the presence of malicious noise. We also give a poly(n, 1/epsilon)-time algorithm for learning origin-centered halfspaces under any isotropic log-concave distribution on R(n) in the presence of adversarial label noise at rate eta - Omega(epsilon(3)/log(1/epsilon)). In the adversarial label noise setting (or agnostic model), labels can be noisy, but not example points themselves. Previous results could handle eta = W(epsilon) but had running time exponential in an unspecified function of 1/epsilon. Our analysis crucially exploits both concentration and anti-concentration properties of isotropic log-concave distributions. Our algorithms combine an iterative outlier removal procedure using Principal Component Analysis together with \"smooth\" boosting.", "paper_title": "Learning Halfspaces with Malicious Noise", "paper_id": "WOS:000273877300001"}