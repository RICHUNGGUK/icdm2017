{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sensor_fusion_layer"}, {"score": 0.049390246951185, "phrase": "reduced_visibility"}, {"score": 0.0047113960612140335, "phrase": "slam._mapping"}, {"score": 0.004643594260262632, "phrase": "mobile_robots"}, {"score": 0.0043345662747335065, "phrase": "big_challenge"}, {"score": 0.004225950334493496, "phrase": "tremendous_advance"}, {"score": 0.004195418010131832, "phrase": "simultaneous_localization"}, {"score": 0.004075469816959996, "phrase": "current_algorithms"}, {"score": 0.00393032656248792, "phrase": "optical_sensors"}, {"score": 0.0039019217388438134, "phrase": "dense_range_data"}, {"score": 0.0038737214010638745, "phrase": "e.g._laser_range_finders"}, {"score": 0.0034118313251390925, "phrase": "slam"}, {"score": 0.003387153161968927, "phrase": "reduced_visibility_conditions"}, {"score": 0.0032783186166708985, "phrase": "complementary_characteristics"}, {"score": 0.003242820629121494, "phrase": "laser_range_finder"}, {"score": 0.003048782877550259, "phrase": "state-of-the-art_slam_technique"}, {"score": 0.002876747208161839, "phrase": "special_attention"}, {"score": 0.00269475174341129, "phrase": "robotic_platforms"}, {"score": 0.002665555782056494, "phrase": "technical_issues"}, {"score": 0.0025334249463305875, "phrase": "heuristic_method"}, {"score": 0.0025059724686541263, "phrase": "fuzzy_logic-based_method"}, {"score": 0.0024253827365474734, "phrase": "different_stages"}, {"score": 0.0023990982005125763, "phrase": "research_work"}, {"score": 0.0023644936731468252, "phrase": "experimental_validation"}, {"score": 0.002239103860494545, "phrase": "robust_solution"}, {"score": 0.0021435864990577945, "phrase": "slam_process"}], "paper_keywords": ["SLAM", " Reduced visibility", " Sensor fusion", " Robot Operating System (ROS)"], "paper_abstract": "Mapping and navigating with mobile robots in scenarios with reduced visibility, e.g. due to smoke, dust, or fog, is still a big challenge nowadays. In spite of the tremendous advance on Simultaneous Localization and Mapping (SLAM) techniques for the past decade, most of current algorithms fail in those environments because they usually rely on optical sensors providing dense range data, e.g. laser range finders, stereo vision, LIDARs, RGB-D, etc., whose measurement process is highly disturbed by particles of smoke, dust, or steam. This article addresses the problem of performing SLAM under reduced visibility conditions by proposing a sensor fusion layer which takes advantage from complementary characteristics between a laser range finder (LRF) and an array of sonars. This sensor fusion layer is ultimately used with a state-of-the-art SLAM technique to be resilient in scenarios where visibility cannot be assumed at all times. Special attention is given to mapping using commercial off-the-shelf (COTS) sensors, namely arrays of sonars which, being usually available in robotic platforms, raise technical issues that were investigated in the course of this work. Two sensor fusion methods, a heuristic method and a fuzzy logic-based method, are presented and discussed, corresponding to different stages of the research work conducted. The experimental validation of both methods with two different mobile robot platforms in smoky indoor scenarios showed that they provide a robust solution, using only COTS sensors, for adequately coping with reduced visibility in the SLAM process, thus decreasing significantly its impact in the mapping and localization results obtained.", "paper_title": "A Sensor Fusion Layer to Cope with Reduced Visibility in SLAM", "paper_id": "WOS:000363981000006"}