{"auto_keywords": [{"score": 0.03594893921057597, "phrase": "gprm"}, {"score": 0.00481495049065317, "phrase": "globally_a_strategy"}, {"score": 0.00470975295439431, "phrase": "manycore_era"}, {"score": 0.004627244470096107, "phrase": "general-purpose_computing_system"}, {"score": 0.004235793609579307, "phrase": "specific_platform"}, {"score": 0.004198510172614725, "phrase": "additional_performance_issues"}, {"score": 0.004070573090297285, "phrase": "multiple_applications"}, {"score": 0.003964007795174772, "phrase": "different_scheduling_and_resource_management_techniques"}, {"score": 0.0038602215057106917, "phrase": "operating_system"}, {"score": 0.003826231150275071, "phrase": "user_level"}, {"score": 0.0037260386566859197, "phrase": "concurrent_workloads"}, {"score": 0.0035806300393050926, "phrase": "task-based_strategy"}, {"score": 0.0033805214040814233, "phrase": "glasgow_parallel_reduction_machine"}, {"score": 0.0032774280543222843, "phrase": "state-of-the-art_manycore_parallel_machine"}, {"score": 0.003149471116658739, "phrase": "well-known_parallel_programming_models"}, {"score": 0.003121739247375333, "phrase": "openmp"}, {"score": 0.0030942116765996426, "phrase": "intel_cilk_plus"}, {"score": 0.0030669639707130705, "phrase": "intel"}, {"score": 0.002999823428906084, "phrase": "single-programming_and_multiprogramming_scenarios"}, {"score": 0.0028572658971252616, "phrase": "single_workloads"}, {"score": 0.0027578496677536373, "phrase": "multiprogramming_workloads"}, {"score": 0.0025692477422626678, "phrase": "parallel_framework"}, {"score": 0.002512990657100351, "phrase": "separate_layer"}, {"score": 0.0021426288417854987, "phrase": "gprm_applications"}], "paper_keywords": ["Parallel programming", " Multiprogramming", " Task stealing", " Manycore processors", " GPRM", " Intel Xeon Phi"], "paper_abstract": "In a general-purpose computing system, several parallel applications run simultaneously on the same platform. Even if each application is highly tuned for that specific platform, additional performance issues are arising in such a dynamic environment in which multiple applications compete for the resources. Different scheduling and resource management techniques have been proposed either at operating system or user level to improve the performance of concurrent workloads. In this paper, we propose a task-based strategy called \"Steal Locally, Share Globally\" implemented in the runtime of our parallel programming model GPRM (Glasgow Parallel Reduction Machine). We have chosen a state-of-the-art manycore parallel machine, the Intel Xeon Phi, to compare GPRM with some well-known parallel programming models, OpenMP, Intel Cilk Plus and Intel TBB, in both single-programming and multiprogramming scenarios. We show that GPRM not only performs well for single workloads, but also outperforms the other models for multiprogramming workloads. There are three considerations regarding our task-based scheme: (i) It is implemented inside the parallel framework, not as a separate layer; (ii) It improves the performance without the need to change the number of threads for each application (iii) It can be further tuned and improved, not only for the GPRM applications, but for other equivalent parallel programming models.", "paper_title": "Steal Locally, Share Globally A Strategy for Multiprogramming in the Manycore Era", "paper_id": "WOS:000358648600010"}