{"auto_keywords": [{"score": 0.04895586541793087, "phrase": "tangent_vectors"}, {"score": 0.033330168836556255, "phrase": "additional_information"}, {"score": 0.0048154473867966585, "phrase": "lda"}, {"score": 0.004769444094319619, "phrase": "srda"}, {"score": 0.004569905085027681, "phrase": "pattern_recognition"}, {"score": 0.004215397034321004, "phrase": "representation_space"}, {"score": 0.0038515000499988673, "phrase": "dimensionality_reduction_approach"}, {"score": 0.0036379729015583975, "phrase": "dimensionality_reduction_techniques"}, {"score": 0.0035864603175592854, "phrase": "better_recognition_performance"}, {"score": 0.003307988966112615, "phrase": "high-dimensional_spaces"}, {"score": 0.002881793492919646, "phrase": "possible_differences"}, {"score": 0.0028409576481842457, "phrase": "sample_data"}, {"score": 0.0026202187680903063, "phrase": "unseen_data"}, {"score": 0.00258307991054107, "phrase": "better_use"}, {"score": 0.002546466113418944, "phrase": "scarce_training_samples"}, {"score": 0.0024397017438101726, "phrase": "tangent_vector_information"}, {"score": 0.0021659920594094407, "phrase": "known_transformations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Subspace learning", " Dimensionality reduction", " Tangent vectors", " LDA", " SRDA"], "paper_abstract": "In the area of pattern recognition, it is common for few training samples to be available with respect to the dimensionality of the representation space; this is known as the curse of dimensionality. This problem can be alleviated by using a dimensionality reduction approach, which overcomes the curse relatively well. Moreover, supervised dimensionality reduction techniques generally provide better recognition performance; however, several of these tend to suffer from the curse when applied directly to high-dimensional spaces. We propose to overcome this problem by incorporating additional information to supervised subspace learning techniques using what is known as tangent vectors. This additional information accounts for the possible differences that the sample data can suffer. In fact, this can be seen as a way to model the unseen data and make better use of the scarce training samples. In this paper, methods for incorporating tangent vector information are described for one classical technique (LDA) and one state-of-the-art technique (SRDA). Experimental results confirm that this additional information improves performance and robustness to known transformations. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "On improving robustness of LDA and SRDA by using tangent vectors", "paper_id": "WOS:000318889800019"}