{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "macro_data_load"}, {"score": 0.01527423871192454, "phrase": "loaded_data_reuse"}, {"score": 0.00446820572467064, "phrase": "novel_mechanism"}, {"score": 0.004146327465630697, "phrase": "maximum-width_data"}, {"score": 0.0039120325834029355, "phrase": "byte_load"}, {"score": 0.003721735062704032, "phrase": "internal_hardware_structure"}, {"score": 0.00352597432462803, "phrase": "saved_data"}, {"score": 0.003439117493141663, "phrase": "later_loads"}, {"score": 0.0033824008297657494, "phrase": "relatively_more_expensive_cache_accesses"}, {"score": 0.0033128141572497704, "phrase": "comprehensive_availability_study"}, {"score": 0.0032717489668326275, "phrase": "generalized_memory_data_reuse_table"}, {"score": 0.003191134522157061, "phrase": "available_memory_data"}, {"score": 0.0030995835539697893, "phrase": "benchmark_programs"}, {"score": 0.002936451109357539, "phrase": "proposed_scheme"}, {"score": 0.002900037689072025, "phrase": "macro_data_load_mechanism"}, {"score": 0.0027473784878300133, "phrase": "previous_schemes"}, {"score": 0.002690820958068933, "phrase": "spatial_locality"}, {"score": 0.002602734354498382, "phrase": "mdrt"}, {"score": 0.002345579707333727, "phrase": "realistic_processor_model"}, {"score": 0.002176275802465835, "phrase": "mibench_programs"}, {"score": 0.0021049977753042253, "phrase": "related_energy_reduction"}], "paper_keywords": ["Superscalar processor architecture", " cache memory", " load store queue", " low-power processor design"], "paper_abstract": "This paper presents a study on macro data load, a novel mechanism to increase the amount of loaded data reuse within a processor. A macro data load brings into the processor a maximum-width data the cache port allows. In a 64-bit processor, for example, a byte load will bring a full 64-bit data from cache and save it in an internal hardware structure, while using for itself only the specified byte out of the 64-bit data. The saved data can be opportunistically reused by later loads internally, reducing relatively more expensive cache accesses. We present a comprehensive availability study using a generalized memory data reuse table (MDRT) to quantify available memory data reuse opportunities in a set of benchmark programs drawn from the SPEC2k and MiBench suites, and to demonstrate the efficacy of the proposed scheme. The macro data load mechanism is shown to open up significantly more loaded data reuse opportunities than previous schemes with no support for spatial locality. We observe 15.1 percent (SPEC2k integer), 20.9 percent (SPEC2k floating-point), and 45.8 percent (MiBench) more load-to-load forwarding instances when a 256-entry MDRT is used. We also describe a modified load store queue design as a possible implementation of the proposed concept. Our quantitative study using a realistic processor model shows that 21.3 percent, 14.8 percent, and 23.6 percent of L1 cache accesses in the SPEC2k integer, floating-point, and MiBench programs can be eliminated, resulting in a related energy reduction of 11.4 percent, 9.0 percent, and 14.3 percent on average, respectively.", "paper_title": "Macro Data Load: An Efficient Mechanism for Enhancing Loaded Data Reuse", "paper_id": "WOS:000287668100008"}