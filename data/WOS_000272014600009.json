{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "efficient_mapping"}, {"score": 0.004765788394931875, "phrase": "program_parallelism"}, {"score": 0.004717125879761534, "phrase": "multi-core_processors"}, {"score": 0.004574085781766281, "phrase": "underlying_architecture"}, {"score": 0.004435363840720883, "phrase": "portable_and_automatic_compiler-based_approach"}, {"score": 0.004300830853119242, "phrase": "machine_learning"}, {"score": 0.003941319912895672, "phrase": "best_mapping"}, {"score": 0.0039010428828989826, "phrase": "parallel_programs"}, {"score": 0.0037057255600764475, "phrase": "scheduling_policy"}, {"score": 0.0034485432592912917, "phrase": "low-cost_profiling_runs"}, {"score": 0.0033096502887394233, "phrase": "new_unseen_program"}, {"score": 0.003275807170434343, "phrase": "multiple_input_data_sets"}, {"score": 0.003160049822855747, "phrase": "parallelism_mapping_configurations"}, {"score": 0.0031277316566463978, "phrase": "openmp_programs"}, {"score": 0.0022738775158524793, "phrase": "openmp_runtime_default_scheme"}, {"score": 0.0022390533383717715, "phrase": "cell_platform"}, {"score": 0.0021377414429251647, "phrase": "better_performance"}, {"score": 0.0021049977753042253, "phrase": "significant_lower_profiling_cost"}], "paper_keywords": ["Experimentation", " Languages", " Performance", " Compiler optimization", " Performance modeling", " Machine learning", " Artificial neural networks", " Support vector machine"], "paper_abstract": "The efficient mapping of program parallelism to multi-core processors is highly dependent on the underlying architecture. This paper proposes a portable and automatic compiler-based approach to mapping such parallelism using machine learning. It develops two predictors: a data sensitive and a data insensitive predictor to select the best mapping for parallel programs. They predict the number of threads and the scheduling policy for any given program using a model learnt off-line. By using low-cost profiling runs, they predict the mapping for a new unseen program across multiple input data sets. We evaluate our approach by selecting parallelism mapping configurations for OpenMP programs on two representative but different multi-core platforms (the Intel Xeon and the Cell processors). Performance of our technique is stable across programs and architectures. On average, it delivers above 96% performance of the maximum available on both platforms. It achieve, on average, a 37% (up to 17.5 times) performance improvement over the OpenMP runtime default scheme on the Cell platform. Compared to two recent prediction models, our predictors achieve better performance with a significant lower profiling cost.", "paper_title": "Mapping Parallelism to Multi-cores: A Machine Learning Based Approach", "paper_id": "WOS:000272014600009"}