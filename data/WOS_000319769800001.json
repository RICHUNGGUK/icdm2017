{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "replica_consistency"}, {"score": 0.04170143341718086, "phrase": "symmetric_difference"}, {"score": 0.040464845026801276, "phrase": "fast_synchronization"}, {"score": 0.03939520645819031, "phrase": "new_records"}, {"score": 0.03455793989549367, "phrase": "large_table"}, {"score": 0.00478163886605204, "phrase": "distributed_databases"}, {"score": 0.004715702140552199, "phrase": "distributed_database"}, {"score": 0.00466684417181804, "phrase": "large_table_replicas"}, {"score": 0.004634552430216732, "phrase": "frequent_asynchronous_insertions"}, {"score": 0.004586531354697466, "phrase": "challenging_problem"}, {"score": 0.004278807087784626, "phrase": "efficient_algorithms"}, {"score": 0.004047500054405469, "phrase": "set_reconciliation_algorithms"}, {"score": 0.00395026016389782, "phrase": "replicated_tables"}, {"score": 0.003909301559145041, "phrase": "distributed_relational_database"}, {"score": 0.003775803846431323, "phrase": "small_sets"}, {"score": 0.0035222812668916736, "phrase": "table_replica"}, {"score": 0.0034376160364306637, "phrase": "asynchronous_manner"}, {"score": 0.003308649946463136, "phrase": "distributed_consistency"}, {"score": 0.0030650070965891653, "phrase": "complementary_algorithm"}, {"score": 0.0030017207229983385, "phrase": "summarization_table"}, {"score": 0.0029705677977833857, "phrase": "foreign_keys"}, {"score": 0.0028000270988465486, "phrase": "distributed_systems_perspective"}, {"score": 0.002761340122199076, "phrase": "first_algorithm"}, {"score": 0.002657677633954843, "phrase": "second_case"}, {"score": 0.002584733102974067, "phrase": "database_algorithms"}, {"score": 0.002566811158487033, "phrase": "linear_communication_complexity"}, {"score": 0.0025490131630603757, "phrase": "cubic_time_complexity"}, {"score": 0.0024704333220467393, "phrase": "respective_table_replicas"}, {"score": 0.0023611760395203106, "phrase": "network_speed"}, {"score": 0.0023285383529595416, "phrase": "cpu_speed"}, {"score": 0.002280423827531483, "phrase": "performance_experimental_evaluation"}, {"score": 0.002264607102799332, "phrase": "synthetic_and_real_databases"}, {"score": 0.002202427244004817, "phrase": "previous_state-of-the_art_algorithm"}, {"score": 0.0021419510072373756, "phrase": "complete_tables"}, {"score": 0.002119702106867801, "phrase": "large_replicated_tables"}, {"score": 0.0021049977753042253, "phrase": "sporadic_asynchronous_insertions"}], "paper_keywords": ["Set reconciliation", " Replica consistency", " Distributed databases"], "paper_abstract": "In a distributed database, maintaining large table replicas with frequent asynchronous insertions is a challenging problem that requires carefully managing a tradeoff between consistency and availability. With that motivation in mind, we propose efficient algorithms to repair and measure replica consistency. Specifically, we adapt, extend and optimize distributed set reconciliation algorithms to efficiently compute the symmetric difference between replicated tables in a distributed relational database. Our novel algorithms enable fast synchronization of replicas being updated with small sets of new records, measuring obsolence of replicas having many insertions and deciding when to update a replica, as each table replica is being continuously updated in an asynchronous manner. We first present an algorithm to repair and measure distributed consistency on a large table continuously updated with new records at several sites when the number of insertions is small. We then present a complementary algorithm that enables fast synchronization of a summarization table based on foreign keys when the number of insertions is large, but happening on a few foreign key values. From a distributed systems perspective, in the first algorithm the large table with data is reconciled, whereas in the second case, its summarization table is reconciled. Both distributed database algorithms have linear communication complexity and cubic time complexity in the size of the symmetric difference between the respective table replicas they work on. That is, they are effective when the network speed is smaller than CPU speed at each site. A performance experimental evaluation with synthetic and real databases shows our algorithms are faster than a previous state-of-the art algorithm as well as more efficient than transferring complete tables, assuming large replicated tables and sporadic asynchronous insertions.", "paper_title": "Efficiently repairing and measuring replica consistency in distributed databases", "paper_id": "WOS:000319769800001"}