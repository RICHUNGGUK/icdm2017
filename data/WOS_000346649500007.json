{"auto_keywords": [{"score": 0.004779085953614184, "phrase": "regression_testing"}, {"score": 0.004655640997424794, "phrase": "test_suite"}, {"score": 0.004620957579769604, "phrase": "different_software_versions"}, {"score": 0.004501578643090429, "phrase": "functional_testing"}, {"score": 0.004208515754292201, "phrase": "nonfunctional_testing"}, {"score": 0.004146015717402716, "phrase": "performance_testing"}, {"score": 0.003569681826459428, "phrase": "existing_profilers"}, {"score": 0.0034385353123700885, "phrase": "source_code"}, {"score": 0.0031195948946937378, "phrase": "open_source_implementation"}, {"score": 0.003073216350080926, "phrase": "spectraperf"}, {"score": 0.0029381648092259064, "phrase": "field_user_study"}, {"score": 0.002916238910306095, "phrase": "tribler"}, {"score": 0.0028195727965553367, "phrase": "peer_client"}, {"score": 0.0027261023678401696, "phrase": "dispersy"}, {"score": 0.0025965187247488876, "phrase": "performance_optimization_process"}, {"score": 0.0025198477493173165, "phrase": "performance_bottlenecks"}, {"score": 0.002364338228716365, "phrase": "performance_optimizations"}, {"score": 0.0022945073938622287, "phrase": "feasibility_study"}, {"score": 0.0022773741222930463, "phrase": "django"}, {"score": 0.002235154811733802, "phrase": "python"}, {"score": 0.002210111187301831, "phrase": "github"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["performance regressions", " performance analysis", " performance optimization"], "paper_abstract": "Regression testing can be performed by reexecuting a test suite on different software versions and comparing the outcome. For functional testing, the outcome of such tests is either pass (correct behavior) or fail (incorrect behavior). For nonfunctional testing, such as performance testing, this is more challenging as correct and incorrect are not clearly defined concepts for these types of testing. In this paper, we present an approach for detecting and analyzing input/output (I/O) performance regressions. Our method is supplemental to existing profilers, and its goal is to analyze the effect of source code changes on the performance of a system. In this paper, we focus on analyzing the amount of I/O writes being performed. The open source implementation of our approach, SPECTRAPERF, is available for download. We evaluate our approach in a field user study on Tribler, an open source peer-to-peer client and its decentralized solution for synchronizing messages, Dispersy. In this evaluation, we show that our approach can guide the performance optimization process, as it helps developers to find performance bottlenecks on the one hand and, on the other, allows them to validate the effect of performance optimizations. In addition, we perform a feasibility study on Django, the most popular Python project on Github, to demonstrate our applicability on other projects. Copyright (c) 2014 John Wiley & Sons, Ltd.", "paper_title": "Detecting and analyzing I/O performance regressions", "paper_id": "WOS:000346649500007"}