{"auto_keywords": [{"score": 0.04292916795380168, "phrase": "lvtm"}, {"score": 0.03581943329664302, "phrase": "recognition_rate"}, {"score": 0.00481495049065317, "phrase": "cross-pose_face_recognition_-_a_virtual_view_generation_approach"}, {"score": 0.004772137871562584, "phrase": "clustering_based_lvtm."}, {"score": 0.004563691742821759, "phrase": "face_recognition"}, {"score": 0.004523103062368843, "phrase": "virtual_view_generation"}, {"score": 0.004009074013302036, "phrase": "linear_transformation"}, {"score": 0.003973398317120165, "phrase": "pixel_values"}, {"score": 0.0039380388343479384, "phrase": "frontal_and_non-frontal_image_pairs"}, {"score": 0.0037827603118271757, "phrase": "small_region"}, {"score": 0.0036335821512795026, "phrase": "entire_image_pattern"}, {"score": 0.0034437574674180365, "phrase": "appearance_transition_model"}, {"score": 0.0032638170723261538, "phrase": "inherent_linear_relationship"}, {"score": 0.0032347516831068715, "phrase": "frontal-nonfrontal_face_image_patch_pairs"}, {"score": 0.002918483026678636, "phrase": "intuitively_frontal-nonfrontal_patch_pairs"}, {"score": 0.0028411768240129585, "phrase": "stronger_linear_relationship"}, {"score": 0.002778317177178732, "phrase": "specific_location"}, {"score": 0.0027047136845332917, "phrase": "common_transformation"}, {"score": 0.002621297299343551, "phrase": "corresponding_local_patches"}, {"score": 0.0025404470150468387, "phrase": "appearance_similarity_distance_metric"}, {"score": 0.0024953686545818587, "phrase": "transition_models"}, {"score": 0.0023861329619604666, "phrase": "testing_stage"}, {"score": 0.002354302193867031, "phrase": "local_patch"}, {"score": 0.0023125191783901367, "phrase": "non-frontal_probe_image"}, {"score": 0.0022613292987262177, "phrase": "learned_local_view_transition_model"}, {"score": 0.0021915573326125428, "phrase": "experimental_results"}, {"score": 0.002162316557411786, "phrase": "real-world_face_dataset"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["face recognition", " pose invariant", " clustering", " local view transition model"], "paper_abstract": "This paper presents an approach for cross-pose face recognition by virtual view generation using an appearance clustering based local view transition model. Previously, the traditional global pattern based view transition model (VTM) method was extended to its local version called LVTM, which learns the linear transformation of pixel values between frontal and non-frontal image pairs from training images using partial image in a small region for each location, instead of transforming the entire image pattern. In this paper we show that the accuracy of the appearance transition model and the recognition rate can be further improved by better exploiting the inherent linear relationship between frontal-nonfrontal face image patch pairs. This is achieved based on the observation that variations in appearance caused by pose are closely related to the corresponding 3D structure and intuitively frontal-nonfrontal patch pairs from more similar local 3D face structures should have a stronger linear relationship. Thus for each specific location, instead of learning a common transformation as in the LVTM, the corresponding local patches are first clustered based on an appearance similarity distance metric and then the transition models are learned separately for each cluster. In the testing stage, each local patch for the input non-frontal probe image is transformed using the learned local view transition model corresponding to the most visually similar cluster. The experimental results on a real-world face dataset demonstrated the superiority of the proposed method in terms of recognition rate.", "paper_title": "Cross-Pose Face Recognition - A Virtual View Generation Approach Using Clustering Based LVTM", "paper_id": "WOS:000320214300020"}