{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multi-step_learning"}, {"score": 0.004650330504968421, "phrase": "dynamic_environment_navigation"}, {"score": 0.004337710093279712, "phrase": "existing_rule-based_approaches"}, {"score": 0.003644714340300824, "phrase": "tedious_rule"}, {"score": 0.0035507457285983268, "phrase": "parameter_tuning_procedures"}, {"score": 0.003340768341189764, "phrase": "simple_dynamic_environments"}, {"score": 0.002957222630196853, "phrase": "proper_feature"}, {"score": 0.002855934865505357, "phrase": "proposed_data"}, {"score": 0.0028065957753430713, "phrase": "refinement_procedure"}, {"score": 0.002640507929183363, "phrase": "multi-step_learning_approach"}, {"score": 0.002594880866651459, "phrase": "goal-related_information"}, {"score": 0.0024412911465483225, "phrase": "successive_motion_behavior"}, {"score": 0.0023371689326096476, "phrase": "complex_environments"}, {"score": 0.0021049977753042253, "phrase": "motion_control_module"}], "paper_keywords": ["learning from demonstration", " robot navigation", " dynamic environments", " learning to search", " motion behavior learning"], "paper_abstract": "While navigation could be done using existing rule-based approaches, it becomes more attractive to use learning from demonstration (LfD) approaches to ease the burden of tedious rule designing and parameter tuning procedures. In our previous work, navigation in simple dynamic environments is achieved using the Learning to Search (LEARCH) algorithm with a proper feature set and the proposed data set refinement procedure. In this paper, the multi-step learning approach with goal-related information is proposed to further capture the successive motion behavior of the user in complex environments. The behaviors of the demonstrator could be matched by the motion control module in which policies of the demonstrator are well captured.", "paper_title": "Multi-Step Learning to Search for Dynamic Environment Navigation", "paper_id": "WOS:000340075300007"}