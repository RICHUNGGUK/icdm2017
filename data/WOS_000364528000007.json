{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "adaptivity_gap"}, {"score": 0.010528096841203892, "phrase": "stochastic_orienteering"}, {"score": 0.0056199526548938415, "phrase": "upper_bound"}, {"score": 0.004662913362513954, "phrase": "stochastic_orienteering_problem"}, {"score": 0.004625674550667542, "phrase": "gupta"}, {"score": 0.004533830486816886, "phrase": "soda"}, {"score": 0.004355516897622305, "phrase": "budget_b"}, {"score": 0.004084662575167727, "phrase": "deterministic_reward"}, {"score": 0.004035777314816881, "phrase": "random_processing_time"}, {"score": 0.003955593400628455, "phrase": "known_distribution"}, {"score": 0.0038925903520043623, "phrase": "processing_times"}, {"score": 0.0037095162593960727, "phrase": "non-anticipatory_policy"}, {"score": 0.003549245218355768, "phrase": "different_vertices"}, {"score": 0.0034926916198124484, "phrase": "expected_reward"}, {"score": 0.0034232607917050392, "phrase": "total_distance"}, {"score": 0.00328849874768242, "phrase": "adaptive_policy"}, {"score": 0.003236086384959101, "phrase": "next_vertex"}, {"score": 0.003171740347925348, "phrase": "observed_random_instantiations"}, {"score": 0.003108669771132255, "phrase": "non-adaptive_policy"}, {"score": 0.003034633345146293, "phrase": "fixed_ordering"}, {"score": 0.0029386456078004863, "phrase": "worst-case_ratio"}, {"score": 0.0029034358982402346, "phrase": "optimal_adaptive_and_non-adaptive_rewards"}, {"score": 0.002722634472231842, "phrase": "negative_answer"}, {"score": 0.0026577673946415583, "phrase": "gupta_et_al"}, {"score": 0.0023369711258107244, "phrase": "correlated_stochastic_orienteering_problem"}, {"score": 0.0021391300942881, "phrase": "improved_quasi-polynomial_time"}, {"score": 0.0021049977753042253, "phrase": "correlated_stochastic_orienteering"}], "paper_keywords": ["Approximation algorithms", " Stochastic", " Vehicle routing"], "paper_abstract": "The input to the stochastic orienteering problem (Gupta et al. in SODA, pp 1522-1538, 2012) consists of a budget B and metric (V, d) where each vertex has a job with a deterministic reward and a random processing time (drawn from a known distribution). The processing times are independent across vertices. The goal is to obtain a non-anticipatory policy (originating from a given root vertex) to run jobs at different vertices, that maximizes expected reward, subject to the total distance traveled plus processing times being at most B. An adaptive policy can choose the next vertex to visit based on observed random instantiations. Whereas, a non-adaptive policy is just given by a fixed ordering of vertices. The adaptivity gap is the worst-case ratio of the optimal adaptive and non-adaptive rewards. We prove an lower bound on the adaptivity gap of stochastic orienteering. This provides a negative answer to the O(1)-adaptivity gap conjectured by Gupta et al. (2012), and comes close to the upper bound. This result holds even on a line metric. We also show an upper bound on the adaptivity gap for the correlated stochastic orienteering problem, where the reward of each job is random and possibly correlated to its processing time. Using this, we obtain an improved quasi-polynomial time -approximation algorithm for correlated stochastic orienteering.", "paper_title": "On the adaptivity gap of stochastic orienteering", "paper_id": "WOS:000364528000007"}