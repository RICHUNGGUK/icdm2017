{"auto_keywords": [{"score": 0.03992659287798476, "phrase": "imbalanced_datasets"}, {"score": 0.00481495049065317, "phrase": "effective_imbalanced_classification"}, {"score": 0.004763565662615503, "phrase": "real-life_datasets"}, {"score": 0.004514714250357749, "phrase": "significantly_more_training_samples"}, {"score": 0.004210458358142504, "phrase": "overall_classification_accuracy"}, {"score": 0.003584025547693836, "phrase": "cost-sensitive_classification"}, {"score": 0.0034148795808317555, "phrase": "effective_ensemble"}, {"score": 0.003378385493203297, "phrase": "cost-sensitive_decision_trees"}, {"score": 0.0032888418378433037, "phrase": "base_classifiers"}, {"score": 0.0030834712590249863, "phrase": "random_feature_subspaces"}, {"score": 0.003034158379493625, "phrase": "sufficient_diversity"}, {"score": 0.002985631781749091, "phrase": "ensemble_members"}, {"score": 0.0029064676523923886, "phrase": "evolutionary_algorithm"}, {"score": 0.002875391177926809, "phrase": "simultaneous_classifier_selection"}, {"score": 0.0028142286985235977, "phrase": "committee_member_weights"}, {"score": 0.002769209760453212, "phrase": "fusion_process"}, {"score": 0.0026384167183586015, "phrase": "benchmark_datasets"}, {"score": 0.0025273383208209922, "phrase": "improved_recognition"}, {"score": 0.0024868972942701582, "phrase": "minority_class"}, {"score": 0.0022213218042340735, "phrase": "useful_and_effective_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Machine learning", " Multiple classifier system", " Ensemble classifier", " Imbalanced classification", " Cost-sensitive classification", " Decision tree", " Classifier selection", " Evolutionary algorithms", " Classifier fusion"], "paper_abstract": "Real-life datasets are often imbalanced, that is, there are significantly more training samples available for some classes than for others, and consequently the conventional aim of reducing overall classification accuracy is not appropriate when dealing with such problems. Various approaches have been introduced in the literature to deal with imbalanced datasets, and are typically based on oversampling, undersampling or cost-sensitive classification. In this paper, we introduce an effective ensemble of cost-sensitive decision trees for imbalanced classification. Base classifiers are constructed according to a given cost matrix, but are trained on random feature subspaces to ensure sufficient diversity of the ensemble members. We employ an evolutionary algorithm for simultaneous classifier selection and assignment of committee member weights for the fusion process. Our proposed algorithm is evaluated on a variety of benchmark datasets, and is confirmed to lead to improved recognition of the minority class, to be capable of outperforming other state-of-the-art algorithms, and hence to represent a useful and effective approach for dealing with imbalanced datasets. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Cost-sensitive decision tree ensembles for effective imbalanced classification", "paper_id": "WOS:000327529200021"}