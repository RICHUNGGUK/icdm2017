{"auto_keywords": [{"score": 0.04809176147872219, "phrase": "multiple_instances"}, {"score": 0.03497872780869258, "phrase": "available_storage_resources"}, {"score": 0.00481495049065317, "phrase": "workflow_computations"}, {"score": 0.004774324145784758, "phrase": "hpc_systems"}, {"score": 0.004734038956773703, "phrase": "storage_constraints"}, {"score": 0.004694092086199672, "phrase": "workflow-based_workloads"}, {"score": 0.004404994123834589, "phrase": "data_dependencies"}, {"score": 0.004294381488413906, "phrase": "well-defined_scientific_computation_task"}, {"score": 0.0040298010803060495, "phrase": "high_degree"}, {"score": 0.00378145978325955, "phrase": "potential_problem"}, {"score": 0.003533347967582184, "phrase": "deadlock_detection-based_scheduling"}, {"score": 0.0034154440860550564, "phrase": "high_performance"}, {"score": 0.0033579708256295847, "phrase": "best_use"}, {"score": 0.0032184697893776052, "phrase": "dataflow_information"}, {"score": 0.003071682293903262, "phrase": "instant_storage"}, {"score": 0.0030071857666598193, "phrase": "constituent_jobs"}, {"score": 0.0029191527226078286, "phrase": "whole_workflow_instance"}, {"score": 0.0028457440091791252, "phrase": "performance_anomaly"}, {"score": 0.002750721367040336, "phrase": "progress_workflow_instances"}, {"score": 0.002380934815435267, "phrase": "simulation-based_study"}, {"score": 0.0023111922860677672, "phrase": "dds_algorithm"}, {"score": 0.0022722584151582616, "phrase": "job_concurrency"}, {"score": 0.002215081186105616, "phrase": "higher_performance"}, {"score": 0.002177762848946681, "phrase": "avoidance_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Computational workflow", " Workflow scheduling", " Concurrency", " Storage resource constraints", " Deadlock detection"], "paper_abstract": "Workflow-based workloads usually consist of multiple instances of the same workflow, which are jobs with control or data dependencies, to carry out a well-defined scientific computation task, with each instance acting on its own input data. To maximize throughput performance, a high degree of concurrency is achievable by running multiple instances simultaneously. However, deadlock is a potential problem when storage is constrained. To address this problem, we design and evaluate a deadlock detection-based scheduling (DDS) algorithm that can achieve high performance by making the best use of the available storage resources. Our algorithm takes advantages of the dataflow information of the workflow to speculatively schedule each instance if the instant storage is sufficient for some constituent jobs, but not necessarily for the whole workflow instance. Whenever deadlock or a performance anomaly is detected, some selected in-progress workflow instances are required to be rollbacked to release storage for other blocked jobs. We develop a suite of strategies to select the victims and beneficiaries (instances or jobs) and evaluate their performance via a simulation-based study. Our results show that the DDS algorithm can adapt the job concurrency to the available storage resources and achieve higher performance than some deadlock avoidance methods in our synthetic and real workflow computations. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "DDS: A deadlock detection-based scheduling algorithm for workflow computations in HPC systems with storage constraints", "paper_id": "WOS:000322608700001"}