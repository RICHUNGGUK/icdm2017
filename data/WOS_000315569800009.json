{"auto_keywords": [{"score": 0.04194133676529322, "phrase": "eye_movements"}, {"score": 0.00481495049065317, "phrase": "visual_saliency"}, {"score": 0.00471700544744796, "phrase": "feature_map"}, {"score": 0.0046239141335338895, "phrase": "bias"}, {"score": 0.0045084510240032535, "phrase": "important_role"}, {"score": 0.00445318311339027, "phrase": "visual_attention"}, {"score": 0.00430907756010007, "phrase": "face_detection"}, {"score": 0.004256243266481051, "phrase": "classical_visual_attention_model"}, {"score": 0.004204054047427536, "phrase": "eye_movement_predictions"}, {"score": 0.00405127920820053, "phrase": "visual_saliency_model"}, {"score": 0.003968806706727518, "phrase": "free_viewing"}, {"score": 0.0037621210719690594, "phrase": "visual_system"}, {"score": 0.0036402965997795536, "phrase": "video_database"}, {"score": 0.0035224030823976186, "phrase": "particular_visual_feature"}, {"score": 0.003284361433827464, "phrase": "spatial_frequency"}, {"score": 0.003204232566468624, "phrase": "'dynamic'_saliency_map"}, {"score": 0.0031004173389196387, "phrase": "motion_amplitude"}, {"score": 0.0030247625535682987, "phrase": "'face'_saliency_map"}, {"score": 0.0027514186391258263, "phrase": "behavioral_experiment"}, {"score": 0.0026953359825383624, "phrase": "record_eye_movements"}, {"score": 0.002544296663333569, "phrase": "models'_saliency_maps"}, {"score": 0.002431570196486513, "phrase": "center_bias"}, {"score": 0.0024017007954518065, "phrase": "saliency_maps"}, {"score": 0.002314271047073693, "phrase": "suitable_way"}, {"score": 0.0022484715639779153, "phrase": "efficient_fusion_method"}, {"score": 0.0021845387990972543, "phrase": "fused_master_saliency_map"}, {"score": 0.0021224200236681498, "phrase": "good_predictor"}, {"score": 0.0021049977753042253, "phrase": "participants'_eye_positions"}], "paper_keywords": ["Visual saliency model", " Static features", " Dynamic features", " Face features", " Center bias", " Gaze prediction", " Video"], "paper_abstract": "Faces play an important role in guiding visual attention, and thus, the inclusion of face detection into a classical visual attention model can improve eye movement predictions. In this study, we proposed a visual saliency model to predict eye movements during free viewing of videos. The model is inspired by the biology of the visual system and breaks down each frame of a video database into three saliency maps, each earmarked for a particular visual feature. (a) A 'static' saliency map emphasizes regions that differ from their context in terms of luminance, orientation and spatial frequency. (b) A 'dynamic' saliency map emphasizes moving regions with values proportional to motion amplitude. (c) A 'face' saliency map emphasizes areas where a face is detected with a value proportional to the confidence of the detection. In parallel, a behavioral experiment was carried out to record eye movements of participants when viewing the videos. These eye movements were compared with the models' saliency maps to quantify their efficiency. We also examined the influence of center bias on the saliency maps and incorporated it into the model in a suitable way. Finally, we proposed an efficient fusion method of all these saliency maps. Consequently, the fused master saliency map developed in this research is a good predictor of participants' eye positions.", "paper_title": "Improving Visual Saliency by Adding 'Face Feature Map' and 'Center Bias'", "paper_id": "WOS:000315569800009"}