{"auto_keywords": [{"score": 0.05007852962010534, "phrase": "kpca"}, {"score": 0.010404392730982424, "phrase": "feature_correlation_evaluation"}, {"score": 0.00820228753465242, "phrase": "basic_patterns"}, {"score": 0.0075015166927110775, "phrase": "feature_extraction"}, {"score": 0.004673290248999786, "phrase": "classic_kernel_principal_component_analysis"}, {"score": 0.004402295697276237, "phrase": "large_data_sets"}, {"score": 0.004024863549087511, "phrase": "computational_efficiency"}, {"score": 0.003906357144047631, "phrase": "linear_combination"}, {"score": 0.003848413823574867, "phrase": "small_portion"}, {"score": 0.0038102613924627647, "phrase": "training_samples"}, {"score": 0.0036070601923916196, "phrase": "kpca_feature_extractor"}, {"score": 0.003448863509392547, "phrase": "covariance_matrix"}, {"score": 0.0032975819773787985, "phrase": "feature_correlation"}, {"score": 0.0031845067058084583, "phrase": "different_feature_components"}, {"score": 0.003075296862372287, "phrase": "cosine_distance"}, {"score": 0.0030296432347510687, "phrase": "kernel_vectors"}, {"score": 0.002955050521440921, "phrase": "column_vectors"}, {"score": 0.0029111767710750117, "phrase": "kernel_matrix"}, {"score": 0.00286795254228461, "phrase": "proposed_algorithm"}, {"score": 0.0025827837420330816, "phrase": "kpca_model"}, {"score": 0.002481747081849501, "phrase": "test_samples"}, {"score": 0.002349228457383314, "phrase": "ekpca_feature_extraction"}, {"score": 0.0022460727898764216, "phrase": "kpca._experimental_results"}, {"score": 0.0021907299375981356, "phrase": "ekpca"}, {"score": 0.0021049977753042253, "phrase": "similar_classification_performance"}], "paper_keywords": ["Kernel principal component analysis (KPCA)", " Feature extraction", " Feature correlation", " Cosine distance"], "paper_abstract": "Classic kernel principal component analysis (KPCA) is less computationally efficient when extracting features from large data sets. In this paper, we propose an algorithm, that is, efficient KPCA (EKPCA), that enhances the computational efficiency of KPCA by using a linear combination of a small portion of training samples, referred to as basic patterns, to approximately express the KPCA feature extractor, that is, the eigenvector of the covariance matrix in the feature extraction. We show that the feature correlation (i.e., the correlation between different feature components) can be evaluated by the cosine distance between the kernel vectors, which are the column vectors in the kernel matrix. The proposed algorithm can be easily implemented. It first uses feature correlation evaluation to determine the basic patterns and then uses these to reconstruct the KPCA model, perform feature extraction, and classify the test samples. Since there are usually many fewer basic patterns than training samples, EKPCA feature extraction is much more computationally efficient than that of KPCA. Experimental results on several benchmark data sets show that EKPCA is much faster than KPCA while achieving similar classification performance.", "paper_title": "An efficient KPCA algorithm based on feature correlation evaluation", "paper_id": "WOS:000336371900029"}