{"auto_keywords": [{"score": 0.039059442498076156, "phrase": "spontaneous_vesicle_formation"}, {"score": 0.03386730131115612, "phrase": "distributed_program"}, {"score": 0.033671496848353466, "phrase": "test_data"}, {"score": 0.00481495049065317, "phrase": "dissipative_particle_dynamics_simulations"}, {"score": 0.004691935905355025, "phrase": "scalable_dissipative_particle_dynamics_simulation_code"}, {"score": 0.004526755897762385, "phrase": "single_gpu"}, {"score": 0.004419864931421362, "phrase": "unified_framework"}, {"score": 0.004367365549837598, "phrase": "efficient_generation"}, {"score": 0.004341349148702215, "phrase": "neighbor_list"}, {"score": 0.004315487055548749, "phrase": "particle_data_locality"}, {"score": 0.004264222169432177, "phrase": "strictly_ordered_neighbor_lists"}, {"score": 0.0041469500929423595, "phrase": "atomic_operations"}, {"score": 0.004089524095913172, "phrase": "optimal_data_loading_efficiency"}, {"score": 0.004048991059922241, "phrase": "two-level_particle_reordering_scheme"}, {"score": 0.004008858147188122, "phrase": "situ_generation_scheme"}, {"score": 0.003992916137718416, "phrase": "gaussian_random_numbers"}, {"score": 0.003961221300821288, "phrase": "precomputed_binary_signatures"}, {"score": 0.003929777057504885, "phrase": "custom_transcendental_functions"}, {"score": 0.0038599335861338796, "phrase": "pairwise_interaction"}, {"score": 0.003746265044326905, "phrase": "test_cases"}, {"score": 0.003731363375583514, "phrase": "poiseuille_flow"}, {"score": 0.0037017369673493735, "phrase": "computer_benchmarks"}, {"score": 0.003643185395780667, "phrase": "cpu_implementation"}, {"score": 0.003614256566240836, "phrase": "strong_and_weak_scalability"}, {"score": 0.003592710247532986, "phrase": "large-scale_simulation"}, {"score": 0.003479939791315719, "phrase": "real-world_applications"}, {"score": 0.0034542399093181004, "phrase": "program"}, {"score": 0.0034112567202121045, "phrase": "dpd_package_for_lammps_catalogue"}, {"score": 0.0033306180744375616, "phrase": "cpc_program_library"}, {"score": 0.003317364235978494, "phrase": "queen's_university"}, {"score": 0.003304163456395087, "phrase": "belfast"}, {"score": 0.0032910140541010232, "phrase": "n._ireland"}, {"score": 0.003264872488939969, "phrase": "gnu_general_public_license"}, {"score": 0.0029906308118343044, "phrase": "coda"}, {"score": 0.0029374224609871815, "phrase": "nvidia_gpgpus"}, {"score": 0.0028852301636448853, "phrase": "linux"}, {"score": 0.002733889729377665, "phrase": "mbytes"}, {"score": 0.0025041309750764326, "phrase": "mesoscale_systems"}, {"score": 0.0024743302255475475, "phrase": "spontaneous_self-assembly_process"}, {"score": 0.0024109700191474693, "phrase": "coarse-grained_particles"}, {"score": 0.0023965798153709377, "phrase": "pairwise_potentials"}, {"score": 0.0023870339802513863, "phrase": "bonded_potentials"}, {"score": 0.0023775260766039476, "phrase": "classical_mechanics"}, {"score": 0.0023586234642944317, "phrase": "newton's_laws"}, {"score": 0.002307410819786043, "phrase": "time-stepping_scheme"}, {"score": 0.00229363730209177, "phrase": "velocity-verlet"}, {"score": 0.002252806984976934, "phrase": "cuda_gpgpus"}, {"score": 0.0021907299375981356, "phrase": "significant_speedup"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["DPD", " CUDA", " LAMMPS", " Spontaneous vesicle formation"], "paper_abstract": "We present a scalable dissipative particle dynamics simulation code, fully implemented on the Graphics Processing Units (GPUs) using a hybrid CUDA/MPI programming model, which achieves 10-30 times speedup on a single GPU over 16 CPU cores and almost linear weak scaling across a thousand nodes. A unified framework is developed within which the efficient generation of the neighbor list and maintaining particle data locality are addressed. Our algorithm generates strictly ordered neighbor lists in parallel, while the construction is deterministic and makes no use of atomic operations or sorting. Such neighbor list leads to optimal data loading efficiency when combined with a two-level particle reordering scheme. A faster in situ generation scheme for Gaussian random numbers is proposed using precomputed binary signatures. We designed custom transcendental functions that are fast and accurate for evaluating the pairwise interaction. The correctness and accuracy of the code is verified through a set of test cases simulating Poiseuille flow and spontaneous vesicle formation. Computer benchmarks demonstrate the speedup of our implementation over the CPU implementation as well as strong and weak scalability. A large-scale simulation of spontaneous vesicle formation consisting of 128 million particles was conducted to further illustrate the practicality of our code in real-world applications. Program summary Program title: CPU-accelerated DPD Package for LAMMPS Catalogue identifier: AETN_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AETN_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: I 602 716 No. of bytes in distributed program, including test data, etc.: 26 489 166 Distribution format: tar.gz Programming language: C/C++, CODA C/C++, MPI. Computer: Any computers having nVidia GPGPUs with compute capability 3.0. Operating system: Linux. Has the code been vectorized or parallelized?: Yes. Number of processors used: 1024 16-core CPUs and 1024 GPUs RAM: 500 Mbytes host memory, 2 Gbytes device memory Supplementary material: The data for the examples discussed in the manuscript is available for download. Classification: 6.5, 12, 16.1, 16.11. Nature of problem: Particle-based simulation of mesoscale systems involving nano/micro-fluids, polymers and spontaneous self-assembly process. Solution method: The system is approximated by a number of coarse-grained particles interacting through pairwise potentials and bonded potentials. Classical mechanics is assumed following Newton's laws. The evolution of the system is integrated using a time-stepping scheme such as Velocity-Verlet. Restrictions: The code runs only on CUDA GPGPUs with compute capability 3.0. Unusual features: Fully implemented on GPGPUs with significant speedup. Running time: 78 h using 1024 GPGPUs for simulating a 128-million-particle system for 18.4 million time steps. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Accelerating dissipative particle dynamics simulations on GPUs: Algorithms, numerics and applications", "paper_id": "WOS:000342244900002"}