{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "different_parallel_implementations"}, {"score": 0.04962913836247387, "phrase": "scientific_applications"}, {"score": 0.04874179879364107, "phrase": "energy_consumption"}, {"score": 0.027667066894097706, "phrase": "hybrid_implementation"}, {"score": 0.023317241230105163, "phrase": "best_case"}, {"score": 0.004724878518219719, "phrase": "multicore_systems"}, {"score": 0.004614643339546038, "phrase": "major_concern"}, {"score": 0.004571269461883144, "phrase": "high-performance_multicore_systems"}, {"score": 0.003986103192038901, "phrase": "message-passing_interface"}, {"score": 0.003802276329283592, "phrase": "nasa"}, {"score": 0.0037843067634883184, "phrase": "advanced"}, {"score": 0.0031619994642255846, "phrase": "central_processing_unit"}, {"score": 0.002931569771634119, "phrase": "power_information"}, {"score": 0.0028495431343111897, "phrase": "eight_nodes"}, {"score": 0.002555761854502478, "phrase": "best_implementation"}, {"score": 0.0022175019341824603, "phrase": "cpu_frequency_scaling"}, {"score": 0.0021656385587456952, "phrase": "energy_saving"}, {"score": 0.0021049977753042253, "phrase": "execution_time"}], "paper_keywords": ["energy consumption", " frequency scaling", " hybrid MPI/OpenMP", " MPI", " multicore system", " performance characteristics", " scientific applications"], "paper_abstract": "Energy consumption is a major concern with high-performance multicore systems. In this paper, we explore the energy consumption and performance (execution time) characteristics of different parallel implementations of scientific applications. In particular, the experiments focus on message-passing interface (MPI)-only versus hybrid MPI/OpenMP implementations for hybrid the NAS (NASA Advanced Supercomputing) BT (Block Tridiagonal) benchmark (strong scaling), a Lattice Boltzmann application (strong scaling), and a Gyrokinetic Toroidal Code - GTC (weak scaling), as well as central processing unit (CPU) frequency scaling. Experiments were conducted on a system instrumented to obtain power information; this system consists of eight nodes with four cores per node. The results indicate, with respect to the MPI-only versus the hybrid implementation, that the best implementation is dependent upon the application executed on 16 or fewer cores. For the case of 32 cores, the results were consistent in that hybrid implementation resulted in less execution time and energy. With CPU frequency scaling, the best case for energy saving was not the best case for execution time.", "paper_title": "Energy and performance characteristics of different parallel implementations of scientific applications on multicore systems", "paper_id": "WOS:000293573900009"}