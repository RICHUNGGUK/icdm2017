{"auto_keywords": [{"score": 0.04755154299589266, "phrase": "probabilistic_context-free_grammars"}, {"score": 0.011779070677750605, "phrase": "infinite_set"}, {"score": 0.00481495049065317, "phrase": "infinite_distributions"}, {"score": 0.004409127897028587, "phrase": "generative_devices"}, {"score": 0.00414830454375092, "phrase": "syntactic_pattern_matching"}, {"score": 0.0040373708328770306, "phrase": "statistical_natural_language_parsing"}, {"score": 0.0031415984808041324, "phrase": "context-free_approximation"}, {"score": 0.002724428781940696, "phrase": "previously_unknown_equivalence"}, {"score": 0.002669525619637691, "phrase": "grammar_cross-entropy"}, {"score": 0.0026157259734105, "phrase": "input_distribution"}, {"score": 0.0025630077791393125, "phrase": "so-called_derivational_entropy"}, {"score": 0.002427549902743566, "phrase": "important_consequences"}, {"score": 0.002330664671716791, "phrase": "standard_application"}, {"score": 0.0022836785412390544, "phrase": "maximum-likelihood_estimator"}, {"score": 0.002252880693912436, "phrase": "finite_tree_and_sentence_samples"}, {"score": 0.0021337783230121307, "phrase": "hidden_markov_models"}, {"score": 0.0021049977753042253, "phrase": "probabilistic_finite_automata"}], "paper_keywords": [""], "paper_abstract": "In this paper, we consider probabilistic context-free grammars, a class of generative devices that has been successfully exploited in several applications of syntactic pattern matching, especially in statistical natural language parsing. We investigate the problem of training probabilistic context-free grammars on the basis of distributions defined over an infinite set of trees or an infinite set of sentences by minimizing the cross-entropy. This problem has applications in cases of context-free approximation of distributions generated by more expressive statistical models. We show several interesting theoretical properties of probabilistic context-free grammars that are estimated in this way, including the previously unknown equivalence between the grammar cross-entropy with the input distribution and the so-called derivational entropy of the grammar itself. We discuss important consequences of these results involving the standard application of the maximum-likelihood estimator on finite tree and sentence samples, as well as other finite-state models such as Hidden Markov Models and probabilistic finite automata.", "paper_title": "Probabilistic context-free grammars estimated from infinite distributions", "paper_id": "WOS:000247186500007"}