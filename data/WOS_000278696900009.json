{"auto_keywords": [{"score": 0.025309521898389746, "phrase": "roc"}, {"score": 0.004235717186207754, "phrase": "observer's_prior_beliefs"}, {"score": 0.00413684709497912, "phrase": "bayes_theorem"}, {"score": 0.003972673543700013, "phrase": "natural_way"}, {"score": 0.00384083110935486, "phrase": "prior_and_posterior_distributions"}, {"score": 0.003763827319426842, "phrase": "available_space"}, {"score": 0.0035539019235553897, "phrase": "dynamic_situations"}, {"score": 0.0032332756920184177, "phrase": "exponential_family"}, {"score": 0.0031258948882273057, "phrase": "sequential_bayesian_learning"}, {"score": 0.0030118779844670353, "phrase": "training_examples"}, {"score": 0.0029915976131073825, "phrase": "theoretical_properties"}, {"score": 0.0028630535573013686, "phrase": "shannon's_definition"}, {"score": 0.002815093247670994, "phrase": "computer_vision_neural_network_architecture"}, {"score": 0.0027123819773059127, "phrase": "video_stimuli"}, {"score": 0.0026759663480185047, "phrase": "surprising_data"}, {"score": 0.002640038329840458, "phrase": "natural_or_artificial_attention_systems"}, {"score": 0.0025436978068459565, "phrase": "psychophysical_experiment"}, {"score": 0.0025180372061991206, "phrase": "human_eye_movements"}, {"score": 0.0024758420717852113, "phrase": "natural_video_stimuli"}, {"score": 0.0024179512279236207, "phrase": "robust_performance"}, {"score": 0.0023935561037315375, "phrase": "human_gaze"}, {"score": 0.002259883463607508, "phrase": "human_inter-observer_repeatability"}, {"score": 0.0022145052062614514, "phrase": "simpler_intensity_contrast-based_predictor"}, {"score": 0.002148136905808445, "phrase": "resulting_theory"}, {"score": 0.0021049977753042253, "phrase": "different_spatio-temporal_scales"}], "paper_keywords": ["Information", " Surprise", " Relative entropy", " Attention", " Eye movements"], "paper_abstract": "The amount of information contained in a piece of data can be measured by the effect this data has on its observer. Fundamentally, this effect is to transform the observer's prior beliefs into posterior beliefs, according to Bayes theorem. Thus the amount of information can be measured in a natural way by the distance (relative entropy) between the prior and posterior distributions of the observer over the available space of hypotheses. This facet of information, termed \"surprise\", is important in dynamic situations where beliefs change, in particular during learning and adaptation. Surprise can often be computed analytically, for instance in the case of distributions from the exponential family, or it can be numerically approximated. During sequential Bayesian learning, surprise decreases as the inverse of the number of training examples. Theoretical properties of surprise are discussed, in particular how it differs and complements Shannon's definition of information. A computer vision neural network architecture is then presented capable of computing surprise over images and video stimuli. Hypothesizing that surprising data ought to attract natural or artificial attention systems, the output of this architecture is used in a psychophysical experiment to analyze human eye movements in the presence of natural video stimuli. Surprise is found to yield robust performance at predicting human gaze (ROC-like ordinal dominance score similar to 0.7 compared to similar to 0.8 for human inter-observer repeatability, similar to 0.6 for simpler intensity contrast-based predictor, and 0.5 for chance). The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Of bits and wows: A Bayesian theory of surprise with applications to attention", "paper_id": "WOS:000278696900009"}