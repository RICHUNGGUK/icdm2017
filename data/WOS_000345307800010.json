{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "state_abstraction"}, {"score": 0.015473436227329588, "phrase": "reinforcement_learning"}, {"score": 0.01168417502747779, "phrase": "future_tasks"}, {"score": 0.006998093838067057, "phrase": "similarity_measures"}, {"score": 0.004776669953449352, "phrase": "knowledge_discovery"}, {"score": 0.004663636232080366, "phrase": "value_function_approximation"}, {"score": 0.004626553103375699, "phrase": "powerful_and_useful_methods"}, {"score": 0.004553265070135597, "phrase": "memory_management"}, {"score": 0.004463278123628969, "phrase": "traditional_trends"}, {"score": 0.004254467919664946, "phrase": "current_task"}, {"score": 0.004137184278868391, "phrase": "multiple_similar_environments"}, {"score": 0.004087910779160781, "phrase": "general_setting"}, {"score": 0.003714195746979591, "phrase": "function_approximation"}, {"score": 0.0033745298175222056, "phrase": "optimal_value_functions"}, {"score": 0.003190878119415192, "phrase": "fuzzy_clustering"}, {"score": 0.0031277316566463978, "phrase": "hard_clustering"}, {"score": 0.002898930717812433, "phrase": "second_part"}, {"score": 0.002752075650871066, "phrase": "fuzzy_value_approximation"}, {"score": 0.0027192536281209463, "phrase": "single_task"}, {"score": 0.0024506642262508735, "phrase": "optimal_value_function"}, {"score": 0.0024214281919523973, "phrase": "abstract_space"}, {"score": 0.0022712795176156536, "phrase": "new_ways"}, {"score": 0.0021821915946356168, "phrase": "different_tasks"}, {"score": 0.0021049977753042253, "phrase": "state_space"}], "paper_keywords": ["Reinforcement learning", " state abstraction", " function approximation", " transfer learning", " fuzzy clustering"], "paper_abstract": "State abstraction and value function approximation are powerful and useful methods for time and memory management in reinforcement learning. In traditional trends, these methods are applied to speed up learning of the current task; however, when we learn multiple similar environments in a general setting, these methods can be applied to improve learning of other tasks. We propose a framework to aggregate the results of state abstraction and function approximation in several tasks of a domain to reuse them in future tasks of that domain. First, we show theoretically how abstraction based on optimal value functions speeds up learning in that same task in the future. In many situations, fuzzy clustering is more natural than hard clustering, since it does not force the states to fully belong to one of the classes. In second part, we examine theoretically and algorithmically how using the knowledge extracted by fuzzy value approximation of a single task improves learning of that same task in the future. In both parts, we show that aggregating states ( hard or fuzzy) preserves the optimal value function in the abstract space with an error bound. Having the support provided by these two parts, we propose new ways to combine the results of abstraction and approximation of different tasks of the domain to infer similarity measures on the state space. Finally, we show empirically that batch learning based on these similarity measures can speed up learning in the future tasks of the setting.", "paper_title": "Q*-based state abstraction and knowledge discovery in reinforcement learning", "paper_id": "WOS:000345307800010"}