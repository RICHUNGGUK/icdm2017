{"auto_keywords": [{"score": 0.04559872235894848, "phrase": "heuristic_rule_evaluation_functions"}, {"score": 0.00481495049065317, "phrase": "rule_evaluation"}, {"score": 0.004575053987970206, "phrase": "innovative_methods"}, {"score": 0.004516962628112147, "phrase": "individual_rules"}, {"score": 0.0044786435725543685, "phrase": "separate-and-conquer_rule_learning_algorithms"}, {"score": 0.004201381218977942, "phrase": "selected_rules"}, {"score": 0.004130376386308024, "phrase": "good_rule"}, {"score": 0.0039244530915794025, "phrase": "suboptimal_design_choices"}, {"score": 0.003841699919521703, "phrase": "main_goal"}, {"score": 0.003634587156159668, "phrase": "existing_rule_induction_techniques"}, {"score": 0.003542786326740931, "phrase": "algorithm_designers"}, {"score": 0.003468053168607071, "phrase": "optimal_heuristic_rule"}, {"score": 0.0033374705499418377, "phrase": "resulting_heuristics"}, {"score": 0.003239304090565123, "phrase": "large_and_significant_improvements"}, {"score": 0.0031981188104809994, "phrase": "predictive_accuracy"}, {"score": 0.003038532168038562, "phrase": "global_optimal_choice"}, {"score": 0.0029744042270542655, "phrase": "good_default_choices"}, {"score": 0.002886885861509621, "phrase": "similar_search_biases"}, {"score": 0.002850168497994985, "phrase": "near-optimal_selection"}, {"score": 0.0027781249331615813, "phrase": "new_algorithms"}, {"score": 0.002754516260621121, "phrase": "minor_experimental_tuning"}, {"score": 0.002696366237523415, "phrase": "major_contribution"}, {"score": 0.0026282000779095987, "phrase": "model's_predictive_accuracy"}, {"score": 0.002539978170928796, "phrase": "pareto_front"}, {"score": 0.0025183881730011597, "phrase": "optimal_solutions"}, {"score": 0.0023123057299992587, "phrase": "parametrized_heuristics"}, {"score": 0.002253826965566618, "phrase": "desired_balance"}, {"score": 0.0022062235065459274, "phrase": "high_flexibility"}, {"score": 0.0021412594173592513, "phrase": "desired_accuracy"}, {"score": 0.0021049977753042253, "phrase": "rule_miners"}], "paper_keywords": ["Classification", " Rule induction", " Heuristics", " Rule evaluation", " Sequential covering"], "paper_abstract": "While many papers propose innovative methods for constructing individual rules in separate-and-conquer rule learning algorithms, comparatively few study the heuristic rule evaluation functions used in these algorithms to ensure that the selected rules combine into a good rule set. Underestimating the impact of this component has led to suboptimal design choices in many algorithms. The main goal of this paper is to demonstrate the importance of heuristic rule evaluation functions by improving existing rule induction techniques and to provide guidelines for algorithm designers. We first select optimal heuristic rule learning functions for several metaheuristic-based algorithms and empirically compare the resulting heuristics across algorithms. This results in large and significant improvements of the predictive accuracy for two techniques. We find that despite the absence of a global optimal choice for all algorithms, good default choices can be shared across algorithms with similar search biases. A near-optimal selection can thus be found for new algorithms with minor experimental tuning. Lastly, a major contribution is made towards balancing a model's predictive accuracy with its comprehensibility. We construct a Pareto front of optimal solutions for this trade-off and show that gains in comprehensibility and/or accuracy are possible for the techniques studied. The parametrized heuristics enable users to select the desired balance as they offer a high flexibility when it comes to selecting the desired accuracy and comprehensibility in rule miners.", "paper_title": "To tune or not to tune: rule evaluation for metaheuristic-based sequential covering algorithms", "paper_id": "WOS:000347948900008"}