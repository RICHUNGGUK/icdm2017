{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "pyramid_histograms"}, {"score": 0.015620840115765184, "phrase": "oriented_gradients"}, {"score": 0.015183418596996921, "phrase": "human_action_recognition"}, {"score": 0.013723651259580546, "phrase": "different_modalities"}, {"score": 0.00475390577915698, "phrase": "collaborative_multi-task"}, {"score": 0.004739810059178863, "phrase": "learning"}, {"score": 0.004560780566066473, "phrase": "collaborative_multi-task_learning"}, {"score": 0.004445836970274954, "phrase": "global_activities"}, {"score": 0.004403480614319231, "phrase": "motion_history_image"}, {"score": 0.004375469083377857, "phrase": "mhi"}, {"score": 0.0043199694330708464, "phrase": "rgb_and_depth_channels"}, {"score": 0.004014196517907476, "phrase": "global_textual_and_structural_characteristics"}, {"score": 0.003925486550801027, "phrase": "average_value"}, {"score": 0.003900501415804221, "phrase": "hierarchical_block"}, {"score": 0.003826492478418294, "phrase": "oriented_gradients_descriptors"}, {"score": 0.0037658881407605445, "phrase": "human_motion"}, {"score": 0.003670904035061963, "phrase": "proposed_method"}, {"score": 0.0036012563939423173, "phrase": "knn"}, {"score": 0.003578419029447555, "phrase": "svm"}, {"score": 0.0034109154088832376, "phrase": "large_scale_experimental_results"}, {"score": 0.0030306348247409703, "phrase": "dha_dataset"}, {"score": 0.002954140458190344, "phrase": "combined_descriptors"}, {"score": 0.002861224098631869, "phrase": "multimodal_features"}, {"score": 0.0028068790615886755, "phrase": "collaborative_multi-task_learning_method"}, {"score": 0.0027360168758119277, "phrase": "transfer_learning_theory"}, {"score": 0.0027099059684582176, "phrase": "main_contributions"}, {"score": 0.0026414854705971553, "phrase": "proposed_encoding"}, {"score": 0.002591303221962211, "phrase": "stationary_part"}, {"score": 0.002574788013427524, "phrase": "human_body"}, {"score": 0.0025502118431246276, "phrase": "noise_interference"}, {"score": 0.0024230370118191267, "phrase": "pyramid_layers"}, {"score": 0.002331825625142384, "phrase": "proposed_model"}, {"score": 0.0022584385508256566, "phrase": "sensor_types"}, {"score": 0.002173409712988074, "phrase": "different_features"}, {"score": 0.0021049977753042253, "phrase": "transfer_learning"}], "paper_keywords": ["Action Recognition", " collaborative multi-task learning", " PHOG", " depth"], "paper_abstract": "In this paper, human action recognition using pyramid histograms of oriented gradients and collaborative multi-task learning is proposed. First, we accumulate global activities and construct motion history image (MHI) for both RGB and depth channels respectively to encode the dynamics of one action in different modalities, and then different action descriptors are extracted from depth and RGB MHI to represent global textual and structural characteristics of these actions. Specially, average value in hierarchical block, GIST and pyramid histograms of oriented gradients descriptors are employed to represent human motion. To demonstrate the superiority of the proposed method, we evaluate them by KNN, SVM with linear and RBF kernels, SRC and CRC models on DHA dataset, the well-known dataset for human action recognition. Large scale experimental results show our descriptors are robust, stable and efficient, and outperform the state-of-the-art methods. In addition, we investigate the performance of our descriptors further by combining these descriptors on DHA dataset, and observe that the performances of combined descriptors are much better than just using only sole descriptor. With multimodal features, we also propose a collaborative multi-task learning method for model learning and inference based on transfer learning theory. The main contributions lie in four aspects: 1) the proposed encoding the scheme can filter the stationary part of human body and reduce noise interference; 2) different kind of features and models are assessed, and the neighbor gradients information and pyramid layers are very helpful for representing these actions; 3) The proposed model can fuse the features from different modalities regardless of the sensor types, the ranges of the value, and the dimensions of different features; 4) The latent common knowledge among different modalities can be discovered by transfer learning to boost the performance.", "paper_title": "Human Action Recognition Using Pyramid Histograms of Oriented Gradients and Collaborative Multi-task Learning", "paper_id": "WOS:000333694900009"}