{"auto_keywords": [{"score": 0.03529624542703246, "phrase": "laplacian"}, {"score": 0.015716636950428683, "phrase": "value_function_approximation"}, {"score": 0.015546327453067513, "phrase": "reinforcement_learning"}, {"score": 0.013714059102330298, "phrase": "rl"}, {"score": 0.010207261639712636, "phrase": "continuous_state_spaces"}, {"score": 0.010040881010284919, "phrase": "basis_functions"}, {"score": 0.004556901872357751, "phrase": "sequential_decision_problems"}, {"score": 0.004506968388996092, "phrase": "large_or_continuous_state_spaces"}, {"score": 0.004384504920139083, "phrase": "function_approximation"}, {"score": 0.00428892403807229, "phrase": "major_research_topic"}, {"score": 0.004014452814186653, "phrase": "clustering-based_graph_laplacian_framework"}, {"score": 0.0037574813073547294, "phrase": "rl."}, {"score": 0.0036553070575015344, "phrase": "clustering-based_techniques"}, {"score": 0.003555944323232166, "phrase": "k-means"}, {"score": 0.0032380986533671395, "phrase": "markov"}, {"score": 0.003030247482854539, "phrase": "vfa"}, {"score": 0.002931569771634119, "phrase": "spectral_analysis"}, {"score": 0.00282048824450047, "phrase": "clustering-based_graph"}, {"score": 0.0026986685585382347, "phrase": "approximation_policy_iteration_algorithms"}, {"score": 0.0026107632962872472, "phrase": "rpi"}, {"score": 0.0024569466759289055, "phrase": "experimental_results"}, {"score": 0.0023768935613577985, "phrase": "previous_rpi_methods"}, {"score": 0.002337848196418384, "phrase": "proposed_approach"}, {"score": 0.002261666793030715, "phrase": "efficient_set"}, {"score": 0.002200077887697072, "phrase": "learning_control_performance"}, {"score": 0.0021049977753042253, "phrase": "parameter_settings"}], "paper_keywords": ["Approximate dynamic programming", " clustering", " learning control", " Markov decision processes", " reinforcement learning", " value function approximation"], "paper_abstract": "In order to deal with the sequential decision problems with large or continuous state spaces, feature representation and function approximation have been a major research topic in reinforcement learning (RL). In this paper, a clustering-based graph Laplacian framework is presented for feature representation and value function approximation (VFA) in RL. By making use of clustering-based techniques, that is, K-means clustering or fuzzy C-means clustering, a graph Laplacian is constructed by subsampling in Markov decision processes (MDPs) with continuous state spaces. The basis functions for VFA can be automatically generated from spectral analysis of the graph Laplacian. The clustering-based graph Laplacian is integrated with a class of approximation policy iteration algorithms called representation policy iteration (RPI) for RL in MDPs with continuous state spaces. Simulation and experimental results show that, compared with previous RPI methods, the proposed approach needs fewer sample points to compute an efficient set of basis functions and the learning control performance can be improved for a variety of parameter settings.", "paper_title": "A Clustering-Based Graph Laplacian Framework for Value Function Approximation in Reinforcement Learning", "paper_id": "WOS:000345629000030"}