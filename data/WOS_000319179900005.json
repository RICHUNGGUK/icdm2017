{"auto_keywords": [{"score": 0.04924143139933198, "phrase": "mapreduce"}, {"score": 0.014115733535750135, "phrase": "shuffle-heavy_mapreductions"}, {"score": 0.01253192002406172, "phrase": "marco"}, {"score": 0.010612275578428138, "phrase": "communication_overlap"}, {"score": 0.009997389917731686, "phrase": "search_engines"}, {"score": 0.009927409825241925, "phrase": "machine_learning"}, {"score": 0.004680851157467745, "phrase": "programming_model"}, {"score": 0.004647980742724688, "phrase": "google"}, {"score": 0.004615202066192795, "phrase": "cluster-based_computing"}, {"score": 0.004455061431809975, "phrase": "data_mining"}, {"score": 0.004392565073466714, "phrase": "automatic_data_management"}, {"score": 0.004240116703465252, "phrase": "mapreduce's_execution_model"}, {"score": 0.004195418010131832, "phrase": "all-map-to-all-reduce_communication"}, {"score": 0.003992950134435965, "phrase": "large_amounts"}, {"score": 0.00385431710959668, "phrase": "input_data"}, {"score": 0.0037868092509097404, "phrase": "bisection_bandwidth"}, {"score": 0.003746871277361436, "phrase": "significant_runtime_overhead"}, {"score": 0.00352835528598178, "phrase": "key_applications"}, {"score": 0.003264316172836322, "phrase": "shuffle-light_mapreductions"}, {"score": 0.0030847461428168614, "phrase": "shuffle_results"}, {"score": 0.002784007794229784, "phrase": "nearly_full_overlap"}, {"score": 0.0027546167254869493, "phrase": "novel_idea"}, {"score": 0.002521421793028698, "phrase": "partial_data"}, {"score": 0.0024947959375982614, "phrase": "map_tasks"}, {"score": 0.002365815362517737, "phrase": "inevitably_high_shuffle_volume"}, {"score": 0.002259426425685765, "phrase": "hadoop's_mapreduce"}, {"score": 0.002165473981732452, "phrase": "hadoop"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Cloud computing", " Parallel computing", " MapReduce", " Performance optimization", " Distributed processing", " Large-scale data processing"], "paper_abstract": "MapReduce is a programming model from Google for cluster-based computing in domains such as search engines, machine learning, and data mining. MapReduce provides automatic data management and fault tolerance to improve programmability of clusters. MapReduce's execution model includes an all-map-to-all-reduce communication, called the shuffle, across the network bisection. Some MapReductions move large amounts of data (e.g., as much as the input data), stressing the bisection bandwidth and introducing significant runtime overhead. Optimizing such shuffle-heavy MapReduction's is important because (1) they include key applications (e.g., inverted indexing for search engines and data clustering for machine learning) and (2) they run longer than shuffle-light MapReductions (e.g., 5x longer). In MapReduce, the asynchronous nature of the shuffle results in some overlap between the shuffle and map. Unfortunately, this overlap is insufficient in shuffle-heavy MapReductions. We propose MapReduce with communication overlap (MaRCO) to achieve nearly full overlap via the novel idea of including reduce in the overlap. While MapReduce lazily performs reduce computation only after receiving all the map data, MaRCO employs eager reduce to process partial data from some map tasks while overlapping with other map tasks' communication. MaRCO's approach of hiding the latency of the inevitably high shuffle volume of shuffle-heavy MapReductions is fundamental for achieving performance. We implement MaRCO in Hadoop's MapReduce and show that on a 128-node Amazon EC2 cluster, MaRCO achieves 23% average speed-up over Hadoop for shuffle-heavy MapReductions. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "MapReduce with communication overlap (MaRCO)", "paper_id": "WOS:000319179900005"}