{"auto_keywords": [{"score": 0.026897811453592893, "phrase": "marginal_features"}, {"score": 0.00481495049065317, "phrase": "featural_representation"}, {"score": 0.004755054742217193, "phrase": "present_work"}, {"score": 0.004695900553353526, "phrase": "complex_semantics"}, {"score": 0.004541712637353326, "phrase": "input_features"}, {"score": 0.004466519142879166, "phrase": "attractor_neural_network"}, {"score": 0.004319830179921363, "phrase": "feature_dominance"}, {"score": 0.004283913685201902, "phrase": "feature_distinctiveness"}, {"score": 0.00417793854311445, "phrase": "hebbian_training"}, {"score": 0.003940706575253453, "phrase": "lexical_network"}, {"score": 0.0038112201846057445, "phrase": "semantic_network"}, {"score": 0.0035499650812710806, "phrase": "different_feature"}, {"score": 0.003418958653159371, "phrase": "hebb_rules"}, {"score": 0.0033905067962602515, "phrase": "different_values"}, {"score": 0.0033622909109253616, "phrase": "pre-synaptic_and_post-synaptic_thresholds"}, {"score": 0.003197843245296877, "phrase": "simple_taxonomy"}, {"score": 0.0031712256091350316, "phrase": "schematic_objects"}, {"score": 0.003028727936200845, "phrase": "shared_features"}, {"score": 0.002941389980674657, "phrase": "distinctive_features"}, {"score": 0.0028926147550022607, "phrase": "individual_members"}, {"score": 0.002856563331151566, "phrase": "different_frequency"}, {"score": 0.0027974705482714884, "phrase": "trained_network"}, {"score": 0.002762601646452813, "phrase": "simple_object_recognition_tasks"}, {"score": 0.0025730425700096365, "phrase": "different_role"}, {"score": 0.002551612225562499, "phrase": "dominant_features"}, {"score": 0.0022507188208913394, "phrase": "topological_organization"}, {"score": 0.002158505888557746, "phrase": "modified_features"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Semantic memory", " Lexical memory", " Conceptual representation", " Hebb rule", " Dominant features", " Category formation"], "paper_abstract": "The present work investigates how complex semantics can be extracted from the statistics of input features, using an attractor neural network. The study is focused on how feature dominance and feature distinctiveness can be naturally coded using Hebbian training, and how similarity among objects can be managed. The model includes a lexical network (which represents word-forms) and a semantic network composed of several areas: each area is topologically organized (similarity) and codes for a different feature. Synapses in the model are created using Hebb rules with different values for pre-synaptic and post-synaptic thresholds, producing patterns of asymmetrical synapses. This work uses a simple taxonomy of schematic objects (i.e., a vector of features), with shared features (to realize categories) and distinctive features (to have individual members) with different frequency of occurrence. The trained network can solve simple object recognition tasks and object naming tasks by maintaining a distinction between categories and their members, and providing a different role for dominant features vs. marginal features. Marginal features are not evoked in memory when thinking of objects, but they facilitate the reconstruction of objects when provided as input. Finally, the topological organization of features allows the recognition of objects with some modified features. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "A neural network for learning the meaning of objects and words from a featural representation", "paper_id": "WOS:000349730800021"}