{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "visual_recognition"}, {"score": 0.00473477283457869, "phrase": "bag-of-words_models"}, {"score": 0.0046092435167207095, "phrase": "sparse_interest_point_operators"}, {"score": 0.004502126088469323, "phrase": "computer_visual_object"}, {"score": 0.004471978831302331, "phrase": "action_recognition_tasks"}, {"score": 0.004309725894724034, "phrase": "visual_processing"}, {"score": 0.004280861452668641, "phrase": "biological_systems"}, {"score": 0.00418134346679498, "phrase": "fixate'_regimes"}, {"score": 0.00407042677958844, "phrase": "human_and_the_computer_vision_communities"}, {"score": 0.003805788648230075, "phrase": "existing_state-of-the_art_large_scale_dynamic_computer_vision_annotated_datasets"}, {"score": 0.0036430378180975667, "phrase": "human_eye_movements"}, {"score": 0.003594369823549604, "phrase": "ecological_constraints"}, {"score": 0.0035702793079311896, "phrase": "visual_action"}, {"score": 0.0035463496793144748, "phrase": "scene_context_recognition_tasks"}, {"score": 0.0034638476252104706, "phrase": "first_large_human_eye_tracking_datasets"}, {"score": 0.002821247621250663, "phrase": "novel_dynamic_consistency_and_alignment_measures"}, {"score": 0.0027741761880779535, "phrase": "remarkable_stability"}, {"score": 0.0027370835853738626, "phrase": "visual_search"}, {"score": 0.0026554236621262515, "phrase": "significant_amount"}, {"score": 0.002637609579129844, "phrase": "collected_data"}, {"score": 0.0024247382451294255, "phrase": "computer_vision_spatio-temporal_interest_point_image_sampling_strategies"}, {"score": 0.0023365892290481472, "phrase": "visual_recognition_performance"}, {"score": 0.0022898630573629144, "phrase": "human_fixations"}, {"score": 0.002206606452557159, "phrase": "end-to-end_automatic_system"}, {"score": 0.0021624739899833868, "phrase": "advanced_computer_vision_practice"}, {"score": 0.0021049977753042253, "phrase": "art_results"}], "paper_keywords": ["Visual action recognition", " human eye-movements", " consistency analysis", " saliency prediction", " large scale learning"], "paper_abstract": "Systems based on bag-of-words models from image features collected at maxima of sparse interest point operators have been used successfully for both computer visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in 'saccade and fixate' regimes, the methodology and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large scale dynamic computer vision annotated datasets like Hollywood-2 [1] and UCF Sports [2] with human eye movements collected under the ecological constraints of visual action and scene context recognition tasks. To our knowledge these are the first large human eye tracking datasets to be collected and made publicly available for video, vision. imar. ro/eyetracking (497,107 frames, each viewed by 19 subjects), unique in terms of their (a) large scale and computer vision relevance, (b) dynamic, video stimuli, (c) task control, as well as free-viewing. Second, we introduce novel dynamic consistency and alignment measures, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the significant amount of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and the human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the advanced computer vision practice, can lead to state of the art results.", "paper_title": "Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition", "paper_id": "WOS:000355931100009"}