{"auto_keywords": [{"score": 0.04392292221959625, "phrase": "data_space"}, {"score": 0.00481495049065317, "phrase": "datum-wise_sparse_representations"}, {"score": 0.004688989566904871, "phrase": "data_representation"}, {"score": 0.004566308708871429, "phrase": "dataset_level"}, {"score": 0.004446823283475876, "phrase": "\"best\"_representation"}, {"score": 0.004106725110985659, "phrase": "different_approach"}, {"score": 0.003759142310009214, "phrase": "sparse_datum-wise_representations"}, {"score": 0.003191560305761507, "phrase": "sequential_decision_process"}, {"score": 0.0030399188412691914, "phrase": "particular_point"}, {"score": 0.002908306074124806, "phrase": "reinforcement_learning"}, {"score": 0.0028699415886017468, "phrase": "proposed_method"}, {"score": 0.00278237555651743, "phrase": "medium-sized_sparse_classification_problems"}, {"score": 0.002697474061842142, "phrase": "global_sparsity_approaches"}, {"score": 0.0026384167183586015, "phrase": "natural_framework"}, {"score": 0.002615156460073562, "phrase": "sequential_classification_problems"}, {"score": 0.0025241429061867633, "phrase": "whole_family"}, {"score": 0.0025018875576502606, "phrase": "sparsity-related_problem"}, {"score": 0.002436289099444074, "phrase": "specific_solutions"}, {"score": 0.002341094670523097, "phrase": "cost-sensitive_and_limited-budget_classification"}, {"score": 0.002161695357601592, "phrase": "non-differentiable_loss_functions"}], "paper_keywords": ["Classification", " Features selection", " Sparsity", " Sequential models", " Reinforcement learning"], "paper_abstract": "In supervised classification, data representation is usually considered at the dataset level: one looks for the \"best\" representation of data assuming it to be the same for all the data in the data space. We propose a different approach where the representations used for classification are tailored to each datum in the data space. One immediate goal is to obtain sparse datum-wise representations: our approach learns to build a representation specific to each datum that contains only a small subset of the features, thus allowing classification to be fast and efficient. This representation is obtained by way of a sequential decision process that sequentially chooses which features to acquire before classifying a particular point; this process is learned through algorithms based on Reinforcement Learning. The proposed method performs well on an ensemble of medium-sized sparse classification problems. It offers an alternative to global sparsity approaches, and is a natural framework for sequential classification problems. The method extends easily to a whole family of sparsity-related problem which would otherwise require developing specific solutions. This is the case in particular for cost-sensitive and limited-budget classification, where feature acquisition is costly and is often performed sequentially. Finally, our approach can handle non-differentiable loss functions or combinatorial optimization encountered in more complex feature selection problems.", "paper_title": "Sequential approaches for learning datum-wise sparse representations", "paper_id": "WOS:000307717100005"}