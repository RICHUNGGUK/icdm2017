{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "direct_weighted_pruning"}, {"score": 0.004768378525561471, "phrase": "instance-based_learning_methods"}, {"score": 0.004608879242346434, "phrase": "high_storage_requirements"}, {"score": 0.004564291332315995, "phrase": "large_computational_costs"}, {"score": 0.004454691230204595, "phrase": "stored_instances"}, {"score": 0.004141365648080381, "phrase": "noisy_samples"}, {"score": 0.0034760655218366, "phrase": "new_algorithm"}, {"score": 0.00312310128226407, "phrase": "indirect_heuristics"}, {"score": 0.00303319936111073, "phrase": "bi-objective_index"}, {"score": 0.0029747028229638625, "phrase": "condensation_rate"}, {"score": 0.0028890603352364273, "phrase": "classification_inaccuracy"}, {"score": 0.0028195727965553367, "phrase": "nearest_neighbor_rule"}, {"score": 0.002778682854371595, "phrase": "proposed_algorithm"}, {"score": 0.00259554612218815, "phrase": "efficient_search"}, {"score": 0.0025578967205807843, "phrase": "simple_genetic_algorithm"}, {"score": 0.0023546121947302877, "phrase": "large_number"}, {"score": 0.0021674482571326283, "phrase": "highest_classification_accuracy"}, {"score": 0.002135995282374723, "phrase": "competitive_condensation_rates"}], "paper_keywords": ["Data condensation", " Prototype reduction", " Instance weight learning"], "paper_abstract": "Instance-based learning methods often suffer from problems related to high storage requirements, large computational costs for searching through the stored instances to find the ones most similar to the queries, and also sensitivity to noisy samples. In order to deal with these issues, various condensation algorithms have been proposed in the literature to reduce the set of prototypes that need to be stored. In this paper, we propose a new algorithm that uses a set of weights to directly control which prototypes have to be discarded or survive. Instead of relying on indirect heuristics, it explicitly optimizes a bi-objective index which incorporates the condensation rate and a measure of the classification inaccuracy as reflected by the nearest neighbor rule. The proposed algorithm, referred to as DWP (Direct Weighted Pruning), performs an efficient search using a simple genetic algorithm, which is however equipped with three novel acceleration mechanisms to notably speed up its convergence. Experiments over a large number of datasets and comparisons against many other successful condensation algorithms, show that DWP is very effective and achieves the highest classification accuracy along with competitive condensation rates. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Prototype reduction based on Direct Weighted Pruning", "paper_id": "WOS:000329145400004"}