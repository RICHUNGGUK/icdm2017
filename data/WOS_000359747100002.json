{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sparse_feature_selection"}, {"score": 0.004688321299229397, "phrase": "fundamental_problem"}, {"score": 0.004638602678192715, "phrase": "statistical_data_analysis"}, {"score": 0.004468684611798047, "phrase": "conditional_mean"}, {"score": 0.004421284843189731, "phrase": "output_given_input"}, {"score": 0.004169394481182493, "phrase": "conditional_probability_density"}, {"score": 0.00378767277402891, "phrase": "conditional_densities"}, {"score": 0.003629392214109574, "phrase": "kernel-based_approach"}, {"score": 0.003244567402785763, "phrase": "ls-cde"}, {"score": 0.0031760175583084274, "phrase": "large_estimation_error"}, {"score": 0.002720376476146181, "phrase": "automatic_feature_selection"}, {"score": 0.0026914710445743693, "phrase": "cde._sa-cde"}, {"score": 0.002662871928330334, "phrase": "kernel_ls-cde"}, {"score": 0.0026205405415623525, "phrase": "input_feature"}, {"score": 0.002578880352208335, "phrase": "additive_manner"}, {"score": 0.0025243592424524764, "phrase": "whole_solution"}, {"score": 0.0024842242921857705, "phrase": "group-sparse_regularizer"}, {"score": 0.002405853977978678, "phrase": "subgradient-based_optimization_method"}, {"score": 0.0023802825369325354, "phrase": "sa-cde"}, {"score": 0.0023051837161137674, "phrase": "high-dimensional_large_data_sets"}, {"score": 0.002220551141101661, "phrase": "humanoid_robot_transition"}, {"score": 0.0021049977753042253, "phrase": "noisy_cde_problems"}], "paper_keywords": ["Conditional density estimation", " Feature selection", " Sparse structured norm"], "paper_abstract": "Regression is a fundamental problem in statistical data analysis, which aims at estimating the conditional mean of output given input. However, regression is not informative enough if the conditional probability density is multi-modal, asymmetric, and heteroscedastic. To overcome this limitation, various estimators of conditional densities themselves have been developed, and a kernel-based approach called least-squares conditional density estimation (LS-CDE) was demonstrated to be promising. However, LS-CDE still suffers from large estimation error if input contains many irrelevant features. In this paper, we therefore propose an extension of LS-CDE called sparse additive CDE (SA-CDE), which allows automatic feature selection in CDE. SA-CDE applies kernel LS-CDE to each input feature in an additive manner and penalizes the whole solution by a group-sparse regularizer. We also give a subgradient-based optimization method for SA-CDE training that scales well to high-dimensional large data sets. Through experiments with benchmark and humanoid robot transition datasets, we demonstrate the usefulness of SA-CDE in noisy CDE problems.", "paper_title": "Direct conditional probability density estimation with sparse feature selection", "paper_id": "WOS:000359747100002"}