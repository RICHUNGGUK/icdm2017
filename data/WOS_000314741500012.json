{"auto_keywords": [{"score": 0.03731816460421424, "phrase": "sound_source"}, {"score": 0.00481495049065317, "phrase": "distant-microphone_speech_recognition"}, {"score": 0.004710854091689325, "phrase": "multiple_sources"}, {"score": 0.004548939310612501, "phrase": "speech_recognition"}, {"score": 0.00450933377534525, "phrase": "reverberant_multisource_noise_conditions"}, {"score": 0.004470071518923524, "phrase": "distant_binaural_microphones"}, {"score": 0.004373398434399966, "phrase": "two-stage_fragment_decoding_approach"}, {"score": 0.004316435167218899, "phrase": "bregman"}, {"score": 0.0042415434489566995, "phrase": "auditory_scene_analysis"}, {"score": 0.004167982855788402, "phrase": "innate_primitive_grouping_'rules"}, {"score": 0.004024651422626329, "phrase": "learnt_schema-driven_processes"}, {"score": 0.003937573222812911, "phrase": "acoustic_mixture"}, {"score": 0.0038692638652494697, "phrase": "local_time-frequency_fragments"}, {"score": 0.003769006950727621, "phrase": "signal-level_primitive_grouping_cues"}, {"score": 0.003453140302583322, "phrase": "hypothesis-driven_stage"}, {"score": 0.003334309053314909, "phrase": "corresponding_acoustic_model_state_sequence"}, {"score": 0.0032621183241997777, "phrase": "recent_advances"}, {"score": 0.003219553875143356, "phrase": "adaptive_noise_floor"}, {"score": 0.0030547611342112693, "phrase": "signal-level_grouping_cues"}, {"score": 0.0029238768315330305, "phrase": "probabilistic_framework"}, {"score": 0.00266693888510356, "phrase": "significant_recognition_performance_benefits"}, {"score": 0.0026436755508085223, "phrase": "different_grouping_cue_estimates"}, {"score": 0.0025977542995083646, "phrase": "noisy_conditions"}, {"score": 0.002497319603143104, "phrase": "missing_data_imputation"}, {"score": 0.002443206010337041, "phrase": "fragment_decoding"}, {"score": 0.0023798115577749225, "phrase": "clean_spectrogram"}, {"score": 0.0022678201262721323, "phrase": "conventional_asr_systems"}, {"score": 0.0022382002476510573, "phrase": "best_performing_system"}, {"score": 0.0022089663761854657, "phrase": "average_keyword_recognition_accuracy"}, {"score": 0.0021610874773389096, "phrase": "pascal_chime_challenge_task"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Distant-microphone speech recognition", " Auditory scene analysis", " Binaural localisation", " Noise robustness"], "paper_abstract": "This paper addresses the problem of speech recognition in reverberant multisource noise conditions using distant binaural microphones. Our scheme employs a two-stage fragment decoding approach inspired by Bregman's account of auditory scene analysis, in which innate primitive grouping 'rules' are balanced by the role of learnt schema-driven processes. First, the acoustic mixture is split into local time-frequency fragments of individual sound sources using signal-level primitive grouping cues. Second, statistical models are employed to select fragments belonging to the sound source of interest, and the hypothesis-driven stage simultaneously searches for the most probable speech/background segmentation and the corresponding acoustic model state sequence. The paper reports recent advances in combining adaptive noise floor modelling and binaural localisation cues within this framework. By integrating signal-level grouping cues with acoustic models of the target sound source in a probabilistic framework, the system is able to simultaneously separate and recognise the sound of interest from the mixture, and derive significant recognition performance benefits from different grouping cue estimates despite their inherent unreliability in noisy conditions. Finally, the paper will show that missing data imputation can be applied via fragment decoding to allow reconstruction of a clean spectrogram that can be further processed and used as input to conventional ASR systems. The best performing system achieves an average keyword recognition accuracy of 85.83% on the PASCAL CHiME Challenge task. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "A hearing-inspired approach for distant-microphone speech recognition in the presence of multiple sources", "paper_id": "WOS:000314741500012"}