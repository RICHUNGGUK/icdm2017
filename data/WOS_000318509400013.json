{"auto_keywords": [{"score": 0.036882109496687944, "phrase": "shared_resources"}, {"score": 0.00481495049065317, "phrase": "efficient_communication"}, {"score": 0.004791260660826503, "phrase": "multicore_architectures"}, {"score": 0.004744228429437688, "phrase": "network_interface_performance"}, {"score": 0.004640079189342692, "phrase": "high_communication_requirements"}, {"score": 0.0045158714850726, "phrase": "high-performance_computing_applications"}, {"score": 0.004438559261607655, "phrase": "network_links"}, {"score": 0.004416713079266374, "phrase": "multiple_gigabits"}, {"score": 0.004394973947158533, "phrase": "second_bandwidths"}, {"score": 0.004330393665579603, "phrase": "communication_tasks"}, {"score": 0.004266758267694741, "phrase": "current_trend"}, {"score": 0.004235290609326164, "phrase": "microprocessor_development"}, {"score": 0.004142267494053487, "phrase": "clock_frequencies"}, {"score": 0.004121873594853149, "phrase": "microarchitecture_efficiencies"}, {"score": 0.004091470081068494, "phrase": "new_opportunities"}, {"score": 0.003981889041223034, "phrase": "efficient_communication_architectures"}, {"score": 0.00393305061215641, "phrase": "present_os_network_stacks"}, {"score": 0.003837156542817238, "phrase": "network_tasks"}, {"score": 0.00359859325519173, "phrase": "account_issues"}, {"score": 0.0034763179943851025, "phrase": "efficient_use"}, {"score": 0.0034167419580838953, "phrase": "common_trend"}, {"score": 0.003341636956320925, "phrase": "network_interrupts"}, {"score": 0.0033169694080493065, "phrase": "corresponding_protocol"}, {"score": 0.003300625286404833, "phrase": "network_application_processing"}, {"score": 0.0030197848438434636, "phrase": "network_interface"}, {"score": 0.0029974860223022626, "phrase": "different_cores"}, {"score": 0.002874191926197893, "phrase": "corresponding_communication_tasks"}, {"score": 0.002797121441432477, "phrase": "different_data_structures"}, {"score": 0.0027423678343077578, "phrase": "processing_core"}, {"score": 0.0026754265406228614, "phrase": "communication_path"}, {"score": 0.0023937976277397972, "phrase": "communication_performance"}, {"score": 0.00233534501966908, "phrase": "full-system_simulation"}, {"score": 0.002239225269052105, "phrase": "mpi_workloads"}, {"score": 0.00217914399056098, "phrase": "response_time"}, {"score": 0.0021049977753042253, "phrase": "dynamic_web_servers"}], "paper_keywords": ["interrupt affinity", " processor affinity", " network interface", " offloading", " SIMICS"], "paper_abstract": "Improving the network interface performance is needed by the demand of applications with high communication requirements (for example, some multimedia, real-time, and high-performance computing applications), and the availability of network links providing multiple gigabits per second bandwidths that could require many processor cycles for communication tasks. Multicore architectures, the current trend in the microprocessor development to cope with the difficulties to further increase clock frequencies and microarchitecture efficiencies, provide new opportunities to exploit the parallelism available in the nodes for designing efficient communication architectures. Nevertheless, although present OS network stacks include multiple threads that make it possible to execute network tasks concurrently in the kernel, the implementations of packet-based or connection-based parallelism are not trivial as they have to take into account issues related with the cost of synchronization in the access to shared resources and the efficient use of caches. Therefore, a common trend in many recent researches on this topic is to assign network interrupts and the corresponding protocol and network application processing to the same core, as with this affinity scheduling it would be possible to reduce the contention for shared resources and the cache misses. In this paper we propose and analyze several configurations to distribute the network interface among the different cores available in the server. These alternatives have been devised according to the affinity of the corresponding communication tasks with the location (proximity to the memories where the different data structures are stored) and characteristics of the processing core. As this approach uses several cores to accelerate the communication path of a given connection, it can be seen as complementary to those that consider several cores to simultaneously process packets belonging to either the same or different connections. Message passing interface (MPI) workloads and dynamic web servers have been considered as applications to evaluate and compare the communication performance of these alternatives. In our experiments, performed by full-system simulation, improvements of up to 35% in the throughput and up to 23% in the latency have been observed in MPI workloads, and up to 100% in the throughput, up to 500% in the response time, and up to 82% in the requests attended per second have been measured in dynamic web servers.", "paper_title": "Affinity-Based Network Interfaces for Efficient Communication on Multicore Architectures", "paper_id": "WOS:000318509400013"}