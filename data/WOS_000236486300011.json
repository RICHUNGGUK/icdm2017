{"auto_keywords": [{"score": 0.05007837343017575, "phrase": "information_potential"}, {"score": 0.04192738033265042, "phrase": "fgt"}, {"score": 0.004704699244406775, "phrase": "fast_gauss_transform"}, {"score": 0.004388796812234033, "phrase": "fast_and_accurate_approximation"}, {"score": 0.004222515162635615, "phrase": "information_theoretic_learning"}, {"score": 0.0036178264212372497, "phrase": "adaptive_systems"}, {"score": 0.0030283730114326014, "phrase": "hermite_approximation"}, {"score": 0.0026143990748127253, "phrase": "actual_entropy_value"}, {"score": 0.002554406040896888, "phrase": "increasing_order_p"}, {"score": 0.0023278569001363263, "phrase": "computational_complexity"}, {"score": 0.002292097733181405, "phrase": "itl."}, {"score": 0.002171202123281881, "phrase": "fgt_methods"}, {"score": 0.0021378442381693847, "phrase": "system_identification"}, {"score": 0.0021049977753042253, "phrase": "encouraging_results"}], "paper_keywords": [""], "paper_abstract": "In this paper, we propose a fast and accurate approximation to the information potential of Information Theoretic Learning (ITL) using the Fast Gauss Transform (FGT). We exemplify here the case of the Minimum Error Entropy criterion to train adaptive systems. The FGT reduces the complexity of the estimation from O(N-2) to O(pkN) where p is the order of the Hermite approximation and k the number of clusters utilized in FGT. Further, we show that FGT converges to the actual entropy value rapidly with increasing order p unlike the Stochastic Information Gradient, the present O(pN) approximation to reduce the computational complexity in ITL. We test the performance of these FGT methods on System Identification with encouraging results.", "paper_title": "Estimating the information potential with the fast gauss transform", "paper_id": "WOS:000236486300011"}