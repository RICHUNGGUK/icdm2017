{"auto_keywords": [{"score": 0.03383893996367042, "phrase": "dcrf"}, {"score": 0.02688592123124467, "phrase": "transfer_learning"}, {"score": 0.0264612610846891, "phrase": "marginal_training"}, {"score": 0.00481495049065317, "phrase": "sequence_data"}, {"score": 0.004756521061849989, "phrase": "sequence_modeling"}, {"score": 0.00462291551375938, "phrase": "complex_interaction"}, {"score": 0.004278807087784626, "phrase": "long-range_dependencies"}, {"score": 0.004091370602573054, "phrase": "linear-chain_conditional_random_fields"}, {"score": 0.003976373766630598, "phrase": "time_slice"}, {"score": 0.0038962096720793443, "phrase": "state_variables"}, {"score": 0.003740679163860117, "phrase": "dynamic_bayesian_networks"}, {"score": 0.003605997900398767, "phrase": "exact_inference"}, {"score": 0.0034761488481944657, "phrase": "approximate_inference"}, {"score": 0.003419943984098415, "phrase": "belief_propagation"}, {"score": 0.003378385493203297, "phrase": "tree-based_reparameterization"}, {"score": 0.0032833625797814474, "phrase": "natural-language_chunking_task"}, {"score": 0.0031139032290048788, "phrase": "linear-chain_crfs"}, {"score": 0.003076052206618737, "phrase": "comparable_performance"}, {"score": 0.0029895074299598275, "phrase": "maximum_conditional_likelihood"}, {"score": 0.0029172611440429963, "phrase": "training_dcrfs"}, {"score": 0.00260246764210499, "phrase": "distinct_data_set"}, {"score": 0.002570817338292482, "phrase": "state_variable"}, {"score": 0.0024182330686905256, "phrase": "real-world_text_data"}, {"score": 0.0022933216359142736, "phrase": "latent_variables"}, {"score": 0.0021837406983565098, "phrase": "cascaded_fashion"}, {"score": 0.0021396386320044172, "phrase": "linear-chain_crf"}, {"score": 0.0021049977753042253, "phrase": "final_task"}], "paper_keywords": ["conditional random fields", " graphical models", " sequence labeling"], "paper_abstract": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields ( DCRFs), a generalization of linear-chain conditional random fields ( CRFs) in which each time slice contains a set of state variables and edges-a distributed state representation as in dynamic Bayesian networks ( DBNs)-and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization ( TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, finding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the final task directly.", "paper_title": "Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data", "paper_id": "WOS:000247002700011"}