{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "markov"}, {"score": 0.0047057084749407485, "phrase": "concurrent_speech_recognition"}, {"score": 0.004494570360108983, "phrase": "new_markov_model"}, {"score": 0.004171402455813099, "phrase": "priori_known_speakers"}, {"score": 0.004007083686875716, "phrase": "recent_work"}, {"score": 0.003961330917648707, "phrase": "nonnegative_representations"}, {"score": 0.0036763623220059933, "phrase": "source_separation_problems"}, {"score": 0.003431481042678356, "phrase": "markov_selection_model"}, {"score": 0.00290473242917264, "phrase": "factorial_markov_models"}, {"score": 0.0027424619603260837, "phrase": "feature_state_spaces"}, {"score": 0.002530359917257626, "phrase": "low_computational_complexity_model"}, {"score": 0.002487083025913648, "phrase": "state_space"}, {"score": 0.0021789194955896124, "phrase": "known_speakers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speech recognition", " Markov models"], "paper_abstract": "In this paper we introduce a new Markov model that is capable of recognizing speech from recordings of simultaneously speaking a priori known speakers. This work is based on recent work on nonnegative representations of spectrograms, which has been shown to be very effective in source separation problems. In this paper we extend these approaches to design a Markov selection model that is able to recognize sequences even when they are presented mixed together. We do so without the need to perform separation on the signals. Unlike factorial Markov models which have been used similarly in the past that feature state spaces that are exponential in the number of sources, this approach features a low computational complexity model with a state space that is linear in the number of sources. We demonstrate the use of this framework in recognizing speech from mixtures of known speakers. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "The Markov selection model for concurrent speech recognition", "paper_id": "WOS:000300817200009"}