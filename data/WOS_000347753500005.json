{"auto_keywords": [{"score": 0.036691162678165204, "phrase": "sparse_auto-encoder"}, {"score": 0.00481495049065317, "phrase": "sae-pca_network"}, {"score": 0.004767261858417823, "phrase": "human_gesture_recognition"}, {"score": 0.004720043307726269, "phrase": "rgbd_images"}, {"score": 0.004558414405811025, "phrase": "depth_sensors"}, {"score": 0.004513255056769647, "phrase": "microsoft_kinect"}, {"score": 0.0044685410845412745, "phrase": "human_hand_gesture_recognition"}, {"score": 0.004358676214574977, "phrase": "research_interests"}, {"score": 0.004272724011826495, "phrase": "successful_gesture_recognition_system"}, {"score": 0.004105850229681625, "phrase": "good_feature_representation"}, {"score": 0.003661382948704217, "phrase": "depth_sensor"}, {"score": 0.0035358770665604657, "phrase": "feature_learning_approach"}, {"score": 0.003448874794071719, "phrase": "sae"}, {"score": 0.003397683082821229, "phrase": "principle_component_analysis"}, {"score": 0.0033140589981208693, "phrase": "human_actions"}, {"score": 0.0031372369299648233, "phrase": "rgb-d_inputs"}, {"score": 0.0030906666356950887, "phrase": "proposed_model"}, {"score": 0.0030600032877646263, "phrase": "feature_learning"}, {"score": 0.0028253682745674608, "phrase": "depth_channels"}, {"score": 0.0027557911038816256, "phrase": "convolutional_neural_networks"}, {"score": 0.002687922713487696, "phrase": "learned_features"}, {"score": 0.0025571463140118805, "phrase": "multiple_layer"}, {"score": 0.0025447024049684254, "phrase": "pca"}, {"score": 0.0024941578231560055, "phrase": "final_feature"}, {"score": 0.0024693979425773993, "phrase": "experimental_results"}, {"score": 0.002444883254621769, "phrase": "american_sign_language"}, {"score": 0.0023846534541336326, "phrase": "proposed_feature_learning_model"}, {"score": 0.0022913497040895586, "phrase": "recognition_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Deep learning", " Auto-encoder", " Convolutional neural networks", " American sign language recognition"], "paper_abstract": "Coming with the emerging of depth sensors link Microsoft Kinect, human hand gesture recognition has received ever increasing research interests recently. A successful gesture recognition system has usually heavily relied on having a good feature representation of data, which is expected to be task-dependent as well as coping with the challenges and opportunities induced by depth sensor. In this paper, a feature learning approach based on sparse auto-encoder (SAE) and principle component analysis is proposed for recognizing human actions, i.e. finger-spelling or sign language, for RGB-D inputs. The proposed model of feature learning is consisted of two components: First, features are learned respectively from the RGB and depth channels, using sparse auto-encoder with convolutional neural networks. Second, the learned features from both channels is concatenated and fed into a multiple layer PCA to get the final feature. Experimental results on American sign language (ASL) dataset demonstrate that the proposed feature learning model is significantly effective, which improves the recognition rate from 75% to 99.05% and outperforms the state-of-the-art. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Feature learning based on SAE-PCA network for human gesture recognition in RGBD images", "paper_id": "WOS:000347753500005"}