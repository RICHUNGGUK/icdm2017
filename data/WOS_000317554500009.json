{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "random_crowded_scenes"}, {"score": 0.04614113533877514, "phrase": "random_movements"}, {"score": 0.00473320761888448, "phrase": "multi-part_sparse_representation_method"}, {"score": 0.004613175917207636, "phrase": "pedestrian_tracking"}, {"score": 0.004496174406337201, "phrase": "crowded_scenes"}, {"score": 0.004382127278148236, "phrase": "orderly_movements"}, {"score": 0.0040223714916651845, "phrase": "different_participants"}, {"score": 0.003971006689166329, "phrase": "different_directions"}, {"score": 0.0038044630943383497, "phrase": "motion_flows"}, {"score": 0.0036137713873827374, "phrase": "fully_unsupervised_tracking_algorithm"}, {"score": 0.0035523480068927614, "phrase": "multi-part_local_sparse_appearance_model"}, {"score": 0.0033454448643854525, "phrase": "feature_matching"}, {"score": 0.003288567185099276, "phrase": "occluded_segments"}, {"score": 0.0032465423581390625, "phrase": "disturbed_ones"}, {"score": 0.003177687211289478, "phrase": "multi-part_sparse_reconstruction_code"}, {"score": 0.0030837288356760973, "phrase": "target_segments"}, {"score": 0.0030183163057929687, "phrase": "whole_target"}, {"score": 0.0028302635493019867, "phrase": "segment_group"}, {"score": 0.0027940789980683, "phrase": "smallest_projection_error"}, {"score": 0.0027230880516264685, "phrase": "tracking_result"}, {"score": 0.0026312238938540787, "phrase": "density_distribution"}, {"score": 0.002597577409911978, "phrase": "bayesian_state_inference_framework"}, {"score": 0.0024884913795612707, "phrase": "template_dictionary"}, {"score": 0.0023942269247089277, "phrase": "appearance_variation"}, {"score": 0.0023233800372985686, "phrase": "numerous_videos"}, {"score": 0.0023035249767734286, "phrase": "different_type"}, {"score": 0.002264321283759897, "phrase": "serious_occlusion"}, {"score": 0.002244969789802093, "phrase": "illumination_variation"}, {"score": 0.002216251512062482, "phrase": "proposed_approach"}, {"score": 0.0021973099304606076, "phrase": "excellent_performance"}, {"score": 0.0021599099979586946, "phrase": "previous_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Visual tracking", " Multi-part sparse representation", " Crowded scenes", " Particle filter"], "paper_abstract": "A multi-part sparse representation method is used in random crowded scenes for pedestrian tracking in this paper. In crowded scenes, there are random movements and orderly movements. Random movements are defined as the motion of each individual in the crowd appears to be unique, and different participants move in different directions over time. This means methods about multi-model in motion flows are not available. As a result, we propose a fully unsupervised tracking algorithm based on a multi-part local sparse appearance model. Based on the facts that only non-occluded segments of a target are effective in feature matching, while the occluded segments are the disturbed ones, our algorithm employs a multi-part sparse reconstruction code. The method is used on target segments in stead of the whole target, and implemented by solving an l(1) regularized least squares problem. The segment group with the smallest projection error will be taken as the tracking result. All the segment groups are drawn based on a density distribution in a Bayesian state inference framework. After tracking process in each frame, the template dictionary will be jointly inferred and updated to adapt appearance variation. We test the method on numerous videos including different type of very crowded scenes with serious occlusion and illumination variation. The proposed approach demonstrates excellent performance in comparison with previous methods. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Multi-part sparse representation in random crowded scenes tracking", "paper_id": "WOS:000317554500009"}