{"auto_keywords": [{"score": 0.03658033990527774, "phrase": "latent_factors"}, {"score": 0.00481495049065317, "phrase": "attribute_shades"}, {"score": 0.004611224230543916, "phrase": "semantic_attributes"}, {"score": 0.004567129988306349, "phrase": "existing_methods"}, {"score": 0.00433200902751133, "phrase": "nameable_properties"}, {"score": 0.004229158339021651, "phrase": "\"one_model"}, {"score": 0.003953932197201054, "phrase": "precise_linguistic_definition"}, {"score": 0.0038230844545071303, "phrase": "precise_visual_definition"}, {"score": 0.0036788184374592706, "phrase": "attribute_meaning"}, {"score": 0.003608732492631878, "phrase": "attribute_name"}, {"score": 0.0035399770257915466, "phrase": "crowdsourced_image_labels"}, {"score": 0.003422781254306241, "phrase": "different_annotators"}, {"score": 0.0033737457695128233, "phrase": "named_concept"}, {"score": 0.0027431953780099826, "phrase": "resulting_models"}, {"score": 0.002601727192945944, "phrase": "users'_interpretations"}, {"score": 0.0025398521156505425, "phrase": "prediction_accuracy"}, {"score": 0.0025155150482635688, "phrase": "novel_images"}, {"score": 0.0023515464194592195, "phrase": "multi-attribute_query_results"}, {"score": 0.0022302306304023602, "phrase": "visual_content"}, {"score": 0.0021049977753042253, "phrase": "photo_collections"}], "paper_keywords": ["Attribute learning and perception", " Vision and language", " Attribute discovery"], "paper_abstract": "To learn semantic attributes, existing methods typically train one discriminative model for each word in a vocabulary of nameable properties. However, this \"one model per word\" assumption is problematic: while a word might have a precise linguistic definition, it need not have a precise visual definition. We propose to discover shades of attribute meaning. Given an attribute name, we use crowdsourced image labels to discover the latent factors underlying how different annotators perceive the named concept. We show that structure in those latent factors helps reveal shades, that is, interpretations for the attribute shared by some group of annotators. Using these shades, we train classifiers to capture the primary (often subtle) variants of the attribute. The resulting models are both semantic and visually precise. By catering to users' interpretations, they improve attribute prediction accuracy on novel images. Shades also enable more successful attribute-based image search, by providing robust personalized models for retrieving multi-attribute query results. They are widely applicable to tasks that involve describing visual content, such as zero-shot category learning and organization of photo collections.", "paper_title": "Discovering Attribute Shades of Meaning with the Crowd", "paper_id": "WOS:000358940300004"}