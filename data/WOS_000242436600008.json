{"auto_keywords": [{"score": 0.025409930489846966, "phrase": "functional_calibration"}, {"score": 0.00481495049065317, "phrase": "pan-tilt-zoom_cameras"}, {"score": 0.00469159356595623, "phrase": "wide-area_context_awareness"}, {"score": 0.004631099806015235, "phrase": "crucial_enabling_technology"}, {"score": 0.004591202593942391, "phrase": "next_generation_smart_buildings"}, {"score": 0.004358903464536199, "phrase": "context_awareness"}, {"score": 0.004284100953217494, "phrase": "entire_building"}, {"score": 0.004174287115855492, "phrase": "significant_gaps"}, {"score": 0.004014800383343256, "phrase": "sparse_way"}, {"score": 0.0038613835226952117, "phrase": "missing_information"}, {"score": 0.003697760799674494, "phrase": "hybrid_perceptual_systems"}, {"score": 0.003634262647873338, "phrase": "comprehensive_model"}, {"score": 0.0035564156572789473, "phrase": "large_space"}, {"score": 0.0034204548167213545, "phrase": "contextual_information"}, {"score": 0.0033762956164519286, "phrase": "dense_network"}, {"score": 0.0033471722682967046, "phrase": "ultra-lightweight_sensor_nodes"}, {"score": 0.00327545462323687, "phrase": "sparse_network"}, {"score": 0.0030693625816617044, "phrase": "relative_geometry"}, {"score": 0.003029722317983915, "phrase": "pan-tilt-zoom_camera"}, {"score": 0.0029647862293448895, "phrase": "one-bit_motion_detectors"}, {"score": 0.0027068691342344545, "phrase": "simple_activity_models"}, {"score": 0.0026146109167103655, "phrase": "metric_calibration"}, {"score": 0.0024183363693242943, "phrase": "novel_goal"}, {"score": 0.002315719347244451, "phrase": "geometry_estimation"}, {"score": 0.0022957232752482196, "phrase": "simple_behavioral_model_discovery"}], "paper_keywords": ["sensor networks", " video surveillance", " adaptive systems"], "paper_abstract": "Wide-area context awareness is a crucial enabling technology for next generation smart buildings and surveillance systems. It is not practical to gather this context awareness by covering the entire building with cameras. However, significant gaps in coverage caused by installing cameras in a sparse way can make it very difficult to infer the missing information. As a solution we advocate a class of hybrid perceptual systems that build a comprehensive model of activity in a large space, such as a building, by merging contextual information from a dense network of ultra-lightweight sensor nodes and video from a sparse network of cameras. In this paper we explore the task of automatically recovering the relative geometry between a pan-tilt-zoom camera and a network of one-bit motion detectors. We present results both for the recovery of geometry alone and also for the recovery of geometry jointly with simple activity models. Because we do not believe a metric calibration is necessary, or even entirely useful, for this task, we formulate and pursue the novel goal we term functional calibration. Functional calibration is a blending of geometry estimation and simple behavioral model discovery. Accordingly, results are evaluated by measuring the ability of the system to automatically foveate targets in a large, non-convex space, rather than by measuring, for example, pixel reconstruction error.", "paper_title": "Functional calibration for pan-tilt-zoom cameras in hybrid sensor networks", "paper_id": "WOS:000242436600008"}