{"auto_keywords": [{"score": 0.0478533931600771, "phrase": "mcml"}, {"score": 0.040818012374293604, "phrase": "parallel_mcml"}, {"score": 0.04006250251719579, "phrase": "photon_packets"}, {"score": 0.014257467457809172, "phrase": "iad"}, {"score": 0.01201424451316623, "phrase": "problem_size"}, {"score": 0.00972054517290565, "phrase": "distributed_program"}, {"score": 0.009669563615009143, "phrase": "test_data"}, {"score": 0.00481495049065317, "phrase": "monte_carlo_multi-layered_programs"}, {"score": 0.0047980173225477315, "phrase": "high_performance_computing_systems"}, {"score": 0.004781143418736325, "phrase": "shared_and_distributed_memory"}, {"score": 0.00471423634868663, "phrase": "optical_studies"}, {"score": 0.004697655699119423, "phrase": "biological_materials"}, {"score": 0.004607494251671736, "phrase": "monte_carlo_multi-layered"}, {"score": 0.004424579223170422, "phrase": "mpi"}, {"score": 0.004401174234629236, "phrase": "standard_c-language"}, {"score": 0.004174518252684267, "phrase": "local_high_performance_computing"}, {"score": 0.004130601564882303, "phrase": "penguin-on-demand_hpc"}, {"score": 0.004079946627291215, "phrase": "parallel_iad"}, {"score": 0.004008653798463933, "phrase": "linear_scalability"}, {"score": 0.003938601811072102, "phrase": "parallel_cores"}, {"score": 0.003788750151373801, "phrase": "classical_performance_curves"}, {"score": 0.00372909677946604, "phrase": "optimal_performance_curve"}, {"score": 0.0036574564367391226, "phrase": "typical_speedup"}, {"score": 0.0035935195469670098, "phrase": "linear_increase"}, {"score": 0.003555693246482063, "phrase": "mcml_results"}, {"score": 0.0034324479586236357, "phrase": "total_optical_response"}, {"score": 0.003390325563054716, "phrase": "spatially-resolved_results"}, {"score": 0.003372431183133612, "phrase": "presented_parallel_versions"}, {"score": 0.0033193108580677985, "phrase": "multiple_computing_platforms"}, {"score": 0.003301790057030982, "phrase": "parallel_programs"}, {"score": 0.0032155591216631893, "phrase": "computing_systems"}, {"score": 0.0031816986246556937, "phrase": "additional_costs"}, {"score": 0.003160209179865715, "phrase": "program"}, {"score": 0.0031426436769610893, "phrase": "mcmlmpi"}, {"score": 0.003131573153805862, "phrase": "iadmpi_catalogue"}, {"score": 0.0030659616752274463, "phrase": "cpc_program_library"}, {"score": 0.0030551604502270812, "phrase": "queen's_university"}, {"score": 0.0030443973876696467, "phrase": "belfast"}, {"score": 0.003033671675676257, "phrase": "n._ireland"}, {"score": 0.0030123335878664064, "phrase": "standard_cpc"}, {"score": 0.002831894638104635, "phrase": "tar.gz_programming_language"}, {"score": 0.002821915685089897, "phrase": "c._computer"}, {"score": 0.0027677552254187186, "phrase": "windows"}, {"score": 0.002757932925007773, "phrase": "linux"}, {"score": 0.0027240346214239017, "phrase": "ansi_c-compatible_compiler"}, {"score": 0.002657539649541994, "phrase": "mpi_directives"}, {"score": 0.002437301480880159, "phrase": "multilayered_semi-transparent_material"}, {"score": 0.0024201479291302693, "phrase": "optical_properties"}, {"score": 0.0024116163689723354, "phrase": "iadmpi"}, {"score": 0.0023735941251333296, "phrase": "multilayered_material_samples"}, {"score": 0.0023403717626473606, "phrase": "monte-carlo"}, {"score": 0.0022471483360675957, "phrase": "turbid_media"}, {"score": 0.002192268793729956, "phrase": "large_problems"}, {"score": 0.0021691603046394385, "phrase": "hpc_clusters"}, {"score": 0.0021425074228497362, "phrase": "single_computer_and_seconds"}], "paper_keywords": ["Parallel computing", " Monte Carlo", " Simulation", " Inverse Adding-Doubling", " Tissue", " Photon"], "paper_abstract": "Parallel implementation of two numerical tools popular in optical studies of biological materials - Inverse Adding-Doubling (IAD) program and Monte Carlo Multi-Layered (MCML) program - was developed and tested in this study. The implementation was based on Message Passing Interface (MPI) and standard C-language. Parallel versions of IAD and MCML programs were compared to their sequential counterparts in validation and performance tests. Additionally, the portability of the programs was tested using a local high performance computing (HPC) cluster, Penguin-On-Demand HPC cluster, and Amazon EC2 cluster. Parallel IAD was tested with up to 150 parallel cores using 1223 input datasets. It demonstrated linear scalability and the speedup was proportional to the number of parallel cores (up to 150x). Parallel MCML was tested with up to 1001 parallel cores using problem sizes of 10(4)-10(9) photon packets. It demonstrated classical performance curves featuring communication overhead and performance saturation point. Optimal performance curve was derived for parallel MCML as a function of problem size. Typical speedup achieved for parallel MCML (up to 326x) demonstrated linear increase with problem size. Precision of MCML results was estimated in a series of tests problem size of 10(6) photon packets was found optimal for calculations of total optical response and 10(8) photon packets for spatially-resolved results. The presented parallel versions of MCML and IAD programs are portable on multiple computing platforms. The parallel programs could significantly speed up the simulation for scientists and be utilized to their full potential in computing systems that are readily available without additional costs. Program summary Program title: MCMLMPI, IADMPI Catalogue identifier: AEWF_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEWF_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 53638 No. of bytes in distributed program, including test data, etc.: 731168 Distribution format: tar.gz Programming language: C. Computer: Up to and including HPC/Cloud CPU-based clusters. Operating system: Windows, Linux, Unix, MacOS - requires ANSI C-compatible compiler. Has the code been vectorized or parallelized?: Yes, using MPI directives. RAM: From megabytes to gigabytes (MCMLMPI), kilobytes to megabytes (IADMPI) Classification: 2.2, 2.5, 18. External routines: dcmt-library (MCMLMPI), cweb-package (IADMPI) Nature of problem: Photon transport in multilayered semi-transparent material, estimation of optical properties (IADMPI) and optical response (MCMLMPI) of multilayered material samples. Solution method: Massively-parallel Monte-Carlo method (MCMLMPI), Inverse Adding-Doubling method (IADMPI) Unusual features: Tracking and analysis of photon packets in turbid media (MCMLMPI) Running time: Many small problems can be solved within seconds, large problems might take hours even on HPC clusters (MCMLMPI); hours on single computer and seconds on HPC cluster (IADMPI) (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Parallel implementation of inverse adding-doubling and Monte Carlo multi-layered programs for high performance computing systems with shared and distributed memory", "paper_id": "WOS:000356196000008"}