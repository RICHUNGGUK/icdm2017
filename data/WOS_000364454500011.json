{"auto_keywords": [{"score": 0.029063297113478832, "phrase": "theta"}, {"score": 0.004681226806682058, "phrase": "algorithm_p"}, {"score": 0.004594137808658537, "phrase": "unbounded_number"}, {"score": 0.004342430078306461, "phrase": "partial_information"}, {"score": 0.004281678802569817, "phrase": "internal_state"}, {"score": 0.004123781267600542, "phrase": "security_guarantee"}, {"score": 0.003953068310598579, "phrase": "p's_input-output_behavior"}, {"score": 0.0037363522297378777, "phrase": "input_an_algorithm_p"}, {"score": 0.003684048704119722, "phrase": "security_parameter_kappa"}, {"score": 0.003615443704375908, "phrase": "functionally_equivalent_algorithm_p"}, {"score": 0.003548111729206587, "phrase": "running_time"}, {"score": 0.003353521531649663, "phrase": "p._p"}, {"score": 0.003038237581227068, "phrase": "adversary_algorithm_a"}, {"score": 0.002818013056816282, "phrase": "adaptively_chosen_leakage_functions"}, {"score": 0.002765490759189057, "phrase": "bounded_output_size"}, {"score": 0.0025053520095236694, "phrase": "computationally_unbounded_a"}, {"score": 0.002447101794397243, "phrase": "computationally_unbounded_leakage_functions"}, {"score": 0.002323663295811215, "phrase": "black-box_access"}, {"score": 0.002280333649167796, "phrase": "input-output_behavior"}, {"score": 0.0022273034991008326, "phrase": "prior_work"}, {"score": 0.0021049977753042253, "phrase": "secure_hardware_components"}], "paper_keywords": ["leakage resilience", " cryptography", " side channels"], "paper_abstract": "We address the following problem: how to execute any algorithm P, for an unbounded number of executions, in the presence of an adversary who observes partial information on the internal state of the computation during executions. The security guarantee is that the adversary learns nothing, beyond P's input-output behavior. Our main result is a compiler, which takes as input an algorithm P and a security parameter kappa and produces a functionally equivalent algorithm P'. The running time of P' is a factor of poly(kappa) slower than P. P' will be composed of a series of calls to poly(kappa)-time computable subalgorithms. During the executions of P', an adversary algorithm A, which can choose the inputs of P', can learn the results of adaptively chosen leakage functions-each of bounded output size (Theta) over tilde(kappa)-on the subalgorithms of P' and the randomness they use. We prove that any computationally unbounded A observing the results of computationally unbounded leakage functions will learn no more from its observations than it could given black-box access only to the input-output behavior of P. Unlike all prior work on this question, this result does not rely on any secure hardware components and is unconditional. Namely, it holds even if P = NP.", "paper_title": "HOW TO COMPUTE IN THE PRESENCE OF LEAKAGE", "paper_id": "WOS:000364454500011"}