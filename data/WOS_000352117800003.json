{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "support_vector_machines"}, {"score": 0.034977456161264894, "phrase": "mic-svm"}, {"score": 0.004779876533667754, "phrase": "modern_hpc_platforms"}, {"score": 0.004591466267316524, "phrase": "data-mining_and_big_data_applications"}, {"score": 0.004442824994700736, "phrase": "increasing_importance"}, {"score": 0.004394350285140955, "phrase": "analytic_capabilities"}, {"score": 0.00431499638498648, "phrase": "svm"}, {"score": 0.004205660924155212, "phrase": "high_performance_computing"}, {"score": 0.004099340908974491, "phrase": "runtime_scheduling"}, {"score": 0.003952082333580496, "phrase": "prediction_accuracy"}, {"score": 0.003908940988435398, "phrase": "insufficient_runtime_information"}, {"score": 0.0037961770933599135, "phrase": "offline_model_training"}, {"score": 0.003754731210715724, "phrase": "significant_runtime_training_overhead"}, {"score": 0.0036866541358511005, "phrase": "many-core_architectures"}, {"score": 0.0036198068824262464, "phrase": "complex_memory_hierarchies"}, {"score": 0.0035671997165370403, "phrase": "runtime_training"}, {"score": 0.0034642599795044445, "phrase": "efficient_parallel_svm_design"}, {"score": 0.0030587884803931964, "phrase": "intel_ivy_bridge_cpus"}, {"score": 0.003036468052876648, "phrase": "intel_xeon_phi_co"}, {"score": 0.002853183476661506, "phrase": "multilevel_parallelism"}, {"score": 0.002770791022032813, "phrase": "general_optimization_methods"}, {"score": 0.0026516301310428756, "phrase": "popular_libsvm"}, {"score": 0.002500662348528587, "phrase": "gpusvm"}, {"score": 0.00234104879370883, "phrase": "cross-platform_performance_comparison_analysis"}, {"score": 0.002306982970571251, "phrase": "ivy_bridge_cpus"}, {"score": 0.0021518035978153878, "phrase": "input_data_patterns"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Machine learning models", " Dynamic modeling", " Support Vector Machine", " Multi & many-core architectures", " Optimization techniques", " Performance analysis"], "paper_abstract": "Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4-84x and 18-47x speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Scaling Support Vector Machines on modern HPC platforms", "paper_id": "WOS:000352117800003"}