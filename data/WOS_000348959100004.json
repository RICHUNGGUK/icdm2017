{"auto_keywords": [{"score": 0.042426215462423846, "phrase": "network_parameters"}, {"score": 0.03138414792442504, "phrase": "different_classes"}, {"score": 0.00481495049065317, "phrase": "deep_neural_network_acoustic_models"}, {"score": 0.00469159356595623, "phrase": "individual_and_combined_impacts"}, {"score": 0.004551647527068771, "phrase": "deep_neural_network"}, {"score": 0.004512488681387335, "phrase": "dnn"}, {"score": 0.004473552224535518, "phrase": "based_speech_recognition_systems"}, {"score": 0.004435006247087171, "phrase": "distant_talking_situations"}, {"score": 0.004377806391261418, "phrase": "acoustic_environmental_distortion"}, {"score": 0.0043213410601860985, "phrase": "recognition_performance"}, {"score": 0.004228839109569368, "phrase": "dnn-based_acoustic_model"}, {"score": 0.004138309011478741, "phrase": "state_alignments"}, {"score": 0.0038115535592184438, "phrase": "speech_quality"}, {"score": 0.00319141242685261, "phrase": "dnn-based_acoustic_models"}, {"score": 0.0030428787018636147, "phrase": "front-end_processing_pipeline"}, {"score": 0.0028513785049061767, "phrase": "combined_effects"}, {"score": 0.0027304365438083874, "phrase": "single_distant_microphone-based_meeting_transcription_task"}, {"score": 0.002661402186598871, "phrase": "si"}, {"score": 0.0026146109167103655, "phrase": "adaptive_training"}, {"score": 0.0025036862991477437, "phrase": "multiple_speech_enhancement_results"}, {"score": 0.0024183363693242943, "phrase": "feature_transformation"}, {"score": 0.0023460397258997525, "phrase": "relative_performance_gains"}, {"score": 0.0022758994740331258, "phrase": "si_and_sat_scenarios"}, {"score": 0.002227083991228041, "phrase": "competitive_dnn-based_systems"}, {"score": 0.002207851585508636, "phrase": "log_mel-filter_bank_features"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Environmental robustness", " Deep neural network", " Front-end", " Meeting transcription"], "paper_abstract": "This paper examines the individual and combined impacts of various front-end approaches on the performance of deep neural network (DNN) based speech recognition systems in distant talking situations, where acoustic environmental distortion degrades the recognition performance. Training of a DNN-based acoustic model consists of generation of state alignments followed by learning the network parameters. This paper first shows that the network parameters are more sensitive to the speech quality than the alignments and thus this stage requires improvement. Then, various front-end robustness approaches to addressing this problem are categorised based on functionality. The degree to which each class of approaches impacts the performance of DNN-based acoustic models is examined experimentally. Based on the results, a front-end processing pipeline is proposed for efficiently combining different classes of approaches. Using this front-end, the combined effects of different classes of approaches are further evaluated in a single distant microphone-based meeting transcription task with both speaker independent (SI) and speaker adaptive training (SAT) set-ups. By combining multiple speech enhancement results, multiple types of features, and feature transformation, the front-end shows relative performance gains of 7.24% and 9.83% in the SI and SAT scenarios, respectively, over competitive DNN-based systems using log mel-filter bank features. (C) 2014 The Authors. Published by Elsevier Ltd.", "paper_title": "Environmentally robust ASR front-end for deep neural network acoustic models", "paper_id": "WOS:000348959100004"}