{"auto_keywords": [{"score": 0.04118724403846205, "phrase": "pose_templates"}, {"score": 0.007474535060937344, "phrase": "contextual_actions"}, {"score": 0.007361050716210174, "phrase": "pose_template"}, {"score": 0.005673863553365263, "phrase": "contextual_objects"}, {"score": 0.00481495049065317, "phrase": "animated_pose_templates_for_modeling_and_detecting_human_actions"}, {"score": 0.004762481202423542, "phrase": "animated_pose_templates"}, {"score": 0.0046287090190243385, "phrase": "cluttered_scenes"}, {"score": 0.0043915410400049825, "phrase": "histogram"}, {"score": 0.0043722825315729045, "phrase": "oriented_gradient"}, {"score": 0.004353157521574124, "phrase": "hog"}, {"score": 0.004184700402824237, "phrase": "hof"}, {"score": 0.004139069256202602, "phrase": "shape_template"}, {"score": 0.0039010960282664, "phrase": "and-or_tree_structure"}, {"score": 0.0038248400678308164, "phrase": "short-term_action_snippets"}, {"score": 0.0037173066877809395, "phrase": "long-term_actions"}, {"score": 0.0036526349178691592, "phrase": "temporal_constraints"}, {"score": 0.0036286730741708667, "phrase": "hidden_markov_model"}, {"score": 0.003511192995693484, "phrase": "additional_parts"}, {"score": 0.00344253225319178, "phrase": "spatial_correlations"}, {"score": 0.0032373623458914085, "phrase": "unknown_parameters"}, {"score": 0.0031532071369076626, "phrase": "unannotated_frames"}, {"score": 0.003064498277886833, "phrase": "training_samples"}, {"score": 0.0030310458515354537, "phrase": "hog_and_hof_features"}, {"score": 0.0030177670428019557, "phrase": "canonical_part_locations"}, {"score": 0.0028944757265206332, "phrase": "semi-supervised_structural_svm_algorithm"}, {"score": 0.0028130251918296284, "phrase": "model_parameters"}, {"score": 0.0028006988338192375, "phrase": "labeled_data"}, {"score": 0.002776207468774595, "phrase": "structural_svm_optimization"}, {"score": 0.0027398703031781403, "phrase": "missing_variables"}, {"score": 0.0026980759969087234, "phrase": "unlabeled_frames"}, {"score": 0.002656917527407142, "phrase": "previous_step"}, {"score": 0.0026336801708726765, "phrase": "high-score_frames"}, {"score": 0.002622137615974489, "phrase": "newly_labeled_examples"}, {"score": 0.002570817338292482, "phrase": "optimization_methods"}, {"score": 0.002548331088496964, "phrase": "concave-convex_procedure"}, {"score": 0.0025039454352400988, "phrase": "local_optimal_solution"}, {"score": 0.0024875003567559695, "phrase": "inference_algorithm"}, {"score": 0.0024441716288045607, "phrase": "top_candidates"}, {"score": 0.002349414996232352, "phrase": "dynamic_programming"}, {"score": 0.0022434880661725493, "phrase": "salient_poses"}, {"score": 0.0021612382221861833, "phrase": "challenging_outdoor_contextual_action_data"}, {"score": 0.0021049977753042253, "phrase": "comparable_or_better_performance"}], "paper_keywords": ["Action detection", " action recognition", " structural SVM", " animated pose templates"], "paper_abstract": "This paper presents animated pose templates (APTs) for detecting short-term, long-term, and contextual actions from cluttered scenes in videos. Each pose template consists of two components: 1) a shape template with deformable parts represented in an And-node whose appearances are represented by the Histogram of Oriented Gradient (HOG) features, and 2) a motion template specifying the motion of the parts by the Histogram of Optical-Flows (HOF) features. A shape template may have more than one motion template represented by an Or-node. Therefore, each action is defined as a mixture (Or-node) of pose templates in an And-Or tree structure. While this pose template is suitable for detecting short-term action snippets in two to five frames, we extend it in two ways: 1) For long-term actions, we animate the pose templates by adding temporal constraints in a Hidden Markov Model (HMM), and 2) for contextual actions, we treat contextual objects as additional parts of the pose templates and add constraints that encode spatial correlations between parts. To train the model, we manually annotate part locations on several keyframes of each video and cluster them into pose templates using EM. This leaves the unknown parameters for our learning algorithm in two groups: 1) latent variables for the unannotated frames including pose-IDs and part locations, 2) model parameters shared by all training samples such as weights for HOG and HOF features, canonical part locations of each pose, coefficients penalizing pose-transition and part-deformation. To learn these parameters, we introduce a semi-supervised structural SVM algorithm that iterates between two steps: 1) learning (updating) model parameters using labeled data by solving a structural SVM optimization, and 2) imputing missing variables (i.e., detecting actions on unlabeled frames) with parameters learned from the previous step and progressively accepting high-score frames as newly labeled examples. This algorithm belongs to a family of optimization methods known as the Concave-Convex Procedure (CCCP) that converge to a local optimal solution. The inference algorithm consists of two components: 1) Detecting top candidates for the pose templates, and 2) computing the sequence of pose templates. Both are done by dynamic programming or, more precisely, beam search. In experiments, we demonstrate that this method is capable of discovering salient poses of actions as well as interactions with contextual objects. We test our method on several public action data sets and a challenging outdoor contextual action data set collected by ourselves. The results show that our model achieves comparable or better performance compared to state-of-the-art methods.", "paper_title": "Animated Pose Templates for Modeling and Detecting Human Actions", "paper_id": "WOS:000331450100004"}