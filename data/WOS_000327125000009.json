{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "f-norm_minimization"}, {"score": 0.004674952036988355, "phrase": "tolerance_criteria"}, {"score": 0.004623503587186056, "phrase": "central_role"}, {"score": 0.0045895181082906715, "phrase": "sparse_approximate_inverse_preconditioning"}, {"score": 0.004154332348800334, "phrase": "empirically_small_positive_quantity"}, {"score": 0.003597284507308585, "phrase": "preconditioner_m."}, {"score": 0.003479692058963528, "phrase": "adaptive_power_sparse_approximate_inverse_algorithm"}, {"score": 0.003428665921276366, "phrase": "mathematical_theory"}, {"score": 0.0034034331737042363, "phrase": "robust_selection_criteria"}, {"score": 0.003378385493203297, "phrase": "drop_tolerances"}, {"score": 0.003267925215811654, "phrase": "adaptive_dropping_criterion"}, {"score": 0.0031727643282003976, "phrase": "small_magnitude"}, {"score": 0.003126224853287381, "phrase": "setup_process"}, {"score": 0.003080365929916083, "phrase": "proposed_criterion"}, {"score": 0.0028928306994090453, "phrase": "comparable_quality"}, {"score": 0.002860938351029155, "phrase": "potentially_denser_matrix"}, {"score": 0.002706659627293686, "phrase": "static_f-norm_minimization_based_preconditioning_procedures"}, {"score": 0.0024768883044869023, "phrase": "static_sparse_approximate_inverse_procedure"}, {"score": 0.002422552850701705, "phrase": "adaptive_procedure"}, {"score": 0.0023781827711414107, "phrase": "static_procedure"}, {"score": 0.0023346234464901978, "phrase": "setup_time"}, {"score": 0.0022498783454674254, "phrase": "sparser_m"}, {"score": 0.002233301265743633, "phrase": "krylov_iterations"}, {"score": 0.002208663870368384, "phrase": "numerical_experiments"}, {"score": 0.0021049977753042253, "phrase": "dropping_criteria"}], "paper_keywords": ["Preconditioning", " Sparse approximate inverse", " Drop tolerance selection criteria", " F-norm minimization", " Adaptive", " Static"], "paper_abstract": "Drop tolerance criteria play a central role in Sparse Approximate Inverse preconditioning. Such criteria have received, however, little attention and have been treated heuristically in the following manner: If the size of an entry is below some empirically small positive quantity, then it is set to zero. The meaning of \"small\" is vague and has not been considered rigorously. It has not been clear how drop tolerances affect the quality and effectiveness of a preconditioner M. In this paper, we focus on the adaptive Power Sparse Approximate Inverse algorithm and establish a mathematical theory on robust selection criteria for drop tolerances. Using the theory, we derive an adaptive dropping criterion that is used to drop entries of small magnitude dynamically during the setup process of M. The proposed criterion enables us to make M both as sparse as possible as well as to be of comparable quality to the potentially denser matrix which is obtained without dropping. As a byproduct, the theory applies to static F-norm minimization based preconditioning procedures, and a similar dropping criterion is given that can be used to sparsify a matrix after it has been computed by a static sparse approximate inverse procedure. In contrast to the adaptive procedure, dropping in the static procedure does not reduce the setup time of the matrix but makes the application of the sparser M for Krylov iterations cheaper. Numerical experiments reported confirm the theory and illustrate the robustness and effectiveness of the dropping criteria.", "paper_title": "Robust dropping criteria for F-norm minimization based sparse approximate inverse preconditioning", "paper_id": "WOS:000327125000009"}