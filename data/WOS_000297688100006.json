{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sigmod"}, {"score": 0.0033167679667625667, "phrase": "sigmod_repeatability_process"}, {"score": 0.0031845067058084583, "phrase": "review_process"}, {"score": 0.0027804579695873827, "phrase": "sharp_contrast"}, {"score": 0.002724428781940696, "phrase": "high_participation"}, {"score": 0.002687702958515561, "phrase": "asian_authors"}, {"score": 0.0026335379469691997, "phrase": "low_participation"}, {"score": 0.002598034157587518, "phrase": "american_authors"}, {"score": 0.002427549902743566, "phrase": "linux_packages"}, {"score": 0.0021049977753042253, "phrase": "executable_papers"}], "paper_keywords": [""], "paper_abstract": "SIGMOD has offered, since 2008, to verify the experiments published in the papers accepted at the conference. This year, we have been in charge of reproducing the experiments provided by the authors (repeatability), and exploring changes to experiment parameters (workability). In this paper, we assess the SIGMOD repeatability process in terms of participation, review process and results. While the participation is stable in terms of number of submissions, we find this year a sharp contrast between the high participation from Asian authors and the low participation from American authors. We also find that most experiments are distributed as Linux packages accompanied by instructions on how to setup and run the experiments. We are still far from the vision of executable papers.", "paper_title": "Repeatability and Workability Evaluation of SIGMOD 2011", "paper_id": "WOS:000297688100006"}