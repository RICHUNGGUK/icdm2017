{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "probabilistic_deterministic_automata"}, {"score": 0.004766388573192505, "phrase": "data_streams"}, {"score": 0.004718314112245913, "phrase": "markovian_models"}, {"score": 0.004670722261414736, "phrase": "hidden_state"}, {"score": 0.004623608221548592, "phrase": "widely-used_formalisms"}, {"score": 0.004553822674247401, "phrase": "sequential_phenomena"}, {"score": 0.004198965651975536, "phrase": "batch_mode"}, {"score": 0.00409382470158851, "phrase": "pac-like_learning_guarantees"}, {"score": 0.003911135156992, "phrase": "probabilistic_deterministic_finite_automata"}, {"score": 0.003736567662016446, "phrase": "pdfa"}, {"score": 0.00349803055484394, "phrase": "restrictive_data_stream_scenario"}, {"score": 0.003445174898115646, "phrase": "existing_methods"}, {"score": 0.0032746712603464235, "phrase": "memory_sublinear"}, {"score": 0.0032251796659158696, "phrase": "stream_length"}, {"score": 0.0031603487697867538, "phrase": "input_items"}, {"score": 0.0031284221314354095, "phrase": "amortized_constant_time"}, {"score": 0.002769483748975581, "phrase": "target_distribution"}, {"score": 0.002592523232809191, "phrase": "input_distribution"}, {"score": 0.0025533148412439166, "phrase": "new_models"}, {"score": 0.0024766636161181544, "phrase": "rigorous_pac-like_bounds"}, {"score": 0.0023539789234610763, "phrase": "key_usage"}, {"score": 0.002271725588755076, "phrase": "memory_and_processing_time"}, {"score": 0.0021591707887653865, "phrase": "different_tests"}, {"score": 0.0021373366163721518, "phrase": "state_equivalence"}, {"score": 0.0021049977753042253, "phrase": "change_detection"}], "paper_keywords": ["PAC learning", " Data streams", " Probabilistic automata", " PDFA", " Stream sketches"], "paper_abstract": "Markovian models with hidden state are widely-used formalisms for modeling sequential phenomena. Learnability of these models has been well studied when the sample is given in batch mode, and algorithms with PAC-like learning guarantees exist for specific classes of models such as Probabilistic Deterministic Finite Automata (PDFA). Here we focus on PDFA and give an algorithm for inferring models in this class in the restrictive data stream scenario: Unlike existing methods, our algorithm works incrementally and in one pass, uses memory sublinear in the stream length, and processes input items in amortized constant time. We also present extensions of the algorithm that (1) reduce to a minimum the need for guessing parameters of the target distribution and (2) are able to adapt to changes in the input distribution, relearning new models when needed. We provide rigorous PAC-like bounds for all of the above. Our algorithm makes a key usage of stream sketching techniques for reducing memory and processing time, and is modular in that it can use different tests for state equivalence and for change detection in the stream.", "paper_title": "Adaptively learning probabilistic deterministic automata from data streams", "paper_id": "WOS:000338201200005"}