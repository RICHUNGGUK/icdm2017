{"auto_keywords": [{"score": 0.04208729377636017, "phrase": "oracle_inequality"}, {"score": 0.00481495049065317, "phrase": "low_noise_condition"}, {"score": 0.004471978831302331, "phrase": "minimax_sense"}, {"score": 0.00415333528095237, "phrase": "binary_classification"}, {"score": 0.003909522412849035, "phrase": "margin_assumption"}, {"score": 0.0036553070575015344, "phrase": "aggregation_procedure"}, {"score": 0.0035823043879169153, "phrase": "exponential_weights"}, {"score": 0.00344062888674746, "phrase": "optimal_residual"}, {"score": 0.003195236629209403, "phrase": "margin_parameter"}, {"score": 0.0027370835853738626, "phrase": "minimax_classifiers"}, {"score": 0.002700485592691974, "phrase": "margin_and_regularity_assumptions"}, {"score": 0.0021049977753042253, "phrase": "easily_implementable_classifier_ad_aptive"}], "paper_keywords": [""], "paper_abstract": "We consider the problem of optimality, in a minimax sense, and adaptivity to the margin and to regularity in binary classification. We prove an oracle inequality, under the margin assumption (low noise condition), satisfied by an aggregation procedure which uses exponential weights. This oracle inequality has an optimal residual: (log M/n)(kappa/(2 kappa-1)) where n is the margin parameter, M the number of classifiers to aggregate and n the number of observations. We use this inequality first to construct minimax classifiers under margin and regularity assumptions and second to aggregate them to obtain a classifier which is adaptive both to the margin and regularity. Moreover, by aggregating plug-in classifiers (only log n), we provide an easily implementable classifier ad aptive both to the margin and to regularity.", "paper_title": "Optimal oracle inequality for aggregation of classifiers under low noise condition", "paper_id": "WOS:000239587900028"}