{"auto_keywords": [{"score": 0.048016861279987125, "phrase": "least-squares_regression"}, {"score": 0.00481495049065317, "phrase": "boolean_relaxations"}, {"score": 0.004721711709176679, "phrase": "novel_relaxations"}, {"score": 0.004675768396728975, "phrase": "cardinality-constrained_learning_problems"}, {"score": 0.004540591176992658, "phrase": "special_but_important_case"}, {"score": 0.004345085569121329, "phrase": "cardinality-constrained_problem"}, {"score": 0.004260906498835763, "phrase": "boolean_program"}, {"score": 0.0041579627441997, "phrase": "convex_relaxations"}, {"score": 0.004077414032048439, "phrase": "lasserre"}, {"score": 0.004037694641919865, "phrase": "sherali-adams_hierarchies"}, {"score": 0.0038827100184187805, "phrase": "first-order_relaxation"}, {"score": 0.0036434002236454305, "phrase": "unified_manner"}, {"score": 0.003572766402294553, "phrase": "special_case"}, {"score": 0.003368948014436942, "phrase": "high_probability"}, {"score": 0.0033361232750221863, "phrase": "random_ensembles"}, {"score": 0.003303617297414837, "phrase": "suitable_incoherence_conditions"}, {"score": 0.003130396338031522, "phrase": "known_methods"}, {"score": 0.0030696773722514105, "phrase": "lower_bounds"}, {"score": 0.0026501935378083663, "phrase": "relaxed_solution"}, {"score": 0.0026115277372906805, "phrase": "principled_way"}, {"score": 0.002573424607574822, "phrase": "provably_good_feasible_solutions"}, {"score": 0.002474505516708906, "phrase": "high_quality_estimates"}, {"score": 0.0024383967028975616, "phrase": "incoherence_conditions"}, {"score": 0.002321787726770895, "phrase": "real_datasets"}, {"score": 0.0022216045996270974, "phrase": "relaxation-randomization_strategy"}, {"score": 0.002125735077004312, "phrase": "-based_methods"}, {"score": 0.0021049977753042253, "phrase": "greedy_selection_heuristics"}], "paper_keywords": ["Sparsity", " Regularization", " Convex relaxation", " Combinatorial optimization", " Machine learning"], "paper_abstract": "We introduce novel relaxations for cardinality-constrained learning problems, including least-squares regression as a special but important case. Our approach is based on reformulating a cardinality-constrained problem exactly as a Boolean program, to which standard convex relaxations such as the Lasserre and Sherali-Adams hierarchies can be applied. We analyze the first-order relaxation in detail, deriving necessary and sufficient conditions for exactness in a unified manner. In the special case of least-squares regression, we show that these conditions are satisfied with high probability for random ensembles satisfying suitable incoherence conditions, similar to results on -relaxations. In contrast to known methods, our relaxations yield lower bounds on the objective, and it can be verified whether or not the relaxation is exact. If it is not, we show that randomization based on the relaxed solution offers a principled way to generate provably good feasible solutions. This property enables us to obtain high quality estimates even if incoherence conditions are not met, as might be expected in real datasets. We numerically illustrate the performance of the relaxation-randomization strategy in both synthetic and real high-dimensional datasets, revealing substantial improvements relative to -based methods and greedy selection heuristics.", "paper_title": "Sparse learning via Boolean relaxations", "paper_id": "WOS:000354623300004"}