{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "minimax_sparse_logistic_regression"}, {"score": 0.004640808212832262, "phrase": "strong_convexity"}, {"score": 0.004592214823540993, "phrase": "probabilistic_underpinnings"}, {"score": 0.004544127927346964, "phrase": "logistic_regression"}, {"score": 0.004068510313497539, "phrase": "small_subset"}, {"score": 0.003839510177926892, "phrase": "prediction_model"}, {"score": 0.003799275066001196, "phrase": "robust_predictions"}, {"score": 0.0037594599958631404, "phrase": "deeper_analysis"}, {"score": 0.0036617315529835497, "phrase": "sparse_solution"}, {"score": 0.0035853745478488254, "phrase": "input_features"}, {"score": 0.003278167066287366, "phrase": "unbiased_sparse_solutions"}, {"score": 0.0029038817737006405, "phrase": "new_minimax_sparse_lr_model"}, {"score": 0.002740253546178098, "phrase": "cutting_plane_algorithm"}, {"score": 0.0026689487424251907, "phrase": "resultant_nonsmooth_minimax_subproblems"}, {"score": 0.0026270572353632297, "phrase": "smoothing_coordinate_descent_method"}, {"score": 0.0025722202862084186, "phrase": "numerical_issues"}, {"score": 0.0025452314655691165, "phrase": "convergence_rate"}, {"score": 0.002440071551819324, "phrase": "experimental_results"}, {"score": 0.0023146962503449186, "phrase": "better_prediction_accuracy"}, {"score": 0.0022544399432457164, "phrase": "selected_features"}, {"score": 0.002219040041886641, "phrase": "better_or_competitive_scalability"}, {"score": 0.0021498955164659145, "phrase": "baseline_methods"}], "paper_keywords": ["Feature selection", " minimax problem", " single-nucleotide polymorphism (SNP) detection", " smoothing method", " sparse logistic regression"], "paper_abstract": "Because of the strong convexity and probabilistic underpinnings, logistic regression (LR) is widely used in many real-world applications. However, in many problems, such as bioinformatics, choosing a small subset of features with the most discriminative power are desirable for interpreting the prediction model, robust predictions or deeper analysis. To achieve a sparse solution with respect to input features, many sparse LR models are proposed. However, it is still challenging for them to efficiently obtain unbiased sparse solutions to very high-dimensional problems (e. g., identifying the most discriminative subset from millions of features). In this paper, we propose a new minimax sparse LR model for very high-dimensional feature selections, which can be efficiently solved by a cutting plane algorithm. To solve the resultant nonsmooth minimax subproblems, a smoothing coordinate descent method is presented. Numerical issues and convergence rate of this method are carefully studied. Experimental results on several synthetic and real-world datasets show that the proposed method can obtain better prediction accuracy with the same number of selected features and has better or competitive scalability on very high-dimensional problems compared with the baseline methods, including the l(1)-regularized LR.", "paper_title": "Minimax Sparse Logistic Regression for Very High-Dimensional Feature Selection", "paper_id": "WOS:000325981400009"}