{"auto_keywords": [{"score": 0.05005188413713696, "phrase": "sensitivity-based_optimization"}, {"score": 0.049226991195811136, "phrase": "markov"}, {"score": 0.004521566043596533, "phrase": "increasingly_important_area"}, {"score": 0.004284302478991387, "phrase": "performance_sensitivity_analysis"}, {"score": 0.004208002469225673, "phrase": "policy-iteration_algorithms"}, {"score": 0.004133055661437163, "phrase": "gradient_estimation_methods"}, {"score": 0.0033914308417007316, "phrase": "average_reward"}, {"score": 0.0030171910207671205, "phrase": "performance-derivative_formulas"}, {"score": 0.0027825094419000637, "phrase": "performance-derivative_formula"}, {"score": 0.0026600905156986317, "phrase": "new_method"}, {"score": 0.0025660347613972573, "phrase": "performance_gradients"}, {"score": 0.00247529638625566, "phrase": "performance-difference_formula"}, {"score": 0.002366361492754389, "phrase": "sufficient_optimality_condition"}, {"score": 0.0023033100935570755, "phrase": "discounted_reward_formulation"}, {"score": 0.0021821915946356168, "phrase": "policy-iteration_algorithm"}, {"score": 0.0021049977753042253, "phrase": "nearly_optimal_finite-state-controller_policy"}], "paper_keywords": ["Finite-state controller (FSC)", " gradient estimation", " partially observable Markov decision processes (POMDPs)", " policy iteration", " sensitivity analysis"], "paper_abstract": "The sensitivity-based optimization of Markov systems has become an increasingly important area. From the perspective of performance sensitivity analysis, policy-iteration algorithms and gradient estimation methods can be directly obtained for Markov decision processes (MDPs). In this correspondence, the sensitivity-based optimization is extended to average reward partially observable MDPs (POMDPs). We derive the performance-difference and performance-derivative formulas of POMDPs. On the basis of the performance-derivative formula, we present a new method to estimate the performance gradients. From the performance-difference formula, we obtain a sufficient optimality condition without the discounted reward formulation. We also propose a policy-iteration algorithm to obtain a nearly optimal finite-state-controller policy.", "paper_title": "Partially Observable Markov Decision Processes and Performance Sensitivity Analysis", "paper_id": "WOS:000261310500019"}