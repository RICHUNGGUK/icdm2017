{"auto_keywords": [{"score": 0.04651161943439759, "phrase": "logistic_loss"}, {"score": 0.035841180256671316, "phrase": "degraded_performance"}, {"score": 0.029363542304911314, "phrase": "dense_hessian"}, {"score": 0.004814955496608903, "phrase": "logitboost"}, {"score": 0.0047722145315845016, "phrase": "popular_boosting_variant"}, {"score": 0.004646258518859209, "phrase": "multi-class_classification"}, {"score": 0.004591345494188032, "phrase": "statistical_viewpoint"}, {"score": 0.004510184935928004, "phrase": "additive_tree_regression"}, {"score": 0.004262481067951517, "phrase": "sound_multi-class"}, {"score": 0.004064427818174677, "phrase": "multiclass_logistic_loss"}, {"score": 0.003992544149307519, "phrase": "invariant_property"}, {"score": 0.003898665270269876, "phrase": "optimal_classifier_output"}, {"score": 0.0037396377535454887, "phrase": "output_vector"}, {"score": 0.00368442237104581, "phrase": "loss_value"}, {"score": 0.003587073648802551, "phrase": "hessian_matrices"}, {"score": 0.0035341028694769036, "phrase": "tree_node"}, {"score": 0.0034922880300145283, "phrase": "node_value_fittings"}, {"score": 0.0034407121288076783, "phrase": "learning_problem"}, {"score": 0.0033398265543234393, "phrase": "original_logitboost_algorithm"}, {"score": 0.0033003051971174104, "phrase": "abc-logitboost"}, {"score": 0.0032612451286025555, "phrase": "latter's_more_careful_treatment"}, {"score": 0.003165605014237376, "phrase": "new_techniques"}, {"score": 0.0031095683360022447, "phrase": "multiclass_logitboost_setting"}, {"score": 0.0030363880573817483, "phrase": "vector_tree_model"}, {"score": 0.003000444353938951, "phrase": "node_value"}, {"score": 0.0029473226605107187, "phrase": "unique_classifier_output"}, {"score": 0.002895138734813006, "phrase": "sum-to-zero_constraint"}, {"score": 0.0028185852354729026, "phrase": "adaptive_block"}, {"score": 0.0027522340188241446, "phrase": "computing_tree"}, {"score": 0.002719644855867468, "phrase": "node_values"}, {"score": 0.0027034948760997564, "phrase": "higher_classification_accuracy"}, {"score": 0.0026874405408023956, "phrase": "faster_convergence_rates"}, {"score": 0.0026319954633536764, "phrase": "public_data_sets"}, {"score": 0.0025623821393275146, "phrase": "abc-logitboost_implementations"}, {"score": 0.00248718591918233, "phrase": "logitboost's_dense_hessian_matrix"}, {"score": 0.0024213933464338885, "phrase": "multi-class_logistic_loss"}, {"score": 0.002385595105638277, "phrase": "diagonal_hessian_matrix"}, {"score": 0.0023224836187453954, "phrase": "newton_descent"}, {"score": 0.0021049977753042253, "phrase": "efficient_implementations"}], "paper_keywords": ["LogitBoost", " Boosting", " Ensemble", " Supervised learning", " Convex optimization"], "paper_abstract": "LogitBoost is a popular Boosting variant that can be applied to either binary or multi-class classification. From a statistical viewpoint LogitBoost can be seen as additive tree regression by minimizing the Logistic loss. Following this setting, it is still non-trivial to devise a sound multi-class LogitBoost compared with to devise its binary counterpart. The difficulties are due to two important factors arising in multiclass Logistic loss. The first is the invariant property implied by the Logistic loss, causing the optimal classifier output being not unique, i.e. adding a constant to each component of the output vector won't change the loss value. The second is the density of the Hessian matrices that arise when computing tree node split gain and node value fittings. Oversimplification of this learning problem can lead to degraded performance. For example, the original LogitBoost algorithm is outperformed by ABC-LogitBoost thanks to the latter's more careful treatment of the above two factors. In this paper we propose new techniques to address the two main difficulties in multiclass LogitBoost setting: (1) we adopt a vector tree model (i.e. each node value is vector) where the unique classifier output is guaranteed by adding a sum-to-zero constraint, and (2) we use an adaptive block coordinate descent that exploits the dense Hessian when computing tree split gain and node values. Higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the ABC-LogitBoost implementations. We also discuss another possibility to cope with LogitBoost's dense Hessian matrix. We derive a loss similar to the multi-class Logistic loss but which guarantees a diagonal Hessian matrix. While this makes the optimization (by Newton descent) easier we unfortunately observe degraded performance for this modification. We argue that working with the dense Hessian is likely unavoidable, therefore making techniques like those proposed in this paper necessary for efficient implementations.", "paper_title": "An improved multiclass LogitBoost using adaptive-one-vs-one", "paper_id": "WOS:000344173100003"}