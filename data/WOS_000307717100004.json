{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "linear_algorithms"}, {"score": 0.025661707592912714, "phrase": "latent_variables"}, {"score": 0.004773040022691547, "phrase": "regularized_sparse_kernel_slow_feature_analysis"}, {"score": 0.004710854091689325, "phrase": "non-linear_basis_functions"}, {"score": 0.004316396683332752, "phrase": "slow_feature_analysis"}, {"score": 0.004223032986655664, "phrase": "non-linear_optimization"}, {"score": 0.004167982855788402, "phrase": "unsupervised_learning_method"}, {"score": 0.0041136473765874815, "phrase": "orthogonal_basis"}, {"score": 0.00406001734835979, "phrase": "unknown_latent_space"}, {"score": 0.0038866559907693166, "phrase": "pca"}, {"score": 0.003852408431344622, "phrase": "sfa"}, {"score": 0.003703611021999795, "phrase": "direct_use"}, {"score": 0.0036553070575015344, "phrase": "latent_space"}, {"score": 0.003623453648863547, "phrase": "real-world_time_series"}, {"score": 0.003529543381704363, "phrase": "current_sfa_algorithms"}, {"score": 0.0032908057622663732, "phrase": "kernel_trick"}, {"score": 0.0031775430407770026, "phrase": "kernelized_sfa_algorithm"}, {"score": 0.0031223774204093713, "phrase": "powerful_function_class"}, {"score": 0.0030951536886513567, "phrase": "large_data_sets"}, {"score": 0.0029755458462182565, "phrase": "pursuit_approach"}, {"score": 0.002835599469639296, "phrase": "small_data_sets"}, {"score": 0.0027741761880779535, "phrase": "kernel_sfa_approach"}, {"score": 0.002737960968372391, "phrase": "over-fitting_and_numerical_instabilities"}, {"score": 0.0026786469998321084, "phrase": "stable_solution"}, {"score": 0.0025977542995083624, "phrase": "sfa_objective"}, {"score": 0.002508284945907166, "phrase": "feature_space"}, {"score": 0.002464709648714996, "phrase": "fourier_basis"}, {"score": 0.0024325244760735566, "phrase": "unknown_space"}, {"score": 0.0022678201262721323, "phrase": "vowel_classification_task"}, {"score": 0.002218668420806254, "phrase": "kernel_pca."}, {"score": 0.0021896891011033105, "phrase": "excellent_classification_accuracy"}, {"score": 0.0021049977753042253, "phrase": "kernel_pca"}], "paper_keywords": ["Time series", " Latent variables", " Unsupervised learning", " Slow feature analysis", " Sparse kernel methods", " Linear classification"], "paper_abstract": "Without non-linear basis functions many problems can not be solved by linear algorithms. This article proposes a method to automatically construct such basis functions with slow feature analysis (SFA). Non-linear optimization of this unsupervised learning method generates an orthogonal basis on the unknown latent space for a given time series. In contrast to methods like PCA, SFA is thus well suited for techniques that make direct use of the latent space. Real-world time series can be complex, and current SFA algorithms are either not powerful enough or tend to over-fit. We make use of the kernel trick in combination with sparsification to develop a kernelized SFA algorithm which provides a powerful function class for large data sets. Sparsity is achieved by a novel matching pursuit approach that can be applied to other tasks as well. For small data sets, however, the kernel SFA approach leads to over-fitting and numerical instabilities. To enforce a stable solution, we introduce regularization to the SFA objective. We hypothesize that our algorithm generates a feature space that resembles a Fourier basis in the unknown space of latent variables underlying a given real-world time series. We evaluate this hypothesis at the example of a vowel classification task in comparison to sparse kernel PCA. Our results show excellent classification accuracy and demonstrate the superiority of kernel SFA over kernel PCA in encoding latent variables.", "paper_title": "Generating feature spaces for linear algorithms with regularized sparse kernel slow feature analysis", "paper_id": "WOS:000307717100004"}