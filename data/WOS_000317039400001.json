{"auto_keywords": [{"score": 0.04904273003005848, "phrase": "standard_enthalpy"}, {"score": 0.015719716506582538, "phrase": "feature_selection"}, {"score": 0.011414951397740307, "phrase": "variable_importance"}, {"score": 0.008984377625651876, "phrase": "proposed_approach"}, {"score": 0.00478867907727921, "phrase": "qspr_models"}, {"score": 0.004571055044692412, "phrase": "main_topics"}, {"score": 0.004508942069742167, "phrase": "quantitative_structure-property_relationship"}, {"score": 0.004459857205489101, "phrase": "predictive_models"}, {"score": 0.004063547375196532, "phrase": "stepwise_procedures"}, {"score": 0.003975515793563275, "phrase": "evolutionary_programming"}, {"score": 0.0038893838564184107, "phrase": "minimum_subset"}, {"score": 0.003784328428527216, "phrase": "good_performance"}, {"score": 0.0036519716087768253, "phrase": "irrelevant_or_redundant_features"}, {"score": 0.003622088701900472, "phrase": "poor_generalization_capacity"}, {"score": 0.0035630518377071916, "phrase": "alternative_selection_method"}, {"score": 0.003524227590204673, "phrase": "random_forests"}, {"score": 0.0034196209470855566, "phrase": "qspr_regression_problems"}, {"score": 0.0033546701239540555, "phrase": "manually_curated_dataset"}, {"score": 0.0032819448594643853, "phrase": "subsequent_predictive_models"}, {"score": 0.003246173672295418, "phrase": "support_vector_machines"}, {"score": 0.0031845067058084583, "phrase": "ranked_list"}, {"score": 0.0030562674358159945, "phrase": "high_dimensional_dataset"}, {"score": 0.0030064251340251196, "phrase": "highly_correlated_variables"}, {"score": 0.002981808717396183, "phrase": "feature_selection_step"}, {"score": 0.00294122712771669, "phrase": "lower_prediction_errors"}, {"score": 0.002925149105307855, "phrase": "rmse_values"}, {"score": 0.0028305047329728254, "phrase": "total_number"}, {"score": 0.0026575551838540676, "phrase": "feature_space"}, {"score": 0.0026357877401570764, "phrase": "predictive_model"}, {"score": 0.002522662955545598, "phrase": "independent_set"}, {"score": 0.002474706265455793, "phrase": "new_data"}, {"score": 0.00239460114991662, "phrase": "training_set"}, {"score": 0.002317082977487583, "phrase": "proposed_methodology"}, {"score": 0.002291804012486431, "phrase": "prediction_performance"}, {"score": 0.0022359278646792153, "phrase": "limited_set"}, {"score": 0.002223696715077018, "phrase": "molecular_descriptors"}, {"score": 0.0022054750513000854, "phrase": "faster_and_more_cost-effective_calculation"}, {"score": 0.0021399277552314067, "phrase": "better_understanding"}, {"score": 0.002122391060295923, "phrase": "underlying_relationship"}, {"score": 0.0021049977753042253, "phrase": "molecular_structure"}], "paper_keywords": ["Feature selection", " Variable importance", " High dimensional data", " Random forests", " Data-mining", " Property prediction", " QSPR", " Hybrid methodology"], "paper_abstract": "Background: One of the main topics in the development of quantitative structure-property relationship (QSPR) predictive models is the identification of the subset of variables that represent the structure of a molecule and which are predictors for a given property. There are several automated feature selection methods, ranging from backward, forward or stepwise procedures, to further elaborated methodologies such as evolutionary programming. The problem lies in selecting the minimum subset of descriptors that can predict a certain property with a good performance, computationally efficient and in a more robust way, since the presence of irrelevant or redundant features can cause poor generalization capacity. In this paper an alternative selection method, based on Random Forests to determine the variable importance is proposed in the context of QSPR regression problems, with an application to a manually curated dataset for predicting standard enthalpy of formation. The subsequent predictive models are trained with support vector machines introducing the variables sequentially from a ranked list based on the variable importance. Results: The model generalizes well even with a high dimensional dataset and in the presence of highly correlated variables. The feature selection step was shown to yield lower prediction errors with RMSE values 23% lower than without feature selection, albeit using only 6% of the total number of variables (89 from the original 1485). The proposed approach further compared favourably with other feature selection methods and dimension reduction of the feature space. The predictive model was selected using a 10-fold cross validation procedure and, after selection, it was validated with an independent set to assess its performance when applied to new data and the results were similar to the ones obtained for the training set, supporting the robustness of the proposed approach. Conclusions: The proposed methodology seemingly improves the prediction performance of standard enthalpy of formation of hydrocarbons using a limited set of molecular descriptors, providing faster and more cost-effective calculation of descriptors by reducing their numbers, and providing a better understanding of the underlying relationship between the molecular structure represented by descriptors and the property of interest.", "paper_title": "Random forests for feature selection in QSPR Models - an application for predicting standard enthalpy of formation of hydrocarbons", "paper_id": "WOS:000317039400001"}