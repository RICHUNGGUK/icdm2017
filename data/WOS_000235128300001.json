{"auto_keywords": [{"score": 0.03631223673599187, "phrase": "theta"}, {"score": 0.00481495049065317, "phrase": "supervised_learning"}, {"score": 0.004707672504464783, "phrase": "unknown_borel_measure"}, {"score": 0.003886770770835211, "phrase": "main_focus"}, {"score": 0.0032085525502105836, "phrase": "possible_algorithms"}, {"score": 0.0027811441123794427, "phrase": "optimal_rate"}, {"score": 0.0026986685585382347, "phrase": "upper_bounds"}, {"score": 0.0025409639091476363, "phrase": "nonlinear_approximation"}, {"score": 0.0025219085140220773, "phrase": "lower_bounds"}, {"score": 0.0024842242921857705, "phrase": "kullback-leibler_information"}, {"score": 0.0024563302149829, "phrase": "fano_inequalities"}, {"score": 0.002161216637621284, "phrase": "second_type"}], "paper_keywords": ["learning theory", " regression estimation", " entropy", " nonlinear methods", " upper and lower estimates"], "paper_abstract": "Let rho be an unknown Borel measure defined on the space Z := X x Y with X subset of R-d and Y = [-M,M]. Given a set z of m samples z(i) =(x(i),y(i)) drawn according to rho, the problem of estimating a regression function f(rho) using these samples is considered. The main focus is to understand what is the rate of approximation, measured either in expectation or probability, that can be obtained under a given prior f(rho) is an element of Theta, i.e., under the assumption that f(rho) is in the set Theta, and what are possible algorithms for obtaining optimal or semioptimal (up to logarithms) results. The optimal rate of decay in terms of m is established for many priors given either in terms of smoothness of f(rho) or its rate of approximation measured in one of several ways. This optimal rate is determined by two types of results. Upper bounds are established using various tools in approximation such as entropy, widths, and linear and nonlinear approximation. Lower bounds are proved using Kullback-Leibler information together with Fano inequalities and a certain type of entropy. A distinction is drawn between algorithms which employ knowledge of the prior in the construction of the estimator and those that do not. Algorithms of the second type which are universally optimal for a certain range of priors are given.", "paper_title": "Approximation methods for supervised learning", "paper_id": "WOS:000235128300001"}