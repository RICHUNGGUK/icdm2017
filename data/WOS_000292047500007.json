{"auto_keywords": [{"score": 0.026941741461993733, "phrase": "pbs"}, {"score": 0.00481495049065317, "phrase": "improved_hpc_center"}, {"score": 0.004672444473700524, "phrase": "modern_high-performance_computing"}, {"score": 0.004488946121663966, "phrase": "data_deluge"}, {"score": 0.0044219992735047954, "phrase": "scientific_applications"}, {"score": 0.0043560464825349275, "phrase": "large_data"}, {"score": 0.004291073117901643, "phrase": "significant_commitment"}, {"score": 0.004227064760262924, "phrase": "high-throughput_center_storage_system"}, {"score": 0.004060986196931943, "phrase": "scratch_space"}, {"score": 0.00396050388346298, "phrase": "simple_\"purge_policies"}, {"score": 0.0038819041309353024, "phrase": "sophisticated_end-user_data_services"}, {"score": 0.003766908559075843, "phrase": "user_serviceability"}, {"score": 0.003729335910488339, "phrase": "end-user_data_services"}, {"score": 0.0036007478932719417, "phrase": "point-to-point_transfers"}, {"score": 0.0034940519373446335, "phrase": "center's_purge"}, {"score": 0.003459191419478298, "phrase": "users'_delivery_deadlines"}, {"score": 0.003356675879109443, "phrase": "changing_dynamics"}, {"score": 0.003306559295382181, "phrase": "end-to-end_data_path"}, {"score": 0.0030823667846259836, "phrase": "high_performance"}, {"score": 0.002816246391862638, "phrase": "application_result_data"}, {"score": 0.002719055901632759, "phrase": "user-specified_intermediate_and_landmark_sites"}, {"score": 0.002665027876903161, "phrase": "decentralized_fault-tolerant_delivery"}, {"score": 0.002560162916190598, "phrase": "production_job_scheduler"}, {"score": 0.002496722870659199, "phrase": "data_transfer_tool"}, {"score": 0.0023864566654035924, "phrase": "supercomputer_job_log-driven_simulations"}, {"score": 0.002327310963701377, "phrase": "offloading_times"}, {"score": 0.0021802872319258977, "phrase": "exposure_window"}, {"score": 0.0021049977753042253, "phrase": "center-user_service_level_agreements"}], "paper_keywords": ["High-performance data management", " HPC center serviceability", " offloading", " end-user data delivery", " peer-to-peer"], "paper_abstract": "Modern High-Performance Computing (HPC) centers are facing a data deluge from emerging scientific applications. Supporting large data entails a significant commitment of the high-throughput center storage system, scratch space. However, the scratch space is typically managed using simple \"purge policies,\" without sophisticated end-user data services to balance resource consumption and user serviceability. End-user data services such as offloading are performed using point-to-point transfers that are unable to reconcile center's purge and users' delivery deadlines, unable to adapt to changing dynamics in the end-to-end data path and are not fault-tolerant. Such inefficiencies can be prohibitive to sustaining high performance. In this paper, we address the above issues by designing a framework for the timely, decentralized offload of application result data. Our framework uses an overlay of user-specified intermediate and landmark sites to orchestrate a decentralized fault-tolerant delivery. We have implemented our techniques within a production job scheduler (PBS) and data transfer tool (BitTorrent). Our evaluation using both a real implementation and supercomputer job log-driven simulations show that: the offloading times can be significantly reduced (90.4 percent for a 5 GB data transfer); the exposure window can be minimized while also meeting center-user service level agreements.", "paper_title": "Timely Result-Data Offloading for Improved HPC Center Scratch Provisioning and Serviceability", "paper_id": "WOS:000292047500007"}