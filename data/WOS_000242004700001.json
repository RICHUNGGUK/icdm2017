{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "reliable_observations"}, {"score": 0.004718431349852634, "phrase": "force-dynamic_models"}, {"score": 0.0046238380150870435, "phrase": "trainable_sequential-inference_technique"}, {"score": 0.004549524321051407, "phrase": "large_state_and_observation_spaces"}, {"score": 0.004144723510171449, "phrase": "critical_component"}, {"score": 0.003980106938194548, "phrase": "force_dynamics"}, {"score": 0.003947973513495612, "phrase": "event_logic"}, {"score": 0.003884479830773329, "phrase": "artificial_intelligence_research"}, {"score": 0.0036850874921244693, "phrase": "leonard"}, {"score": 0.0036553070575015344, "phrase": "event_definitions"}, {"score": 0.0035819501337406596, "phrase": "force-dynamic_primitives"}, {"score": 0.0035386420095120706, "phrase": "robust_and_efficient_force-dynamic_inference"}, {"score": 0.0033029038236817372, "phrase": "process_state"}, {"score": 0.0029482691362918423, "phrase": "\"state-inference_function"}, {"score": 0.0029008064765926414, "phrase": "observation_sequences"}, {"score": 0.002877361619893651, "phrase": "underlying_hidden_states"}, {"score": 0.002751751956416832, "phrase": "efficient_sequential-inference_algorithm"}, {"score": 0.0025682999078906936, "phrase": "state-inference_function"}, {"score": 0.0024963543716522087, "phrase": "state-inference_functions"}, {"score": 0.0024264193359373977, "phrase": "corresponding_supervised_learning_algorithm"}, {"score": 0.0023776656246300063, "phrase": "force-dynamic_state_inference"}, {"score": 0.002320449502337321, "phrase": "significantly_improved_accuracy"}, {"score": 0.0023016843709107297, "phrase": "speed_relative"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["sequence learning", " relational learning", " event recognition", " temporal learning", " inductive logic programming"], "paper_abstract": "We present a trainable sequential-inference technique for processes with large state and observation spaces and relational structure. We apply our technique to the problem of force-dynamic state inference from video, which is a critical component of the LEONARD [J.M. Siskind, Grounding lexical semantics of verbs in visual perception using force dynamics and event logic, Journal of Artificial Intelligence Research 15 (2001) 31-90] visual-event recognition system. LEONARD uses event definitions that are grounded in force-dynamic primitives-making robust and efficient force-dynamic inference critical to good performance. Our sequential-inference method assumes \"reliable observations\", i.e., that each process state (e.g., force-dynamic state) persists long enough to be reliably inferred from the observations (e.g., video frames) it generates. We introduce the idea of a \"state-inference function\" (from observation sequences to underlying hidden states) for representing knowledge about a process and develop an efficient sequential-inference algorithm, utilizing this function, that is correct for processes that generate reliable observations consistent with the state-inference function. We describe a representation for state-inference functions in relational domains and give a corresponding supervised learning algorithm. Our experiments in force-dynamic state inference show that our technique provides significantly improved accuracy and speed relative to a variety of recent, hand-coded, non-trainable systems, and a trainable system based on probabilistic modeling. (C) 2006 Published by Elsevier B.V.", "paper_title": "Sequential inference with reliable observations: Learning to construct force-dynamic models", "paper_id": "WOS:000242004700001"}