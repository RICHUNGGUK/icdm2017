{"auto_keywords": [{"score": 0.04926889079413175, "phrase": "lattice_boltzmann_method"}, {"score": 0.045701947504313264, "phrase": "multi-node_gpu_cluster"}, {"score": 0.0048149540663384284, "phrase": "multi-gpu"}, {"score": 0.004765889452740394, "phrase": "incompressible_flow_computation"}, {"score": 0.004701247803858319, "phrase": "gpu_cluster"}, {"score": 0.004669276466938039, "phrase": "gpgpu"}, {"score": 0.00457457085958402, "phrase": "non-graphic_applications"}, {"score": 0.004331402003067145, "phrase": "cuda"}, {"score": 0.004287166497171827, "phrase": "mpi_library"}, {"score": 0.004243460284890507, "phrase": "gpu_code"}, {"score": 0.004143196873246147, "phrase": "tokyo_institute_of_technology"}, {"score": 0.004017745870068031, "phrase": "nvidia_tesla"}, {"score": 0.003949693102823493, "phrase": "multi-gpu_computation"}, {"score": 0.003922794571908569, "phrase": "domain_partitioning_method"}, {"score": 0.0038563438670516127, "phrase": "computational_load"}, {"score": 0.0038300786263543835, "phrase": "multiple_gpus"}, {"score": 0.003803991592917943, "phrase": "gpu-to-gpu_data_transfer"}, {"score": 0.0037267877774290374, "phrase": "total_performance"}, {"score": 0.003613899384402803, "phrase": "parallel_results"}, {"score": 0.0031845067058084583, "phrase": "performance_curve"}, {"score": 0.0031305230217636495, "phrase": "idealistic_line"}, {"score": 0.0030879936732269293, "phrase": "long_communicational_time"}, {"score": 0.002994396880146532, "phrase": "communication_time"}, {"score": 0.0029436264166293317, "phrase": "overlapping_technique"}, {"score": 0.002854392852477005, "phrase": "data_transfer_process"}, {"score": 0.0026565317818265394, "phrase": "overlapping_mode"}, {"score": 0.002620424858132324, "phrase": "benchmark_problem"}, {"score": 0.0025936662520897992, "phrase": "large-scaled_computation"}, {"score": 0.002447101794397243, "phrase": "mesh_system"}, {"score": 0.0022618732192713235, "phrase": "computational_time"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["GPU", " Lattice Boltzmann method", " Multi-node GPU cluster", " Parallel", " Data communication", " Domain partitioning", " Overlapping mode", " Large-scaled"], "paper_abstract": "GPGPU has drawn much attention on accelerating non-graphic applications. The simulation by D3Q19 model of the lattice Boltzmann method was executed successfully on multi-node GPU cluster by using CUDA programming and MPI library. The GPU code runs on the multi-node GPU cluster TSUBAME of Tokyo Institute of Technology, in which a total of 680 GPUs of NVIDIA Tesla are equipped. For multi-GPU computation, domain partitioning method is used to distribute computational load to multiple GPUs and GPU-to-GPU data transfer becomes severe overhead for the total performance. Comparison and analysis were made among the parallel results by 1D, 2D and 3D domain partitionings. As a result, with 384 x 384 x 384 mesh system and 96 GPUs, the performance by 3D partitioning is about 3-4 times higher than that by 1D partitioning. The performance curve is deviated from the idealistic line due to the long communicational time between GPUs. In order to hide the communication time, we introduced the overlapping technique between computation and communication, in which the data transfer process and computation were done in two streams simultaneously. Using 8-96 GPUs, the performances increase by a factor about 1.1-1.3 with a overlapping mode. As a benchmark problem, a large-scaled computation of a flow around a sphere at Re = 13,000 was carried on successfully using the mesh system 2000 x 1000 x 1000 and 100 GPUs. For such a computation with 2 Giga lattice nodes, 6.0 h were used for processing 100,000 time steps. Under this condition, the computational time (2.79 h) and the data communication time (3.06 h) are almost the same. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Multi-GPU performance of incompressible flow computation by lattice Boltzmann method on GPU cluster", "paper_id": "WOS:000295150400003"}