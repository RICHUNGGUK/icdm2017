{"auto_keywords": [{"score": 0.03467996204914694, "phrase": "dictionary_matrix"}, {"score": 0.02142679766233211, "phrase": "data_matrix"}, {"score": 0.010472909097295263, "phrase": "matrix"}, {"score": 0.008630279977691004, "phrase": "dictionary_columns"}, {"score": 0.008106737893597261, "phrase": "previously_proposed_greedy_algorithms"}, {"score": 0.006948435619232377, "phrase": "run_time"}, {"score": 0.00481495049065317, "phrase": "sparse_approximation"}, {"score": 0.0039685202936462815, "phrase": "small_subset"}, {"score": 0.00376293585033711, "phrase": "accurate_prediction"}, {"score": 0.0037087324991876727, "phrase": "entire_data_matrix"}, {"score": 0.0035679633126097115, "phrase": "data_column"}, {"score": 0.00320770578058916, "phrase": "recent_approach"}, {"score": 0.003115908199991072, "phrase": "large_amounts"}, {"score": 0.0030414140732789186, "phrase": "temporary_values"}, {"score": 0.00269475174341129, "phrase": "first_algorithm"}, {"score": 0.002530359917257626, "phrase": "significantly_less_memory"}, {"score": 0.002469830206270585, "phrase": "current_state-of-the-art"}, {"score": 0.002319127685733821, "phrase": "second_algorithm"}, {"score": 0.002285674008055581, "phrase": "low_rank_approximation"}, {"score": 0.0021461840368981475, "phrase": "new_recursive_formulas"}, {"score": 0.0021049977753042253, "phrase": "greedy_selection_criterion"}], "paper_keywords": [""], "paper_abstract": "We consider simultaneously approximating all the columns of a data matrix in terms of few selected columns of another matrix that is sometimes called \"the dictionary\". The challenge is to determine a small subset of the dictionary columns that can be used to obtain an accurate prediction of the entire data matrix. Previously proposed greedy algorithms for this task compare each data column with all dictionary columns, resulting in algorithms that may be too slow when both the data matrix and the dictionary matrix are large. A recent approach for accelerating the run time requires large amounts of memory to keep temporary values during the run of the algorithm. We propose two new algorithms that can be used even when both the data matrix and the dictionary matrix are large. The first algorithm is exact, with output identical to some previously proposed greedy algorithms. It takes significantly less memory when compared to the current state-of-the-art, and runs much faster when the dictionary matrix is sparse. The second algorithm uses a low rank approximation to the data matrix to further improve the run time. The algorithms use new recursive formulas for computing the greedy selection criterion. The formulas enable decoupling most of the computations related to the data matrix from the computations related to the dictionary matrix.", "paper_title": "Improved Greedy Algorithms for Sparse Approximation of a Matrix in Terms of Another Matrix", "paper_id": "WOS:000349621800014"}