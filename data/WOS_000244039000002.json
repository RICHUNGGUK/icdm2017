{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaussian_mixture"}, {"score": 0.01382566904120666, "phrase": "gaussians"}, {"score": 0.004621671570727774, "phrase": "automatic_model_selection"}, {"score": 0.004527947981009528, "phrase": "gaussian_mixture_modeling"}, {"score": 0.00417160036938041, "phrase": "mixture_model"}, {"score": 0.003949693102823493, "phrase": "regularization_theory"}, {"score": 0.0037140731069193896, "phrase": "model_selection_problem"}, {"score": 0.0036386875640595944, "phrase": "entropy_regularized_likelihood"}, {"score": 0.0034215569542785907, "phrase": "batch_gradient_learning_algorithm"}, {"score": 0.0032616462517547477, "phrase": "simulation_experiments"}, {"score": 0.003195414285736136, "phrase": "gradient_erl_learning_algorithm"}, {"score": 0.0031091858573262265, "phrase": "appropriate_number"}, {"score": 0.002903628734069691, "phrase": "sample_data"}, {"score": 0.002786858120570499, "phrase": "good_estimation"}, {"score": 0.0026747708876640377, "phrase": "actual_gaussian_mixture"}, {"score": 0.0024138345020219333, "phrase": "adaptive_gradient_implementation"}, {"score": 0.002364780955216792, "phrase": "erl"}, {"score": 0.0022696277881969896, "phrase": "theoretic_analysis"}, {"score": 0.0021634370496463793, "phrase": "generalized_competitive_learning"}, {"score": 0.0021049977753042253, "phrase": "erl_learning"}], "paper_keywords": ["competitive learning", " Gaussian mixture", " model selection", " regularization theory"], "paper_abstract": "In Gaussian mixture modeling, it is crucial to select the number of Gaussians or mixture model for a sample data set. Under regularization theory, we aim to solve this kind of model selection problem through implementing entropy regularized likelihood (ERL) learning on Gaussian mixture via a batch gradient learning algorithm. It is demonstrated by the simulation experiments that this gradient ERL learning algorithm can select an appropriate number of Gaussians automatically during the parameter learning on a sample data set and lead to a good estimation of the parameters in the actual Gaussian mixture, even in the cases of two or more actual Gaussians overlapped strongly. We further give an adaptive gradient implementation of the ERL learning on Gaussian mixture followed with theoretic analysis, and find a mechanism of generalized competitive learning implied in the ERL learning.", "paper_title": "Entropy regularized likelihood learning on Gaussian mixture: Two gradient implementations for automatic model selection", "paper_id": "WOS:000244039000002"}