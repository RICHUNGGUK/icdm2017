{"auto_keywords": [{"score": 0.050078515420922065, "phrase": "context-based_music_recommendation"}, {"score": 0.03789351823580061, "phrase": "estm"}, {"score": 0.0047379542803396915, "phrase": "rapidly_emerging_applications"}, {"score": 0.00464343008299973, "phrase": "ubiquitous_era"}, {"score": 0.0045876189888390895, "phrase": "multidisciplinary_efforts"}, {"score": 0.004550783060775205, "phrase": "low_level_feature_extraction"}, {"score": 0.00445997627516653, "phrase": "human_emotion_description"}, {"score": 0.004388631450145507, "phrase": "ontology-based_representation"}, {"score": 0.003935892934907777, "phrase": "context_awareness"}, {"score": 0.0038885527668812807, "phrase": "music_recommendation_field"}, {"score": 0.003780286517897356, "phrase": "novel_emotion_state_transition_model"}, {"score": 0.0036898803847106023, "phrase": "human_emotional_states"}, {"score": 0.003515479934694842, "phrase": "user_situation_information"}, {"score": 0.0034452601929727752, "phrase": "low-level_music_features"}, {"score": 0.003216804708106142, "phrase": "desired_emotional_state"}, {"score": 0.0030523478413779686, "phrase": "user's_musical_preferences"}, {"score": 0.002931569771634119, "phrase": "user's_desired_emotion"}, {"score": 0.002872979681637667, "phrase": "comus"}, {"score": 0.0028498718992429825, "phrase": "music-dedicated_ontology"}, {"score": 0.0027816546451972725, "phrase": "domain-specific_classes"}, {"score": 0.002628747284714371, "phrase": "musical_features"}, {"score": 0.0025658094607383646, "phrase": "low-level_features"}, {"score": 0.0024346026323359754, "phrase": "nmf"}, {"score": 0.002357131515622894, "phrase": "support_vector_machine"}, {"score": 0.0023099954987126, "phrase": "emotional_state_transition_classifier"}, {"score": 0.002263799931018574, "phrase": "prototype_music_recommendation_system"}, {"score": 0.0021049977753042253, "phrase": "experimental_results"}], "paper_keywords": ["Emotion state transition model", " Music information retrieval", " Mood", " Emotion", " Classification", " Recommendation"], "paper_abstract": "Context-based music recommendation is one of rapidly emerging applications in the advent of ubiquitous era and requires multidisciplinary efforts including low level feature extraction and music classification, human emotion description and prediction, ontology-based representation and recommendation, and the establishment of connections among them. In this paper, we contributed in three distinctive ways to take into account the idea of context awareness in the music recommendation field. Firstly, we propose a novel emotion state transition model (ESTM) to model human emotional states and their transitions by music. ESTM acts like a bridge between user situation information along with his/her emotion and low-level music features. With ESTM, we can recommend the most appropriate music to the user for transiting to the desired emotional state. Secondly, we present context-based music recommendation (COMUS) ontology for modeling user's musical preferences and context, and for supporting reasoning about the user's desired emotion and preferences. The COMUS is music-dedicated ontology in OWL constructed by incorporating domain-specific classes for music recommendation into the Music Ontology, which includes situation, mood, and musical features. Thirdly, for mapping low-level features to ESTM, we collected various high-dimensional music feature data and applied nonnegative matrix factorization (NMF) for their dimension reduction. We also used support vector machine (SVM) as emotional state transition classifier. We constructed a prototype music recommendation system based on these features and carried out various experiments to measure its performance. We report some of the experimental results.", "paper_title": "Music emotion classification and context-based music recommendation", "paper_id": "WOS:000275800200005"}