{"auto_keywords": [{"score": 0.04843603269514023, "phrase": "data_fusion"}, {"score": 0.04397371205917316, "phrase": "distributed_visual_sensor_network"}, {"score": 0.04252248047287167, "phrase": "inherent_redundancy"}, {"score": 0.04204898741255953, "phrase": "overlapped_field"}, {"score": 0.023698606691537907, "phrase": "sensor's_errors"}, {"score": 0.004817860389586586, "phrase": "multi"}, {"score": 0.0046859752697029355, "phrase": "bdi_model"}, {"score": 0.004613825044585923, "phrase": "visual_sensor_networks"}, {"score": 0.004560438999891081, "phrase": "newest_surveillance_applications"}, {"score": 0.004269329235934791, "phrase": "complex_tasks"}, {"score": 0.003874512990986722, "phrase": "multi-agent_architecture"}, {"score": 0.0035850198321277418, "phrase": "raw_images"}, {"score": 0.003488874535154583, "phrase": "visual_network"}, {"score": 0.0034618794528555063, "phrase": "local_signal_processing"}, {"score": 0.003304224820203298, "phrase": "calibration_phase"}, {"score": 0.00326594229173048, "phrase": "proposed_architecture"}, {"score": 0.003141498438356646, "phrase": "proposed_multi-agent_architecture"}, {"score": 0.003021781956684419, "phrase": "estimated_positions"}, {"score": 0.0029867619862229853, "phrase": "ground_plane"}, {"score": 0.002963640433319524, "phrase": "different_agents"}, {"score": 0.002861782260525497, "phrase": "fusion_process"}, {"score": 0.0025766822940260963, "phrase": "distributed_network"}, {"score": 0.0024307237540684076, "phrase": "data_fusion_techniques"}, {"score": 0.00238392747425758, "phrase": "non_reliable_sensors"}, {"score": 0.0023654615987859402, "phrase": "experimental_results"}, {"score": 0.002310915790114425, "phrase": "designed_architecture"}, {"score": 0.0021049977753042253, "phrase": "visual_sensor_network"}], "paper_keywords": ["Distributed visual sensor networks", " Multi-agent Systems", " Data fusion"], "paper_abstract": "The newest surveillance applications is attempting more complex tasks such as the analysis of the behavior of individuals and crowds. These complex tasks may use a distributed visual sensor network in order to gain coverage and exploit the inherent redundancy of the overlapped field of views. This article, presents a Multi-agent architecture based on the Belief-Desire-Intention (BDI) model for processing the information and fusing the data in a distributed visual sensor network. Instead of exchanging raw images between the agents involved in the visual network, local signal processing is performed and only the key observed features are shared. After a registration or calibration phase, the proposed architecture performs tracking, data fusion and coordination. Using the proposed Multi-agent architecture, we focus on the means of fusing the estimated positions on the ground plane from different agents which are applied to the same object. This fusion process is used for two different purposes: (1) to obtain a continuity in the tracking along the field of view of the cameras involved in the distributed network, (2) to improve the quality of the tracking by means of data fusion techniques, and by discarding non reliable sensors. Experimental results on two different scenarios show that the designed architecture can successfully track an object even when occlusions or sensor's errors take place. The sensor's errors are reduced by exploiting the inherent redundancy of a visual sensor network with overlapped field of views.", "paper_title": "A Multi-agent Architecture Based on the BDI Model for Data Fusion in Visual Sensor Networks", "paper_id": "WOS:000290277600001"}