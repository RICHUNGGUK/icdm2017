{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cumulative_hessian_directions"}, {"score": 0.04780597585803098, "phrase": "predictors_dimension"}, {"score": 0.003964007795174772, "phrase": "sufficient_dimension_reduction_method"}, {"score": 0.0033271045779740683, "phrase": "tuning_parameters"}, {"score": 0.0030778231027679434, "phrase": "slicing_estimation"}, {"score": 0.0027383842769094354, "phrase": "asymptotic_properties"}, {"score": 0.0021049977753042253, "phrase": "existing_methods"}], "paper_keywords": ["Central subspace", " Diverging parameters", " Inverse regression", " Sufficient dimension reduction"], "paper_abstract": "To reduce the predictors dimension without loss of information on the regression, we develop in this paper a sufficient dimension reduction method which we term cumulative Hessian directions. Unlike many other existing sufficient dimension reduction methods, the estimation of our proposal avoids completely selecting the tuning parameters such as the number of slices in slicing estimation or the bandwidth in kernel smoothing. We also investigate the asymptotic properties of our proposal when the predictors dimension diverges. Illustrations through simulations and an application are presented to evidence the efficacy of our proposal and to compare it with existing methods.", "paper_title": "Sufficient dimension reduction in regressions through cumulative Hessian directions", "paper_id": "WOS:000291390600003"}