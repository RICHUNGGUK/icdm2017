{"auto_keywords": [{"score": 0.04920822493417556, "phrase": "dirac"}, {"score": 0.00481495049065317, "phrase": "system_behavior"}, {"score": 0.004762202897268383, "phrase": "lhc_production_grid"}, {"score": 0.004641351004344458, "phrase": "remote_agent_control"}, {"score": 0.0045736684402340275, "phrase": "grid_solution"}, {"score": 0.004506968388996092, "phrase": "production_activities"}, {"score": 0.004360412703519539, "phrase": "large_hadron_collider_\"beauty\"_experiment"}, {"score": 0.004265354774450696, "phrase": "distributed_services"}, {"score": 0.004187718090913465, "phrase": "light-weight_agents"}, {"score": 0.004096409451697279, "phrase": "grid_resources"}, {"score": 0.003977741661623133, "phrase": "running_jobs"}, {"score": 0.0038909934410379775, "phrase": "specific_goals"}, {"score": 0.0038342108263454628, "phrase": "database_back-ends"}, {"score": 0.0037921662574137535, "phrase": "dynamic_state_information"}, {"score": 0.0036285315458900284, "phrase": "data_transfer"}, {"score": 0.003510439768484619, "phrase": "service_states"}, {"score": 0.003297728079517964, "phrase": "main_source"}, {"score": 0.0031207372860045427, "phrase": "services'_databases"}, {"score": 0.003086492169113408, "phrase": "shared_memory"}, {"score": 0.0030414140732789186, "phrase": "state_transitions"}, {"score": 0.002878140062594849, "phrase": "inconsistent_states"}, {"score": 0.002763988044737562, "phrase": "inherent_parallelism"}, {"score": 0.002733646947011804, "phrase": "distributed_components"}, {"score": 0.0023164335087514252, "phrase": "race_conditions"}, {"score": 0.0022327141073440436, "phrase": "real_system"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Model checking", " Process algebra", " Grid", " LHC", " Distributed system", " Workflow"], "paper_abstract": "DIRAC (Distributed Infrastructure with Remote Agent Control) is the grid solution designed to support production activities as well as user data analysis for the Large Hadron Collider \"beauty\" experiment. It consists of cooperating distributed services and a plethora of light-weight agents delivering the workload to the grid resources. Services accept requests from agents and running jobs, while agents actively fulfill specific goals. Services maintain database back-ends to store dynamic state information of entities such as jobs, queues, or requests for data transfer. Agents continuously check for changes in the service states and react to these accordingly. The logic of each agent is rather simple; the main source of complexity lies in their cooperation. These agents run concurrently and communicate using the services' databases as a shared memory for synchronizing the state transitions. Despite the effort invested in making DIRAC reliable, entities occasionally get into inconsistent states. Tracing and fixing such behaviors is difficult, given the inherent parallelism among the distributed components and the size of the implementation. In this paper we present an analysis of DIRAC with mCRL2, process algebra with data. We have reverse engineered two critical and related DIRAC subsystems, and subsequently modeled their behavior with the mCRL2 toolset. This enabled us to easily locate race conditions and livelocks which were confirmed to occur in the real system. We further formalized and verified several behavioral properties of the two modeled subsystems. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Using model checking to analyze the system behavior of the LHC production grid", "paper_id": "WOS:000326613400032"}