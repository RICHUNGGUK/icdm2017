{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "strategy_selection"}, {"score": 0.004649302313891199, "phrase": "multiple_pursuer-evader_games"}, {"score": 0.0042446888127531945, "phrase": "markov_game"}, {"score": 0.0038751504050385183, "phrase": "decentralized_unit"}, {"score": 0.0034640450599342488, "phrase": "distributed_multiagent_decision_problem"}, {"score": 0.0031182092708983184, "phrase": "central_coordination"}, {"score": 0.002826562521633885, "phrase": "learning_automaton"}, {"score": 0.002598333022293788, "phrase": "difficult_problem"}, {"score": 0.0023223663501882917, "phrase": "proposed_learning_process"}, {"score": 0.0022739766507258105, "phrase": "players'_policies"}, {"score": 0.002195552442011313, "phrase": "equilibrium_point"}, {"score": 0.0021049977753042253, "phrase": "multiple_pursuers"}], "paper_keywords": ["Learning", " pursuer-evader games", " intelligent systems", " reinforcement learning", " learning automata"], "paper_abstract": "The multiple pursuers and evaders game may be represented as a Markov game. Using this modeling, one may interpret each player as a decentralized unit that has to work independently in order to complete a task. This is a distributed multiagent decision problem and several different possible solutions have already been proposed. However, most solutions require some sort of central coordination. In this paper, we intend to model each player as a learning automaton and let them evolve and adapt in order to solve the difficult problem they have at hand. We are also going to show that, using the proposed learning process, the players' policies will converge to an equilibrium point. Simulations of such scenarios with multiple pursuers and evaders are presented in order to show the feasibility of the approach.", "paper_title": "Decentralized strategy selection with learning automata for multiple pursuer-evader games", "paper_id": "WOS:000342977600001"}