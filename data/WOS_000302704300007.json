{"auto_keywords": [{"score": 0.03234313591536326, "phrase": "rkhs"}, {"score": 0.00481495049065317, "phrase": "kernel_hilbert_spaces"}, {"score": 0.004616034337089482, "phrase": "wide_framework"}, {"score": 0.0044408896450445125, "phrase": "multiregression_tasks"}, {"score": 0.004317682099870507, "phrase": "general_infinite-dimensional_reproducing_kernel_hilbert_space"}, {"score": 0.004168448339217754, "phrase": "fairly_large_number"}, {"score": 0.004139223748460922, "phrase": "nonlinear_multiregression_models"}, {"score": 0.0040957690050613185, "phrase": "special_cases"}, {"score": 0.0040385352882334235, "phrase": "linear_case"}, {"score": 0.0038444185258456245, "phrase": "loss_function"}, {"score": 0.0036467403626789666, "phrase": "desired_response"}, {"score": 0.0035330290755534234, "phrase": "adopted_loss_function"}, {"score": 0.003459191419478298, "phrase": "analytic_form"}, {"score": 0.0032812562034088646, "phrase": "robust_loss_functions"}, {"score": 0.003223995503078124, "phrase": "multiregression_task"}, {"score": 0.003079736435077217, "phrase": "online_schemes"}, {"score": 0.0029731625157696845, "phrase": "iteration_step"}, {"score": 0.002890557197908352, "phrase": "simple_sparsification_strategy"}, {"score": 0.002810240488642749, "phrase": "algorithmic_scheme"}, {"score": 0.002790511479398411, "phrase": "linear_complexity"}, {"score": 0.0027225413530200505, "phrase": "unknown_parameters"}, {"score": 0.002693919110230191, "phrase": "convergence_analysis"}, {"score": 0.002609840513713399, "phrase": "convex_analysis"}, {"score": 0.002501793273494325, "phrase": "proposed_method"}, {"score": 0.00243225236680506, "phrase": "multiaccess_multiple-input_multiple-output_channel_equalization_task"}, {"score": 0.0023646398663605493, "phrase": "nonavailable_channel_information"}, {"score": 0.0023480317852018133, "phrase": "numerical_results"}, {"score": 0.0022192891418394514, "phrase": "state-of-the-art_linear_techniques"}, {"score": 0.002157583854640276, "phrase": "space-time_coding"}, {"score": 0.0021049977753042253, "phrase": "full_channel_information"}], "paper_keywords": ["Adaptive kernel learning", " convex analysis", " multiple-input multiple-output channel equalization", " projection", " regression", " subgradient"], "paper_abstract": "This paper introduces a wide framework for online, i.e., time-adaptive, supervised multiregression tasks. The problem is formulated in a general infinite-dimensional reproducing kernel Hilbert space (RKHS). In this context, a fairly large number of nonlinear multiregression models fall as special cases, including the linear case. Any convex, continuous, and not necessarily differentiable function can be used as a loss function in order to quantify the disagreement between the output of the system and the desired response. The only requirement is the subgradient of the adopted loss function to be available in an analytic form. To this end, we demonstrate a way to calculate the subgradients of robust loss functions, suitable for the multiregression task. As it is by now well documented, when dealing with online schemes in RKHS, the memory keeps increasing with each iteration step. To attack this problem, a simple sparsification strategy is utilized, which leads to an algorithmic scheme of linear complexity with respect to the number of unknown parameters. A convergence analysis of the technique, based on arguments of convex analysis, is also provided. To demonstrate the capacity of the proposed method, the multiregressor is applied to the multiaccess multiple-input multiple-output channel equalization task for a setting with poor resources and nonavailable channel information. Numerical results verify the potential of the method, when its performance is compared with those of the state-of-the-art linear techniques, which, in contrast, use space-time coding, more antenna elements, as well as full channel information.", "paper_title": "Adaptive Multiregression in Reproducing Kernel Hilbert Spaces: The Multiaccess MIMO Channel Case", "paper_id": "WOS:000302704300007"}