{"auto_keywords": [{"score": 0.04762443660575816, "phrase": "radiological_images"}, {"score": 0.00481495049065317, "phrase": "operating_room"}, {"score": 0.004743164872278112, "phrase": "contextual_cues"}, {"score": 0.004020492138367616, "phrase": "sterile_hand_gesture_recognition_interface"}, {"score": 0.0039308449157320815, "phrase": "attentional_contextual_cues"}, {"score": 0.0038721890530859578, "phrase": "computer_vision_algorithms"}, {"score": 0.003673675864705682, "phrase": "attention_cues"}, {"score": 0.003591733916159209, "phrase": "surgeon's_behavior"}, {"score": 0.003459191419478298, "phrase": "sensory_data"}, {"score": 0.003382017342396959, "phrase": "commodity_depth_camera"}, {"score": 0.003306559295382181, "phrase": "developed_interface"}, {"score": 0.0031845067058084583, "phrase": "usability_experiment"}, {"score": 0.003021141590548526, "phrase": "new_interface"}, {"score": 0.002953711377604328, "phrase": "image_navigation_and_manipulation_task"}, {"score": 0.0028233196851239753, "phrase": "gesture_recognition_accuracy"}, {"score": 0.0027602925235474317, "phrase": "task_completion_times"}, {"score": 0.0026583500966092044, "phrase": "system_performance"}, {"score": 0.0026186324161493225, "phrase": "experimental_results"}, {"score": 0.002560162916190598, "phrase": "gesture_interaction"}, {"score": 0.0025219085140220773, "phrase": "surgeon_behavior_analysis"}, {"score": 0.0023214774334744713, "phrase": "mri_images"}, {"score": 0.0021049977753042253, "phrase": "mice-based_interfaces"}], "paper_keywords": [""], "paper_abstract": "This paper presents a method to improve the navigation and manipulation of radiological images through a sterile hand gesture recognition interface based on attentional contextual cues. Computer vision algorithms were developed to extract intention and attention cues from the surgeon's behavior and combine them with sensory data from a commodity depth camera. The developed interface was tested in a usability experiment to assess the effectiveness of the new interface. An image navigation and manipulation task was performed, and the gesture recognition accuracy, false positives and task completion times were computed to evaluate system performance. Experimental results show that gesture interaction and surgeon behavior analysis can be used to accurately navigate, manipulate and access MRI images, and therefore this modality could replace the use of keyboard and mice-based interfaces.", "paper_title": "Hand-gesture-based sterile interface for the operating room using contextual cues for the navigation of radiological images", "paper_id": "WOS:000319192100030"}