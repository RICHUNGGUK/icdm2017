{"auto_keywords": [{"score": 0.04924496525218245, "phrase": "video_concept"}, {"score": 0.039041475324239155, "phrase": "multi-graph_optimization"}, {"score": 0.00481495049065317, "phrase": "multi-modality_structure"}, {"score": 0.004772818068176176, "phrase": "cross_domain_adaptation"}, {"score": 0.004468304427278713, "phrase": "significant_attention"}, {"score": 0.00439041997682389, "phrase": "existing_video_adaptation_processes"}, {"score": 0.00416478405812793, "phrase": "unique_and_important_property"}, {"score": 0.0038647641000798135, "phrase": "novel_approach"}, {"score": 0.00352371416065881, "phrase": "multi-modality_knowledge"}, {"score": 0.003477505397536663, "phrase": "auxiliary_classifiers"}, {"score": 0.003170520716404193, "phrase": "target_domain"}, {"score": 0.0030206579994328975, "phrase": "first_time"}, {"score": 0.002981026404927824, "phrase": "multi-modality_transfer"}, {"score": 0.0029161225023394363, "phrase": "domain_adaptive_video_concept_detection"}, {"score": 0.002802825862847951, "phrase": "efficient_incremental_extension_scheme"}, {"score": 0.0027417911324611917, "phrase": "small_batch"}, {"score": 0.002717750004930131, "phrase": "new_emerging_data"}, {"score": 0.002646880721119838, "phrase": "multi-graph_scheme"}, {"score": 0.0026121403051734744, "phrase": "proposed_scheme"}, {"score": 0.0025665261770229757, "phrase": "comparable_accuracy"}, {"score": 0.0025217065652145443, "phrase": "brand-new_round_optimization"}, {"score": 0.0024776677004255104, "phrase": "new_data"}, {"score": 0.0024451427672857458, "phrase": "data_corpus"}, {"score": 0.0024130437638400404, "phrase": "nearest_round_optimization"}, {"score": 0.002288797099841088, "phrase": "extensive_experiments"}, {"score": 0.002258745901862197, "phrase": "data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multi-modality transfer", " Domain adaptive video annotation", " Multi-graph optimization", " Incremental extension"], "paper_abstract": "Domain adaptive video concept detection and annotation has recently received significant attention, but in existing video adaptation processes, all the features are treated as one modality, while multi-modalities, the unique and important property of video data, is typically ignored. To fill this blank, we propose a novel approach, named multi-modality transfer based on multi-graph optimization (MMT-MGO) in this paper, which leverages multi-modality knowledge generalized by auxiliary classifiers in the source domains to assist multi-graph optimization (a graph-based semi-supervised learning method) in the target domain for video concept annotation. To our best knowledge, it is the first time to introduce multi-modality transfer into the field of domain adaptive video concept detection and annotation. Moreover, we propose an efficient incremental extension scheme to sequentially estimate a small batch of new emerging data without modifying the structure of multi-graph scheme. The proposed scheme can achieve a comparable accuracy with that of brand-new round optimization which combines these new data with the data corpus for the nearest round optimization, while the time for estimation has been reduced greatly. Extensive experiments over TRECVID2005-2007 data sets demonstrate the effectiveness of both the multi-modality transfer scheme and the incremental extension scheme. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Exploring multi-modality structure for cross domain adaptation in video concept annotation", "paper_id": "WOS:000307152800003"}