{"auto_keywords": [{"score": 0.05006805598880613, "phrase": "high-dimensional_spaces"}, {"score": 0.03919507376438318, "phrase": "verification_systems"}, {"score": 0.03347279022910549, "phrase": "pca"}, {"score": 0.030455558932001706, "phrase": "high_dimensionality"}, {"score": 0.004622450915803531, "phrase": "data_sets"}, {"score": 0.0041316803676012155, "phrase": "second-order_statistics"}, {"score": 0.004068907083697369, "phrase": "high-dimensional_data"}, {"score": 0.003986684503518077, "phrase": "resulting_covariance_matrices"}, {"score": 0.003926105480202798, "phrase": "full_rank"}, {"score": 0.0036553070575015344, "phrase": "likelihood_ratio"}, {"score": 0.003581412258960711, "phrase": "ill-posed_problem"}, {"score": 0.0034911335714333507, "phrase": "singularity_problem"}, {"score": 0.003438058620195309, "phrase": "classical_solution"}, {"score": 0.0032336801561165113, "phrase": "lower_dimensional_subspace"}, {"score": 0.0032008144731234265, "phrase": "principle_component_analysis"}, {"score": 0.0029951555610569225, "phrase": "dimension-reduced_data"}, {"score": 0.0028026735035370206, "phrase": "sos_estimation"}, {"score": 0.0025046245057922557, "phrase": "sole_source"}, {"score": 0.002441422510910065, "phrase": "moderate_dimensionality"}, {"score": 0.0023197517832705297, "phrase": "euclidean_distances"}, {"score": 0.0021375377706211686, "phrase": "new_method"}, {"score": 0.0021049977753042253, "phrase": "fixed-point_eigenwise_correction"}], "paper_keywords": ["High-dimensional verification", " eigenvalue bias correction", " variance correction", " euclidean distance", " principle component analysis", " Marcenko Pastur equation", " eigenwise correction", " fixed-point eigenvalue correction"], "paper_abstract": "The increase of the dimensionality of data sets often leads to problems during estimation, which are denoted as the curse of dimensionality. One of the problems of second-order statistics (SOS) estimation in high-dimensional data is that the resulting covariance matrices are not full rank, so their inversion, for example, needed in verification systems based on the likelihood ratio, is an ill-posed problem, known as the singularity problem. A classical solution to this problem is the projection of the data onto a lower dimensional subspace using principle component analysis (PCA) and it is assumed that any further estimation on this dimension-reduced data is free from the effects of the high dimensionality. Using theory on SOS estimation in high-dimensional spaces, we show that the solution with PCA is far from optimal in verification systems if the high dimensionality is the sole source of error. For moderate dimensionality, it is already outperformed by solutions based on euclidean distances and it breaks down completely if the dimensionality becomes very high. We propose a new method, the fixed-point eigenwise correction, which does not have these disadvantages and performs close to optimal.", "paper_title": "Likelihood-Ratio-Based Verification in High-Dimensional Spaces", "paper_id": "WOS:000327965100011"}