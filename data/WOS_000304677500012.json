{"auto_keywords": [{"score": 0.04336705121455173, "phrase": "np"}, {"score": 0.00481495049065317, "phrase": "finely_grained_component-conditional_class_labeling"}, {"score": 0.0044863018460183784, "phrase": "semisupervised_mixtures"}, {"score": 0.004407690586271672, "phrase": "label_extrapolation"}, {"score": 0.0040347380310668994, "phrase": "accurate_classification"}, {"score": 0.003946519058822334, "phrase": "labeled_samples"}, {"score": 0.00353342812142574, "phrase": "standard_finite_mixture"}, {"score": 0.0034714553759044664, "phrase": "class_labels"}, {"score": 0.0032342101452939977, "phrase": "underlying_markov_random_field"}, {"score": 0.003163439005885435, "phrase": "mixture_component"}, {"score": 0.003066945490483524, "phrase": "pseudo-likelihood_formulation"}, {"score": 0.0029602552900111433, "phrase": "approximate_generalized_expectation-maximization_model_learning_algorithm"}, {"score": 0.002807118943000936, "phrase": "nn-based_model"}, {"score": 0.0026736945502820303, "phrase": "within-component_class_proportions"}, {"score": 0.0026036031415940563, "phrase": "feature_space_region"}, {"score": 0.0023935128386292966, "phrase": "uc_irvine_data_sets"}, {"score": 0.002351485871160907, "phrase": "significant_gains"}, {"score": 0.0023307492812556204, "phrase": "classification_accuracy"}, {"score": 0.0023101951336207955, "phrase": "previous_semisupervised_mixtures"}, {"score": 0.0022396694076443446, "phrase": "knn_classification"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_kernel_support_vector_machines"}], "paper_keywords": [""], "paper_abstract": "We introduce new inductive, generative semisupervised mixtures with more finely grained class label generation mechanisms than in previous work. Our models combine advantages of semisupervised mixtures, which achieve label extrapolation over a component, and nearest-neighbor (NN)/nearest-prototype (NP) classification, which achieve accurate classification in the vicinity of labeled samples or prototypes. For our NN-based method, we propose a novel two-stage stochastic data generation, with all samples first generated using a standard finite mixture and then all class labels generated, conditioned on the samples and their components of origin. This mechanism entails an underlying Markov random field, specific to each mixture component or cluster. We invoke the pseudo-likelihood formulation, which forms the basis for an approximate generalized expectation-maximization model learning algorithm. Our NP-based model overcomes a problem with the NN-based model that manifests at very low labeled fractions. Both models are advantageous when within-component class proportions are not constant over the feature space region \"owned by\" a component. The practicality of this scenario is borne out by experiments on UC Irvine data sets, which demonstrate significant gains in classification accuracy over previous semisupervised mixtures and also overall gains, over KNN classification. Moreover, for very small labeled fractions, our methods overall outperform supervised linear and nonlinear kernel support vector machines.", "paper_title": "Improved Generative Semisupervised Learning Based on Finely Grained Component-Conditional Class Labeling", "paper_id": "WOS:000304677500012"}