{"auto_keywords": [{"score": 0.04973174391930041, "phrase": "mapreduce"}, {"score": 0.03055922274508438, "phrase": "spark"}, {"score": 0.008191777907490622, "phrase": "performance_differences"}, {"score": 0.004814950919730487, "phrase": "titans"}, {"score": 0.004710944813540894, "phrase": "large_scale_data_analytics"}, {"score": 0.004396120480895021, "phrase": "task_parallelism"}, {"score": 0.004285489630701221, "phrase": "simple_programming_api"}, {"score": 0.004117215958803593, "phrase": "major_architectural_components"}, {"score": 0.003969957613513855, "phrase": "execution_model"}, {"score": 0.0038279459368975965, "phrase": "important_analytic_workloads"}, {"score": 0.0037588493723080757, "phrase": "detailed_analysis"}, {"score": 0.0035980453088879424, "phrase": "task_execution_plan"}, {"score": 0.0035589270056254605, "phrase": "resource_utilization"}, {"score": 0.003284718806701356, "phrase": "task_execution_time"}, {"score": 0.003260860308644701, "phrase": "in-depth_analysis"}, {"score": 0.0032253960472377356, "phrase": "detailed_experiments"}, {"score": 0.0030205408413942272, "phrase": "different_components"}, {"score": 0.0027876981671591202, "phrase": "micro-benchmark_experiments"}, {"score": 0.002582159550634798, "phrase": "word_count"}, {"score": 0.0025355069315351375, "phrase": "pagerank"}, {"score": 0.002489667521500195, "phrase": "main_causes"}, {"score": 0.0024092539025697014, "phrase": "hash-based_aggregation_component"}, {"score": 0.0023485057246680854, "phrase": "reduced_cpu"}, {"score": 0.0023314314877152494, "phrase": "disk_overheads"}, {"score": 0.002306052819427683, "phrase": "rdd"}, {"score": 0.002215329893200788, "phrase": "sort_workload"}, {"score": 0.0021049977753042253, "phrase": "mapreduce's_execution_model"}], "paper_keywords": [""], "paper_abstract": "MapReduce and Spark are two very popular open source cluster computing frameworks for large scale data analytics. These frameworks hide the complexity of task parallelism and fault-tolerance, by exposing a simple programming API to users. In this paper, we evaluate the major architectural components in MapReduce and Spark frameworks including: shuffle, execution model, and caching, by using a set of important analytic workloads. To conduct a detailed analysis, we developed two profiling tools: (1) We correlate the task execution plan with the resource utilization for both MapReduce and Spark, and visually present this correlation; (2) We provide a break-down of the task execution time for in-depth analysis. Through detailed experiments, we quantify the performance differences between MapReduce and Spark. Furthermore, we attribute these performance differences to different components which are architected differently in the two frameworks. We further expose the source of these performance differences by using a set of micro-benchmark experiments. Overall, our experiments show that Spark is about 2.5x, 5x, and 5x faster than MapReduce, for Word Count, k-means, and PageRank, respectively. The main causes of these speedups are the efficiency of the hash-based aggregation component for combine, as well as reduced CPU and disk overheads due to RDD caching in Spark. An exception to this is the Sort workload, for which MapReduce is 2x faster than Spark. We show that MapReduce's execution model is more efficient for shuffling data than Spark, thus making Sort run faster on MapReduce.", "paper_title": "Clash of the Titans: MapReduce vs. Spark for Large Scale Data Analytics", "paper_id": "WOS:000386424900005"}