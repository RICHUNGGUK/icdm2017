{"auto_keywords": [{"score": 0.04538980158905762, "phrase": "face_region"}, {"score": 0.00481495049065317, "phrase": "visual_speech"}, {"score": 0.004652845998457721, "phrase": "automatic_multimodal_person_authentication"}, {"score": 0.0045348429403481464, "phrase": "visual_speech_modalities"}, {"score": 0.004476963405985556, "phrase": "proposed_method"}, {"score": 0.004419819325202464, "phrase": "motion_information"}, {"score": 0.0041984140090404985, "phrase": "ycrcb_color_space"}, {"score": 0.003971006689166329, "phrase": "nonlip_region"}, {"score": 0.0038702288460744274, "phrase": "gaussian_distribution"}, {"score": 0.0036448787596988423, "phrase": "facial_and_visual_speech_features"}, {"score": 0.003582928547553209, "phrase": "multiscale_morphological_erosion_and_dilation_operations"}, {"score": 0.00350696416714684, "phrase": "facial_features"}, {"score": 0.003331134183901191, "phrase": "visual_speech_features"}, {"score": 0.0031505544969981096, "phrase": "acoustic_features"}, {"score": 0.0030837288356760973, "phrase": "speech_signal"}, {"score": 0.0030054007508185858, "phrase": "weighted_linear_prediction"}, {"score": 0.002954287204671032, "phrase": "wlpcc"}, {"score": 0.002866916212443467, "phrase": "aann"}, {"score": 0.0026312238938540787, "phrase": "visual_speech_models"}, {"score": 0.002575385171755899, "phrase": "weighting_rule"}, {"score": 0.0024461474613359994, "phrase": "identity_claim"}, {"score": 0.0022936610196951962, "phrase": "tv_broadcast_news_data"}, {"score": 0.0022257833099336858, "phrase": "equal_error_rate"}, {"score": 0.002206764386713735, "phrase": "eer"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["multimodal person authentication", " face tracking", " eye location", " visual speech", " multiscale morphological dilation and erosion", " autoassociative neural network"], "paper_abstract": "This paper presents a method for automatic multimodal person authentication using speech, face and visual speech modalities. The proposed method uses the motion information to localize the face region, and the face region is processed in YCrCb color space to determine the locations of the eyes. The system models the nonlip region of the face using a Gaussian distribution, and it is used to estimate the center of the mouth. Facial and visual speech features are extracted using multiscale morphological erosion and dilation operations, respectively. The facial features are extracted relative to the locations of the eyes, and visual speech features are extracted relative to the locations of the eyes and mouth. Acoustic features are derived from the speech signal, and are represented by weighted linear prediction cepstral coefficients (WLPCC). Autoassociative neural network (AANN) models are used to capture the distribution of the extracted acoustic, facial and visual speech features. The evidence from speech, face and visual speech models are combined using a weighting rule, and the result is used to accept or reject the identity claim of the subject. The performance of the system is evaluated for newsreaders in TV broadcast news data, and the system achieves an equal error rate (EER) of about 0.45% for 50 subjects. (c) 2007 Elsevier Inc. All rights reserved.", "paper_title": "Multimodal person authentication using speech, face and visual speech", "paper_id": "WOS:000252518900003"}