{"auto_keywords": [{"score": 0.05007682588877103, "phrase": "cloud_computing"}, {"score": 0.0489853873227799, "phrase": "mapreduce"}, {"score": 0.013316295190174236, "phrase": "big_data"}, {"score": 0.004732215871738028, "phrase": "computing_environment"}, {"score": 0.004466519142879166, "phrase": "data_mining_and_business_intelligence"}, {"score": 0.004427966503146096, "phrase": "new_scenario"}, {"score": 0.004191377834552202, "phrase": "standard_computing_resources"}, {"score": 0.004013524489681638, "phrase": "large_volumes"}, {"score": 0.0038099959232779194, "phrase": "acceptable_elapsed_time"}, {"score": 0.003766182463933667, "phrase": "scalability_term"}, {"score": 0.0037228709592978556, "phrase": "usually_traditional_parallel-type_solutions"}, {"score": 0.0036063061558261546, "phrase": "high_performance"}, {"score": 0.003575151757072538, "phrase": "database_management_systems"}, {"score": 0.003513645211548667, "phrase": "new_paradigm"}, {"score": 0.0031845067058084583, "phrase": "computing_resources"}, {"score": 0.003120657021758196, "phrase": "flexible_costs"}, {"score": 0.002953711377604328, "phrase": "current_problem"}, {"score": 0.0027795345823932406, "phrase": "large-scale_analytics"}, {"score": 0.0027475383418803724, "phrase": "mapreduce_scheme"}, {"score": 0.002731698353001784, "phrase": "hadoop"}, {"score": 0.0026384167183586015, "phrase": "software_projects"}, {"score": 0.0025557195732514915, "phrase": "new_programming_model"}, {"score": 0.0024400265512135397, "phrase": "classical_solutions"}, {"score": 0.0023566987138026285, "phrase": "programming_frameworks"}, {"score": 0.0021794454466797382, "phrase": "wires_data_mining_knowl"}], "paper_keywords": [""], "paper_abstract": "The term Big Data' has spread rapidly in the framework of Data Mining and Business Intelligence. This new scenario can be defined by means of those problems that cannot be effectively or efficiently addressed using the standard computing resources that we currently have. We must emphasize that Big Data does not just imply large volumes of data but also the necessity for scalability, i.e., to ensure a response in an acceptable elapsed time. When the scalability term is considered, usually traditional parallel-type solutions are contemplated, such as the Message Passing Interface or high performance and distributed Database Management Systems. Nowadays there is a new paradigm that has gained popularity over the latter due to the number of benefits it offers. This model is Cloud Computing, and among its main features we has to stress its elasticity in the use of computing resources and space, less management effort, and flexible costs. In this article, we provide an overview on the topic of Big Data, and how the current problem can be addressed from the perspective of Cloud Computing and its programming frameworks. In particular, we focus on those systems for large-scale analytics based on the MapReduce scheme and Hadoop, its open-source implementation. We identify several libraries and software projects that have been developed for aiding practitioners to address this new programming model. We also analyze the advantages and disadvantages of MapReduce, in contrast to the classical solutions in this field. Finally, we present a number of programming frameworks that have been proposed as an alternative to MapReduce, developed under the premise of solving the shortcomings of this model in certain scenarios and platforms. WIREs Data Mining Knowl Discov 2014, 4:380-409. doi: 10.1002/widm.1134 For further resources related to this article, please visit the . Conflict of interest: The authors have declared no conflicts of interest for this article.", "paper_title": "Big Data with Cloud Computing: an insight on the computing environment, MapReduce, and programming frameworks", "paper_id": "WOS:000344523100003"}