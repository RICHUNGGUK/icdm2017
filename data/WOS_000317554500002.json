{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "multiscale_spatiotemporal_energy_features"}, {"score": 0.0047459060595229224, "phrase": "dynamic_objects"}, {"score": 0.0047049532961375, "phrase": "high_level"}, {"score": 0.0046240999614456605, "phrase": "scene_understanding"}, {"score": 0.004597457370802855, "phrase": "behavioural_analysis"}, {"score": 0.004544629724830453, "phrase": "attentive_selective_processes"}, {"score": 0.0044536312768349, "phrase": "pre-attentive_operations"}, {"score": 0.004377077848734575, "phrase": "visual_input"}, {"score": 0.004301834610818624, "phrase": "meaningful_\"chunks"}, {"score": 0.004155190157284551, "phrase": "moving_objects"}, {"score": 0.004119313626110943, "phrase": "crucial_step"}, {"score": 0.004060203647513207, "phrase": "dynamic_scenes"}, {"score": 0.003978866233891427, "phrase": "powerful_cue"}, {"score": 0.0037336520904376687, "phrase": "coarse_level"}, {"score": 0.0036694287631595995, "phrase": "coherent_motion"}, {"score": 0.0036167509502276294, "phrase": "biological_motion"}, {"score": 0.003513645211548667, "phrase": "higher_level"}, {"score": 0.003268516165008941, "phrase": "qualitative_segmentation"}, {"score": 0.003066945490483524, "phrase": "discrimination_boundaries"}, {"score": 0.003022890602492506, "phrase": "segmentation_phase"}, {"score": 0.0029708567739251254, "phrase": "automatic_and_efficient_way"}, {"score": 0.002945175711537758, "phrase": "support_vector_machine_classifier"}, {"score": 0.0029197159945990015, "phrase": "multi-class_implementation"}, {"score": 0.0029028648824177715, "phrase": "motion-related_features"}, {"score": 0.002852891176791431, "phrase": "human_fixations"}, {"score": 0.00282822695516646, "phrase": "extensive_dataset"}, {"score": 0.0026460656993236354, "phrase": "feature_extraction_step"}, {"score": 0.0025631294247072476, "phrase": "object_files"}, {"score": 0.0024899850926034567, "phrase": "pooled_motion_energy"}, {"score": 0.0024400265512135397, "phrase": "meaningful_moving_objects"}, {"score": 0.0023295585212850154, "phrase": "object_basis"}, {"score": 0.002289431798120276, "phrase": "pixel_basis"}, {"score": 0.002198464289683489, "phrase": "selective_attention"}, {"score": 0.002141898874982106, "phrase": "scene_interpretation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Video segmentation", " Spatiotemporal features", " Visual attention", " Object-based saliency"], "paper_abstract": "High level visual cognitive abilities such as scene understanding and behavioural analysis are modulated by attentive selective processes. These in turn rely on pre-attentive operations delivering perceptual organisation of the visual input and enabling the extraction of meaningful \"chunks\" of information. Specifically, the extraction and prioritisation of moving objects is a crucial step in the processing of dynamic scenes. Motion is of course a powerful cue for grouping regions and segregating objects but not all kinds of motion are equally meaningful and should be equally attended. On a coarse level, most interesting moving objects are associated with coherent motion, reflecting our sensitivity to biological motion. On the other hand, attention operates on a higher level, prioritising what moves differently with respect to both its surrounding and the global scene. In this paper, we propose how a qualitative segmentation of multiscale spatiotemporal energy features according to their frequency spectrum distribution can be used to pre-attentively extract regions of interest. We also show that discrimination boundaries between classes in the segmentation phase can be learned in an automatic and efficient way by a Support Vector Machine classifier in a multi-class implementation. Motion-related features are shown to best predict human fixations on an extensive dataset. The model generalises well to datasets other than that used for training, if scale is taken into account in the feature extraction step. Regions labelled as coherently moving are clustered in moving object files, described by the magnitude and phase of the pooled motion energy. The method succeeds in extracting meaningful moving objects from the background and identifying other less interesting motion patterns. A saliency function is finally computed on an object basis, instead of on a pixel basis, as in most current approaches. The same features are thus used for segmentation and selective attention and can be further used for recognition and scene interpretation. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Classification of multiscale spatiotemporal energy features for video segmentation and dynamic objects prioritisation", "paper_id": "WOS:000317554500002"}