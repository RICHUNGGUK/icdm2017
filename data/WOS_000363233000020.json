{"auto_keywords": [{"score": 0.049763552914073306, "phrase": "preliminary_cognition"}, {"score": 0.014967362359141197, "phrase": "visual_cognition_process"}, {"score": 0.013973317909675721, "phrase": "primate_visual_cortex"}, {"score": 0.013044417804530457, "phrase": "hmax_model"}, {"score": 0.00481495049065317, "phrase": "biologically_inspired_visual_model"}, {"score": 0.004736045157669388, "phrase": "attention_adjustment"}, {"score": 0.004403874076980828, "phrase": "hierarchical_and_bottom-up_structure"}, {"score": 0.004303127794432684, "phrase": "ventral_pathway"}, {"score": 0.004163172790791119, "phrase": "scale-tolerant_recognition"}, {"score": 0.0036352070472201086, "phrase": "new_formation"}, {"score": 0.0035637545647574853, "phrase": "visual_processing"}, {"score": 0.0035402494307399733, "phrase": "different_circumstances"}, {"score": 0.003413705707789296, "phrase": "inferior_temporal_cortex"}, {"score": 0.0033025823350563087, "phrase": "main_contributions"}, {"score": 0.0031950646670695546, "phrase": "memory_and_association_part"}, {"score": 0.003153041030690117, "phrase": "deep_convolutional_neural_networks"}, {"score": 0.003050376875258722, "phrase": "different_features"}, {"score": 0.003030247482854539, "phrase": "object_recognition"}, {"score": 0.0029608318458226755, "phrase": "fast_and_robust_recognition"}, {"score": 0.002931569771634119, "phrase": "retrieval_and_association_process"}, {"score": 0.002845502417672334, "phrase": "separated_clusters"}, {"score": 0.0027346529452880585, "phrase": "loop_discharge_manner"}, {"score": 0.002663156413861478, "phrase": "active_adjustment_part"}, {"score": 0.002602127996667857, "phrase": "different_types"}, {"score": 0.0025678829712650437, "phrase": "distinct_neural_circuits"}, {"score": 0.0025424945305785374, "phrase": "human_brain"}, {"score": 0.0024434201175883674, "phrase": "active_cognition_adjustment"}, {"score": 0.00234043119560776, "phrase": "top-down_effect"}, {"score": 0.0023249757987087055, "phrase": "human_cognition_process"}, {"score": 0.0022417769544462386, "phrase": "ar."}, {"score": 0.002175913488493802, "phrase": "visual_recognition_process"}, {"score": 0.0021049977753042253, "phrase": "traditional_purely_computational_methods"}], "paper_keywords": ["Active attention adjustment", " association", " biologically inspired visual model", " memory", " object recognition"], "paper_abstract": "Recently, many computational models have been proposed to simulate visual cognition process. For example, the hierarchical Max-Pooling (HMAX) model was proposed according to the hierarchical and bottom-up structure of V1 to V4 in the ventral pathway of primate visual cortex, which could achieve position-and scale-tolerant recognition. In our previous work, we have introduced memory and association into the HMAX model to simulate visual cognition process. In this paper, we improve our theoretical framework by mimicking a more elaborate structure and function of the primate visual cortex. We will mainly focus on the new formation of memory and association in visual processing under different circumstances as well as preliminary cognition and active adjustment in the inferior temporal cortex, which are absent in the HMAX model. The main contributions of this paper are: 1) in the memory and association part, we apply deep convolutional neural networks to extract various episodic features of the objects since people use different features for object recognition. Moreover, to achieve a fast and robust recognition in the retrieval and association process, different types of features are stored in separated clusters and the feature binding of the same object is stimulated in a loop discharge manner and 2) in the preliminary cognition and active adjustment part, we introduce preliminary cognition to classify different types of objects since distinct neural circuits in a human brain are used for identification of various types of objects. Furthermore, active cognition adjustment of occlusion and orientation is implemented to the model to mimic the top-down effect in human cognition process. Finally, our model is evaluated on two face databases CAS-PEAL-R1 and AR. The results demonstrate that our model exhibits its efficiency on visual recognition process with much lower memory storage requirement and a better performance compared with the traditional purely computational methods.", "paper_title": "Biologically Inspired Visual Model With Preliminary Cognition and Active Attention Adjustment", "paper_id": "WOS:000363233000020"}