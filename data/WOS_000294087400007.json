{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "syntactic"}, {"score": 0.004704699244406775, "phrase": "information_retrieval"}, {"score": 0.003702702337768558, "phrase": "lexical_and_syntactic_knowledge"}, {"score": 0.003427246079774314, "phrase": "traditional_ir_similarity_measures"}, {"score": 0.0033228699494959172, "phrase": "dependency_information"}, {"score": 0.003075583447856341, "phrase": "discourse_structure"}, {"score": 0.002419708657850547, "phrase": "discourse_references"}, {"score": 0.0022394839933089074, "phrase": "spanish_and_english_corpora"}, {"score": 0.0021378442381693847, "phrase": "answering_tasks"}, {"score": 0.0021049977753042253, "phrase": "significant_increases"}], "paper_keywords": ["Information Retrieval", " Natural Language Processing", " Term Proximity", " Question Answering", " Lexical and syntactic relationships"], "paper_abstract": "Traditional Information Retrieval (IR) models assume that the index terms of queries and documents are statistically independent of each other, which is intuitively wrong. This paper proposes the incorporation of the lexical and syntactic knowledge generated by a POS-tagger and a syntactic Chunker into traditional IR similarity measures for including this dependency information between terms. Our proposal is based on theories of discourse structure by means of the segmentation of documents and queries into sentences and entities. Therefore, we measure dependencies between entities instead of between terms. Moreover, we handle discourse references for each entity. It has been evaluated on Spanish and English corpora as well as on Question Answering tasks obtaining significant increases. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Lexical and Syntactic knowledge for Information Retrieval", "paper_id": "WOS:000294087400007"}