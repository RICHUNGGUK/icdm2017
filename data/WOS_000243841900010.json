{"auto_keywords": [{"score": 0.04081578574791827, "phrase": "student-t_distribution"}, {"score": 0.033951251111328025, "phrase": "svensen"}, {"score": 0.00481495049065317, "phrase": "new_variational_bayesian_learning_algorithm"}, {"score": 0.0047784273577245505, "phrase": "student-t_mixture_models"}, {"score": 0.004582423717777699, "phrase": "robust_density_estimation"}, {"score": 0.004311527998148399, "phrase": "gaussian_mixture_models"}, {"score": 0.0041662189933067, "phrase": "divide-and-conquer_approach"}, {"score": 0.00405658135587089, "phrase": "density_estimation"}, {"score": 0.004025787455454387, "phrase": "clustering_tasks"}, {"score": 0.003802135020791801, "phrase": "gaussian_distribution"}, {"score": 0.00364603066037512, "phrase": "empirical_distribution"}, {"score": 0.0036187400242247273, "phrase": "gaussianity"}, {"score": 0.003456554957885269, "phrase": "robust_mixture_models"}, {"score": 0.0033399639721674954, "phrase": "bayesian_student-t_mixture_model"}, {"score": 0.0032396224508754387, "phrase": "different_way"}, {"score": 0.003190615553174836, "phrase": "bishop"}, {"score": 0.0028783840417661694, "phrase": "main_difference"}, {"score": 0.002739121808942775, "phrase": "factorized_approximation"}, {"score": 0.0027079507175554857, "phrase": "posterior_distribution"}, {"score": 0.0026771333996253783, "phrase": "latent_indicator_variables"}, {"score": 0.002646665861584495, "phrase": "latent_scale_variables"}, {"score": 0.0025867643733794724, "phrase": "tractable_solution"}, {"score": 0.0025089940342989227, "phrase": "unobserved_random_variables"}, {"score": 0.002470987929071079, "phrase": "bayesian_model"}, {"score": 0.0024428607154967806, "phrase": "increased_robustness"}, {"score": 0.0022375675189464715, "phrase": "model_complexity"}, {"score": 0.0021049977753042253, "phrase": "higher_confidence"}], "paper_keywords": ["Bayesian learning", " graphical models", " approximate inference", " variational inference", " mixture models", " density estimation", " clustering", " model selection", " student-t distribution", " robustness to outliers"], "paper_abstract": "A new variational Bayesian learning algorithm for Student-t mixture models is introduced. This algorithm leads to (i) robust density estimation, (ii) robust clustering and (iii) robust automatic model selection. Gaussian mixture models are learning machines which are based on a divide-and-conquer approach. They are commonly used for density estimation and clustering tasks, but are sensitive to outliers. The Student-t distribution has heavier tails than the Gaussian distribution and is therefore less sensitive to any departure of the empirical distribution from Gaussianity. As a consequence. the Student-t distribution is suitable for constructing robust mixture models. In this work, we formalize the Bayesian Student-t mixture model as a latent variable model in a different way from Svensen and Bishop [Svensen, M., & Bishop, C. M. (2005). Robust Bayesian mixture modelling. Neurocomputing, 64, 235-252]. The main difference resides in the fact that it is not necessary to assume a factorized approximation of the posterior distribution on the latent indicator variables and the latent scale variables in order to obtain a tractable solution. Not neglecting the correlations between these unobserved random variables leads to a Bayesian model having an increased robustness. Furthermore, it is expected that the lower bound on the log-evidence is tighter. Based on this bound, the model complexity, i.e. the number of components in the mixture. can be inferred with a higher confidence. (c) 2006 Elsevier Ltd. All rights reserved.", "paper_title": "Robust Bayesian clustering", "paper_id": "WOS:000243841900010"}