{"auto_keywords": [{"score": 0.04697668243769894, "phrase": "auc"}, {"score": 0.006904092838662299, "phrase": "roc_curve"}, {"score": 0.00481495049065317, "phrase": "classifier_performance"}, {"score": 0.004758457292840376, "phrase": "coherent_alternative"}, {"score": 0.004329630571352367, "phrase": "diagnostic_rules"}, {"score": 0.0042452552248398445, "phrase": "appealing_property"}, {"score": 0.004129873531628877, "phrase": "subjective_input"}, {"score": 0.0036987521452843987, "phrase": "potentially_misleading_results"}, {"score": 0.003669739058724546, "phrase": "roc"}, {"score": 0.00320970209394799, "phrase": "misclassification_costs"}, {"score": 0.00314708280192844, "phrase": "different_misclassification_cost_distributions"}, {"score": 0.002966438503447793, "phrase": "different_metrics"}, {"score": 0.002931569771634119, "phrase": "different_classification_rules"}, {"score": 0.0023976048394496446, "phrase": "relative_severities"}, {"score": 0.0023787690061742566, "phrase": "different_kinds"}, {"score": 0.0023415390666563177, "phrase": "individual_points"}, {"score": 0.0021049977753042253, "phrase": "simple_valid_alternative"}], "paper_keywords": ["ROC curves", " Classification", " AUC", " Specificity", " Sensitivity", " Misclassification rate", " Cost", " Loss", " Error rate"], "paper_abstract": "The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p not equal P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.", "paper_title": "Measuring classifier performance: a coherent alternative to the area under the ROC curve", "paper_id": "WOS:000269423100004"}