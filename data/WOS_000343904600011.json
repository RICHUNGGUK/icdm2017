{"auto_keywords": [{"score": 0.04227539537967157, "phrase": "mismatched_features"}, {"score": 0.009550124054024143, "phrase": "neutral_acoustic_space"}, {"score": 0.009342443150679192, "phrase": "speaker_recognition_performance"}, {"score": 0.007630028579425598, "phrase": "phoneme_recognition"}, {"score": 0.0067145501369568796, "phrase": "feature_regulation_methods"}, {"score": 0.00481495049065317, "phrase": "finer_granularity"}, {"score": 0.004772593750252861, "phrase": "emotional_speaker_recognition"}, {"score": 0.0046683172874085015, "phrase": "speakers'_vocal_organs"}, {"score": 0.0044272138900038095, "phrase": "emotional_acoustic_space"}, {"score": 0.004388252997694986, "phrase": "short-time_features"}, {"score": 0.004292337783332826, "phrase": "thereby_the_degradation"}, {"score": 0.003929107176349023, "phrase": "speaker_recognition_systems"}, {"score": 0.003894512634265439, "phrase": "emotion_variation"}, {"score": 0.0038602215057106917, "phrase": "different_feature_deformations"}, {"score": 0.003826231150275071, "phrase": "different_phonemes"}, {"score": 0.003676926952533304, "phrase": "finer_model"}, {"score": 0.0033955105098235345, "phrase": "acoustic_class_recognition-phoneme_classes"}, {"score": 0.003366432862787467, "phrase": "gaussian"}, {"score": 0.003066945490483524, "phrase": "feature_pruning"}, {"score": 0.0028826733506942554, "phrase": "feature_regulation_method"}, {"score": 0.0027947199430705077, "phrase": "between-class_distance"}, {"score": 0.0027456676902333304, "phrase": "within-class_distance"}, {"score": 0.0026736945502820303, "phrase": "transformation_matrix"}, {"score": 0.0025578967205807843, "phrase": "mandarin_affective_speech_corpus"}, {"score": 0.002425524064992056, "phrase": "identification_rate"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_algorithm"}], "paper_keywords": ["Emotional speaker recognition", " Mismatched feature detection", " Feature regulation"], "paper_abstract": "The shapes of speakers' vocal organs change under their different emotional states, which leads to the deviation of the emotional acoustic space of short-time features from the neutral acoustic space and thereby the degradation of the speaker recognition performance. Features deviating greatly from the neutral acoustic space are considered as mismatched features, and they negatively affect speaker recognition systems. Emotion variation produces different feature deformations for different phonemes, so it is reasonable to build a finer model to detect mismatched features under each phoneme. However, given the difficulty of phoneme recognition, three sorts of acoustic class recognition-phoneme classes, Gaussian mixture model (GMM) tokenizer, and probabilistic GMM tokenizer-are proposed to replace phoneme recognition. We propose feature pruning and feature regulation methods to process the mismatched features to improve speaker recognition performance. As for the feature regulation method, a strategy of maximizing the between-class distance and minimizing the within-class distance is adopted to train the transformation matrix to regulate the mismatched features. Experiments conducted on the Mandarin affective speech corpus (MASC) show that our feature pruning and feature regulation methods increase the identification rate (IR) by 3.64% and 6.77%, compared with the baseline GMM-UBM (universal background model) algorithm. Also, corresponding IR increases of 2.09% and 3.32% can be obtained with our methods when applied to the state-of-the-art algorithm i-vector.", "paper_title": "Mismatched feature detection with finer granularity for emotional speaker recognition", "paper_id": "WOS:000343904600011"}