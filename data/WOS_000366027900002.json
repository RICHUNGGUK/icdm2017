{"auto_keywords": [{"score": 0.029979913616334355, "phrase": "joint_representation"}, {"score": 0.00481495049065317, "phrase": "emotion_intensity"}, {"score": 0.004700027722284931, "phrase": "significant_clue"}, {"score": 0.00450000390667755, "phrase": "effective_techniques"}, {"score": 0.004456717971917794, "phrase": "video_arousal_recognition"}, {"score": 0.004205570965859848, "phrase": "novel_framework"}, {"score": 0.0041450181887917135, "phrase": "arousal_levels"}, {"score": 0.004085333687668526, "phrase": "low-level_audio-visual_features"}, {"score": 0.0039685202936462815, "phrase": "human_brain's_functional_activity"}, {"score": 0.0038364365168131586, "phrase": "functional_magnetic_resonance_imaging"}, {"score": 0.003637669525737665, "phrase": "audio-visual_features"}, {"score": 0.0034826993555190765, "phrase": "video_arousal"}, {"score": 0.0033667304423940893, "phrase": "fmri-derived_features"}, {"score": 0.0033021988455635403, "phrase": "brain_activity"}, {"score": 0.003270396852243212, "phrase": "comprehending_videos"}, {"score": 0.003146212656743413, "phrase": "brain_regions"}, {"score": 0.0030121159909273897, "phrase": "universal_brain_reference_system"}, {"score": 0.0027741761880779535, "phrase": "multimodal_deep_boltzmann_machine"}, {"score": 0.00269475174341129, "phrase": "learned_joint_representation"}, {"score": 0.002592369213499121, "phrase": "training_classifiers"}, {"score": 0.0025059724686541263, "phrase": "fmri_scanning"}, {"score": 0.002274630121183437, "phrase": "fmri_scans"}, {"score": 0.002241816883550397, "phrase": "experimental_results"}, {"score": 0.0022094759531152072, "phrase": "video_benchmark"}, {"score": 0.0021049977753042253, "phrase": "integrated_features"}], "paper_keywords": ["Arousal recognition", " affective computing", " fMRI-derived features", " multimodal DBM"], "paper_abstract": "As the indicator of emotion intensity, arousal is a significant clue for users to find their interested content. Hence, effective techniques for video arousal recognition are highly required. In this paper, we propose a novel framework for recognizing arousal levels by integrating low-level audio-visual features derived from video content and human brain's functional activity in response to videos measured by functional magnetic resonance imaging (fMRI). At first, a set of audio-visual features which have been demonstrated to be correlated with video arousal are extracted. Then, the fMRI-derived features that convey the brain activity of comprehending videos are extracted based on a number of brain regions of interests (ROIs) identified by a universal brain reference system. Finally, these two sets of features are integrated to learn a joint representation by using a multimodal deep Boltzmann machine (DBM). The learned joint representation can be utilized as the feature for training classifiers. Due to the fact that fMRI scanning is expensive and time-consuming, our DBM fusion model has the ability to predict the joint representation of the videos without fMRI scans. The experimental results on a video benchmark demonstrated the effectiveness of our framework and the superiority of integrated features.", "paper_title": "Arousal Recognition Using Audio-Visual Features and FMRI-Based Brain Response", "paper_id": "WOS:000366027900002"}