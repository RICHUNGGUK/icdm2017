{"auto_keywords": [{"score": 0.04131289278612189, "phrase": "training_samples"}, {"score": 0.00481495049065317, "phrase": "temporal-spatial_local_gaussian_process_experts"}, {"score": 0.0047026237903774895, "phrase": "motion_tracking_systems"}, {"score": 0.004592905436175987, "phrase": "regression_problem"}, {"score": 0.004539005582217305, "phrase": "discriminative_framework"}, {"score": 0.00443308765153716, "phrase": "challenging_task"}, {"score": 0.004329630571352367, "phrase": "observation_space"}, {"score": 0.004295681900150244, "phrase": "state_space"}, {"score": 0.004228577663320847, "phrase": "high-dimensional_characteristic"}, {"score": 0.004178935413340453, "phrase": "multimodal_conditional_distribution"}, {"score": 0.0040334640350288, "phrase": "existing_techniques"}, {"score": 0.003970439654470374, "phrase": "large_set"}, {"score": 0.0038930368103015467, "phrase": "learning_process"}, {"score": 0.0035699363258657212, "phrase": "gaussian_process"}, {"score": 0.0035421096742066617, "phrase": "gp"}, {"score": 0.003432044343307207, "phrase": "monocular_videos"}, {"score": 0.0030735451757603555, "phrase": "unified_input-output_space"}, {"score": 0.0030017207229983385, "phrase": "local_mixture_gp_experts_system"}, {"score": 0.002966438503447793, "phrase": "different_local_gp_experts"}, {"score": 0.0028857130140890787, "phrase": "mapping_behavior"}, {"score": 0.002851790448644913, "phrase": "specific_covariance_function"}, {"score": 0.002807178116892454, "phrase": "local_region"}, {"score": 0.0026986685585382347, "phrase": "temporal_and_spatial_information"}, {"score": 0.0026355812883224203, "phrase": "local_experts"}, {"score": 0.0026045914281028473, "phrase": "temporal_and_spatial_experts"}, {"score": 0.0025537472239953807, "phrase": "seamless_hybrid_system"}, {"score": 0.002445347925313158, "phrase": "visual_tracking"}, {"score": 0.002426137929553722, "phrase": "nonlinear_human_motion"}, {"score": 0.0022777800735119405, "phrase": "extensive_experiments"}, {"score": 0.0022421274859655217, "phrase": "humaneva"}, {"score": 0.002224511037826177, "phrase": "pear"}, {"score": 0.0021049977753042253, "phrase": "existing_models"}], "paper_keywords": ["Gaussian process regression", " human motion tracking", " local experts model", " pose estimation", " temporal-spatial model"], "paper_abstract": "Human pose estimation via motion tracking systems can be considered as a regression problem within a discriminative framework. It is always a challenging task to model the mapping from observation space to state space because of the high-dimensional characteristic in the multimodal conditional distribution. In order to build the mapping, existing techniques usually involve a large set of training samples in the learning process which are limited in their capability to deal with multimodality. We propose, in this work, a novel online sparse Gaussian Process (GP) regression model to recover 3-D human motion in monocular videos. Particularly, we investigate the fact that for a given test input, its output is mainly determined by the training samples potentially residing in its local neighborhood and defined in the unified input-output space. This leads to a local mixture GP experts system composed of different local GP experts, each of which dominates a mapping behavior with the specific covariance function adapting to a local region. To handle the multimodality, we combine both temporal and spatial information therefore to obtain two categories of local experts. The temporal and spatial experts are integrated into a seamless hybrid system, which is automatically self-initialized and robust for visual tracking of nonlinear human motion. Learning and inference are extremely efficient as all the local experts are defined online within very small neighborhoods. Extensive experiments on two real-world databases, HumanEva and PEAR, demonstrate the effectiveness of our proposed model, which significantly improve the performance of existing models.", "paper_title": "Human Motion Tracking by Temporal-Spatial Local Gaussian Process Experts", "paper_id": "WOS:000288675500020"}