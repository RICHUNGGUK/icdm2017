{"auto_keywords": [{"score": 0.03868577115089794, "phrase": "cart"}, {"score": 0.00481495049065317, "phrase": "ensemble_pruning_mechanism"}, {"score": 0.004604023526673226, "phrase": "classification_problems"}, {"score": 0.004424268136268051, "phrase": "ordinal_data"}, {"score": 0.004272724011826495, "phrase": "explanatory_and_class_variables"}, {"score": 0.004188459638572375, "phrase": "class_variable"}, {"score": 0.004024863549087511, "phrase": "explanatory_variables"}, {"score": 0.0038869467872381957, "phrase": "monotonicity_constraint"}, {"score": 0.003753738096719609, "phrase": "standard_classification_tree_algorithms"}, {"score": 0.0035183010916001664, "phrase": "monotonic_trees"}, {"score": 0.003380791705551809, "phrase": "completely_monotone"}, {"score": 0.0031061129850110994, "phrase": "decision_trees"}, {"score": 0.003075296862372287, "phrase": "growing_and_pruning_mechanisms"}, {"score": 0.0026745503978347143, "phrase": "random_forests"}, {"score": 0.002420611343093316, "phrase": "resulting_trees"}, {"score": 0.0023143284925318916, "phrase": "experimental_studies"}, {"score": 0.0022913497040895586, "phrase": "monotonic_data_sets"}, {"score": 0.0021474369836979048, "phrase": "monotonicity_restriction"}, {"score": 0.0021049977753042253, "phrase": "slightly_better_predictive_performance"}], "paper_keywords": ["Monotonic Classification", " Decision Tree Induction", " Random Forest", " Ensemble Pruning"], "paper_abstract": "In classification problems, it is very common to have ordinal data in the variables, in both explanatory and class variables. When the class variable should increase according to a subset of explanatory variables, the problem must satisfy the monotonicity constraint. It is well known that standard classification tree algorithms, such as CART or C4.5, are not guaranteed to produce monotonic trees, even if the data set is completely monotone. Recently, some classifiers have been designed to handle these kinds of problems. In decision trees, growing and pruning mechanisms have been updated to improve the monotonicity of the trees. In this paper, we study the suitability of using these mechanisms in the generation of Random Forests. For this, we propose a simple ensemble pruning mechanism based on the degree of monotonicity of the resulting trees. The performance of several decision trees are evaluated through experimental studies on monotonic data sets. We deduce that the trees produced by the Random Forest also hold the monotonicity restriction but achieve a slightly better predictive performance than standard algorithms.", "paper_title": "Monotonic Random Forest with an Ensemble Pruning Mechanism based on the Degree of Monotonicity", "paper_id": "WOS:000363956700003"}