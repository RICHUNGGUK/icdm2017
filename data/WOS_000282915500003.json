{"auto_keywords": [{"score": 0.0450260882774516, "phrase": "classification_phase"}, {"score": 0.00481495049065317, "phrase": "common_assumption"}, {"score": 0.004768649568826951, "phrase": "supervised_machine_learning"}, {"score": 0.004677372885331908, "phrase": "training_examples"}, {"score": 0.004587835274176236, "phrase": "learning_algorithm"}, {"score": 0.0038364365168131586, "phrase": "binary_classification_problem"}, {"score": 0.0037087324991876727, "phrase": "training_phase"}, {"score": 0.0033182152378613767, "phrase": "deleted_and_corrupted_features"}, {"score": 0.0027474453262607834, "phrase": "learning_problem"}, {"score": 0.0027078298877475965, "phrase": "linear_program"}, {"score": 0.002630299922281952, "phrase": "particular_structure"}, {"score": 0.002530359917257626, "phrase": "computational_efficiency"}, {"score": 0.0024818194799172263, "phrase": "statistical_bounds"}, {"score": 0.0024107449502335583, "phrase": "resulting_classifier"}, {"score": 0.0023530698447434308, "phrase": "robust_learning_problem"}, {"score": 0.0023079225317714815, "phrase": "modified_version"}, {"score": 0.002274630121183437, "phrase": "perceptron_algorithm"}, {"score": 0.002241816883550397, "phrase": "online-to-batch_conversion_technique"}, {"score": 0.002177600561610171, "phrase": "statistical_generalization_guarantees"}], "paper_keywords": ["Adversarial environment", " Binary classification", " Deleted features"], "paper_abstract": "A common assumption in supervised machine learning is that the training examples provided to the learning algorithm are statistically identical to the instances encountered later on, during the classification phase. This assumption is unrealistic in many real-world situations where machine learning techniques are used. We focus on the case where features of a binary classification problem, which were available during the training phase, are either deleted or become corrupted during the classification phase. We prepare for the worst by assuming that the subset of deleted and corrupted features is controlled by an adversary, and may vary from instance to instance. We design and analyze two novel learning algorithms that anticipate the actions of the adversary and account for them when training a classifier. Our first technique formulates the learning problem as a linear program. We discuss how the particular structure of this program can be exploited for computational efficiency and we prove statistical bounds on the risk of the resulting classifier. Our second technique addresses the robust learning problem by combining a modified version of the Perceptron algorithm with an online-to-batch conversion technique, and also comes with statistical generalization guarantees. We demonstrate the effectiveness of our approach with a set of experiments.", "paper_title": "Learning to classify with missing and corrupted features", "paper_id": "WOS:000282915500003"}