{"auto_keywords": [{"score": 0.047582591498013686, "phrase": "rotboost"}, {"score": 0.02322730111336098, "phrase": "rotation_forest"}, {"score": 0.009541139986820962, "phrase": "adaboost"}, {"score": 0.004591345494188032, "phrase": "novel_ensemble_classifier_generation_technique"}, {"score": 0.004076536554245035, "phrase": "uci"}, {"score": 0.003933609372601274, "phrase": "classification_tree"}, {"score": 0.003818326867123525, "phrase": "base_learning_algorithm"}, {"score": 0.0036625643243684827, "phrase": "ensemble_classifiers"}, {"score": 0.003619235073669304, "phrase": "significantly_lower_prediction_error"}, {"score": 0.003175044515761395, "phrase": "bagging"}, {"score": 0.0031374619658664843, "phrase": "multiboost"}, {"score": 0.003009390313137981, "phrase": "variance_decompositions"}, {"score": 0.0028693932802982417, "phrase": "considered_classification_methods"}, {"score": 0.0026241684260044414, "phrase": "single_tree"}, {"score": 0.0022610379771517966, "phrase": "considered_classification_procedures"}, {"score": 0.002168664249034287, "phrase": "potential_advantage"}, {"score": 0.0021049977753042253, "phrase": "parallel_execution"}], "paper_keywords": ["ensemble method", " base learning algorithm", " AdaBoost", " Rotation Forest", " Bagging", " MultiBoost"], "paper_abstract": "This paper presents a novel ensemble classifier generation technique RotBoost, which is constructed by combining Rotation Forest and AdaBoost. The experiments conducted with 36 real-world data sets available from the UCI repository, among which a classification tree is adopted as the base learning algorithm, demonstrate that RotBoost can generate ensemble classifiers with significantly lower prediction error than either Rotation Forest or AdaBoost more often than the reverse. Meanwhile, RotBoost is found to perform much better than Bagging and MultiBoost. Through employing the bias and variance decompositions of error to gain more insight of the considered classification methods, RotBoost is seen to simultaneously reduce the bias and variance terms of a single tree and the decrement achieved by it is much greater than that done by the other ensemble methods, which leads RotBoost to perform best among the considered classification procedures. Furthermore, RotBoost has a potential advantage over AdaBoost of suiting parallel execution. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "RotBoost: A technique for combining Rotation Forest and AdaBoost", "paper_id": "WOS:000257375300008"}