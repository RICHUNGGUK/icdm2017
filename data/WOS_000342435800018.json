{"auto_keywords": [{"score": 0.039533171561611936, "phrase": "multi-scale_patches"}, {"score": 0.03895000187878607, "phrase": "sparse_coefficients"}, {"score": 0.00481495049065317, "phrase": "robust_object_tracking"}, {"score": 0.004692980391580608, "phrase": "large_pose_change"}, {"score": 0.004645057834614705, "phrase": "illumination_variation"}, {"score": 0.004597622380240611, "phrase": "partial_occlusion"}, {"score": 0.004002513373645485, "phrase": "multi-scale_patch-based_appearance_model"}, {"score": 0.003861175855298779, "phrase": "efficient_scheme"}, {"score": 0.0036490529923833884, "phrase": "key_idea"}, {"score": 0.0034308688638878286, "phrase": "different_scale_patches"}, {"score": 0.003292685392413911, "phrase": "different_scale_dictionaries"}, {"score": 0.003048370487544551, "phrase": "similarity_score"}, {"score": 0.002940626369102927, "phrase": "particle_filter_framework"}, {"score": 0.0028807718824400697, "phrase": "target_state"}, {"score": 0.0028076592725295646, "phrase": "visual_tracking"}, {"score": 0.002708399936381754, "phrase": "visual_drift"}, {"score": 0.0025726416100664853, "phrase": "novel_two-step_object_tracking_method"}, {"score": 0.002443671484766342, "phrase": "first_frame"}, {"score": 0.0023451553255837317, "phrase": "multi-scale_patch_information"}, {"score": 0.0022856052476424344, "phrase": "publicly_available_benchmarks"}, {"score": 0.002262209824105154, "phrase": "video_sequences"}, {"score": 0.002193447487477984, "phrase": "complementary_information"}, {"score": 0.0021049977753042253, "phrase": "proposed_tracker"}], "paper_keywords": ["Object tracking", " Multi-scale patch", " Sparse representation", " Appearance model"], "paper_abstract": "When objects undergo large pose change, illumination variation or partial occlusion, most existing visual tracking algorithms tend to drift away from targets and even fail to track them. To address the issue, in this paper we propose a multi-scale patch-based appearance model with sparse representation and provide an efficient scheme involving the collaboration between multi-scale patches encoded by sparse coefficients. The key idea of our method is to model the appearance of an object by different scale patches, which are represented by sparse coefficients with different scale dictionaries. The model exploits both partial and spatial information of targets based on multi-scale patches. Afterwards, a similarity score of one candidate target is input into a particle filter framework to estimate the target state sequentially over time in visual tracking. Additionally, to decrease the visual drift caused by frequently updating model, we present a novel two-step object tracking method which exploits both the ground truth information of the target labeled in the first frame and the target obtained online with the multi-scale patch information. Experiments on some publicly available benchmarks of video sequences showed that the similarity involving complementary information can locate targets more accurately and the proposed tracker is more robust and effective than others.", "paper_title": "Multi-scale patch-based sparse appearance model for robust object tracking", "paper_id": "WOS:000342435800018"}