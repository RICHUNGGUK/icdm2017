{"auto_keywords": [{"score": 0.029559271850767357, "phrase": "new_samples"}, {"score": 0.00481495049065317, "phrase": "smote-rsb"}, {"score": 0.004521803560561489, "phrase": "high_imbalanced_data-sets"}, {"score": 0.004478310207047588, "phrase": "smote"}, {"score": 0.004435230698216773, "phrase": "rough_sets"}, {"score": 0.0043713857434347254, "phrase": "imbalanced_data"}, {"score": 0.004308455856718928, "phrase": "common_problem"}, {"score": 0.003949377440799037, "phrase": "special_relevance"}, {"score": 0.003911367220315185, "phrase": "highly_imbalanced_data-sets"}, {"score": 0.0035507457285983268, "phrase": "imbalanced_training_sets"}, {"score": 0.003516558537200823, "phrase": "supervised_learning"}, {"score": 0.0033182152378613767, "phrase": "algorithm_level"}, {"score": 0.0032389001343643064, "phrase": "data_level"}, {"score": 0.00320770578058916, "phrase": "data_level_groups"}, {"score": 0.0030267296900520217, "phrase": "training_sets"}, {"score": 0.002968695624366393, "phrase": "larger_class"}, {"score": 0.0026049517737353365, "phrase": "new_hybrid_method"}, {"score": 0.0025673856614952854, "phrase": "imbalanced_data-sets"}, {"score": 0.002387507583705495, "phrase": "editing_technique"}, {"score": 0.0023417010799384524, "phrase": "rough_set_theory"}, {"score": 0.0023079225317714815, "phrase": "lower_approximation"}, {"score": 0.002241816883550397, "phrase": "proposed_method"}, {"score": 0.002177600561610171, "phrase": "experimental_study"}, {"score": 0.0021566056345478373, "phrase": "good_results"}, {"score": 0.0021049977753042253, "phrase": "learning_algorithm"}], "paper_keywords": ["Imbalanced data-sets", " Classification", " Data preparation", " Oversampling", " Undersampling", " Rough sets theory"], "paper_abstract": "Imbalanced data is a common problem in classification. This phenomenon is growing in importance since it appears in most real domains. It has special relevance to highly imbalanced data-sets (when the ratio between classes is high). Many techniques have been developed to tackle the problem of imbalanced training sets in supervised learning. Such techniques have been divided into two large groups: those at the algorithm level and those at the data level. Data level groups that have been emphasized are those that try to balance the training sets by reducing the larger class through the elimination of samples or increasing the smaller one by constructing new samples, known as undersampling and oversampling, respectively. This paper proposes a new hybrid method for preprocessing imbalanced data-sets through the construction of new samples, using the Synthetic Minority Oversampling Technique together with the application of an editing technique based on the Rough Set Theory and the lower approximation of a subset. The proposed method has been validated by an experimental study showing good results using C4.5 as the learning algorithm.", "paper_title": "SMOTE-RSB*: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory", "paper_id": "WOS:000309874800001"}