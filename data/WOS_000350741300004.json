{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "liris-accede"}, {"score": 0.015203972017243274, "phrase": "affective_content_analysis"}, {"score": 0.004551781491498328, "phrase": "affective_computing"}, {"score": 0.004500891637057374, "phrase": "ground_truth_data"}, {"score": 0.004376131419366239, "phrase": "computational_models"}, {"score": 0.004022137057452406, "phrase": "large_video_database"}, {"score": 0.003845154818609644, "phrase": "related_applications"}, {"score": 0.00378080526397273, "phrase": "video_indexing"}, {"score": 0.0035941231858949035, "phrase": "existing_datasets"}, {"score": 0.0034359094594153304, "phrase": "copyright_constraints"}, {"score": 0.003284637361174143, "phrase": "large_content_diversity"}, {"score": 0.0031577303835881964, "phrase": "creative_commons_licenses"}, {"score": 0.003018668521745676, "phrase": "copyright_issues"}, {"score": 0.002984867789900281, "phrase": "affective_annotations"}, {"score": 0.0028695095228791724, "phrase": "pair-wise_video_comparison_protocol"}, {"score": 0.002651960559564796, "phrase": "high_inter-annotator_agreement"}, {"score": 0.0025928815987676535, "phrase": "large_diversity"}, {"score": 0.0025638362414466278, "phrase": "raters'_cultural_backgrounds"}, {"score": 0.002464709648714996, "phrase": "fair_comparison"}, {"score": 0.0024370965602952496, "phrase": "landmark_progresses"}, {"score": 0.0024097920845706795, "phrase": "future_affective_computational_models"}, {"score": 0.0022145052062614514, "phrase": "large_set"}, {"score": 0.0021773853011112882, "phrase": "visual_and_audio_features"}, {"score": 0.0021049977753042253, "phrase": "video_clips"}], "paper_keywords": ["Video database", " induced emotion", " computational emotion modeling", " emotion classification", " affective computing"], "paper_abstract": "Research in affective computing requires ground truth data for training and benchmarking computational models for machine-based emotion understanding. In this paper, we propose a large video database, namely LIRIS-ACCEDE, for affective content analysis and related applications, including video indexing, summarization or browsing. In contrast to existing datasets with very few video resources and limited accessibility due to copyright constraints, LIRIS-ACCEDE consists of 9,800 good quality video excerpts with a large content diversity. All excerpts are shared under creative commons licenses and can thus be freely distributed without copyright issues. Affective annotations were achieved using crowdsourcing through a pair-wise video comparison protocol, thereby ensuring that annotations are fully consistent, as testified by a high inter-annotator agreement, despite the large diversity of raters' cultural backgrounds. In addition, to enable fair comparison and landmark progresses of future affective computational models, we further provide four experimental protocols and a baseline for prediction of emotions using a large set of both visual and audio features. The dataset (the video clips, annotations, features and protocols) is publicly available at: http://liris-accede.ec-lyon.fr/.", "paper_title": "LIRIS-ACCEDE: A Video Database for Affective Content Analysis", "paper_id": "WOS:000350741300004"}