{"auto_keywords": [{"score": 0.02546576272351151, "phrase": "youtube"}, {"score": 0.01193793789819631, "phrase": "msr"}, {"score": 0.00481495049065317, "phrase": "enhanced_sparse_coding"}, {"score": 0.004690025569165577, "phrase": "natural_visual_signal"}, {"score": 0.004637478829869904, "phrase": "sparse_space"}, {"score": 0.004602773636967002, "phrase": "visual_codebook_generation"}, {"score": 0.004568326972997448, "phrase": "feature_quantization"}, {"score": 0.00419016653753787, "phrase": "increased_complexity"}, {"score": 0.004127657126534723, "phrase": "visual_patterns"}, {"score": 0.004096751549280256, "phrase": "diverse_human_actions"}, {"score": 0.003975416846030462, "phrase": "conventional_sparse_coding_scheme"}, {"score": 0.0038431888954930083, "phrase": "enhanced_sparse_coding_scheme"}, {"score": 0.003800093834008562, "phrase": "discriminative_dictionary"}, {"score": 0.003743381699791, "phrase": "local_pooling_strategy"}, {"score": 0.0036461569988218267, "phrase": "specific_action"}, {"score": 0.0036052633308076933, "phrase": "realistic_videos"}, {"score": 0.0035648266669315943, "phrase": "challenging_task"}, {"score": 0.0034984340449789745, "phrase": "sparse_coding"}, {"score": 0.003485304124598714, "phrase": "based_representations"}, {"score": 0.003459191419478298, "phrase": "human_actions"}, {"score": 0.003330031568711011, "phrase": "voi"}, {"score": 0.003306559295382181, "phrase": "fine_framework"}, {"score": 0.003113441660343867, "phrase": "local_visual_features"}, {"score": 0.0030554300179552415, "phrase": "sparse_signal_domain"}, {"score": 0.002942619810718864, "phrase": "exhaustive_scan"}, {"score": 0.002920561105784389, "phrase": "entire_videos"}, {"score": 0.0028877818105020434, "phrase": "voi_localization"}, {"score": 0.002833962854197749, "phrase": "spatial_pyramid_matching"}, {"score": 0.0028127163745564777, "phrase": "temporal_domain"}, {"score": 0.0027811441123794427, "phrase": "spatial_temporal_pyramid_matching"}, {"score": 0.002729307099580315, "phrase": "voi_candidates"}, {"score": 0.002678433668156678, "phrase": "multi-level_branch-and-bound_approach"}, {"score": 0.0025795066118838067, "phrase": "proposed_framework"}, {"score": 0.0025219085140220773, "phrase": "prohibitive_computations"}, {"score": 0.002502995662011305, "phrase": "local_similarity_matching"}, {"score": 0.0024563302149829, "phrase": "experimental_results"}, {"score": 0.002347843727281585, "phrase": "widely_used_localization_dataset"}, {"score": 0.0022781884538102264, "phrase": "computational_cost"}, {"score": 0.0022441378838655712, "phrase": "comparable_classification_accuracy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Human action classification", " Localization", " Sparse coding", " Volume of Interest (VOI)"], "paper_abstract": "Sparse coding which encodes the natural visual signal into a sparse space for visual codebook generation and feature quantization, has been successfully utilized for many image classification applications. However, it has been seldom explored for many video analysis tasks. In particular, the increased complexity in characterizing the visual patterns of diverse human actions with both the spatial and temporal variations imposes more challenges to the conventional sparse coding scheme. In this paper, we propose an enhanced sparse coding scheme through learning discriminative dictionary and optimizing the local pooling strategy. Localizing when and where a specific action happens in realistic videos is another challenging task. By utilizing the sparse coding based representations of human actions, this paper further presents a novel coarse-to-fine framework to localize the Volumes of Interest (VOIs) for the actions. Firstly, local visual features are transformed into the sparse signal domain through our enhanced sparse coding scheme. Secondly, in order to avoid exhaustive scan of entire videos for the VOI localization, we extend the Spatial Pyramid Matching into temporal domain, namely Spatial Temporal Pyramid Matching, to obtain the VOI candidates. Finally, a multi-level branch-and-bound approach is developed to refine the VOI candidates. The proposed framework is also able to avoid prohibitive computations in local similarity matching (e.g., nearest neighbors voting). Experimental results on both two popular benchmark datasets (KTH and YouTube UCF) and the widely used localization dataset (MSR) demonstrate that our approach reduces computational cost significantly while maintaining comparable classification accuracy to that of the state-of-the-art methods. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Fast human action classification and VOI localization with enhanced sparse coding", "paper_id": "WOS:000314859000006"}