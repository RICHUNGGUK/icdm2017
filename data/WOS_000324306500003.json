{"auto_keywords": [{"score": 0.04186634688011743, "phrase": "ota"}, {"score": 0.010225338740038885, "phrase": "multiagent_reinforcement_learning"}, {"score": 0.00481495049065317, "phrase": "multiagent_systems"}, {"score": 0.00477513738640734, "phrase": "summarythe_curse"}, {"score": 0.004677032188041644, "phrase": "ubiquitous_problem"}, {"score": 0.004077995480957875, "phrase": "new_framework"}, {"score": 0.00402748353567517, "phrase": "optimal_tracking_agent"}, {"score": 0.003737234606207431, "phrase": "reduced_form"}, {"score": 0.0036756196381448015, "phrase": "optimal_decision"}, {"score": 0.003482276061796395, "phrase": "action_space"}, {"score": 0.0032581736321024373, "phrase": "reinforcement_learning"}, {"score": 0.0032312172795243184, "phrase": "rl"}, {"score": 0.002960979624379786, "phrase": "dynamic_model"}, {"score": 0.002900037689072025, "phrase": "model-based_rl"}, {"score": 0.002805121439110729, "phrase": "dynamic_environment"}, {"score": 0.0026796495639791426, "phrase": "dynamic_process"}, {"score": 0.002602734354498382, "phrase": "traditional_rl"}, {"score": 0.002517524071220657, "phrase": "stationary_process"}, {"score": 0.0023651843133643785, "phrase": "greedy_action_selection_mechanism"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["estimator", " action selection mechanism", " curse of dimensionality", " optimal tracking agent", " multiagent systems"], "paper_abstract": "SUMMARYThe curse of dimensionality is a ubiquitous problem for multiagent reinforcement learning, which means the learning and storing space grows exponentially with the number of agents and hinders the application of multiagent reinforcement learning. To relieve this problem, we propose a new framework named as optimal tracking agent (OTA). The OTA views the other agents as part of the environment and uses a reduced form to learn the optimal decision. Although merging other agents into the environment may reduce the dimension of action space, the environment characterized by such form is dynamic and does not satisfy the convergence of reinforcement learning (RL). Thus, we develop an estimator to track the dynamics of the environment. The estimator obtains the dynamic model, and then the model-based RL can be used to react to the dynamic environment optimally. Because the Q-function in OTA is also a dynamic process because of other agents' dynamics, different from traditional RL, in which the learning is a stationary process and the usual action selection mechanisms just suit to such stationary process, we improve the greedy action selection mechanism to adapt to such dynamics. Thus, the OTA will have convergence. An experiment illustrates the validity and efficiency of the OTA.Copyright (c) 2012 John Wiley & Sons, Ltd.", "paper_title": "Optimal tracking agent: a new framework of reinforcement learning for multiagent systems", "paper_id": "WOS:000324306500003"}