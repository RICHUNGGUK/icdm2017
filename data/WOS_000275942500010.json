{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "message-passing_interface"}, {"score": 0.004653815971302021, "phrase": "parallel_applications"}, {"score": 0.004625100791402994, "phrase": "distributed_memory"}, {"score": 0.004456475967147909, "phrase": "mpi_standard"}, {"score": 0.004388035083357706, "phrase": "specific_performance_guarantees"}, {"score": 0.00432064072724161, "phrase": "mpi_implementations"}, {"score": 0.0042806998378295425, "phrase": "good_and_consistent_performance"}, {"score": 0.004214947028602297, "phrase": "efficient_utilization"}, {"score": 0.0041759790955637225, "phrase": "underlying_parallel"}, {"score": 0.004086443472472481, "phrase": "performance_portability_reasons"}, {"score": 0.004011221986096511, "phrase": "communication_optimizations"}, {"score": 0.003852939975815287, "phrase": "mpi_implementation"}, {"score": 0.003723879111071547, "phrase": "performance_consistency"}, {"score": 0.003666647736663085, "phrase": "performance_guidelines"}, {"score": 0.0035879931522580745, "phrase": "good_mpi_implementations"}, {"score": 0.0035001588456889904, "phrase": "specific_performance_model"}, {"score": 0.003372408902017458, "phrase": "mpi_protocol"}, {"score": 0.0033515736929172644, "phrase": "algorithm_assumptions"}, {"score": 0.0031697368057340895, "phrase": "semantically_strongly_interrelated_mpi_standard"}, {"score": 0.003120995227740011, "phrase": "common-sense_expectations"}, {"score": 0.003044558129699335, "phrase": "mpi_function"}, {"score": 0.0029062336294411384, "phrase": "specialized_function"}, {"score": 0.002765588197487234, "phrase": "weak_semantic_guarantees"}, {"score": 0.0027146141712666673, "phrase": "similar_function"}, {"score": 0.0026978319081381145, "phrase": "stronger_semantics"}, {"score": 0.002599289171876573, "phrase": "higher_quality_mpi_implementations"}, {"score": 0.002575219653364536, "phrase": "performance_surprises"}, {"score": 0.0024128447214999647, "phrase": "self-consistent_performance_guidelines"}, {"score": 0.002398006389642744, "phrase": "mpi"}, {"score": 0.002253682813700968, "phrase": "experiment_management_tools"}, {"score": 0.0022258906408344973, "phrase": "experimental_results"}, {"score": 0.002157897198476163, "phrase": "common_mpi_implementations"}, {"score": 0.0021049977753042253, "phrase": "today's_mpi_implementations"}], "paper_keywords": ["Parallel processing", " message passing", " message-passing interface", " MPI", " performance portability", " performance prediction", " performance model", " public benchmarking", " performance guidelines"], "paper_abstract": "Message passing using the Message-Passing Interface (MPI) is at present the most widely adopted framework for programming parallel applications for distributed memory and clustered parallel systems. For reasons of ( universal) implementability, the MPI standard does not state any specific performance guarantees, but users expect MPI implementations to deliver good and consistent performance in the sense of efficient utilization of the underlying parallel (communication) system. For performance portability reasons, users also naturally desire communication optimizations performed on one parallel platform with one MPI implementation to be preserved when switching to another MPI implementation on another platform. We address the problem of ensuring performance consistency and portability by formulating performance guidelines and conditions that are desirable for good MPI implementations to fulfill. Instead of prescribing a specific performance model (which may be realistic on some systems, under some MPI protocol and algorithm assumptions, etc.), we formulate these guidelines by relating the performance of various aspects of the semantically strongly interrelated MPI standard to each other. Common-sense expectations, for instance, suggest that no MPI function should perform worse than a combination of other MPI functions that implement the same functionality, no specialized function should perform worse than a more general function that can implement the same functionality, no function with weak semantic guarantees should perform worse than a similar function with stronger semantics, and so on. Such guidelines may enable implementers to provide higher quality MPI implementations, minimize performance surprises, and eliminate the need for users to make special, nonportable optimizations by hand. We introduce and semiformalize the concept of self-consistent performance guidelines for MPI, and provide a (nonexhaustive) set of such guidelines in a form that could be automatically verified by benchmarks and experiment management tools. We present experimental results that show cases where guidelines are not satisfied in common MPI implementations, thereby indicating room for improvement in today's MPI implementations.", "paper_title": "Self-Consistent MPI Performance Guidelines", "paper_id": "WOS:000275942500010"}