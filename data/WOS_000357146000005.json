{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "concurrent_and_interactive_visual_attention"}, {"score": 0.004685224205630963, "phrase": "brain-inspired_developmental_architecture"}, {"score": 0.004653339900995785, "phrase": "where-what_network"}, {"score": 0.004543436145505023, "phrase": "second_version"}, {"score": 0.004512512374933493, "phrase": "wwn"}, {"score": 0.004346139434813303, "phrase": "complementary_pathways"}, {"score": 0.004301834610818624, "phrase": "\"type\"_motor"}, {"score": 0.0042725481832853315, "phrase": "\"location\"_motor"}, {"score": 0.00422899036681456, "phrase": "motor-driven_top-down_signals"}, {"score": 0.00417160036938041, "phrase": "bottom-up_excitatory_activities"}, {"score": 0.003990385737207714, "phrase": "y-shaped_network"}, {"score": 0.003922794571908569, "phrase": "constrained_sparse_coding_scheme"}, {"score": 0.0037140731069193896, "phrase": "non-iterative_cell-centered_synaptic_update_model"}, {"score": 0.003540539868937462, "phrase": "dual_optimization"}, {"score": 0.003516417950483685, "phrase": "update_directions"}, {"score": 0.0034924598001567944, "phrase": "step_sizes"}, {"score": 0.0034215569542785907, "phrase": "firing_ages"}, {"score": 0.0033406481167452436, "phrase": "cluttered_scenes"}, {"score": 0.0032952735750239924, "phrase": "learning_process"}, {"score": 0.003195414285736136, "phrase": "motor_area"}, {"score": 0.003173636240475185, "phrase": "context-free_mode"}, {"score": 0.0030985717040741875, "phrase": "cluttered_scene"}, {"score": 0.003066945490483524, "phrase": "learned_object"}, {"score": 0.002984173420996039, "phrase": "object_recognition"}, {"score": 0.0028937142757436683, "phrase": "object_search"}, {"score": 0.0028446460307503343, "phrase": "single_network"}, {"score": 0.002796407494570146, "phrase": "attention_capabilities"}, {"score": 0.002748984715897838, "phrase": "visual_processing"}, {"score": 0.002720916895509682, "phrase": "proposed_network"}, {"score": 0.0026656357767832245, "phrase": "complex_backgrounds"}, {"score": 0.0025759787655463212, "phrase": "graded_performance"}, {"score": 0.002549672862868133, "phrase": "pixel_errors"}, {"score": 0.002532284627202572, "phrase": "recognition_accuracy"}, {"score": 0.002497862210672172, "phrase": "proposed_architecture"}, {"score": 0.0024387423375226507, "phrase": "internal_representations"}, {"score": 0.0023728841359713106, "phrase": "motor_signal"}, {"score": 0.002300912218086077, "phrase": "specific_task"}, {"score": 0.002238767763626708, "phrase": "computational_architecture"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Developmental learning", " Where-what sensorimotor pathways", " Attention", " Recognition", " Brain-inspired neural network"], "paper_abstract": "This paper presents a brain-inspired developmental architecture called Where-What Network (WWN). In this second version of WWN, WWN-2 is learned for concurrent and interactive visual attention and recognition, via complementary pathways guided by \"type\" motor and \"location\" motor. The motor-driven top-down signals, together with bottom-up excitatory activities from the visual input, shape three possible information flows through a Y-shaped network. Using l(0) constrained sparse coding scheme, the top-down and bottom-up co-firing leads to a non-iterative cell-centered synaptic update model, entailing the strict entropy reduction from early to later layers, as well as a dual optimization of update directions and step sizes that dynamically depend on the firing ages of the neurons. Three operational modes for cluttered scenes emerge from the learning process, depending on what is available in the motor area: context-free mode for detection and recognition from a cluttered scene for a learned object, location-context mode for doing object recognition, and type-context mode for doing object search, all by a single network. To demonstrate the attention capabilities along with their interaction of visual processing, the proposed network is in the presence of complex backgrounds, learns on the fly, and produces engineering graded performance regarding attended pixel errors and recognition accuracy. As the proposed architecture is developmental, meaning that the internal representations are learned from pairs of input and motor signal, and thereby not manipulated internally for a specific task, we argue that the same learning principles and computational architecture can be potentially applicable to other sensory modalities, such as audition and touch. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "A developmental where-what network for concurrent and interactive visual attention and recognition", "paper_id": "WOS:000357146000005"}