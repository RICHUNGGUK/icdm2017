{"auto_keywords": [{"score": 0.04859258959287385, "phrase": "multiple_gpus"}, {"score": 0.031638796172898664, "phrase": "real_hardware"}, {"score": 0.00481495049065317, "phrase": "runtime_and_architecture_support_for_efficient_data_exchange"}, {"score": 0.0047846807459660376, "phrase": "multi-accelerator_applications"}, {"score": 0.004754600386388821, "phrase": "heterogeneous_parallel_computing_applications"}, {"score": 0.00470983237900705, "phrase": "large_data_sets"}, {"score": 0.004578030096508602, "phrase": "physical_memory_capacity"}, {"score": 0.004435885269134359, "phrase": "high-level_abstractions"}, {"score": 0.0044079880067019765, "phrase": "previous_heterogeneous_parallel_programming_models"}, {"score": 0.004325339932975582, "phrase": "multiple_code_versions"}, {"score": 0.004298134888203576, "phrase": "complex_data"}, {"score": 0.004244234872662471, "phrase": "synchronization_schemes"}, {"score": 0.004177805391358892, "phrase": "multiple_gpu_devices"}, {"score": 0.004112411350177274, "phrase": "high_software_development_cost"}, {"score": 0.004035282762331904, "phrase": "even_poor_performance"}, {"score": 0.003959594975607075, "phrase": "hpe_runtime_system"}, {"score": 0.0037056529033273693, "phrase": "cross-node_network_interfaces"}, {"score": 0.0036476222647061243, "phrase": "architecture_support"}, {"score": 0.0034352452951903305, "phrase": "simplified_programming_interface"}, {"score": 0.0034136194753440745, "phrase": "programming_complexity"}, {"score": 0.003194580334752371, "phrase": "hpe_runtime_systems"}, {"score": 0.0031247264362609614, "phrase": "nvidia_gpu_hardware"}, {"score": 0.0030855535871512833, "phrase": "cuda"}, {"score": 0.0029613556844064713, "phrase": "key_hpe_features"}, {"score": 0.00291494666099687, "phrase": "rare_opportunity"}, {"score": 0.0028511898481970595, "phrase": "hardware_support"}, {"score": 0.002824292971306322, "phrase": "important_benchmarks"}, {"score": 0.0028065024340850615, "phrase": "real_runtime"}, {"score": 0.0027712559151232843, "phrase": "experimental_results"}, {"score": 0.0027278178983579085, "phrase": "exemplar_heterogeneous_system"}, {"score": 0.0027106334164712057, "phrase": "peer_dma"}, {"score": 0.00264296841319955, "phrase": "software_techniques"}, {"score": 0.002609770322855842, "phrase": "inter-accelerator_data_communication_bandwidth"}, {"score": 0.0025365874386986493, "phrase": "execution_speed"}, {"score": 0.0023587312574818208, "phrase": "proposed_architecture_support"}, {"score": 0.002336469133929162, "phrase": "hpe_runtime"}, {"score": 0.002285335999629823, "phrase": "simple_portable_user_code"}, {"score": 0.0022637650216685906, "phrase": "system_designers"}, {"score": 0.0022212306049833397, "phrase": "different_capabilities"}, {"score": 0.002179493631738601, "phrase": "simple_interfaces"}, {"score": 0.002158923060406189, "phrase": "hpe"}, {"score": 0.0021049977753042253, "phrase": "advanced_hardware_features"}], "paper_keywords": ["Distributed architectures", " hardware/software interfaces", " heterogeneous (hybrid) systems", " data communications"], "paper_abstract": "Heterogeneous parallel computing applications often process large data sets that require multiple GPUs to jointly meet their needs for physical memory capacity and compute throughput. However, the lack of high-level abstractions in previous heterogeneous parallel programming models force programmers to resort to multiple code versions, complex data copy steps and synchronization schemes when exchanging data between multiple GPU devices, which results in high software development cost, poor maintainability, and even poor performance. This paper describes the HPE runtime system, and the associated architecture support, which enables a simple, efficient programming interface for exchanging data between multiple GPUs through either interconnects or cross-node network interfaces. The runtime and architecture support presented in this paper can also be used to support other types of accelerators. We show that the simplified programming interface reduces programming complexity. The research presented in this paper started in 2009. It has been implemented and tested extensively in several generations of HPE runtime systems as well as adopted into the NVIDIA GPU hardware and drivers for CUDA 4.0 and beyond since 2011. The availability of real hardware that support key HPE features gives rise to a rare opportunity for studying the effectiveness of the hardware support by running important benchmarks on real runtime and hardware. Experimental results show that in a exemplar heterogeneous system, peer DMA and double-buffering, pinned buffers, and software techniques can improve the inter-accelerator data communication bandwidth by 2 x. They can also improve the execution speed by 1.6 x for a 3D finite difference, 2.5 x for 1D FFT, and 1.6 x for merge sort, all measured on real hardware. The proposed architecture support enables the HPE runtime to transparently deploy these optimizations under simple portable user code, allowing system designers to freely employ devices of different capabilities. We further argue that simple interfaces such as HPE are needed for most applications to benefit from advanced hardware features in practice.", "paper_title": "Runtime and Architecture Support for Efficient Data Exchange in Multi-Accelerator Applications", "paper_id": "WOS:000352728200017"}