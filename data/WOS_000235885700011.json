{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "bottom-up_visual_attention"}, {"score": 0.004626553103375699, "phrase": "redundant_visual_information"}, {"score": 0.004481132747216135, "phrase": "automatic_determination"}, {"score": 0.004254467919664946, "phrase": "video_coding"}, {"score": 0.004187049198138107, "phrase": "video_browsing"}, {"score": 0.004137184278868391, "phrase": "quality_assessment"}, {"score": 0.004039221742250308, "phrase": "computational_modeling"}, {"score": 0.003991110291836103, "phrase": "visual_attention_system"}, {"score": 0.003943569631183138, "phrase": "first_published_computational_models"}, {"score": 0.003640730820018083, "phrase": "visual_system"}, {"score": 0.003583002730249742, "phrase": "complex_features"}, {"score": 0.0035403244470159447, "phrase": "hvs"}, {"score": 0.0034981163169818803, "phrase": "hierarchical_perceptual_representation"}, {"score": 0.003456427643307907, "phrase": "visual_input"}, {"score": 0.0034152340901585374, "phrase": "bottom-up_mechanism"}, {"score": 0.003334309053314909, "phrase": "modern_models"}, {"score": 0.003255295296893174, "phrase": "involuntary_attention"}, {"score": 0.003005151718290603, "phrase": "coherent_computational_approach"}, {"score": 0.0028302049070944944, "phrase": "current_understanding"}, {"score": 0.0027964537059186893, "phrase": "hvs_behavior"}, {"score": 0.0027631038845413393, "phrase": "sensitivity_functions"}, {"score": 0.002741091312632307, "phrase": "perceptual_decomposition"}, {"score": 0.0027192536281209463, "phrase": "visual_masking"}, {"score": 0.0024703502756471514, "phrase": "natural_images"}, {"score": 0.0024506642262508735, "phrase": "experimental_measurements"}, {"score": 0.0024214281919523973, "phrase": "eye-tracking_system"}, {"score": 0.0023545567809591804, "phrase": "kullbacl-leibler"}, {"score": 0.0021049977753042253, "phrase": "reference_bottom-up_model"}], "paper_keywords": ["computationally modeled human vision", " bottom-up visual attention", " coherent modeling", " eye tracking experiments"], "paper_abstract": "Visual attention is a mechanism which filters out redundant visual information and detects the most relevant parts of our visual field. Automatic determination of the most visually relevant areas would be useful in many applications such as image and video coding, watermarking, video browsing, and quality assessment. Many research groups are currently investigating computational modeling of the visual attention system. The first published computational models have been based on some basic and well-understood Human Visual System (HVS) properties. These models feature a single perceptual layer that simulates only one aspect of the visual system. More recent models integrate complex features of the HVS and simulate hierarchical perceptual representation of the visual input. The bottom-up mechanism is the most occurring feature found in modern models. This mechanism refers to involuntary attention (i.e., salient spatial visual features that effortlessly or involuntary attract our attention). This paper presents a coherent computational approach to the modeling of the bottom-up visual attention. This model is mainly based on the current understanding of the HVS behavior. Contrast sensitivity functions, perceptual decomposition, visual masking, and center-surround interactions are some of the features implemented in this model. The performances of this algorithm are assessed by using natural images and experimental measurements from an eye-tracking system. Two adequate well-known metrics (correlation coefficient and Kullbacl-Leibler divergence) are used to validate this model. A further metric is also defined. The results from this model are finally compared to those from a reference bottom-up model.", "paper_title": "A coherent computational approach to model bottom-up visual attention", "paper_id": "WOS:000235885700011"}