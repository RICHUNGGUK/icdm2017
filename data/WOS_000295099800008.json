{"auto_keywords": [{"score": 0.042426166957065956, "phrase": "sdram_utilization"}, {"score": 0.039523947436356864, "phrase": "sdram"}, {"score": 0.00481495049065317, "phrase": "efficient_sdram_access"}, {"score": 0.0045124316982387315, "phrase": "guaranteed_synchronous_dynamic_random_access_memory"}, {"score": 0.004247180402119585, "phrase": "best-effort_packet"}, {"score": 0.004156259296367594, "phrase": "priority_service"}, {"score": 0.004102639724186862, "phrase": "guaranteed_latency"}, {"score": 0.003962998498942111, "phrase": "overall_system"}, {"score": 0.003795086335272422, "phrase": "data_size"}, {"score": 0.003618558599449415, "phrase": "sdram_access_granularity"}, {"score": 0.003205268682789165, "phrase": "memory_latency_demands"}, {"score": 0.003177615880037735, "phrase": "memory_access_granularities"}, {"score": 0.0030693625816617044, "phrase": "short_latency"}, {"score": 0.0029647862293448895, "phrase": "memory_request_packets"}, {"score": 0.002863762666256937, "phrase": "hybrid_flow_controller"}, {"score": 0.0028390477456009568, "phrase": "priority-first_and_priority-equal_algorithms"}, {"score": 0.002754221284034087, "phrase": "noc"}, {"score": 0.0026951616152303373, "phrase": "memory_performance"}, {"score": 0.0026488343783448273, "phrase": "memory_request_packet"}, {"score": 0.0025696647013416863, "phrase": "short_memory_request_packets"}, {"score": 0.0025364615261136655, "phrase": "partially_open-page_mode"}, {"score": 0.0025036862991477437, "phrase": "auto-precharge_operation"}, {"score": 0.0024713335323674223, "phrase": "memory_subsystem"}, {"score": 0.002449997042686432, "phrase": "experimental_results"}, {"score": 0.002315719347244451, "phrase": "latency-sensitive_cores"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_sdram-aware_noc_design"}], "paper_keywords": ["Access granularity", " flow control", " memory", " networks-on-chip", " quality-of-service", " router"], "paper_abstract": "In systems-on-chip (SoCs), a microprocessor demands guaranteed synchronous dynamic random access memory (SDRAM) latency whereas most of the other cores are served as a best-effort packet. However, a priority service for the guaranteed latency causes the SDRAM utilization and latency of an overall system to be degraded critically. In addition, the data size of SDRAM requested by various cores is not matched with an SDRAM access granularity such that the SDRAM utilization and latency are further deteriorated. In this paper, we propose an application-aware networks-on-chip (NoCs) design for an efficient SDRAM access, which can consider memory latency demands and memory access granularities in various applications. In order to provide short latency for priority memory requests with few penalties, memory request packets are scheduled by our guaranteed SDRAM service router that includes a hybrid flow controller of priority-first and priority-equal algorithms. In addition, our SDRAM access granularity matching NoC design further improves the memory performance by splitting a memory request packet to several short memory request packets and then controlling the short memory request packets with a partially open-page mode and an auto-precharge operation in a memory subsystem. Experimental results show that our cost-effective application-aware NoC design significantly improves, on average, memory latency for latency-sensitive cores up to 32.8%, overall memory latency up to 7.8%, and memory utilization up to 3.4%, compared to the state-of-the-art SDRAM-aware NoC design [4].", "paper_title": "Application-Aware NoC Design for Efficient SDRAM Access", "paper_id": "WOS:000295099800008"}