{"auto_keywords": [{"score": 0.048398179269243564, "phrase": "action_recognition"}, {"score": 0.00481495049065317, "phrase": "single_example"}, {"score": 0.004687645903373103, "phrase": "novel_approach"}, {"score": 0.004462892860919036, "phrase": "hierarchical_codebook_model"}, {"score": 0.0044231966846655394, "phrase": "local_spatio-temporal_video_volumes"}, {"score": 0.004211069233133584, "phrase": "query_video"}, {"score": 0.004154995396099332, "phrase": "proposed_method"}, {"score": 0.004118026552723285, "phrase": "similar_videos"}, {"score": 0.004009074013302036, "phrase": "target_video_dataset"}, {"score": 0.0038338299812503, "phrase": "video_words"}, {"score": 0.0036826452656927877, "phrase": "prior_knowledge"}, {"score": 0.0036173729405604674, "phrase": "background_subtraction"}, {"score": 0.0035851703423654432, "phrase": "motion_estimation"}, {"score": 0.0034437574674180365, "phrase": "spatial_and_temporal_scale_changes"}, {"score": 0.0032059442984238664, "phrase": "compact_set"}, {"score": 0.003177392642813126, "phrase": "spatio-temporal_volumes"}, {"score": 0.00303839203670705, "phrase": "spatial_and_temporal_contextual_information"}, {"score": 0.0028795712616923462, "phrase": "spatio-temporal_video_volumes"}, {"score": 0.0028284924551705516, "phrase": "large_contextual_volume"}, {"score": 0.0026095920086044145, "phrase": "probabilistic_model"}, {"score": 0.0025863375961196005, "phrase": "video_volumes"}, {"score": 0.0024292428060702803, "phrase": "different_complexities"}, {"score": 0.002364865100579589, "phrase": "msr_ii"}, {"score": 0.0022013916728990564, "phrase": "single_training_example"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action recognition", " Bag of video words", " Hierarchical codebook", " Spatio-temporal contextual information", " Probabilistic modeling", " Context", " Ensemble of volumes"], "paper_abstract": "This paper presents a novel approach for action recognition, localization and video matching based on a hierarchical codebook model of local spatio-temporal video volumes. Given a single example of an activity as a query video, the proposed method finds similar videos to the query in a target video dataset. The method is based on the bag of video words (BOV) representation and does not require prior knowledge about actions, background subtraction, motion estimation or tracking. It is also robust to spatial and temporal scale changes, as well as some deformations. The hierarchical algorithm codes a video as a compact set of spatio-temporal volumes, while considering their spatio-temporal compositions in order to account for spatial and temporal contextual information. This hierarchy is achieved by first constructing a codebook of spatio-temporal video volumes. Then a large contextual volume containing many spatio-temporal volumes (ensemble of volumes) is considered. These ensembles are used to construct a probabilistic model of video volumes and their spatio-temporal compositions. The algorithm was applied to three available video datasets for action recognition with different complexities (KTH. Weizmann, and MSR II) and the results were superior to other approaches, especially in the case of a single training example and cross-dataset(1) action recognition. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Human activity recognition in videos using a single example", "paper_id": "WOS:000328184100004"}