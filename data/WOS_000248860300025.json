{"auto_keywords": [{"score": 0.04926778266856686, "phrase": "mutual_information"}, {"score": 0.00481495049065317, "phrase": "feature_selection_wrapper"}, {"score": 0.004548253531664032, "phrase": "hybrid_genetic_algorithm"}, {"score": 0.004175533280178052, "phrase": "classification_task"}, {"score": 0.003966667994579941, "phrase": "outer_optimization_stage"}, {"score": 0.003899385109646589, "phrase": "global_search"}, {"score": 0.0038332390893263844, "phrase": "best_subset"}, {"score": 0.003725470458263889, "phrase": "wrapper_way"}, {"score": 0.003559284776569813, "phrase": "predictive_labels"}, {"score": 0.0034988876789164235, "phrase": "trained_classifier"}, {"score": 0.0034395119150793787, "phrase": "true_classes"}, {"score": 0.0033619033669956317, "phrase": "fitness_function"}, {"score": 0.0033048445033866795, "phrase": "genetic_algorithm"}, {"score": 0.0032487508969192293, "phrase": "inner_optimization"}, {"score": 0.003193606324727067, "phrase": "local_search"}, {"score": 0.0031393948330209224, "phrase": "filter_manner"}, {"score": 0.002881793492919646, "phrase": "independent_measure"}, {"score": 0.0027218799769229596, "phrase": "candidate_feature"}, {"score": 0.002675654549462983, "phrase": "output_classes"}, {"score": 0.002570817338292482, "phrase": "already-selected_features"}, {"score": 0.002527150892127982, "phrase": "inner_and_outer_optimizations"}, {"score": 0.002414290278501595, "phrase": "high_global_predictive_accuracy"}, {"score": 0.0023463203637929466, "phrase": "high_local_search_efficiency"}, {"score": 0.0022672718785099666, "phrase": "parsimonious_feature_selection"}, {"score": 0.002241517354570928, "phrase": "excellent_classification_accuracy"}, {"score": 0.0021413855297794946, "phrase": "benchmark_data_sets"}], "paper_keywords": ["pattern classification", " machine learning", " hybrid genetic algorithm", " feature selection", " mutual information"], "paper_abstract": "In this study, a hybrid genetic algorithm is adopted to find a subset of features that are most relevant to the classification task. Two stages of optimization are involved. The outer optimization stage completes the global search for the best subset of features in a wrapper way, in which the mutual information between the predictive labels of a trained classifier and the true classes serves as the fitness function for the genetic algorithm. The inner optimization performs the local search in a filter manner, in which an improved estimation of the conditional mutual information acts as an independent measure for feature ranking taking account of not only the relevance of the candidate feature to the output classes but also the redundancy to the already-selected features. The inner and outer optimizations cooperate with each other and achieve the high global predictive accuracy as well as the high local search efficiency. Experimental results demonstrate both parsimonious feature selection and excellent classification accuracy of the method on a range of benchmark data sets. (c) 2007 Elsevier B.V. All rights reserved.", "paper_title": "A hybrid genetic algorithm for feature selection wrapper based on mutual information", "paper_id": "WOS:000248860300025"}