{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "complexity_issues_of_online_learning_algorithms"}, {"score": 0.004490842671590769, "phrase": "new_probabilistic_upper_bounds"}, {"score": 0.004272724011826495, "phrase": "online_learning_algorithm"}, {"score": 0.003829290275367182, "phrase": "linear_stochastic_approximations"}, {"score": 0.003753738096719609, "phrase": "hilbert_spaces"}, {"score": 0.0036070601923916196, "phrase": "upper_bounds"}, {"score": 0.003397683082821229, "phrase": "almost_sure_convergence"}, {"score": 0.0032004205378150354, "phrase": "square_summable_condition"}, {"score": 0.0031061129850110994, "phrase": "step_size"}, {"score": 0.002984665318717504, "phrase": "early_work"}, {"score": 0.0027557911038816256, "phrase": "averaging_process"}, {"score": 0.002420611343093316, "phrase": "\"batch_learning"}, {"score": 0.0021474369836979048, "phrase": "sample_size"}, {"score": 0.0021049977753042253, "phrase": "regularization_parameter"}], "paper_keywords": ["Averaging process", " online learning", " regularization", " reproducing kernel Hilbert space", " stochastic approximation"], "paper_abstract": "In this paper, some new probabilistic upper bounds are presented for the online learning algorithm proposed in [1], and more generally for linear stochastic approximations in Hilbert spaces. With these upper bounds not only does one recover almost sure convergence, but also relaxes the square summable condition on the step size appeared in the early work. Furthermore two probabilistic upper bounds are given for an averaging process, both of which achieve the same rate with respect to sample size as in \"batch learning\" algorithms, and one of which is tight in both sample size and regularization parameter.", "paper_title": "On Complexity Issues of Online Learning Algorithms", "paper_id": "WOS:000284419900039"}