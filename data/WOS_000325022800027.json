{"auto_keywords": [{"score": 0.04970377057560368, "phrase": "mapreduce"}, {"score": 0.015237606622822412, "phrase": "big_data"}, {"score": 0.00481495049065317, "phrase": "massive_data_analysis"}, {"score": 0.00462919213721188, "phrase": "huge_amounts"}, {"score": 0.004592905436175987, "phrase": "structured_and_unstructured_data"}, {"score": 0.00443308765153716, "phrase": "ubiquitous_sources"}, {"score": 0.004347540379309049, "phrase": "proposed_algorithm"}, {"score": 0.004261998281333649, "phrase": "massively_parallel_software"}, {"score": 0.004195418010131832, "phrase": "large_number"}, {"score": 0.004081385289516914, "phrase": "recent_programming_model"}, {"score": 0.0040176151419992956, "phrase": "distributed_applications"}, {"score": 0.00320970209394799, "phrase": "workload_distribution"}, {"score": 0.0029547698888028697, "phrase": "data_sampling"}, {"score": 0.0026459927476753585, "phrase": "partitioning_mechanism"}, {"score": 0.0025841336988141235, "phrase": "improved_partitioning_algorithm"}, {"score": 0.0025537472239953807, "phrase": "load_balancing"}, {"score": 0.002533687835358909, "phrase": "memory_consumption"}, {"score": 0.002464709648714996, "phrase": "improved_sampling_algorithm"}, {"score": 0.0022598839683648213, "phrase": "terasort"}, {"score": 0.0021049977753042253, "phrase": "current_implementation"}], "paper_keywords": ["TeraSort", " MapReduce", " Load balance", " Partitioning", " Sampling", " Cloud computing", " Hadoop"], "paper_abstract": "In the era of Big Data, huge amounts of structured and unstructured data are being produced daily by a myriad of ubiquitous sources. Big Data is difficult to work with and requires massively parallel software running on a large number of computers. MapReduce is a recent programming model that simplifies writing distributed applications that handle Big Data. In order for MapReduce to work, it has to divide the workload among computers in a network. Consequently, the performance of MapReduce strongly depends on how evenly it distributes this workload. This can be a challenge, especially in the advent of data skew. In MapReduce, workload distribution depends on the algorithm that partitions the data. One way to avoid problems inherent from data skew is to use data sampling. How evenly the partitioner distributes the data depends on how large and representative the sample is and on how well the samples are analyzed by the partitioning mechanism. This paper proposes an improved partitioning algorithm that improves load balancing and memory consumption. This is done via an improved sampling algorithm and partitioner. To evaluate the proposed algorithm, its performance was compared against a state of the art partitioning mechanism employed by TeraSort. Experiments show that the proposed algorithm is faster, more memory efficient, and more accurate than the current implementation.", "paper_title": "An improved partitioning mechanism for optimizing massive data analysis using MapReduce", "paper_id": "WOS:000325022800027"}