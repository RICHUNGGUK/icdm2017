{"auto_keywords": [{"score": 0.044322505446268326, "phrase": "inertial_sensors"}, {"score": 0.00481495049065317, "phrase": "device_mobile_visual_location_recognition"}, {"score": 0.004430629452265786, "phrase": "city_scale"}, {"score": 0.004333406378146173, "phrase": "mobile_visual_location_recognition"}, {"score": 0.004191540451296395, "phrase": "computer_vision_techniques"}, {"score": 0.004122351497440992, "phrase": "main_contributions"}, {"score": 0.0038998328617557013, "phrase": "efficient_vector_quantization_strategy"}, {"score": 0.0037931018103829427, "phrase": "residual_vector_quantization"}, {"score": 0.003608266640131664, "phrase": "visual_descriptor"}, {"score": 0.003509487090932835, "phrase": "reasonable_searching_accuracy"}, {"score": 0.0033757067564871494, "phrase": "city_scale_image_database"}, {"score": 0.003319939715683536, "phrase": "mobile_devices"}, {"score": 0.002905629026254221, "phrase": "on-device_implementation"}, {"score": 0.002779318608820933, "phrase": "location_recognition_accuracy"}, {"score": 0.002445865715873089, "phrase": "difficult_set"}, {"score": 0.002418829014435841, "phrase": "query_images"}, {"score": 0.00230080652063411, "phrase": "new_benchmark"}, {"score": 0.0022007302738063566, "phrase": "experimental_results"}, {"score": 0.0021285340816566906, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "on-device_mobile_visual_location_recognition_applications"}], "paper_keywords": ["Mobile visual location recognition", " on-device", " vector quantization", " vision and inertial sensors integration"], "paper_abstract": "This paper deals with the problem of city scale on-device mobile visual location recognition by fusing the inertial sensors and computer vision techniques. The main contributions are as follows: Firstly, we design an efficient vector quantization strategy by combining the Transform Coding (TC) and Residual Vector Quantization (RVQ). Our method can compress a visual descriptor into only several bytes while providing reasonable searching accuracy, which makes the managing of city scale image database directly on mobile devices come true. Secondly, we integrate the information from inertial sensors into the Vector of Locally Aggregated Descriptors (VLAD) generation and image similarity evaluation processes. Our method is not only fast enough for on-device implementation, but it also can improve the location recognition accuracy obviously. Thirdly, we also release a set of 1.295 million geo-tagged street view images with the information from inertial sensors, as well as a difficult set of query images. These resources can be used as a new benchmark to facilitate further research in the area. Experimental results prove the validity of the proposed methods for on-device mobile visual location recognition applications.", "paper_title": "On-Device Mobile Visual Location Recognition by Integrating Vision and Inertial Sensors", "paper_id": "WOS:000325811800019"}