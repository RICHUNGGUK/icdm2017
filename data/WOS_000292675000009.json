{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "conjugate_gradient_method"}, {"score": 0.004746263721902201, "phrase": "feedforward_neural_networks"}, {"score": 0.00467855218490191, "phrase": "conjugate_gradient_methods"}, {"score": 0.004546000131069232, "phrase": "real_numerical_experiments"}, {"score": 0.00441718685609235, "phrase": "fast_convergence"}, {"score": 0.004354149483556514, "phrase": "low_memory_requirements"}, {"score": 0.004110831924517065, "phrase": "conjugate_gradient_learning_methods"}, {"score": 0.004052148727852149, "phrase": "backpropagation_neural_networks"}, {"score": 0.003853251284913932, "phrase": "new_learning_algorithm"}, {"score": 0.003798231095121328, "phrase": "almost_cyclic_learning"}, {"score": 0.00374399358035391, "phrase": "neural_networks"}, {"score": 0.0034841647157189985, "phrase": "deterministic_convergence_properties"}, {"score": 0.0030609320300175953, "phrase": "weak_and_strong_convergence"}, {"score": 0.002889672003089062, "phrase": "error_function"}, {"score": 0.0027675275077806744, "phrase": "weight_sequence"}, {"score": 0.0026889720516076205, "phrase": "fixed_point"}, {"score": 0.0025202582748972122, "phrase": "deterministic_convergence_results"}, {"score": 0.002448704249007665, "phrase": "different_learning_modes"}, {"score": 0.0023791769068748194, "phrase": "different_selection_strategies"}, {"score": 0.0022950308685133224, "phrase": "illustrative_numerical_examples"}, {"score": 0.0021979660485319523, "phrase": "theoretical_analysis"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Deterministic convergence", " Conjugate gradient", " Backpropagation", " Feedforward neural networks"], "paper_abstract": "Conjugate gradient methods have many advantages in real numerical experiments, such as fast convergence and low memory requirements. This paper considers a class of conjugate gradient learning methods for backpropagation neural networks with three layers. We propose a new learning algorithm for almost cyclic learning of neural networks based on PRP conjugate gradient method. We then establish the deterministic convergence properties for three different learning modes, i.e., batch mode, cyclic and almost cyclic learning. The two deterministic convergence properties are weak and strong convergence that indicate that the gradient of the error function goes to zero and the weight sequence goes to a fixed point, respectively. It is shown that the deterministic convergence results are based on different learning modes and dependent on different selection strategies of learning rate. Illustrative numerical examples are given to support the theoretical analysis. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Deterministic convergence of conjugate gradient method for feedforward neural networks", "paper_id": "WOS:000292675000009"}