{"auto_keywords": [{"score": 0.0453665111651286, "phrase": "temporary_minima"}, {"score": 0.0053134973060343675, "phrase": "closed-form_solution"}, {"score": 0.00481495049065317, "phrase": "jacobian_eigen-analysis_scheme"}, {"score": 0.004765482708043842, "phrase": "accelerating_learning"}, {"score": 0.004716520738331953, "phrase": "feedforward_neural_networks"}, {"score": 0.004549063619065006, "phrase": "learning_process"}, {"score": 0.004456044928556639, "phrase": "artificial_neural_networks"}, {"score": 0.004081385289516914, "phrase": "previous_works"}, {"score": 0.0038959121068990517, "phrase": "dynamical_system_model"}, {"score": 0.0035866734383189366, "phrase": "hidden_layer"}, {"score": 0.003370907075333389, "phrase": "dynamical_model"}, {"score": 0.0033190180867511605, "phrase": "constrained_optimization_algorithm"}, {"score": 0.003267925215811654, "phrase": "prompt_abandonment"}, {"score": 0.0030239842475439814, "phrase": "constrained_optimization_framework"}, {"score": 0.0028567126120064546, "phrase": "critical_dynamical_system_model_parameters"}, {"score": 0.0026161698049580804, "phrase": "matrix_perturbation_theory"}, {"score": 0.0025361866386341796, "phrase": "previous_work"}, {"score": 0.002420762293555924, "phrase": "present_paper"}, {"score": 0.0023711591828229736, "phrase": "weight_update_rule"}, {"score": 0.00226322835801988, "phrase": "network's_weights"}, {"score": 0.0022053994402287925, "phrase": "computational_complexity"}, {"score": 0.0021049977753042253, "phrase": "simple_back-propagation_weight_update_rule"}], "paper_keywords": ["Supervised learning", " Back-propagation", " Temporary minima", " Dynamical systems", " Jacobian matrix", " Constrained optimization"], "paper_abstract": "An important problem in the learning process when training feedforward artificial neural networks is the occurrence of temporary minima which considerably slows down learning convergence. In a series of previous works, we analyzed this problem by deriving a dynamical system model which is valid in the vicinity of temporary minima caused by redundancy of nodes in the hidden layer. We also demonstrated how to incorporate the characteristics of the dynamical model into a constrained optimization algorithm that allows prompt abandonment of temporary minima and acceleration of learning. In this work, we revisit the constrained optimization framework in order to develop a closed-form solution for the evolution of critical dynamical system model parameters during learning in the vicinity of temporary minima. We show that this formalism is equivalent to matrix perturbation theory which was discussed in a previous work, but that the closed-form solution presented in the present paper allows for a weight update rule which is linear to the number of the network's weights. In terms of computational complexity, this is equivalent to that of the simple back-propagation weight update rule. Simulations demonstrate the computational efficiency and effectiveness of this approach in reducing the time spent in the vicinity of temporary minima as suggested by the analysis.", "paper_title": "Improved Jacobian Eigen-Analysis Scheme for Accelerating Learning in Feedforward Neural Networks", "paper_id": "WOS:000349525500010"}