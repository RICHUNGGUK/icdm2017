{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "expression_recognition"}, {"score": 0.004762893197005439, "phrase": "cross_modal_data_association"}, {"score": 0.004635187610736025, "phrase": "novel_facial_expression_recognition_framework"}, {"score": 0.0045850648862091085, "phrase": "audio-visual_information_analysis"}, {"score": 0.004413846562278824, "phrase": "cross-modality_data_correlation"}, {"score": 0.004225950334493496, "phrase": "asynchronous_streams"}, {"score": 0.004002242925856335, "phrase": "recognition_performance"}, {"score": 0.0038948525410795517, "phrase": "computational_cost"}, {"score": 0.003831801113154043, "phrase": "redundant_or_insignificant_frame_processing"}, {"score": 0.003589602322126282, "phrase": "single_good_image_representation"}, {"score": 0.00349324530379071, "phrase": "weighted_sums"}, {"score": 0.0034554281349858836, "phrase": "registered_face_images"}, {"score": 0.0033262524445013303, "phrase": "auditory_features"}, {"score": 0.003236941658990869, "phrase": "still_image"}, {"score": 0.0031500213134140953, "phrase": "expression_recognition_task"}, {"score": 0.002934764205541183, "phrase": "dynamic_features"}, {"score": 0.0023730978395564116, "phrase": "multi-class_classification_performances"}, {"score": 0.0023219374444738723, "phrase": "subject_dependent_and_independent_strategies"}, {"score": 0.0022472527731744974, "phrase": "multi-class_classification_accuracies"}, {"score": 0.0021987994275190314, "phrase": "previously_published_literature"}, {"score": 0.0021049977753042253, "phrase": "promising_results"}], "paper_keywords": ["Facial expression recognition", " audio-visual expression recognition", " key frames selection", " multi-modal expression recognition", " emotion recognition", " affective computing", " affect analysis"], "paper_abstract": "We present a novel facial expression recognition framework using audio-visual information analysis. We propose to model the cross-modality data correlation while allowing them to be treated as asynchronous streams. We also show that our framework can improve the recognition performance while significantly reducing the computational cost by avoiding redundant or insignificant frame processing by incorporating auditory information. In particular, we design a single good image representation of image sequence by weighted sums of registered face images where the weights are derived using auditory features. We use a still image based technique for the expression recognition task. Our framework, however, can be generalized to work with dynamic features as well. We performed experiments using eNTERFACE' 05 audio-visual emotional database containing six archetypal emotion classes: Happy, Sad, Surprise, Fear, Anger and Disgust. We present one-to-one binary classification as well as multi-class classification performances evaluated using both subject dependent and independent strategies. Furthermore, we compare multi-class classification accuracies with those of previously published literature which use the same database. Our analyses show promising results.", "paper_title": "Face Expression Recognition by Cross Modal Data Association", "paper_id": "WOS:000325811800007"}