{"auto_keywords": [{"score": 0.04350254797212334, "phrase": "thumbnail_scale"}, {"score": 0.04317162283015952, "phrase": "object_completeness"}, {"score": 0.032490691027297694, "phrase": "soas"}, {"score": 0.00481495049065317, "phrase": "image_thumbnailing"}, {"score": 0.004682917599491709, "phrase": "effective_approaches"}, {"score": 0.004590820632326585, "phrase": "input_images"}, {"score": 0.004342430078306461, "phrase": "human_visual_system"}, {"score": 0.004290969772225093, "phrase": "thumbnailing_algorithm"}, {"score": 0.004091130285858113, "phrase": "local_structure_smoothness"}, {"score": 0.003931697356115479, "phrase": "new_thumbnailing_framework"}, {"score": 0.00385431710959668, "phrase": "aware_thumbnailing"}, {"score": 0.0036893794227117194, "phrase": "saliency_measure"}, {"score": 0.0036167509502276294, "phrase": "first_component"}, {"score": 0.0034208822062692127, "phrase": "human_perception"}, {"score": 0.0033668870709083884, "phrase": "visual_acuity_theory"}, {"score": 0.0032099453192629976, "phrase": "\"objectness\"_measurement"}, {"score": 0.0029762350325031207, "phrase": "second_component"}, {"score": 0.0028037753646260937, "phrase": "retargeting_version"}, {"score": 0.002748547368851832, "phrase": "tps"}, {"score": 0.002694375425481759, "phrase": "structure_smoothness"}, {"score": 0.002662393295121998, "phrase": "extended_seam_carving_algorithm"}, {"score": 0.0026203386550421558, "phrase": "sample_control_points"}, {"score": 0.0025892330566358503, "phrase": "tps_model_estimation"}, {"score": 0.002558495760339272, "phrase": "cropping_version"}, {"score": 0.00252812242767189, "phrase": "cropping_window"}, {"score": 0.002488183412451029, "phrase": "spatial_efficiency"}, {"score": 0.002468450551616404, "phrase": "soas-based_content_preservation"}, {"score": 0.0023069016803337365, "phrase": "browsing_efficiency"}, {"score": 0.0022434880661725493, "phrase": "subject_preference"}, {"score": 0.0021731423856364003, "phrase": "retargetme_dataset"}, {"score": 0.0021218313648947926, "phrase": "soat"}, {"score": 0.0021049977753042253, "phrase": "promising_performances"}], "paper_keywords": ["Image thumbnailing", " Image retargeting", " Thumbnail cropping", " Contrast sensitivity function", " Seam carving", " Thin plate spline"], "paper_abstract": "In this paper we study effective approaches to create thumbnails from input images. Since a thumbnail will eventually be presented to and perceived by a human visual system, a thumbnailing algorithm should consider several important issues in the process including thumbnail scale, object completeness and local structure smoothness. To address these issues, we propose a new thumbnailing framework named scale and object aware thumbnailing (SOAT), which contains two components focusing respectively on saliency measure and thumbnail warping/cropping. The first component, named scale and object aware saliency (SOAS), models the human perception of thumbnails using visual acuity theory, which takes thumbnail scale into consideration. In addition, the \"objectness\" measurement (Alexe et al. 2012) is integrated in SOAS, as to preserve object completeness. The second component uses SOAS to guide the thumbnailing based on either retargeting or cropping. The retargeting version uses the thin-plate-spline (TPS) warping for preserving structure smoothness. An extended seam carving algorithm is developed to sample control points used for TPS model estimation. The cropping version searches a cropping window that balances the spatial efficiency and SOAS-based content preservation. The proposed algorithms were evaluated in three experiments: a quantitative user study to evaluate thumbnail browsing efficiency, a quantitative user study for subject preference, and a qualitative study on the RetargetMe dataset. In all studies, SOAT demonstrated promising performances in comparison with state-of-the-art algorithms.", "paper_title": "Scale and Object Aware Image Thumbnailing", "paper_id": "WOS:000322251800002"}