{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "kaggle_algorithmic_trading_challenge"}, {"score": 0.004185108816866677, "phrase": "winning_solution"}, {"score": 0.003974349782102708, "phrase": "analysis_challenge"}, {"score": 0.0034034331737042363, "phrase": "empirical_predictive_models"}, {"score": 0.0033288399506044763, "phrase": "stock_market_prices"}, {"score": 0.0032558762499901727, "phrase": "liquidity_shock"}, {"score": 0.0031845067058084613, "phrase": "winning_system"}, {"score": 0.003091767402370757, "phrase": "optimal_composition"}, {"score": 0.0029796206196589115, "phrase": "feature_extraction"}, {"score": 0.002935905915691203, "phrase": "selection_strategy"}, {"score": 0.002850385670169814, "phrase": "random_forest"}, {"score": 0.0027878797103973313, "phrase": "modeling_technique"}, {"score": 0.0025892330566358503, "phrase": "optimal_feature"}, {"score": 0.0025137856138000036, "phrase": "modeling_approach"}, {"score": 0.0024405312482831646, "phrase": "highly_complex_data"}, {"score": 0.002404706573379854, "phrase": "low_maximal_information_coefficients"}, {"score": 0.0023519508354777215, "phrase": "dependent_variable"}], "paper_keywords": ["Kaggle challenge", " model architecture", " boosting", " feature selection", " high frequency trading", " liquidity shock", " maximal information coefficient"], "paper_abstract": "This letter presents the ideas and methods of the winning solution* for the Kaggle Algorithmic Trading Challenge. This analysis challenge took place between 11th November 2011 and 8th January 2012, and 264 competitors submitted solutions. The objective of this competition was to develop empirical predictive models to explain stock market prices following a liquidity shock. The winning system builds upon the optimal composition of several models and a feature extraction and selection strategy. We used Random Forest as a modeling technique to train all sub-models as a function of an optimal feature set. The modeling approach can cope with highly complex data having low Maximal Information Coefficients between the dependent variable and the feature set and provides a feature ranking metric which we used in our feature selection algorithm.", "paper_title": "Winning the Kaggle Algorithmic Trading Challenge with the Composition of Many Models and Feature Engineering", "paper_id": "WOS:000320214300045"}