{"auto_keywords": [{"score": 0.04439952284800597, "phrase": "proposed_approach"}, {"score": 0.00481495049065317, "phrase": "visual_scene_analysis"}, {"score": 0.004589763483242731, "phrase": "improved_approach"}, {"score": 0.004508048086020253, "phrase": "visually_salient_regions"}, {"score": 0.004322967486451877, "phrase": "known_visual_search_task"}, {"score": 0.0041703614123283165, "phrase": "robust_model"}, {"score": 0.004120694410890683, "phrase": "instantaneous_visual_attention"}, {"score": 0.0038119123317396954, "phrase": "pixel_probability_map"}, {"score": 0.0037216229812089686, "phrase": "automatic_detection"}, {"score": 0.0036553070575015344, "phrase": "previously-seen_object"}, {"score": 0.0031277316566463978, "phrase": "viewpoint-invariant_spatial_distribution"}, {"score": 0.0030904433394138963, "phrase": "speeded_up_robust_features"}, {"score": 0.002963390095037466, "phrase": "bottom-up_and_top-down_object_probability_images"}, {"score": 0.0028586409810976367, "phrase": "task-dependent_saliency_map"}, {"score": 0.002724696729311532, "phrase": "observer_eye-tracker_data"}, {"score": 0.0026760974046034854, "phrase": "object_search-and-count_tasking"}, {"score": 0.0025660347613972573, "phrase": "true_attention_areas"}, {"score": 0.0024901940672239784, "phrase": "bottom-up_saliency"}, {"score": 0.0024311346712789553, "phrase": "new_combined_saliency_map"}, {"score": 0.0023311233166534214, "phrase": "new_intelligent_compression_technique"}, {"score": 0.0022486729928886885, "phrase": "discrete_cosine_transform"}, {"score": 0.0022218700984162226, "phrase": "dct"}, {"score": 0.0021049977753042253, "phrase": "surveillance-style_footage"}], "paper_keywords": ["Visualization", " DCT", " image compression", " scene analysis"], "paper_abstract": "This paper presents an improved approach for indicating visually salient regions of an image based upon a known visual search task. The proposed approach employs a robust model of instantaneous visual attention (i.e., \"bottbm-up') combined with a pixel probability map derived from the automatic detection of a previously-seen object (task-dependent i.e., \"top-down\"). The objects to be recognized are parameterized quickly in advance by a viewpoint-invariant spatial distribution of Speeded Up Robust Features (SURF) interest-points. The bottom-up and top-down object probability images are fused to produce a task-dependent saliency map. The proposed approach is validated using observer eye-tracker data collected under object search-and-count tasking. Proposed approach shows 13% higher overlap with true attention areas under task compared to bottom-up saliency alone. The new combined saliency map is further used to develop a new intelligent compression technique which is an extension of Discrete Cosine Transform (DCT) encoding. The proposed approach is demonstrated on surveillance-style footage throughout.", "paper_title": "An Intelligent Model for Visual Scene Analysis and Compression", "paper_id": "WOS:000316841900003"}