{"auto_keywords": [{"score": 0.04847493599744397, "phrase": "large_scale_systems"}, {"score": 0.00481495049065317, "phrase": "performance_evaluation"}, {"score": 0.004733625412289433, "phrase": "group_communication_architectures"}, {"score": 0.004575053987970206, "phrase": "mpi._group_communication"}, {"score": 0.004459605573798577, "phrase": "important_paradigm"}, {"score": 0.0043842558719324526, "phrase": "fault_tolerance"}, {"score": 0.0037287578312533596, "phrase": "separate_clusters"}, {"score": 0.0036037265181631324, "phrase": "theoretical_performance_bounds"}, {"score": 0.0033660587983313536, "phrase": "mpi_group_communication_primitives"}, {"score": 0.0032255172568437965, "phrase": "time_bounds"}, {"score": 0.00317095235382607, "phrase": "multicast_message_deliveries"}, {"score": 0.0027194778713860715, "phrase": "multicast_message_delivery_times"}, {"score": 0.002372298201966367, "phrase": "multicast_message"}, {"score": 0.0022731540640627307, "phrase": "daisy_architecture"}, {"score": 0.0021412594173592513, "phrase": "delivery_times"}, {"score": 0.0021049977753042253, "phrase": "message_sizes"}], "paper_keywords": [""], "paper_abstract": "Group communication is an important paradigm for fault tolerance in large scale systems. We describe various group architectures as pipelined, hierarchical, daisy and hypercube groups each consisting of separate clusters, investigate the theoretical performance bounds of these architectures and evaluate their experimental performances using MPI group communication primitives. We first derive time bounds for multicast message deliveries in these architectures and then provide tests to measure the times taken for the same operation. The multicast message delivery times are tested against the number of clusters within a group and the size of the multicast message. We conclude that daisy architecture is favorable both in terms of delivery times and message sizes theoretically and experimentally.", "paper_title": "Performance evaluation of group communication architectures in large scale systems using MPI", "paper_id": "WOS:000243318600028"}