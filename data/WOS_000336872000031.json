{"auto_keywords": [{"score": 0.03600704009648378, "phrase": "dbn"}, {"score": 0.00481495049065317, "phrase": "rate_distortion_theory"}, {"score": 0.004765173195920904, "phrase": "deep_belief_networks"}, {"score": 0.004594951819987416, "phrase": "dominant_technique"}, {"score": 0.004500414250942443, "phrase": "architectural_depth"}, {"score": 0.004250302819248558, "phrase": "greedy_layer-wise_unsupervised_learning_manner"}, {"score": 0.004098397888277889, "phrase": "narrow_hidden_bottleneck"}, {"score": 0.003850464100878052, "phrase": "rate_distortion"}, {"score": 0.003693533648584145, "phrase": "original_data"}, {"score": 0.0033285306462537884, "phrase": "sparse-response_dbn"}, {"score": 0.003143341238802547, "phrase": "kullback-leibler_divergence"}, {"score": 0.0030151432267065815, "phrase": "equilibrium_distribution"}, {"score": 0.0029530125680590413, "phrase": "building_block"}, {"score": 0.002847339938387273, "phrase": "distortion_function"}, {"score": 0.0027886573969044042, "phrase": "sparse_response_regularization"}, {"score": 0.002633425544817301, "phrase": "small_code_rate"}, {"score": 0.00253916011199984, "phrase": "different_scale_image_datasets"}, {"score": 0.002435542816459651, "phrase": "small_rate"}, {"score": 0.002372935208038141, "phrase": "multiple_levels"}, {"score": 0.0022879728308318205, "phrase": "cortical_hierarchy"}, {"score": 0.0022178115837423905, "phrase": "pca"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Deep belief network", " Kullback-Leibler divergence", " Information entropy", " Rate distortion theory", " Unsupervised feature learning"], "paper_abstract": "Deep belief networks (DBNs) are currently the dominant technique for modeling the architectural depth of brain, and can be trained efficiently in a greedy layer-wise unsupervised learning manner. However, DBNs without a narrow hidden bottleneck typically produce redundant, continuous-valued codes and unstructured weight patterns. Taking inspiration from rate distortion (RD) theory, which encodes original data using as few bits as possible, we introduce in this paper a variant of DBN, referred to as sparse-response DBN (SR-DBN). In this approach, Kullback-Leibler divergence between the distribution of data and the equilibrium distribution defined by the building block of DBN is considered as a distortion function, and the sparse response regularization induced by L-1-norm of codes is used to achieve a small code rate. Several experiments by extracting features from different scale image datasets show that our approach SR-DBN learns codes with small rate, extracts features at multiple levels of abstraction mimicking computations in the cortical hierarchy, and obtains more discriminative representation than PCA and several basic algorithms of DBNs. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "A sparse-response deep belief network based on rate distortion theory", "paper_id": "WOS:000336872000031"}