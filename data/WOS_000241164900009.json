{"auto_keywords": [{"score": 0.02426819892373926, "phrase": "identification_rate"}, {"score": 0.024025707949948398, "phrase": "audio-visual_system"}, {"score": 0.00481495049065317, "phrase": "audio-visual_speaker_identification"}, {"score": 0.00478218760286811, "phrase": "dynamic_facial_muscle_model"}, {"score": 0.004717325943509345, "phrase": "human_identification"}, {"score": 0.004685224205630963, "phrase": "physiological_characteristics"}, {"score": 0.004590217765551337, "phrase": "great_concern"}, {"score": 0.004558977045730675, "phrase": "security_systems"}, {"score": 0.004497129150551236, "phrase": "robust_multimodal_identification_systems"}, {"score": 0.0044512920090463105, "phrase": "audio-visual_information"}, {"score": 0.004185874747231179, "phrase": "model-based_feature_extraction_method"}, {"score": 0.004100952337610039, "phrase": "facial_muscles"}, {"score": 0.004073027900718392, "phrase": "lip_movements"}, {"score": 0.003990385737207714, "phrase": "intrinsic_properties"}, {"score": 0.003778081566896723, "phrase": "dynamic_lip_model"}, {"score": 0.003663645143713398, "phrase": "neuro-muscular_properties"}, {"score": 0.0035648266669315943, "phrase": "valid_speakers"}, {"score": 0.0034924598001567944, "phrase": "large_extent"}, {"score": 0.0032952735750239924, "phrase": "audio_and_video_features"}, {"score": 0.0032173412933955117, "phrase": "multistream_pseudo-synchronized_hmm_training_method"}, {"score": 0.0031305230217636495, "phrase": "mel-frequency_cepstral_coefficients"}, {"score": 0.003004655258223784, "phrase": "relative_spectra_perceptual_linear_prediction"}, {"score": 0.0028349323966217386, "phrase": "multimodal_system"}, {"score": 0.002767856819656175, "phrase": "superior_performance"}, {"score": 0.0027395968397212053, "phrase": "proposed_system"}, {"score": 0.002693134880651768, "phrase": "large_multispeaker_database"}, {"score": 0.0026747708876640377, "phrase": "continuously_spoken_digits"}, {"score": 0.002472352057126189, "phrase": "genetically_identical_twins"}, {"score": 0.002422108787774162, "phrase": "speaker_voice"}, {"score": 0.002389180450772203, "phrase": "drug_inhalation_tests"}, {"score": 0.0023486473525617794, "phrase": "noise_ratio"}, {"score": 0.002300912218086077, "phrase": "dynamic_muscle_model"}, {"score": 0.0021932609696277937, "phrase": "identical_twins"}, {"score": 0.002148677187261496, "phrase": "apparent_improvement"}, {"score": 0.0021049977753042253, "phrase": "dynamic_muscle_model-based_system"}], "paper_keywords": ["biometry", " speaker identification", " multistream hidden Markov models", " visual feature extraction"], "paper_abstract": "Science of human identification using physiological characteristics or biometry has been of great concern in security systems. However, robust multimodal identification systems based on audio-visual information has not been thoroughly investigated yet. Therefore, the aim of this work to propose a model-based feature extraction method which employs physiological characteristics of facial muscles producing lip movements. This approach adopts the intrinsic properties of muscles such as viscosity, elasticity, and mass which are extracted from the dynamic lip model. These parameters are exclusively dependent on the neuro-muscular properties of speaker; consequently, imitation of valid speakers could be reduced to a large extent. These parameters are applied to a hidden Markov model (HMM) audio-visual identification system. In this work, a combination of audio and video features has been employed by adopting a multistream pseudo-synchronized HMM training method. Noise robust audio features such as Mel-frequency cepstral coefficients (MFCC), spectral subtraction (SS), and relative spectra perceptual linear prediction (J-RASTA-PLP) have been used to evaluate the performance of the multimodal system once efficient audio feature extraction methods have been utilized. The superior performance of the proposed system is demonstrated on a large multispeaker database of continuously spoken digits, along with a sentence that is phonetically rich. To evaluate the robustness of algorithms, some experiments were performed on genetically identical twins. Furthermore, changes in speaker voice were simulated with drug inhalation tests. In 3 dB signal to noise ratio (SNR), the dynamic muscle model improved the identification rate of the audio-visual system from 91 to 98%. Results on identical twins revealed that there was an apparent improvement on the performance for the dynamic muscle model-based system, in which the identification rate of the audio-visual system was enhanced from 87 to 96%.", "paper_title": "Performance enhancement for audio-visual speaker identification using dynamic facial muscle model", "paper_id": "WOS:000241164900009"}