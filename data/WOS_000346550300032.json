{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "search_targets"}, {"score": 0.04805001769300636, "phrase": "search_target"}, {"score": 0.03548517282140461, "phrase": "target_pattern"}, {"score": 0.032130598823801926, "phrase": "target_inference"}, {"score": 0.0045748683993128425, "phrase": "fixation_behavior"}, {"score": 0.004539005582217309, "phrase": "visual_search"}, {"score": 0.004278807087784626, "phrase": "visual_features"}, {"score": 0.003787191644093659, "phrase": "available_eye_movement_data"}, {"score": 0.0036987521452843987, "phrase": "target_objects"}, {"score": 0.003584025547693833, "phrase": "random-dot_search_paradigm"}, {"score": 0.003247868216333148, "phrase": "previously_employed_method"}, {"score": 0.00320970209394799, "phrase": "sufficient_power"}, {"score": 0.0031595084685030845, "phrase": "interface_control"}, {"score": 0.003097865379563127, "phrase": "current_data"}, {"score": 0.0030614565460256897, "phrase": "principal_limitation"}, {"score": 0.0029781530607129653, "phrase": "interface_design"}, {"score": 0.00281826552772148, "phrase": "observers'_eye_movements"}, {"score": 0.0027200305495826797, "phrase": "third_experiment"}, {"score": 0.002594342451017824, "phrase": "natural_scenes"}, {"score": 0.002573965014439041, "phrase": "pattern_classifiers"}, {"score": 0.0025537472239953807, "phrase": "classic_computer_vision"}, {"score": 0.002464709648714996, "phrase": "compelling_inferential_algorithms"}, {"score": 0.002426137929553722, "phrase": "new_generation"}, {"score": 0.002224510044064184, "phrase": "broader_perspective"}, {"score": 0.0021724831184390192, "phrase": "efficient_intent_decoding"}, {"score": 0.002155412043665635, "phrase": "eye_movements"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Visual attention", " Eye movements", " Visual search", " Intent decoding", " Mind reading", " Yarbus"], "paper_abstract": "We address the question of inferring the search target from fixation behavior in visual search. Such inference is possible since during search, our attention and gaze are guided toward visual features similar to those in the search target. We strive to answer two fundamental questions: what are the most powerful algorithmic principles for this task, and how does their performance depend on the amount of available eye movement data and the complexity of the target objects? In the first two experiments, we choose a random-dot search paradigm to eliminate contextual influences on search. We present an algorithm that correctly infers the target pattern up to 50 times as often as a previously employed method and promises sufficient power and robustness for interface control. Moreover, the current data suggest a principal limitation of target inference that is crucial for interface design: if the target pattern exceeds a certain spatial complexity level, only a subpattern tends to guide the observers' eye movements, which drastically impairs target inference. In the third experiment, we show that it is possible to predict search targets in natural scenes using pattern classifiers and classic computer vision features significantly above chance. The availability of compelling inferential algorithms could initiate a new generation of smart, gaze-controlled interfaces and wearable visual technologies that deduce from their users' eye movements the visual information for which they are looking. In a broader perspective, our study shows directions for efficient intent decoding from eye movements. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "What do eyes reveal about the mind? Algorithmic inference of search targets from fixations", "paper_id": "WOS:000346550300032"}