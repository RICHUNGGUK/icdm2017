{"auto_keywords": [{"score": 0.04843318154064616, "phrase": "citation_data"}, {"score": 0.015719716506582538, "phrase": "information_science"}, {"score": 0.014989395382472345, "phrase": "expert_judgments"}, {"score": 0.011308833669937996, "phrase": "scopus"}, {"score": 0.00882623781510355, "phrase": "gs_data"}, {"score": 0.004710854091689325, "phrase": "data_sources"}, {"score": 0.004431149592844886, "phrase": "peer_review"}, {"score": 0.004392565073466714, "phrase": "citation_indicators"}, {"score": 0.00433531459712787, "phrase": "research_quality"}, {"score": 0.004167982855788402, "phrase": "forty-two_lis_experts"}, {"score": 0.0038862297330905836, "phrase": "median_rankings"}, {"score": 0.003703611021999795, "phrase": "g-_and_h-index_values"}, {"score": 0.00342311944859913, "phrase": "google"}, {"score": 0.0032764307492734145, "phrase": "basic_h-index"}, {"score": 0.0032054891061528896, "phrase": "peer_judgment"}, {"score": 0.0029239403476691147, "phrase": "gs"}, {"score": 0.0027741844455102206, "phrase": "wos"}, {"score": 0.0027140798043053986, "phrase": "carefully_cleaned_version"}, {"score": 0.002508284945907166, "phrase": "citation_databases"}, {"score": 0.002475532161577715, "phrase": "broadly_similar_rankings"}, {"score": 0.002443206010337041, "phrase": "lis_academics"}, {"score": 0.0024113009621042677, "phrase": "disadvantaged_researchers"}, {"score": 0.002328238264850759, "phrase": "wos_disadvantaged_researchers"}, {"score": 0.0022777800735119405, "phrase": "information_retrieval"}, {"score": 0.002218706081432745, "phrase": "uk"}, {"score": 0.0021801135036476136, "phrase": "uk_academics"}, {"score": 0.0021610874773389096, "phrase": "higher_scores"}], "paper_keywords": ["Expert judgments", " g-index", " h-index", " H-index", " Library and information science", " Peer review"], "paper_abstract": "This paper studies the correlations between peer review and citation indicators when evaluating research quality in library and information science (LIS). Forty-two LIS experts provided judgments on a 5-point scale of the quality of research published by 101 scholars; the median rankings resulting from these judgments were then correlated with h-, g- and H-index values computed using three different sources of citation data: Web of Science (WoS), Scopus and Google Scholar (GS). The two variants of the basic h-index correlated more strongly with peer judgment than did the h-index itself; citation data from Scopus was more strongly correlated with the expert judgments than was data from GS, which in turn was more strongly correlated than data from WoS; correlations from a carefully cleaned version of GS data were little different from those obtained using swiftly gathered GS data; the indices from the citation databases resulted in broadly similar rankings of the LIS academics; GS disadvantaged researchers in bibliometrics compared to the other two citation database while WoS disadvantaged researchers in the more technical aspects of information retrieval; and experts from the UK and other European countries rated UK academics with higher scores than did experts from the USA. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Ranking of library and information science researchers: Comparison of data sources for correlating citation data, and expert judgments", "paper_id": "WOS:000281616200010"}