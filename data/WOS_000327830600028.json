{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "finger_control"}, {"score": 0.004717960684395242, "phrase": "cad_application"}, {"score": 0.0046797114400479135, "phrase": "natural_and_intuitive_interfaces"}, {"score": 0.004641835656360175, "phrase": "cad"}, {"score": 0.004175533280178052, "phrase": "actual_applications"}, {"score": 0.004008897491380646, "phrase": "conventional_mouse_interface"}, {"score": 0.003960210592458847, "phrase": "user_physical_fatigue"}, {"score": 0.0039280803011236395, "phrase": "long_periods"}, {"score": 0.003755949722774176, "phrase": "improved_gesture_control_interface"}, {"score": 0.003680212979775528, "phrase": "conventional_interface_level_usability"}, {"score": 0.0036503460615862574, "phrase": "low_user_fatigue"}, {"score": 0.0035913348026306306, "phrase": "high_level"}, {"score": 0.0032700071110068323, "phrase": "multi-modal_control_interface_gafinc"}, {"score": 0.0031393948330209224, "phrase": "precise_hand_positions"}, {"score": 0.0030386598795433474, "phrase": "independent_gaze"}, {"score": 0.0028583875909257766, "phrase": "gafinc"}, {"score": 0.0028121429464519733, "phrase": "manipulation_accuracy"}, {"score": 0.00266693888510356, "phrase": "conventional_mouse"}, {"score": 0.002613103872300205, "phrase": "intuitiveness_level"}, {"score": 0.002539550974361945, "phrase": "user_interviews"}, {"score": 0.0024580159789774516, "phrase": "gafinc_interface"}, {"score": 0.002438043578514396, "phrase": "insufficient_performance"}, {"score": 0.002321563536990284, "phrase": "applicable_level_performance"}, {"score": 0.00221063611719246, "phrase": "mouse_interface"}, {"score": 0.0021748483128942687, "phrase": "usable_level"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["User interface", " Multi-modal", " Gaze tracking", " Hands tracking", " Gesture recognition", " Manipulation"], "paper_abstract": "Natural and intuitive interfaces for CAD modeling such as hand gesture controls have received a lot of attention recently. However, in spite of its high intuitiveness and familiarity, their use for actual applications has been found to be less comfortable than a conventional mouse interface because of user physical fatigue over long periods of operation. In this paper, we propose an improved gesture control interface for 3D modeling manipulation tasks that possesses conventional interface level usability with low user fatigue while maintaining a high level of intuitiveness. By analyzing problems associated with previous hand gesture controls in translation, rotation and zooming, we developed a multi-modal control interface GaFinC: Gaze and Finger Control interface. GaFinC can track precise hand positions, recognizes several finger gestures, and utilizes an independent gaze pointing interface for setting the point of interest. To verify the performance of GaFinC, tests of manipulation accuracy and time are conducted and their results are compared with those of a conventional mouse. The comfort and intuitiveness level are also scored by means of user interviews. As a result, although the GaFinC interface posted insufficient performance in accuracy and times compared with a mouse, it shows applicable level performance. Also users found it to be more intuitive than a mouse interface while maintaining a usable level of comfort. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "GaFinC: Gaze and Finger Control interface for 3D model manipulation in CAD application", "paper_id": "WOS:000327830600028"}