{"auto_keywords": [{"score": 0.04963419481547922, "phrase": "distributed_evolutionary_algorithms"}, {"score": 0.044786117849615066, "phrase": "pi-sigma_networks"}, {"score": 0.00481495049065317, "phrase": "hardware-friendly_higher-order_neural_network_training"}, {"score": 0.00453144352814823, "phrase": "higher-order_neural_networks"}, {"score": 0.003976007772514762, "phrase": "pi-sigma_neural_networks"}, {"score": 0.0038661089070527424, "phrase": "distributed_versions"}, {"score": 0.003504756881063581, "phrase": "potential_solutions"}, {"score": 0.0033761303704612734, "phrase": "parallel_and_occasional_migration"}, {"score": 0.0032219438233946312, "phrase": "proposed_approach"}, {"score": 0.0031182092708983184, "phrase": "threshold_activation_functions"}, {"score": 0.002948066398723126, "phrase": "narrow_band"}, {"score": 0.002722758109991685, "phrase": "trained_pi-sigma_neural_networks"}, {"score": 0.002550165781755603, "phrase": "real_weight_ones"}, {"score": 0.0025264171945423254, "phrase": "hardware_implementation"}, {"score": 0.0024336086025284836, "phrase": "low_amplitude_noise"}, {"score": 0.00237733844863506, "phrase": "training_data"}, {"score": 0.0023551955270737215, "phrase": "experimental_results"}, {"score": 0.002311525062244175, "phrase": "proposed_training_process"}, {"score": 0.0022058510091498666, "phrase": "trained_pi-sigma_networks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Pi-Sigma Networks", " Distributed Differential Evolution", " Distributed Particle Swarm Optimization", " Back-propagation Neural Networks", " Integer Weight Neural Networks", " Threshold activation functions", " \"Hardware-Friendly'' Implementations", " 'On-chip' training", " Higher-Order Neural Networks"], "paper_abstract": "In this paper, we study the class of Higher-Order Neural Networks and especially the Pi-Sigma Networks. The performance of Pi-Sigma Networks is evaluated through several well known Neural Network Training benchmarks. In the experiments reported here, Distributed Evolutionary Algorithms are implemented for Pi-Sigma neural networks training. More specifically the distributed versions of the Differential Evolution and the Particle Swarm Optimization algorithms have been employed. To this end, each processor is assigned a subpopulation of potential solutions. The subpopulations are independently evolved in parallel and occasional migration is employed to allow cooperation between them. The proposed approach is applied to train Pi-Sigma Networks using threshold activation functions. Moreover, the weights and biases were confined to a narrow band of integers, constrained in the range [-32; 32]. Thus, the trained Pi-Sigma neural networks can be represented by using 6 bits. Such networks are better suited than the real weight ones for hardware implementation and to some extend are immune to low amplitude noise that possibly contaminates the training data. Experimental results suggest that the proposed training process is fast, stable and reliable and the distributed trained Pi-Sigma Networks exhibited good generalization capabilities. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Hardware-friendly Higher-Order Neural Network Training using Distributed Evolutionary Algorithms", "paper_id": "WOS:000272206500006"}