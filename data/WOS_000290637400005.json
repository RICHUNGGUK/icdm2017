{"auto_keywords": [{"score": 0.04704420838626198, "phrase": "markov_boundaries"}, {"score": 0.010122289046667168, "phrase": "matroid_theory"}, {"score": 0.00481495049065317, "phrase": "markov_boundary_induction"}, {"score": 0.004758053266234474, "phrase": "bayesian_networks"}, {"score": 0.00430067200637091, "phrase": "bayesian_network"}, {"score": 0.004125314447335291, "phrase": "feature_subset_selection_problem"}, {"score": 0.0038870860864224656, "phrase": "wide_applications"}, {"score": 0.0038411109150557504, "phrase": "ai_techniques"}, {"score": 0.003795677450778112, "phrase": "popular_constraint"}, {"score": 0.0037731618765688584, "phrase": "based_methods"}, {"score": 0.003706410380455668, "phrase": "high_computational_complexity"}, {"score": 0.0035341028694769036, "phrase": "high_dimensionality"}, {"score": 0.0034304883989739804, "phrase": "new_perspective"}, {"score": 0.003251552926963083, "phrase": "random_variable"}, {"score": 0.0031003255054939524, "phrase": "learning_algorithm"}, {"score": 0.002991524908432975, "phrase": "true_markov_boundaries"}, {"score": 0.002938560664857691, "phrase": "greedy_learning_algorithm"}, {"score": 0.0028354207806133153, "phrase": "precision_matrix"}, {"score": 0.0027852126030055305, "phrase": "original_distribution"}, {"score": 0.002608583927848071, "phrase": "large_scale_problems"}, {"score": 0.002472412920623605, "phrase": "probabilistic_relations"}, {"score": 0.0024431613662843628, "phrase": "gaussians"}, {"score": 0.002385595105638277, "phrase": "possible_variables"}, {"score": 0.002329412930430366, "phrase": "low_computational_complexity"}, {"score": 0.0023018188296933923, "phrase": "experimental_results"}, {"score": 0.0022745508619577927, "phrase": "standard_bayesian_networks"}, {"score": 0.0021049977753042253, "phrase": "complex_networks"}], "paper_keywords": ["Markov boundary", " conditional independence", " matroid"], "paper_abstract": "Learning Markov boundaries from data without having to learn a Bayesian network first can be viewed as a feature subset selection problem and has received much attention due to its significance in the wide applications of AI techniques. Popular constraint based methods suffer from high computational complexity and are usually unstable in spaces of high dimensionality. We propose a new perspective from matroid theory towards the discovery of Markov boundaries of random variable in the domain, and develop a learning algorithm which guarantees to recover the true Markov boundaries by a greedy learning algorithm. Then we use the precision matrix of the original distribution as a measure of independence to make our algorithm feasible in large scale problems, which is essentially an approximation of the probabilistic relations with Gaussians and can find possible variables in Markov boundaries with low computational complexity. Experimental results on standard Bayesian networks show that our analysis and approximation can efficiently and accurately identify Markov boundaries in complex networks from data.", "paper_title": "Analysis of Markov Boundary Induction in Bayesian Networks: A New View From Matroid Theory", "paper_id": "WOS:000290637400005"}