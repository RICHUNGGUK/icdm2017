{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "convex_quadratic_programming_problems"}, {"score": 0.004672444473700524, "phrase": "k-winners-take-all_problem"}, {"score": 0.004511485198940203, "phrase": "new_recurrent_neural_network"}, {"score": 0.004399905285945236, "phrase": "convex_quadratic_programming"}, {"score": 0.004227064760262924, "phrase": "existing_neural_networks"}, {"score": 0.004164007198843467, "phrase": "proposed_one"}, {"score": 0.004122490120202936, "phrase": "global_convergence_property"}, {"score": 0.004081385289516914, "phrase": "weak_conditions"}, {"score": 0.0038819041309353024, "phrase": "matrix_inverse"}, {"score": 0.003766908559075843, "phrase": "competitive_alternative"}, {"score": 0.0037106898330901534, "phrase": "neural_network_family"}, {"score": 0.0036188435232325337, "phrase": "quadratic_programming_problems"}, {"score": 0.003424677513304057, "phrase": "variable_substitution"}, {"score": 0.0033735490732324713, "phrase": "proposed_network"}, {"score": 0.00327356326590461, "phrase": "existing_model"}, {"score": 0.0032246835734680377, "phrase": "minimax_problems"}, {"score": 0.003021141590548526, "phrase": "special_case"}, {"score": 0.002976019715894587, "phrase": "minimax_neural_network"}, {"score": 0.002534596206835737, "phrase": "simple_structure"}, {"score": 0.002509284173541882, "phrase": "global_convergence"}, {"score": 0.0024105326885334962, "phrase": "ill_cases"}, {"score": 0.0023864566654035924, "phrase": "numerical_simulations"}, {"score": 0.002315658491366464, "phrase": "theoretical_results"}, {"score": 0.00223570488212345, "phrase": "network_design_method"}, {"score": 0.0021693692831601745, "phrase": "great_potential"}], "paper_keywords": ["Asymptotic stability", " k-winners-take-all (k-WTA)", " linear programming", " neural network", " quadratic programming"], "paper_abstract": "In this paper, a new recurrent neural network is proposed for solving convex quadratic programming (QP) problems. Compared with existing neural networks, the proposed one features global convergence property under weak conditions, low structural complexity, and no calculation of matrix inverse. It serves as a competitive alternative in the neural network family for solving linear or quadratic programming problems. In addition, it is found that by some variable substitution, the proposed network turns out to be an existing model for solving minimax problems. In this sense, it can be also viewed as a special case of the minimax neural network. Based on this scheme, a k-winners-take-all (k-WTA) network with O (n) complexity is designed, which is characterized by simple structure, global convergence, and capability to deal with some ill cases. Numerical simulations are provided to validate the theoretical results obtained. More importantly, the network design method proposed in this paper has great potential to inspire other competitive inventions along the same line.", "paper_title": "A New Recurrent Neural Network for Solving Convex Quadratic Programming Problems With an Application to the k-Winners-Take-All Problem", "paper_id": "WOS:000265376200008"}