{"auto_keywords": [{"score": 0.04523828542897123, "phrase": "spanish"}, {"score": 0.013662909268269364, "phrase": "target_language"}, {"score": 0.004814959429716252, "phrase": "miracle"}, {"score": 0.00384731849108269, "phrase": "six_runs"}, {"score": 0.003500313767208306, "phrase": "source_languages"}, {"score": 0.0033388087048811825, "phrase": "english"}, {"score": 0.0032607133098446176, "phrase": "italian"}, {"score": 0.0030735451757603555, "phrase": "simple_representation"}, {"score": 0.0027961342030212353, "phrase": "semantic_information"}, {"score": 0.0027307746993761035, "phrase": "typed_named_entities"}, {"score": 0.0026045914281028473, "phrase": "different_strategies"}, {"score": 0.0025436978068459565, "phrase": "answer_extraction"}, {"score": 0.0021049977753042253, "phrase": "answer_selection"}], "paper_keywords": [""], "paper_abstract": "Our second participation in CLEF-QA consited in six runs with Spanish as a target language. The source languages were Spanish, English an Italian. miraQA uses a simple representation of the question that is enriched with semantic information like typed Named Entities. Runs used different strategies for answer extraction and selection, achieving at best a 25'5% accuracy. The analysis of the errors suggests that improvements in answer selection are the most critical.", "paper_title": "MIRACLE's cross-lingual question answering experiments with Spanish as a target language", "paper_id": "WOS:000241359000054"}