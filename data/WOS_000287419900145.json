{"auto_keywords": [{"score": 0.04795274928516732, "phrase": "kdd_cup"}, {"score": 0.00481495049065317, "phrase": "multiple_class_datasets"}, {"score": 0.004240116703465252, "phrase": "new_method"}, {"score": 0.0035677736570659813, "phrase": "performance_results"}, {"score": 0.003378385493203297, "phrase": "significantly_reduced_set"}, {"score": 0.002947611960613497, "phrase": "multiple_class_classification_problem"}, {"score": 0.0025716240685409513, "phrase": "comparative_study"}, {"score": 0.0024572448003065423, "phrase": "kdd_winner"}, {"score": 0.002203016940021265, "phrase": "proposed_method"}, {"score": 0.0021436720821474973, "phrase": "better_performance"}], "paper_keywords": ["Feature selection", " Filters", " Classification", " Discretization", " KDD Cup 99 dataset"], "paper_abstract": "In this work, a new method consisting of a combination of discretizers, filters and classifiers is presented. Its aim is to improve the performance results of classifiers but using a significantly reduced set of features. The method has been applied to a binary and to a multiple class classification problem. Specifically, the KDD Cup 99 benchmark was used for testing its effectiveness. A comparative study with other methods and the KDD winner was accomplished. The results obtained showed the adequacy of the proposed method, achieving better performance in most cases while reducing the number of features in more than 80%. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Feature selection and classification in multiple class datasets: An application to KDD Cup 99 dataset", "paper_id": "WOS:000287419900145"}