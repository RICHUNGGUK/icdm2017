{"auto_keywords": [{"score": 0.040247258573975694, "phrase": "sparse_recovery_methods"}, {"score": 0.01107989755510793, "phrase": "efs"}, {"score": 0.00481495049065317, "phrase": "greedy_feature_selection"}, {"score": 0.004772137871562584, "phrase": "subspace_clustering"}, {"score": 0.004625255533545965, "phrase": "powerful_generalization"}, {"score": 0.00458412183693466, "phrase": "single_subspace_models"}, {"score": 0.004502943680955863, "phrase": "high-dimensional_data"}, {"score": 0.004403480614319231, "phrase": "multiple_subspaces"}, {"score": 0.003973398317120165, "phrase": "subspace_estimation"}, {"score": 0.003732368382223596, "phrase": "provable_and_robust_strategy"}, {"score": 0.0036991458715587163, "phrase": "exact_feature_selection"}, {"score": 0.00341309513558946, "phrase": "recent_studies"}, {"score": 0.0032347516831068715, "phrase": "sufficient_conditions"}, {"score": 0.003163212007747804, "phrase": "greedy_method"}, {"score": 0.0031350397225314262, "phrase": "sparse_signal_recovery"}, {"score": 0.0030932495918679285, "phrase": "orthogonal_matching_pursuit"}, {"score": 0.0029447150254434842, "phrase": "empirical_study"}, {"score": 0.002918483026678636, "phrase": "feature_selection_strategies"}, {"score": 0.002729029534060092, "phrase": "nearest_neighbor"}, {"score": 0.00259793885118959, "phrase": "significant_advantages"}, {"score": 0.002574788013427524, "phrase": "nn_methods"}, {"score": 0.0023754752864092437, "phrase": "data_set"}, {"score": 0.0022311596751824416, "phrase": "exact_feature_sets"}, {"score": 0.0021624496659466068, "phrase": "nn"}, {"score": 0.0021049977753042253, "phrase": "subspace_membership"}], "paper_keywords": ["subspace clustering", " unions of subspaces", " hybrid linear models", " sparse approximation", " structured sparsity", " nearest neighbors", " low-rank approximation"], "paper_abstract": "Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation-the identification of points that live in the same subspace-and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)-recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with l(1)-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble.", "paper_title": "Greedy Feature Selection for Subspace Clustering", "paper_id": "WOS:000327007400001"}