{"auto_keywords": [{"score": 0.04573124107512906, "phrase": "gaze_gestures"}, {"score": 0.006962864582793164, "phrase": "intentional_gaze_gestures"}, {"score": 0.006355737293005852, "phrase": "real_time_recognition"}, {"score": 0.00481495049065317, "phrase": "remote_gaze_gesture_recognition"}, {"score": 0.0047310525657970615, "phrase": "predefined_sequences"}, {"score": 0.004697902218671618, "phrase": "eye_movements"}, {"score": 0.004648609688625397, "phrase": "'gaze_gestures"}, {"score": 0.00440976345264104, "phrase": "remote_video_oculography"}, {"score": 0.004302522426263979, "phrase": "human-computer_interaction"}, {"score": 0.0041102032028337366, "phrase": "potential_users"}, {"score": 0.0040385352882334235, "phrase": "cost_gaze"}, {"score": 0.0038715697646612766, "phrase": "spatio-temporal_structure"}, {"score": 0.003817456968395921, "phrase": "typical_gaze_activity"}, {"score": 0.0037773674108196376, "phrase": "standard_hci."}, {"score": 0.003621160116056119, "phrase": "bioinspired_bayesian_pattern_recognition_algorithm"}, {"score": 0.0035831245825960617, "phrase": "hierarchical_temporal_memory"}, {"score": 0.003410822685334396, "phrase": "user_study"}, {"score": 0.0033277936960179892, "phrase": "traditional_htm"}, {"score": 0.0031234247639701134, "phrase": "temporal_structure"}, {"score": 0.0030259817489390302, "phrase": "additional_top_node"}, {"score": 0.002994178996584381, "phrase": "htm_topology"}, {"score": 0.0029212625534156063, "phrase": "input_data"}, {"score": 0.002900756392266265, "phrase": "sequence_alignment"}, {"score": 0.002880393760798211, "phrase": "dynamic_programming"}, {"score": 0.002850116748762153, "phrase": "spatio-temporal_codification"}, {"score": 0.0027225413530200505, "phrase": "temporal_evolution"}, {"score": 0.0027034263575471352, "phrase": "gaze_gestures_instances"}, {"score": 0.0026375718041787, "phrase": "reliable_discrimination"}, {"score": 0.002600661525224505, "phrase": "otherwise_standard_human-machine_gaze_interaction"}, {"score": 0.0025194861926779223, "phrase": "data_set"}, {"score": 0.0024667783849055634, "phrase": "acceptable_completion_speeds"}, {"score": 0.0024408384127819734, "phrase": "low_rate"}, {"score": 0.0024236964502593254, "phrase": "false_positives"}, {"score": 0.0024066745849759706, "phrase": "standard_gaze-computer_interaction"}, {"score": 0.0023480317852018133, "phrase": "low_cost_hardware"}, {"score": 0.0022667208478315323, "phrase": "new_hci_paradigm"}, {"score": 0.0021652024032375833, "phrase": "projected_displays"}, {"score": 0.0021499920551456956, "phrase": "traditional_desktop_computers"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Encoding", " Motion analysis", " Multidimensional sequences", " Multidimensional signal processing", " Neural network architecture", " Patternrecognition", " Gaze tracking", " Human computer interaction"], "paper_abstract": "Predefined sequences of eye movements, or 'gaze gestures', can be consciously performed by humans and monitored non-invasively using remote video oculography. Gaze gestures hold great potential in human-computer interaction, HCI, as long as they can be easily assimilated by potential users, monitored using low cost gaze tracking equipment and machine learning algorithms are able to distinguish the spatio-temporal structure of intentional gaze gestures from typical gaze activity performed during standard HCI. In this work, an evaluation of the performance of a bioinspired Bayesian pattern recognition algorithm known as Hierarchical Temporal Memory (HTM) on the real time recognition of gaze gestures is carried out through a user study. To improve the performance of traditional HTM during real time recognition, an extension of the algorithm is proposed in order to adapt HTM to the temporal structure of gaze gestures. The extension consists of an additional top node in the HTM topology that stores and compares sequences of input data by sequence alignment using dynamic programming. The spatio-temporal codification of a gesture in a sequence serves the purpose of handling the temporal evolution of gaze gestures instances. The extended HTM allows for reliable discrimination of intentional gaze gestures from otherwise standard human-machine gaze interaction reaching up to 98% recognition accuracy for a data set of 10 categories of gaze gestures, acceptable completion speeds and a low rate of false positives during standard gaze-computer interaction. These positive results despite the low cost hardware employed supports the notion of using gaze gestures as a new HCI paradigm for the fields of accessibility and interaction with smartphones, tablets, projected displays and traditional desktop computers. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Low cost remote gaze gesture recognition in real time", "paper_id": "WOS:000305275800012"}