{"auto_keywords": [{"score": 0.041443559435410254, "phrase": "key_elements"}, {"score": 0.00481495049065317, "phrase": "clef_medical_automatic_annotation_task"}, {"score": 0.004749106104067268, "phrase": "medical_automatic_annotation_task"}, {"score": 0.004662705558154439, "phrase": "cross_language_evaluation_forum"}, {"score": 0.0046200986018141155, "phrase": "clef"}, {"score": 0.004515252649226697, "phrase": "fair_comparison"}, {"score": 0.004473982381245469, "phrase": "state-of-the_art_algorithms"}, {"score": 0.00443308765153716, "phrase": "medical_content-based_image_retrieval"}, {"score": 0.0041001740903338834, "phrase": "logical_decomposition"}, {"score": 0.004044064541716598, "phrase": "cbir_task"}, {"score": 0.0038802832668554457, "phrase": "relevant_steps"}, {"score": 0.003689053436223376, "phrase": "feature_extraction"}, {"score": 0.0036553070575015344, "phrase": "feature_comparison"}, {"score": 0.0036052633308076933, "phrase": "classifier_combination"}, {"score": 0.0034911335714333507, "phrase": "retrieval_results"}, {"score": 0.00327356326590461, "phrase": "retrieval_algorithms"}, {"score": 0.0030414140732789186, "phrase": "existing_framework"}, {"score": 0.0030135741897058844, "phrase": "image_retrieval"}, {"score": 0.002985988377995789, "phrase": "medical_applications"}, {"score": 0.00283870598459543, "phrase": "clef_annotation_tasks"}, {"score": 0.0027614468443073028, "phrase": "irma_framework"}, {"score": 0.002723607240517261, "phrase": "global_features"}, {"score": 0.0026986685585382347, "phrase": "corresponding_distance_measures"}, {"score": 0.0026373136897864763, "phrase": "nearest_neighbor_approach"}, {"score": 0.0026011706232569316, "phrase": "identical_classifier_parameters"}, {"score": 0.00257735013358606, "phrase": "combination_weights"}, {"score": 0.0024956789310376635, "phrase": "task_difficulty"}, {"score": 0.002427732996314194, "phrase": "declining_rank"}, {"score": 0.0023944552329463035, "phrase": "baseline_submission"}, {"score": 0.0023507916951658455, "phrase": "overall_advances"}, {"score": 0.002329258721870127, "phrase": "cbir_concepts"}, {"score": 0.002276283303864666, "phrase": "rough_comparison"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Medical image retrieval", " Content-based image retrieval system", " Classifier combination", " Image processing", " Image database"], "paper_abstract": "The medical automatic annotation task issued by the cross language evaluation forum (CLEF) aims at a fair comparison of state-of-the art algorithms for medical content-based image retrieval (CBIR). The contribution of this work is twofold: at first, a logical decomposition of the CBIR task is presented, and key elements to support the relevant steps are identified: (i) implementation of algorithms for feature extraction, feature comparison, and classifier combination, (ii) visualization of extracted features and retrieval results, (iii) generic evaluation of retrieval algorithms, and (iv) optimization of the parameters for the retrieval algorithms and their combination. Data structures and tools to address these key elements are integrated into an existing framework for image retrieval in medical applications (IRMA). Secondly, baseline results for the CLEF annotation tasks 2005-2007 are provided applying the IRMA framework, where global features and corresponding distance measures are combined within a nearest neighbor approach. Using identical classifier parameters and combination weights for each year shows that the task difficulty decreases over the years. The declining rank of the baseline submission also indicates the overall advances in CBIR concepts. Furthermore, a rough comparison between participants who submitted in only one of the years becomes possible. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "A framework and baseline results for the CLEF medical automatic annotation task", "paper_id": "WOS:000260321000007"}