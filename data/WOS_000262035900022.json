{"auto_keywords": [{"score": 0.03512271130246492, "phrase": "replay_compilation"}, {"score": 0.013573187887020326, "phrase": "virtual_machine"}, {"score": 0.01067998569034998, "phrase": "compilation_load"}, {"score": 0.008292545380330661, "phrase": "compilation_plans"}, {"score": 0.008051036962510572, "phrase": "matched-pair_comparison"}, {"score": 0.00481495049065317, "phrase": "java_performance_evaluation"}, {"score": 0.004774324145784758, "phrase": "rigorous_replay_compilation"}, {"score": 0.004714023408114857, "phrase": "managed_runtime_environment"}, {"score": 0.004615202066192795, "phrase": "java_virtual_machine"}, {"score": 0.0044424917428102445, "phrase": "java_performance"}, {"score": 0.003797530813314321, "phrase": "timer-based_sampling"}, {"score": 0.0037654595873971492, "phrase": "jit"}, {"score": 0.0037178503842258087, "phrase": "thread_scheduling"}, {"score": 0.0036090884921108086, "phrase": "java_performance_benchmarking_process"}, {"score": 0.0035183910023550246, "phrase": "recently_introduced_java_performance_analysis_methodology"}, {"score": 0.003400984514637656, "phrase": "experimental_repeatability"}, {"score": 0.0033579708256295847, "phrase": "key_idea"}, {"score": 0.0031777570808834213, "phrase": "pre-recorded_compilation_plan"}, {"score": 0.0030457198309616694, "phrase": "performance_effects"}, {"score": 0.002857849698346116, "phrase": "current_practice"}, {"score": 0.002809732883291955, "phrase": "single_compilation_plan"}, {"score": 0.002785978280998868, "phrase": "replay_time"}, {"score": 0.002762423953243486, "phrase": "multiple_compilation_plans"}, {"score": 0.0027390682208834013, "phrase": "statistical_rigor"}, {"score": 0.002704403349815203, "phrase": "replay_compilation_methodology"}, {"score": 0.0024320326084093465, "phrase": "statistical_data_analysis"}, {"score": 0.002380934815435267, "phrase": "performance_measurements"}, {"score": 0.0021501863832966966, "phrase": "accurate_performance_analysis"}, {"score": 0.0021229583685841405, "phrase": "statistical_analysis"}, {"score": 0.0021049977753042253, "phrase": "unpaired_measurements"}], "paper_keywords": ["Experimentation", " Measurement", " Performance", " Java", " virtual machine", " performance evaluation", " benchmarking", " replay compilation", " matched-pair comparison"], "paper_abstract": "A managed runtime environment, such as the Java virtual machine, is non-trivial to benchmark. Java performance is affected in various complex ways by the application and its input, as well as by the virtual machine (JIT optimizer, garbage collector, thread scheduler, etc.). In addition, nondeterminism due to timer-based sampling for JIT optimization, thread scheduling, and various system effects further complicate the Java performance benchmarking process. Replay compilation is a recently introduced Java performance analysis methodology that aims at controlling nondeterminism to improve experimental repeatability. The key idea of replay compilation is to control the compilation load during experimentation by inducing a pre-recorded compilation plan at replay time. Replay compilation also enables teasing apart performance effects of the application versus the virtual machine. This paper argues that in contrast to current practice which uses a single compilation plan at replay time, multiple compilation plans add statistical rigor to the replay compilation methodology. By doing so, replay compilation better accounts for the variability observed in compilation load across compilation plans. In addition, we propose matched-pair comparison for statistical data analysis. Matched-pair comparison considers the performance measurements per compilation plan before and after an innovation of interest as a pair, which enables limiting the number of compilation plans needed for accurate performance analysis compared to statistical analysis assuming unpaired measurements.", "paper_title": "Java Performance Evaluation through Rigorous Replay Compilation", "paper_id": "WOS:000262035900022"}