{"auto_keywords": [{"score": 0.035240068277609664, "phrase": "classification_accuracy"}, {"score": 0.00481495049065317, "phrase": "gp_ensemble"}, {"score": 0.00474946741204285, "phrase": "selective_algorithm"}, {"score": 0.004600100053651545, "phrase": "pattern_classification"}, {"score": 0.004537525607322823, "phrase": "boosting_algorithm"}, {"score": 0.004475798524102238, "phrase": "cellular_genetic_programming"}, {"score": 0.0044353455180102825, "phrase": "gp"}, {"score": 0.0040479661688863884, "phrase": "fixed_number"}, {"score": 0.0036776458116294986, "phrase": "clustering_algorithm"}, {"score": 0.003221111762383486, "phrase": "distributed_hybrid_environment"}, {"score": 0.0031339552246625463, "phrase": "cellular_models"}, {"score": 0.003105428650735113, "phrase": "parallel_gp."}, {"score": 0.002993887707344862, "phrase": "efficient_implementation"}, {"score": 0.0029666322924554274, "phrase": "distributed_gp"}, {"score": 0.0028210902750714075, "phrase": "low_sized_and_accurate_decision_trees"}, {"score": 0.0025047659706737215, "phrase": "suitable_pruning_strategies"}, {"score": 0.002349357968054711, "phrase": "misclassification_errors"}, {"score": 0.0022962183207691188, "phrase": "data_sets"}, {"score": 0.0021049977753042253, "phrase": "ensemble_approach"}], "paper_keywords": ["boosting", " classification", " clustering", " data mining", " ensemble", " genetic programming (GP)"], "paper_abstract": "A boosting algorithm based on cellular genetic programming (GP) to build an ensemble of predictors is proposed. The method evolves a population of trees for a fixed number of rounds and, after each round, it chooses the predictors to include in the ensemble by applying a clustering algorithm to the population of classifiers. Clustering the population allows the selection of the most diverse and fittest trees that best contribute to improve classification accuracy. The method proposed runs on a distributed hybrid environment that combines the island and cellular models of parallel GP. The combination of the two models provides an efficient implementation of distributed GP, and, at the same time, the generation of low sized and accurate decision trees. The large amount of memory required to store the ensemble affects the performance of the method. This paper shows that, by applying suitable pruning strategies, it is possible to select a subset of the classifiers without increasing misclassification errors; indeed for some data sets, for up to 30% of pruning, ensemble accuracy increases. Experimental results show that the combination of clustering and pruning enhances classification accuracy of the ensemble approach.", "paper_title": "Training distributed GP ensemble with a selective algorithm based on clustering and pruning for pattern classification", "paper_id": "WOS:000258373500004"}