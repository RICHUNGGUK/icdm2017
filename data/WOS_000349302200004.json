{"auto_keywords": [{"score": 0.04588971531846284, "phrase": "private_caches"}, {"score": 0.031042443925142046, "phrase": "coffd"}, {"score": 0.015057811791245978, "phrase": "hard_real-time_systems"}, {"score": 0.009843581853768003, "phrase": "high_utilization_task_sets"}, {"score": 0.00481495049065317, "phrase": "static_task_partitioning_for_locked_caches"}, {"score": 0.004794059969542758, "phrase": "multicore_real-time_systems"}, {"score": 0.0047732596509542135, "phrase": "growing_processing_demand"}, {"score": 0.00475254914916922, "phrase": "multitasking_real-time_systems"}, {"score": 0.004690952711665194, "phrase": "scalable_multicore_architectures"}, {"score": 0.004630150894006861, "phrase": "cache_lines"}, {"score": 0.00456020621016909, "phrase": "data_references"}, {"score": 0.004520711085350684, "phrase": "worst-case_execution_time"}, {"score": 0.004413846562278824, "phrase": "massive_multicore_architectures"}, {"score": 0.004318881178805575, "phrase": "shared_cache_architectures"}, {"score": 0.004262881313926916, "phrase": "single_resource"}, {"score": 0.004162086114221198, "phrase": "scalable_cache_architectures"}, {"score": 0.004002242925856335, "phrase": "cache-aware_allocation"}, {"score": 0.0038568987435766014, "phrase": "memory_accesses"}, {"score": 0.003652656006084564, "phrase": "real-time_tasks"}, {"score": 0.003399466007862412, "phrase": "novel_variant"}, {"score": 0.0033626664007941195, "phrase": "naive"}, {"score": 0.003129509028432954, "phrase": "task_sets"}, {"score": 0.0030888830649190282, "phrase": "intratask_conflicts"}, {"score": 0.0030754582519753474, "phrase": "locked_regions"}, {"score": 0.00303553181618577, "phrase": "nffd"}, {"score": 0.0029896049909328157, "phrase": "ffd"}, {"score": 0.002912475841630631, "phrase": "gffd"}, {"score": 0.002880927789104712, "phrase": "lower_number"}, {"score": 0.002855934865505357, "phrase": "lower_system_utilization"}, {"score": 0.0028127163745564777, "phrase": "core_requirements"}, {"score": 0.0027822459052173113, "phrase": "nffd."}, {"score": 0.002583597507349819, "phrase": "global_level"}, {"score": 0.0024842242921857705, "phrase": "region_level_instead_of_task_level"}, {"score": 0.0024412911465483225, "phrase": "dynamic_ordering"}, {"score": 0.002352500100943661, "phrase": "basic_scheme"}, {"score": 0.002286781334850213, "phrase": "regional_unlocking"}, {"score": 0.0022472527731744974, "phrase": "medium_utilization_task_sets"}, {"score": 0.002237477619351199, "phrase": "scenario_a._however"}, {"score": 0.0021844677283262798, "phrase": "scenario_a."}, {"score": 0.002128067018576581, "phrase": "future_multicore_architectures"}, {"score": 0.002118809200853959, "phrase": "real-time_systems"}, {"score": 0.0021049977753042253, "phrase": "key_insights"}], "paper_keywords": ["Design", " Experimentation", " Real-time systems", " multicore architectures", " timing analysis"], "paper_abstract": "Growing processing demand on multitasking real-time systems can be met by employing scalable multicore architectures. For such environments, locking cache lines for hard real-time systems ensures timing predictability of data references and may lower worst-case execution time. This work studies the benefits of cache locking on massive multicore architectures with private caches in the context of hard real-time systems. In shared cache architectures, the cache is a single resource shared among all of the tasks. However, in scalable cache architectures with private caches, conflicts exist only among the tasks scheduled on one core. This calls for a cache-aware allocation of tasks onto cores. The objective of this work is to increase the predictability of memory accesses resolved by caches while reducing the number of cores for a given task set. This allows designers to reduce the footprint of their subsystem of real-time tasks and thereby cost, either by choosing a product with fewer cores as a target or to allow more subsystems to be co-located on a given fixed number of cores. Our work proposes a novel variant of the cache-unaware First Fit Decreasing (FFD) algorithm called Naive locked First Fit Decreasing (NFFD) policy. We propose two cache-aware static scheduling schemes: (a) Greedy First Fit Decreasing (GFFD) and (b) Colored First Fit Decreasing (CoFFD) for task sets where tasks do not have intratask conflicts among locked regions (Scenario A). NFFD is capable of scheduling high utilization task sets that FFD cannot schedule. Experiments also show that CoFFD consistently outperforms GFFD, resulting in a lower number of cores and lower system utilization. CoFFD reduces the number of core requirements by 30% to 60% compared to NFFD. For a more generic case where tasks have intratask conflicts, we split the task partitioning between two phases: task selection and task allocation (Scenario B). Instead of resolving conflicts at a global level, these algorithms resolve conflicts among regions while allocating a task onto a core and unlocking at region level instead of task level. We show that a combination of dynamic ordering (task selection) with Chaitin's Coloring (task allocation) scheme reduces the number of cores required by up to 22% over a basic scheme (in a combination of monotone ordering and regional FFD). Regional unlocking allows this scheme to outperform CoFFD for medium utilization task sets from Scenario A. However, CoFFD performs better than any other scheme for high utilization task sets from Scenario A. Overall, this work is unique in considering the challenges of future multicore architectures for real-time systems and provides key insights into task partitioning and cache-locking mechanisms for architectures with private caches.", "paper_title": "Static Task Partitioning for Locked Caches in Multicore Real-Time Systems", "paper_id": "WOS:000349302200004"}