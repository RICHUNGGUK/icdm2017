{"auto_keywords": [{"score": 0.04611848253156265, "phrase": "latent_variables"}, {"score": 0.04285022510086753, "phrase": "time_delays"}, {"score": 0.014075074127282455, "phrase": "observed_variables"}, {"score": 0.011171557212147494, "phrase": "dimensionality_reduction"}, {"score": 0.00481495049065317, "phrase": "low-dimensional_latent_structure"}, {"score": 0.0045317702763072445, "phrase": "low-dimensional_latent_variables"}, {"score": 0.00437546648278148, "phrase": "instantaneous_relationships"}, {"score": 0.003925486550801027, "phrase": "larger_number"}, {"score": 0.0037658881407605445, "phrase": "latent_representation"}, {"score": 0.0036127549845584, "phrase": "novel_probabilistic_technique"}, {"score": 0.0033461647738159826, "phrase": "different_time_delay"}, {"score": 0.003293142761521425, "phrase": "latent_and_observed_variables"}, {"score": 0.003220315882881027, "phrase": "gaussian_process"}, {"score": 0.0031490944606172152, "phrase": "latent_variable"}, {"score": 0.003050064985384108, "phrase": "continuous_domain"}, {"score": 0.002907312188714594, "phrase": "common_probabilistic_framework"}, {"score": 0.0027800937731919276, "phrase": "model_parameters"}, {"score": 0.002599600404892265, "phrase": "latent_space"}, {"score": 0.002477878389967939, "phrase": "macaque_motor_cortex"}, {"score": 0.0024542249391904256, "phrase": "reaching_task"}, {"score": 0.0023846075213405284, "phrase": "neural_activity"}, {"score": 0.002346786090713685, "phrase": "gpfa"}, {"score": 0.0022729292542096077, "phrase": "motor_cortex_data"}, {"score": 0.00213893083537587, "phrase": "high-dimensional_time_series_data"}, {"score": 0.0021049977753042253, "phrase": "physical_delays"}], "paper_keywords": [""], "paper_abstract": "Noisy, high-dimensional time series observations can often be described by a set of low-dimensional latent variables. Commonly used methods to extract these latent variables typically assume instantaneous relationships between the latent and observed variables. In many physical systems, changes in the latent variables manifest as changes in the observed variables after time delays. Techniques that do not account for these delays can recover a larger number of latent variables than are present in the system, thereby making the latent representation more difficult to interpret. In this work, we introduce a novel probabilistic technique, time-delay gaussian-process factor analysis (TD-GPFA), that performs dimensionality reduction in the presence of a different time delay between each pair of latent and observed variables. We demonstrate how using a gaussian process to model the evolution of each latent variable allows us to tractably learn these delays over a continuous domain. Additionally, we show how TD-GPFA combines temporal smoothing and dimensionality reduction into a common probabilistic framework. We present an expectation/conditional maximization either (ECME) algorithm to learn the model parameters. Our simulations demonstrate that when time delays are present, TD-GPFA is able to correctly identify these delays and recover the latent space. We then applied TD-GPFA to the activity of tens of neurons recorded simultaneously in the macaque motor cortex during a reaching task. TD-GPFA is able to better describe the neural activity using a more parsimonious latent space than GPFA, a method that has been used to interpret motor cortex data but does not account for time delays. More broadly, TD-GPFA can help to unravel the mechanisms underlying high-dimensional time series data by taking into account physical delays in the system.", "paper_title": "Extracting Low-Dimensional Latent Structure from Time Series in the Presence of Delays", "paper_id": "WOS:000360091800001"}