{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "instruction_cache_design"}, {"score": 0.0036958398682001015, "phrase": "average_number"}, {"score": 0.0033025823350563087, "phrase": "last_object"}, {"score": 0.0029903851294825023, "phrase": "unified_formulation"}, {"score": 0.002912222095383313, "phrase": "packing_process"}, {"score": 0.0028549396580672417, "phrase": "markov_chain"}, {"score": 0.00250073564820402, "phrase": "cache_design"}, {"score": 0.0024353398669404334, "phrase": "line_size"}, {"score": 0.002234358539133758, "phrase": "cache_line"}, {"score": 0.0021903801614787423, "phrase": "micro-op_cache_design"}], "paper_keywords": ["bin packing", " stock cutting", " instruction cache", " Markov chain", " partitions of integer"], "paper_abstract": "In this paper we study the problem of packing a sequence of objects into bins. The objects are all either divisible or indivisible and occur in accordance with a certain probability distribution. We would like to find the average number of entries wasted in a bin if objects are indivisible and the probability of splitting the last object in a bin if objects are divisible. We solve this problem under a unified formulation by modeling a packing process as a Markov chain whose state transition probabilities are derived from an application of the partitions of integers. An application of this study to instruction cache design shows that a line size of 16 bytes has minimized the probability of splitting the last x 86 instruction in a cache line. For micro-op cache design, a line size of four entries has minimized the number of entries wasted per line. (c) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Variable-sized object packing and its applications to instruction cache design", "paper_id": "WOS:000258057300008"}