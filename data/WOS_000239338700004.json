{"auto_keywords": [{"score": 0.04437825901136091, "phrase": "available_information"}, {"score": 0.03718034097391595, "phrase": "affine_camera_model"}, {"score": 0.00481495049065317, "phrase": "direct_batch_recovery"}, {"score": 0.004778258911940497, "phrase": "euclidian_structure"}, {"score": 0.0047057084749407485, "phrase": "sparse_data"}, {"score": 0.004616560363089716, "phrase": "batch_method"}, {"score": 0.004563880494020726, "phrase": "euclidian_camera_motion"}, {"score": 0.004529093447865156, "phrase": "sparse_image_data"}, {"score": 0.004477407179841716, "phrase": "main_purpose"}, {"score": 0.004342430078306461, "phrase": "motion_parameters"}, {"score": 0.003961330917648707, "phrase": "factorisation_schemes"}, {"score": 0.0038125469099459905, "phrase": "initial_recovery_step"}, {"score": 0.0036274201504044685, "phrase": "image_data"}, {"score": 0.0035997452281985465, "phrase": "euclidian_camera_matrices"}, {"score": 0.003372844338742438, "phrase": "intermediate_projective_reconstruction"}, {"score": 0.0030414140732789186, "phrase": "affine_fundamental_matrices"}, {"score": 0.0029722900280316216, "phrase": "presented_work"}, {"score": 0.00290473242917264, "phrase": "presented_formulation"}, {"score": 0.002860546817196148, "phrase": "particularly_good_conditioning"}, {"score": 0.0027955219242123013, "phrase": "initial_motion_parameters"}, {"score": 0.0027424619603260837, "phrase": "unprecedented_diversity"}, {"score": 0.002690406375639144, "phrase": "possible_regularisation_terms"}, {"score": 0.0026393362602995254, "phrase": "new_autocalibration_scheme"}, {"score": 0.002520678372430923, "phrase": "calibration_parameters"}, {"score": 0.0024073421672242486, "phrase": "useful_model"}, {"score": 0.002370704665003412, "phrase": "scene_configurations"}, {"score": 0.0023435920614377306, "phrase": "wide_an-le_lenses"}, {"score": 0.0022990901047984197, "phrase": "close_range"}, {"score": 0.0022640963589007457, "phrase": "real_and_synthetic_data"}, {"score": 0.0021622717349518744, "phrase": "previous_structure"}, {"score": 0.002145750895912557, "phrase": "motion_techniques"}, {"score": 0.0021212055097236527, "phrase": "local_ambiguities"}, {"score": 0.0021049977753042253, "phrase": "error_accumulation"}], "paper_keywords": ["structure from motion", " batch recovery", " closure constraints", " affine camera model", " autocalibration", " contraction mapping"], "paper_abstract": "We present a batch method for recovering Euclidian camera motion from sparse image data. The main purpose of the algorithm is to recover the motion parameters using as much of the available information and as few computational steps as possible. The algorithm thus places itself in the gap between factorisation schemes, which make use of all available information in the initial recovery step, and sequential approaches which are able to handle sparseness in the image data. Euclidian camera matrices are approximated via the affine camera model, thus making the recovery direct in the sense that no intermediate projective reconstruction is made. Using a little known closure constraint, the FA-closure, we are able to formulate the camera coefficients linearly in the entries of the affine fundamental matrices. The novelty of the presented work is twofold: Firstly the presented formulation allows for a particularly good conditioning of the estimation of the initial motion parameters but also for an unprecedented diversity in the choice of possible regularisation terms. Secondly, the new autocalibration scheme presented here is in practice guaranteed to yield a Least Squares Estimate of the calibration parameters. As a bi-product, the affine camera model is rehabilitated as a useful model for most cameras and scene configurations. e.g. wide an-le lenses observing a scene at close range. Experiments on real and synthetic data demonstrate the ability to reconstruct scenes which are very problematic for previous structure from motion techniques due to local ambiguities and error accumulation.", "paper_title": "Affine approximation for direct batch recovery of euclidian structure and motion from sparse data", "paper_id": "WOS:000239338700004"}