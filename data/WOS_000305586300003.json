{"auto_keywords": [{"score": 0.037238124360209075, "phrase": "mode_mutation"}, {"score": 0.00481495049065317, "phrase": "multimodal_networks"}, {"score": 0.004773904938276846, "phrase": "multitask_games"}, {"score": 0.00473320761888448, "phrase": "intelligent_opponent_behavior"}, {"score": 0.0046928556063004214, "phrase": "video_games"}, {"score": 0.004632968695963776, "phrase": "human_players"}, {"score": 0.004593467306126913, "phrase": "evolutionary_computation"}, {"score": 0.004289291421348352, "phrase": "multiple_separate_tasks"}, {"score": 0.003988055287053139, "phrase": "separate_controllers"}, {"score": 0.0037719988989298983, "phrase": "separate_output_units"}, {"score": 0.0036137713873827374, "phrase": "network's_hidden_layer"}, {"score": 0.00350696416714684, "phrase": "new_output_modes"}, {"score": 0.003316884515934182, "phrase": "first_two_methods"}, {"score": 0.0032604908177429493, "phrase": "task_division"}, {"score": 0.0030183163057929687, "phrase": "different_strengths"}, {"score": 0.0029925405190240452, "phrase": "multinetwork"}, {"score": 0.002854645802851, "phrase": "clear_division"}, {"score": 0.0028060946024406766, "phrase": "multitask"}, {"score": 0.0026998264237910884, "phrase": "relative_difficulty"}, {"score": 0.0023134312557290043, "phrase": "human-specified_task_division"}, {"score": 0.0021973099304606076, "phrase": "human_knowledge"}, {"score": 0.0021049977753042253, "phrase": "multimodal_behavior"}], "paper_keywords": ["Multiagent", " multimodal", " multiobjective", " multitask", " neuroevolution", " Predator/Prey games"], "paper_abstract": "Intelligent opponent behavior makes video games interesting to human players. Evolutionary computation can discover such behavior, however, it is challenging to evolve behavior that consists of multiple separate tasks. This paper evaluates three ways of meeting this challenge via neuroevolution: 1) multinetwork learns separate controllers for each task, which are then combined manually; 2) multitask evolves separate output units for each task, but shares information within the network's hidden layer; and 3) mode mutation evolves new output modes, and includes a way to arbitrate between them. Whereas the first two methods require that the task division be known, mode mutation does not. Results in Front/Back Ramming and Predator/Prey games show that each of these methods has different strengths. Multinetwork is good in both domains, taking advantage of the clear division between tasks. Multitask performs well in Front/Back Ramming, in which the relative difficulty of the tasks is even, but poorly in Predator/Prey, in which it is lopsided. Interestingly, mode mutation adapts to this asymmetry and performs well in Predator/Prey. This result demonstrates how a human-specified task division is not always the best. Altogether the results suggest how human knowledge and learning can be combined most effectively to evolve multimodal behavior.", "paper_title": "Evolving Multimodal Networks for Multitask Games", "paper_id": "WOS:000305586300003"}