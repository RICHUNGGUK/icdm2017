{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "double-bagging_ensembles"}, {"score": 0.004518518120356522, "phrase": "boosting"}, {"score": 0.0041074234667687875, "phrase": "base_predictors"}, {"score": 0.0038953958723049287, "phrase": "double-bagging_ensemble"}, {"score": 0.003466536738232506, "phrase": "aggregation_process"}, {"score": 0.0030521898646210413, "phrase": "pruned_ensembles"}, {"score": 0.0028037824581900457, "phrase": "bagging"}, {"score": 0.002391067964886602, "phrase": "proposed_method"}, {"score": 0.0022916428586256723, "phrase": "good_choice"}, {"score": 0.002196342933346204, "phrase": "prediction_problems"}], "paper_keywords": [""], "paper_abstract": "In this paper, Boosting is used to determine the order in which base predictors are aggregated into a Double-Bagging ensemble, and a subensemble is constructed by early stopping the aggregation process based on two heuristic stopping rules. In all the investigated classification and regression problems, the pruned ensembles perform better than or as well as Bagging, Boosting and the full randomly ordered Double-Bagging ensembles in most cases. Therefore, the proposed method may be a good choice for solving the prediction problems at hand when prediction accuracy, prediction speed and storage requirements are all taken into account. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Using Boosting to prune Double-Bagging ensembles", "paper_id": "WOS:000263626700037"}