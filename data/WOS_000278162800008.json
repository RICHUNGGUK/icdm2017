{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "weakly_annotated_images"}, {"score": 0.012089398624634935, "phrase": "existing_annotations"}, {"score": 0.01142236712110156, "phrase": "proposed_method"}, {"score": 0.011131323652495924, "phrase": "visual-textual_classification"}, {"score": 0.009337728286167685, "phrase": "recognition_rate"}, {"score": 0.009005644263684281, "phrase": "semantic_relations"}, {"score": 0.004765173195920904, "phrase": "bayesian_network"}, {"score": 0.004547438494330851, "phrase": "probabilistic_graphical_model"}, {"score": 0.004014035333627087, "phrase": "maximum_number"}, {"score": 0.003931402465972151, "phrase": "ground_truth"}, {"score": 0.0036553070575015344, "phrase": "new_images"}, {"score": 0.003580032420161922, "phrase": "account_semantic_relations"}, {"score": 0.0031597433845085092, "phrase": "visual_and_textual_information"}, {"score": 0.0031107911390188055, "phrase": "experimental_results"}, {"score": 0.002579140290642792, "phrase": "proposed_model"}, {"score": 0.00239778324260337, "phrase": "missing_keywords"}, {"score": 0.0023361440016375972, "phrase": "mean_rate"}, {"score": 0.002311933249234786, "phrase": "good_annotations"}, {"score": 0.002171836484200008, "phrase": "state-of-art_model"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Probabilistic graphical models", " Bayesian networks", " Image classification", " Image annotation", " Semantic similarity", " Wordnet", " Visual features", " Bayesian classifier"], "paper_abstract": "In this paper, we propose a probabilistic graphical model to represent weakly annotated images. We consider an image as weakly annotated if the number of keywords defined for it is less than the maximum number defined in the ground truth. This model is used to classify images and automatically extend existing annotations to new images by taking into account semantic relations between keywords. The proposed method has been evaluated in visual-textual classification and automatic annotation of images. The visual-textual classification is performed by using both visual and textual information. The experimental results, obtained from a database of more than 30,000 images, show an improvement by 50.5% in terms of recognition rate against only visual information classification. Taking into account semantic relations between keywords improves the recognition rate by 10.5%. Moreover. the proposed model can be used to extend existing annotations to weakly annotated images, by computing distributions of missing keywords. Semantic relations improve the mean rate of good annotations by 6.9%. Finally, the proposed method is competitive with a state-of-art model. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Modeling, classifying and annotating weakly annotated images using Bayesian network", "paper_id": "WOS:000278162800008"}