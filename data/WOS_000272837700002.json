{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "incorrect_occlusion"}, {"score": 0.004779876533667754, "phrase": "dynamic_augmented_reality_engineering_environments"}, {"score": 0.004693621370429541, "phrase": "ar"}, {"score": 0.004642105253191792, "phrase": "significant_potential"}, {"score": 0.004475436844227947, "phrase": "graphical_visualization"}, {"score": 0.004283280680222718, "phrase": "real-world_objects"}, {"score": 0.004084372444216856, "phrase": "realistic_visual_output"}, {"score": 0.0036866541358511005, "phrase": "animated_scene"}, {"score": 0.0035802796531956413, "phrase": "existing_layout"}, {"score": 0.0033397385936467204, "phrase": "surrounding_environment"}, {"score": 0.003279160988534828, "phrase": "final_visualization"}, {"score": 0.0032433414042726356, "phrase": "visually_convincing_representations"}, {"score": 0.0031382058937207413, "phrase": "ar_animation"}, {"score": 0.003115307756631991, "phrase": "virtual_and_real_objects"}, {"score": 0.002959617637433, "phrase": "visually_convincing_illusion"}, {"score": 0.0028847066125323893, "phrase": "critical_challenge"}, {"score": 0.0027505663559338825, "phrase": "real_objects"}, {"score": 0.002720505061804933, "phrase": "ar_scene"}, {"score": 0.0026419338604879404, "phrase": "virtual_objects"}, {"score": 0.0026035011274172753, "phrase": "presented_research"}, {"score": 0.002575042977852052, "phrase": "new_ar_occlusion_handling_system"}, {"score": 0.002500662348528587, "phrase": "frame_buffer_manipulation_techniques"}, {"score": 0.0023669270875629205, "phrase": "dynamic_ar_environments"}, {"score": 0.0023496433751182162, "phrase": "real_time"}, {"score": 0.0023324855762635616, "phrase": "depth-sensing_equipment"}, {"score": 0.002306982970571251, "phrase": "laser_detection"}, {"score": 0.0022158306933307685, "phrase": "mobile_ar_platform"}, {"score": 0.0021282722932911427, "phrase": "dynamic_ar_scene"}, {"score": 0.0021049977753042253, "phrase": "vantage_position"}], "paper_keywords": [""], "paper_abstract": "Augmented reality (AR) offers significant potential in construction, manufacturing, and other engineering disciplines that employ graphical visualization to plan and design their operations. As a result of introducing real-world objects into the visualization, less virtual models have to be deployed to create a realistic visual output that directly translates into less time and effort required to create, render, manipulate, manage, and update three-dimensional (3D) virtual contents (CAD model engineering) of the animated scene. At the same time, using the existing layout of land or plant as the background of visualization significantly alleviates the need to collect data about the surrounding environment prior to creating the final visualization while providing visually convincing representations of the processes being studied. In an AR animation, virtual and real objects must be simultaneously managed and accurately displayed to a user to create a visually convincing illusion of their coexistence and interaction. A critical challenge impeding this objective is the problem of incorrect occlusion that manifests itself when real objects in an AR scene partially or wholly block the view of virtual objects. In the presented research, a new AR occlusion handling system based on depth-sensing algorithms and frame buffer manipulation techniques was designed and implemented. This algorithm is capable of resolving incorrect occlusion occurring in dynamic AR environments in real time using depth-sensing equipment such as laser detection and ranging (LADAR) devices, and can be integrated into any mobile AR platform that allows a user to navigate freely and observe a dynamic AR scene from any vantage position.", "paper_title": "Scalable Algorithm for Resolving Incorrect Occlusion in Dynamic Augmented Reality Engineering Environments", "paper_id": "WOS:000272837700002"}