{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "weighted_finite_state_transducer"}, {"score": 0.035203114357019644, "phrase": "adaptive_pruning_algorithm"}, {"score": 0.03377834700355587, "phrase": "active_nodes"}, {"score": 0.004536244367689415, "phrase": "considerable_interest"}, {"score": 0.004237338758318918, "phrase": "wfst"}, {"score": 0.003958049372486465, "phrase": "memory_usage_challenges"}, {"score": 0.0036037265181631324, "phrase": "token_propagation"}, {"score": 0.0034828730516958807, "phrase": "wfst_epsilon_input_arcs"}, {"score": 0.0032531496610470377, "phrase": "dramatic_reduction"}, {"score": 0.0027662954811372175, "phrase": "better_practical_performance"}, {"score": 0.0027194778713860715, "phrase": "conventional_wfst"}, {"score": 0.0022731540640627307, "phrase": "speech_recognition_accuracy"}, {"score": 0.0021049977753042253, "phrase": "deterministic_performance"}], "paper_keywords": ["Embedded processors", " memory organization", " speech recognition", " WFST"], "paper_abstract": "There is considerable interest in creating embedded, speech recognition hardware using the weighted finite state transducer (WFST) technique but there are performance and memory usage challenges. Two system optimization techniques are presented to address this; one approach improves token propagation by removing the WFST epsilon input arcs; another one-pass, adaptive pruning algorithm gives a dramatic reduction in active nodes to be computed. Results for memory and bandwidth are given for a 5,000 word vocabulary giving a better practical performance than conventional WFST; this is then exploited in an adaptive pruning algorithm that reduces the active nodes from 30,000 down to 4,000 with only a 2 percent sacrifice in speech recognition accuracy; these optimizations lead to a more simplified design with deterministic performance.", "paper_title": "Optimization of Weighted Finite State Transducer for Speech Recognition", "paper_id": "WOS:000321221000010"}