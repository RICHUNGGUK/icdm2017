{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayesian_networks"}, {"score": 0.04922082634695279, "phrase": "mutual_information"}, {"score": 0.004671932143798307, "phrase": "conditional_independence_tests"}, {"score": 0.004533142516832776, "phrase": "new_scoring_function"}, {"score": 0.004017846065981193, "phrase": "well-known_properties"}, {"score": 0.0038749521906474593, "phrase": "novel_way"}, {"score": 0.0037597491055035895, "phrase": "statistical_independence_test"}, {"score": 0.0036700484729006136, "phrase": "chi-square_distribution"}, {"score": 0.0035609154384221567, "phrase": "mutual_information_measure"}, {"score": 0.0034135407979996673, "phrase": "additive_decomposition"}, {"score": 0.0028823939507764238, "phrase": "non-bayesian_scoring_function"}, {"score": 0.0028479329482725066, "phrase": "mit"}, {"score": 0.002600995957987461, "phrase": "information_theory"}, {"score": 0.0025542617599268323, "phrase": "mit_score"}, {"score": 0.002448446849997207, "phrase": "kullback-leibler_divergence"}, {"score": 0.002404447017429959, "phrase": "joint_probability_distributions"}, {"score": 0.0023470051993423483, "phrase": "candidate_network"}, {"score": 0.0022909325048608054, "phrase": "available_data"}, {"score": 0.0022497569011987587, "phrase": "detailed_results"}, {"score": 0.002209319719984651, "phrase": "complete_experimental_evaluation"}, {"score": 0.002169607777109061, "phrase": "proposed_scoring_function"}], "paper_keywords": ["Bayesian networks", " scoring functions", " learning", " mutual information", " conditional independence tests"], "paper_abstract": "We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented.", "paper_title": "A scoring function for learning Bayesian networks based on mutual information and conditional independence tests", "paper_id": "WOS:000245390500008"}