{"auto_keywords": [{"score": 0.026878029726556292, "phrase": "random_simulations"}, {"score": 0.00481495049065317, "phrase": "strategic_behaviour"}, {"score": 0.004561019117546361, "phrase": "combinatorial_problems"}, {"score": 0.0044692705329574, "phrase": "monte-carlo_tree_search"}, {"score": 0.0043793594257092805, "phrase": "generic_method"}, {"score": 0.00414830454375092, "phrase": "notable_results"}, {"score": 0.003983024386003228, "phrase": "mcts"}, {"score": 0.0039427304473829165, "phrase": "search_tree"}, {"score": 0.0039028497942174777, "phrase": "new_nodes"}, {"score": 0.0038242899532391914, "phrase": "search_space"}, {"score": 0.0037473054818679763, "phrase": "monte-carlo_simulations"}, {"score": 0.003684332524184009, "phrase": "efficient_monte-carlo_policies"}, {"score": 0.0035615322947497013, "phrase": "improved_monte-carlo_policy"}, {"score": 0.003249966782844541, "phrase": "strategic_elements"}, {"score": 0.0030369270700452387, "phrase": "monte-carlo"}, {"score": 0.002427549902743566, "phrase": "tactical_and_strategic_elements"}, {"score": 0.0023545181737729065, "phrase": "improved_monte-carlo_policies"}, {"score": 0.002322767202375341, "phrase": "poolrave"}, {"score": 0.0023070522694236583, "phrase": "last-good-reply"}, {"score": 0.0022452462028184654, "phrase": "strong_tactical_element"}, {"score": 0.0022300545742463262, "phrase": "small_numbers"}, {"score": 0.002119339298633587, "phrase": "strong_strategic_element"}, {"score": 0.0021049977753042253, "phrase": "higher_numbers"}], "paper_keywords": [""], "paper_abstract": "Over the last few years, many new algorithms have been proposed to solve combinatorial problems. In this field, Monte-Carlo Tree Search (MCTS) is a generic method which performs really well on several applications; for instance, it has been used with notable results in the game of Go. To find the most promising decision, MCTS builds a search tree where the new nodes are selected by sampling the search space randomly (i.e., by Monte-Carlo simulations). However, efficient Monte-Carlo policies are generally difficult to learn. Even if an improved Monte-Carlo policy performs adequately in some games, it can become useless or harmful in other games depending on how the algorithm takes into account the tactical and the strategic elements of the game. In this article, we address this problem by studying when and why a learned Monte-Carlo policy works. To this end, we use (1) two known Monte-Carlo policy improvements (Pool-Rave and Last-Good-Reply) and (2) two connection games (Hex and Havannah). We aim to understand how the benefit is related (a) to the number of random simulations and (b) to the various game rules (within them, tactical and strategic elements of the game). Our results indicate that improved Monte-Carlo policies, such as PoolRave or Last-Good-Reply, work better for games with a strong tactical element for small numbers of random simulations, whereas more general policies seem to be more suited for games with a strong strategic element for higher numbers of random simulations.", "paper_title": "ON THE TACTICAL AND STRATEGIC BEHAVIOUR OF MCTS WHEN BIASING RANDOM SIMULATIONS", "paper_id": "WOS:000367314100001"}