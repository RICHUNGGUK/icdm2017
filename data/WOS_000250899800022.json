{"auto_keywords": [{"score": 0.04100507700391364, "phrase": "negative_instances"}, {"score": 0.03140437097457245, "phrase": "positive_class"}, {"score": 0.025160476037580235, "phrase": "pnb"}, {"score": 0.00481495049065317, "phrase": "bayesian_classifiers"}, {"score": 0.00477924690801092, "phrase": "positive_and_unlabeled_examples"}, {"score": 0.004726184966910756, "phrase": "positive_unlabeled_learning_term"}, {"score": 0.00465634671242413, "phrase": "binary_classification_problem"}, {"score": 0.004570491683870396, "phrase": "negative_examples"}, {"score": 0.00445293569103488, "phrase": "semi-supervised_classification_algorithms"}, {"score": 0.004306205029641728, "phrase": "new_algorithms"}, {"score": 0.004195418010131832, "phrase": "positive_unlabeled_learning_algorithms"}, {"score": 0.0041488110506566825, "phrase": "positive_naive_bayes"}, {"score": 0.0039822874870125095, "phrase": "naive_bayes_induction_algorithm"}, {"score": 0.003601235776892337, "phrase": "pnb_one_step"}, {"score": 0.003367611042980526, "phrase": "new_algorithm"}, {"score": 0.003293142761521425, "phrase": "naive_bayes"}, {"score": 0.0031963987773420068, "phrase": "naive_bayes_models"}, {"score": 0.003160854709666311, "phrase": "positive_unlabeled_domain"}, {"score": 0.0030565692640988585, "phrase": "new_bayesian_approach"}, {"score": 0.002836942411239067, "phrase": "beta_distribution"}, {"score": 0.0027433143290371293, "phrase": "ptan"}, {"score": 0.0026232532659260033, "phrase": "positive_unlabeled_learning_problems"}, {"score": 0.0025940660106162404, "phrase": "real_and_synthetic_databases"}, {"score": 0.00247128526955136, "phrase": "predicting_variables"}, {"score": 0.002319431279824863, "phrase": "classification_performance"}, {"score": 0.0021446391766277817, "phrase": "ptan."}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["positive unlabeled learning", " Bayesian classifiers", " naive Bayes", " tree augmented naive Bayes", " Bayesian approach"], "paper_abstract": "The positive unlabeled learning term refers to the binary classification problem in the absence of negative examples. When only positive and unlabeled instances are available, semi-supervised classification algorithms cannot be directly applied, and thus new algorithms are required. One of these positive unlabeled learning algorithms is the positive naive Bayes (PNB), which is an adaptation of the naive Bayes induction algorithm that does not require negative instances. In this work we propose two ways of enhancing this algorithm. On one hand, we have taken the concept behind PNB one step further, proposing a procedure to build more complex Bayesian classifiers in the absence of negative instances. We present a new algorithm (named positive tree augmented naive Bayes, PTAN) to obtain tree augmented naive Bayes models in the positive unlabeled domain. On the other hand, we propose a new Bayesian approach to deal with the a priori probability of the positive class that models the uncertainty over this parameter by means of a Beta distribution. This approach is applied to both PNB and PTAN, resulting in two new algorithms. The four algorithms are empirically compared in positive unlabeled learning problems based on real and synthetic databases. The results obtained in these comparisons suggest that, when the predicting variables are not conditionally independent given the class, the extension of PNB to more complex networks increases the classification performance. They also show that our Bayesian approach to the a priori probability of the positive class can improve the results obtained by PNB and PTAN. (c) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Learning Bayesian classifiers from positive and unlabeled examples", "paper_id": "WOS:000250899800022"}