{"auto_keywords": [{"score": 0.030727488236870985, "phrase": "classification_accuracy"}, {"score": 0.00481495049065317, "phrase": "feature_selection_methods"}, {"score": 0.00451642066363115, "phrase": "empirical_evaluation"}, {"score": 0.004464156619275582, "phrase": "five_feature_selection_methods"}, {"score": 0.0040201041829593595, "phrase": "gini"}, {"score": 0.003904713104101532, "phrase": "evaluated_methods"}, {"score": 0.003837089839193524, "phrase": "random_forest_feature_selector"}, {"score": 0.0034551632687788857, "phrase": "implemented_feature_selection"}, {"score": 0.0032596049901888724, "phrase": "six_different_classifiers"}, {"score": 0.0032031180198375283, "phrase": "feature_selection"}, {"score": 0.003093055630003373, "phrase": "relieff"}, {"score": 0.0030572112793962004, "phrase": "random_forest"}, {"score": 0.0029349892964423197, "phrase": "highest_increase"}, {"score": 0.002752695791351096, "phrase": "unnecessary_attributes"}, {"score": 0.0027049694095434905, "phrase": "achieved_conclusions"}, {"score": 0.002642615537454746, "phrase": "machine_learning_users"}, {"score": 0.002566685388128598, "phrase": "selection_method"}, {"score": 0.0023654615987859402, "phrase": "risk-sensitive_applications"}, {"score": 0.0021049977753042253, "phrase": "unnecessary_data"}], "paper_keywords": ["Feature selection", " ReliefF", " random forest feature selector", " sequential forward selection", " sequential backward selection", " Gini index"], "paper_abstract": "In the paper, we present an empirical evaluation of five feature selection methods: ReliefF, random forest feature selector, sequential forward selection, sequential backward selection, and Gini index. Among the evaluated methods, the random forest feature selector has not yet been widely compared to the other methods. In our evaluation, we test how the implemented feature selection can affect (i.e. improve) the accuracy of six different classifiers by performing feature selection. The results show that ReliefF and random forest enabled the classifiers to achieve the highest increase in classification accuracy on the average while reducing the number of unnecessary attributes. The achieved conclusions can advise the machine learning users which classifier and feature selection method to use to optimize the classification accuracy, which may be important especially in risk-sensitive applications of Machine Learning (e. g. medicine, business decisions, control applications) as well as in the aim to reduce costs of collecting, processing and storage of unnecessary data.", "paper_title": "Empirical evaluation of feature selection methods in classification", "paper_id": "WOS:000278714100002"}