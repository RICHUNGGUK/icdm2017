{"auto_keywords": [{"score": 0.04461509693576422, "phrase": "dimension_reduction"}, {"score": 0.01295967240368833, "phrase": "information_retrieval_process"}, {"score": 0.00481495049065317, "phrase": "latent_semantic_term_self-correlation"}, {"score": 0.004554097638131679, "phrase": "lsa"}, {"score": 0.004388035083357706, "phrase": "generalized_vector_space_method"}, {"score": 0.00415019999222481, "phrase": "term_correlations"}, {"score": 0.002607362103727335, "phrase": "direct_relationship"}, {"score": 0.0024658019116387845, "phrase": "lsa_dimension_reduction"}, {"score": 0.002397923568994474, "phrase": "lsa_self-correlation"}, {"score": 0.0021848422732893926, "phrase": "lsa_term_self-correlations"}, {"score": 0.0021049977753042253, "phrase": "substantial_increase"}], "paper_keywords": ["Theory", " Experimentation", " Latent semantic analysis", " term correlation"], "paper_abstract": "Latent semantic analysis (LSA) is a generalized vector space method that uses dimension reduction to generate term correlations for use during the information retrieval process. We hypothesized that even though the dimension reduction establishes correlations between terms, the dimension reduction is causing a degradation in the correlation of a term to itself (self-correlation). In this article, we have proven that there is a direct relationship to the size of the LSA dimension reduction and the LSA self-correlation. We have also shown that by altering the LSA term self-correlations we gain a substantial increase in precision, while also reducing the computation required during the information retrieval process.", "paper_title": "An Analysis of Latent Semantic Term Self-Correlation", "paper_id": "WOS:000264682400002"}