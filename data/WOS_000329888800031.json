{"auto_keywords": [{"score": 0.049031285206733555, "phrase": "automatic_facial_expression_analysis"}, {"score": 0.04552728018001717, "phrase": "video_sequences"}, {"score": 0.0293498674373722, "phrase": "proposed_framework"}, {"score": 0.00481495049065317, "phrase": "dynamic_sequences"}, {"score": 0.004640590144749795, "phrase": "human_facial_expressions"}, {"score": 0.004555781299332848, "phrase": "discrete_categories"}, {"score": 0.00447251540886613, "phrase": "existing_work"}, {"score": 0.004278807087784626, "phrase": "subjective_thresholding"}, {"score": 0.00424734451992536, "phrase": "dynamic_information"}, {"score": 0.004154332348800334, "phrase": "particular_individual_frames"}, {"score": 0.004093453936415085, "phrase": "expected_behaviour"}, {"score": 0.003916098498242888, "phrase": "tedious_manual_work"}, {"score": 0.0037463983616303786, "phrase": "dynamic_signature"}, {"score": 0.0037188360359702182, "phrase": "facial_movements"}, {"score": 0.0036508108698357932, "phrase": "expression_recognition"}, {"score": 0.003557653536102669, "phrase": "novel_framework"}, {"score": 0.003466865016432561, "phrase": "salient_information"}, {"score": 0.003353521531649663, "phrase": "subjective_preprocessing"}, {"score": 0.0033288399506044763, "phrase": "additional_user-supplied_information"}, {"score": 0.003267925215811654, "phrase": "peak_expressions"}, {"score": 0.0032319109979344184, "phrase": "experimental_framework"}, {"score": 0.0031845067058084613, "phrase": "proposed_method"}, {"score": 0.003161065111494053, "phrase": "static_expression_recognition_systems"}, {"score": 0.0031146967095233586, "phrase": "recognition_rate"}, {"score": 0.0030239842475439814, "phrase": "action_units"}, {"score": 0.002850385670169814, "phrase": "final_result"}, {"score": 0.0028189599390180536, "phrase": "incorrect_initial_identification"}, {"score": 0.0027368368719283298, "phrase": "parametric_space"}, {"score": 0.00266693888510356, "phrase": "six_state-of-the-art_machine"}, {"score": 0.0025606791601860543, "phrase": "important_foundation"}, {"score": 0.0024860616730750158, "phrase": "future_work"}, {"score": 0.0023606625824479956, "phrase": "user_study"}, {"score": 0.002258212866482022, "phrase": "human_cognitive_systems"}, {"score": 0.002192389791199368, "phrase": "human_emotion_classification"}, {"score": 0.0021522261505574035, "phrase": "public_databases"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Facial expression analysis", " Dynamic feature extraction and", " visualisation"], "paper_abstract": "Automatic facial expression analysis aims to analyse human facial expressions and classify them into discrete categories. Methods based on existing work are reliant on extracting information from video sequences and employ either some form of subjective thresholding of dynamic information or attempt to identify the particular individual frames in which the expected behaviour occurs. These methods are inefficient as they require either additional subjective information, tedious manual work or fail to take advantage of the information contained in the dynamic signature from facial movements for the task of expression recognition. In this paper, a novel framework is proposed for automatic facial expression analysis which extracts salient information from video sequences but does not rely on any subjective preprocessing or additional user-supplied information to select frames with peak expressions. The experimental framework demonstrates that the proposed method outperforms static expression recognition systems in terms of recognition rate. The approach does not rely on action units (AUs), and therefore, eliminates errors which are otherwise propagated to the final result due to incorrect initial identification of AUs. The proposed framework explores a parametric space of over 300 dimensions and is tested with six state-of-the-art machine learning techniques. Such robust and extensive experimentation provides an important foundation for the assessment of the performance for future work. A further contribution of the paper is offered in the form of a user study. This was conducted in order to investigate the correlation between human cognitive systems and the proposed framework for the understanding of human emotion classification and the reliability of public databases. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Facial expression recognition in dynamic sequences: An integrated approach", "paper_id": "WOS:000329888800031"}