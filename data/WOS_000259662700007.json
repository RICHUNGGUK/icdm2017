{"auto_keywords": [{"score": 0.045051084910580984, "phrase": "evader's_action_preference"}, {"score": 0.008229667632058183, "phrase": "expected_payoffs"}, {"score": 0.00794826275119082, "phrase": "single_payoffs"}, {"score": 0.00481495049065317, "phrase": "pursuit-evasion_game"}, {"score": 0.004731638279336065, "phrase": "proposed_self-adaptive_predictive_pursuing_policy_consists"}, {"score": 0.004236321075918639, "phrase": "correct_estimation"}, {"score": 0.004187284711626886, "phrase": "opponent's_intention"}, {"score": 0.004043541331944951, "phrase": "adversarial_games"}, {"score": 0.0038820411362656803, "phrase": "action_preference"}, {"score": 0.003837089839193524, "phrase": "model_opponent's_decision-making"}, {"score": 0.003705323407433896, "phrase": "different_action_preference"}, {"score": 0.003662410853488772, "phrase": "different_situation"}, {"score": 0.0035989687070420977, "phrase": "model_evader's_decision-making"}, {"score": 0.0034551632687788857, "phrase": "situation_space"}, {"score": 0.0031293191522247374, "phrase": "pursuer"}, {"score": 0.0029349892964423197, "phrase": "evader's_action"}, {"score": 0.0029009722548088306, "phrase": "action_decision-making_procedure"}, {"score": 0.0026272523687965615, "phrase": "action_decision-making"}, {"score": 0.002551762519273805, "phrase": "decision_tree"}, {"score": 0.0022841153001549193, "phrase": "adversarial_situation"}, {"score": 0.0022184630090454132, "phrase": "middle_size_soccer_robots"}, {"score": 0.0021049977753042253, "phrase": "proposed_policy"}], "paper_keywords": ["action preference", " payoff function", " predictive", " pursuit-evasion games", " self-adaptive"], "paper_abstract": "The proposed self-adaptive predictive pursuing policy consists of an action decision-making procedure and a procedure of adjusting the estimation of evader's action preference, Since correct estimation of opponent's intention would do good to win adversarial games, it introduces the conception of action preference to model opponent's decision-making. Because evader often has different action preference in different situation, to model evader's decision-making, pursuer has to divide the situation space into many categories and provide a set of estimation of evader's action preference for each kind of situation. Pursuer adjusts the estimation of evader's action preference in certain situation by observing evader's action. Action decision-making procedure consists of situation sorting, possible future states computation, payoff evaluation and action selection. Action decision-making is based on the decision tree constructed by expected payoffs. Expected payoffs are integrated from single payoffs. Single payoffs are evaluated by gains of features reflecting adversarial situation. A simulation of middle size soccer robots has been carried out and illustrated that the proposed policy is effective.", "paper_title": "A self-adaptive predictive policy for pursuit-evasion game", "paper_id": "WOS:000259662700007"}