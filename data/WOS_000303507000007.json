{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "variational_learning_for_finite_dirichlet_mixture_models"}, {"score": 0.0044237033788643715, "phrase": "variational_learning"}, {"score": 0.004361644989356558, "phrase": "finite_dirichlet_mixture_models"}, {"score": 0.004064117897000019, "phrase": "mixture_models"}, {"score": 0.003334309053314909, "phrase": "mixture_model"}, {"score": 0.002833689453299307, "phrase": "bayesian_inference_procedure"}, {"score": 0.0027159094186967247, "phrase": "whole_inference_process"}, {"score": 0.0026214978443388653, "phrase": "closed-form_solutions"}, {"score": 0.0024947959375982614, "phrase": "large_applications"}, {"score": 0.002391067964886602, "phrase": "real-life_challenging_applications"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Bayesian estimation", " dirichlet distribution", " factorized approximation", " image databases", " intrusion detection", " mixture models", " unsupervised learning", " variational inference"], "paper_abstract": "In this paper, we focus on the variational learning of finite Dirichlet mixture models. Compared to other algorithms that are commonly used for mixture models (such as expectation-maximization), our approach has several advantages: first, the problem of over-fitting is prevented; furthermore, the complexity of the mixture model (i.e., the number of components) can be determined automatically and simultaneously with the parameters estimation as part of the Bayesian inference procedure; finally, since the whole inference process is analytically tractable with closed-form solutions, it may scale well to large applications. Both synthetic and real data, generated from real-life challenging applications namely image databases categorization and anomaly intrusion detection, are experimented to verify the effectiveness of the proposed approach.", "paper_title": "Variational Learning for Finite Dirichlet Mixture Models and Applications", "paper_id": "WOS:000303507000007"}