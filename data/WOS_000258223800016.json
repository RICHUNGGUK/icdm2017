{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "radio_recordings"}, {"score": 0.01552915658292207, "phrase": "dynamic_programming"}, {"score": 0.00471700544744796, "phrase": "bayesian_networks"}, {"score": 0.004602085616330634, "phrase": "multistage_system"}, {"score": 0.00445318311339027, "phrase": "three-step_procedure"}, {"score": 0.004398589723893764, "phrase": "first_step"}, {"score": 0.004344662696228875, "phrase": "computationally_efficient_scheme"}, {"score": 0.004256243266481051, "phrase": "growing_technique"}, {"score": 0.00405127920820053, "phrase": "raw_audio_stream"}, {"score": 0.003920128102807887, "phrase": "preprocessing_stage"}, {"score": 0.00388800655625083, "phrase": "yields_segments"}, {"score": 0.0038561471948364723, "phrase": "high_music"}, {"score": 0.003824547895167011, "phrase": "speech_precision"}, {"score": 0.0036104596826132965, "phrase": "unclassified_parts"}, {"score": 0.0035661605757956096, "phrase": "audio_stream"}, {"score": 0.0033115124244647736, "phrase": "probabilistic_segmentation_task"}, {"score": 0.00313894917464766, "phrase": "proposed_scheme"}, {"score": 0.0030497740719977835, "phrase": "respective_class_labels"}, {"score": 0.0029147175927233546, "phrase": "posterior_class_probabilities"}, {"score": 0.002740109776120878, "phrase": "bayesian_network_combiner"}, {"score": 0.002684257009647205, "phrase": "posterior_probability_estimator"}, {"score": 0.0026403934420630155, "phrase": "final_stage"}, {"score": 0.002575934917174136, "phrase": "boundary_correction"}, {"score": 0.002523420125608051, "phrase": "possible_errors"}, {"score": 0.002333421318779658, "phrase": "proposed_system"}, {"score": 0.002239225269052105, "phrase": "overall_system_accuracy"}, {"score": 0.002202617924488607, "phrase": "performance_results"}, {"score": 0.0021488235554917128, "phrase": "musical_genre_basis"}, {"score": 0.0021049977753042253, "phrase": "existing_methods"}], "paper_keywords": ["Bayesian networks", " dynamic programming", " speech-music discrimination"], "paper_abstract": "This paper presents a multistage system for speech/music discrimination which is based on a three-step procedure. The first step is a computationally efficient scheme consisting of a region growing technique and operates on a 1-D feature sequence, which is extracted from the raw audio stream, This scheme is used as a preprocessing stage and yields segments with high music and speech precision at the expense of leaving certain parts of the audio recording unclassified. The unclassified parts of the audio stream are then fed as input to a more computationally demanding scheme. The latter treats speech/music discrimination of radio recordings as a probabilistic segmentation task, where the solution is obtained by means of dynamic programming. The proposed scheme seeks the sequence of segments and respective class labels (i.e., speech/music) that maximize the product of posterior class probabilities, given the data that form the segments. To this end, a Bayesian Network combiner is embedded as a posterior probability estimator. At a final stage, an algorithm that performs boundary correction is applied to remove possible errors at the boundaries of the segments (speech or music) that have been previously generated. The proposed system has been tested on radio recordings from various sources. The overall system accuracy is approximately 96%. Performance results are also reported on a musical genre basis and a comparison with existing methods is given.", "paper_title": "Speech/music discriminator of radio recordings based on dynamic programming and Bayesian networks", "paper_id": "WOS:000258223800016"}