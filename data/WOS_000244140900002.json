{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "reward-based_learning"}, {"score": 0.024095146252357525, "phrase": "hebbian_plasticity"}, {"score": 0.004483909657713567, "phrase": "large_number"}, {"score": 0.004326987090971404, "phrase": "network_function"}, {"score": 0.004029359223675788, "phrase": "reward_signal"}, {"score": 0.003944112858235306, "phrase": "improved_performance"}, {"score": 0.0038332390893263844, "phrase": "parameter_space"}, {"score": 0.003752126270661821, "phrase": "optimal_solution"}, {"score": 0.003419943984098415, "phrase": "hebbian_forms"}, {"score": 0.003371508176308189, "phrase": "synaptic_plasticity"}, {"score": 0.0032302647179383915, "phrase": "supervisor_circuit"}, {"score": 0.0027807832748641195, "phrase": "efficient_reinforcement-based_learning"}, {"score": 0.002664221051021009, "phrase": "critical_element"}, {"score": 0.0025343769150179764, "phrase": "supervisor_units"}, {"score": 0.002359759876402679, "phrase": "appropriate_connections"}, {"score": 0.002212892265364764, "phrase": "reinforcement-based_learning_procedure"}, {"score": 0.0021049977753042253, "phrase": "function_approximation_task"}], "paper_keywords": ["PGA", " neural networks", " functional approximation"], "paper_abstract": "Reward-based learning in neural systems is challenging because a large number of parameters that affect network function must be optimized solely on the basis of a reward signal that indicates improved performance. Searching the parameter space for an optimal solution is particularly difficult if the network is large. We show that Hebbian forms of synaptic plasticity applied to synapses between a supervisor circuit and the network it is controlling can effectively reduce the dimension of the space of parameters being searched to support efficient reinforcement-based learning in large networks. The critical element is that the connections between the supervisor units and the network must be reciprocal. Once the appropriate connections have been set up by Hebbian plasticity, a reinforcement-based learning procedure leads to rapid learning in a function approximation task. Hebbian plasticity within the network being supervised ultimately allows the network to perform the task without input from the supervisor.", "paper_title": "Dimensional reduction for reward-based learning", "paper_id": "WOS:000244140900002"}