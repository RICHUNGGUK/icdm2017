{"auto_keywords": [{"score": 0.043183257806000776, "phrase": "human_actions"}, {"score": 0.00481495049065317, "phrase": "unified_spatio-temporal_human_body_region"}, {"score": 0.004627244470096107, "phrase": "nurnerous_instances"}, {"score": 0.004466519142879166, "phrase": "direct_observation"}, {"score": 0.004407690586271672, "phrase": "human_body"}, {"score": 0.0042734067085669885, "phrase": "related_objects"}, {"score": 0.004016938499717623, "phrase": "present_paper"}, {"score": 0.0038602215057106917, "phrase": "multi-feature_method"}, {"score": 0.0036445446531633368, "phrase": "sturdy_region"}, {"score": 0.00353342812142574, "phrase": "conventional_space-time"}, {"score": 0.0035178323426661626, "phrase": "interest_point_feature_based_techniques"}, {"score": 0.003456132182564149, "phrase": "region_descriptors"}, {"score": 0.003365598242636633, "phrase": "action_classification_task"}, {"score": 0.0033212215487678854, "phrase": "cutting-edge_human_detection_method"}, {"score": 0.0032057142600527, "phrase": "generic_object_foreground_segments"}, {"score": 0.003080548510001051, "phrase": "non-human_objects"}, {"score": 0.0029733865110222785, "phrase": "video_scene"}, {"score": 0.0028826733506942554, "phrase": "extracted_segments"}, {"score": 0.002697474061842142, "phrase": "llc_coding"}, {"score": 0.002592100731070259, "phrase": "coding_scheme"}, {"score": 0.0025241429061867633, "phrase": "spatio-temporal_descriptors"}, {"score": 0.002490833392316362, "phrase": "local_coordinate_representation"}, {"score": 0.002425524064992056, "phrase": "human_action_classification_tasks"}, {"score": 0.002269636237060843, "phrase": "ucf"}, {"score": 0.002161695357601592, "phrase": "state-of-the-art_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Spatio-temporal segmentation", " Human body volume", " Object tracking", " Regions of interest", " Action recognition"], "paper_abstract": "There are nurnerous instances in which, in addition to the direct observation of a human body in motion, the characteristics of related objects can also contribute to the identification of human actions. The aim of the present paper is to address this issue and suggest a multi-feature method of determining human actions. This study addresses the matter by applying a sturdy region tracking method, instead of the conventional space-time interest point feature based techniques, demonstrating that region descriptors can be attained for the action classification task. A cutting-edge human detection method is applied to generate a model incorporating generic object foreground segments. These segments have been extended to include non-human objects which interact with a human in a video scene to capture the action semantically. Extracted segments are subsequently expressed using HOG/HOF descriptors in order to delineate their appearance and movement. The LLC coding is employed to optimise the codebook, the coding scheme projecting every one of the spatio-temporal descriptors into a local coordinate representation developed via max pooling. Human action classification tasks were used to assess the performance of this model. Experiments using KTH, UCF sports and Hollywood2 dataset show that this approach achieves the state-of-the-art performance. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "A unified spatio-temporal human body region tracking approach to action recognition", "paper_id": "WOS:000357910700006"}