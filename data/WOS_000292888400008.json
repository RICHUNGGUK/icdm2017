{"auto_keywords": [{"score": 0.037215072668367835, "phrase": "real_time"}, {"score": 0.03518546144454444, "phrase": "image_tags"}, {"score": 0.03301414096234223, "phrase": "human_tagging"}, {"score": 0.004777917315737992, "phrase": "multimedia_content"}, {"score": 0.00456159659731398, "phrase": "digital_images"}, {"score": 0.004526503177267299, "phrase": "multimedia_databases"}, {"score": 0.004439941010250576, "phrase": "multimedia_analysis"}, {"score": 0.004405779372462873, "phrase": "effective_tagging"}, {"score": 0.004338239182984759, "phrase": "manual_process"}, {"score": 0.004271729935895193, "phrase": "descriptive_tags"}, {"score": 0.0034941337957136013, "phrase": "alternate_approach"}, {"score": 0.0034140224493254935, "phrase": "speech_interface"}, {"score": 0.0032592499906265882, "phrase": "textual_annotations"}, {"score": 0.0032216622984027558, "phrase": "automated_speech_recognizers"}, {"score": 0.0029589086332593674, "phrase": "key_challenge"}, {"score": 0.002902236818129282, "phrase": "potential_low_recognition_quality"}, {"score": 0.0027492483892113, "phrase": "noisy_environments"}, {"score": 0.0026551731045904854, "phrase": "semantic_knowledge"}, {"score": 0.0024957862318504753, "phrase": "speech_recognition"}, {"score": 0.002429090371567499, "phrase": "speech_annotation"}, {"score": 0.002373339527491525, "phrase": "multiple_alternatives"}, {"score": 0.0023099082804523044, "phrase": "empirical_evaluation"}, {"score": 0.0022568866348667547, "phrase": "real_speech_recognizer's_output"}, {"score": 0.0022222152430170254, "phrase": "synthetic_data_sets"}, {"score": 0.0021796224306652326, "phrase": "significant_advantages"}, {"score": 0.0021544587602892466, "phrase": "proposed_approach"}, {"score": 0.0021213575697367148, "phrase": "recognizer's_output"}, {"score": 0.0021049977753042253, "phrase": "varying_conditions"}], "paper_keywords": ["Using speech for tagging and annotation", " using semantics to improve ASR", " maximum entropy approach", " correlation-based approach", " branch and bound algorithm"], "paper_abstract": "Associating textual annotations/tags with multimedia content is among the most effective approaches to organize and to support search over digital images and multimedia databases. Despite advances in multimedia analysis, effective tagging remains largely a manual process wherein users add descriptive tags by hand, usually when uploading or browsing the collection, much after the pictures have been taken. This approach, however, is not convenient in all situations or for many applications, e.g., when users would like to publish and share pictures with others in real time. An alternate approach is to instead utilize a speech interface using which users may specify image tags that can be transcribed into textual annotations by employing automated speech recognizers. Such a speech-based approach has all the benefits of human tagging without the cumbersomeness and impracticality typically associated with human tagging in real time. The key challenge in such an approach is the potential low recognition quality of the state-of-the-art recognizers, especially, in noisy environments. In this paper, we explore how semantic knowledge in the form of co-occurrence between image tags can be exploited to boost the quality of speech recognition. We postulate the problem of speech annotation as that of disambiguating among multiple alternatives offered by the recognizer. An empirical evaluation has been conducted over both real speech recognizer's output as well as synthetic data sets. The results demonstrate significant advantages of the proposed approach compared to the recognizer's output under varying conditions.", "paper_title": "A Semantics-Based Approach for Speech Annotation of Images", "paper_id": "WOS:000292888400008"}