{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "wearable_camera"}, {"score": 0.043812395733775156, "phrase": "physical_activity"}, {"score": 0.03733329838869367, "phrase": "pixel_correspondences"}, {"score": 0.004760765880314089, "phrase": "new_technique"}, {"score": 0.004671803485255806, "phrase": "physical_activity_patterns"}, {"score": 0.00463668357590034, "phrase": "image_sequences"}, {"score": 0.00443141503046841, "phrase": "standard_activity_recognition_schemes"}, {"score": 0.004381527499253281, "phrase": "video_data"}, {"score": 0.003942059405218598, "phrase": "camera_motion"}, {"score": 0.003882970272082245, "phrase": "acquired_video_frames"}, {"score": 0.0036553070575015344, "phrase": "activity_patterns"}, {"score": 0.0035869274480340727, "phrase": "multiscale_approach"}, {"score": 0.003467040410632942, "phrase": "existing_methods"}, {"score": 0.0033511469200347907, "phrase": "speed-up_robust_feature"}, {"score": 0.0030955250132409964, "phrase": "representative_motion_vectors"}, {"score": 0.002958281631770322, "phrase": "motion_statistics"}, {"score": 0.0027428939498108746, "phrase": "global_motion_distribution"}, {"score": 0.0026511422582578027, "phrase": "different_machine"}, {"score": 0.002591681512611419, "phrase": "k-nearest_neighbor"}, {"score": 0.0025342444933899857, "phrase": "bayesian"}, {"score": 0.002514464610980629, "phrase": "support_vector_machine"}, {"score": 0.0024120247307686084, "phrase": "physical_activities"}, {"score": 0.0023490143960001675, "phrase": "real-world_video"}, {"score": 0.0022447938263410023, "phrase": "specific_motion_features"}, {"score": 0.0022194677375148063, "phrase": "input_vectors"}, {"score": 0.002202742267484358, "phrase": "different_classifiers"}, {"score": 0.002153316680437517, "phrase": "similar_performances"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Activity recognition", " Classification", " Feature extraction", " Feature matching", " Motion histogram", " Multiscale"], "paper_abstract": "A new technique to extract and evaluate physical activity patterns from image sequences captured by a wearable camera is presented in this paper. Unlike standard activity recognition schemes, the video data captured by our device do not include the wearer him/herself. The physical activity of the wearer, such as walking or exercising, is analyzed indirectly through the camera motion extracted from the acquired video frames. Two key tasks, pixel correspondence identification and motion feature extraction, are studied to recognize activity patterns. We utilize a multiscale approach to identify pixel correspondences. When compared with the existing methods such as the good features detector and the speed-up robust feature (SURF) detector, our technique is more accurate and computationally efficient. Once the pixel correspondences are determined which define representative motion vectors, we build a set of activity pattern features based on motion statistics in each frame. Finally, the physical activity of the person wearing a camera is determined according to the global motion distribution in the video. Our algorithms are tested using different machine learning techniques such as the K-nearest neighbor (KNN), naive Bayesian and support vector machine (SVM). The results show that many types of physical activities can be recognized from field acquired real-world video. Our results also indicate that, with a design of specific motion features in the input vectors, different classifiers can be used successfully with similar performances. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Physical activity recognition based on motion in images acquired by a wearable camera", "paper_id": "WOS:000291915500015"}