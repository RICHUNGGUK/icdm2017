{"auto_keywords": [{"score": 0.0478899259608579, "phrase": "visual_stimuli"}, {"score": 0.04665550215861353, "phrase": "auditory_stimuli"}, {"score": 0.015719716506582538, "phrase": "physiological_signals"}, {"score": 0.014385333458227954, "phrase": "emotion_prediction"}, {"score": 0.004135859558144883, "phrase": "effective_elicitor"}, {"score": 0.004040211152784802, "phrase": "physiological_channels"}, {"score": 0.003931402465972151, "phrase": "well-controlled_experiment"}, {"score": 0.0036648265911110164, "phrase": "laboratory_setting"}, {"score": 0.0036363418376496484, "phrase": "numerous_physiological_signals"}, {"score": 0.0035940277928341265, "phrase": "facial_electromyogram"}, {"score": 0.0035383712891119937, "phrase": "skin_conductivity"}, {"score": 0.0033633514001658086, "phrase": "stimulus_presentation"}, {"score": 0.0031721009574803127, "phrase": "rough_set_technique"}, {"score": 0.0030986716686217768, "phrase": "emotion_prediction_models"}, {"score": 0.00300339527261653, "phrase": "physiological_data"}, {"score": 0.0029800360974218836, "phrase": "experimental_results"}, {"score": 0.002810520668490478, "phrase": "systematic_physiological_reactivity"}, {"score": 0.0027347383772384102, "phrase": "best_prediction_accuracy"}, {"score": 0.002579140290642792, "phrase": "six_emotion_categories"}, {"score": 0.0022583704173318123, "phrase": "culture-specific_models"}, {"score": 0.002197443382039439, "phrase": "affective_auditory_stimuli"}, {"score": 0.0021803388613879896, "phrase": "human-computer_interaction"}], "paper_keywords": ["human computer interaction", " interaction paradigms", " empirical studies in HCI"], "paper_abstract": "Unlike visual stimuli, little attention has been paid to auditory stimuli in terms of emotion prediction with physiological signals. This paper aimed to investigate whether auditory stimuli can be used as an effective elicitor as visual stimuli for emotion prediction using physiological channels. For this purpose, a well-controlled experiment was designed, in which standardized visual and auditory stimuli were systematically selected and presented to participants to induce various emotions spontaneously in a laboratory setting. Numerous physiological signals, including facial electromyogram, electroencephalography, skin conductivity and respiration data, were recorded when participants were exposed to the stimulus presentation. Two data mining methods, namely decision rules and k-nearest neighbor based on the rough set technique, were applied to construct emotion prediction models based on the features extracted from the physiological data. Experimental results demonstrated that auditory stimuli were as effective as visual stimuli in eliciting emotions in terms of systematic physiological reactivity. This was evidenced by the best prediction accuracy quantified by the F-1 measure (visual: 76.2% vs. auditory: 76.1%) among six emotion categories (excited, happy, neutral, sad, fearful and disgusted). Furthermore, we also constructed culture-specific (Chinese vs. Indian) prediction models. The results showed that model prediction accuracy was not significantly different between culture-specific models. Finally, the implications of affective auditory stimuli in human-computer interaction, limitations of the study and suggestions for further research are discussed.", "paper_title": "Emotion Prediction from Physiological Signals: A Comparison Study Between Visual and Auditory Elicitors", "paper_id": "WOS:000334363300007"}