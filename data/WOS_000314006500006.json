{"auto_keywords": [{"score": 0.025344561058257088, "phrase": "trec"}, {"score": 0.00481495049065317, "phrase": "modern_information_technologies"}, {"score": 0.00477924690801092, "phrase": "internet_services"}, {"score": 0.004587535692463736, "phrase": "growing_amount"}, {"score": 0.004553510708976456, "phrase": "textual_information"}, {"score": 0.004403480614319231, "phrase": "machine_learning_techniques"}, {"score": 0.004338390136272005, "phrase": "excellent_performance"}, {"score": 0.004211069233133584, "phrase": "artificial_intelligence"}, {"score": 0.004179824713643488, "phrase": "pattern_recognition"}, {"score": 0.00399714680711765, "phrase": "exact_answer_sentences"}, {"score": 0.0039674831861175935, "phrase": "vast_document_collections"}, {"score": 0.0038798015183611275, "phrase": "machine_learning-based_question-answering_framework"}, {"score": 0.0035479614996805383, "phrase": "answer_type"}, {"score": 0.0034566143156030426, "phrase": "context-ranking_model"}, {"score": 0.0033301694494391643, "phrase": "initial_retrievers"}, {"score": 0.0032686865485199806, "phrase": "flexible_features"}, {"score": 0.0031963987773420068, "phrase": "word_forms"}, {"score": 0.0031726587374218277, "phrase": "syntactic_features"}, {"score": 0.003137377828938824, "phrase": "semantic_word_features"}, {"score": 0.003102488033561956, "phrase": "proposed_context-ranking_model"}, {"score": 0.0030225753906647935, "phrase": "sequential_labeling"}, {"score": 0.00296675450822924, "phrase": "rich_features"}, {"score": 0.002911961514073769, "phrase": "input_passage"}, {"score": 0.0028581775908203683, "phrase": "question_type"}, {"score": 0.002815864555488112, "phrase": "trec-qa_tracks"}, {"score": 0.0027949428646076627, "phrase": "question_classification_benchmarks"}, {"score": 0.0027535633837592597, "phrase": "proposed_method"}, {"score": 0.002722930209707764, "phrase": "experimental_results"}, {"score": 0.0026232532659260033, "phrase": "additional_semantic_or_syntactic_taggers"}, {"score": 0.0025366596616434793, "phrase": "proposed_term_expansion_techniques"}, {"score": 0.0023107943665108465, "phrase": "qa_model"}, {"score": 0.0021687806803474367, "phrase": "simple_document"}, {"score": 0.0021526564019119466, "phrase": "passage_retrieval_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Question answering", " Information retrieval", " Question classification", " Passage retrieval", " Support vector machines"], "paper_abstract": "Modern information technologies and Internet services are suffering from the problem of selecting and managing a growing amount of textual information, to which access is often critical. Machine learning techniques have recently shown excellent performance and flexibility in many applications, such as artificial intelligence and pattern recognition. Question answering (QA) is a method of locating exact answer sentences from vast document collections. This paper presents a machine learning-based question-answering framework, which integrates a question classifier, simple document/passage retrievers, and the proposed context-ranking models. The question classifier is trained to categorize the answer type of the given question and instructs the context-ranking model to re-rank the passages retrieved from the initial retrievers. This method provides flexible features to learners, such as word forms, syntactic features, and semantic word features. The proposed context-ranking model, which is based on the sequential labeling of tasks, combines rich features to predict whether the input passage is relevant to the question type. We employ TREC-QA tracks and question classification benchmarks to evaluate the proposed method. The experimental results show that the question classifier achieves 85.60% accuracy without any additional semantic or syntactic taggers, and reached 88.60% after we employed the proposed term expansion techniques and a predefined related-word set. In the TREC-10 QA task, by using the gold TREC-provided relevant document set, the QA model achieves a 0.563 mean reciprocal rank (MRR) score, and a 0.342 MRR score is achieved after using the simple document and passage retrieval algorithms. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "A support vector machine-based context-ranking model for question answering", "paper_id": "WOS:000314006500006"}