{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "highly_parallel_fast_multipole_method"}, {"score": 0.042953630786438136, "phrase": "numerical_engine"}, {"score": 0.039282223508626896, "phrase": "spectral_method"}, {"score": 0.03771465750530587, "phrase": "pseudo-spectral_method"}, {"score": 0.004674952036988355, "phrase": "large-scale_direct_numerical_simulations"}, {"score": 0.004635701867952543, "phrase": "homogeneous-isotropic_fluid_turbulence"}, {"score": 0.004481954307639453, "phrase": "gpu_hardware"}, {"score": 0.004444317282171449, "phrase": "single_precision"}, {"score": 0.004333283735537634, "phrase": "vortex_particle_method"}, {"score": 0.004260800171269249, "phrase": "navier-stokes_equations"}, {"score": 0.004016485454498076, "phrase": "current_record"}, {"score": 0.003982741546775355, "phrase": "mesh_size"}, {"score": 0.0036914757355166966, "phrase": "standard_numerical_approach"}, {"score": 0.0035091888798162176, "phrase": "fft_algorithm"}, {"score": 0.003421437601439154, "phrase": "particle-based_simulations"}, {"score": 0.0033078284621762817, "phrase": "kinetic_energy_spectrum"}, {"score": 0.0031979796189858206, "phrase": "trusted_code"}, {"score": 0.003131178497790961, "phrase": "parallel_performance"}, {"score": 0.003104849179057156, "phrase": "weak_scaling_results"}, {"score": 0.0030657684627749364, "phrase": "fmm-based_vortex_method"}, {"score": 0.002939007076062123, "phrase": "mpi_process"}, {"score": 0.0027937732927852646, "phrase": "fft-based_spectral_method"}, {"score": 0.0025674490885184173, "phrase": "all-to-all_communication_pattern"}, {"score": 0.0024926348205500715, "phrase": "calculation_time"}, {"score": 0.002419995316129757, "phrase": "vortex_method"}, {"score": 0.0021773853011112882, "phrase": "largest_vortex-method_calculations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Isotropic turbulence", " Fast multipole method", " Integral equations", " GPU"], "paper_abstract": "This paper reports large-scale direct numerical simulations of homogeneous-isotropic fluid turbulence, achieving sustained performance of 1.08 petaflop/s on GPU hardware using single precision. The simulations use a vortex particle method to solve the Navier-Stokes equations, with a highly parallel fast multipole method (FMM) as numerical engine, and match the current record in mesh size for this application, a cube of 4096(3) computational points solved with a spectral method. The standard numerical approach used in this field is the pseudo-spectral method, relying on the FFT algorithm as the numerical engine. The particle-based simulations presented in this paper quantitatively match the kinetic energy spectrum obtained with a pseudo-spectral method, using a trusted code. In terms of parallel performance, weak scaling results show the FMM-based vortex method achieving 74% parallel efficiency on 4096 processes (one GPU per MPI process, 3 GPUS per node of the TSUBAME-2.0 system). The FFT-based spectral method is able to achieve just 14% parallel efficiency on the same number of MPI processes (using only CPU cores), due to the all-to-all communication pattern of the FFT algorithm. The calculation time for one time step was 108 s for the vortex method and 154 s for the spectral method, under these conditions. Computing with 69 billion particles, this work exceeds by an order of magnitude the largest vortex-method calculations to date. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Petascale turbulence simulation using a highly parallel fast multipole method on GPUs", "paper_id": "WOS:000315125500001"}