{"auto_keywords": [{"score": 0.01973408576503465, "phrase": "ncl"}, {"score": 0.010612359839915061, "phrase": "negative_correlation"}, {"score": 0.007041755771369889, "phrase": "g-ncl"}, {"score": 0.006108438324809517, "phrase": "mnce"}, {"score": 0.006014372032745795, "phrase": "control_parameter"}, {"score": 0.005906447089340327, "phrase": "error_function"}, {"score": 0.004728446189704709, "phrase": "proposed_ensemble_methods"}, {"score": 0.004643488753834639, "phrase": "accurate_neural_networks"}, {"score": 0.004548253531664032, "phrase": "negative_error_correlation"}, {"score": 0.004296264970612285, "phrase": "different_special_error_functions"}, {"score": 0.004262997588841983, "phrase": "simultaneous_training"}, {"score": 0.004208120996604837, "phrase": "negatively_correlated_nns"}, {"score": 0.003893325145898287, "phrase": "different_but_complementary_features"}, {"score": 0.0038431888954930083, "phrase": "hybrid_system"}, {"score": 0.0032639543564967908, "phrase": "negatively_correlated_experts"}, {"score": 0.0032052251526888697, "phrase": "first_approach"}, {"score": 0.0030749123945149753, "phrase": "base_experts"}, {"score": 0.003051073364818384, "phrase": "ncl_method"}, {"score": 0.0030274185919322506, "phrase": "suggested_combiner_method"}, {"score": 0.003003946660366632, "phrase": "efficient_tool"}, {"score": 0.002957545712858241, "phrase": "ncl_experts"}, {"score": 0.0028668768145593635, "phrase": "different_competences"}, {"score": 0.002829921088580641, "phrase": "different_parts"}, {"score": 0.002778979787458387, "phrase": "second_approach"}, {"score": 0.002624756485573134, "phrase": "training_algorithm"}, {"score": 0.0024855143489864976, "phrase": "regularization_term"}, {"score": 0.0024281181691350085, "phrase": "better_balance"}, {"score": 0.0024155441065367078, "phrase": "bias-variance-covariance_trade-offs"}, {"score": 0.002240353655129156, "phrase": "experimental_results"}, {"score": 0.002154772490492128, "phrase": "significantly_improved_performance"}, {"score": 0.0021380517771409763, "phrase": "original_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural networks ensemble", " Mixture of experts", " Negative correlation learning"], "paper_abstract": "Both theoretical and experimental studies have shown that combining accurate neural networks (NNs) in the ensemble with negative error correlation greatly improves their generalization abilities. Negative correlation learning (NCL) and mixture of experts (ME), two popular combining methods, each employ different special error functions for the simultaneous training of NNs to produce negatively correlated NNs. In this paper, we review the properties of the NCL and ME methods, discussing their advantages and disadvantages. Characterization of both methods showed that they have different but complementary features, so if a hybrid system can be designed to include features of both NCL and ME, it may be better than each of its basis approaches. In this study, two approaches are proposed to combine the features of both methods in order to solve the weaknesses of one method with the strength of the other method, i.e., gated-NCL (G-NCL) and mixture of negatively correlated experts (MNCE). In the first approach, G-NCL, a dynamic combiner of ME is used to combine the outputs of base experts in the NCL method. The suggested combiner method provides an efficient tool to evaluate and combine the NCL experts by the weights estimated dynamically from the inputs based on the different competences of each expert regarding different parts of the problem. In the second approach, MNCE, the capability of a control parameter for NCL is incorporated in the error function of ME, which enables the training algorithm of ME to efficiently adjust the measure of negative correlation between the experts. This control parameter can be regarded as a regularization term added to the error function of ME to establish better balance in bias-variance-covariance trade-offs and thus improves the generalization ability. The two proposed hybrid ensemble methods, G-NCL and MNCE, are compared with their constituent methods, ME and NCL, in solving several benchmark problems. The experimental results show that our proposed methods preserve the advantages and alleviate the disadvantages of their basis approaches, offering significantly improved performance over the original methods. (c) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Combining features of negative correlation learning with mixture of experts in proposed ensemble methods", "paper_id": "WOS:000308666500020"}