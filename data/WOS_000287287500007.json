{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "feature_selection"}, {"score": 0.004740476936977635, "phrase": "power_set_tree"}, {"score": 0.004571133708117498, "phrase": "important_preprocessing_step"}, {"score": 0.004523865550323285, "phrase": "pattern_recognition"}, {"score": 0.004477083974172596, "phrase": "machine_learning"}, {"score": 0.004430784015995566, "phrase": "data_mining"}, {"score": 0.004362226376603478, "phrase": "hill-climbing_search"}, {"score": 0.004206338218873218, "phrase": "optimal_reducts"}, {"score": 0.004119762981892093, "phrase": "current_stochastic_search_strategies"}, {"score": 0.004035201053052614, "phrase": "ga"}, {"score": 0.003993238603713839, "phrase": "aco"}, {"score": 0.003951934826017726, "phrase": "pso"}, {"score": 0.0037516215763875225, "phrase": "increased_computational_effort"}, {"score": 0.0036174746598098863, "phrase": "fast_and_effective_search_algorithms"}, {"score": 0.0034881076466982226, "phrase": "mathematical_tool"}, {"score": 0.003434085751246066, "phrase": "data_dependencies"}, {"score": 0.003226209534553224, "phrase": "purely_structural_methods"}, {"score": 0.0028622019660721363, "phrase": "order_tree"}, {"score": 0.0028178463431115562, "phrase": "power_set"}, {"score": 0.002759769970554556, "phrase": "possible_reduct"}, {"score": 0.0025524176761139413, "phrase": "rough_set_approach"}, {"score": 0.002435542816459651, "phrase": "pruning_rules"}, {"score": 0.0024103044278902916, "phrase": "ps-tree"}, {"score": 0.0022524959504997303, "phrase": "experiment_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Rough sets", " Feature selection", " Data mining", " PS-tree", " Reduction"], "paper_abstract": "Feature selection is viewed as an important preprocessing step for pattern recognition, machine learning and data mining. Traditional hill-climbing search approaches to feature selection have difficulties to find optimal reducts. And the current stochastic search strategies, such as GA, ACO and PSO, provide a more robust solution but at the expense of increased computational effort. It is necessary to investigate fast and effective search algorithms. Rough set theory provides a mathematical tool to discover data dependencies and reduce the number of features contained in a dataset by purely structural methods. In this paper, we define a structure called power set tree (PS-tree), which is an order tree representing the power set, and each possible reduct is mapped to a node of the tree. Then, we present a rough set approach to feature selection based on PS-tree. Two kinds of pruning rules for PS-tree are given. And two novel feature selection algorithms based on PS-tree are also given. Experiment results demonstrate that our algorithms are effective and efficient. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "A rough set approach to feature selection based on power set tree", "paper_id": "WOS:000287287500007"}