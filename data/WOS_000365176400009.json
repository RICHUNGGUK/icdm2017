{"auto_keywords": [{"score": 0.04967088836874542, "phrase": "temporally_extended_actions"}, {"score": 0.033849332343709, "phrase": "value_function"}, {"score": 0.0287723191055145, "phrase": "landmark_states"}, {"score": 0.00481495049065317, "phrase": "approximate_value_iteration"}, {"score": 0.004712988631183401, "phrase": "extended_actions"}, {"score": 0.004613175917207636, "phrase": "reinforcement_learning"}, {"score": 0.00443878620915958, "phrase": "efficient_planning"}, {"score": 0.004382127278148236, "phrase": "options_framework"}, {"score": 0.004326188427877999, "phrase": "concrete_way"}, {"score": 0.00418046991987597, "phrase": "existing_literature"}, {"score": 0.0038868464660354626, "phrase": "theoretical_analysis"}, {"score": 0.003692040748097991, "phrase": "primitive_actions"}, {"score": 0.0036137713873827374, "phrase": "general_analysis"}, {"score": 0.0035676056286816915, "phrase": "convergence_rate"}, {"score": 0.003522027550662857, "phrase": "popular_approximate_value_iteration"}, {"score": 0.003432604816716035, "phrase": "fitted_value_iteration"}, {"score": 0.003288567185099276, "phrase": "longer_duration_options"}, {"score": 0.0032465423581390625, "phrase": "pessimistic_estimate"}, {"score": 0.0031505544969981096, "phrase": "faster_convergence"}, {"score": 0.0028060889647071787, "phrase": "useful_options"}, {"score": 0.0026538960171433985, "phrase": "new_algorithm"}, {"score": 0.0024566663076299733, "phrase": "fvi"}, {"score": 0.0024356753573166056, "phrase": "lavi"}, {"score": 0.002404522326122611, "phrase": "proposed_landmark-based_options"}, {"score": 0.0022936610196951962, "phrase": "key_properties"}, {"score": 0.002178529882942255, "phrase": "important_role"}, {"score": 0.002159912892848893, "phrase": "avi"}, {"score": 0.0021322775033565805, "phrase": "approximation_error"}, {"score": 0.0021049977753042253, "phrase": "fast_convergence"}], "paper_keywords": [""], "paper_abstract": "Temporally extended actions have proven useful for reinforcement learning, but their duration also makes them valuable for efficient planning. The options framework provides a concrete way to implement and reason about temporally extended actions. Existing literature has demonstrated the value of planning with options empirically, but there is a lack of theoretical analysis formalizing when planning with options is more efficient than planning with primitive actions. We provide a general analysis of the convergence rate of a popular Approximate Value Iteration (AVI) algorithm called Fitted Value Iteration (FVI) with options. Our analysis reveals that longer duration options and a pessimistic estimate of the value function both lead to faster convergence. Furthermore, options can improve convergence even when they are suboptimal and sparsely distributed throughout the statespace. Next we consider the problem of generating useful options for planning based on a subset of landmark states. This suggests a new algorithm, Landmark-based AVI (LAVI), that represents the value function only at the landmark states. We analyze both FVI and LAVI using the proposed landmark-based options and compare the two algorithms. Our experimental results in three different domains demonstrate the key properties from the analysis. Our theoretical and experimental results demonstrate that options can play an important role in AVI by decreasing approximation error and inducing fast convergence.", "paper_title": "Approximate Value Iteration with Temporally Extended Actions", "paper_id": "WOS:000365176400009"}