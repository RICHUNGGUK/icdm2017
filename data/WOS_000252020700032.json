{"auto_keywords": [{"score": 0.0486293934284276, "phrase": "group-linking_method"}, {"score": 0.009723003219253465, "phrase": "recurrent_neural_networks"}, {"score": 0.00481495049065317, "phrase": "recurrent_neural_network"}, {"score": 0.004493130934067195, "phrase": "sequential_function"}, {"score": 0.004441616106463486, "phrase": "finite_memory_machines"}, {"score": 0.004407599980762272, "phrase": "minimal_order-the_machines"}, {"score": 0.004160612392068268, "phrase": "maximum_number"}, {"score": 0.004081385289516914, "phrase": "nontrivial_problem"}, {"score": 0.004034572092608487, "phrase": "total_number"}, {"score": 0.0039729853486821995, "phrase": "memory_order_k"}, {"score": 0.0038674532861321864, "phrase": "pretty_large_number"}, {"score": 0.003567338146229516, "phrase": "fmm"}, {"score": 0.003240139593712436, "phrase": "traditional_greedy-based_machine_learning_algorithm"}, {"score": 0.003178384350932103, "phrase": "useful_systematic_way"}, {"score": 0.0031418958350109038, "phrase": "unified_benchmarks"}, {"score": 0.003070166825722112, "phrase": "machine_learning_techniques"}, {"score": 0.0029885438439583075, "phrase": "learning_capability"}, {"score": 0.002897906501262407, "phrase": "finite_state_machines"}, {"score": 0.0027777389949155996, "phrase": "great_representation_power"}, {"score": 0.0026523079380037706, "phrase": "learning_exists"}, {"score": 0.002631959004685823, "phrase": "previous_learning_benchmarks"}, {"score": 0.002493810638588771, "phrase": "weight_space"}, {"score": 0.002427473263013356, "phrase": "great_expressive_power"}, {"score": 0.002381169982803105, "phrase": "convenient_framework"}, {"score": 0.002308910575113746, "phrase": "computation_capabilities"}, {"score": 0.0022648637058989463, "phrase": "fundamental_understanding"}], "paper_keywords": ["recurrent neural networks", " finite state machines", " grammatical inference", " NARX neural networks"], "paper_abstract": "This paper proposes a method (Group-Linking Method) that has control over the complexity of the sequential function to construct Finite Memory Machines with minimal order-the machines have the largest number of states based on their memory taps. Finding a machine with maximum number of states is a nontrivial problem because the total number of machines with memory order k is (256)(2k-2), a pretty large number. Based on the analysis of Group-Linking Method, it is shown that the amount of data necessary to reconstruct an FMM is the set of strings not longer than the depth of the machine plus one, which is significantly less than that required for traditional greedy-based machine learning algorithm. Group-Linking Method provides a useful systematic way of generating unified benchmarks to evaluate the capability of machine learning techniques. One example is to test the learning capability of recurrent neural networks. The problem of encoding finite state machines with recurrent neural networks has been extensively explored. However, the great representation power of those networks does not guarantee the solution in terms of learning exists. Previous learning benchmarks are shown to be not rich enough structurally in term of solutions in weight space. This set of benchmarks with great expressive power can serve as a convenient framework in which to study the learning and computation capabilities of various network models. A fundamental understanding of the capabilities of these networks will allow users to be able to select the most appropriate model for a given application.", "paper_title": "Group-linking method: A unified benchmark for machine learning with recurrent neural network", "paper_id": "WOS:000252020700032"}