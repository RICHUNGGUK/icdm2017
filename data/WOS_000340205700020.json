{"auto_keywords": [{"score": 0.04128881285829009, "phrase": "common_latent_factors"}, {"score": 0.03903298079028843, "phrase": "statistical_property"}, {"score": 0.03863573199265333, "phrase": "geometric_structure"}, {"score": 0.00481495049065317, "phrase": "graph_co-regularization"}, {"score": 0.004615202066192795, "phrase": "effective_technology"}, {"score": 0.004542441734438654, "phrase": "rich_labeled_data"}, {"score": 0.004470823343432748, "phrase": "source_domain"}, {"score": 0.004377077848734575, "phrase": "accurate_classifier"}, {"score": 0.0043080553835735825, "phrase": "target_domain"}, {"score": 0.004240116703465252, "phrase": "basic_assumption"}, {"score": 0.004151188559520802, "phrase": "input_domains"}, {"score": 0.0038339398365449507, "phrase": "important_property"}, {"score": 0.0037935066882208235, "phrase": "original_data"}, {"score": 0.003753498345597498, "phrase": "e._g."}, {"score": 0.003503497128638099, "phrase": "different_properties"}, {"score": 0.003466536738232506, "phrase": "input_data"}, {"score": 0.0032184697893776052, "phrase": "learning_model"}, {"score": 0.003150900886669123, "phrase": "domain_difference"}, {"score": 0.0030684249587688826, "phrase": "general_framework"}, {"score": 0.002988101395789399, "phrase": "graph_co-regularized_transfer_learning"}, {"score": 0.0027889366191837504, "phrase": "gtl"}, {"score": 0.0027015344365793016, "phrase": "knowledge_transfer"}, {"score": 0.0025214217930287003, "phrase": "latent_factors"}, {"score": 0.002481588370380518, "phrase": "negative_transfer"}, {"score": 0.0022554772846302773, "phrase": "nmf"}, {"score": 0.002231608290536979, "phrase": "nmtf"}, {"score": 0.0021847119298612264, "phrase": "extensive_experiments"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_learning_methods"}], "paper_keywords": ["Transfer learning", " negative transfer", " graph regularization", " matrix factorization", " text mining", " image classification"], "paper_abstract": "Transfer learning is established as an effective technology to leverage rich labeled data from some source domain to build an accurate classifier for the target domain. The basic assumption is that the input domains may share certain knowledge structure, which can be encoded into common latent factors and extracted by preserving important property of original data, e. g., statistical property and geometric structure. In this paper, we show that different properties of input data can be complementary to each other and exploring them simultaneously can make the learning model robust to the domain difference. We propose a general framework, referred to as Graph Co-Regularized Transfer Learning (GTL), where various matrix factorization models can be incorporated. Specifically, GTL aims to extract common latent factors for knowledge transfer by preserving the statistical property across domains, and simultaneously, refine the latent factors to alleviate negative transfer by preserving the geometric structure in each domain. Based on the framework, we propose two novel methods using NMF and NMTF, respectively. Extensive experiments verify that GTL can significantly outperform state-of-the-art learning methods on several public text and image datasets.", "paper_title": "Transfer Learning with Graph Co-Regularization", "paper_id": "WOS:000340205700020"}