{"auto_keywords": [{"score": 0.026950344619639553, "phrase": "krippendorff"}, {"score": 0.00481495049065317, "phrase": "human-computer_interaction"}, {"score": 0.004535124351828787, "phrase": "naturalistic_human-computer_interaction"}, {"score": 0.004375061788023814, "phrase": "intentions_experiences"}, {"score": 0.004120694410890683, "phrase": "human_communication_abilities"}, {"score": 0.003975200317233532, "phrase": "naturalistic_material"}, {"score": 0.003677280504320636, "phrase": "crucial_points"}, {"score": 0.0034016119212097826, "phrase": "realistic_user_reactions"}, {"score": 0.003321008590273796, "phrase": "clear_labels"}, {"score": 0.0032229262928681304, "phrase": "good_transcription"}, {"score": 0.002945669860541221, "phrase": "human_annotators"}, {"score": 0.0028245516301681713, "phrase": "good_measurement"}, {"score": 0.002741091312632307, "phrase": "labelled_material"}, {"score": 0.002692200353719145, "phrase": "inter-rater_agreement"}, {"score": 0.0025660347613972573, "phrase": "achieved_inter-rater_agreement"}, {"score": 0.00247529638625566, "phrase": "emotional_annotated_interaction_corpora"}, {"score": 0.002262209824105154, "phrase": "different_methods"}], "paper_keywords": ["Affective state", " Annotation", " Context influence", " Inter-rater agreement", " Labelling"], "paper_abstract": "To enable a naturalistic human-computer interaction the recognition of emotions and intentions experiences increased attention and several modalities are comprised to cover all human communication abilities. For this reason, naturalistic material is recorded, where the subjects are guided through an interaction with crucial points, but with the freedom to react individually. This material captures realistic user reactions but lacks of clear labels. So, a good transcription and annotation of the given material is essential. For that, the assignment of human annotators has become widely accepted. A good measurement for the reliability of labelled material is the inter-rater agreement. In this paper we investigate the achieved inter-rater agreement utilizing Krippendorff's alpha for emotional annotated interaction corpora and present methods to improve the reliability, we showthat the reliabilities obtained with different methods does not differ much, so a choice could rely on other aspects. Furthermore, a multimodal presentation of the items in their natural order increases the reliability.", "paper_title": "Inter-rater reliability for emotion annotation in human-computer interaction: comparison and methodological improvements", "paper_id": "WOS:000337753000003"}