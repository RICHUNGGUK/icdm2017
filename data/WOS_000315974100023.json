{"auto_keywords": [{"score": 0.04805304397885495, "phrase": "adaptive_mesh_refinement"}, {"score": 0.014831755644672269, "phrase": "cell_data"}, {"score": 0.014655168002956406, "phrase": "grid_cells"}, {"score": 0.011158528075485865, "phrase": "distributed_program"}, {"score": 0.011101186746209588, "phrase": "test_data"}, {"score": 0.00481495049065317, "phrase": "rapid_and_flexible_simulation_development"}, {"score": 0.004740631536499042, "phrase": "flexible_grid_library"}, {"score": 0.004716113134313215, "phrase": "highly_scalable_parallel_simulations"}, {"score": 0.004325568312096711, "phrase": "fluid_and_particle_codes"}, {"score": 0.004273525263209985, "phrase": "neighboring_cells"}, {"score": 0.004258770666682798, "phrase": "different_processes"}, {"score": 0.004156899121447682, "phrase": "excellent_scalability"}, {"score": 0.004092695445765521, "phrase": "magnetohydrodynamic_tests"}, {"score": 0.003967235882351874, "phrase": "mesh_metadata"}, {"score": 0.003939880387831148, "phrase": "mpi_processes"}, {"score": 0.0038925504383256595, "phrase": "amr"}, {"score": 0.003858936387653633, "phrase": "dccrg"}, {"score": 0.0038456073694665, "phrase": "free_software"}, {"score": 0.003633207722858776, "phrase": "program"}, {"score": 0.0036133826040506798, "phrase": "dccrg_catalogue"}, {"score": 0.0035391223341472502, "phrase": "cpc_program_library"}, {"score": 0.0035268943257252224, "phrase": "queen's_university"}, {"score": 0.0035147086785806674, "phrase": "belfast"}, {"score": 0.0035025644647418983, "phrase": "n._ireland"}, {"score": 0.0032569835069392924, "phrase": "tar.gz_programming_language"}, {"score": 0.0031407611978571575, "phrase": "mpi"}, {"score": 0.003113724055670354, "phrase": "ram"}, {"score": 0.0030024632953761935, "phrase": "external_routines"}, {"score": 0.002865320947289691, "phrase": "arbitrary_data"}, {"score": 0.002845541839157798, "phrase": "parallel_adaptive_mesh_refinement"}, {"score": 0.002835703398546821, "phrase": "transparent_remote_neighbor_data_updates"}, {"score": 0.002791848499778041, "phrase": "simulation_grid"}, {"score": 0.0027677775883341556, "phrase": "adjacency_list"}, {"score": 0.002724970309386977, "phrase": "hash_table"}, {"score": 0.0027061574024542243, "phrase": "contiguous_arrays"}, {"score": 0.002687474027832908, "phrase": "interface_standard"}, {"score": 0.0026367554281038572, "phrase": "template_parameter"}, {"score": 0.0025691287480907815, "phrase": "running_time"}, {"score": 0.0025293860389728516, "phrase": "solution_method"}, {"score": 0.0025206379415975083, "phrase": "small_problems"}, {"score": 0.002405450643837367, "phrase": "default_options"}, {"score": 0.002315488281619843, "phrase": "total_created_cells"}, {"score": 0.0022404996930993413, "phrase": "k._devine"}, {"score": 0.0022327484973125936, "phrase": "e._boman"}, {"score": 0.0022250240576349267, "phrase": "r._heaphy"}, {"score": 0.002217326282120832, "phrase": "b._hendrickson"}, {"score": 0.0022096550791379033, "phrase": "c._vaughan"}, {"score": 0.0022020103573658035, "phrase": "zoltan_data"}, {"score": 0.0021867999937268084, "phrase": "parallel_dynamic_applications"}, {"score": 0.002179234170770192, "phrase": "comput_sci"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Parallel grid", " Adaptive mesh refinement", " Free open source software"], "paper_abstract": "We present an easy to use and flexible grid library for developing highly scalable parallel simulations. The distributed cartesian cell-refinable grid (dccrg) supports adaptive mesh refinement and allows an arbitrary C++ class to be used as cell data. The amount of data in grid cells can vary both in space and time allowing dccrg to be used in very different types of simulations, for example in fluid and particle codes. Dccrg transfers the data between neighboring cells on different processes transparently and asynchronously allowing one to overlap computation and communication. This enables excellent scalability at least up to 32 k cores in magnetohydrodynamic tests depending on the problem and hardware. In the version of dccrg presented here part of the mesh metadata is replicated between MPI processes reducing the scalability of adaptive mesh refinement (AMR) to between 200 and 600 processes. Dccrg is free software that anyone can use, study and modify and is available at https://gitorious.org/dccrg. Users are also kindly requested to cite this work when publishing results obtained with dccrg. Program summary Program title: DCCRG Catalogue identifier: AEOM_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEOM_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU Lesser General Public License version 3 No. of lines in distributed program, including test data, etc.: 54975 No, of bytes in distributed program, including test data, etc.: 974015 Distribution format: tar.gz Programming language: C++. Computer: PC, cluster, supercomputer. Operating system: POSIX. The code has been parallelized using MPI and tested with 1-32768 processes RAM: 10 MB-10 GB per process Classification: 4.12, 4.14, 6.5, 19.3, 19.10, 20. External routines: MPI-2 [1], boost [2], Zoltan [3], sfc++ [4] Nature of problem: Grid library supporting arbitrary data in grid cells, parallel adaptive mesh refinement, transparent remote neighbor data updates and load balancing. Solution method: The simulation grid is represented by an adjacency list (graph) with vertices stored into a hash table and edges into contiguous arrays. Message Passing Interface standard is used for parallelization. Cell data is given as a template parameter when instantiating the grid. Restrictions: Logically cartesian grid. Running time: Running time depends on the hardware, problem and the solution method. Small problems can be solved in under a minute and very large problems can take weeks. The examples and tests provided with the package take less than about one minute using default options. In the version of dccrg presented here the speed of adaptive mesh refinement is at most of the order of 10(6) total created cells per second. References: [1] http://www.mpi-forum.org/. [2] http://www.boost.org/. [3] K. Devine, E. Boman, R. Heaphy, B. Hendrickson, C. Vaughan, Zoltan data management services for parallel dynamic applications, Comput Sci. Eng. 4 (2002) 90-97. http://dx.doi.org/10.1109/5992.988653. [4] https://gitorious.org/sfc++. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Parallel grid library for rapid and flexible simulation development", "paper_id": "WOS:000315974100023"}