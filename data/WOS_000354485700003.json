{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "smt-lib."}, {"score": 0.0046269981365987915, "phrase": "smt_competitions"}, {"score": 0.004490842671590769, "phrase": "smt_steering_committee"}, {"score": 0.003906357144047631, "phrase": "smt_benchmarks"}, {"score": 0.003075296862372287, "phrase": "key_observations"}, {"score": 0.00286795254228461, "phrase": "competition_results"}, {"score": 0.0023259039577313294, "phrase": "smt_applications"}, {"score": 0.0021049977753042253, "phrase": "general_coverage"}], "paper_keywords": ["SMT-COMP", " Competition", " SMT-LIB", " SMT solvers", " Model checking"], "paper_abstract": "After 8 years of SMT Competitions, the SMT Steering Committee decided, for 2013, to sponsor an evaluation of the status of SMT benchmarks and solvers, rather than another competition. This report summarizes the results of the evaluation, conducted by the authors. The key observations are that (1) the competition results are quite sensitive to randomness and (2) the most significant need for the future is assessment and improvement of benchmarks in the light of SMT applications. The evaluation also measured competitiveness of solvers, general coverage of solvers, logics, and benchmarks, and degree of repeatability of measurements and competitions.", "paper_title": "The 2013 Evaluation of SMT-COMP and SMT-LIB", "paper_id": "WOS:000354485700003"}