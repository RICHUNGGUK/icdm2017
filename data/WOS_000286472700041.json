{"auto_keywords": [{"score": 0.02916096320955431, "phrase": "awar"}, {"score": 0.008713768692145013, "phrase": "expensive_synchronization"}, {"score": 0.005013674244159618, "phrase": "raw"}, {"score": 0.004785896736401194, "phrase": "concurrent_algorithms"}, {"score": 0.004685575787741084, "phrase": "correct_and_efficient_concurrent_algorithms"}, {"score": 0.00460125407969429, "phrase": "difficult_problem"}, {"score": 0.004573483858954342, "phrase": "fundamental_importance"}, {"score": 0.004437115547184535, "phrase": "unnecessary_and_costly_synchronization"}, {"score": 0.004330941610682453, "phrase": "manual_trial-and-error_process_ad-hoc"}, {"score": 0.003578600071301459, "phrase": "concurrent_implementations"}, {"score": 0.0035569798792881935, "phrase": "classic_and_ubiquitous_specifications"}, {"score": 0.0034612884978779163, "phrase": "mutual_exclusion"}, {"score": 0.0034403745992576808, "phrase": "read-modify-write_operations"}, {"score": 0.0030847461428168614, "phrase": "shared_variable_a"}, {"score": 0.003010834638993771, "phrase": "different_shared_variable_b"}, {"score": 0.0028595832692215766, "phrase": "atomic_operation"}, {"score": 0.00280802944821096, "phrase": "shared_locations"}, {"score": 0.002707685828465434, "phrase": "current_mainstream_processors"}, {"score": 0.002383825621519496, "phrase": "regular_instructions"}, {"score": 0.0023622296334998324, "phrase": "algorithm_designers"}, {"score": 0.002169848044716866, "phrase": "flip_side"}, {"score": 0.0021049977753042253, "phrase": "new_algorithms"}], "paper_keywords": ["Algorithms", " Theory", " Concurrency", " Algorithms", " Lower Bounds", " Memory Fences", " Memory Barriers"], "paper_abstract": "Building correct and efficient concurrent algorithms is known to be a difficult problem of fundamental importance. To achieve efficiency, designers try to remove unnecessary and costly synchronization. However, not only is this manual trial-and-error process ad-hoc, time consuming and error-prone, but it often leaves designers pondering the question of: is it inherently impossible to eliminate certain synchronization, or is it that I was unable to eliminate it on this attempt and I should keep trying? In this paper we respond to this question. We prove that it is impossible to build concurrent implementations of classic and ubiquitous specifications such as sets, queues, stacks, mutual exclusion and read-modify-write operations, that completely eliminate the use of expensive synchronization. We prove that one cannot avoid the use of either: i) read-after-write (RAW), where a write to shared variable A is followed by a read to a different shared variable B without a write to B in between, or ii) atomic write-after-read (AWAR), where an atomic operation reads and then writes to shared locations. Unfortunately, enforcing RAW or AWAR is expensive on all current mainstream processors. To enforce RAW, memory ordering-also called fence or barrier-instructions must be used. To enforce AWAR, atomic instructions such as compare-and-swap are required. However, these instructions are typically substantially slower than regular instructions. Although algorithm designers frequently struggle to avoid RAW and AWAR, their attempts are often futile. Our result characterizes the cases where avoiding RAW and AWAR is impossible. On the flip side, our result can be used to guide designers towards new algorithms where RAW and AWAR can be eliminated.", "paper_title": "Laws of Order: Expensive Synchronization in Concurrent Algorithms Cannot be Eliminated", "paper_id": "WOS:000286472700041"}