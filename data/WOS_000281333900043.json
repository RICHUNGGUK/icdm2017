{"auto_keywords": [{"score": 0.04726673405217005, "phrase": "general_linear"}, {"score": 0.004823530554614883, "phrase": "matrix"}, {"score": 0.004655924035639928, "phrase": "least_trimmed_squares"}, {"score": 0.004471978831302331, "phrase": "sur_models"}, {"score": 0.004266501496765747, "phrase": "exact_least_trimmed_squares"}, {"score": 0.004209564126659545, "phrase": "lts"}, {"score": 0.00407042677958844, "phrase": "standard_regression_model"}, {"score": 0.0038833278847237858, "phrase": "lts_algorithm"}, {"score": 0.003704797078215222, "phrase": "seemingly_unrelated_regressions_models"}, {"score": 0.0036553070575015344, "phrase": "possible_singular_dispersion_matrices"}, {"score": 0.003510754567808717, "phrase": "regression_tree"}, {"score": 0.003417565253539315, "phrase": "optimal_estimates"}, {"score": 0.0033492947290751996, "phrase": "combinatorial_complexity"}, {"score": 0.0031738126998349775, "phrase": "generalized_linear"}, {"score": 0.0030688220103201836, "phrase": "efficient_matrix_techniques"}, {"score": 0.0029473940028924748, "phrase": "generalized_residual_sum"}, {"score": 0.0028498718992429825, "phrase": "subset_model"}, {"score": 0.002755567647101251, "phrase": "new_algorithm"}, {"score": 0.0027187231747900814, "phrase": "previous_computations"}, {"score": 0.002646501671714952, "phrase": "generalized_qr_decomposition"}, {"score": 0.0025935941070969575, "phrase": "single_row"}, {"score": 0.002541741549046619, "phrase": "sparse_structure"}, {"score": 0.0024247382451294255, "phrase": "theoretical_measures"}, {"score": 0.0023923066544220277, "phrase": "computational_complexity"}, {"score": 0.0023287359930308864, "phrase": "experimental_results"}, {"score": 0.0022365262201954643, "phrase": "new_algorithms"}, {"score": 0.0021917966722180132, "phrase": "outlying_observations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Least trimmed squares", " General linear model", " Seemingly unrelated regressions", " Generalized linear least squares"], "paper_abstract": "An algorithm for computing the exact least trimmed squares (LTS) estimator of the standard regression model has recently been proposed. The LTS algorithm is adapted to the general linear and seemingly unrelated regressions models with possible singular dispersion matrices. It searches through a regression tree to find the optimal estimates and has combinatorial complexity. The model is formulated as a generalized linear least squares problem. Efficient matrix techniques are employed to update the generalized residual sum of squares of a subset model. Specifically, the new algorithm utilizes previous computations to update a generalized QR decomposition by a single row. The sparse structure of the model is exploited. Theoretical measures of computational complexity are provided. Experimental results confirm the ability of the new algorithms to identify outlying observations. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Matrix strategies for computing the least trimmed squares estimation of the general linear and SUR models", "paper_id": "WOS:000281333900043"}