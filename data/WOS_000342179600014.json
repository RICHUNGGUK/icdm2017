{"auto_keywords": [{"score": 0.04069634062156296, "phrase": "gpu"}, {"score": 0.00481495049065317, "phrase": "mpi_derived_datatypes"}, {"score": 0.004761850836420121, "phrase": "noncontiguous_gpu-resident_data"}, {"score": 0.004580550308652044, "phrase": "efficient_and_generic_communication"}, {"score": 0.004530023871441992, "phrase": "noncontiguous_data_layouts"}, {"score": 0.0044800522651664695, "phrase": "gpu_memory"}, {"score": 0.004099683540502375, "phrase": "mpi"}, {"score": 0.003814212645832246, "phrase": "arbitrary_noncontiguous_gpu_data"}, {"score": 0.003730465168354817, "phrase": "datatypes_encoding"}, {"score": 0.003509487090932835, "phrase": "typically_tree-based_datatype_encoding"}, {"score": 0.0033016173793176075, "phrase": "cuda"}, {"score": 0.0031933617014256676, "phrase": "computational_method"}, {"score": 0.003158088307044493, "phrase": "dma-based_alternatives"}, {"score": 0.0029544572689681934, "phrase": "reasonable_dma-based_processing"}, {"score": 0.0028417741269408194, "phrase": "low_overhead"}, {"score": 0.002810373366887749, "phrase": "data_layouts"}, {"score": 0.0027486060614330043, "phrase": "best-case_dma_usage"}, {"score": 0.002643753276264565, "phrase": "layout-specific_implementations"}, {"score": 0.0025713100093065645, "phrase": "usage_scenarios"}, {"score": 0.002542890176213921, "phrase": "data_packing"}, {"score": 0.0025008468316682036, "phrase": "resource_contention"}, {"score": 0.0024594969044952108, "phrase": "potential_pitfalls"}, {"score": 0.0023265270905455334, "phrase": "kernel-based_packing"}, {"score": 0.002262756631083292, "phrase": "multifold_improvement"}, {"score": 0.0022377394845878268, "phrase": "point-to-point_communication"}, {"score": 0.0021285340816566906, "phrase": "shoc_stencil_benchmark"}, {"score": 0.0021049984022540494, "phrase": "hacc"}], "paper_keywords": ["MPI", " graphics processing unit", " CUDA", " datatype"], "paper_abstract": "Driven by the goals of efficient and generic communication of noncontiguous data layouts in GPU memory, for which solutions do not currently exist, we present a parallel, noncontiguous data-processing methodology through the MPI datatypes specification. Our processing algorithm utilizes a kernel on the GPU to pack arbitrary noncontiguous GPU data by enriching the datatypes encoding to expose a fine-grained, data-point level of parallelism. Additionally, the typically tree-based datatype encoding is preprocessed to enable efficient, cached access across GPU threads. Using CUDA, we show that the computational method outperforms DMA-based alternatives for several common data layouts as well as more complex data layouts for which reasonable DMA-based processing does not exist. Our method incurs low overhead for data layouts that closely match best-case DMA usage or that can be processed by layout-specific implementations. We additionally investigate usage scenarios for data packing that incur resource contention, identifying potential pitfalls for various packing strategies. We also demonstrate the efficacy of kernel-based packing in various communication scenarios, showing multifold improvement in point-to-point communication and evaluating packing within the context of the SHOC stencil benchmark and HACC mesh analysis.", "paper_title": "Processing MPI Derived Datatypes on Noncontiguous GPU-Resident Data", "paper_id": "WOS:000342179600014"}