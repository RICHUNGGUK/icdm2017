{"auto_keywords": [{"score": 0.046040545801964726, "phrase": "automated_oracles"}, {"score": 0.011527962758150967, "phrase": "sbst"}, {"score": 0.01027252855246574, "phrase": "evosuite"}, {"score": 0.008653952521928595, "phrase": "undeclared_exceptions"}, {"score": 0.004697902218671618, "phrase": "high_coverage"}, {"score": 0.00463229351153731, "phrase": "automated_unit_test_generation_techniques"}, {"score": 0.004153810449519519, "phrase": "representative_test_suites"}, {"score": 0.004081385289516914, "phrase": "branch_coverage"}, {"score": 0.00395417485449239, "phrase": "test_oracles"}, {"score": 0.0039264465913491356, "phrase": "search-based_testing"}, {"score": 0.0038444185258456245, "phrase": "promising_results"}, {"score": 0.0033277936960179892, "phrase": "search-based_approach"}, {"score": 0.0031901183059524804, "phrase": "evosuite_tool"}, {"score": 0.0031565959573536194, "phrase": "empirical_study"}, {"score": 0.003079736435077217, "phrase": "open_source_software_projects"}, {"score": 0.0029627094945653427, "phrase": "unique_advantage"}, {"score": 0.002880393760798211, "phrase": "traditional_goals"}, {"score": 0.002770920590799572, "phrase": "representative_test_sets"}, {"score": 0.0027417911324611917, "phrase": "chosen_coverage_criterion"}, {"score": 0.0026750045328382717, "phrase": "twice_as_many_failures"}, {"score": 0.002609840513713399, "phrase": "traditional_random_testing_approach"}, {"score": 0.0025642664420937327, "phrase": "real_faults"}, {"score": 0.0024842242921857705, "phrase": "five_classes"}, {"score": 0.002449454693767392, "phrase": "actual_faults"}, {"score": 0.0023480317852018133, "phrase": "implicit_preconditions"}, {"score": 0.0021273762177807876, "phrase": "software_development"}, {"score": 0.0021049977753042253, "phrase": "clean_program_interfaces"}], "paper_keywords": ["Search-based testing", " Automated test generation", " Test oracles"], "paper_abstract": "Automated unit test generation techniques traditionally follow one of two goals: Either they try to find violations of automated oracles (e.g., assertions, contracts, undeclared exceptions), or they aim to produce representative test suites (e.g., satisfying branch coverage) such that a developer can manually add test oracles. Search-based testing (SBST) has delivered promising results when it comes to achieving coverage, yet the use in conjunction with automated oracles has hardly been explored, and is generally hampered as SBST does not scale well when there are too many testing targets. In this paper we present a search-based approach to handle both objectives at the same time, implemented in the EvoSuite tool. An empirical study applying EvoSuite on 100 randomly selected open source software projects (the SF100 corpus) reveals that SBST has the unique advantage of being well suited to perform both traditional goals at the same time-efficiently triggering faults, while producing representative test sets for any chosen coverage criterion. In our study, EvoSuite detected twice as many failures in terms of undeclared exceptions as a traditional random testing approach, witnessing thousands of real faults in the 100 open source projects. Two out of every five classes with undeclared exceptions have actual faults, but these are buried within many failures that are caused by implicit preconditions. This \"noise\" can be interpreted as either a call for further research in improving automated oracles-or to make tools like EvoSuite an integral part of software development to enforce clean program interfaces.", "paper_title": "1600 faults in 100 projects: automatically finding faults while achieving high coverage with EvoSuite", "paper_id": "WOS:000354480800002"}