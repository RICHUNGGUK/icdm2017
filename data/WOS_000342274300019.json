{"auto_keywords": [{"score": 0.04911745263284474, "phrase": "three-dimensional_time-dependent_incompressible_navier-stokes_equations"}, {"score": 0.00481495049065317, "phrase": "mixed_spectral-finite_difference_computational_code"}, {"score": 0.004623443744732925, "phrase": "high-performance-computing_machines"}, {"score": 0.00457013354092966, "phrase": "computational-fluid-dynamics"}, {"score": 0.0043629425069254945, "phrase": "high-speed_interconnects"}, {"score": 0.004300133642951283, "phrase": "cluster_performance"}, {"score": 0.004250535418141079, "phrase": "upper_limit"}, {"score": 0.004189337956225294, "phrase": "scalar-processor_ability"}, {"score": 0.0041170581247889654, "phrase": "sustained_flops"}, {"score": 0.0040226133433241065, "phrase": "declared_peak_values"}, {"score": 0.0038962653593060427, "phrase": "electronic_industry"}, {"score": 0.0038624981944755813, "phrase": "new_trends"}, {"score": 0.0037195105792632937, "phrase": "high-performance_scientific_computing"}, {"score": 0.0036553070575015344, "phrase": "multicore-_and_manycore_chips"}, {"score": 0.0035610663187553614, "phrase": "parallel_programming"}, {"score": 0.003479331357315815, "phrase": "present_work"}, {"score": 0.0033699898113395328, "phrase": "fluid-flow_investigation"}, {"score": 0.003340768341189764, "phrase": "numerical_integration"}, {"score": 0.003311799410921826, "phrase": "navier-stokes_equations"}, {"score": 0.0032830808525570903, "phrase": "mixed_spectral-finite_difference"}, {"score": 0.00323576714040102, "phrase": "numerical_solution"}, {"score": 0.0030092017034068666, "phrase": "accurate_calculations"}, {"score": 0.002991774917857528, "phrase": "high_values"}, {"score": 0.0029658232220207254, "phrase": "flow_reynolds_number"}, {"score": 0.0027822459052173113, "phrase": "governing_equations"}, {"score": 0.0027341763737751467, "phrase": "different_code_implementations"}, {"score": 0.0026252106881752067, "phrase": "viscous_incompressible_fluid"}, {"score": 0.0026024303981224827, "phrase": "plane_channel"}, {"score": 0.0025500402081330394, "phrase": "long_tradition"}, {"score": 0.0025132639823395807, "phrase": "wall-bounded_turbulence_research"}, {"score": 0.0024770168196064, "phrase": "reference_case"}, {"score": 0.002427145218465736, "phrase": "direct_numerical_simulation"}, {"score": 0.00237137451107918, "phrase": "parallel_performance"}, {"score": 0.0023507916951658455, "phrase": "computational_code"}, {"score": 0.0023371689326096476, "phrase": "different_hardware_partitions"}, {"score": 0.0023168823213024856, "phrase": "computing_system"}, {"score": 0.002283460959126296, "phrase": "corresponding_different_sizes"}, {"score": 0.002263639474937039, "phrase": "computational_grid"}, {"score": 0.002237477619351199, "phrase": "remarkably-high_computational_performance"}, {"score": 0.0021607883988873492, "phrase": "gpgpu_boards"}, {"score": 0.0021420294823879292, "phrase": "computing-hardware_architecture"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Navier-Stokes equations", " Direct Numerical Simulation of turbulence", " Turbulent channel flow", " Hybrid CPU/GPGPU high-performance", " computing systems"], "paper_abstract": "In the recent past, High-Performance-Computing machines and in particular Computational-Fluid-Dynamics hardware, have been dominated by architectures that mainly included clusters of processors communicating on high-speed interconnects. Since a few years though, cluster performance has reached an upper limit for many technical codes. The scalar-processor ability has saturated, and the sustained Flops often result to be only a fraction of the declared peak values. In an attempt to overcome these difficulties, the electronic industry has unveiled new trends. Since about 2004, there has been a transition taking place in high-performance scientific computing, with the advent of multicore- and manycore chips, so that the different software models for parallel programming needed to be revisited. In the present work these issues are addressed with reference to the field of fluid-flow investigation through the numerical integration of the Navier-Stokes equations. A mixed spectral-finite difference computational code for the numerical solution of the three-dimensional time-dependent incompressible Navier-Stokes equations (with no forcing), is implemented on a high-performance hybrid CPU/GPGPU computing system. The computational algorithm is thought for the execution of accurate calculations at high values of the flow Reynolds number, so allowing the investigation of turbulence with the method of the Direct Numerical Simulation (no turbulence models in the system of the governing equations). The performance of different code implementations has been measured as related to the case of the flow of a viscous incompressible fluid in a plane channel, a problem that has a long tradition in the field of wall-bounded turbulence research, so becoming a reference case for the study of turbulence via Direct Numerical Simulation. Results are presented in terms of parallel performance of the computational code on different hardware partitions of the computing system, as related to corresponding different sizes of the computational grid, showing that remarkably-high computational performance is reached, mainly in virtue of the presence of the GPGPU boards in the computing-hardware architecture. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "GPGPU implementation of mixed spectral-finite difference computational code for the numerical integration of the three-dimensional time-dependent incompressible Navier-Stokes equations", "paper_id": "WOS:000342274300019"}