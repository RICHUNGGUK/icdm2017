{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "distributed_multi-gpu_systems"}, {"score": 0.010509123254216334, "phrase": "novel_gpu_direct-mpi_hybrid_approach"}, {"score": 0.007937046314168406, "phrase": "multiple_gpus"}, {"score": 0.007148640359315778, "phrase": "fragmented_data_exchange"}, {"score": 0.004392565073466714, "phrase": "limited_memory"}, {"score": 0.0043495571869907376, "phrase": "single_gpu"}, {"score": 0.004306968571854912, "phrase": "multi-gpu_systems"}, {"score": 0.004209202947762995, "phrase": "large-scale_mhd_simulations"}, {"score": 0.004140726362519362, "phrase": "data_transfer"}, {"score": 0.0038271712887391015, "phrase": "overall_performance_enhancement"}, {"score": 0.0036793800647811365, "phrase": "cpu_direct"}, {"score": 0.0036194914688460656, "phrase": "data_transfers"}, {"score": 0.0035605741849764187, "phrase": "single_node"}, {"score": 0.0035141291835319682, "phrase": "total_number"}, {"score": 0.0034911335714333507, "phrase": "message_passing_interface"}, {"score": 0.0033673122420551952, "phrase": "compute_unified_device_architecture"}, {"score": 0.0032800186243040663, "phrase": "memory_copy"}, {"score": 0.0028293966058473476, "phrase": "memory_usage"}, {"score": 0.0028108690791085536, "phrase": "computation_time"}, {"score": 0.0027560092576928595, "phrase": "computational_domain"}, {"score": 0.002737960968372391, "phrase": "experiment_results"}, {"score": 0.0027200305495826797, "phrase": "twice_the_flops"}, {"score": 0.00268452026617243, "phrase": "mpi-only_implementation_method"}, {"score": 0.0025977542995083624, "phrase": "efficient_implementation"}, {"score": 0.0025807396931129926, "phrase": "mhd_simulations"}, {"score": 0.002538687900036229, "phrase": "mgpu-mhd"}, {"score": 0.002480961181275199, "phrase": "gpu_parallelization"}, {"score": 0.0024566237485691383, "phrase": "total_variation"}, {"score": 0.0024086610445773096, "phrase": "multidimensional_ideal_mhd_equations"}, {"score": 0.0023079225317714815, "phrase": "numerical_tests"}, {"score": 0.0022928018832993387, "phrase": "performance_measurements"}, {"score": 0.0021968983291791784, "phrase": "double_precision"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["MHD simulations", " GPUs", " Distributed multi-GPU systems", " CUDA", " Parallel computing"], "paper_abstract": "Modern graphics processing units (GPUs) have been widely utilized in magnetohydrodynamic (MHD) simulations in recent years. Due to the limited memory of a single GPU, distributed multi-GPU systems are needed to be explored for large-scale MHD simulations. However, the data transfer between GPUs bottlenecks the efficiency of the simulations on such systems. In this paper we propose a novel GPU Direct-MPI hybrid approach to address this problem for overall performance enhancement. Our approach consists of two strategies: (1) We exploit CPU Direct 2.0 to speedup the data transfers between multiple GPUs in a single node and reduce the total number of message passing interface (MPI) communications; (2) We design Compute Unified Device Architecture (CUDA) kernels instead of using memory copy to speedup the fragmented data exchange in the three-dimensional (3D) decomposition. 3D decomposition is usually not preferable for distributed multi-GPU systems due to its low efficiency of the fragmented data exchange. Our approach has made a breakthrough to make 3D decomposition available on distributed multi-GPU systems. As a result, it can reduce the memory usage and computation time of each partition of the computational domain. Experiment results show twice the FLOPS comparing to common 2D decomposition MPI-only implementation method. The proposed approach has been developed in an efficient implementation for MHD simulations on distributed multi-GPU systems, called MGPU-MHD code. The code realizes the GPU parallelization of a total variation diminishing (TVD) algorithm for solving the multidimensional ideal MHD equations, extending our work from single GPU computation (Wong et al., 2011) to multiple GPUs. Numerical tests and performance measurements are conducted on the TSUBAME 2.0 supercomputer at the Tokyo Institute of Technology. Our code achieves 2 TFLOPS in double precision for the problem with 12003 grid points using 216 GPUs. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Efficient magnetohydrodynamic simulations on distributed multi-GPU systems using a novel GPU Direct-MPI hybrid approach", "paper_id": "WOS:000337768700006"}