{"auto_keywords": [{"score": 0.04314074539597285, "phrase": "upper_layer"}, {"score": 0.004815315858763141, "phrase": "distributed"}, {"score": 0.0047292449487169345, "phrase": "dataflow_engine_for_high_performance"}, {"score": 0.004481132747216135, "phrase": "rapidly_increasing_concurrency"}, {"score": 0.0044277810648955624, "phrase": "multi-petaflop_computing_systems"}, {"score": 0.004348936976349555, "phrase": "significant_programming_challenge"}, {"score": 0.003951453722517865, "phrase": "tightly-coupled_parallel_function"}, {"score": 0.0038348235692738783, "phrase": "many-task\"_programming_models"}, {"score": 0.003526186748088922, "phrase": "massive_numbers"}, {"score": 0.003361069456060196, "phrase": "significant_tightly_coupled_parallelism"}, {"score": 0.0033011569787033297, "phrase": "lower_level"}, {"score": 0.0032229262928681304, "phrase": "message_passing"}, {"score": 0.003053598205821532, "phrase": "large_scales"}, {"score": 0.0029280552768499056, "phrase": "task_distribution"}, {"score": 0.0028931405607654495, "phrase": "data_dependencies"}, {"score": 0.002841545331522694, "phrase": "intertask_data_movement"}, {"score": 0.002790867657645595, "phrase": "significant_performance_challenge"}, {"score": 0.0026601051654387793, "phrase": "turbine"}, {"score": 0.00240213106042981, "phrase": "generalized_many-task_intermediate_representation"}, {"score": 0.002262209824105154, "phrase": "multi-petaflop_infrastructures"}, {"score": 0.0021049977753042253, "phrase": "highly_concurrent_systems"}], "paper_keywords": ["dataflow language", " Swift", " ADLB", " MPI", " Turbine"], "paper_abstract": "Efficiently utilizing the rapidly increasing concurrency of multi-petaflop computing systems is a significant programming challenge. One approach is to structure applications with an upper layer of many loosely coupled coarse-grained tasks, each comprising a tightly-coupled parallel function or program. \"Many-task\" programming models such as functional parallel dataflow may be used at the upper layer to generate massive numbers of tasks, each of which generates significant tightly coupled parallelism at the lower level through multithreading, message passing, and/or partitioned global address spaces. At large scales, however, the management of task distribution, data dependencies, and intertask data movement is a significant performance challenge. In this work, we describe Turbine, a new highly scalable and distributed many-task dataflow engine. Turbine executes a generalized many-task intermediate representation with automated self-distribution and is scalable to multi-petaflop infrastructures. We present here the architecture of Turbine and its performance on highly concurrent systems.", "paper_title": "Turbine: A Distributed-memory Dataflow Engine for High Performance Many-task Applications", "paper_id": "WOS:000327181900005"}