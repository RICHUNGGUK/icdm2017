{"auto_keywords": [{"score": 0.049215829826051515, "phrase": "semantic_labels"}, {"score": 0.04786476323484584, "phrase": "emotion_recognition"}, {"score": 0.04753254042026914, "phrase": "affective_speech"}, {"score": 0.04671154347092391, "phrase": "acoustic-prosodic_information"}, {"score": 0.037459531374115645, "phrase": "sl-based_recognition"}, {"score": 0.00481495049065317, "phrase": "emotion_recognition_of_affective_speech"}, {"score": 0.004762665113390216, "phrase": "multiple_classifiers_using_acoustic-prosodic_information"}, {"score": 0.004509594355983191, "phrase": "multiple_classifiers"}, {"score": 0.00444464545285612, "phrase": "ap"}, {"score": 0.00430112267051817, "phrase": "ap-based_recognition"}, {"score": 0.004269913167716059, "phrase": "acoustic_and_prosodic_features"}, {"score": 0.004162445117081609, "phrase": "pitch-related_features"}, {"score": 0.0040873354289955605, "phrase": "detected_emotional_salient_segments"}, {"score": 0.004042919099733537, "phrase": "input_speech"}, {"score": 0.003800156761779008, "phrase": "base-level_classifiers"}, {"score": 0.0037588493723080757, "phrase": "meta_decision_tree"}, {"score": 0.0036508702548731383, "phrase": "classifier_fusion"}, {"score": 0.0035980453088879424, "phrase": "ap-based_emotion_recognition_confidence"}, {"score": 0.0034692911858848893, "phrase": "existing_chinese_knowledge_base"}, {"score": 0.0033696019683761274, "phrase": "emotion_association_rules"}, {"score": 0.003296713221268488, "phrase": "recognized_word_sequence"}, {"score": 0.0032253960472377356, "phrase": "maximum_entropy_model"}, {"score": 0.0030873424635143045, "phrase": "emotional_states"}, {"score": 0.002987682397171167, "phrase": "weighted_product_fusion_method"}, {"score": 0.0029230308228548807, "phrase": "ap-based_and_sl-based_recognition_results"}, {"score": 0.002777636514890319, "phrase": "neutral"}, {"score": 0.002648935542912504, "phrase": "speaker-independent_experimental_results"}, {"score": 0.0026105697442146357, "phrase": "emotion_recognition_performance"}, {"score": 0.002582168391175446, "phrase": "mdt"}, {"score": 0.002489667521500195, "phrase": "individual_classifier"}, {"score": 0.0024268966855267153, "phrase": "average_recognition_accuracy"}, {"score": 0.002215329893200788, "phrase": "ap-based_or_sl-based_approaches"}, {"score": 0.002167355210897559, "phrase": "individual_personality_trait"}, {"score": 0.0021515951441422082, "phrase": "personalized_application"}, {"score": 0.002128169201317242, "phrase": "recognition_accuracy"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Emotion recognition", " acoustic-prosodic features", " semantic labels", " meta decision trees", " personality trait"], "paper_abstract": "This work presents an approach to emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information (AP) and semantic labels (SLs). For AP-based recognition, acoustic and prosodic features including spectrum, formant, and pitch-related features are extracted from the detected emotional salient segments of the input speech. Three types of models, GMMs, SVMs, and MLPs, are adopted as the base-level classifiers. A Meta Decision Tree (MDT) is then employed for classifier fusion to obtain the AP-based emotion recognition confidence. For SL-based recognition, semantic labels derived from an existing Chinese knowledge base called HowNet are used to automatically extract Emotion Association Rules (EARs) from the recognized word sequence of the affective speech. The maximum entropy model (MaxEnt) is thereafter utilized to characterize the relationship between emotional states and EARs for emotion recognition. Finally, a weighted product fusion method is used to integrate the AP-based and SL-based recognition results for the final emotion decision. For evaluation, 2,033 utterances for four emotional states (Neutral, Happy, Angry, and Sad) are collected. The speaker-independent experimental results reveal that the emotion recognition performance based on MDT can achieve 80.00 percent, which is better than each individual classifier. On the other hand, an average recognition accuracy of 80.92 percent can be obtained for SL-based recognition. Finally, combining acoustic-prosodic information and semantic labels can achieve 83.55 percent, which is superior to either AP-based or SL-Based approaches. Moreover, considering the individual personality trait for personalized application, the recognition accuracy of the proposed approach can be further improved to 85.79 percent.", "paper_title": "Emotion Recognition of Affective Speech Based on Multiple Classifiers Using Acoustic-Prosodic Information and Semantic Labels", "paper_id": "WOS:000208758100003"}