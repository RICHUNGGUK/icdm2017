{"auto_keywords": [{"score": 0.03779444038009023, "phrase": "ml_technique"}, {"score": 0.00481495049065317, "phrase": "polyvalent_machine_learning_accelerator"}, {"score": 0.004773040022691547, "phrase": "machine_learning"}, {"score": 0.004649474569983707, "phrase": "pervasive_tools"}, {"score": 0.004450568197899267, "phrase": "powerful_computer_systems"}, {"score": 0.00433531459712787, "phrase": "general-purpose_cpus"}, {"score": 0.0042415434489566995, "phrase": "straightforward_solutions"}, {"score": 0.004042295865725559, "phrase": "hardware_accelerators"}, {"score": 0.00398959232769596, "phrase": "better_energy-efficiencies"}, {"score": 0.0037361664435836845, "phrase": "famous_no-free-lunch_theorem"}, {"score": 0.0032764307492734145, "phrase": "poor_learning_accuracy"}, {"score": 0.0031775430407770026, "phrase": "learning_accuracy"}, {"score": 0.0030414140732789186, "phrase": "concrete_ml_task"}, {"score": 0.0028108690791085536, "phrase": "ml_accelerator"}, {"score": 0.002737960968372391, "phrase": "seven_representative_ml_techniques"}, {"score": 0.00266693888510356, "phrase": "-nearest_neighbors"}, {"score": 0.0026321198895843173, "phrase": "support_vector_machine"}, {"score": 0.0026091594819333654, "phrase": "linear_regression"}, {"score": 0.0025863988422430797, "phrase": "classification_tree"}, {"score": 0.002552628669352939, "phrase": "deep_neural_network"}, {"score": 0.0024864020775253767, "phrase": "computational_primitives"}, {"score": 0.002464709648714996, "phrase": "locality_properties"}, {"score": 0.002443206010337041, "phrase": "different_ml_techniques"}], "paper_keywords": [""], "paper_abstract": "Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique. In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1 0 5 6 GOP/s (e.g., additions and multiplications) in an area of 3 : 5 1 mm 2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.", "paper_title": "PuDianNao: A Polyvalent Machine Learning Accelerator", "paper_id": "WOS:000370874900026"}