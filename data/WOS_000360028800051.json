{"auto_keywords": [{"score": 0.043720404506452065, "phrase": "elm"}, {"score": 0.01103506240406792, "phrase": "massive_learning_applications"}, {"score": 0.00481495049065317, "phrase": "distributed_extreme_learning_machine"}, {"score": 0.004652963867892925, "phrase": "extreme_learning_machine"}, {"score": 0.00413767314434795, "phrase": "support_vector_machine"}, {"score": 0.003978866233891427, "phrase": "optimization_point"}, {"score": 0.003863758357292741, "phrase": "unified_learning_schemes"}, {"score": 0.0038074544783102226, "phrase": "widespread_type"}, {"score": 0.003770373393393271, "phrase": "feature_mappings"}, {"score": 0.003697287104870263, "phrase": "unified_algorithms"}, {"score": 0.0035379628447303703, "phrase": "random_feature_mappings"}, {"score": 0.0034355662264234864, "phrase": "exponentially_increasing_volume"}, {"score": 0.0034020946201397057, "phrase": "training_data"}, {"score": 0.0032554492909625653, "phrase": "great_memory_consumption"}, {"score": 0.0032237268289353983, "phrase": "large_matrix_operations"}, {"score": 0.00311510526624626, "phrase": "high_communication_cost"}, {"score": 0.0030397596289795143, "phrase": "matrix_operations"}, {"score": 0.0029086869572567072, "phrase": "computing_model"}, {"score": 0.0027969169552385974, "phrase": "distributed_solution"}, {"score": 0.002769650208851401, "phrase": "distributed_kernelized_elm"}, {"score": 0.002742648551009682, "phrase": "dk-elm"}, {"score": 0.00258609443122187, "phrase": "mapreduce"}, {"score": 0.002548331088496964, "phrase": "kernel_matrix_calculation"}, {"score": 0.0024028135317872, "phrase": "parallel_calculation"}, {"score": 0.002379379676378223, "phrase": "dk-elm._extensive_experiments"}, {"score": 0.002356173823922118, "phrase": "massive_datasets"}, {"score": 0.002265586601922693, "phrase": "training_performance"}, {"score": 0.0022434880661725493, "phrase": "dk-elm._experimental_results"}, {"score": 0.0021891781017389783, "phrase": "good_scalability"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme Learning Machine", " Extreme Learning Machine with kernels", " Massive data learning", " MapReduce"], "paper_abstract": "Extreme Learning Machine (ELM) has shown its good generalization performance and extremely fast learning speed in many learning applications. Recently, it has been proved that ELM outperforms Support Vector Machine (SVM) with less constraints from the optimization point of view. ELM provides unified learning schemes with a widespread type of feature mappings. Among these unified algorithms, ELM with kernels applies kernels instead of random feature mappings. However, with the exponentially increasing volume of training data in massive learning applications, centralized ELM with kernels suffers from the great memory consumption of large matrix operations. Besides, due to the high communication cost, some of these matrix operations cannot be directly implemented on shared-nothing distributed computing model like MapReduce. This paper proposes a distributed solution named Distributed Kernelized ELM (DK-ELM), which realizes an implementation of ELM with kernels on MapReduce. Distributed kernel matrix calculation and multiplication of matrix with vector are also applied to realize parallel calculation of DK-ELM. Extensive experiments on massive datasets are conducted to verify both the scalability and training performance of DK-ELM. Experimental results show that DK-ELM has good scalability for massive learning applications. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Distributed Extreme Learning Machine with kernels based on MapReduce", "paper_id": "WOS:000360028800051"}