{"auto_keywords": [{"score": 0.028953640115423073, "phrase": "hamat"}, {"score": 0.00481495049065317, "phrase": "smt_processors"}, {"score": 0.004783483480147339, "phrase": "resizable_caches"}, {"score": 0.004674952036988355, "phrase": "access_speed"}, {"score": 0.00450933377534525, "phrase": "single-threaded_cores"}, {"score": 0.004378182273895574, "phrase": "processor_performance"}, {"score": 0.004264795176205885, "phrase": "running_application"}, {"score": 0.003967835031625103, "phrase": "even_more_opportunities"}, {"score": 0.003916098498242888, "phrase": "cache_resources"}, {"score": 0.003703611021999795, "phrase": "data_cache_reconfiguring"}, {"score": 0.0036673238814279286, "phrase": "smt_core_changes"}, {"score": 0.003607630811220324, "phrase": "running_threads_increases"}, {"score": 0.0035141291835319682, "phrase": "resizable_cache_control_algorithm"}, {"score": 0.003378385493203297, "phrase": "critical_path"}, {"score": 0.0032160319265621285, "phrase": "hit_behavior"}, {"score": 0.0031636612155559267, "phrase": "large_smt_workloads"}, {"score": 0.003091767402370757, "phrase": "cache_miss"}, {"score": 0.0030115953761477503, "phrase": "seemingly_diametrically_opposed_policies"}, {"score": 0.002914288956035066, "phrase": "arithmetic_mean_cache_access_time"}, {"score": 0.0028574168946229675, "phrase": "amat"}, {"score": 0.002547043222086858, "phrase": "previously_proposed_globally_asynchronous"}, {"score": 0.002464709648714996, "phrase": "smt_support"}, {"score": 0.0024485643104830814, "phrase": "dynamically_resizable_caches"}, {"score": 0.0024007585788965655, "phrase": "hamat_algorithm"}, {"score": 0.0023694065215394593, "phrase": "amat_algorithm"}, {"score": 0.0023538840032613535, "phrase": "four-thread_workloads"}, {"score": 0.0022703060319871713, "phrase": "overall_performance_improvements"}, {"score": 0.002146926767291475, "phrase": "best_fixed-configuration_cache_design"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Adaptive caches", " Reconfigurable caches", " Cache memories", " GALS", " Simultaneous Multi-Threading"], "paper_abstract": "Resizable caches can trade-off capacity for access speed to dynamically match the needs of the workload. In single-threaded cores, resizable caches have demonstrated their ability to improve processor performance by adapting to the phases of the running application. In Simultaneous Multi-Threaded (SMT) cores, the caching needs can vary greatly across the number of threads and their characteristics, thus, offering even more opportunities to dynamically adjust cache resources to the workload. In this paper, we demonstrate that the preferred control methodology for data cache reconfiguring in a SMT core changes as the number of running threads increases. In workloads with one or two threads, the resizable cache control algorithm should optimize for cache miss behavior because misses typically form the critical path. In contrast, with several independent threads running, we show that optimizing for cache hit behavior has more impact, since large SMT workloads have other threads to run during a cache miss. Moreover, we demonstrate that these seemingly diametrically opposed policies are closely related mathematically; the former minimizes the arithmetic mean cache access time (which we will call AMAT), while the latter minimizes its harmonic mean. We introduce an algorithm (HAMAT) that smoothly and naturally adjusts between the two strategies with the degree of multi-threading. We extend a previously proposed Globally Asynchronous, Locally Synchronous (GALS) processor core with SMT support and dynamically resizable caches. We show that the HAMAT algorithm significantly outperforms the AMAT algorithm on four-thread workloads while matching its performance on one and two thread workloads. Moreover, HAMAT achieves overall performance improvements of 18.7%, 10.1%, and 14.2% on one, two, and four thread workloads, respectively, over the best fixed-configuration cache design. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A phase adaptive cache hierarchy for SMT processors", "paper_id": "WOS:000298456500004"}