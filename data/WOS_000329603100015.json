{"auto_keywords": [{"score": 0.04404097903507225, "phrase": "tao"}, {"score": 0.005203947231142527, "phrase": "dml"}, {"score": 0.00481495049065317, "phrase": "non-smooth_empirical_risk_minimization_based_distance"}, {"score": 0.004437161896512157, "phrase": "bian"}, {"score": 0.0036498216883673338, "phrase": "dml."}, {"score": 0.003629557750546267, "phrase": "constrained_empirical_risk_minimization"}, {"score": 0.003310813458610484, "phrase": "smooth_approximation_method"}, {"score": 0.003150900886669123, "phrase": "non-differentiable_hinge_loss_function"}, {"score": 0.0030199761431186434, "phrase": "objective_function"}, {"score": 0.0029775511937434797, "phrase": "hinge_loss"}, {"score": 0.0028740705307540317, "phrase": "non-smooth_min-max_representation"}, {"score": 0.0027741761880779535, "phrase": "approximate_objective_function"}, {"score": 0.002640114583897493, "phrase": "original_objective_function"}, {"score": 0.0025846562485378247, "phrase": "approximate_one"}, {"score": 0.0025125151618810523, "phrase": "lipschitz-continuous_gradient"}, {"score": 0.0024423826953133844, "phrase": "nesterov's_optimal_first-order_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Distance metric learning", " Empirical risk minimization", " Smooth approximation", " Nesterov's optimal first-order method"], "paper_abstract": "Distance metric learning (DML) has become a very active research field in recent years. Bian and Tao (IEEE Trans. Neural Netw. Learn. Syst. 23(8) (2012) 1194-1205) presented a constrained empirical risk minimization (ERM) framework for DML. In this paper, we utilize smooth approximation method to make their algorithm applicable to the non-differentiable hinge loss function. We show that the objective function with hinge loss is equivalent to a non-smooth min-max representation, from which an approximate objective function is derived. Compared to the original objective function, the approximate one becomes differentiable with Lipschitz-continuous gradient. Consequently, Nesterov's optimal first-order method can be directly used. Finally, the effectiveness of our method is evaluated on various UCI datasets. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Smooth approximation method for non-smooth empirical risk minimization based distance metric learning", "paper_id": "WOS:000329603100015"}