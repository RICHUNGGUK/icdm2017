{"auto_keywords": [{"score": 0.04574703079843426, "phrase": "upc"}, {"score": 0.00481495049065317, "phrase": "large_scale_machines"}, {"score": 0.004558414405811025, "phrase": "scalable_run-time_system"}, {"score": 0.004424268136268051, "phrase": "unified_parallel_c"}, {"score": 0.004294052530714228, "phrase": "experimental_evaluation"}, {"score": 0.003848413823574867, "phrase": "runtime_system"}, {"score": 0.00367967106192446, "phrase": "efficient_mpi_programs"}, {"score": 0.003643185395780667, "phrase": "good_performance_scalability"}, {"score": 0.003397683082821229, "phrase": "shared_object_consistency"}, {"score": 0.0033306180744375616, "phrase": "distributed_memory_machine"}, {"score": 0.0032004205378150354, "phrase": "parallel_loops"}, {"score": 0.0030906666356950887, "phrase": "affinity_tests"}, {"score": 0.002925728709297618, "phrase": "shared_arrays"}, {"score": 0.0027557911038816256, "phrase": "remote_update_operations"}, {"score": 0.0027148677103585985, "phrase": "lowercost_asynchronous_message"}, {"score": 0.0026745503978347143, "phrase": "performance_evaluation"}, {"score": 0.0025827837420330816, "phrase": "hpc_stream"}, {"score": 0.0025571463140118805, "phrase": "nas_cg_-"}, {"score": 0.0022127018991294047, "phrase": "hpc_challenge_competition"}, {"score": 0.002168975681697056, "phrase": "seattle_wa"}, {"score": 0.002126111716875142, "phrase": "pgas_languages"}], "paper_keywords": ["performance", " experimentation", " PGAS programming model", " UPC", " BlueGene"], "paper_abstract": "This paper describes the design and implementation of a scalable run-time system and an optimizing compiler for Unified Parallel C (UPC). An experimental evaluation on BlueGene/L (R), a distributed-memory machine, demonstrates that the combination of the compiler with the runtime system produces programs with performance comparable to that of efficient MPI programs and good performance scalability up to hundreds of thousands of processors. Our runtime system design solves the problem of maintaining shared object consistency efficiently in a distributed memory machine. Our compiler infrastructure simplifies the code generated for parallel loops in UPC through the elimination of affinity tests, eliminates several levels of indirection for accesses to segments of shared arrays that the compiler can prove to be local, and implements remote update operations through a lowercost asynchronous message. The performance evaluation uses three well-known benchmarks - HPC RandomAccess, HPC STREAM and NAS CG - to obtain scaling and absolute performance numbers for these benchmarks on up to 131072 processors, the full BlueGene/L machine. These results were used to win the HPC Challenge Competition at SC05 in Seattle WA, demonstrating that PGAS languages support both productivity and performance.", "paper_title": "Shared memory programming for large scale machines", "paper_id": "WOS:000202972100010"}