{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "mapreduce_workloads"}, {"score": 0.004741304218199546, "phrase": "shared_scans"}, {"score": 0.004683195250422422, "phrase": "mapreduce_clusters"}, {"score": 0.004625795150473604, "phrase": "multiple_concurrent_jobs"}, {"score": 0.0044853402274804815, "phrase": "distinct_datasets"}, {"score": 0.004204054047427536, "phrase": "multiple_map_phase_jobs"}, {"score": 0.004026359831748917, "phrase": "map_phase_jobs"}, {"score": 0.003750529559133893, "phrase": "previous_work"}, {"score": 0.0036477941360224435, "phrase": "highly_general_method"}, {"score": 0.0031551446537729107, "phrase": "optimal_schedule"}, {"score": 0.0031164146336810717, "phrase": "natural_chain_precedence"}, {"score": 0.0030403706811144, "phrase": "significant_but_natural_generalization"}, {"score": 0.0030123335878664064, "phrase": "recently_introduced_flex_scheduler"}, {"score": 0.0029297595441334823, "phrase": "cyclic_piggybacking_paradigm"}, {"score": 0.00284065508239492, "phrase": "cost_metrics"}, {"score": 0.002805774617782258, "phrase": "average_response_time"}, {"score": 0.0027884948773692153, "phrase": "average_stretch"}, {"score": 0.002754253119972439, "phrase": "minimax-type_metric"}, {"score": 0.002565345470805401, "phrase": "identical_datasets"}, {"score": 0.0024340759772801847, "phrase": "chain_precedence"}, {"score": 0.002404175869425017, "phrase": "arbitrary_precedence"}, {"score": 0.00230950802983053, "phrase": "overall_approach"}, {"score": 0.002281134611390528, "phrase": "cyclic_piggybacking"}, {"score": 0.002260083020280497, "phrase": "flex_scheduling_generalization"}, {"score": 0.0022048882869630114, "phrase": "practical_implementation_strategies"}, {"score": 0.0021510385886131557, "phrase": "circumflex"}, {"score": 0.0021049977753042253, "phrase": "real_benchmark_experiments"}], "paper_keywords": ["MapReduce", " Shared scans", " Scheduling", " Allocation", " Optimization", " Amortization"], "paper_abstract": "We consider MapReduce clusters designed to support multiple concurrent jobs, concentrating on environments in which the number of distinct datasets is modest relative to the number of jobs. In such scenarios, many individual datasets are likely to be scanned concurrently by multiple Map phase jobs. As has been noticed previously, this scenario provides an opportunity for Map phase jobs to cooperate, sharing the scans of these datasets, and thus reducing the costs of such scans. Our paper has three main contributions over previous work. First, we present a novel and highly general method for sharing scans and thus amortizing their costs. This concept, which we call cyclic piggybacking, has a number of advantages over the more traditional batching scheme described in the literature. Second, we notice that the various subjobs generated in this manner can be assumed in an optimal schedule to respect a natural chain precedence ordering. Third, we describe a significant but natural generalization of the recently introduced FLEX scheduler for optimizing schedules within the context of this cyclic piggybacking paradigm, which can be tailored to a variety of cost metrics. Such cost metrics include average response time, average stretch, and any minimax-type metric-a total of 11 separate and standard metrics in all. Moreover, most of this carries over in the more general case of overlapping rather than identical datasets as well, employing what we will call semi-shared scans. In such scenarios, chain precedence is replaced by arbitrary precedence, but we can still handle 8 of the original 11 metrics. The overall approach, including both cyclic piggybacking and the FLEX scheduling generalization, is called CIRCUMFLEX. We describe some practical implementation strategies. And we evaluate the performance of CIRCUMFLEX via a variety of simulation and real benchmark experiments.", "paper_title": "On the optimization of schedules for MapReduce workloads in the presence of shared scans", "paper_id": "WOS:000308959900002"}