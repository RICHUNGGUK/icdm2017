{"auto_keywords": [{"score": 0.037989064425371935, "phrase": "approximate_solution"}, {"score": 0.031542935663382905, "phrase": "proposed_controller"}, {"score": 0.00481495049065317, "phrase": "bayesian-game-based"}, {"score": 0.0046848707075530256, "phrase": "decentralized_pomdps"}, {"score": 0.004558289038745564, "phrase": "bayesian-game-based_fuzzy_reinforcement_learning"}, {"score": 0.00451696570324833, "phrase": "rl"}, {"score": 0.004435112270935294, "phrase": "decentralized_partially_observable_markov_decision_processes"}, {"score": 0.0042178318724732005, "phrase": "powerful_platform"}, {"score": 0.004160435626849478, "phrase": "multiagent_sequential_decision"}, {"score": 0.004103817210338686, "phrase": "partially_observable_stochastic_environments"}, {"score": 0.0040111533608228195, "phrase": "exact_optimal_solutions"}, {"score": 0.0038672216986617145, "phrase": "nexp"}, {"score": 0.003561865840737996, "phrase": "fuzzy_inference_systems"}, {"score": 0.003465521267039616, "phrase": "game-based_rl"}, {"score": 0.003371773890338284, "phrase": "powerful_universal_approximation_capability"}, {"score": 0.0029396242707327986, "phrase": "fis-based_rl_controller"}, {"score": 0.002912864010632337, "phrase": "dec-pomdps"}, {"score": 0.0028340217672278975, "phrase": "bayesian_games"}, {"score": 0.002634031703695684, "phrase": "free_communication"}, {"score": 0.0024593458703463474, "phrase": "proposed_approach"}, {"score": 0.002234031373581961, "phrase": "simulation_results"}, {"score": 0.0022136780933978612, "phrase": "comparative_evaluation"}, {"score": 0.0021049977753042253, "phrase": "fis-based_game-theoretic_rl"}], "paper_keywords": ["Bayesian games (BGs)", " decentralized partially observable Markov decision processes (Dec-POMDPs)", " fuzzy systems", " reinforcement learning (RL)"], "paper_abstract": "This paper proposes a Bayesian-game-based fuzzy reinforcement learning (RL) controller for decentralized partially observable Markov decision processes (Dec-POMDPs). Dec-POMDPs have recently emerged as a powerful platform for optimizing multiagent sequential decision making in partially observable stochastic environments. However, finding exact optimal solutions to a Dec-POMDP is provably intractable (NEXP-complete), necessitating the use of approximate/suboptimal solution approaches. This approach proposes an approximate solution by employing fuzzy inference systems (FISs) in a game-based RL setting. It uses the powerful universal approximation capability of fuzzy systems to compactly represent a Dec-POMDP as a fuzzy Dec-POMDP, allowing the controller to progressively learn and update an approximate solution to the underlying Dec-POMDP. The proposed controller envisages an FIS-based RL controller for Dec-POMDPs modeled as a sequence of Bayesian games (BGs). We implement the proposed controller for two scenarios: 1) Dec-POMDPs with free communication between agents; and 2) Dec-POMDPs without communication. We empirically evaluate the proposed approach on three standard benchmark problems: 1) multiagent tiger; 2) multiaccess broadcast channel; and 3) recycling robot. Simulation results and comparative evaluation against other Dec-POMDP solution approaches elucidate the effectiveness and feasibility of employing FIS-based game-theoretic RL for designing Dec-POMDP controllers.", "paper_title": "Bayesian-Game-Based Fuzzy Reinforcement Learning Control for Decentralized POMDPs", "paper_id": "WOS:000312561100006"}