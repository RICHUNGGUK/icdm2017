{"auto_keywords": [{"score": 0.044546764346383334, "phrase": "tars"}, {"score": 0.010890718034190186, "phrase": "tars."}, {"score": 0.010573457658167688, "phrase": "evaluation_protocols"}, {"score": 0.004837531799805358, "phrase": "time"}, {"score": 0.00462773520532301, "phrase": "existing_evaluation_protocols"}, {"score": 0.0045820747375409435, "phrase": "temporal_context"}, {"score": 0.004477270137801732, "phrase": "effective_approach"}, {"score": 0.00443308765153716, "phrase": "recommendation_performance"}, {"score": 0.00428892403807229, "phrase": "time-aware_recommender_systems"}, {"score": 0.004190796368475121, "phrase": "increasing_attention"}, {"score": 0.004149429072483132, "phrase": "wide_range"}, {"score": 0.004067910470129423, "phrase": "time_dimension"}, {"score": 0.00404109360612109, "phrase": "user_modeling"}, {"score": 0.004014452814186653, "phrase": "recommendation_strategies"}, {"score": 0.0037450707321601963, "phrase": "time_information"}, {"score": 0.0035285548582081627, "phrase": "existing_discrepancies"}, {"score": 0.0034363735647539267, "phrase": "comprehensive_survey"}, {"score": 0.003269953798005508, "phrase": "meaningful_divergences"}, {"score": 0.0031115683920520773, "phrase": "key_conditions"}, {"score": 0.003091036449708673, "phrase": "offline_evaluation"}, {"score": 0.0029608318458226755, "phrase": "comprehensive_classification"}, {"score": 0.0028644081075973877, "phrase": "methodological_description_framework"}, {"score": 0.0028173769301594745, "phrase": "evaluation_process"}, {"score": 0.0027346529452880585, "phrase": "empirical_study"}, {"score": 0.002689746491842271, "phrase": "different_evaluation_protocols"}, {"score": 0.002663156413861478, "phrase": "relative_performances"}, {"score": 0.0026455755027979533, "phrase": "well-known_tars."}, {"score": 0.0025935242726503964, "phrase": "different_uses"}, {"score": 0.0025424945305785374, "phrase": "remarkably_distinct_performance"}, {"score": 0.0025257081555100556, "phrase": "relative_ranking_values"}, {"score": 0.0024112587007106535, "phrase": "evaluation_conditions"}, {"score": 0.0023481972886024347, "phrase": "reported_results"}, {"score": 0.0022641658133181115, "phrase": "methodological_issues"}, {"score": 0.0022492129111567824, "phrase": "robust_evaluation"}, {"score": 0.002147265537489893, "phrase": "general_guidelines"}, {"score": 0.0021260268415459805, "phrase": "proper_conditions"}, {"score": 0.0021049977753042253, "phrase": "particular_tars."}], "paper_keywords": ["Time-aware recommender systems", " Context-aware recommender systems", " Evaluation methodologies", " Survey"], "paper_abstract": "Exploiting temporal context has been proved to be an effective approach to improve recommendation performance, as shown, e.g. in the Netflix Prize competition. Time-aware recommender systems (TARS) are indeed receiving increasing attention. A wide range of approaches dealing with the time dimension in user modeling and recommendation strategies have been proposed. In the literature, however, reported results and conclusions about how to incorporate and exploit time information within the recommendation processes seem to be contradictory in some cases. Aiming to clarify and address existing discrepancies, in this paper we present a comprehensive survey and analysis of the state of the art on TARS. The analysis show that meaningful divergences appear in the evaluation protocols used-metrics and methodologies. We identify a number of key conditions on offline evaluation of TARS, and based on these conditions, we provide a comprehensive classification of evaluation protocols for TARS. Moreover, we propose a methodological description framework aimed to make the evaluation process fair and reproducible. We also present an empirical study on the impact of different evaluation protocols on measuring relative performances of well-known TARS. The results obtained show that different uses of the above evaluation conditions yield to remarkably distinct performance and relative ranking values of the recommendation approaches. They reveal the need of clearly stating the evaluation conditions used to ensure comparability and reproducibility of reported results. From our analysis and experiments, we finally conclude with methodological issues a robust evaluation of TARS should take into consideration. Furthermore we provide a number of general guidelines to select proper conditions for evaluating particular TARS.", "paper_title": "Time-aware recommender systems: a comprehensive survey and analysis of existing evaluation protocols", "paper_id": "WOS:000330979900004"}