{"auto_keywords": [{"score": 0.004566567411073082, "phrase": "large_ensemble"}, {"score": 0.004470823343432748, "phrase": "conjunctive_rules"}, {"score": 0.0036942726653704213, "phrase": "high_dimensional_regression_problems"}, {"score": 0.003322540556404882, "phrase": "partial_least_squares"}, {"score": 0.0032184697893776052, "phrase": "significantly_better_prediction_performance"}, {"score": 0.002988101395789399, "phrase": "random_forest"}, {"score": 0.0028944757265206332, "phrase": "rulefit_algorithms"}, {"score": 0.0028037753646260937, "phrase": "equal_weights"}, {"score": 0.002658863179011018, "phrase": "lasso_regression"}, {"score": 0.002575526825728026, "phrase": "rule_ensembles"}, {"score": 0.0021501863832966966, "phrase": "cluster_validity_point"}, {"score": 0.0021049977753042253, "phrase": "high_quality_groupings"}], "paper_keywords": ["Decision trees", " ensemble learning", " rule ensembles", " semi-supervised learning", " clustering"], "paper_abstract": "In this article, we propose several new approaches for post processing a large ensemble of conjunctive rules for supervised, semi-supervised and unsupervised learning problems. We show with various examples that for high dimensional regression problems the models constructed by post processing the rules with partial least squares regression have significantly better prediction performance than the ones produced by the random forest or the rulefit algorithms which use equal weights or weights estimated from lasso regression. When rule ensembles are used for semi-supervised and unsupervised learning, the internal and external measures of cluster validity point to high quality groupings.", "paper_title": "Ensemble learning with trees and rules: Supervised, semi-supervised, unsupervised", "paper_id": "WOS:000342556300006"}