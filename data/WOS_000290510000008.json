{"auto_keywords": [{"score": 0.04560362315992008, "phrase": "hidden_units"}, {"score": 0.00481495049065317, "phrase": "output_surface_geometry"}, {"score": 0.0047113960612140335, "phrase": "multilayer_perceptron_networks"}, {"score": 0.004462105963334103, "phrase": "affine_combinations"}, {"score": 0.004225950334493496, "phrase": "tanh_activation_function"}, {"score": 0.004135010810625326, "phrase": "universal_function_approximators"}, {"score": 0.003668759094002022, "phrase": "mse"}, {"score": 0.003399466007862412, "phrase": "neural_network_weight_learning_algorithm"}, {"score": 0.0031500213134140953, "phrase": "input_space"}, {"score": 0.002887210021573995, "phrase": "output_surface"}, {"score": 0.002675252546913683, "phrase": "sampling_requirements"}, {"score": 0.0023730978395564116, "phrase": "neural_network"}, {"score": 0.0022472527731744974, "phrase": "data_set"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Neural network", " Regression", " Function approximation", " Supervised learning"], "paper_abstract": "Multilayer perceptron networks whose outputs consist of affine combinations of hidden units using the tanh activation function are universal function approximators and are used for regression, typically by reducing the MSE with backpropagation. We present a neural network weight learning algorithm that directly positions the hidden units within input space by numerically analyzing the curvature of the output surface. Our results show that under some sampling requirements, this method can reliably recover the parameters of a neural network used to generate a data set. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Neural network tomography: Network replication from output surface geometry", "paper_id": "WOS:000290510000008"}