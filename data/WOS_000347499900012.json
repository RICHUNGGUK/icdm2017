{"auto_keywords": [{"score": 0.048398179269243564, "phrase": "badminton_robot"}, {"score": 0.03284586955868887, "phrase": "model-free_controllers"}, {"score": 0.00481495049065317, "phrase": "model-free_and_model-based_methods"}, {"score": 0.004523103062368843, "phrase": "time_optimal_control"}, {"score": 0.0044231966846655394, "phrase": "hit_motion"}, {"score": 0.004306205029641728, "phrase": "serve_operation"}, {"score": 0.0039380388343479384, "phrase": "target_position"}, {"score": 0.0038855864345030563, "phrase": "target_velocity"}, {"score": 0.003505908076430666, "phrase": "target_state"}, {"score": 0.002971182100871581, "phrase": "natural_actor-critic"}, {"score": 0.0028539179130824786, "phrase": "model-based_controllers"}, {"score": 0.0027907771648224273, "phrase": "desired_motions"}, {"score": 0.0027535633837592597, "phrase": "prior_model_information"}, {"score": 0.0027047136845332917, "phrase": "model-free_methods"}, {"score": 0.0026330549554033876, "phrase": "desired_robot_motions"}, {"score": 0.002473129577319634, "phrase": "good_choice"}, {"score": 0.0024401411219526774, "phrase": "reward_function"}, {"score": 0.002322895056238477, "phrase": "resulting_controller"}, {"score": 0.002291905937251069, "phrase": "simulation_study"}, {"score": 0.0022311596751824416, "phrase": "model-based_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Robot control", " Time optimal motion", " Optimization", " Reinforcement learning", " Natural actor-critic"], "paper_abstract": "In this research, time optimal control is considered for the hit motion of a badminton robot during a serve operation. Even though the robot always starts at rest in a given position, it has to move to a target position where the target velocity is not zero, as the robot has to hit the shuttle at that point. The goal is to reach this target state as quickly as possible, yet without violating the limitations of the actuator. To find controllers satisfying these requirements, both model-based and model-free controllers have been developed, with the model-free controllers employing a Natural Actor-Critic (NAC) reinforcement learning algorithm. The model-based controllers can immediately achieve the desired motions relying on prior model information, while the model-free methods are shown to yield the desired robot motions after about 200 trials. However, in order to achieve this result, a good choice of the reward function is essential. To illustrate this choice and validate the resulting controller, a simulation study is presented in which the model-based results are compared to those obtained with two different reward functions. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Comparison of model-free and model-based methods for time optimal hit control of a badminton robot", "paper_id": "WOS:000347499900012"}