{"auto_keywords": [{"score": 0.030874807943480125, "phrase": "input_time_window"}, {"score": 0.027922242680918582, "phrase": "lda"}, {"score": 0.00481495049065317, "phrase": "nam-captured_whisper"}, {"score": 0.004741845603430361, "phrase": "speech_system"}, {"score": 0.004669845451682529, "phrase": "tissue-conductive_sensor"}, {"score": 0.004529094793496899, "phrase": "naist"}, {"score": 0.004477407179841716, "phrase": "non-audible_murmur"}, {"score": 0.004443287535023951, "phrase": "nam"}, {"score": 0.004392565073466714, "phrase": "audible_speech"}, {"score": 0.004359078118356132, "phrase": "gm_m-based_statistical_mapping"}, {"score": 0.004179392391349015, "phrase": "converted_speech"}, {"score": 0.004084510791138056, "phrase": "computer-mediated_communication"}, {"score": 0.003991774582021095, "phrase": "poor_estimation"}, {"score": 0.003931118518507637, "phrase": "unvoiced_speech"}, {"score": 0.0038713805573219297, "phrase": "impoverished_phonetic_contrasts"}, {"score": 0.003669330508576073, "phrase": "synthesized_speech"}, {"score": 0.0036413370053909886, "phrase": "first_objective_and_subjective_evaluations"}, {"score": 0.0033857878144822906, "phrase": "single_gmm"}, {"score": 0.0031967297437935772, "phrase": "voiced_segments"}, {"score": 0.003088384809415812, "phrase": "voicing_decision"}, {"score": 0.0030414140732789186, "phrase": "neural_network"}, {"score": 0.003006654007926993, "phrase": "objective_and_subjective_improvement"}, {"score": 0.002949598537993663, "phrase": "second_improvement"}, {"score": 0.0026494723416072316, "phrase": "linear_discriminant_analysis"}, {"score": 0.002579326836642461, "phrase": "original_principal_component_analysis"}, {"score": 0.002559908761725667, "phrase": "pca"}, {"score": 0.0025110337774260773, "phrase": "spectral_envelope"}, {"score": 0.0024073421672242486, "phrase": "larger_time_windows"}, {"score": 0.0023798115577749225, "phrase": "third_improvement"}, {"score": 0.0023435920614377306, "phrase": "visual_parameters"}, {"score": 0.002316788811533867, "phrase": "input_and_output_parameters"}, {"score": 0.002290291401956985, "phrase": "positive_contribution"}, {"score": 0.0022211005680689666, "phrase": "subjective_test"}, {"score": 0.0021872912745910127, "phrase": "h_m_m-based_conversion"}, {"score": 0.0021539955106204354, "phrase": "gmm-based_conversion"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Non-audible murmur", " Whispered speech", " Audiovisual voice conversion", " Silent speech interface"], "paper_abstract": "Exploiting a tissue-conductive sensor a stethoscopic microphone the system developed at NAIST which converts non-audible murmur (NAM) to audible speech by GM M-based statistical mapping is a very promising technique. The quality of the converted speech is however still insufficient for computer-mediated communication, notably because of the poor estimation of F-0 from unvoiced speech and because of impoverished phonetic contrasts. This paper presents our investigations to improve the intelligibility and naturalness of the synthesized speech and first objective and subjective evaluations of the resulting system. The first improvement concerns voicing and F-0 estimation. Instead of using a single GMM for both, we estimate a continuous F-0 using a GMM, trained on target voiced segments only. The continuous F-0 estimation is filtered by a voicing decision computed by a neural network. The objective and subjective improvement is significant. The second improvement concerns the input time window and its dimensionality reduction: we show that the precision of F-0 estimation is also significantly improved by extending the input time window from 90 to 450 ms and by using a Linear Discriminant Analysis (LDA) instead of the original Principal Component Analysis (PCA). Estimation of spectral envelope is also slightly improved with LDA but is degraded with larger time windows. A third improvement consists in adding visual parameters both as input and output parameters. The positive contribution of this information is confirmed by a subjective test. Finally, H M M-based conversion is compared with GMM-based conversion. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Improvement to a NAM-captured whisper-to-speech system", "paper_id": "WOS:000276026400005"}