{"auto_keywords": [{"score": 0.047748521831607424, "phrase": "image_matting"}, {"score": 0.01014391546143317, "phrase": "unknown_pixel"}, {"score": 0.00481495049065317, "phrase": "accurate_object_extraction"}, {"score": 0.004713162564945179, "phrase": "natural_image_matting"}, {"score": 0.004613516457919661, "phrase": "digital_multimedia_technologies"}, {"score": 0.0045320819060445045, "phrase": "increasing_interests"}, {"score": 0.004483909657713567, "phrase": "academic_and_industrial_communities"}, {"score": 0.004265760046406154, "phrase": "arbitrary_shapes"}, {"score": 0.004175533280178052, "phrase": "video_frame"}, {"score": 0.004015025120733852, "phrase": "inherently_an_ill-posed_problem"}, {"score": 0.003765525342866404, "phrase": "comprehensive_survey"}, {"score": 0.003725470458263889, "phrase": "existing_image_matting_algorithms"}, {"score": 0.003607834976869384, "phrase": "blue_screen_matting"}, {"score": 0.003531474887552889, "phrase": "existing_natural_image_matting_methods"}, {"score": 0.003018595266561849, "phrase": "nearby_pixels"}, {"score": 0.0029971349193857093, "phrase": "propagation-based_methods"}, {"score": 0.002912803994461996, "phrase": "foreground_and_background_colors"}, {"score": 0.002861302930564756, "phrase": "learning-based_methods"}, {"score": 0.002830839166653185, "phrase": "matting_process"}, {"score": 0.0028006988338192375, "phrase": "supervised_or_semisupervised_learning_problem"}, {"score": 0.0027610089418453614, "phrase": "learning_process"}, {"score": 0.0027025233739857374, "phrase": "linear_or_nonlinear_model"}, {"score": 0.0026737455737636867, "phrase": "alpha_mattes"}, {"score": 0.0026452734009203764, "phrase": "image_colors"}, {"score": 0.002570817338292482, "phrase": "alpha_matte"}, {"score": 0.0024630345285372958, "phrase": "testing_image"}, {"score": 0.0021659920594094407, "phrase": "research_trends"}, {"score": 0.002120083747913758, "phrase": "promising_directions"}, {"score": 0.0021049977753042253, "phrase": "future_development"}], "paper_keywords": ["Alpha matte", " evaluation", " image composition", " image matting", " image segmentation", " survey"], "paper_abstract": "With the development of digital multimedia technologies, image matting has gained increasing interests from both academic and industrial communities. The purpose of image matting is to precisely extract the foreground objects with arbitrary shapes from an image or a video frame for further editing. It is generally known that image matting is inherently an ill-posed problem because we need to output three images out of only one input image. In this paper, we provide a comprehensive survey of the existing image matting algorithms and evaluate their performance. In addition to the blue screen matting, we systematically divide all existing natural image matting methods into four categories: 1) color sampling-based; 2) propagation-based; 3) combination of sampling-based and propagation-based; and 4) learning-based approaches. Sampling-based methods assume that the foreground and background colors of an unknown pixel can be explicitly estimated by examining nearby pixels. Propagation-based methods are instead based on the assumption that foreground and background colors are locally smooth. Learning-based methods treat the matting process as a supervised or semisupervised learning problem. Via the learning process, users can construct a linear or nonlinear model between the alpha mattes and the image colors using a training set to estimate the alpha matte of an unknown pixel without any assumption about the characteristics of the testing image. With three benchmark data sets, the various matting algorithms are evaluated and compared using several metrics to demonstrate the strengths and weaknesses of each method both quantitatively and qualitatively. Finally, we conclude this paper by outlining the research trends and suggesting a number of promising directions for future development.", "paper_title": "Targeting Accurate Object Extraction From an Image: A Comprehensive Study of Natural Image Matting", "paper_id": "WOS:000348856200001"}