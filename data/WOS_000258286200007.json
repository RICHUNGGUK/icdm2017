{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "stochastic_gradient_algorithms"}, {"score": 0.04931727328027492, "phrase": "weak_conditions"}, {"score": 0.00455202992136173, "phrase": "stochastic_martingale_theory"}, {"score": 0.00437302709192313, "phrase": "stochastic_gradient"}, {"score": 0.003971502222405927, "phrase": "parameter_estimates"}, {"score": 0.003876996400581697, "phrase": "sg_algorithms"}, {"score": 0.0037244390953401533, "phrase": "true_parameters"}, {"score": 0.003549245218355768, "phrase": "information_vector"}, {"score": 0.0033283624928018177, "phrase": "data_product_moment"}, {"score": 0.0032231139180761183, "phrase": "bounded_condition_number"}, {"score": 0.0030962064913500164, "phrase": "process_noises"}, {"score": 0.0028342734482926677, "phrase": "strict_assumptions"}, {"score": 0.0027446057983474994, "phrase": "existing_references"}, {"score": 0.0026577673946415583, "phrase": "noise_variances"}, {"score": 0.002615381301695394, "phrase": "high-order_moments"}, {"score": 0.002374855916752648, "phrase": "strong_persistent_excitation_condition"}, {"score": 0.0022268964676199292, "phrase": "convergence_conditions"}, {"score": 0.0021391300942881, "phrase": "simulation_results"}, {"score": 0.0021049977753042253, "phrase": "bounded_and_unbounded_noise_variances"}], "paper_keywords": ["recursive identification", " parameter estimation", " least squares", " stochastic gradient", " multivariable systems", " convergence properties", " martingale convergence theorem"], "paper_abstract": "By using the stochastic martingale theory, convergence properties of stochastic gradient (SG) identification algorithms are studied under weak conditions. The analysis indicates that the parameter estimates by the SG algorithms consistently converge to the true parameters, as long as the information vector is persistently exciting (i.e., the data product moment matrix has a bounded condition number) and that the process noises are zero mean and uncorrelated. These results remove the strict assumptions, made in existing references, that the noise variances and high-order moments exist, and the processes are stationary and ergodic and the strong persistent excitation condition holds. This contribution greatly relaxes the convergence conditions of stochastic gradient algorithms. The simulation results with bounded and unbounded noise variances confirm the convergence conclusions proposed.", "paper_title": "Performance analysis of stochastic gradient algorithms under weak conditions", "paper_id": "WOS:000258286200007"}