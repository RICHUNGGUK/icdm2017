{"auto_keywords": [{"score": 0.04916809664432026, "phrase": "direct_policy_search"}, {"score": 0.00481495049065317, "phrase": "sample_reuse"}, {"score": 0.004633429997452688, "phrase": "reinforcement_learning"}, {"score": 0.004416079619543104, "phrase": "promising_reinforcement_learning_framework"}, {"score": 0.0038973170902076707, "phrase": "large_number"}, {"score": 0.0036788184374592706, "phrase": "stable_policy_update_estimator"}, {"score": 0.0034063576796640603, "phrase": "sampling_cost"}, {"score": 0.0030642636185693054, "phrase": "expectation-maximization-based_policy_search_method"}, {"score": 0.0026523079380037706, "phrase": "proposed_method"}, {"score": 0.002340250894008014, "phrase": "robot_learning_experiments"}, {"score": 0.0021876814398268775, "phrase": "extended_version"}, {"score": 0.0021049977753042253, "phrase": "hachiya"}], "paper_keywords": [""], "paper_abstract": "Direct policy search is a promising reinforcement learning framework, in particular for controlling continuous, high-dimensional systems. Policy search often requires a large number of samples for obtaining a stable policy update estimator, and this is prohibitive when the sampling cost is expensive. In this letter, we extend an expectation-maximization-based policy search method so that previously collected samples can be efficiently reused. The usefulness of the proposed method, reward-weighted regression with sample reuse (R(3)), is demonstrated through robot learning experiments. (This letter is an extended version of our earlier conference paper: Hachiya, Peters, & Sugiyama, 2009.)", "paper_title": "Reward-Weighted Regression with Sample Reuse for Direct Policy Search in Reinforcement Learning", "paper_id": "WOS:000295321100004"}