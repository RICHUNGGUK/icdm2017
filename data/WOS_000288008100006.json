{"auto_keywords": [{"score": 0.045137059867648106, "phrase": "visual_scenes"}, {"score": 0.04461799901716509, "phrase": "gam"}, {"score": 0.00481495049065317, "phrase": "grasping_affordance_model"}, {"score": 0.004588158931776543, "phrase": "computational_account"}, {"score": 0.004533142516832776, "phrase": "perceptual_processes"}, {"score": 0.004372002674739612, "phrase": "action_possibilities"}, {"score": 0.004140924670517666, "phrase": "affordance_perception"}, {"score": 0.004091248980678087, "phrase": "visuo-motor_transformations"}, {"score": 0.003922011960904278, "phrase": "visually_presented_objects"}, {"score": 0.0038054151244558123, "phrase": "hand_grasping_configurations"}, {"score": 0.0036479584876742085, "phrase": "neuroscientific_models"}, {"score": 0.00360417515510547, "phrase": "relevant_visuo-motor_functions"}, {"score": 0.0034759420565135253, "phrase": "monkey_brain"}, {"score": 0.003332072426367186, "phrase": "biological_grasping_affordances"}, {"score": 0.0029707940194245216, "phrase": "object_features"}, {"score": 0.0027630232270330402, "phrase": "previously_unseen_objects"}, {"score": 0.0026970382454914437, "phrase": "gam_information_processing"}, {"score": 0.0026326249204932733, "phrase": "semantic_memory_access"}, {"score": 0.0025542617599268323, "phrase": "object_recognition"}, {"score": 0.0023755529073496394, "phrase": "substantive_computational_mechanisms"}, {"score": 0.002304824112812496, "phrase": "object_parts"}, {"score": 0.0022771244331421586, "phrase": "selective_analysis"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Affordances", " Grasping actions", " Computational model", " Brain", " Visuo-motor transformation"], "paper_abstract": "The Grasping Affordance Model (GAM) introduced here provides a computational account of perceptual processes enabling one to identify grasping action possibilities from visual scenes. GAM identifies the core of affordance perception with visuo-motor transformations enabling one to associate features of visually presented objects to a collection of hand grasping configurations. This account is coherent with neuroscientific models of relevant visuo-motor functions and their localization in the monkey brain. GAM differs from other computational models of biological grasping affordances in the way of modeling focus, functional account, and tested abilities. Notably, by learning to associate object features to hand shapes, GAM generalizes its grasp identification abilities to a variety of previously unseen objects. Even though GAM information processing does not involve semantic memory access and full-fledged object recognition, perceptions of (grasping) affordances are mediated there by substantive computational mechanisms which include learning of object parts, selective analysis of visual scenes, and guessing from experience. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Perceiving affordances: A computational investigation of grasping affordances", "paper_id": "WOS:000288008100006"}