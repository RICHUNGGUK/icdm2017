{"auto_keywords": [{"score": 0.041674071475816026, "phrase": "erasure_codes"}, {"score": 0.039185541388815304, "phrase": "traditional_erasure_codes"}, {"score": 0.03409824254329683, "phrase": "src"}, {"score": 0.00481495049065317, "phrase": "networked_distributed_data_storage_systems"}, {"score": 0.004626553103375699, "phrase": "massive_volumes"}, {"score": 0.004305740330804528, "phrase": "node_failures"}, {"score": 0.00405538690701843, "phrase": "redundancy_replenishment"}, {"score": 0.003927848423072668, "phrase": "storage_efficient_alternative"}, {"score": 0.0038965930422333035, "phrase": "replication_based_redundancy"}, {"score": 0.0038655854071187115, "phrase": "storage_systems"}, {"score": 0.003758981686762617, "phrase": "lower_storage_overhead_cost"}, {"score": 0.0036262124965265015, "phrase": "high_communication_overhead"}, {"score": 0.0035544816252832375, "phrase": "encoded_fragments"}, {"score": 0.0034841647157189985, "phrase": "storage_device_failures"}, {"score": 0.0033745298175222056, "phrase": "new_nodes"}, {"score": 0.0033077610063168093, "phrase": "new_family"}, {"score": 0.003255295296893174, "phrase": "self-repairing_codes"}, {"score": 0.0031277316566463978, "phrase": "distributed_storage_systems"}, {"score": 0.002708399936381754, "phrase": "code_properties"}, {"score": 0.002686821995335494, "phrase": "bandwidth_efficient_and_fast_recovery"}, {"score": 0.002623111480310373, "phrase": "multiple_failures"}, {"score": 0.002560907803820395, "phrase": "better_system_robustness"}, {"score": 0.002530359917257626, "phrase": "concrete_family"}, {"score": 0.0024703502756471514, "phrase": "homomorphic_src"}, {"score": 0.0021389638834794136, "phrase": "recent_representative_codes"}, {"score": 0.0021049977753042253, "phrase": "storage_applications"}], "paper_keywords": ["Erasure codes", " Local repair", " Networked storage"], "paper_abstract": "Networked distributed data storage systems are essential to deal with the needs of storing massive volumes of data. Dependability of such a system relies on its fault tolerance (data should be available in case of node failures) as well as its maintainability (its ability to repair lost data to ensure redundancy replenishment over time). Erasure codes provide a storage efficient alternative to replication based redundancy in storage systems, ensuring the same fault tolerance at a lower storage overhead cost. Traditional erasure codes however have the drawback of entailing high communication overhead for maintenance, when encoded fragments are lost due to storage device failures, and need to be replenished in new nodes. We propose a new family of erasure codes called self-repairing codes (SRC) taking into account the peculiarities of distributed storage systems, specifically to improve its maintainability by 'localizing' the repairs. SRC have the property that encoded fragments can be repaired directly from other small subsets of (typically 2 or 3) encoded fragments. These code properties allow bandwidth efficient and fast recovery even in the presence of multiple failures, in turn translating into better system robustness. A concrete family of such locally repairable codes, namely, homomorphic SRC are proposed and various aspects and properties of the same are studied in detail and compared-quantitatively or qualitatively (as may be suitable) with respect to other codes including traditional erasure codes as well as some recent representative codes designed specifically for storage applications.", "paper_title": "Self-repairing codes", "paper_id": "WOS:000348437200003"}