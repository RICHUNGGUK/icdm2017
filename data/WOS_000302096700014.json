{"auto_keywords": [{"score": 0.041765365816587165, "phrase": "decision_tree"}, {"score": 0.014031897437744434, "phrase": "impurity_measures"}, {"score": 0.00481495049065317, "phrase": "geometric_decision_tree"}, {"score": 0.0045726187330434025, "phrase": "new_algorithm"}, {"score": 0.004494570360108983, "phrase": "oblique_decision_trees"}, {"score": 0.00436742595270452, "phrase": "current_decision_tree_algorithms"}, {"score": 0.003938650032712796, "phrase": "top-down_fashion"}, {"score": 0.0037402558657576124, "phrase": "geometric_structures"}, {"score": 0.0033152062780180073, "phrase": "geometric_structure"}, {"score": 0.0029722900280316216, "phrase": "clustering_hyperplanes"}, {"score": 0.00283870598459543, "phrase": "split_rule"}, {"score": 0.002711109302199029, "phrase": "empirical_studies"}, {"score": 0.0026191799769765085, "phrase": "small_decision_trees"}, {"score": 0.0025892330566358503, "phrase": "better_performance"}, {"score": 0.0024305268807926056, "phrase": "angle_bisectors"}, {"score": 0.0023079225317714815, "phrase": "split_rules"}, {"score": 0.0022041312401435346, "phrase": "interesting_optimization_problem"}, {"score": 0.0021049977753042253, "phrase": "principled_method"}], "paper_keywords": ["Decision trees", " generalized eigenvalue problem", " multiclass classification", " oblique decision tree"], "paper_abstract": "In this paper, we present a new algorithm for learning oblique decision trees. Most of the current decision tree algorithms rely on impurity measures to assess the goodness of hyperplanes at each node while learning a decision tree in top-down fashion. These impurity measures do not properly capture the geometric structures in the data. Motivated by this, our algorithm uses a strategy for assessing the hyperplanes in such a way that the geometric structure in the data is taken into account. At each node of the decision tree, we find the clustering hyperplanes for both the classes and use their angle bisectors as the split rule at that node. We show through empirical studies that this idea leads to small decision trees and better performance. We also present some analysis to show that the angle bisectors of clustering hyperplanes that we use as the split rules at each node are solutions of an interesting optimization problem and hence argue that this is a principled method of learning a decision tree.", "paper_title": "Geometric Decision Tree", "paper_id": "WOS:000302096700014"}