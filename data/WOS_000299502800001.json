{"auto_keywords": [{"score": 0.04766724089045989, "phrase": "breadth-first_search"}, {"score": 0.04040281946301967, "phrase": "local_optima"}, {"score": 0.00481495049065317, "phrase": "stochastic_enforced_hill-climbing"}, {"score": 0.004773692073488661, "phrase": "enforced_hill-climbing"}, {"score": 0.004712463245074863, "phrase": "effective_deterministic_hill-climbing_technique"}, {"score": 0.004268293864953256, "phrase": "stochastic_generalization"}, {"score": 0.004141575435071002, "phrase": "online_use"}, {"score": 0.004106063237069123, "phrase": "goal-oriented_probabilistic_planning_problems"}, {"score": 0.0040186038781533946, "phrase": "provided_heuristic_function"}, {"score": 0.0039841418890102925, "phrase": "expected_cost"}, {"score": 0.003767203575060969, "phrase": "straightforward_greedy_action_choice"}, {"score": 0.003577414339646767, "phrase": "deterministic_problems"}, {"score": 0.003531474887552889, "phrase": "stochastic_problems"}, {"score": 0.003426556090707852, "phrase": "heuristic-based_markov_decision_process"}, {"score": 0.0032398798149196432, "phrase": "good_escape_policy"}, {"score": 0.0031982609080278643, "phrase": "local_optimum"}, {"score": 0.0030109772626153797, "phrase": "mdp_problem"}, {"score": 0.0029722900280316216, "phrase": "local_goal"}, {"score": 0.0028346293790895024, "phrase": "twenty-four_recent_probabilistic_planning-competition_benchmark_domains"}, {"score": 0.0027503564589458837, "phrase": "recent_literature"}, {"score": 0.002566997205079849, "phrase": "better_policies"}, {"score": 0.0025449518236394103, "phrase": "greedy_heuristic_following"}, {"score": 0.002416589459425828, "phrase": "deterministic_heuristics"}, {"score": 0.0023855210798264205, "phrase": "deterministic_relaxation"}, {"score": 0.0023548511815224098, "phrase": "second_type"}, {"score": 0.0023245746777044766, "phrase": "automatic_learning"}, {"score": 0.002304606411453748, "phrase": "bellman-error"}, {"score": 0.0022749744354271816, "phrase": "domain-specific_experience"}, {"score": 0.002236055625099805, "phrase": "first_type"}, {"score": 0.002197801473591492, "phrase": "seh"}, {"score": 0.0021049977753042253, "phrase": "first_three_international_probabilistic_planning_competitions"}], "paper_keywords": [""], "paper_abstract": "Enforced hill-climbing is an effective deterministic hill-climbing technique that deals with local optima using breadth-first search (a process called \"basin flooding\"). We propose and evaluate a stochastic generalization of enforced hill-climbing for online use in goal-oriented probabilistic planning problems. We assume a provided heuristic function estimating expected cost to the goal with flaws such as local optima and plateaus that thwart straightforward greedy action choice. While breadth-first search is effective in exploring basins around local optima in deterministic problems, for stochastic problems we dynamically build and solve a heuristic-based Markov decision process (MDP) model of the basin in order to find a good escape policy exiting the local optimum. We note that building this model involves integrating the heuristic into the MDP problem because the local goal is to improve the heuristic. We evaluate our proposal in twenty-four recent probabilistic planning-competition benchmark domains and twelve probabilistically interesting problems from recent literature. For evaluation, we show that stochastic enforced hill-climbing (SEH) produces better policies than greedy heuristic following for value/cost functions derived in two very different ways: one type derived by using deterministic heuristics on a deterministic relaxation and a second type derived by automatic learning of Bellman-error features from domain-specific experience. Using the first type of heuristic, SEH is shown to generally outperform all planners from the first three international probabilistic planning competitions.", "paper_title": "Stochastic Enforced Hill-Climbing", "paper_id": "WOS:000299502800001"}