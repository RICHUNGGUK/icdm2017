{"auto_keywords": [{"score": 0.041577008053686304, "phrase": "nvidia"}, {"score": 0.006561718254704429, "phrase": "lapack"}, {"score": 0.004977288970702003, "phrase": "scalapack"}, {"score": 0.00481495049065317, "phrase": "gemm_kernels"}, {"score": 0.004725385523804256, "phrase": "fermi_gpu."}, {"score": 0.004522796947899013, "phrase": "graphics_chips"}, {"score": 0.0043560464825349275, "phrase": "viable_way"}, {"score": 0.004274981117736524, "phrase": "scientific_and_engineering_applications"}, {"score": 0.004015458740397976, "phrase": "fermi_architecture"}, {"score": 0.003819187440994939, "phrase": "numerical_computing"}, {"score": 0.0037246656582642272, "phrase": "fast_double_precision_arithmetic"}, {"score": 0.0036097844103129043, "phrase": "error_correction_codes"}, {"score": 0.0035204268675160257, "phrase": "crucial_component"}, {"score": 0.00347657813672945, "phrase": "numerical_software_packages"}, {"score": 0.0032858981413454802, "phrase": "general_dense_matrix_multiplication"}, {"score": 0.0028805474861014722, "phrase": "matrix_multiplication_kernels"}, {"score": 0.0028091907694771613, "phrase": "specific_architecture"}, {"score": 0.0027395968397212053, "phrase": "canonical_process"}, {"score": 0.0027054473441439422, "phrase": "heuristic_autotuning"}, {"score": 0.0026055251533586804, "phrase": "multiple_code_variants"}, {"score": 0.0025409639091476363, "phrase": "fastest_ones"}, {"score": 0.0024625017739528096, "phrase": "key_contribution"}, {"score": 0.002298288831092204, "phrase": "search_space"}, {"score": 0.0021857667493036786, "phrase": "manageable_size"}, {"score": 0.002158505888557746, "phrase": "performance_numbers"}], "paper_keywords": ["Graphics processing unit", " matrix multiplication", " code generation", " automatic tuning", " GEMM", " BLAS", " CUDA"], "paper_abstract": "In recent years, the use of graphics chips has been recognized as a viable way of accelerating scientific and engineering applications, even more so since the introduction of the Fermi architecture by NVIDIA, with features essential to numerical computing, such as fast double precision arithmetic and memory protected with error correction codes. Being the crucial component of numerical software packages, such as LAPACK and ScaLAPACK, the general dense matrix multiplication routine is one of the more important workloads to be implemented on these devices. This paper presents a methodology for producing matrix multiplication kernels tuned for a specific architecture, through a canonical process of heuristic autotuning, based on generation of multiple code variants and selecting the fastest ones through benchmarking. The key contribution of this work is in the method for generating the search space; specifically, pruning it to a manageable size. Performance numbers match or exceed other available implementations.", "paper_title": "Autotuning GEMM Kernels for the Fermi GPU", "paper_id": "WOS:000308998100005"}