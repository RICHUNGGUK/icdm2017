{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multirelational_classification"}, {"score": 0.0047650691627251825, "phrase": "subgraph-based_approach"}, {"score": 0.0046184900568047565, "phrase": "multiple_interlinked_tables"}, {"score": 0.004523272849033429, "phrase": "relational_database"}, {"score": 0.00436890075128947, "phrase": "numerous_departments"}, {"score": 0.004249188865832993, "phrase": "different_aspects"}, {"score": 0.004161553532799662, "phrase": "customer_profiling"}, {"score": 0.004132743580000062, "phrase": "fraud_detection"}, {"score": 0.004104132253183181, "phrase": "inventory_management"}, {"score": 0.0040757181942937875, "phrase": "financial_management"}, {"score": 0.003936560022538905, "phrase": "different_phases"}, {"score": 0.0038957429132649175, "phrase": "knowledge_discovery_process"}, {"score": 0.0038419751490849133, "phrase": "economic_utility"}, {"score": 0.003749654338704576, "phrase": "data_preprocessing_process"}, {"score": 0.003559199464487982, "phrase": "large_volumes"}, {"score": 0.003449585750738867, "phrase": "data_mining_models"}, {"score": 0.0033317342432676385, "phrase": "data_size"}, {"score": 0.0032971676471625646, "phrase": "running_time"}, {"score": 0.0032629585038738856, "phrase": "learning_algorithm"}, {"score": 0.0031845067058084583, "phrase": "utility-based_issues"}, {"score": 0.0030756833977949273, "phrase": "pruned_database"}, {"score": 0.00301217724562369, "phrase": "predictive_performance_loss"}, {"score": 0.002980916123365584, "phrase": "final_model"}, {"score": 0.0029092257197427195, "phrase": "strongly_uncorrelated_subgraphs"}, {"score": 0.002879029968532794, "phrase": "original_database_schema"}, {"score": 0.002648448458159033, "phrase": "predictive_accuracy"}, {"score": 0.002320449502337321, "phrase": "computational_cost"}, {"score": 0.0022963507669678815, "phrase": "learning_process"}, {"score": 0.002248899833549883, "phrase": "multirelational_learning_algorithms'_execution_time"}, {"score": 0.0021419510072373756, "phrase": "accurate_model"}, {"score": 0.0021049977753042253, "phrase": "provided_database"}], "paper_keywords": ["Multi-relational classification", " Relational data mining"], "paper_abstract": "Multirelational classification aims to discover patterns across multiple interlinked tables (relations) in a relational database. In many large organizations, such a database often spans numerous departments and/or subdivisions, which are involved in different aspects of the enterprise such as customer profiling, fraud detection, inventory management, financial management, and so on. When considering classification, different phases of the knowledge discovery process are affected by economic utility. For instance, in the data preprocessing process, one must consider the cost associated with acquiring, cleaning, and transforming large volumes of data. When training and testing the data mining models, one has to consider the impact of the data size on the running time of the learning algorithm. In order to address these utility-based issues, the paper presents an approach to create a pruned database for multirelational classification, while minimizing predictive performance loss on the final model. Our method identifies a set of strongly uncorrelated subgraphs from the original database schema, to use for training, and discards all others. The experiments performed show that our strategy is able to, without sacrificing predictive accuracy, significantly reduce the size of the databases, in terms of the number of relations, tuples, and attributes.The approach prunes the sizes of databases by as much as 94 %. Such reduction also results in decreasing computational cost of the learning process. The method improves the multirelational learning algorithms' execution time by as much as 80 %. In particular, our results demonstrate that one may build an accurate model with only a small subset of the provided database.", "paper_title": "Reducing the size of databases for multirelational classification: a subgraph-based approach", "paper_id": "WOS:000316337200009"}