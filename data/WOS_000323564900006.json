{"auto_keywords": [{"score": 0.03495285014461313, "phrase": "nsgaii"}, {"score": 0.021686918664230875, "phrase": "classification_performance"}, {"score": 0.009347580240498669, "phrase": "small_number"}, {"score": 0.006699762783609347, "phrase": "proposed_multi-objective_algorithms"}, {"score": 0.00481495049065317, "phrase": "multi-objective_evolutionary_algorithms_for"}, {"score": 0.004727995968069753, "phrase": "classification._feature"}, {"score": 0.004670896353428476, "phrase": "multi-objective_problem"}, {"score": 0.004303023184313407, "phrase": "actual_need"}, {"score": 0.004212453011596424, "phrase": "multi-objective_feature_selection_algorithms"}, {"score": 0.004024716151211568, "phrase": "filter_algorithms"}, {"score": 0.004000322474812552, "phrase": "evolutionary_computation_techniques"}, {"score": 0.003939980614608309, "phrase": "multi-objective_optimisation"}, {"score": 0.003857022173615399, "phrase": "candidate_solutions"}, {"score": 0.0037873013080596137, "phrase": "multiple_non-dominated_solutions"}, {"score": 0.0035855828944304506, "phrase": "genetic_algorithm"}, {"score": 0.0035100601949451028, "phrase": "pareto_evolutionary_algorithm"}, {"score": 0.003394571753283408, "phrase": "based_feature_selection"}, {"score": 0.003223494720348768, "phrase": "based_feature_selection_frameworks"}, {"score": 0.0031459887611258765, "phrase": "mutual_information"}, {"score": 0.002969276778056666, "phrase": "single_objective_method"}, {"score": 0.0028715299874549245, "phrase": "eight_benchmark_datasets"}, {"score": 0.0028454331415283213, "phrase": "decision_tree"}, {"score": 0.0027769919995947676, "phrase": "experimental_results"}, {"score": 0.002685558057339722, "phrase": "non-dominated_solutions"}, {"score": 0.0026530590435527527, "phrase": "smaller_number"}, {"score": 0.0026129863248033807, "phrase": "better_classification_performance"}, {"score": 0.002519257735651759, "phrase": "single_objective_algorithm"}, {"score": 0.002481200970201421, "phrase": "even_the_traditional_wrapper_algorithm"}, {"score": 0.002370437342182879, "phrase": "similar_performance"}, {"score": 0.0021700901058417602, "phrase": "first_study"}, {"score": 0.0021307975800397816, "phrase": "filter_feature_selection"}, {"score": 0.00211785847468134, "phrase": "classification_problems"}], "paper_keywords": ["Feature selection", " evolutionary algorithms", " multi-objective optimisation", " filter approaches", " genetic algorithms"], "paper_abstract": "Feature selection is a multi-objective problem with the two main conflicting objectives of minimising the number of features and maximising the classification performance. However,most existing feature selection algorithms are single objective and do not appropriately reflect the actual need. There are a small number of multi-objective feature selection algorithms, which are wrapper based and accordingly are computationally expensive and less general than filter algorithms. Evolutionary computation techniques are particularly suitable for multi-objective optimisation because they use a population of candidate solutions and are able to find multiple non-dominated solutions in a ;single run. However, the two well-known evolutionary multi-objective algorithms, non dominated sorting based multi-objective genetic algorithm II (NSGAII) and strength Pareto evolutionary algorithm 2 (SPEA2) have not been applied to filter based feature selection. In this work, based on NSGAII and SPEA2, we develop two multi-objective, filter based feature selection frameworks. Four multi-objective feature selection methods are then developed by applying mutual information and entropy as two different filter evaluation criteria in each of the two proposed frameworks. The proposed multi-objective algorithms are examined and compared with a single objective method and three traditional methods (two filters and one wrapper) on eight benchmark datasets. A decision tree is employed to test the classification performance. Experimental results show that the proposed multi-objective algorithms can automatically evolve a set of non-dominated solutions that include a smaller number of features and achieve better classification performance than using all features. NSGAII and SPEA2 out perform the single objective algorithm,the two traditional filter algorithms and even the traditional wrapper algorithm in terms of both the number of features and the classification performance in most cases. NSGAII achieves similar performance to SPEA2 for the datasets that consist of a small number of features and slightly better results when the number of features is large. This work represents the first study on NSGAII and SPEA2 for filter feature selection in classification problems with both providing field leading classification performance.", "paper_title": "MULTI-OBJECTIVE EVOLUTIONARY ALGORITHMS FOR FILTER BASED FEATURE SELECTION IN CLASSIFICATION", "paper_id": "WOS:000323564900006"}