{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "virtual_avatars"}, {"score": 0.009721834424559908, "phrase": "large_diversity"}, {"score": 0.006440569513850737, "phrase": "facial_components"}, {"score": 0.006370127319824813, "phrase": "different_avatars"}, {"score": 0.006265896881936255, "phrase": "proposed_method"}, {"score": 0.004762202897268383, "phrase": "component-based_transferring_of_facial_expressions"}, {"score": 0.0046073860169839305, "phrase": "efficient_and_economic_video-driven_technique"}, {"score": 0.00428892403807229, "phrase": "automatic_syntheses"}, {"score": 0.004241914481080374, "phrase": "vivid_facial_animations"}, {"score": 0.004172360444377281, "phrase": "proposed_technique"}, {"score": 0.0039053194919285725, "phrase": "source_human_character"}, {"score": 0.003757480190200645, "phrase": "synthesized_avatar"}, {"score": 0.003516898780234098, "phrase": "component-based_approach"}, {"score": 0.0033837135367113004, "phrase": "existing_approaches"}, {"score": 0.003309877074892399, "phrase": "whole_face"}, {"score": 0.00325555555110395, "phrase": "single_unit"}, {"score": 0.003030247482854539, "phrase": "facial_expressions"}, {"score": 0.0029641011108505785, "phrase": "synthesized_target_face"}, {"score": 0.002789528662623221, "phrase": "good_way"}, {"score": 0.0027286224888843956, "phrase": "synthesizing_parameters"}, {"score": 0.002669042572650594, "phrase": "source_human_face"}, {"score": 0.0025963890234079333, "phrase": "target_avatar_face"}, {"score": 0.002511803988580901, "phrase": "person-specific_characteristics"}, {"score": 0.002470547930651519, "phrase": "target_avatar"}, {"score": 0.0023768935613577985, "phrase": "color_inconsistencies"}, {"score": 0.002224510044064184, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "interesting_and_colorful_transfers"}], "paper_keywords": ["facial expression synthesis", " active appearance model", " color correction", " facial feature tracking", " virtual avatar"], "paper_abstract": "This paper proposes an efficient and economic video-driven technique that enables the instant creation of a large diversity of virtual avatars and the automatic syntheses of vivid facial animations. The proposed technique addresses the expression transferring problem which transfers a given facial expression of a source human character to the corresponding one of a synthesized avatar. In tackling the expression transferring problem, we propose a component-based approach which is more appealing than the existing approaches which treat the whole face as a single unit for expression transferring. Our approach acquires a much higher diversity in synthesizing virtual avatars and facial expressions by composing the synthesized target face from the facial components of different avatars. The proposed method achieves a good way to transfer the synthesizing parameters acquired from the source human face to those of the target avatar face which complies well with the person-specific characteristics of the target avatar. Additionally, the removal of color inconsistencies among the facial components from different avatars is also well handled. Some experimental results are demonstrated to show that the proposed method can achieve interesting and colorful transfers of facial expressions and synthesize a large diversity of virtual avatars instantly.", "paper_title": "Video-Driven Creation of Virtual Avatars by Component-based Transferring of Facial Expressions", "paper_id": "WOS:000327280200013"}