{"auto_keywords": [{"score": 0.0487007149356068, "phrase": "velocity_fields"}, {"score": 0.0481212980650811, "phrase": "video_sequence"}, {"score": 0.04514008383848609, "phrase": "multiple_motion_fields"}, {"score": 0.00481495049065317, "phrase": "multiple_motion_fields_from_video_sequences"}, {"score": 0.004755054742217193, "phrase": "region_matching_based_approach"}, {"score": 0.0045607070754363245, "phrase": "important_step"}, {"score": 0.004522796947899013, "phrase": "activity_classification"}, {"score": 0.004466519142879166, "phrase": "surveillance_system"}, {"score": 0.00421297029755273, "phrase": "efficient_tool"}, {"score": 0.004040688646993605, "phrase": "automatic_classification"}, {"score": 0.003875424718379985, "phrase": "trajectory_detection"}, {"score": 0.0038431888954930083, "phrase": "noisy_environments"}, {"score": 0.0037324526526073404, "phrase": "sort_manual_editing"}, {"score": 0.0034620831765465425, "phrase": "far-field_surveillance_scenarios"}, {"score": 0.0033905067962602515, "phrase": "user_intervention"}, {"score": 0.003279039724748895, "phrase": "multiple_moving_objects"}, {"score": 0.0031579997262619758, "phrase": "active_regions"}, {"score": 0.00290473242917264, "phrase": "consecutive_time"}, {"score": 0.0027741761880779535, "phrase": "corresponding_velocity_vectors"}, {"score": 0.002728166176898539, "phrase": "local_motions"}, {"score": 0.0026494723416072316, "phrase": "motion_correspondence_algorithm"}, {"score": 0.0025409639091476363, "phrase": "contiguous_sequence"}, {"score": 0.0024573577356299765, "phrase": "moving_object"}, {"score": 0.002426717832834137, "phrase": "second_contribution"}, {"score": 0.002298288831092204, "phrase": "previously_computed_ones"}, {"score": 0.0022601535748259785, "phrase": "extensive_video_sequences"}, {"score": 0.0022413233627905696, "phrase": "university_campuses"}, {"score": 0.0022133711375320244, "phrase": "motion_fields"}, {"score": 0.002149494537191303, "phrase": "automatically_detected_trajectories"}, {"score": 0.0021049977753042253, "phrase": "fully_automatic_procedure"}], "paper_keywords": ["Region matching", " trajectories", " vector fields", " video segmentation"], "paper_abstract": "Estimation of velocity fields from a video sequence is an important step towards activity classification in a surveillance system. It has been recently shown that multiple motion fields estimated from trajectories are an efficient tool to describe the movement of objects, allowing an automatic classification of activities in the scene. However, the trajectory detection in noisy environments is difficult, usually requiring some sort manual editing to complete or correct them. This paper proposes two novel contributions. First, an automatic method for building pedestrian trajectories in far-field surveillance scenarios is presented not requiring user intervention. This basically comprises the detection of multiple moving objects in a video sequence through the detection of the active regions, followed by the estimation of the velocity fields that is accomplished by performing region matching of the above regions at consecutive time instants. This leads to a sequence of centroids and corresponding velocity vectors, describing the local motions presented in the image. A motion correspondence algorithm is then applied to group the centroids in a contiguous sequence of frames into trajectories corresponding to each moving object. The second contribution is a method for automatically finding the trajectories from a library of previously computed ones. Experiments on extensive video sequences from university campuses show that motion fields can be reliably estimated from these automatically detected trajectories, leading to a fully automatic procedure for the estimation of multiple motion fields.", "paper_title": "Automatic Estimation of Multiple Motion Fields From Video Sequences Using a Region Matching Based Approach", "paper_id": "WOS:000328948100001"}