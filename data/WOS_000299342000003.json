{"auto_keywords": [{"score": 0.049027569260597106, "phrase": "based_anonymization"}, {"score": 0.0349694581102903, "phrase": "published_data"}, {"score": 0.00481495049065317, "phrase": "anonymized_data"}, {"score": 0.004605748839642513, "phrase": "privacy-preserving_data_publishing"}, {"score": 0.00450456611645112, "phrase": "group_based_anonymization"}, {"score": 0.004066898031169739, "phrase": "fundamental_issue"}, {"score": 0.004013025478863664, "phrase": "privacy_exposure"}, {"score": 0.0037209035703970705, "phrase": "group_based_anonymization_approach"}, {"score": 0.0036229421570951807, "phrase": "individual_record"}, {"score": 0.003527550673371394, "phrase": "data_privacy"}, {"score": 0.0031845067058084583, "phrase": "individual_privacy"}, {"score": 0.0030868583963299698, "phrase": "medical_records"}, {"score": 0.0026652435674846095, "phrase": "anonymized_group"}, {"score": 0.00260657976055413, "phrase": "higher_likelihood"}, {"score": 0.0025492038831408715, "phrase": "derived_patterns"}, {"score": 0.0023951660734372877, "phrase": "background_knowledge"}, {"score": 0.0022604585778195152, "phrase": "previous_work"}, {"score": 0.002152397315548125, "phrase": "privacy_benchmark_dataset"}, {"score": 0.002123831297790248, "phrase": "traditional_group"}, {"score": 0.0021049977753042253, "phrase": "anonymization_approach"}], "paper_keywords": ["Privacy preservation", " data publishing", " l-diversity", " k-anonymity"], "paper_abstract": "Group based anonymization is the most widely studied approach for privacy-preserving data publishing. Privacy models/definitions using group based anonymization includes k-anonymity, l-diversity, and t-closeness, to name a few. The goal of this article is to raise a fundamental issue regarding the privacy exposure of the approaches using group based anonymization. This has been overlooked in the past. The group based anonymization approach by bucketization basically hides each individual record behind a group to preserve data privacy. If not properly anonymized, patterns can actually be derived from the published data and be used by an adversary to breach individual privacy. For example, from the medical records released, if patterns such as that people from certain countries rarely suffer from some disease can be derived, then the information can be used to imply linkage of other people in an anonymized group with this disease with higher likelihood. We call the derived patterns from the published data the foreground knowledge. This is in contrast to the background knowledge that the adversary may obtain from other channels, as studied in some previous work. Finally, our experimental results show such an attack is realistic in the privacy benchmark dataset under the traditional group based anonymization approach.", "paper_title": "Can the Utility of Anonymized Data be Used for Privacy Breaches?", "paper_id": "WOS:000299342000003"}