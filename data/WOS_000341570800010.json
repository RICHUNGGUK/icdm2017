{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "entity_resolution"}, {"score": 0.004692766317526887, "phrase": "inherently_quadratic_task"}, {"score": 0.0046073860169839305, "phrase": "large_data_collections"}, {"score": 0.004473982381245469, "phrase": "highly_heterogeneous_information_spaces"}, {"score": 0.004296809240805996, "phrase": "high_effectiveness"}, {"score": 0.004218602458288531, "phrase": "lower_efficiency"}, {"score": 0.004007083686875716, "phrase": "coarse-grained_block_processing_techniques"}, {"score": 0.0038483286199081, "phrase": "resolution_process"}, {"score": 0.0036958398682001015, "phrase": "generic_procedure"}, {"score": 0.003510439768484619, "phrase": "initial_set"}, {"score": 0.0034464962568390265, "phrase": "new_one"}, {"score": 0.0034212450542713607, "phrase": "substantially_fewer_comparisons"}, {"score": 0.0033961782274326948, "phrase": "equally_high_effectiveness"}, {"score": 0.003109280367693954, "phrase": "block-to-entity_relationships"}, {"score": 0.002996992362027833, "phrase": "abstract_graph_representation"}, {"score": 0.0029641011108505785, "phrase": "original_set"}, {"score": 0.0028675712105299496, "phrase": "entity_profiles"}, {"score": 0.00280496585980086, "phrase": "co-occurring_ones"}, {"score": 0.002713604305939145, "phrase": "redundant_comparisons"}, {"score": 0.0026543514413663893, "phrase": "superfluous_ones"}, {"score": 0.0025490525773769063, "phrase": "lowest_weight"}, {"score": 0.0024389277933858054, "phrase": "edge_weighting_schemes"}, {"score": 0.0022081920438290193, "phrase": "thorough_experimental_study"}, {"score": 0.0021049977753042253, "phrase": "negligible_cost"}], "paper_keywords": ["Entity resolution", " redundancy-positive blocking", " meta-blocking"], "paper_abstract": "Entity Resolution is an inherently quadratic task that typically scales to large data collections through blocking. In the context of highly heterogeneous information spaces, blocking methods rely on redundancy in order to ensure high effectiveness at the cost of lower efficiency (i.e., more comparisons). This effect is partially ameliorated by coarse-grained block processing techniques that discard entire blocks either a-priori or during the resolution process. In this paper, we introduce meta-blocking as a generic procedure that intervenes between the creation and the processing of blocks, transforming an initial set of blocks into a new one with substantially fewer comparisons and equally high effectiveness. In essence, meta-blocking aims at extracting the most similar pairs of entities by leveraging the information that is encapsulated in the block-to-entity relationships. To this end, it first builds an abstract graph representation of the original set of blocks, with the nodes corresponding to entity profiles and the edges connecting the co-occurring ones. During the creation of this structure all redundant comparisons are discarded, while the superfluous ones can be removed by pruning of the edges with the lowest weight. We analytically examine both procedures, proposing a multitude of edge weighting schemes, graph pruning algorithms as well as pruning criteria. Our approaches are schema-agnostic, thus accommodating any type of blocks. We evaluate their performance through a thorough experimental study over three large-scale, real-world data sets, with the outcomes verifying significant efficiency enhancements at a negligible cost in effectiveness.", "paper_title": "Meta-Blocking: Taking Entity Resolution to the Next Level", "paper_id": "WOS:000341570800010"}