{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "matrix_factorization"}, {"score": 0.015405701639645041, "phrase": "shared_memory_systems"}, {"score": 0.0087370300963425, "phrase": "fpsg"}, {"score": 0.004347972379250724, "phrase": "effective_method"}, {"score": 0.004260134699483774, "phrase": "recommender_systems"}, {"score": 0.0033004240095380623, "phrase": "sequential_approach"}, {"score": 0.003233732344874969, "phrase": "sg"}, {"score": 0.0029798924689472014, "phrase": "web-scale_problems"}, {"score": 0.002718045523483502, "phrase": "fast_parallel_sg_method"}, {"score": 0.0024289743270061157, "phrase": "cache-miss_rate"}, {"score": 0.0023079225317714815, "phrase": "load_balance"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_parallel_algorithms"}], "paper_keywords": ["Parallel and Vector Implementations", " Recommender system", " matrix factorization", " stochastic gradient descent", " parallel computing", " shared memory algorithm"], "paper_abstract": "Matrix factorization is known to be an effective method for recommender systems that are given only the ratings from users to items. Currently, stochastic gradient (SG) method is one of the most popular algorithms for matrix factorization. However, as a sequential approach, SG is difficult to be parallelized for handling web-scale problems. In this article, we develop a fast parallel SG method, FPSG, for shared memory systems. By dramatically reducing the cache-miss rate and carefully addressing the load balance of threads, FPSG is more efficient than state-of-the-art parallel algorithms for matrix factorization.", "paper_title": "A Fast Parallel Stochastic Gradient Method for Matrix Factorization in Shared Memory Systems", "paper_id": "WOS:000353638800002"}