{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "affect_recognition"}, {"score": 0.0046164478775379105, "phrase": "multimodal_database"}, {"score": 0.004496542293425754, "phrase": "affective_stimuli"}, {"score": 0.004379737380744606, "phrase": "emotion_recognition"}, {"score": 0.00433386573276633, "phrase": "implicit_tagging_research"}, {"score": 0.004265953661191515, "phrase": "multimodal_setup"}, {"score": 0.004177049813981273, "phrase": "synchronized_recording"}, {"score": 0.004133292222875435, "phrase": "face_videos"}, {"score": 0.004089991141220082, "phrase": "audio_signals"}, {"score": 0.004047141845635495, "phrase": "eye_gaze_data"}, {"score": 0.003819339842998955, "phrase": "different_cultural_backgrounds"}, {"score": 0.0036810726176482278, "phrase": "first_experiment"}, {"score": 0.003278167066287366, "phrase": "emotional_keywords"}, {"score": 0.003209782650616367, "phrase": "second_experiment"}, {"score": 0.0031761255583359726, "phrase": "short_videos"}, {"score": 0.0029814443878969973, "phrase": "correct_or_incorrect_tags"}, {"score": 0.0028734230458881903, "phrase": "displayed_tags"}, {"score": 0.0027547408718334603, "phrase": "recorded_videos"}, {"score": 0.0027258422018680453, "phrase": "bodily_responses"}, {"score": 0.0025052769325080255, "phrase": "academic_community"}, {"score": 0.0024659480471085405, "phrase": "web-based_system"}, {"score": 0.0024272350606639147, "phrase": "collected_data"}, {"score": 0.002376559242152662, "phrase": "single_modality_and_modality_fusion_results"}, {"score": 0.0023025177990160487, "phrase": "implicit_tagging_experiments"}, {"score": 0.002207363758847308, "phrase": "potential_uses"}, {"score": 0.002172701452347454, "phrase": "recorded_modalities"}, {"score": 0.0021049977753042253, "phrase": "emotion_elicitation_protocol"}], "paper_keywords": ["Emotion recognition", " EEG", " physiological signals", " facial expressions", " eye gaze", " implicit tagging", " pattern classification", " affective computing"], "paper_abstract": "MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.", "paper_title": "A Multimodal Database for Affect Recognition and Implicit Tagging", "paper_id": "WOS:000323625000006"}