{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "live_video_streams"}, {"score": 0.04693443770971553, "phrase": "field_effects"}, {"score": 0.0047267100475487595, "phrase": "stereo_solution"}, {"score": 0.004555028829645471, "phrase": "dynamic_depth"}, {"score": 0.004362564831572539, "phrase": "recently_a_quality"}, {"score": 0.00428257928723, "phrase": "movie_cameras"}, {"score": 0.00405127920820053, "phrase": "computational_camera_solution"}, {"score": 0.003976978140059542, "phrase": "real-time_gpu_processing"}, {"score": 0.0039040344230094164, "phrase": "runtime_dynamic_depth"}, {"score": 0.0037159680715717056, "phrase": "hybrid-resolution_stereo_camera"}, {"score": 0.0035369291417564606, "phrase": "low-res_disparity_map"}, {"score": 0.0034294219963679857, "phrase": "gpu-based_belief_propagation"}, {"score": 0.003204232566468624, "phrase": "recovered_high-resolution_disparity_map"}, {"score": 0.0031068064097811844, "phrase": "high-resolution_video_stream"}, {"score": 0.0029753513692467315, "phrase": "light_field"}, {"score": 0.002849442549612025, "phrase": "parallel_processing_and_atomic_operations"}, {"score": 0.0027973345391441584, "phrase": "gpu"}, {"score": 0.0027120400060701034, "phrase": "multiple_pixels"}, {"score": 0.0025495426598675583, "phrase": "racking_focus"}, {"score": 0.0025027144548626975, "phrase": "focus_effects"}, {"score": 0.002456744238900652, "phrase": "synthesized_light_field"}, {"score": 0.0023967582707115354, "phrase": "processing_stages"}, {"score": 0.0023382335380460304, "phrase": "nvidia's_cuda_architecture"}], "paper_keywords": ["Dynamic depth of field", " Racking focus", " Tracking focus", " Belief propagation", " Cross bilateral Filtering", " Light field", " CUDA"], "paper_abstract": "The ability to produce dynamic Depth of Field effects in live video streams was until recently a quality unique to movie cameras. In this paper, we present a computational camera solution coupled with real-time GPU processing to produce runtime dynamic Depth of Field effects. We first construct a hybrid-resolution stereo camera with a high-res/low-res camera pair. We recover a low-res disparity map of the scene using GPU-based Belief Propagation, and subsequently upsample it via fast Cross/Joint Bilateral Upsampling. With the recovered high-resolution disparity map, we warp the high-resolution video stream to nearby viewpoints to synthesize a light field toward the scene. We exploit parallel processing and atomic operations on the GPU to resolve visibility when multiple pixels warp to the same image location. Finally, we generate racking focus and tracking focus effects from the synthesized light field rendering. All processing stages are mapped onto NVIDIA's CUDA architecture. Our system can produce racking and tracking focus effects for the resolution of 640x480 at 15 fps.", "paper_title": "Racking focus and tracking focus on live video streams: a stereo solution", "paper_id": "WOS:000329800500004"}