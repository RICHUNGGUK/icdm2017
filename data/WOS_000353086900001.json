{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "expressive_dynamic_physical_behaviors"}, {"score": 0.004690694501758065, "phrase": "social_robots"}, {"score": 0.004522080178250592, "phrase": "appropriate_manner"}, {"score": 0.004382364826200616, "phrase": "apt_affect_displays"}, {"score": 0.004314127287770832, "phrase": "underlying_emotional_intelligence"}, {"score": 0.004202741104973709, "phrase": "artificial_emotional_intelligence_system"}, {"score": 0.004030449969901066, "phrase": "perceptual_aspect"}, {"score": 0.003946960862020283, "phrase": "generative_side"}, {"score": 0.0038450179181069833, "phrase": "expressive_capabilities"}, {"score": 0.0035733198242955634, "phrase": "facial_expressions"}, {"score": 0.0035176353680157367, "phrase": "complex_humanoid_design"}, {"score": 0.003444732262403694, "phrase": "emotionally_expressive_robots"}, {"score": 0.003085989256759819, "phrase": "animated_vs_static_affect_expressions"}, {"score": 0.0029438813080245544, "phrase": "personal_preference_ratings"}, {"score": 0.0027071661844502992, "phrase": "second_experiment"}, {"score": 0.0026096659285441384, "phrase": "expression_variables"}, {"score": 0.0025555322596460036, "phrase": "intended_user"}, {"score": 0.002476424729269258, "phrase": "generative_system"}, {"score": 0.0024377918036784336, "phrase": "perceptual_component"}, {"score": 0.002412371116984224, "phrase": "natural_language_sentiment_analysis"}, {"score": 0.002337684811187166, "phrase": "third_experiment"}, {"score": 0.0022299587520401747, "phrase": "increased_engagement"}, {"score": 0.002172266188075892, "phrase": "arbitrarily_chosen_comparable_motion_parameters"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Human robotic interaction", " Emotion", " Affective computing", " Expression", " Sentiment analysis"], "paper_abstract": "For social robots to respond to humans in an appropriate manner, they need to use apt affect displays, revealing underlying emotional intelligence. We present an artificial emotional intelligence system for robots, with both a generative and a perceptual aspect. On the generative side, we explore the expressive capabilities of an abstract, faceless, creature-like robot, with very few degrees of freedom, lacking both facial expressions and the complex humanoid design found often in emotionally expressive robots. We validate our system in a series of experiments: in one study, we find an advantage in classification for animated vs static affect expressions and advantages in valence and arousal estimation and personal preference ratings for both animated vs static and physical vs on-screen expressions. In a second experiment, we show that our parametrically generated expression variables correlate with the intended user affect perception. Combining the generative system with a perceptual component of natural language sentiment analysis, we show in a third experiment that our automatically generated affect responses cause participants to show signs of increased engagement and enjoyment compared with arbitrarily chosen comparable motion parameters. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Emotionally expressive dynamic physical behaviors in robots", "paper_id": "WOS:000353086900001"}