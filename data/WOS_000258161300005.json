{"auto_keywords": [{"score": 0.029302631271982942, "phrase": "worst_case"}, {"score": 0.025820356182494404, "phrase": "new_heuristic"}, {"score": 0.00481495049065317, "phrase": "automatic_derivation"}, {"score": 0.004785356349906913, "phrase": "heuristic_functions"}, {"score": 0.0046544071297876695, "phrase": "fundamental_technique"}, {"score": 0.004376039612434174, "phrase": "simple_planning_models"}, {"score": 0.00405127920820053, "phrase": "corresponding_heuristic"}, {"score": 0.003738973626996355, "phrase": "well-known_delete-relaxation_heuristic"}, {"score": 0.003558830226733501, "phrase": "preferred_models"}, {"score": 0.0033768959341690524, "phrase": "suitable_relaxation"}, {"score": 0.0032945169562318575, "phrase": "strong_completion"}, {"score": 0.0032641439000713306, "phrase": "logic_program"}, {"score": 0.0032340499528688495, "phrase": "time_indices"}, {"score": 0.0030876784650745973, "phrase": "search_state"}, {"score": 0.002984554267578035, "phrase": "compiled_representation"}, {"score": 0.0029297595441334823, "phrase": "evaluation_network"}, {"score": 0.0028670988845838296, "phrase": "heuristic_values"}, {"score": 0.0027884948773692153, "phrase": "exponential_size"}, {"score": 0.002728856202777593, "phrase": "obdds"}, {"score": 0.0026458370516131255, "phrase": "empirical_results"}, {"score": 0.0023893635026860727, "phrase": "delete-based_relaxations"}, {"score": 0.0023527296826339225, "phrase": "valid_but_implicit_plan_constraints"}, {"score": 0.0023023818355050237, "phrase": "traveling_salesman_problem"}, {"score": 0.002260083020280497, "phrase": "exact_cost"}, {"score": 0.002239225269052105, "phrase": "delete-relaxation_heuristic"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["planning", " planning heuristics", " planning with rewards", " knowledge compilation"], "paper_abstract": "The automatic derivation of heuristic functions for guiding the search for plans is a fundamental technique in planning. The type of heuristics that have been considered so far, however, deal only with simple planning models where costs are associated with actions but not with states. In this work we address this limitation by formulating a more expressive planning model and a corresponding heuristic where preferences in the form of penalties and rewards are associated with fluents as well. The heuristic, that is a generalization of the well-known delete-relaxation heuristic, is admissible, informative, but intractable. Exploiting a correspondence between heuristics and preferred models, and a property of formulas compiled in d-DNNF, we show however that if a suitable relaxation of the domain, expressed as the strong completion of a logic program with no time indices or horizon is compiled into d-DNNF, the heuristic can be computed for any search state in time that is linear in the size of the compiled representation. This representation defines an evaluation network or circuit that maps states into heuristic values in linear-time. While this circuit may have exponential size in the worst case, as for OBDDs, this is not necessarily so. We report empirical results, discuss the application of the framework in settings where there are no goals but just preferences, and illustrate the versatility of the account by developing a new heuristic that overcomes limitations of delete-based relaxations through the use of valid but implicit plan constraints. In particular, for the Traveling Salesman Problem, the new heuristic captures the exact cost while the delete-relaxation heuristic, which is also exponential in the worst case, captures only the Minimum Spanning Tree lower bound. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Heuristics for planning with penalties and rewards formulated in logic and computed through circuits", "paper_id": "WOS:000258161300005"}