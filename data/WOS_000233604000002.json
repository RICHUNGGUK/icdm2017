{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "discriminant_projections"}, {"score": 0.04010882425349306, "phrase": "projection_matrix"}, {"score": 0.004762202897268383, "phrase": "nearest_neighbor_classification"}, {"score": 0.004556901872357751, "phrase": "new_embedding_technique"}, {"score": 0.004457579615095599, "phrase": "linear_projection"}, {"score": 0.004408729661992651, "phrase": "best_projects"}, {"score": 0.004360412703519539, "phrase": "data_samples"}, {"score": 0.00428892403807229, "phrase": "new_space"}, {"score": 0.004149429072483132, "phrase": "nearest_neighbor_classifier"}, {"score": 0.003992385826251404, "phrase": "large_set"}, {"score": 0.003948613642680196, "phrase": "one-dimensional_projections"}, {"score": 0.0034975571855450343, "phrase": "classifier_selection_task"}, {"score": 0.0033837135367113004, "phrase": "adaboost_algorithm"}, {"score": 0.003309877074892399, "phrase": "optimal_set"}, {"score": 0.0032198357970567595, "phrase": "main_advantage"}, {"score": 0.003097865379563127, "phrase": "final_projection_matrix"}, {"score": 0.0030135741897058844, "phrase": "global_assumption"}, {"score": 0.0029641011108505785, "phrase": "data_distribution"}, {"score": 0.00280496585980086, "phrase": "classification_error"}, {"score": 0.002758907973783665, "phrase": "training_data"}, {"score": 0.0026838147966444783, "phrase": "resulting_features"}, {"score": 0.0022492129111567824, "phrase": "manuscript_digits"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["feature extraction", " classifier selection", " linear discriminant analysis", " boosting", " prototype selection", " dimensionality reduction"], "paper_abstract": "In this paper we introduce a new embedding technique to find the linear projection that best projects labeled data samples into a new space where the performance of a Nearest Neighbor classifier is maximized. We consider a large set of one-dimensional projections and combine them into a projection matrix, which is not restricted to be orthogonal. The embedding is defined as a classifier selection task that makes use of the AdaBoost algorithm to find an optimal set of discriminant projections. The main advantage of the algorithm is that the final projection matrix does not make any global assumption on the data distribution, and the projection matrix is created by minimizing the classification error in the training data set. Also the resulting features can be ranked according to a set of coefficients computed during the algorithm. The performance of our embedding is tested in two different pattern recognition tasks, a gender recognition problem and the classification of manuscript digits. (c) 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Boosted discriminant projections for nearest neighbor classification", "paper_id": "WOS:000233604000002"}