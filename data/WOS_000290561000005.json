{"auto_keywords": [{"score": 0.049496721057450455, "phrase": "reinforcement_learning"}, {"score": 0.004501453956464956, "phrase": "machine_intelligence_techniques"}, {"score": 0.004260134699483774, "phrase": "real-world_problems"}, {"score": 0.004157058883925715, "phrase": "rl_agents"}, {"score": 0.0036553070575015344, "phrase": "optimal_policy"}, {"score": 0.003501846136829077, "phrase": "special_type"}, {"score": 0.003022825806274343, "phrase": "discrete_state"}, {"score": 0.002985988377995789, "phrase": "action_space"}, {"score": 0.002860546817196148, "phrase": "rl_agent"}, {"score": 0.002409188797323273, "phrase": "value_function"}, {"score": 0.0023507916951658455, "phrase": "faster_convergence"}, {"score": 0.002210903392596343, "phrase": "reinforcement_learning_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Q(lambda)", " Opposite action", " Opposition-based learning (OBL)", " OQ(lambda) algorithm", " NOQ(lambda) algorithm", " Opposition weight"], "paper_abstract": "Reinforcement learning (RL) is one of the machine intelligence techniques with several characteristics that make it suitable for solving real-world problems. However, RL agents generally face a very large state space in many applications. They must take actions in every state many times to find the optimal policy. In this work, a special type of knowledge about actions is employed to improve the performance of the off-policy, incremental, and model-free reinforcement learning with discrete state and action space. One of the components of RL agent is the action. For each action, its associate opposite action is defined. The actions and opposite actions are implemented in the framework of reinforcement learning to update the value function resulting in a faster convergence. The effects of opposite action on some of the reinforcement learning algorithms are investigated. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Knowledge of opposite actions for reinforcement learning", "paper_id": "WOS:000290561000005"}