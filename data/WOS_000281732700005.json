{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "abstract_features"}, {"score": 0.004413846562278824, "phrase": "inverse_reinforcement_learning"}, {"score": 0.004135010810625326, "phrase": "dimension_reduction_methods"}, {"score": 0.0038737214010638745, "phrase": "human-demonstrated_policies"}, {"score": 0.0032546105164772995, "phrase": "importance_rating"}, {"score": 0.0031500213134140953, "phrase": "abstract_feature"}, {"score": 0.0029830992938695007, "phrase": "reward_function"}, {"score": 0.0026175952455594277, "phrase": "five-lane_highway"}, {"score": 0.0025059724686541263, "phrase": "controlled_car"}, {"score": 0.0024253827365474734, "phrase": "largest_fixed_speed"}, {"score": 0.0021049977753042253, "phrase": "importance_ratings"}], "paper_keywords": ["Importance rating", " Abstract feature", " Feature extraction", " Inverse reinforcement learning (IRL)", " Markov decision process (MDP)"], "paper_abstract": "We improve inverse reinforcement learning (IRL) by applying dimension reduction methods to automatically extract abstract features from human-demonstrated policies, to deal with the cases where features are either unknown or numerous. The importance rating of each abstract feature is incorporated into the reward function. Simulation is performed on a task of driving in a five-lane highway, where the controlled car has the largest fixed speed among all the cars. Performance is almost 10.6% better on average with than without importance ratings.", "paper_title": "Modified reward function on abstract features in inverse reinforcement learning", "paper_id": "WOS:000281732700005"}