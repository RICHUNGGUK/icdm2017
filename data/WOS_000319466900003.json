{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sample_complexity"}, {"score": 0.015344558961682095, "phrase": "generative_model"}, {"score": 0.014798291687851458, "phrase": "optimal_action-value_function"}, {"score": 0.013597400948170087, "phrase": "rl"}, {"score": 0.004420472010542188, "phrase": "discounted-reward_markov_decision"}, {"score": 0.004278807087784626, "phrase": "new_pac_bounds"}, {"score": 0.0038332390893263844, "phrase": "first_result"}, {"score": 0.003725470458263889, "phrase": "n_state-action_pairs"}, {"score": 0.0032302647179383915, "phrase": "small_values"}, {"score": 0.002581324549141829, "phrase": "first_minimax_result"}, {"score": 0.0024984517279573906, "phrase": "upper_bounds"}, {"score": 0.0023121112251855667, "phrase": "constant_factor"}], "paper_keywords": ["Sample complexity", " Markov decision processes", " Reinforcement learning", " Learning theory"], "paper_abstract": "We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor gamma a[0,1) only O(Nlog(N/delta)/((1-gamma)(3) epsilon (2))) state-transition samples are required to find an epsilon-optimal estimation of the action-value function with the probability (w.p.) 1-delta. Further, we prove that, for small values of epsilon, an order of O(Nlog(N/delta)/((1-gamma)(3) epsilon (2))) samples is required to find an epsilon-optimal policy w.p. 1-delta. We also prove a matching lower bound of I similar to(Nlog(N/delta)/((1-gamma)(3) epsilon (2))) on the sample complexity of estimating the optimal action-value function with epsilon accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of N, epsilon, delta and 1/(1-gamma) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1-gamma).", "paper_title": "Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model", "paper_id": "WOS:000319466900003"}