{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "high-dimensional_domains"}, {"score": 0.004352876088312741, "phrase": "sequential_decision_problems"}, {"score": 0.004069631519385246, "phrase": "problem-specific_domain_information"}, {"score": 0.004011366422179016, "phrase": "carefully_crafted_representation"}, {"score": 0.0037864980290391354, "phrase": "new_approaches"}, {"score": 0.00334144503360907, "phrase": "small_set"}, {"score": 0.003309452522760298, "phrase": "human_demonstrations"}, {"score": 0.003138874040786261, "phrase": "state-space_abstraction"}, {"score": 0.0029914315871119104, "phrase": "ada"}, {"score": 0.002864628676257225, "phrase": "task_decomposition"}, {"score": 0.002756497259882976, "phrase": "rl"}, {"score": 0.002690886043654469, "phrase": "higher-complexity_domains"}, {"score": 0.00247944491239733, "phrase": "videogame-like_domains"}, {"score": 0.002329009499335603, "phrase": "exponential_speed-ups"}, {"score": 0.0023066880707738736, "phrase": "table-based_representations"}, {"score": 0.002208853811052348, "phrase": "approximation-based_rl_algorithms"}, {"score": 0.0021771713131746636, "phrase": "fitted_q-learning"}, {"score": 0.002156302290805023, "phrase": "lspi."}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Learning from demonstration", " Dimensionality reduction", " Function approximation"], "paper_abstract": "Reinforcement learning (RL) and learning from demonstration (LID) are two popular families of algorithms for learning policies for sequential decision problems, but they are often ineffective in high-dimensional domains unless provided with either a great deal of problem-specific domain information or a carefully crafted representation of the state and dynamics of the world. We introduce new approaches inspired by these two techniques, which we broadly call abstraction from demonstration. Our first algorithm, state abstraction from demonstration (AM), uses a small set of human demonstrations of the task the agent must learn to determine a state-space abstraction. Our second algorithm, abstraction and decomposition from demonstration (ADA), is additionally able to determine a task decomposition from the demonstrations. These abstractions allow RL to scale up to higher-complexity domains, and offer much better performance than LfD with orders of magnitude fewer demonstrations. Using a set of videogame-like domains, we demonstrate that using abstraction from demonstration can obtain up to exponential speed-ups in table-based representations, and polynomial speed-ups when compared with function approximation-based RL algorithms such as fitted Q-learning and LSPI. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Abstraction from demonstration for efficient reinforcement learning in high-dimensional domains", "paper_id": "WOS:000342253000005"}