{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "naive_bayes"}, {"score": 0.01560544606717282, "phrase": "probability_estimation"}, {"score": 0.008586651502377135, "phrase": "feature_selection"}, {"score": 0.005016537112879559, "phrase": "sbc"}, {"score": 0.004657739551687038, "phrase": "well-known_effective_and_efficient_classification_algorithm"}, {"score": 0.004423284899590467, "phrase": "accurate_probability_estimation"}, {"score": 0.004294625285753825, "phrase": "optimal_decisions"}, {"score": 0.004169692306713217, "phrase": "conditional_log_likelihood"}, {"score": 0.0040334640350288, "phrase": "learning_algorithms"}, {"score": 0.003930581102447622, "phrase": "high_cll"}, {"score": 0.003895342370729657, "phrase": "significant_improvement"}, {"score": 0.003872970200742635, "phrase": "erl"}, {"score": 0.0038666885140844496, "phrase": "classification_accuracy"}, {"score": 0.003597284507308585, "phrase": "simple_but_effective_and_efficient_approach"}, {"score": 0.003267925215811654, "phrase": "search_process"}, {"score": 0.003064025731282175, "phrase": "sbc-cll"}, {"score": 0.0030576885787607796, "phrase": "selected_attribute"}, {"score": 0.0028293966058473476, "phrase": "feature_selection_algorithms"}, {"score": 0.0027878797103973313, "phrase": "selective_bayesian_classifiers"}, {"score": 0.0027368368719283298, "phrase": "langley_et_al"}, {"score": 0.0026768142335374156, "phrase": "good_performance"}, {"score": 0.0024677487011556427, "phrase": "improved_sbc_algorithm_sbc-cll"}, {"score": 0.002422552850701705, "phrase": "cll_score"}, {"score": 0.0023781827711414107, "phrase": "attribute_selection"}, {"score": 0.002136367070353946, "phrase": "cll."}, {"score": 0.0021049977753042253, "phrase": "efficient_and_surprisingly_effective_approach"}], "paper_keywords": [""], "paper_abstract": "Naive Bayes is a well-known effective and efficient classification algorithm. But its probability estimation is poor. In many applications, however, accurate probability estimation is often required in order to make optimal decisions. Usually, probability estimation is measured by conditional log likelihood (CLL). There have been some learning algorithms proposed recently to extend naive Bayes for high CLL, such as ERL [8,9] and BNC-2P [10]. Unfortunately, their computational complexity is relatively high. Is there a simple but effective and efficient approach to improve the probability estimation of naive Bayes? In this paper, we propose to use feature selection for this purpose. More precisely, a search process is conducted to select a subset of attributes, and then a naive Bayes is deployed on the selected attribute set. In fact, feature selection has been successfully applied to naive Bayes and achieves significant improvement in classification accuracy. Among the feature selection algorithms for naive Bayes, selective Bayesian classifiers (SBC) by Langley et al. [13] demonstrates good performance. In this paper, we first study the performance of SBC in terms of probability estimation, and then propose an improved SBC algorithm SBC-CLL, in which the CLL score is directly used for attribute selection, instead of using classification accuracy. Our experiments show that both SBC and SBC-CLL achieve significant improvement over naive Bayes, and that SBC-CLL outperforms SBC substantially, in probability estimation measured by CLL. Our work provides an efficient and surprisingly effective approach to improve the probability estimation of naive Bayes.", "paper_title": "Learning naive Bayes for probability estimation by feature selection", "paper_id": "WOS:000239589600043"}