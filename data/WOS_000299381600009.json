{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "human_actions"}, {"score": 0.004585433265422727, "phrase": "shape-motion_prototype-based_approach"}, {"score": 0.004261419841494814, "phrase": "efficient_and_flexible_action"}, {"score": 0.004209679094023478, "phrase": "long_video_sequences"}, {"score": 0.004108066884382925, "phrase": "action_prototype_tree"}, {"score": 0.004025258572860201, "phrase": "joint_shape"}, {"score": 0.00399260264462941, "phrase": "motion_space"}, {"score": 0.0039442060761906515, "phrase": "k-means"}, {"score": 0.0038803710796509227, "phrase": "training_sequence"}, {"score": 0.003802135020791801, "phrase": "labeled_prototype_sequence"}, {"score": 0.003695237613399466, "phrase": "prototype_distances"}, {"score": 0.0035477010181860376, "phrase": "joint_probability_model"}, {"score": 0.003504595508262064, "phrase": "actor_location"}, {"score": 0.0034761488481944657, "phrase": "action_prototype"}, {"score": 0.003310236836421397, "phrase": "prototype_correspondence"}, {"score": 0.0032302647179383915, "phrase": "joint_probability"}, {"score": 0.0031139032290048788, "phrase": "learned_prototype_tree"}, {"score": 0.0030262967460085366, "phrase": "dynamic_prototype_sequence_matching"}, {"score": 0.0029531640201737384, "phrase": "sequence_matching"}, {"score": 0.0028935680100446583, "phrase": "look-up_table_indexing"}, {"score": 0.002777949746964955, "phrase": "brute-force_computation"}, {"score": 0.0027108021939930014, "phrase": "frame_distances"}, {"score": 0.00266693888510356, "phrase": "robust_action"}, {"score": 0.002634506575564286, "phrase": "challenging_situations"}, {"score": 0.002508663916554954, "phrase": "automatic_alignment"}, {"score": 0.0024882810071079797, "phrase": "action_sequences"}, {"score": 0.0024680632991561074, "phrase": "experimental_results"}, {"score": 0.0024182330686905256, "phrase": "recognition_rates"}, {"score": 0.0023694065215394593, "phrase": "large_gesture"}, {"score": 0.002321563536990284, "phrase": "dynamic_backgrounds"}, {"score": 0.0022561982694417116, "phrase": "weizmann_action_data_set"}, {"score": 0.00221063611719246, "phrase": "kth_action_data_set"}, {"score": 0.0021659920594094407, "phrase": "ucf_sports_data"}, {"score": 0.0021049977753042253, "phrase": "cmu_action_data_set"}], "paper_keywords": ["Action recognition", " shape-motion prototype tree", " hierarchical K-means clustering", " joint probability", " dynamic time warping"], "paper_abstract": "A shape-motion prototype-based approach is introduced for action recognition. The approach represents an action as a sequence of prototypes for efficient and flexible action matching in long video sequences. During training, an action prototype tree is learned in a joint shape and motion space via hierarchical K-means clustering and each training sequence is represented as a labeled prototype sequence; then a look-up table of prototype-to-prototype distances is generated. During testing, based on a joint probability model of the actor location and action prototype, the actor is tracked while a frame-to-prototype correspondence is established by maximizing the joint probability, which is efficiently performed by searching the learned prototype tree; then actions are recognized using dynamic prototype sequence matching. Distance measures used for sequence matching are rapidly obtained by look-up table indexing, which is an order of magnitude faster than brute-force computation of frame-to-frame distances. Our approach enables robust action matching in challenging situations (such as moving cameras, dynamic backgrounds) and allows automatic alignment of action sequences. Experimental results demonstrate that our approach achieves recognition rates of 92.86 percent on a large gesture data set (with dynamic backgrounds), 100 percent on the Weizmann action data set, 95.77 percent on the KTH action data set, 88 percent on the UCF sports data set, and 87.27 percent on the CMU action data set.", "paper_title": "Recognizing Human Actions by Learning and Matching Shape-Motion Prototype Trees", "paper_id": "WOS:000299381600009"}