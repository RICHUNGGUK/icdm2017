{"auto_keywords": [{"score": 0.037074354377624366, "phrase": "backpropagation_algorithm"}, {"score": 0.026880572298754506, "phrase": "distributed_rbms"}, {"score": 0.00481495049065317, "phrase": "deep_belief_nets"}, {"score": 0.004674952036988355, "phrase": "restricted_boltzmann_machines"}, {"score": 0.0045122922461952805, "phrase": "building_block"}, {"score": 0.004406994912939125, "phrase": "wide_attention"}, {"score": 0.0037797419108291227, "phrase": "whole_net"}, {"score": 0.003626626504149897, "phrase": "sequential_implementation"}, {"score": 0.003479692058963528, "phrase": "significant_amount"}, {"score": 0.0034388111429203222, "phrase": "computational_time"}, {"score": 0.003378385493203297, "phrase": "massive_data_sets"}, {"score": 0.0033190180867511605, "phrase": "emerging_big_data_learning"}, {"score": 0.0030374213037172803, "phrase": "distributed_learning_paradigm"}, {"score": 0.0028971786856032172, "phrase": "mapreduce"}, {"score": 0.002620040660435039, "phrase": "distributed_way"}, {"score": 0.002455009746748708, "phrase": "distributed_backpropagation"}, {"score": 0.0023415390666563177, "phrase": "benchmark_data_sets"}, {"score": 0.002273292707863821, "phrase": "experimental_results"}, {"score": 0.002142696637011647, "phrase": "large-scale_data"}, {"score": 0.0021049977753042253, "phrase": "good_performance"}], "paper_keywords": ["Big data", " deep learning", " MapReduce", " Hadoop", " deep belief net (DBN)", " restricted Boltzmann machine (RBM)"], "paper_abstract": "Deep belief nets (DBNs) with restricted Boltzmann machines (RBMs) as the building block have recently attracted wide attention due to their great performance in various applications. The learning of a DBN starts with pretraining a series of the RBMs followed by fine-tuning the whole net using backpropagation. Generally, the sequential implementation of both RBMs and backpropagation algorithm takes significant amount of computational time to process massive data sets. The emerging big data learning requires distributed computing for the DBNs. In this paper, we present a distributed learning paradigm for the RBMs and the backpropagation algorithm using MapReduce, a popular parallel programming model. Thus, the DBNs can be trained in a distributed way by stacking a series of distributed RBMs for pretraining and a distributed backpropagation for fine-tuning. Through validation on the benchmark data sets of various practical problems, the experimental results demonstrate that the distributed RBMs and DBNs are amenable to large-scale data with a good performance in terms of accuracy and efficiency.", "paper_title": "Large-Scale Deep Belief Nets With MapReduce", "paper_id": "WOS:000209653800029"}