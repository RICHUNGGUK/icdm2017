{"auto_keywords": [{"score": 0.04420612461768112, "phrase": "gpu_kernel"}, {"score": 0.04138506305744283, "phrase": "dynamic_parallelism"}, {"score": 0.013401617287558884, "phrase": "parallel_loops"}, {"score": 0.010560641410231525, "phrase": "gpgpu_benchmarks"}, {"score": 0.00915122163320373, "phrase": "nested_parallelism"}, {"score": 0.00481495049065317, "phrase": "cuda-np"}, {"score": 0.004765686919942249, "phrase": "nested_thread-level_parallelism"}, {"score": 0.0046846932351254636, "phrase": "parallel_programs"}, {"score": 0.004605069682942661, "phrase": "code_sections"}, {"score": 0.004573598742696912, "phrase": "different_thread-level_parallelism"}, {"score": 0.004299823165101385, "phrase": "parallel_program"}, {"score": 0.004197817125707719, "phrase": "cuda_programs"}, {"score": 0.004126434472502474, "phrase": "sequential_code"}, {"score": 0.003973619384183145, "phrase": "latest_nvidia_kepler_architecture"}, {"score": 0.0038793218280304825, "phrase": "gpu_thread"}, {"score": 0.0035972293900260414, "phrase": "parent_thread"}, {"score": 0.0035118324133358706, "phrase": "global_memory"}, {"score": 0.0034402448248944434, "phrase": "gpu_kernels"}, {"score": 0.003019444569786258, "phrase": "tlp."}, {"score": 0.002771027577087516, "phrase": "cuda"}, {"score": 0.002723815159181826, "phrase": "cuda-np."}, {"score": 0.002649978056195926, "phrase": "high_number"}, {"score": 0.002604847103905297, "phrase": "gpu_program"}, {"score": 0.0025255346035867564, "phrase": "different_numbers"}, {"score": 0.0024910622362019573, "phrase": "different_code_sections"}, {"score": 0.002431861192375809, "phrase": "directive-based_compiler_approach"}, {"score": 0.002374063735432571, "phrase": "application_developer"}, {"score": 0.002333620799300123, "phrase": "openmp-like_pragmas"}, {"score": 0.002317636745271539, "phrase": "parallelizable_code_sections"}, {"score": 0.002254785420533933, "phrase": "optimized_gpu_kernels"}, {"score": 0.002201187194326849, "phrase": "scan_primitives"}, {"score": 0.0021786074765975006, "phrase": "different_ways"}, {"score": 0.0021562588804528667, "phrase": "parallel_loop_iterations"}, {"score": 0.0021049977753042253, "phrase": "on-chip_resource"}], "paper_keywords": ["Performance", " Design", " Experimentation", " Languages", " GPGPU", " nested parallelism", " compiler", " local memory"], "paper_abstract": "Parallel programs consist of series of code sections with different thread-level parallelism (TLP). As a result, it is rather common that a thread in a parallel program, such as a GPU kernel in CUDA programs, still contains both sequential code and parallel loops. In order to leverage such parallel loops, the latest Nvidia Kepler architecture introduces dynamic parallelism, which allows a GPU thread to start another GPU kernel, thereby reducing the overhead of launching kernels from a CPU. However, with dynamic parallelism, a parent thread can only communicate with its child threads through global memory and the overhead of launching GPU kernels is non-trivial even within GPUs. In this paper, we first study a set of GPGPU benchmarks that contain parallel loops, and highlight that these benchmarks do not have a very high loop count or high degrees of TLP. Consequently, the benefits of leveraging such parallel loops using dynamic parallelism are too limited to offset its overhead. We then present our proposed solution to exploit nested parallelism in CUDA, referred to as CUDA-NP. With CUDA-NP, we initially enable a high number of threads when a GPU program starts, and use control flow to activate different numbers of threads for different code sections. We implemented our proposed CUDA-NP framework using a directive-based compiler approach. For a GPU kernel, an application developer only needs to add OpenMP-like pragmas for parallelizable code sections. Then, our CUDA-NP compiler automatically generates the optimized GPU kernels. It supports both the reduction and the scan primitives, explores different ways to distribute parallel loop iterations into threads, and efficiently manages on-chip resource. Our experiments show that for a set of GPGPU benchmarks, which have already been optimized and contain nested parallelism, our proposed CUDA-NP framework further improves the performance by up to 6.69 times and 2.18 times on average.", "paper_title": "CUDA-NP: Realizing Nested Thread-Level Parallelism in GPGPU Applications", "paper_id": "WOS:000349142100009"}