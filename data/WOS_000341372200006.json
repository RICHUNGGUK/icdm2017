{"auto_keywords": [{"score": 0.04961597900198373, "phrase": "action_recognition"}, {"score": 0.015719716506582538, "phrase": "hierarchical_model"}, {"score": 0.004676575291629554, "phrase": "video_data"}, {"score": 0.004498213310668266, "phrase": "background_activity"}, {"score": 0.004181839414007266, "phrase": "commercial_depth_cameras"}, {"score": 0.004141365648080381, "phrase": "skeleton-based_action_recognition"}, {"score": 0.003925633992809873, "phrase": "skeleton-based_approach"}, {"score": 0.003775803846431323, "phrase": "large_variation"}, {"score": 0.0037392451233558234, "phrase": "human_actions"}, {"score": 0.0037030390595465673, "phrase": "temporal_dynamics"}, {"score": 0.0034423989472667756, "phrase": "confusing_motions"}, {"score": 0.003392507399258166, "phrase": "motion-based_grouping_method"}, {"score": 0.003215659239270654, "phrase": "group_label"}, {"score": 0.0030778231027679434, "phrase": "pre-trained_classifier"}, {"score": 0.0029747028229638625, "phrase": "previous_methods"}, {"score": 0.0029031614127920232, "phrase": "bottom-up_approach"}, {"score": 0.0027651847105654363, "phrase": "final_action_label"}, {"score": 0.0024481856932959227, "phrase": "online_real-time_performance"}, {"score": 0.0022756672722130424, "phrase": "classification_features"}, {"score": 0.002242647404307089, "phrase": "proposed_method"}], "paper_keywords": ["Robust online action recognition", " Hierarchical model", " Bottom-up approach"], "paper_abstract": "Action recognition solely based on video data has known to be very sensitive to background activity, and also lacks the ability to discriminate complex 3D motion. With the development of commercial depth cameras, skeleton-based action recognition is becoming more and more popular. However, the skeleton-based approach is still very challenging because of the large variation in human actions and temporal dynamics. In this paper, we propose a hierarchical model for action recognition. To handle confusing motions, a motion-based grouping method is proposed, which can efficiently assign each video a group label, and then for each group, a pre-trained classifier is used for frame-labeling. Unlike previous methods, we adopt a bottom-up approach that first performs action recognition for each frame. The final action label is obtained by fusing the classification to its frames, with the effect of each frame being adaptively adjusted based on its local properties. To achieve online real-time performance and suppressing noise, bag-of-words is used to represent the classification features. The proposed method is evaluated using two challenge datasets captured by a Kinect. Experiments show that our method can robustly recognize actions in real-time.", "paper_title": "Online robust action recognition based on a hierarchical model", "paper_id": "WOS:000341372200006"}