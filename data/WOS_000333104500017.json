{"auto_keywords": [{"score": 0.03644451033122438, "phrase": "hidden_variables"}, {"score": 0.02907660981165939, "phrase": "supervised_learning"}, {"score": 0.00481495049065317, "phrase": "high_dimensional_support_vector_machines"}, {"score": 0.004549063619065006, "phrase": "simple_euclidean_distance"}, {"score": 0.00450231514359539, "phrase": "orthogonal_projection"}, {"score": 0.00443308765153716, "phrase": "high_dimensional_feature_space"}, {"score": 0.004297795905522757, "phrase": "small_training_sets"}, {"score": 0.004081385289516914, "phrase": "training_data"}, {"score": 0.004039423410198574, "phrase": "input_vectors"}, {"score": 0.003956784357357341, "phrase": "full_input_space"}, {"score": 0.0037769520261285872, "phrase": "future_data"}, {"score": 0.003623949225126944, "phrase": "missed_orthogonal_subspace"}, {"score": 0.0034951469511975346, "phrase": "inflated_variance"}, {"score": 0.0030712749385664686, "phrase": "different_probability_law"}, {"score": 0.0029467742734417255, "phrase": "basic_means"}, {"score": 0.0028127163745564777, "phrase": "unsupervised_learning"}, {"score": 0.00257586859792573, "phrase": "variance_inflation"}, {"score": 0.0024842242921857705, "phrase": "support_vector_machines"}, {"score": 0.0023834640793589435, "phrase": "non-parametric_scheme"}, {"score": 0.0023467391904246834, "phrase": "proper_generalizability"}, {"score": 0.0021940118003312397, "phrase": "wide_range"}, {"score": 0.0021714122348552747, "phrase": "benchmark_data_sets"}, {"score": 0.002105007807034302, "phrase": "elsevier"}], "paper_keywords": ["Variance inflation", " SVM", " Generalization"], "paper_abstract": "Many important machine learning models, supervised and unsupervised, are based on simple Euclidean distance or orthogonal projection in a high dimensional feature space. When estimating such models from small training sets we face the problem that the span of the training data set input vectors is not the full input space. Hence, when applying the model to future data the model is effectively blind to the missed orthogonal subspace. This can lead to an inflated variance of hidden variables estimated in the training set and when the model is applied to test data we may find that the hidden variables follow a different probability law with less variance. While the problem and basic means to reconstruct and deflate are well understood in unsupervised learning, the case of supervised learning is less well understood. We here investigate the effect of variance inflation in supervised learning including the case of Support Vector Machines (SVMS) and we propose a non-parametric scheme to restore proper generalizability. We illustrate the algorithm and its ability to restore performance on a wide range of benchmark data sets. (C) 2013 Elsevier B. V. All rights reserved.", "paper_title": "Variance inflation in high dimensional Support Vector Machines", "paper_id": "WOS:000333104500017"}