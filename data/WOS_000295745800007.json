{"auto_keywords": [{"score": 0.030068903531129926, "phrase": "emotional_prosody"}, {"score": 0.004734853555630616, "phrase": "eye_movements"}, {"score": 0.004636594774238505, "phrase": "cross-modal_effects"}, {"score": 0.004597861947580946, "phrase": "emotional_voice_tone"}, {"score": 0.004502433187097914, "phrase": "face_processing"}, {"score": 0.00444612502361583, "phrase": "visual_search"}, {"score": 0.004317450843594688, "phrase": "emotional_prosodic_cues"}, {"score": 0.004122699822340296, "phrase": "emotionally-related_face"}, {"score": 0.003986581218661184, "phrase": "semantic_information"}, {"score": 0.0038549394272661356, "phrase": "six_emotional_faces"}, {"score": 0.003727628340195049, "phrase": "emotionally_congruent_or_incongruent_prosody"}, {"score": 0.003604506544515223, "phrase": "happy_face"}, {"score": 0.0035296217979800463, "phrase": "happy_or_angry_voice"}, {"score": 0.0034130176595901104, "phrase": "eye_fixations"}, {"score": 0.0031778255332904487, "phrase": "emotional_semantic_information"}, {"score": 0.0029463874016490976, "phrase": "pre-emotional_label_window"}, {"score": 0.0028609920171902186, "phrase": "immediate_use"}, {"score": 0.002778064757980489, "phrase": "significantly_longer_frequent_fixations"}, {"score": 0.0027203027216256013, "phrase": "incongruent_faces"}, {"score": 0.0026637384761565605, "phrase": "explicit_semantic_information"}, {"score": 0.002438699976481979, "phrase": "eye_gaze"}, {"score": 0.0023284972218506157, "phrase": "rapid_impact"}, {"score": 0.002309000874224222, "phrase": "gaze_behavior"}, {"score": 0.002289667393538803, "phrase": "social_information_processing"}, {"score": 0.0022514836269627186, "phrase": "prosodic_meanings"}, {"score": 0.002204646157481869, "phrase": "semantic_cues"}, {"score": 0.0021861845339709533, "phrase": "linguistic_information"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Eye-tracking", " Gaze", " Speech processing", " Affective prosody", " Semantics"], "paper_abstract": "This study investigated cross-modal effects of emotional voice tone (prosody) on face processing during instructed visual search. Specifically, we evaluated whether emotional prosodic cues in speech have a rapid, mandatory influence on eye movements to an emotionally-related face, and whether these effects persist as semantic information unfolds. Participants viewed an array of six emotional faces while listening to instructions spoken in an emotionally congruent or incongruent prosody (e.g., \"Click on the happy face\" spoken in a happy or angry voice). The duration and frequency of eye fixations were analyzed when only prosodic cues were emotionally meaningful (pre-emotional label window: \"Click on the/ ... \"), and after emotional semantic information was available (post-emotional label window: \" ... /happy face\"). In the pre-emotional label window, results showed that participants made immediate use of emotional prosody, as reflected in significantly longer frequent fixations to emotionally congruent versus incongruent faces. However, when explicit semantic information in the instructions became available (post-emotional label window), the influence of prosody on measures of eye gaze was relatively minimal. Our data show that emotional prosody has a rapid impact on gaze behavior during social information processing, but that prosodic meanings can be overridden by semantic cues when linguistic information is task relevant. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "How emotional prosody guides your way: Evidence from eye movements", "paper_id": "WOS:000295745800007"}