{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "extreme_learning_machines"}, {"score": 0.004569905085027681, "phrase": "efficient_system"}, {"score": 0.00452670409375785, "phrase": "large-scale_action_recognition"}, {"score": 0.004077508669922702, "phrase": "fisher_vector_encoding"}, {"score": 0.003925414526537783, "phrase": "extreme_learning_machine_classifiers"}, {"score": 0.0038698476651328898, "phrase": "resulting_system"}, {"score": 0.003815064377019477, "phrase": "fast_and_accurate_alternative"}, {"score": 0.003603549914861701, "phrase": "support_vector_machines"}, {"score": 0.0035022161192936234, "phrase": "mid-level_features"}, {"score": 0.003261133560558666, "phrase": "color_distributions"}, {"score": 0.003094920033010818, "phrase": "comparative_manner"}, {"score": 0.002895535087931938, "phrase": "challenge_dataset"}, {"score": 0.002868116925014397, "phrase": "temporally_untrimmed_videos"}, {"score": 0.0027610089418453614, "phrase": "average_precision"}, {"score": 0.0027218799769229596, "phrase": "challenge_protocol"}, {"score": 0.0025343769150179764, "phrase": "third_rank"}, {"score": 0.0025103699913760057, "phrase": "eleven_participants"}, {"score": 0.002371018116266096, "phrase": "high_accuracy"}, {"score": 0.0023152577218214804, "phrase": "efficient_way"}, {"score": 0.0022608057001451414, "phrase": "extensively_trained_and_computationally_heavy_deep_neural_networks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Action recognition", " Extreme learning machine", " Fisher vector", " Multimedia mining"], "paper_abstract": "In this paper, we propose a novel and efficient system for large-scale action recognition from realistic video clips. Our approach combines several recent advances in this area. We use improved dense trajectory features in combination with Fisher vector encoding, and perform learning and classification with extreme learning machine classifiers. The resulting system is a fast and accurate alternative to more traditional action classification approaches like bag of words and support vector machines. Additionally, we use mid-level features that encode information about presence of humans in the videos, as well as color distributions. We extensively evaluate each step of our pipeline in a comparative manner, and report results on the recently published THUMOS 2014 benchmark, which was introduced as a challenge dataset with temporally untrimmed videos and 101 action classes. We achieve 63.37% mean average precision using the challenge protocol (i.e. sequestered test labels and limited system submissions), and got the third rank among eleven participants. The results show that it is possible to obtain a high accuracy with extreme learning machines in an efficient way, without using the extensively trained and computationally heavy deep neural networks that the top performing systems of the challenge incorporated. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Efficient large-scale action recognition in videos using extreme learning machines", "paper_id": "WOS:000360772500083"}