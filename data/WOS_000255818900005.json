{"auto_keywords": [{"score": 0.03156763959900261, "phrase": "bayes"}, {"score": 0.0046739596755881185, "phrase": "matrix-based_representation"}, {"score": 0.004352123742308589, "phrase": "matrix-based_linear_discriminant_analysis"}, {"score": 0.004064427818174677, "phrase": "comprehensive_comparison"}, {"score": 0.0038755411593867284, "phrase": "heteroscedastic_problem"}, {"score": 0.003818326867123525, "phrase": "mathematical_equalities"}, {"score": 0.0036408354779365643, "phrase": "potential_problems"}, {"score": 0.0034407121288076783, "phrase": "different_local_geometric_structures"}, {"score": 0.0033003027425118005, "phrase": "discriminant_feature_extraction"}, {"score": 0.0031939996970411027, "phrase": "new_finding"}, {"score": 0.003118838635447237, "phrase": "higher_fisher_score"}, {"score": 0.0030727610199158813, "phrase": "extreme_case"}, {"score": 0.0030363880573817483, "phrase": "sufficient_conditions"}, {"score": 0.0029561107046138136, "phrase": "two-class_classification_problem"}, {"score": 0.002616364603666959, "phrase": "theoretical_analysis"}, {"score": 0.002600826330254548, "phrase": "comprehensive_experimental_results"}, {"score": 0.00248718591918233, "phrase": "existing_view"}, {"score": 0.0023927122637929813, "phrase": "training_samples"}, {"score": 0.002315574871863272, "phrase": "discriminant_features"}, {"score": 0.002142970278347126, "phrase": "challenging_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Fisher's linear discriminant analysis (LDA)", " matrix-based representation", " vector-based representation", " pattern recognition"], "paper_abstract": "Recent advances have shown that algorithms with (21)) matrix-based representation perform better than the traditional (I D) vector-based ones. In particular, 2D-LDA has been widely reported to outperform 1D-LDA. However, would the matrix-based linear discriminant analysis be always superior and when would 1D-LDA be better! In this paper, we investigate into these questions and have a comprehensive comparison between 1D-LDA and 2D-LDA in theory and in experiments. We analyze the heteroscedastic problem in 2D-LDA and formulate mathematical equalities to explore the relationship between 1D-LDA and 2D-LDA; then we point out potential problems in 2D-LDA. It is shown that 2D-LDA has eliminated the information contained in the covariance, information between different local geometric structures, such as the rows or the columns, which is useful for discriminant feature extraction, whereas 1D-LDA could preserve such information. Interestingly, this new finding indicates that 1D-LDA is able to gain higher Fisher score than 2D-LDA in some extreme case. Furthermore, sufficient conditions on which 2D-LDA would be Bayes optimal for two-class classification problem are derived and comparison with 1D-LDA in this aspect is also analyzed. This could help understand how 2D-LDA is expected to achieve at its best, further discover its relationship with 1D-LDA, and well support other findings. After the theoretical analysis, comprehensive experimental results are reported by fairly and extensively comparing 1D-LDA with 2D-LDA. In contrast to the existing view that some 2D-LDA based algorithms would perform better than 1D-LDA when the number of training samples for each class is small or when the number of discriminant features used is small, we show that it is not always true and show that some standard 1D-LDA based algorithms could perform better in those cases on some challenging data sets. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "1D-LDA vs. 2D-LDA: When is vector-based linear discriminant analysis better than matrix-based?", "paper_id": "WOS:000255818900005"}