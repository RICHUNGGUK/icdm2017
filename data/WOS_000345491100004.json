{"auto_keywords": [{"score": 0.04267297110591664, "phrase": "relevant_features"}, {"score": 0.015583657937346082, "phrase": "kernel_principal_component_analysis"}, {"score": 0.015315037926094453, "phrase": "kpca"}, {"score": 0.00481495049065317, "phrase": "noisy_variation_patterns"}, {"score": 0.004468304427278713, "phrase": "nonlinear_variation_patterns"}, {"score": 0.004429191773525288, "phrase": "inverse_mapping"}, {"score": 0.00431388720289049, "phrase": "high-dimensional_feature_space"}, {"score": 0.004238682850598745, "phrase": "original_input_space"}, {"score": 0.004201571673071299, "phrase": "variation_patterns"}, {"score": 0.0041102032028337366, "phrase": "small_number"}, {"score": 0.004003169429008801, "phrase": "overall_set"}, {"score": 0.003492840830999027, "phrase": "feature_selection_procedure"}, {"score": 0.0034018288705284427, "phrase": "importance_estimates"}, {"score": 0.0033131804981786747, "phrase": "noisy_training_data"}, {"score": 0.0031845067058084583, "phrase": "sparse_random_vectors"}, {"score": 0.0031289290660162145, "phrase": "kernel_matrix"}, {"score": 0.002693919110230191, "phrase": "preimages'_differences"}, {"score": 0.0024559368099486647, "phrase": "suitable_kpca_algorithm"}, {"score": 0.0023294867572824147, "phrase": "significant_speedup"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Nonlinear PCA", " Kernel feature space", " Preimages", " Variation patterns", " Feature ensembles"], "paper_abstract": "Kernel Principal Component Analysis (KPCA) is a technique widely used to understand and visualize nonlinear variation patterns by inverse mapping the projected data from a high-dimensional feature space back to the original input space. Variation patterns often occur in a small number of relevant features out of the overall set of features that are recorded in the data. It is, therefore, crucial to discern this set of relevant features that define the pattern. Here we propose a feature selection procedure that augments KPCA to obtain importance estimates of the features given the noisy training data. Our feature selection strategy involves projecting the data points onto sparse random vectors for calculating the kernel matrix. We then match pairs of such projections, and determine the preimages of the data with and without a feature, thereby trying to identify the importance of that feature. Thus, preimages' differences within pairs are used to identify the relevant features. An advantage of our method is it can be used with any suitable KPCA algorithm. Moreover, the computations can be parallelized easily leading to significant speedup. We demonstrate our method on several simulated and real data sets, and compare the results to alternative approaches in the literature. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Feature selection for noisy variation patterns using kernel principal component analysis", "paper_id": "WOS:000345491100004"}