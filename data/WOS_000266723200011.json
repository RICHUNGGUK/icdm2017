{"auto_keywords": [{"score": 0.03769021112631509, "phrase": "neural_network"}, {"score": 0.013801761008619217, "phrase": "proposed_neural_network"}, {"score": 0.013484416273430763, "phrase": "differential_inclusion"}, {"score": 0.012633645981778557, "phrase": "objective_function"}, {"score": 0.011191893918617891, "phrase": "feasible_region"}, {"score": 0.01108797891737729, "phrase": "finite_time"}, {"score": 0.00481495049065317, "phrase": "nonsmooth_nonconvex_optimization_problems"}, {"score": 0.004681226806682058, "phrase": "subgradient-based_neural_network"}, {"score": 0.004594137808658537, "phrase": "nonsmooth_nonconvex_optimization_problem"}, {"score": 0.004529881123652277, "phrase": "nonsmooth_nonconvex_objective_function"}, {"score": 0.004424768776701965, "phrase": "affine_equality_constraints"}, {"score": 0.004281678802569817, "phrase": "convex_inequality_constraints"}, {"score": 0.004047020753789236, "phrase": "suitable_assumption"}, {"score": 0.003916098498242888, "phrase": "proper_assumption"}, {"score": 0.0037188360359702182, "phrase": "sufficiently_large_penalty_parameter"}, {"score": 0.0036324746826127997, "phrase": "unique_global_solution"}, {"score": 0.003038237581227068, "phrase": "equilibrium_points"}, {"score": 0.0028446460307503343, "phrase": "critical_points"}, {"score": 0.002613709503583786, "phrase": "equilibrium_point"}, {"score": 0.0025171669791790438, "phrase": "suitable_assumptions"}, {"score": 0.0023789822182914877, "phrase": "\"slow_solution"}, {"score": 0.0021755038991725147, "phrase": "theoretic_results"}, {"score": 0.0021049977753042253, "phrase": "good_performance"}], "paper_keywords": ["Convergence in finite time", " differential inclusion", " neural network", " nonsmooth nonconvex optimization", " slow solution"], "paper_abstract": "This paper presents a subgradient-based neural network to solve a nonsmooth nonconvex optimization problem with a nonsmooth nonconvex objective function, a class of affine equality constraints, and a class of nonsmooth convex inequality constraints. The proposed neural network is modeled with a differential inclusion. Under a suitable assumption on the constraint set and a proper assumption on the objective function, it is proved that for a sufficiently large penalty parameter, there exists a unique global solution to the neural network and the trajectory of the network can reach the feasible region in finite time and stay there thereafter. It is proved that the trajectory of the neural network converges to the set which consists of the equilibrium points of the neural network, and coincides with the set which consists of the critical points of the objective function in the feasible region. A condition is given to ensure the convergence to the equilibrium point set in finite time. Moreover, under suitable assumptions, the coincidence between the solution to the differential inclusion and the \"slow solution\" of it is also proved. Furthermore, three typical examples are given to present the effectiveness of the theoretic results obtained in this paper and the good performance of the proposed neural network.", "paper_title": "Subgradient-Based Neural Networks for Nonsmooth Nonconvex Optimization Problems", "paper_id": "WOS:000266723200011"}