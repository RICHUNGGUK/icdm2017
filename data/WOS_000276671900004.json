{"auto_keywords": [{"score": 0.02520539739504684, "phrase": "lda"}, {"score": 0.00481495049065317, "phrase": "time-stamped_documents"}, {"score": 0.004358368398332723, "phrase": "known_publication_dates"}, {"score": 0.0038761135052054765, "phrase": "mixture_model"}, {"score": 0.0038085117064624208, "phrase": "proposed_model"}, {"score": 0.003676811616723809, "phrase": "topic_mixture_weights"}, {"score": 0.003447035529485425, "phrase": "general_framework"}, {"score": 0.0033868916699805224, "phrase": "different_forms"}, {"score": 0.00304737045343781, "phrase": "base_measures"}, {"score": 0.002994178996584381, "phrase": "independent_multinomial-dirichlet_measures"}, {"score": 0.002924694271730544, "phrase": "topic-dependent_word_counts"}, {"score": 0.0028234703730349894, "phrase": "hierarchical_model"}, {"score": 0.0027905114793984084, "phrase": "efficient_variational_bayesian_inference"}, {"score": 0.00270978310478061, "phrase": "large-scale_problems"}, {"score": 0.0025106241866273897, "phrase": "dynamic_character"}, {"score": 0.0023953929817357882, "phrase": "latent_dirichlet_allocation"}, {"score": 0.0021677478798910565, "phrase": "neural_information_processing_systems_papers"}], "paper_keywords": ["Hierarchical models", " variational Bayes", " Dirichlet process", " text modeling"], "paper_abstract": "We consider the problem of inferring and modeling topics in a sequence of documents with known publication dates. The documents at a given time are each characterized by a topic and the topics are drawn from a mixture model. The proposed model infers the change in the topic mixture weights as a function of time. The details of this general framework may take different forms, depending on the specifics of the model. For the examples considered here, we examine base measures based on independent multinomial-Dirichlet measures for representation of topic-dependent word counts. The form of the hierarchical model allows efficient variational Bayesian inference, of interest for large-scale problems. We demonstrate results and make comparisons to the model when the dynamic character is removed, and also compare to latent Dirichlet allocation (LDA) and Topics over Time (TOT). We consider a database of Neural Information Processing Systems papers as well as the US Presidential State of the Union addresses from 1790 to 2008.", "paper_title": "Hierarchical Bayesian Modeling of Topics in Time-Stamped Documents", "paper_id": "WOS:000276671900004"}