{"auto_keywords": [{"score": 0.031409938068082406, "phrase": "proposed_approach"}, {"score": 0.00481495049065317, "phrase": "imbalanced_data_classification"}, {"score": 0.004766683200410046, "phrase": "equal_or_unequal_misclassification_costs"}, {"score": 0.004718897469501381, "phrase": "support_vector_machines"}, {"score": 0.0045324756541157574, "phrase": "popular_classifier_algorithms"}, {"score": 0.004419703921314433, "phrase": "two-class_classification_problems"}, {"score": 0.004139401327448807, "phrase": "data_imbalance"}, {"score": 0.003975781274672803, "phrase": "target_class"}, {"score": 0.003780286517897356, "phrase": "near-bayesian_support_vector_machine"}, {"score": 0.003558294449586273, "phrase": "decision_boundary_shift"}, {"score": 0.003522579870903474, "phrase": "unequal_regularization_costs"}, {"score": 0.0030895394261534776, "phrase": "boundary_shift"}, {"score": 0.003012560489169148, "phrase": "asymmetric_regularization_costs"}, {"score": 0.0028933521924166287, "phrase": "multi-class_scenario"}, {"score": 0.0027929100418926725, "phrase": "unequal_misclassification_costs"}, {"score": 0.002750934994181092, "phrase": "different_classes"}, {"score": 0.002723301730223192, "phrase": "extensive_comparison"}, {"score": 0.0026959452932289797, "phrase": "standard_svm"}, {"score": 0.0023762535034673017, "phrase": "imbalanced_datasets"}, {"score": 0.0023287359930308864, "phrase": "sequential_minimal_optimization"}, {"score": 0.002202894673025544, "phrase": "nbsvm_optimization_problem"}, {"score": 0.0021697677572213086, "phrase": "computationally_efficient_manner"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Support Vector Machines", " Bayes error", " Imbalanced data", " Decision boundary shift", " Unequal costs", " Multi-class classification"], "paper_abstract": "Support Vector Machines (SVMs) form a family of popular classifier algorithms originally developed to solve two-class classification problems. However, SVMs are likely to perform poorly in situations with data imbalance between the classes, particularly when the target class is under-represented. This paper proposes a Near-Bayesian Support Vector Machine (NBSVM) for such imbalanced classification problems, by combining the philosophies of decision boundary shift and unequal regularization costs. Based on certain assumptions which hold true for most real-world datasets, we use the fractions of representation from each of the classes, to achieve the boundary shift as well as the asymmetric regularization costs. The proposed approach is extended to the multi-class scenario and also adapted for cases with unequal misclassification costs for the different classes. Extensive comparison with standard SVM and some state-of-the-art methods is furnished as a proof of the ability of the proposed approach to perform competitively on imbalanced datasets. A modified Sequential Minimal Optimization (SMO) algorithm is also presented to solve the NBSVM optimization problem in a computationally efficient manner. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Near-Bayesian Support Vector Machines for imbalanced data classification with equal or unequal misclassification costs", "paper_id": "WOS:000361582200005"}