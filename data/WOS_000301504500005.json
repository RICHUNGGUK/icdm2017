{"auto_keywords": [{"score": 0.04937491786272553, "phrase": "continuous_environments"}, {"score": 0.046985412940270525, "phrase": "high-level_states"}, {"score": 0.036340076548093875, "phrase": "qualitative_representation"}, {"score": 0.00481495049065317, "phrase": "autonomous_learning_of_high-level_states"}, {"score": 0.004571559854487074, "phrase": "low-level_representation"}, {"score": 0.004276541976450121, "phrase": "learning_agent"}, {"score": 0.004198010835867604, "phrase": "continuous_variables"}, {"score": 0.0038122196479548425, "phrase": "autonomous_learning"}, {"score": 0.0036734281414482735, "phrase": "discrete_environments"}, {"score": 0.0033730181385496186, "phrase": "continuous_and_discrete_variable_representations"}, {"score": 0.0032381271420557052, "phrase": "broad_discretization"}, {"score": 0.0028436862612859896, "phrase": "predictive_models"}, {"score": 0.0026401983633087267, "phrase": "learned_actions"}, {"score": 0.0025251455169484557, "phrase": "simulated_robot"}, {"score": 0.0025064622665903645, "phrase": "realistic_physics"}, {"score": 0.0021049977753042253, "phrase": "necessary_states"}], "paper_keywords": ["Active learning", " intrinsic motivation", " qualitative reasoning", " reinforcement learning", " unsupervised learning"], "paper_abstract": "How can an agent bootstrap up from a low-level representation to autonomously learn high-level states and actions using only domain-general knowledge? In this paper, we assume that the learning agent has a set of continuous variables describing the environment. There exist methods for learning models of the environment, and there also exist methods for planning. However, for autonomous learning, these methods have been used almost exclusively in discrete environments. We propose attacking the problem of learning high-level states and actions in continuous environments by using a qualitative representation to bridge the gap between continuous and discrete variable representations. In this approach, the agent begins with a broad discretization and initially can only tell if the value of each variable is increasing, decreasing, or remaining steady. The agent then simultaneously learns a qualitative representation (discretization) and a set of predictive models of the environment. These models are converted into plans to perform actions. The agent then uses those learned actions to explore the environment. The method is evaluated using a simulated robot with realistic physics. The robot is sitting at a table that contains a block and other distractor objects that are out of reach. The agent autonomously explores the environment without being given a task. After learning, the agent is given various tasks to determine if it learned the necessary states and actions to complete them. The results show that the agent was able to use this method to autonomously learn to perform the tasks.", "paper_title": "Autonomous Learning of High-Level States and Actions in Continuous Environments", "paper_id": "WOS:000301504500005"}