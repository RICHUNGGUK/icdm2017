{"auto_keywords": [{"score": 0.040537135041717604, "phrase": "mpgr"}, {"score": 0.011870302616230462, "phrase": "sparse_gaussian_processes"}, {"score": 0.010720338163727545, "phrase": "gaussian_processes"}, {"score": 0.0077427829423972186, "phrase": "gaussian"}, {"score": 0.0076430132266428205, "phrase": "manifold-preserving_graph_reduction"}, {"score": 0.004634799977448855, "phrase": "popular_and_effective_bayesian_method"}, {"score": 0.004461359547152617, "phrase": "cube_time_complexity"}, {"score": 0.004261904657465999, "phrase": "mpgro"}, {"score": 0.004258128672571261, "phrase": "time_consumption"}, {"score": 0.004081385289516914, "phrase": "big_data_sets"}, {"score": 0.004033376370306929, "phrase": "ivm"}, {"score": 0.003995772867224968, "phrase": "manifold_assumption"}, {"score": 0.003797530813314321, "phrase": "simple_and_efficient_graph_sparsification_method"}, {"score": 0.0037336520904376687, "phrase": "sparse_semi-supervised_learning"}, {"score": 0.0035634522350563107, "phrase": "noisy_points"}, {"score": 0.0034738975998176323, "phrase": "low_time_consumption"}, {"score": 0.0033295966773779174, "phrase": "sparse_supervised_learning"}, {"score": 0.0032184697893776052, "phrase": "sparse_representation"}, {"score": 0.003150900886669123, "phrase": "fatal_shortcoming"}, {"score": 0.0027390682208834013, "phrase": "mpgr_framework"}, {"score": 0.002526780881314497, "phrase": "informative_vector_machine"}, {"score": 0.0024736973328071026, "phrase": "mpgro._experimental_results"}, {"score": 0.0024527767211457046, "phrase": "seven_real_data_sets"}, {"score": 0.002205692205167157, "phrase": "error_rates"}, {"score": 0.0021593396036472777, "phrase": "full_training_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sparse representation", " Gaussian process", " Manifold-preserving graph reduction (MPGR)", " Informative vector machine (IVM)"], "paper_abstract": "Gaussian processes are a popular and effective Bayesian method for classification and regression. Due to the cube time complexity with respect to the size of the training set, time consumption will be too high for Gaussian processes to deal with big data sets. Based on the manifold assumption, manifold-preserving graph reduction (MPGR) is a simple and efficient graph sparsification method originally proposed for sparse semi-supervised learning. High representative points are selected, while outliers and noisy points are excluded by MPGR with low time consumption. In this paper, we apply MPGR to sparse supervised learning, and utilize MPGR to get a sparse representation of Gaussian processes. A fatal shortcoming of MPGR is that it does not effectively consider the influence of outputs to Gaussian processes. We proceed to exploit outputs for sparse Gaussian processes by embedding a method that considers outputs in the MPGR framework, and propose manifold-preserving graph reduction with outputs (MPGRO) for sparse Gaussian processes. In this paper, we utilize informative vector machine (IVM) for MPGRO. Experimental results on seven real data sets show that MPGR is better than IVM for sparse Gaussian processes, and MPGRO is better than MPGR and IVM for sparse Gaussian processes and is comparable on error rates to the method utilizing full training sets. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Sparse Gaussian processes with manifold-preserving graph reduction", "paper_id": "WOS:000337261700011"}