{"auto_keywords": [{"score": 0.041539323347481553, "phrase": "bayeslsh"}, {"score": 0.01541946122820572, "phrase": "kernel_methods"}, {"score": 0.013633929165518973, "phrase": "lsh"}, {"score": 0.010272314392820364, "phrase": "similarity_estimation"}, {"score": 0.0085844106925867, "phrase": "kernel_functions"}, {"score": 0.00481495049065317, "phrase": "bayesian_perspective"}, {"score": 0.0047469083685836985, "phrase": "extensions"}, {"score": 0.004619530769352269, "phrase": "associated_similarity_measure"}, {"score": 0.004585873765376289, "phrase": "all-pairs_similarity_search_problem"}, {"score": 0.004139524432379627, "phrase": "first_phase"}, {"score": 0.004119383876155527, "phrase": "similarity_search"}, {"score": 0.004039792863335641, "phrase": "candidate_generation"}, {"score": 0.003923805234691883, "phrase": "bayesian"}, {"score": 0.003885176600463076, "phrase": "subsequent_phase"}, {"score": 0.003866268750750096, "phrase": "similarity_search-performing_candidate_pruning"}, {"score": 0.003819400038772774, "phrase": "lsh."}, {"score": 0.0037730959366509558, "phrase": "bayeslsh-lite"}, {"score": 0.003602185827766227, "phrase": "large_majority"}, {"score": 0.0035759143731219994, "phrase": "false_positive_candidate_pairs"}, {"score": 0.003541182515693916, "phrase": "significant_speedups"}, {"score": 0.0035239428472087137, "phrase": "baseline_approaches"}, {"score": 0.003455816485737094, "phrase": "probabilistic_guarantees"}, {"score": 0.003283165201986546, "phrase": "bayeslsh's_output"}, {"score": 0.0032039991043498507, "phrase": "manual_setting"}, {"score": 0.0031115076263594607, "phrase": "standard_approaches"}, {"score": 0.0029560068587160203, "phrase": "wide_variety"}, {"score": 0.0028988281367834444, "phrase": "bayeslsh_algorithm"}, {"score": 0.0028014064098604093, "phrase": "kernel_function"}, {"score": 0.00276066021951467, "phrase": "data_points"}, {"score": 0.0027405092973494293, "phrase": "transformed_kernel_space"}, {"score": 0.002694059293047076, "phrase": "allpairs"}, {"score": 0.0026613618933208467, "phrase": "inverted_index_structure"}, {"score": 0.002648394103700668, "phrase": "fast_similarity_search"}, {"score": 0.0026035011274172753, "phrase": "exhaustive_search"}, {"score": 0.002584494385587982, "phrase": "possible_pairs"}, {"score": 0.0024915167458844914, "phrase": "kernel_values"}, {"score": 0.0023669270875629226, "phrase": "k-bayeslsh"}, {"score": 0.0023496433751182162, "phrase": "recently_proposed_idea-kernelized_locality"}, {"score": 0.002265095354846931, "phrase": "aforementioned_bayeslsh_idea"}, {"score": 0.0022540540553063902, "phrase": "candidate_pruning"}, {"score": 0.0022158306933307685, "phrase": "broad_spectrum"}, {"score": 0.0021623451173388352, "phrase": "different_domains"}, {"score": 0.0021465520836247225, "phrase": "distinct_kernels"}, {"score": 0.0021049977753042253, "phrase": "vanilla_klsh."}], "paper_keywords": ["Locality-sensitive hashing", " all-pairs similarity search", " bayesian inference", " kernel similarity measure"], "paper_abstract": "Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. In order to reduce the number of candidates to search, locality-sensitive hashing (LSH) based indexing methods are very effective. However, most such methods only use LSH for the first phase of similarity search-that is, efficient indexing for candidate generation. In this article, we present BayesLSH, a principled Bayesian algorithm for the subsequent phase of similarity search-performing candidate pruning and similarity estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates similarities exactly, is also presented. Our algorithms are able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and recall. Finally, the quality of BayesLSH's output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2x-20x for a wide variety of datasets. We also extend the BayesLSH algorithm for kernel methods-in which the similarity between two data objects is defined by a kernel function. Since the embedding of data points in the transformed kernel space is unknown, algorithms such as AllPairs which rely on building inverted index structure for fast similarity search do not work with kernel functions. Exhaustive search across all possible pairs is also not an option since the dataset can be huge and computing the kernel values for each pair can be prohibitive. We propose K-BayesLSH an all-pairs similarity search problem for kernel functions. K-BayesLSH leverages a recently proposed idea-kernelized locality sensitive hashing (KLSH)-for hash bit computation and candidate generation, and uses the aforementioned BayesLSH idea for candidate pruning and similarity estimation. We ran a broad spectrum of experiments on a variety of datasets drawn from different domains and with distinct kernels and find a speedup of 2x-7x over vanilla KLSH.", "paper_title": "A Bayesian Perspective on Locality Sensitive Hashing with Extensions for Kernel Methods", "paper_id": "WOS:000363675500009"}