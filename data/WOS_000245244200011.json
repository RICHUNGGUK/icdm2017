{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "em_algorithm"}, {"score": 0.038749692047715355, "phrase": "log-likelihood_function"}, {"score": 0.004695900553353526, "phrase": "gradient_approach"}, {"score": 0.004618166136069052, "phrase": "slow_convergence"}, {"score": 0.0043560464825349275, "phrase": "linear_state-space_models"}, {"score": 0.0039737570920809215, "phrase": "off-the-shelf_quasi-newton-type_optimizer"}, {"score": 0.003535165233011893, "phrase": "practical_alternative"}, {"score": 0.003334309053314909, "phrase": "exact_gradient"}, {"score": 0.002728166176898539, "phrase": "proposed_method"}, {"score": 0.0026164433143193015, "phrase": "linear_state-space_model"}, {"score": 0.002551612225562499, "phrase": "high_signal-to-noise_ratios"}, {"score": 0.0024886135327401343, "phrase": "em"}, {"score": 0.0021405207258001118, "phrase": "sizable_reduction"}, {"score": 0.0021049977753042253, "phrase": "computation_time"}], "paper_keywords": [""], "paper_abstract": "Slow convergence is observed in the EM algorithm for linear state-space models. We propose to circumvent the problem by applying any off-the-shelf quasi-Newton-type optimizer, which operates on the gradient of the log-likelihood function. Such an algorithm is a practical alternative due to the fact that the exact gradient of the log-likelihood function can be computed by recycling components of the expectation-maximization (EM) algorithm. We demonstrate the efficiency of the proposed method in three relevant instances of the linear state-space model. In high signal-to-noise ratios, where EM is particularly prone to converge slowly, we show that gradient-based learning results in a sizable reduction of computation time.", "paper_title": "State-space models: From the EM algorithm to a gradient approach", "paper_id": "WOS:000245244200011"}