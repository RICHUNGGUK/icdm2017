{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "feature_selection"}, {"score": 0.013955716026719444, "phrase": "projection_matrix"}, {"score": 0.01317369503024093, "phrase": "nearest_missing"}, {"score": 0.008455349681398536, "phrase": "proposed_algorithm"}, {"score": 0.007826279022659251, "phrase": "llfs"}, {"score": 0.004777744121315302, "phrase": "recent_research"}, {"score": 0.004667833113864304, "phrase": "large_margin_framework"}, {"score": 0.00447282529270722, "phrase": "novel_feature_selection_algorithm"}, {"score": 0.00440394242852293, "phrase": "large_margin_subspace_learning"}, {"score": 0.003919900973086169, "phrase": "different_label"}, {"score": 0.0035989687070420977, "phrase": "nearest_neighbor"}, {"score": 0.0032031180198375283, "phrase": "kernel_density_estimation"}, {"score": 0.002763415176377921, "phrase": "efficient_algorithm"}, {"score": 0.0026892447164623247, "phrase": "resultant_optimization_problem"}, {"score": 0.002668420182908979, "phrase": "comprehensive_experiments"}, {"score": 0.002440193310844978, "phrase": "spfs"}, {"score": 0.0024025891950669, "phrase": "tr"}, {"score": 0.0023471384232499016, "phrase": "better_performance"}, {"score": 0.002205560249972527, "phrase": "competitive_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feature selection", " l(2,1)-norm regularization", " Large margin maximization", " Subspace learning"], "paper_abstract": "Recent research has shown the benefits of large margin framework for feature selection. In this paper, we propose a novel feature selection algorithm, termed as Large Margin Subspace Learning (LMSL), which seeks a projection matrix to maximize the margin of a given sample, defined as the distance between the nearest missing (the nearest neighbor with the different label) and the nearest hit (the nearest neighbor with the same label) of the given sample. Instead of calculating the nearest neighbor of the given sample directly, we treat each sample with different (same) labels with the given sample as a potential nearest missing (hint), with the probability estimated by kernel density estimation. By this way, the nearest missing (hint) is calculated as an expectation of all different (same) class samples. In order to perform feature selection, an l(2,1)-norm is imposed on the projection matrix to enforce row-sparsity. An efficient algorithm is then proposed to solve the resultant optimization problem. Comprehensive experiments are conducted to compare the performance of the proposed algorithm with the other five state-of-the-art algorithms RFS, SPFS, mRMR, TR and LLFS, it achieves better performance than the former four. Compared with the algorithm LLFS, the proposed algorithm has a competitive performance with however a significantly faster computational. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Large Margin Subspace Learning for feature selection", "paper_id": "WOS:000320477400015"}