{"auto_keywords": [{"score": 0.04599283119366167, "phrase": "cbr"}, {"score": 0.020705573241930564, "phrase": "logit"}, {"score": 0.010558824054916607, "phrase": "optimal_feature_subsets"}, {"score": 0.007152348819411161, "phrase": "case-based_reasoning"}, {"score": 0.007149559514678032, "phrase": "anova"}, {"score": 0.007080160108538092, "phrase": "business_failure_prediction"}, {"score": 0.006413172606544605, "phrase": "mda"}, {"score": 0.004338290738750213, "phrase": "multiple_discriminant_analysis"}, {"score": 0.004123125711680019, "phrase": "k-nearest_neighbor"}, {"score": 0.003978866233891427, "phrase": "machine_learning_techniques"}, {"score": 0.003800753174956217, "phrase": "so-called_optimal_feature_subsets"}, {"score": 0.0037814607202870626, "phrase": "bfp"}, {"score": 0.0037527024812648653, "phrase": "feature_subset"}, {"score": 0.0037241630556876094, "phrase": "important_factor"}, {"score": 0.0036864465771943933, "phrase": "cbr's_performance"}, {"score": 0.0035846771553815536, "phrase": "retrieval_process"}, {"score": 0.0034945909783585753, "phrase": "classical_euclidean_metric_technique"}, {"score": 0.003468007666078983, "phrase": "case_similarity"}, {"score": 0.003450397507627697, "phrase": "empirical_data"}, {"score": 0.0033808435504493767, "phrase": "shanghai_stock_exchange"}, {"score": 0.00336367450773776, "phrase": "shenzhen_stock_exchange"}, {"score": 0.0033466264543913988, "phrase": "china"}, {"score": 0.00331268701820534, "phrase": "i.e._mda_stepwise_method"}, {"score": 0.0031242709509664806, "phrase": "genetic_algorithm"}, {"score": 0.0030847461428168614, "phrase": "five_optimal_feature_subsets"}, {"score": 0.0030457198309616694, "phrase": "thirty-times_hold-out_method"}, {"score": 0.0029995374790735526, "phrase": "predictive_performances"}, {"score": 0.002961585972358972, "phrase": "one-out_cross-validation"}, {"score": 0.002814507985446349, "phrase": "support_vector_machine"}, {"score": 0.002785978280998868, "phrase": "comparative_models"}, {"score": 0.0027718217591792644, "phrase": "empirical_results"}, {"score": 0.00268837647967599, "phrase": "medium-term_bfp."}, {"score": 0.002600802496267485, "phrase": "first_choice"}, {"score": 0.0025353790642826, "phrase": "stepwise_method"}, {"score": 0.002440309168744176, "phrase": "fourth_choice"}, {"score": 0.002421726275597666, "phrase": "mda_stepwise_method"}, {"score": 0.002391067964886602, "phrase": "optimal_feature_subset"}, {"score": 0.00237285915034425, "phrase": "cbr_system"}, {"score": 0.002342818029541256, "phrase": "significant_difference"}, {"score": 0.0023309080951251335, "phrase": "predictive_performance"}, {"score": 0.002319058565235998, "phrase": "medium-term_bfp"}, {"score": 0.002175913488493802, "phrase": "significant_level"}, {"score": 0.0021157558534140287, "phrase": "feature_selection_method"}], "paper_keywords": ["Business failure prediction (BFP)", " Case-based reasoning (CBR)", " k-Nearest neighbor", " Feature selection", " Filters", " Wrappers", " Chinese listed company"], "paper_abstract": "Case-based reasoning (CBR) was firstly introduced into the area of business failure prediction (BFP) in 1996. The conclusion drawn out in its first application in this area is that CBR is not more applicable than multiple discriminant analysis (MDA) and Logit. On the contrary, there are some arguments which claim that CBR with k-nearest neighbor (k-NN) as its heart is not surely outranked by those machine learning techniques. In this research, we attempt to investigate whether or not CBR is sensitive to the so-called optimal feature subsets in BFP, since feature subset is an important factor that accounts for CBR's performance. When CBR is used to solve such classification problem, the retrieval process of its life-cycle is mainly used. We use the classical Euclidean metric technique to calculate case similarity. Empirical data two years prior to failure are collected from Shanghai Stock Exchange and Shenzhen Stock Exchange in China. Four filters, i.e. MDA stepwise method, Logit stepwise method, One-way ANOVA, independent-samples t-test, and the wrapper approach of genetic algorithm are employed to generate five optimal feature subsets after data normalization. Thirty-times hold-out method is used as assessment of predictive performances by combining leave-one-out cross-validation and hold-out method. The two statistical baseline models, i.e. MDA and Logit, and the new model of support vector machine are employed as comparative models. Empirical results indicate that CBR is truly sensitive to optimal feature subsets with data for medium-term BFP. The stepwise method of MDA, a filter approach, is the first choice for CBR to select optimal feature subsets, followed by the stepwise method of Logit and the wrapper. The two filter approaches of ANOVA and t-test are the fourth choice. If MDA stepwise method is employed to select optimal feature subset for the CBR system, there are no significant difference on predictive performance of medium-term BFP between CBR and the other three models, i.e. MDA, Logit, SVM. On the contrary, CBR is outperformed by the three models at the significant level of 1%, if ANOVA or t-test is used as feature selection method for CBR. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "On sensitivity of case-based reasoning to optimal feature subsets in business failure prediction", "paper_id": "WOS:000277726300012"}