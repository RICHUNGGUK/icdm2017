{"auto_keywords": [{"score": 0.041241409909057336, "phrase": "computational_models"}, {"score": 0.0344525139183746, "phrase": "inter-voice_contextual_attributes"}, {"score": 0.00481495049065317, "phrase": "machine_learning_approach"}, {"score": 0.004762550320451225, "phrase": "expressive_performance_modelling"}, {"score": 0.00471071771178029, "phrase": "string_quartets"}, {"score": 0.00465944657565638, "phrase": "computational_approaches"}, {"score": 0.004608730887154718, "phrase": "modelling_expressive_music_performance"}, {"score": 0.004459857205489101, "phrase": "music_expression"}, {"score": 0.004199247711875386, "phrase": "ensemble_performance"}, {"score": 0.003997343989408636, "phrase": "novel_method"}, {"score": 0.0038893838564184107, "phrase": "ensemble_expressive_performance"}, {"score": 0.003661987063211563, "phrase": "new_insights"}, {"score": 0.0030899524399043863, "phrase": "multi-modal_recordings"}, {"score": 0.0030562674358159945, "phrase": "string_quartet_performances"}, {"score": 0.0028932554846476718, "phrase": "machine-learning_algorithms"}, {"score": 0.0027691111795848183, "phrase": "timing_deviations"}, {"score": 0.0027389142053945246, "phrase": "vibrato_extent"}, {"score": 0.0026942333853202556, "phrase": "bowing_speed"}, {"score": 0.002388043660169452, "phrase": "expressive_parameters"}, {"score": 0.00221153232497724, "phrase": "ensemble_recordings"}, {"score": 0.0021049977753042253, "phrase": "solo_recordings"}], "paper_keywords": ["music analysis", " expressive performance", " ensemble music", " machine learning", " performance modelling"], "paper_abstract": "Computational approaches for modelling expressive music performance have produced systems that emulate music expression, but few steps have been taken in the domain of ensemble performance. In this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. In order to address the problem of inter-dependence among musicians we propose the introduction of inter-voice contextual attributes. We evaluate the method on data extracted from multi-modal recordings of string quartet performances in two different conditions: solo and ensemble. We used machine-learning algorithms to produce computational models for predicting intensity, timing deviations, vibrato extent, and bowing speed of each note. As a result, the introduced inter-voice contextual attributes generally improved the prediction of the expressive parameters. Furthermore, results on attribute selection show that the models trained on ensemble recordings took more advantage of inter-voice contextual attributes than those trained on solo recordings.", "paper_title": "The Sense of Ensemble: a Machine Learning Approach to Expressive Performance Modelling in String Quartets", "paper_id": "WOS:000342325300006"}