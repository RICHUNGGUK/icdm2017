{"auto_keywords": [{"score": 0.03843856552198327, "phrase": "inter-label_correlations"}, {"score": 0.027104674127658032, "phrase": "random_selection"}, {"score": 0.00481495049065317, "phrase": "multi-label_classification"}, {"score": 0.004655924035639928, "phrase": "effective_tool"}, {"score": 0.0046092435167207095, "phrase": "multi-label_classification_tasks"}, {"score": 0.004382737590042589, "phrase": "small_randomly-selected_subset"}, {"score": 0.004266501496765747, "phrase": "single_label_classifier"}, {"score": 0.004002596930301241, "phrase": "similar_approach"}, {"score": 0.0038314621564116192, "phrase": "minimum_required_subsets"}, {"score": 0.003805788648230075, "phrase": "k_labels"}, {"score": 0.003717273538121102, "phrase": "additional_constraints"}, {"score": 0.003522579870903474, "phrase": "subset_selection"}, {"score": 0.003487222504700868, "phrase": "minimum_set"}, {"score": 0.003440654984167018, "phrase": "scp"}, {"score": 0.003360578019762703, "phrase": "approximation_algorithms"}, {"score": 0.003238517900104576, "phrase": "offline_algorithms"}, {"score": 0.002830757050018908, "phrase": "general_framework"}, {"score": 0.0028023242231187363, "phrase": "label_covers"}, {"score": 0.0027370835853738626, "phrase": "cover_construction_constraints"}, {"score": 0.0025675372701474035, "phrase": "prediction_performance"}, {"score": 0.002524688179363081, "phrase": "better_coverage"}, {"score": 0.002441118140830714, "phrase": "theoretical_bounds"}, {"score": 0.0023365892290481472, "phrase": "proposed_construction_criteria"}, {"score": 0.0023131084326717755, "phrase": "experimental_results"}, {"score": 0.002282166507956957, "phrase": "proposed_methods"}, {"score": 0.0022668507535947976, "phrase": "multi-label_classification_accuracy"}, {"score": 0.0022215160785078797, "phrase": "rakel_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multi-label classification", " Ensemble learning"], "paper_abstract": "Ensemble methods have been shown to be an effective tool for solving multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm, each member of the ensemble is associated with a small randomly-selected subset of k labels. Then, a single label classifier is trained according to each combination of elements in the subset. In this paper we adopt a similar approach, however, instead of randomly choosing subsets, we select the minimum required subsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correlations. Construction of the cover is achieved by formulating the subset selection as a minimum set covering problem (SCP) and solving it by using approximation algorithms. Every cover needs only to be prepared once by offline algorithms. Once prepared, a cover may be applied to the classification of any given multi-label dataset whose properties conform with those of the cover. The contribution of this paper is two-fold. First, we introduce SCP as a general framework for constructing label covers while allowing the user to incorporate cover construction constraints. We demonstrate the effectiveness of this framework by proposing two construction constraints whose enforcement produces covers that improve the prediction performance of random selection by achieving better coverage of labels and inter-label correlations. Second, we provide theoretical bounds that quantify the probabilities of random selection to produce covers that meet the proposed construction criteria. The experimental results indicate that the proposed methods improve multi-label classification accuracy and stability compared to the RAKEL algorithm and to other state-of-the-art algorithms. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Ensemble methods for multi-label classification", "paper_id": "WOS:000340689700046"}