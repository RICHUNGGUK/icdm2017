{"auto_keywords": [{"score": 0.040108824253493075, "phrase": "scene_identification"}, {"score": 0.00481495049065317, "phrase": "video_summarization"}, {"score": 0.004408729661992651, "phrase": "important_frames"}, {"score": 0.003948613642680196, "phrase": "top-down_approach"}, {"score": 0.00373682045166829, "phrase": "scene_summarization"}, {"score": 0.0033465923623290034, "phrase": "global_features"}, {"score": 0.0032021226849872054, "phrase": "scalable_clustering_method"}, {"score": 0.0026543514413663893, "phrase": "local_descriptors"}, {"score": 0.0025963890234079333, "phrase": "minimal_redundancy"}, {"score": 0.0023768935613577985, "phrase": "visual_word-based_approach"}], "paper_keywords": ["Algorithms", " Design", " Experimentation", " Keyframe extraction", " scene identification", " clustering", " keypoint", " local visual word"], "paper_abstract": "While most existing video summarization approaches aim to identify important frames of a video from either a global or local perspective, we propose a top-down approach consisting of scene identification and scene summarization. For scene identification, we represent each frame with global features and utilize a scalable clustering method. We then formulate scene summarization as choosing those frames that best cover a set of local descriptors with minimal redundancy. In addition, we develop a visual word-based approach to make our approach more computationally scalable. Experimental results on two benchmark datasets demonstrate that our proposed approach clearly outperforms the state-of-the-art.", "paper_title": "A Top-Down Approach for Video Summarization", "paper_id": "WOS:000341939800004"}