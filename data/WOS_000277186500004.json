{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "approximate_tree_kernels"}, {"score": 0.01048113336357843, "phrase": "convolution_kernels"}, {"score": 0.004637478829869904, "phrase": "simple_means"}, {"score": 0.004522796947899013, "phrase": "tree-structured_data"}, {"score": 0.004438642216475526, "phrase": "computation_time"}, {"score": 0.004383406949799696, "phrase": "tree_kernels"}, {"score": 0.003819187440994939, "phrase": "large_parse_trees"}, {"score": 0.0037246656582642272, "phrase": "html_documents"}, {"score": 0.0036782824183282823, "phrase": "structured_network_data"}, {"score": 0.0033905067962602515, "phrase": "effective_approximation_technique"}, {"score": 0.0033482708495261864, "phrase": "parse_tree_kernels"}, {"score": 0.0031845067058084583, "phrase": "kernel_computation"}, {"score": 0.003125175084962886, "phrase": "sparse_subset"}, {"score": 0.0030862341714020694, "phrase": "relevant_subtrees"}, {"score": 0.003028727936200845, "phrase": "redundant_structures"}, {"score": 0.0028805474861014722, "phrase": "kernel-based_learning_methods"}, {"score": 0.002756832447586547, "phrase": "linear_programming_approaches"}, {"score": 0.0026384167183586015, "phrase": "unsupervised_learning_tasks"}, {"score": 0.002493592357827492, "phrase": "run-time_improvements"}, {"score": 0.0023419588513908783, "phrase": "predictive_accuracy"}, {"score": 0.0023127544795063263, "phrase": "regular_tree_kernels"}, {"score": 0.0022696277881969896, "phrase": "unsupervised_tasks"}, {"score": 0.0021315842986705485, "phrase": "relevant_dimensions"}, {"score": 0.0021049977753042253, "phrase": "feature_space"}], "paper_keywords": ["tree kernels", " approximation", " kernel methods", " convolution kernels"], "paper_abstract": "Convolution kernels for trees provide simple means for learning with tree-structured data. The computation time of tree kernels is quadratic in the size of the trees, since all pairs of nodes need to be compared. Thus, large parse trees, obtained from HTML documents or structured network data, render convolution kernels inapplicable. In this article, we propose an effective approximation technique for parse tree kernels. The approximate tree kernels (ATKs) limit kernel computation to a sparse subset of relevant subtrees and discard redundant structures, such that training and testing of kernel-based learning methods are significantly accelerated. We devise linear programming approaches for identifying such subsets for supervised and unsupervised learning tasks, respectively. Empirically, the approximate tree kernels attain run-time improvements up to three orders of magnitude while preserving the predictive accuracy of regular tree kernels. For unsupervised tasks, the approximate tree kernels even lead to more accurate predictions by identifying relevant dimensions in feature space.", "paper_title": "Approximate Tree Kernels", "paper_id": "WOS:000277186500004"}