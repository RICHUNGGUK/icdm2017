{"auto_keywords": [{"score": 0.04714038006665976, "phrase": "act-detect"}, {"score": 0.03912955297452614, "phrase": "face-detection_task"}, {"score": 0.036949976776391356, "phrase": "window-sliding_methods"}, {"score": 0.00481495049065317, "phrase": "adaptive_gaze_control_for_object_detection"}, {"score": 0.004720717782144015, "phrase": "novel_gaze-control_model"}, {"score": 0.004413846562278824, "phrase": "local_image_samples"}, {"score": 0.004293336465549723, "phrase": "object_locations"}, {"score": 0.004159617445102161, "phrase": "first_contribution"}, {"score": 0.0040943214728975845, "phrase": "model's_setup"}, {"score": 0.00395111389294443, "phrase": "existing_window-sliding_methods"}, {"score": 0.003828012660311739, "phrase": "acceptable_detection_performance"}, {"score": 0.0036794956254952126, "phrase": "publicly_available_image"}, {"score": 0.0035931556794840027, "phrase": "detection_performance"}, {"score": 0.0033460624879847667, "phrase": "computational_efficiency"}, {"score": 0.00316565009987584, "phrase": "hundreds_fewer_samples"}, {"score": 0.0031035949505887083, "phrase": "second_contribution"}, {"score": 0.002971309306027602, "phrase": "previous_models"}, {"score": 0.002867268370615848, "phrase": "object_presence"}, {"score": 0.0028334016912935165, "phrase": "gaze_location"}, {"score": 0.002627985415362773, "phrase": "simultaneous_adaptation"}, {"score": 0.00246657044320953, "phrase": "local_context"}, {"score": 0.0023613726075968986, "phrase": "spatial_relations"}, {"score": 0.0022339335462778437, "phrase": "resulting_gaze_control"}, {"score": 0.002207530926563892, "phrase": "temporal_process"}, {"score": 0.002164216680934624, "phrase": "object's_context"}, {"score": 0.0021301766910531137, "phrase": "different_scales"}, {"score": 0.0021049977753042253, "phrase": "different_image_locations"}], "paper_keywords": ["Gaze control", " Computationally efficient object detection", " Active vision", " Evolutionary algorithms"], "paper_abstract": "We propose a novel gaze-control model for detecting objects in images. The model, named ACT-DETECT, uses the information from local image samples in order to shift its gaze towards object locations. The model constitutes two main contributions. The first contribution is that the model's setup makes it computationally highly efficient in comparison with existing window-sliding methods for object detection, while retaining an acceptable detection performance. ACT-DETECT is evaluated on a face-detection task using a publicly available image set. In terms of detection performance, ACT-DETECT slightly outperforms the window-sliding methods that have been applied to the face-detection task. In terms of computational efficiency, ACT-DETECT clearly outperforms the window-sliding methods: it requires in the order of hundreds fewer samples for detection. The second contribution of the model lies in its more extensive use of local samples than previous models: instead of merely using them for verifying object presence at the gaze location, the model uses them to determine a direction and distance to the object of interest. The simultaneous adaptation of both the model's visual features and its gaze-control strategy leads to the discovery of features and strategies for exploiting the local context of objects. For example, the model uses the spatial relations between the bodies of the persons in the images and their faces. The resulting gaze control is a temporal process, in which the object's context is exploited at different scales and at different image locations relative to the object.", "paper_title": "Adaptive Gaze Control for Object Detection", "paper_id": "WOS:000292777700019"}