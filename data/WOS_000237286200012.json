{"auto_keywords": [{"score": 0.030796442893680727, "phrase": "rrl"}, {"score": 0.00481495049065317, "phrase": "learning_explorations"}, {"score": 0.004771435352180973, "phrase": "multi-state_coordination_tasks"}, {"score": 0.004559661374976129, "phrase": "relational_state_space"}, {"score": 0.004518442937844318, "phrase": "multi-agent_reinforcement_learning"}, {"score": 0.004437115547184535, "phrase": "growing_evidence"}, {"score": 0.004377077848734575, "phrase": "reinforcement_learning_research_community"}, {"score": 0.004317848984770719, "phrase": "relational_representation"}, {"score": 0.004259418153299952, "phrase": "state_space"}, {"score": 0.0041637779492720295, "phrase": "propositional_one"}, {"score": 0.004126123386192962, "phrase": "complex_tasks"}, {"score": 0.00385431710959668, "phrase": "relational_form"}, {"score": 0.003767738552860633, "phrase": "relational_structure"}, {"score": 0.0036663981146086103, "phrase": "multi-agent_reinforcement_learning_tasks"}, {"score": 0.003535489842686145, "phrase": "single_agent_context"}, {"score": 0.003378385493203297, "phrase": "powerful_possibilities"}, {"score": 0.0033326252993345685, "phrase": "relational_reinforcement_learning"}, {"score": 0.0032577270508271305, "phrase": "complex_multi-agent_coordination_tasks"}, {"score": 0.003141364320497748, "phrase": "abstract_multi-state_coordination_problem"}, {"score": 0.00297454367746853, "phrase": "repeated_stateless_dispersion_games"}, {"score": 0.0028422946420821075, "phrase": "complex_state_space"}, {"score": 0.0028037753646260937, "phrase": "multi-agent_environment"}, {"score": 0.0027282915297520624, "phrase": "fast_convergence"}, {"score": 0.002536763634539386, "phrase": "complex_interactive_models"}, {"score": 0.002233301265743633, "phrase": "complex_multi-agent_planning_tasks"}], "paper_keywords": [""], "paper_abstract": "In this paper we report on using a relational state space in multi-agent reinforcement learning. There is growing evidence in the Reinforcement Learning research community that a relational representation of the state space has many benefits over a propositional one. Complex tasks as planning or information retrieval on the web can be represented more naturally in relational form. Yet, this relational structure has not been exploited for multi-agent reinforcement learning tasks and has only been studied in a single agent context so far. In this paper we explore the powerful possibilities of using Relational Reinforcement Learning (RRL) in complex multi-agent coordination tasks. More precisely, we consider an abstract multi-state coordination problem, which can be considered as a variation and extension of repeated stateless Dispersion Games. Our approach shows that RRL allows to represent a complex state space in a multi-agent environment more compactly and allows for fast convergence of learning agents, Moreover, with this technique, agents are able to make complex interactive models (in the sense of learning from an expert), to predict what other agents will do and generalize over this model. This enables to solve complex multi-agent planning tasks, in which agents need to be adaptive and learn, with more powerful tools.", "paper_title": "Multi-agent relational reinforcement learning explorations in multi-state coordination tasks", "paper_id": "WOS:000237286200012"}