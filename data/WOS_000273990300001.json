{"auto_keywords": [{"score": 0.032598867375888525, "phrase": "irrelevant_parameters"}, {"score": 0.00481495049065317, "phrase": "learning._high_dimensionality"}, {"score": 0.004746263721902197, "phrase": "state_representation"}, {"score": 0.004645057834614705, "phrase": "major_limitation"}, {"score": 0.004513450434595191, "phrase": "reinforcement_learning"}, {"score": 0.0044491531725956756, "phrase": "rl"}, {"score": 0.0041703614123283165, "phrase": "complexity_reduction"}, {"score": 0.004110831924517065, "phrase": "partial_solutions"}, {"score": 0.003965684670332574, "phrase": "automated_dimension_reduction"}, {"score": 0.003909067679938542, "phrase": "rl."}, {"score": 0.003798231095121328, "phrase": "cascading_decomposition_algorithm"}, {"score": 0.003690527690610707, "phrase": "spectral_analysis"}, {"score": 0.003611751858608048, "phrase": "normalized_graph"}, {"score": 0.003586815921345351, "phrase": "laplacian"}, {"score": 0.00333697550984782, "phrase": "parameter_relevance_analysis"}, {"score": 0.0031959843979467704, "phrase": "dynamic_state_abstraction"}, {"score": 0.0030171910207671205, "phrase": "original_state_space"}, {"score": 0.002910545717767559, "phrase": "lower_dimension"}, {"score": 0.0025568135952982345, "phrase": "performed_action_sequences"}, {"score": 0.0024311346712789553, "phrase": "high_dimensionality"}, {"score": 0.0021821915946356168, "phrase": "dimension_reduction_approach"}, {"score": 0.0021049977753042253, "phrase": "infeasible_problem"}], "paper_keywords": ["reinforcement learning", " complexity reduction", " spectral graph theory"], "paper_abstract": "High dimensionality of state representation is a major limitation for scale-up in reinforcement learning (RL). This work derives the knowledge of complexity reduction from partial solutions and provides algorithms for automated dimension reduction in RL. We propose the cascading decomposition algorithm based on the spectral analysis on a normalized graph Laplacian to decompose a problem into several subproblems and then conduct parameter relevance analysis on each subproblem to perform dynamic state abstraction. The elimination of irrelevant parameters projects the original state space into the one with lower dimension in which some subtasks are projected onto the same shared subtasks. The framework could identify irrelevant parameters based on performed action sequences and thus relieve the problem of high dimensionality in learning process. We evaluate the framework with experiments and show that the dimension reduction approach could indeed make some infeasible problem to become learnable.", "paper_title": "AUTOMATIC COMPLEXITY REDUCTION IN REINFORCEMENT LEARNING", "paper_id": "WOS:000273990300001"}