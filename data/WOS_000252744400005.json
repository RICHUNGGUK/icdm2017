{"auto_keywords": [{"score": 0.04728331461408166, "phrase": "continuous_variable_networks"}, {"score": 0.032591969753678916, "phrase": "structure_search"}, {"score": 0.00481495049065317, "phrase": "ideal_parent"}, {"score": 0.004719473826242326, "phrase": "continuous_variable_bayesian_networks"}, {"score": 0.004122490120202936, "phrase": "automatic_learning"}, {"score": 0.003901407184706592, "phrase": "key_task"}, {"score": 0.003729335910488339, "phrase": "computationally_intensive_procedure"}, {"score": 0.003129096013910623, "phrase": "general_method"}, {"score": 0.003021141590548526, "phrase": "common_parametric_distributions"}, {"score": 0.002931569771634119, "phrase": "approximate_merit"}, {"score": 0.002902304851882577, "phrase": "candidate_structure_modifications"}, {"score": 0.002719055901632759, "phrase": "significant_improvement"}, {"score": 0.002678433668156676, "phrase": "running_time"}, {"score": 0.0026384167183586015, "phrase": "search_algorithm"}, {"score": 0.002496722870659199, "phrase": "useful_new_hidden_variables"}, {"score": 0.002459414063407306, "phrase": "network_structure"}, {"score": 0.00223570488212345, "phrase": "synthetic_and_real-life_data_sets"}, {"score": 0.002158505888557746, "phrase": "fully_and_partially_observable_data"}, {"score": 0.0021049977753042253, "phrase": "new_hidden_variables"}], "paper_keywords": ["Bayesian networks", " structure learning", " continuous variables", " hidden variables"], "paper_abstract": "Bayesian networks in general, and continuous variable networks in particular, have become increasingly popular in recent years, largely due to advances in methods that facilitate automatic learning from data. Yet, despite these advances, the key task of learning the structure of such models remains a computationally intensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks in the presence of missing values or hidden variables, a scenario that is part of many real-life problems. In this work we present a general method for speeding structure search for continuous variable networks with common parametric distributions. We efficiently evaluate the approximate merit of candidate structure modifications and apply time consuming (exact) computations only to the most promising ones, thereby achieving significant improvement in the running time of the search algorithm. Our method also naturally and efficiently facilitates the addition of useful new hidden variables into the network structure, a task that is typically considered both conceptually difficult and computationally prohibitive. We demonstrate our method on synthetic and real-life data sets, both for learning structure on fully and partially observable data, and for introducing new hidden variables during structure search.", "paper_title": "Ideal Parent structure learning for continuous variable Bayesian networks", "paper_id": "WOS:000252744400005"}