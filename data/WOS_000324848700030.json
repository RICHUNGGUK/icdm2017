{"auto_keywords": [{"score": 0.046800023565389516, "phrase": "human_action_recognition"}, {"score": 0.03356195330360003, "phrase": "salient_interest_points"}, {"score": 0.00481495049065317, "phrase": "realistic_human_action_recognition"}, {"score": 0.004724878518219719, "phrase": "bag-of-features_model"}, {"score": 0.004680473391946691, "phrase": "space-time_interest_points"}, {"score": 0.0043194180167921165, "phrase": "large_number"}, {"score": 0.004278807087784626, "phrase": "irrelevant_stips"}, {"score": 0.0041987222897385676, "phrase": "specific_action"}, {"score": 0.004159241394996858, "phrase": "realistic_scenarios"}, {"score": 0.004023947242621452, "phrase": "discriminative_codewords"}, {"score": 0.0038930368103015467, "phrase": "features_model"}, {"score": 0.003459191419478298, "phrase": "computational_cost"}, {"score": 0.003378385493203297, "phrase": "recognition_performance"}, {"score": 0.0033150974333333214, "phrase": "human_perception"}, {"score": 0.0032529910932231924, "phrase": "attention_based_saliency_map"}, {"score": 0.0031174595932694036, "phrase": "salient_regions"}, {"score": 0.0030735451757603555, "phrase": "visual_saliency"}, {"score": 0.003030247482854539, "phrase": "strong_evidence"}, {"score": 0.0029594318330247614, "phrase": "acting_subjects"}, {"score": 0.0028495431343111897, "phrase": "human_action"}, {"score": 0.0026045914281028473, "phrase": "unsupervised_codeword_selection_algorithm"}, {"score": 0.002437645786158106, "phrase": "comprehensive_experimental_results"}, {"score": 0.002292199117499287, "phrase": "youtube_dataset"}, {"score": 0.002196609505859369, "phrase": "improved_performance"}, {"score": 0.0021656385587456952, "phrase": "realistic_human_actions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Realistic human action recognition", " Visual saliency", " Space-time interest points", " Unsupervised codeword selection", " Maximal information compression index", " Bag-of-features model", " Saliency map", " Support vector machine"], "paper_abstract": "Constructing the bag-of-features model from Space-time interest points (STIPs) has been successfully utilized for human action recognition. However, how to eliminate a large number of irrelevant STIPs for representing a specific action in realistic scenarios as well as how to select discriminative codewords for effective bag-of-features model still need to be further investigated. In this paper, we propose to select more representative codewords based on our pruned interest points algorithm so as to reduce computational cost as well as improve recognition performance. By taking human perception into account, attention based saliency map is employed to choose salient interest points which fall into salient regions, since visual saliency can provide strong evidence for the location of acting subjects. After salient interest points are identified, each human action is represented with the bag-of-features model. In order to obtain more discriminative codewords, an unsupervised codeword selection algorithm is utilized. Finally, the Support Vector Machine (SVM) method is employed to perform human action recognition. Comprehensive experimental results on the widely used and challenging Hollywood-2 Human Action (HOHA-2) dataset and YouTube dataset demonstrate that our proposed method is computationally efficient while achieving improved performance in recognizing realistic human actions. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Discriminative two-level feature selection for realistic human action recognition", "paper_id": "WOS:000324848700030"}