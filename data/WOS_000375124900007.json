{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "automated_essay_evaluation"}, {"score": 0.0046161999598320486, "phrase": "practical_solution"}, {"score": 0.004500891637057374, "phrase": "time-consuming_activity"}, {"score": 0.0043150485532080065, "phrase": "students'_essays"}, {"score": 0.0037070855410818986, "phrase": "semantic_content"}, {"score": 0.0036144035798673967, "phrase": "automated_feedback"}, {"score": 0.002805597755796506, "phrase": "open_challenges"}, {"score": 0.0023296961971785357, "phrase": "meaningful_feedback"}, {"score": 0.0022906498741695094, "phrase": "students'_writing"}, {"score": 0.0022145052062614514, "phrase": "useful_complement"}, {"score": 0.0021049977753042253, "phrase": "human_scoring"}], "paper_keywords": ["automated essay evaluation", " automated scoring", " natural language processing"], "paper_abstract": "Automated essay evaluation represents a practical solution to a time-consuming activity of manual grading of students' essays. During the last 50 years, many challenges have arisen in the field, including seeking ways to evaluate the semantic content, providing automated feedback, determining validity and reliability of grades and others. In this paper we provide comparison of 21 state-of-the-art approaches for automated essay evaluation and highlight their weaknesses and open challenges in the field. We conclude with the findings that the field has developed to the point where the systems provide meaningful feedback on students' writing and represent a useful complement (not replacement) to human scoring.", "paper_title": "Advances in the Field of Automated Essay Evaluation", "paper_id": "WOS:000375124900007"}