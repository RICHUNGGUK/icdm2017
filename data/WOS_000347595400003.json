{"auto_keywords": [{"score": 0.040537135041717604, "phrase": "proposed_neural_network"}, {"score": 0.011328424034649669, "phrase": "penalty_parameter"}, {"score": 0.00481495049065317, "phrase": "constrained_nonconvex_optimization"}, {"score": 0.004588389551682397, "phrase": "one-layer_recurrent_neural_network"}, {"score": 0.00443308765153716, "phrase": "nonconvex_optimization_problems"}, {"score": 0.004342430078306461, "phrase": "general_inequality_constraints"}, {"score": 0.0041666157975446564, "phrase": "exact_penalty_function_method"}, {"score": 0.0039431758381711125, "phrase": "neuron_state"}, {"score": 0.003731673223879279, "phrase": "feasible_region"}, {"score": 0.003680587852807675, "phrase": "finite_time"}, {"score": 0.0033190180867511605, "phrase": "lower_bounds"}, {"score": 0.0032065418360603293, "phrase": "convergence_time"}, {"score": 0.0030135741897058844, "phrase": "neural_state"}, {"score": 0.0027741761880779535, "phrase": "karush-kuhn-tucker_conditions"}, {"score": 0.00271735110571002, "phrase": "optimization_problem"}, {"score": 0.0026252106881752067, "phrase": "equilibrium_point"}, {"score": 0.0025361866386341796, "phrase": "optimal_solution"}, {"score": 0.0024842242921857705, "phrase": "nonconvex_optimization_problem"}, {"score": 0.0024333239726872604, "phrase": "objective_function"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Nonconvex optimization", " Exact penalty function", " Recurrent neural network", " Finite time convergence"], "paper_abstract": "In this paper, a one-layer recurrent neural network is proposed for solving nonconvex optimization problems subject to general inequality constraints, designed based on an exact penalty function method. It is proved herein that any neuron state of the proposed neural network is convergent to the feasible region in finite time and stays there thereafter, provided that the penalty parameter is sufficiently large. The lower bounds of the penalty parameter and convergence time are also estimated. In addition, any neural state of the proposed neural network is convergent to its equilibrium point set which satisfies the Karush-Kuhn-Tucker conditions of the optimization problem. Moreover, the equilibrium point setis equivalent to the optimal solution to the nonconvex optimization problem if the objective function and constraints satisfy given conditions. Four numerical examples are provided to illustrate the performances of the proposed neural network. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "A one-layer recurrent neural network for constrained nonconvex optimization", "paper_id": "WOS:000347595400003"}