{"auto_keywords": [{"score": 0.04966447721457641, "phrase": "dimensionality_reduction"}, {"score": 0.0069250441418606395, "phrase": "feature_extraction"}, {"score": 0.00481495049065317, "phrase": "functional_data_extension"}, {"score": 0.004650330504968421, "phrase": "beneficial_tool"}, {"score": 0.004491313367627919, "phrase": "main_advantages"}, {"score": 0.003924629168434799, "phrase": "growing_interest"}, {"score": 0.0038737214010638745, "phrase": "combined_schemes"}, {"score": 0.0037903327482565097, "phrase": "built-in_feature_selection"}, {"score": 0.0035974243349266894, "phrase": "rank-deficient_problem"}, {"score": 0.0035507457285983268, "phrase": "variable_selection"}, {"score": 0.0033699898113395328, "phrase": "weighting_algorithm"}, {"score": 0.0031023662967184216, "phrase": "relevance_criterion"}, {"score": 0.0029701328654852246, "phrase": "resulting_weights"}, {"score": 0.002893505913503472, "phrase": "classification_problems"}, {"score": 0.002818850283787242, "phrase": "modified_versions"}, {"score": 0.0027943943921568456, "phrase": "principal_component_analysis"}, {"score": 0.002770226128851542, "phrase": "pca"}, {"score": 0.0027341763737751467, "phrase": "expectation_maximization"}, {"score": 0.002675252546913683, "phrase": "linear_regularized_discriminant_analysis"}, {"score": 0.0025611773887867255, "phrase": "simple_extension"}, {"score": 0.0025389513338018414, "phrase": "wrda"}, {"score": 0.0024950747439601863, "phrase": "functional_features"}, {"score": 0.002237477619351199, "phrase": "artificial_and_real_data_sets"}, {"score": 0.0021607883988873492, "phrase": "small_sized_training_samples"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Dimensionality reduction", " Feature selection", " Feature extraction", " Principal component analysis", " Regularized discriminant analysis"], "paper_abstract": "Dimensionality reduction has proved to be a beneficial tool in learning problems. Two of the main advantages provided by dimensionality reduction are interpretation and generalization. Typically, dimensionality reduction is addressed in two separate ways: variable selection and feature extraction. However, in the recent years there has been a growing interest in developing combined schemes such as feature extraction with built-in feature selection. In this paper, we look at dimensionality reduction as a rank-deficient problem that embraces variable selection and feature extraction, simultaneously. From our analysis, we derive a weighting algorithm that is able to select and linearly transform variables by fixing the dimensionality of the space where a relevance criterion is evaluated. This step enforces sparseness on the resulting weights. Our main goal is dimensionality reduction for classification problems. Namely, we introduce modified versions of principal component analysis (PCA) by expectation maximization (EM) and linear regularized discriminant analysis (RDA). Finally, we propose a simple extension of WRDA that deals with functional features. In this case, observations are described by a set of functions defined over the same domain. Methods were put to test on artificial and real data sets showing high levels of generalization even for small sized training samples. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Weighted feature extraction with a functional data extension", "paper_id": "WOS:000279134100026"}