{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "simple_linear_regression"}, {"score": 0.004263972370406983, "phrase": "second_order_expansion"}, {"score": 0.004161553532799662, "phrase": "probability_mass"}, {"score": 0.004061584718427419, "phrase": "inspection_point"}, {"score": 0.0035100601949451028, "phrase": "density_function"}, {"score": 0.003158776195052182, "phrase": "probability_density_function"}, {"score": 0.0029126002350975634, "phrase": "empirical_average"}, {"score": 0.0028425481299483254, "phrase": "negative_logarithm"}, {"score": 0.0027741761880779535, "phrase": "density_estimates"}, {"score": 0.0025167026017217926, "phrase": "linear_regression"}, {"score": 0.0023776656246300063, "phrase": "proposed_four_estimators"}, {"score": 0.002246292543825748, "phrase": "numerical_experiments"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Entropy estimation", " Non-parametric", " Simple linear regression"], "paper_abstract": "Estimators for differential entropy are proposed. The estimators are based on the second order expansion of the probability mass around the inspection point with respect to the distance from the point. Simple linear regression is utilized to estimate the values of density function and its second derivative at a point. After estimating the values of the probability density function at each of the given sample points, by taking the empirical average of the negative logarithm of the density estimates, two entropy estimators are derived. Other entropy estimators which directly estimate entropy by linear regression, are also proposed. The proposed four estimators are shown to perform well through numerical experiments for various probability distributions. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Non-parametric entropy estimators based on simple linear regression", "paper_id": "WOS:000357348000006"}