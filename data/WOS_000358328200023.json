{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "evaluation_of_visual_quality_measures"}, {"score": 0.004741167616693272, "phrase": "visual_quality_measures"}, {"score": 0.00456159659731398, "phrase": "human_judgments"}, {"score": 0.004388796812234033, "phrase": "class_separability"}, {"score": 0.00393886140366453, "phrase": "novel_data-driven_framework"}, {"score": 0.0037314328720368453, "phrase": "basic_idea"}, {"score": 0.0035899674590142653, "phrase": "large_set"}, {"score": 0.003534889186471058, "phrase": "visually_encoded_data"}, {"score": 0.0033486631403337555, "phrase": "reliable_human_ground_truth_judgements"}, {"score": 0.0031968441980848436, "phrase": "human-labeled_data"}, {"score": 0.002981885091155911, "phrase": "human_judgements"}, {"score": 0.0029361086955646625, "phrase": "previously_unseen_data"}, {"score": 0.0027386335451725762, "phrase": "predictive_performancean_approach"}], "paper_keywords": [""], "paper_abstract": "Visual quality measures seek to algorithmically imitate human judgments of patterns such as class separability, correlation, or outliers. In this paper, we propose a novel data-driven framework for evaluating such measures. The basic idea is to take a large set of visually encoded data, such as scatterplots, with reliable human ground truth judgements, and to use this human-labeled data to learn how well a measure would predict human judgements on previously unseen data. Measures can then be evaluated based on predictive performancean approach that is crucial for generalizing across datasets but has gained little attention so far. To illustrate our framework, we use it to evaluate 15 state-of-the-art class separation measures, using human ground truth data from 828 class separation judgments on color-coded 2D scatterplots.", "paper_title": "Data-driven Evaluation of Visual Quality Measures", "paper_id": "WOS:000358328200023"}