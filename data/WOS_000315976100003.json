{"auto_keywords": [{"score": 0.049352273134126434, "phrase": "highly_complex_scenes"}, {"score": 0.036050693039587356, "phrase": "collective_classification"}, {"score": 0.027852783834442715, "phrase": "gentle_boosting"}, {"score": 0.00481495049065317, "phrase": "accurate_object"}, {"score": 0.004646088032529303, "phrase": "accurate_classification"}, {"score": 0.004483120978156604, "phrase": "important_problem"}, {"score": 0.004195418010131832, "phrase": "sliding_window_approach"}, {"score": 0.004152818376630929, "phrase": "fast_processing"}, {"score": 0.00411064950774695, "phrase": "accurate_results"}, {"score": 0.004068907083697369, "phrase": "particular_object_categories"}, {"score": 0.003926105480202798, "phrase": "desired_performance"}, {"score": 0.003807684498435389, "phrase": "recent_research"}, {"score": 0.003769006950727621, "phrase": "computer_vision"}, {"score": 0.0036740168936985314, "phrase": "object_context"}, {"score": 0.003636692152882991, "phrase": "relational_dependencies"}, {"score": 0.0035997452281985465, "phrase": "object_categories"}, {"score": 0.003545024951402821, "phrase": "improved_accuracy"}, {"score": 0.003509006038574667, "phrase": "object_recognition"}, {"score": 0.0033173234105492895, "phrase": "complex_algorithms"}, {"score": 0.0032668821950788533, "phrase": "offline_processing"}, {"score": 0.0032172054825137866, "phrase": "real-time_nature"}, {"score": 0.003088384809415812, "phrase": "simpler_algorithms"}, {"score": 0.002949598537993663, "phrase": "simple_iterative_algorithm"}, {"score": 0.0027741761880779535, "phrase": "global_co-occurrence_frequencies"}, {"score": 0.002690406375639144, "phrase": "proposed_algorithm"}, {"score": 0.0026630475331025955, "phrase": "multiple_detectors"}, {"score": 0.002466510117718297, "phrase": "co-occurrence_relations"}, {"score": 0.0023798115577749225, "phrase": "remaining_unclassified_objects"}, {"score": 0.0023197517832705297, "phrase": "real-world_dataset"}, {"score": 0.0021049977753042253, "phrase": "full_joint_distribution"}], "paper_keywords": ["Computer vision", " Object recognition", " Co-occurrence", " Confidence", " Real-Time"], "paper_abstract": "Real-time and accurate classification of objects in highly complex scenes is an important problem for the Computer Vision community due to its many application areas. While boosting methods with the sliding window approach provide fast processing and accurate results for particular object categories, they cannot achieve the desired performance for more involved categories of objects. Recent research in Computer Vision has shown that exploiting object context through relational dependencies between object categories leads to improved accuracy in object recognition. While efforts in collective classification in images have resulted in complex algorithms suitable for offline processing, the real-time nature of the problem requires the use of simpler algorithms. In this paper, we propose a simple iterative algorithm for collective classification of all objects in an image, exploiting the global co-occurrence frequencies of object categories. The proposed algorithm uses multiple detectors trained using Gentle Boosting, where the category of the most confident estimate is propagated through the co-occurrence relations to determine the categories of the remaining unclassified objects. Experiments on a real-world dataset demonstrate the superiority of our approach over using Gentle Boosting alone as well as classic collective classification approaches modeling the full joint distribution for each object in the scene.", "paper_title": "A Confidence Ranked Co-Occurrence Approach for Accurate Object Recognition in Highly Complex Scenes", "paper_id": "WOS:000315976100003"}