{"auto_keywords": [{"score": 0.040782441538461355, "phrase": "subgraph_feature_selection"}, {"score": 0.029709021816766247, "phrase": "rlmd"}, {"score": 0.00481495049065317, "phrase": "graph_classification"}, {"score": 0.004743164872278112, "phrase": "structure_data"}, {"score": 0.004602773636967002, "phrase": "wide_interest"}, {"score": 0.004449772140968921, "phrase": "explicit_features"}, {"score": 0.004366970206156021, "phrase": "training_classification_models"}, {"score": 0.004334280585565239, "phrase": "extensive_studies"}, {"score": 0.004174451361776433, "phrase": "training_graph"}, {"score": 0.004081385289516914, "phrase": "vector_data"}, {"score": 0.0038144050983900286, "phrase": "model_learning_process"}, {"score": 0.003757480190200645, "phrase": "selected_most_discriminative_subgraphs"}, {"score": 0.003673675864705682, "phrase": "subsequent_learning_model"}, {"score": 0.0036188435232325337, "phrase": "deteriorated_classification_results"}, {"score": 0.003369322866499541, "phrase": "suboptimally_specified_k_values"}, {"score": 0.0033190180867511605, "phrase": "significantly_reduced_classification_accuracy"}, {"score": 0.0032085525502105836, "phrase": "new_graph_classification_paradigm"}, {"score": 0.003078504229140364, "phrase": "k-dimensional_feature_space"}, {"score": 0.003043957650600757, "phrase": "implicit_and_large_subgraph_space"}, {"score": 0.0029984960344766705, "phrase": "optimal_k_value"}, {"score": 0.0027499252661633525, "phrase": "model_learning"}, {"score": 0.002719055901632759, "phrase": "unified_framework"}, {"score": 0.002688532127821979, "phrase": "discriminative_subgraphs"}, {"score": 0.00266837303806956, "phrase": "guaranteed_minimum_loss_w.r.t"}, {"score": 0.002569816610348772, "phrase": "optimal_number"}, {"score": 0.002550545394070177, "phrase": "subgraphs_k"}, {"score": 0.0025219085140220773, "phrase": "exponentially_large_subgraph_space"}, {"score": 0.002493592357827492, "phrase": "effective_elastic_net"}, {"score": 0.002465593351432208, "phrase": "subgradient_method"}, {"score": 0.0024105326885334962, "phrase": "stopping_criterion"}, {"score": 0.002286781334850213, "phrase": "proposed_rlmd_method"}, {"score": 0.0022696277881969896, "phrase": "gratifying_property"}, {"score": 0.002252602622785975, "phrase": "proved_convergence"}, {"score": 0.0022022878843247274, "phrase": "experimental_results"}, {"score": 0.0021857667493036786, "phrase": "real-life_graph_datasets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feature selection", " Classification", " Graph classification", " Sparse learning"], "paper_abstract": "Classification on structure data, such as graphs, has drawn wide interest in recent years. Due to the lack of explicit features to represent graphs for training classification models, extensive studies have been focused on extracting the most discriminative subgraphs features from the training graph dataset to transfer graphs into vector data. However, such filter-based methods suffer from two major disadvantages: (1) the subgraph feature selection is separated from the model learning process, so the selected most discriminative subgraphs may not best fit the subsequent learning model, resulting in deteriorated classification results; (2) all these methods rely on users to specify the number of subgraph features K, and suboptimally specified K values often result in significantly reduced classification accuracy. In this paper, we propose a new graph classification paradigm which overcomes the above disadvantages by formulating subgraph feature selection as learning a K-dimensional feature space from an implicit and large subgraph space, with the optimal K value being automatically determined. To achieve the goal, we propose a regularized loss minimization-driven (RLMD) feature selection method for graph classification. RLMD integrates subgraph selection and model learning into a unified framework to find discriminative subgraphs with guaranteed minimum loss w.r.t. the objective function. To automatically determine the optimal number of subgraphs K from the exponentially large subgraph space, an effective elastic net and a subgradient method are proposed to derive the stopping criterion, so that K can be automatically obtained once RLMD converges. The proposed RLMD method enjoys gratifying property including proved convergence and applicability to various loss functions. Experimental results on real-life graph datasets demonstrate significant performance gain. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Finding the best not the most: regularized loss minimization subgraph selection for graph classification", "paper_id": "WOS:000359028900040"}