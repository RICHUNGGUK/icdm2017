{"auto_keywords": [{"score": 0.039233071846805946, "phrase": "time-varying_linear_matrix_equations"}, {"score": 0.015439904543519832, "phrase": "finite-time_convergence"}, {"score": 0.00481495049065317, "phrase": "different_zhang_functions"}, {"score": 0.004685575787741084, "phrase": "time-varying_linear_matrix_equation"}, {"score": 0.004559661374976129, "phrase": "parallel-distributed_nature"}, {"score": 0.004518442937844318, "phrase": "recurrent_neural_networks"}, {"score": 0.004397000006837371, "phrase": "designated_hardware"}, {"score": 0.004278807087784626, "phrase": "broad_applications"}, {"score": 0.0041074234667687875, "phrase": "special_class"}, {"score": 0.004070276454809864, "phrase": "recurrent_neural_network"}, {"score": 0.004033632467688628, "phrase": "zhang"}, {"score": 0.004015182362035361, "phrase": "neural_network"}, {"score": 0.003767738552860633, "phrase": "online_solution"}, {"score": 0.0036167509502276294, "phrase": "zhang_function"}, {"score": 0.0033175095965171674, "phrase": "plentiful_activation_functions"}, {"score": 0.0029610472910087176, "phrase": "theoretical_solution"}, {"score": 0.0028037753646260937, "phrase": "new_activation_function"}, {"score": 0.0027660654629803657, "phrase": "li"}, {"score": 0.0025833500717948343, "phrase": "li_activation_function"}, {"score": 0.00245724480030654, "phrase": "time-varying_theoretical_solution"}, {"score": 0.002391067964886602, "phrase": "upper_bound"}, {"score": 0.0023586493263143553, "phrase": "convergence_time"}, {"score": 0.002305589818994465, "phrase": "lyapunov_theory"}, {"score": 0.0022434880661725493, "phrase": "extensive_simulations"}, {"score": 0.0021632743048552536, "phrase": "theoretical_analysis"}, {"score": 0.0021049977753042253, "phrase": "proposed_znn_models"}], "paper_keywords": ["Activation function", " Finite-time convergence", " Time-varying linear matrix equations", " Zhang function", " ZNN"], "paper_abstract": "In addition to the parallel-distributed nature, recurrent neural networks can be implemented physically by designated hardware and thus have been found broad applications in many fields. In this paper, a special class of recurrent neural network named Zhang neural network (ZNN), together with its electronic realization, is investigated and exploited for online solution of time-varying linear matrix equations. By following the idea of Zhang function (i.e., error function), two ZNN models are proposed and studied, which allow us to choose plentiful activation functions (e.g., any monotonically-increasing odd activation function). It is theoretically proved that such two ZNN models globally and exponentially converge to the theoretical solution of time-varying linear matrix equations when using linear activation functions. Besides, the new activation function, named Li activation function, is exploited. It is theoretically proved that, when using Li activation function, such two ZNN models can be further accelerated to finite-time convergence to the time-varying theoretical solution. In addition, the upper bound of the convergence time is derived analytically via Lyapunov theory. Then, we conduct extensive simulations using such two ZNN models. The results substantiate the theoretical analysis and the efficacy of the proposed ZNN models for solving time-varying linear matrix equations.", "paper_title": "From Different Zhang Functions to Various ZNN Models Accelerated to Finite-Time Convergence for Time-Varying Linear Matrix Equation", "paper_id": "WOS:000336026700007"}