{"auto_keywords": [{"score": 0.04475067331481927, "phrase": "snomed_ct"}, {"score": 0.010537510099484315, "phrase": "biomedical_ontologies"}, {"score": 0.00908051064978804, "phrase": "scalable_ontology_verification"}, {"score": 0.007523702277064671, "phrase": "inter-rater_agreement"}, {"score": 0.00481495049065317, "phrase": "critical_errors"}, {"score": 0.004696407360309416, "phrase": "snomed_ct._objectives"}, {"score": 0.004580769304731227, "phrase": "arduous_process"}, {"score": 0.004515967518891287, "phrase": "peer_review"}, {"score": 0.004483909657713567, "phrase": "subject-matter_experts"}, {"score": 0.004357927940322685, "phrase": "crowdsourcing_methods"}, {"score": 0.0042354707960660706, "phrase": "medicine_clinical_terms"}, {"score": 0.003986509098918949, "phrase": "ontology_verification"}, {"score": 0.0038606630003037864, "phrase": "bayesian_classifier"}, {"score": 0.0037789720824759503, "phrase": "prospective_study"}, {"score": 0.0035063811070537233, "phrase": "single_expert"}, {"score": 0.002923214590192792, "phrase": "'septic_shock"}, {"score": 0.0028920935761027724, "phrase": "soft-tissue_infection"}, {"score": 0.002570817338292482, "phrase": "similar_ontologies"}, {"score": 0.0023346234464901978, "phrase": "manual_error_checking"}, {"score": 0.002268891304166886, "phrase": "online_anonymous_crowd"}], "paper_keywords": ["crowdsourcing", " biomedical ontology", " ontology engineering", " SNOMED CT"], "paper_abstract": "Objectives The verification of biomedical ontologies is an arduous process that typically involves peer review by subject-matter experts. This work evaluated the ability of crowdsourcing methods to detect errors in SNOMED CT (Systematized Nomenclature of Medicine Clinical Terms) and to address the challenges of scalable ontology verification. Methods We developed a methodology to crowdsource ontology verification that uses micro-tasking combined with a Bayesian classifier. We then conducted a prospective study in which both the crowd and domain experts verified a subset of SNOMED CT comprising 200 taxonomic relationships. Results The crowd identified errors as well as any single expert at about one-quarter of the cost. The inter-rater agreement (kappa) between the crowd and the experts was 0.58; the inter-rater agreement between experts themselves was 0.59, suggesting that the crowd is nearly indistinguishable from any one expert. Furthermore, the crowd identified 39 previously undiscovered, critical errors in SNOMED CT (eg, 'septic shock is a soft-tissue infection'). Discussion The results show that the crowd can indeed identify errors in SNOMED CT that experts also find, and the results suggest that our method will likely perform well on similar ontologies. The crowd may be particularly useful in situations where an expert is unavailable, budget is limited, or an ontology is too large for manual error checking. Finally, our results suggest that the online anonymous crowd could successfully complete other domain-specific tasks. Conclusions We have demonstrated that the crowd can address the challenges of scalable ontology verification, completing not only intuitive, common-sense tasks, but also expert-level, knowledge-intensive tasks.", "paper_title": "Using the wisdom of the crowds to find critical errors in biomedical ontologies: a study of SNOMED CT", "paper_id": "WOS:000356717100018"}