{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "class_label"}, {"score": 0.04098693034043568, "phrase": "generative_models"}, {"score": 0.015576048676418864, "phrase": "generative_score_spaces"}, {"score": 0.011131646517530852, "phrase": "hidden_information"}, {"score": 0.0077634534334837906, "phrase": "existing_methods"}, {"score": 0.0046394362688524475, "phrase": "increasing_attention"}, {"score": 0.004367707617503722, "phrase": "wide_range"}, {"score": 0.00432733352307636, "phrase": "recognition_tasks"}, {"score": 0.00415019999222481, "phrase": "training_data"}, {"score": 0.00411182818527434, "phrase": "probabilistic_generative_models"}, {"score": 0.003852939975815287, "phrase": "derived_feature"}, {"score": 0.003543804852018315, "phrase": "staged_way"}, {"score": 0.003367188029221534, "phrase": "discriminative_models"}, {"score": 0.0032898346872897383, "phrase": "underlying_point"}, {"score": 0.0031845067058084583, "phrase": "hidden_variables"}, {"score": 0.002942495422585405, "phrase": "general_extension"}, {"score": 0.00290173234777844, "phrase": "existing_score_space_methods"}, {"score": 0.0028218877037665484, "phrase": "rich_discriminative_information"}, {"score": 0.0027698788736535865, "phrase": "feature_mappings"}, {"score": 0.0026687115458522326, "phrase": "regular_generative_models"}, {"score": 0.0026440008776012665, "phrase": "class_conditional_models"}, {"score": 0.0024429654627197393, "phrase": "resulted_methods"}, {"score": 0.002420339995088838, "phrase": "simple_and_intuitive_forms"}, {"score": 0.0023867931451842087, "phrase": "weighted_versions"}, {"score": 0.0023103100997344072, "phrase": "bayesian_inference"}, {"score": 0.0022571810910697013, "phrase": "empirical_evaluation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Generative score spaces", " Discriminative extension", " Classification", " Feature mapping"], "paper_abstract": "Generative score spaces have recently received increasing attention due to their state-of-the-art performance in a wide range of recognition tasks. These methods model the distribution of the training data using probabilistic generative models and derive the feature for each sample based on the generative models. The derived feature encodes the information of the sample, hidden variables and model parameters for classification, providing a staged way to integrate the abilities of generative models in inferring hidden information and discriminative models in classification. The underlying point is that the hidden information carried by hidden variables in generative models is informative and useful in classification. In this paper, we propose a general extension for the existing score space methods to exploit class label that encodes rich discriminative information, when deriving feature mappings. This is achieved by extending the regular generative models to class conditional models over both observed variable and class label, and deriving feature mapping over such extended models. The resulted methods take simple and intuitive forms which are weighted versions of existing methods, benefitting from the Bayesian inference of class label. The empirical evaluation over two typical generative models and 6 datasets shows its significant improvement over existing methods. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Exploiting class label in generative score spaces", "paper_id": "WOS:000342248100051"}