{"auto_keywords": [{"score": 0.04974208475172307, "phrase": "object_retrieval"}, {"score": 0.04809176147872219, "phrase": "query_object"}, {"score": 0.04617998264114106, "phrase": "query_image"}, {"score": 0.00481495049065317, "phrase": "visual_query_context"}, {"score": 0.004315670715009727, "phrase": "wide_popularity"}, {"score": 0.004285313136123701, "phrase": "sift"}, {"score": 0.003936910716320182, "phrase": "real-life_applications"}, {"score": 0.0038679618585498597, "phrase": "existing_object_retrieval_methods"}, {"score": 0.003720479367430766, "phrase": "satisfactory_results"}, {"score": 0.0034299649219748513, "phrase": "discriminative_features"}, {"score": 0.003346119094502001, "phrase": "similar_objects"}, {"score": 0.003310813458610484, "phrase": "image_collection"}, {"score": 0.003229870860371533, "phrase": "object_retrieval_performance"}, {"score": 0.0031845067058084583, "phrase": "difficult_cases"}, {"score": 0.003095674983836725, "phrase": "object_retrieval_method"}, {"score": 0.0030199761431186434, "phrase": "visual_context"}, {"score": 0.0029150253706493852, "phrase": "possible_uncertainty"}, {"score": 0.0028944757265206332, "phrase": "feature-based_query_object_representation"}, {"score": 0.00281371157349156, "phrase": "visual_elements"}, {"score": 0.002677744558540711, "phrase": "uncertain_observation"}, {"score": 0.0026494723416072316, "phrase": "latent_search_intent"}, {"score": 0.0026214978443388653, "phrase": "saliency_map"}, {"score": 0.002521421793028698, "phrase": "language_modeling_approach"}, {"score": 0.002468450551616404, "phrase": "contextual_object_retrieval"}, {"score": 0.002451045703137587, "phrase": "cor"}, {"score": 0.002391067964886602, "phrase": "relevance_score"}, {"score": 0.0023408288327263294, "phrase": "search_intent_scores"}, {"score": 0.00229164285862567, "phrase": "uncertain_roi"}, {"score": 0.002211947103318718, "phrase": "contextual_information"}, {"score": 0.0021501863832966966, "phrase": "proposed_cor_model"}], "paper_keywords": ["Context", " image retrieval", " object retrieval"], "paper_abstract": "Object retrieval aims at retrieving images containing objects similar to the query object captured in the region of interest (ROI) of the query image. Boosted by the invention and wide popularity of SIFT image features and bag-of-visual-words image representation, object retrieval has progressed significantly in the past years and has already found deployment in real-life applications and products. While existing object retrieval methods perform well in many cases, they may fail to return satisfactory results if the ROI specified by the user is inaccurate or if the object captured there is too small to be represented using discriminative features and consequently to be matched with similar objects in the image collection. In order to improve the object retrieval performance also in these difficult cases, we propose in this paper an object retrieval method that exploits the information about the visual context of the query object and employ it to compensate for possible uncertainty in feature-based query object representation. Contextual information is drawn from the visual elements surrounding the query object in the query image. We consider the ROI as an uncertain observation of the latent search intent and the saliency map detected for the query image as a prior. Then a language modeling approach is employed to devise a contextual object retrieval (COR) model. There, the relevance score is determined based on the search intent scores that are inferred from the uncertain ROI and the saliency prior. The usefulness of the contextual information for object retrieval and the effectiveness of the proposed COR model are demonstrated and evaluated on three representative image datasets.", "paper_title": "Object Retrieval Using Visual Query Context", "paper_id": "WOS:000297343400010"}