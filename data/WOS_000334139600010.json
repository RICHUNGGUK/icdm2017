{"auto_keywords": [{"score": 0.042918232558145336, "phrase": "rl"}, {"score": 0.00481495049065317, "phrase": "ball_screw_feed_drive_controllers"}, {"score": 0.004649302313891199, "phrase": "ball_screw_feed_drives"}, {"score": 0.004552648949390462, "phrase": "great_accuracy"}, {"score": 0.004334832612029898, "phrase": "close_analytical_solution"}, {"score": 0.004215057618794356, "phrase": "desired_controller"}, {"score": 0.004156411748278609, "phrase": "reinforcement_learning"}, {"score": 0.003929843413153981, "phrase": "autonomous_adaptation"}, {"score": 0.003741712498615481, "phrase": "ri_paradigm"}, {"score": 0.003689628075766217, "phrase": "different_approaches"}, {"score": 0.0031845067058084583, "phrase": "five_algorithms"}, {"score": 0.0030747771905143273, "phrase": "accurate_simulation_model"}, {"score": 0.003010757286626405, "phrase": "commercial_device"}, {"score": 0.0028866770996648057, "phrase": "noisy_disturbance"}, {"score": 0.0028265625216338875, "phrase": "state_observation_values"}, {"score": 0.002787181295054319, "phrase": "benchmark_results"}, {"score": 0.002691105581191125, "phrase": "double-loop_pid_controller"}, {"score": 0.0025264171945423254, "phrase": "random_search_optimization"}, {"score": 0.0024912073108138613, "phrase": "action-critic_methods"}, {"score": 0.002456486927713543, "phrase": "continuous_action_space"}, {"score": 0.0023551955270737215, "phrase": "cacla"}, {"score": 0.002289993730291138, "phrase": "pid_controller"}, {"score": 0.002242277042923036, "phrase": "computational_experiments"}, {"score": 0.002195552442011313, "phrase": "future_research"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Reinforcement learning", " Feedback control", " Ball screw feed drive"], "paper_abstract": "Feedback controllers for ball screw feed drives may provide great accuracy in positioning, but have no close analytical solution to derive the desired controller. Reinforcement Learning (RL) is proposed to provide autonomous adaptation and learning of them. The RI paradigm allows different approaches, which are tested in this paper looking for the best suited for the ball screw drivers. Specifically, five algorithms are compared on an accurate simulation model of a commercial device, with and without a noisy disturbance on the state observation values. Benchmark results are provided by a double-loop PID controller, whose parameters have been tuned by a random search optimization. Action-critic methods with continuous action space (Policy-Gradient and CACLA) outperform the PID controller in the computational experiments, encouraging future research. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Reinforcement learning of ball screw feed drive controllers", "paper_id": "WOS:000334139600010"}