{"auto_keywords": [{"score": 0.048201896565263386, "phrase": "generalized_lasso"}, {"score": 0.04351879568458235, "phrase": "variable_selection"}, {"score": 0.04241767252094026, "phrase": "prior_knowledge"}, {"score": 0.00481495049065317, "phrase": "dual_and_degrees"}, {"score": 0.00421410521089045, "phrase": "simultaneous_estimation"}, {"score": 0.0027495914857929584, "phrase": "original_one"}, {"score": 0.0026240423738883704, "phrase": "coordinate_descent_algorithm"}, {"score": 0.002235433376874495, "phrase": "tuning_parameter"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Constrained optimization", " Coordinate descent", " Degrees of freedom", " Duality", " KKT condition", " Lasso"], "paper_abstract": "The lasso and its variants have attracted much attention recently because of its ability of simultaneous estimation and variable selection. When some prior knowledge exists in applications, the performance of estimation and variable selection can be further improved by incorporating the prior knowledge as constraints on parameters. In this article, we consider linearly constrained generalized lasso, where the constraints are either linear inequalities or equalities or both. The dual of the problem is derived, which is a much simpler problem than the original one. As a by-product, a coordinate descent algorithm is feasible to solve the dual. A formula for the number of degrees of freedom is derived. The method for selecting tuning parameter is also discussed. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "The dual and degrees of freedom of linearly constrained generalized lasso", "paper_id": "WOS:000352250900002"}