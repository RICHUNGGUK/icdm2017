{"auto_keywords": [{"score": 0.04271552770636889, "phrase": "spatio-temporal_features"}, {"score": 0.00481495049065317, "phrase": "tiny_activity_analysis"}, {"score": 0.004780334370089564, "phrase": "automatic_understanding"}, {"score": 0.004745965928357563, "phrase": "human_activities"}, {"score": 0.004694873827535293, "phrase": "huge_challenge"}, {"score": 0.004661116930794176, "phrase": "multimedia_analysis_field"}, {"score": 0.004528488165249491, "phrase": "small-scale_activities"}, {"score": 0.0044635893862404385, "phrase": "finger_motions"}, {"score": 0.004383766516923386, "phrase": "complex_scenes"}, {"score": 0.004336556638639097, "phrase": "typical_camera_views"}, {"score": 0.004274396662662802, "phrase": "local_feature_analysis_methods"}, {"score": 0.004049096735491828, "phrase": "feature_selection_methods"}, {"score": 0.004005476499933792, "phrase": "video_representation"}, {"score": 0.003712964709503484, "phrase": "meaningful_foreground"}, {"score": 0.0034046409849387365, "phrase": "biological_feature_selection_method"}, {"score": 0.0032484270911235526, "phrase": "feature_space"}, {"score": 0.0031444844772458504, "phrase": "graph_based_co-attention_model"}, {"score": 0.0030881781330491104, "phrase": "activity_analysis"}, {"score": 0.0029571101238632783, "phrase": "interest_points"}, {"score": 0.002872827714395808, "phrase": "individual_tiny_activities"}, {"score": 0.002790940765338191, "phrase": "integrated_top-down_and_bottom-up_visual_attention_model"}, {"score": 0.002672454243212711, "phrase": "optical_flow"}, {"score": 0.002624577999455744, "phrase": "typical_attention_models"}, {"score": 0.002577557228072987, "phrase": "multiple_regions"}, {"score": 0.002441477009112022, "phrase": "kth_dataset"}, {"score": 0.002423896050725285, "phrase": "youtube"}, {"score": 0.0023042157412517333, "phrase": "visual_observation_data"}, {"score": 0.0022629214153694504, "phrase": "infusion_pump"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Tiny activity recognition", " Visual attention", " Co-Attention model", " MoSIFT"], "paper_abstract": "Automatic understanding of human activities is a huge challenge in multimedia analysis field. This challenge is especially critical in small-scale activities, such as finger motions, and activities in complex scenes. For typical camera views, both global feature and local feature analysis methods are unsuitable. To solve this problem, many studies focus on using spatio-temporal features and feature selection methods to get video representation. However, these spatio-temporal features are problematic for two reasons. First, we are not sure whether these features are meaningful foreground or noise. Second, we are unable to foresee where an activity will occur based on these features. Therefore, a biological feature selection method is needed to reorganize these spatio-temporal features and represent the video in a feature space. In this paper, we propose a graph based Co-Attention model to select more efficient features for activity analysis. Without reducing the dimensionality, our Co-Attention model considers the number of interest points. Our model is derived from correlations among individual tiny activities, whose salient regions are identified by combining an integrated top-down and bottom-up visual attention model, and a motion attention model built by spatio-temporal features instead of optical flow directly. Different from typical attention models, the Co-Attention model allows multiple regions of interest in video co-existing for further analysis. Experimental results on the KTH dataset, YouTube dataset and a new tiny activity dataset, Pump dataset which consist of visual observation data from patients operating an infusion pump, validate our activity analysis approach is more effective than state-of-the-art methods. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "The co-attention model for tiny activity analysis", "paper_id": "WOS:000317091700008"}