{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "retrieval_systems"}, {"score": 0.012361090382037759, "phrase": "partial_relevance_judgments"}, {"score": 0.004752221133109136, "phrase": "partial_relevance_judgements"}, {"score": 0.004568871736067739, "phrase": "mean_average_precision_and_recall_level_precision"}, {"score": 0.004450568197899267, "phrase": "good_system-oriented_measures"}, {"score": 0.004086744751579196, "phrase": "effectiveness_evaluation"}, {"score": 0.0040334640350288, "phrase": "information_retrieval_systems"}, {"score": 0.0032266093099550955, "phrase": "major_retrieval_evaluation_events"}, {"score": 0.0031636612155559267, "phrase": "trec_conferences"}, {"score": 0.0031223774204093713, "phrase": "ntcir_workshops"}, {"score": 0.002982068153662372, "phrase": "average_precision"}, {"score": 0.0029238768315330305, "phrase": "level_precision"}, {"score": 0.0028857130140890787, "phrase": "normalized_discount_cumulative_gain"}, {"score": 0.0028293966058473476, "phrase": "normalized_average_precision"}, {"score": 0.0022628564592874147, "phrase": "borda_count"}, {"score": 0.002233301265743633, "phrase": "condorcet_voting"}, {"score": 0.0021753413850499467, "phrase": "zero-one_normalization_method"}, {"score": 0.0021049977753042253, "phrase": "experimental_results"}], "paper_keywords": ["distributed information retrieval", " ranking retrieval systems"], "paper_abstract": "Some measures such as mean average precision and recall level precision are considered as good system-oriented measures, because they concern both precision and recall that are two important aspects for effectiveness evaluation of information retrieval systems. However, such good system-oriented measures suffer from some shortcomings when partial relevance judgments are used. In this paper, we discuss how to rank retrieval systems in the condition of partial relevance judgments, which is common in major retrieval evaluation events such as TREC conferences and NTCIR workshops. Four system-oriented measures, which are mean average precision, recall level precision, normalized discount cumulative gain, and normalized average precision over all documents, are discussed. Our investigation shows that averaging values over a set of queries may not be the most reliable approach to rank a group of retrieval systems. Some alternatives such as Borda count, Condorcet voting, and the Zero-one normalization method, are investigated. Experimental results are also presented for the evaluation of these methods.", "paper_title": "Ranking retrieval systems with partial relevance judgements", "paper_id": "WOS:000257026400002"}