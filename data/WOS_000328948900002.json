{"auto_keywords": [{"score": 0.03981137560236148, "phrase": "nnbc"}, {"score": 0.01150917796100724, "phrase": "joint_p.d.f"}, {"score": 0.008705221820825153, "phrase": "bayesian"}, {"score": 0.00481495049065317, "phrase": "classification_problems"}, {"score": 0.0047747348769918, "phrase": "continuous_attributes"}, {"score": 0.00471503739362446, "phrase": "important_way"}, {"score": 0.004597861947580946, "phrase": "naive_bayesian_classifiers"}, {"score": 0.004408976280483648, "phrase": "fundamental_assumption"}, {"score": 0.004157446556371582, "phrase": "joint_probability_density_function"}, {"score": 0.003986581218661184, "phrase": "marginal_p.d.f"}, {"score": 0.003920209964956682, "phrase": "nbc_design"}, {"score": 0.0038227112550093863, "phrase": "non-naive_bayesian_classifier"}, {"score": 0.003712011863784581, "phrase": "independence_assumption"}, {"score": 0.003370287778450475, "phrase": "class-conditional_p.d.f"}, {"score": 0.00330025288273863, "phrase": "optimal_bandwidth_selection"}, {"score": 0.00323166859966514, "phrase": "crucial_part"}, {"score": 0.0030216031606457214, "phrase": "bayesian_classifiers"}, {"score": 0.002971249577042111, "phrase": "classification_accuracy"}, {"score": 0.002897283610433351, "phrase": "characteristic_curve"}, {"score": 0.0028370498057725796, "phrase": "square_error"}, {"score": 0.0025115226497100687, "phrase": "fnb"}, {"score": 0.0024081387633065206, "phrase": "comparative_results"}, {"score": 0.0022139352210982398, "phrase": "support_vector_machine"}, {"score": 0.002149722786728988, "phrase": "relatively_favorable_classification_accuracy"}, {"score": 0.0021049977753042253, "phrase": "training_time"}], "paper_keywords": ["Joint probability density estimation", " kernel function", " naive Bayesian classifier (NBC)", " optimal bandwidth", " probability mean square error"], "paper_abstract": "An important way to improve the performance of naive Bayesian classifiers (NBCs) is to remove or relax the fundamental assumption of independence among the attributes, which usually results in an estimation of joint probability density function (p.d.f.) instead of the estimation of marginal p.d.f. in the NBC design. This paper proposes a non-naive Bayesian classifier (NNBC) in which the independence assumption is removed and the marginal p.d.f. estimation is replaced by the joint p.d.f. estimation. A new technique of estimating the class-conditional p.d.f. based on the optimal bandwidth selection, which is the crucial part of the joint p.d.f. estimation, is applied in our NNBC. Three well-known indexes for measuring the performance of Bayesian classifiers, which are classification accuracy, area under receiver operating characteristic curve, and probability mean square error, are adopted to conduct a comparison among the four Bayesian models, i.e., normal naive Bayesian, flexible naive Bayesian (FNB), the homologous model of FNB (FNBROT), and our proposed NNBC. The comparative results show that NNBC is statistically superior to the other three models regarding the three indexes. And, in the comparison with support vector machine and four boosting-based classification methods, NNBC achieves a relatively favorable classification accuracy while significantly reducing the training time.", "paper_title": "Non-Naive Bayesian Classifiers for Classification Problems With Continuous Attributes", "paper_id": "WOS:000328948900002"}