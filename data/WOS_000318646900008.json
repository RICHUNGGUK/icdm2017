{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "classifier_combination"}, {"score": 0.00470862827326028, "phrase": "general_approach"}, {"score": 0.00443638953122199, "phrase": "probabilistic_classifier"}, {"score": 0.004306205029641728, "phrase": "class_membership_posterior_probability"}, {"score": 0.004179824713643488, "phrase": "general_scenario"}, {"score": 0.0030113280682863234, "phrase": "evaluation_measures"}, {"score": 0.002944748761305397, "phrase": "mse"}, {"score": 0.0029011328022367274, "phrase": "auc"}, {"score": 0.0027741761880779535, "phrase": "good_probability_estimation"}, {"score": 0.002234491924392104, "phrase": "new_non-monotonic_calibration_method"}], "paper_keywords": ["Classifier combination", " Classifier calibration", " Classifier diversity", " Probability estimation", " Calibration measures", " Separability measures"], "paper_abstract": "A general approach to classifier combination considers each model as a probabilistic classifier which outputs a class membership posterior probability. In this general scenario, it is not only the quality and diversity of the models which are relevant, but the level of calibration of their estimated probabilities as well. In this paper, we study the role of calibration before and after classifier combination, focusing on evaluation measures such as MSE and AUC, which better account for good probability estimation than other evaluation measures. We present a series of findings that allow us to recommend several layouts for the use of calibration in classifier combination. We also empirically analyse a new non-monotonic calibration method that obtains better results for classifier combination than other monotonic calibration methods.", "paper_title": "On the effect of calibration in classifier combination", "paper_id": "WOS:000318646900008"}