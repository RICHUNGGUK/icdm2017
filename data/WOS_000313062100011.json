{"auto_keywords": [{"score": 0.04955111375780366, "phrase": "ncl_experts"}, {"score": 0.04902896582647278, "phrase": "gating_network"}, {"score": 0.01447765818092333, "phrase": "special_error_function"}, {"score": 0.012970887746424953, "phrase": "ncl_method"}, {"score": 0.00481495049065317, "phrase": "combination_method"}, {"score": 0.004657393640152439, "phrase": "negative_correlation_learning"}, {"score": 0.004606025546142392, "phrase": "ncl"}, {"score": 0.004504969093304219, "phrase": "popular_combining_method"}, {"score": 0.004357511171877593, "phrase": "simultaneous_training"}, {"score": 0.004309434349688514, "phrase": "base_neural_network"}, {"score": 0.004009555045206973, "phrase": "improved_version"}, {"score": 0.003751228706861377, "phrase": "combining_part"}, {"score": 0.003668859054928118, "phrase": "experts_method"}, {"score": 0.0035290250358997904, "phrase": "base_nns"}, {"score": 0.0034707339811048403, "phrase": "ncl_ensemble_method"}, {"score": 0.0033015555065209865, "phrase": "nn_expert"}, {"score": 0.0032470094613727433, "phrase": "different_parts"}, {"score": 0.003158088307044493, "phrase": "training_data"}, {"score": 0.0030715948036518603, "phrase": "local_competence"}, {"score": 0.002921815042028607, "phrase": "combining_approach"}, {"score": 0.002763919794139171, "phrase": "needed_functionality"}, {"score": 0.002643753276264565, "phrase": "proposed_method"}, {"score": 0.0026000466400112974, "phrase": "gated_ncl."}, {"score": 0.0025713100093065645, "phrase": "improved_ensemble_method"}, {"score": 0.0025008468316682036, "phrase": "previous_approaches"}, {"score": 0.002288055715068212, "phrase": "uci"}, {"score": 0.0022129983153880467, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "previous_combining_approaches"}], "paper_keywords": ["Neural networks ensemble", " Negative correlation learning", " Gating network", " Gated NCL"], "paper_abstract": "Negative Correlation Learning (NCL) is a popular combining method that employs special error function for the simultaneous training of base neural network (NN) experts. In this article, we propose an improved version of NCL method in which the capability of gating network, as the combining part of Mixture of Experts method, is used to combine the base NNs in the NCL ensemble method. The special error function of the NCL method encourages each NN expert to learn different parts or aspects of the training data. Thus, the local competence of the experts should be considered in the combining approach. The gating network provides a way to support this needed functionality for combining the NCL experts. So the proposed method is called Gated NCL. The improved ensemble method is compared with the previous approaches were used for combining NCL experts, including winner-take-all (WTA) and average (AVG) combining techniques, in solving several classification problems from UCI machine learning repository. The experimental results show that our proposed ensemble method significantly improved performance over the previous combining approaches.", "paper_title": "Improving combination method of NCL experts using gating network", "paper_id": "WOS:000313062100011"}