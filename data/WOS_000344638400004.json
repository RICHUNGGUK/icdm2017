{"auto_keywords": [{"score": 0.04577680896363523, "phrase": "stochastic_strongly-convex_optimization"}, {"score": 0.04495971759688538, "phrase": "gradient_oracle_model"}, {"score": 0.00481495049065317, "phrase": "regret_minimization_barrier"}, {"score": 0.004438642216475526, "phrase": "novel_algorithms"}, {"score": 0.004066076430818046, "phrase": "first_algorithm"}, {"score": 0.0038431888954930083, "phrase": "gradient_updates"}, {"score": 0.003795335309109018, "phrase": "historical_averaging"}, {"score": 0.0037246656582642272, "phrase": "second_algorithm"}, {"score": 0.003542557455905503, "phrase": "pure_gradient_steps"}, {"score": 0.00347657813672945, "phrase": "random_step_size"}, {"score": 0.0031448288291084, "phrase": "previously_known_best_rate"}, {"score": 0.0029169007179202164, "phrase": "strongly-convex_optimization_algorithm"}, {"score": 0.002431797888365548, "phrase": "online_stochastic"}, {"score": 0.0022696277881969896, "phrase": "online-to-batch_conversion"}, {"score": 0.0021315842986705485, "phrase": "first_formal_evidence"}, {"score": 0.0021049977753042253, "phrase": "online_convex_optimization"}], "paper_keywords": ["stochastic gradient descent", " convex optimization", " regret minimization", " online learning"], "paper_abstract": "We give novel algorithms for stochastic strongly-convex optimization in the gradient oracle model which return a O(1/T)-approximate solution after T iterations. The first algorithm is deterministic, and achieves this rate via gradient updates and historical averaging. The second algorithm is randomized, and is based on pure gradient steps with a random step size. This rate of convergence is optimal in the gradient oracle model. This improves upon the previously known best rate of O(log(T)/T), which was obtained by applying an online strongly-convex optimization algorithm with regret O(log(T)) to the batch setting. We complement this result by proving that any algorithm has expected regret of Omega(log(T)) in the online stochastic strongly-convex optimization setting. This shows that any online-to-batch conversion is inherently suboptimal for stochastic strongly-convex optimization. This is the first formal evidence that online convex optimization is strictly more difficult than batch stochastic convex optimization.(1)", "paper_title": "Beyond the Regret Minimization Barrier: Optimal Algorithms for Stochastic Strongly-Convex Optimization", "paper_id": "WOS:000344638400004"}