{"auto_keywords": [{"score": 0.04284796714076845, "phrase": "compressed_llcs"}, {"score": 0.02587471518851888, "phrase": "lru"}, {"score": 0.01468445531813322, "phrase": "llc"}, {"score": 0.012811334692370157, "phrase": "size_information"}, {"score": 0.010832312181175988, "phrase": "compressed_caches"}, {"score": 0.00481495049065317, "phrase": "practical_way"}, {"score": 0.004755054742217193, "phrase": "effective_capacity"}, {"score": 0.004710620078577185, "phrase": "microprocessor's_cache"}, {"score": 0.004622986791421418, "phrase": "cache_size"}, {"score": 0.004551199992546761, "phrase": "data_compression"}, {"score": 0.004522796947899013, "phrase": "last-level_caches"}, {"score": 0.0043560464825349275, "phrase": "primary_purpose"}, {"score": 0.004248294537944227, "phrase": "miss_rate"}, {"score": 0.004130243047687995, "phrase": "larger_logical_capacity"}, {"score": 0.004053362743156545, "phrase": "cacheline_size"}, {"score": 0.003990385737207714, "phrase": "achieved_compression_ratio"}, {"score": 0.00389164348807591, "phrase": "useful_hints"}, {"score": 0.0036782824183282823, "phrase": "increased_cache_performance"}, {"score": 0.003587235362628753, "phrase": "replacement_policies"}, {"score": 0.003531474887552889, "phrase": "existing_techniques"}, {"score": 0.003487489029588045, "phrase": "locality_information"}, {"score": 0.003401148484767927, "phrase": "size-aware_cache_management"}, {"score": 0.003125175084962886, "phrase": "novel_mechanism-called_effective_capacity_maximizer"}, {"score": 0.003047776993499616, "phrase": "energy_consumption"}, {"score": 0.0030003766828110277, "phrase": "proposed_technique"}, {"score": 0.0028268629720802772, "phrase": "ecm_eviction_scheduling"}, {"score": 0.002756832447586547, "phrase": "ecm_replacement"}, {"score": 0.0026969765105207457, "phrase": "extensive_simulations"}, {"score": 0.002655017467085696, "phrase": "real_applications"}, {"score": 0.00262191949451482, "phrase": "full-system_simulator"}, {"score": 0.0025730425700096365, "phrase": "compressed_cache_schemes"}, {"score": 0.0025569530630374995, "phrase": "conventional_locality-aware_cache_replacement_policies"}, {"score": 0.002493592357827492, "phrase": "average_effective_capacity_increase"}, {"score": 0.002371741293636516, "phrase": "dynamic"}, {"score": 0.002262518422441041, "phrase": "average_system_performance_improvements"}, {"score": 0.0021789194955896102, "phrase": "average_energy_consumption"}, {"score": 0.0021049977753042253, "phrase": "drrip."}], "paper_keywords": ["Cache", " compression", " data compression", " cache compression", " cache replacement policy"], "paper_abstract": "A practical way to increase the effective capacity of a microprocessor's cache, without physically increasing the cache size, is to employ data compression. Last-Level Caches (LLC) are particularly amenable to such compression schemes, since the primary purpose of the LLC is to minimize the miss rate, i.e., it directly benefits from a larger logical capacity. In compressed LLCs, the cacheline size varies depending on the achieved compression ratio. Our observations indicate that this size information gives useful hints when managing the cache (e.g., when selecting a victim), which can lead to increased cache performance. However, there are currently no replacement policies tailored to compressed LLCs; existing techniques focus primarily on locality information. This article introduces the concept of size-aware cache management as a way to maximize the performance of compressed caches. Upon analyzing the benefits of considering size information in the management of compressed caches, we propose a novel mechanism-called Effective Capacity Maximizer (ECM)-to further enhance the performance and energy consumption of compressed LLCs. The proposed technique revolves around four fundamental principles: ECM Insertion (ECM-I), ECM Promotion (ECM-P), ECM Eviction Scheduling (ECM-ES), and ECM Replacement (ECM-R). Extensive simulations with memory traces from real applications running on a full-system simulator demonstrate significant improvements compared to compressed cache schemes employing conventional locality-aware cache replacement policies. Specifically, our ECM shows an average effective capacity increase of 18.4 percent over the Least-Recently Used (LRU) policy, and 23.9 percent over the Dynamic Re-Reference Interval Prediction (DRRIP) [1] scheme. This translates into average system performance improvements of 7.2 percent over LRU and 4.2 percent over DRRIP. Moreover, the average energy consumption is also reduced by 5.9 percent over LRU and 3.8 percent over DRRIP.", "paper_title": "Size-Aware Cache Management for Compressed Cache Architectures", "paper_id": "WOS:000357929500018"}