{"auto_keywords": [{"score": 0.04718079151846455, "phrase": "unlabeled_instances"}, {"score": 0.01130505785276466, "phrase": "labeled_instances"}, {"score": 0.009986955185670615, "phrase": "labeled_set"}, {"score": 0.008866930145741135, "phrase": "aco_algorithm"}, {"score": 0.00481495049065317, "phrase": "classification_rules"}, {"score": 0.004765173195920904, "phrase": "semi-supervised_learning_methods"}, {"score": 0.004571133708117502, "phrase": "great_number"}, {"score": 0.004384960761826366, "phrase": "good_option"}, {"score": 0.004184526032558792, "phrase": "unlabeled_data"}, {"score": 0.0040771431404398855, "phrase": "labeling_instances"}, {"score": 0.003771185758226876, "phrase": "semi-supervised_self-training_algorithm"}, {"score": 0.0037321585095070483, "phrase": "ant-labeler"}, {"score": 0.003693533648584145, "phrase": "self-training_algorithms"}, {"score": 0.003598705020845104, "phrase": "learning_algorithms"}, {"score": 0.0031762308449055305, "phrase": "high_confidence"}, {"score": 0.0026609940110902666, "phrase": "supervised_learning_method"}, {"score": 0.002619748321217153, "phrase": "self-training_procedure"}, {"score": 0.002579140290642792, "phrase": "interpretable_rule-based_models"}, {"score": 0.002473895542219933, "phrase": "accurate_predictions"}, {"score": 0.002435542816459651, "phrase": "pheromone_matrix"}, {"score": 0.002385326947545194, "phrase": "different_executions"}, {"score": 0.002127046102734894, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "better_predictive_accuracy"}], "paper_keywords": ["Semi-supervised learning", " Self-training", " Ant colony optimization", " Classification rules"], "paper_abstract": "Semi-supervised learning methods create models from a few labeled instances and a great number of unlabeled instances. They appear as a good option in scenarios where there is a lot of unlabeled data and the process of labeling instances is expensive, such as those where most Web applications stand. This paper proposes a semi-supervised self-training algorithm called Ant-Labeler. Self-training algorithms take advantage of supervised learning algorithms to iteratively learn a model from the labeled instances and then use this model to classify unlabeled instances. The instances that receive labels with high confidence are moved from the unlabeled to the labeled set, and this process is repeated until a stopping criteria is met, such as labeling all unlabeled instances. Ant-Labeler uses an ACO algorithm as the supervised learning method in the self-training procedure to generate interpretable rule-based models-used as an ensemble to ensure accurate predictions. The pheromone matrix is reused across different executions of the ACO algorithm to avoid rebuilding the models from scratch every time the labeled set is updated. Results showed that the proposed algorithm obtains better predictive accuracy than three state-of-the-art algorithms in roughly half of the datasets on which it was tested, and the smaller the number of labeled instances, the better the Ant-Labeler performance.", "paper_title": "An ant colony-based semi-supervised approach for learning classification rules", "paper_id": "WOS:000365754600004"}