{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "bayesian_variable_selection"}, {"score": 0.004754600386388821, "phrase": "high_dimensional_binary_regression"}, {"score": 0.0046361494357125355, "phrase": "modern_data_mining"}, {"score": 0.004463958296056236, "phrase": "important_playground"}, {"score": 0.0044079880067019765, "phrase": "statistical_learning_techniques"}, {"score": 0.004244234872662471, "phrase": "input_variables"}, {"score": 0.00406083086983316, "phrase": "sample_size"}, {"score": 0.003984665807155269, "phrase": "training_data"}, {"score": 0.003909923690310641, "phrase": "supervised_learning"}, {"score": 0.0038608730929994696, "phrase": "logistic_regression"}, {"score": 0.0036246643358397272, "phrase": "binary_output"}, {"score": 0.00355665081072721, "phrase": "perceptron_classification_rules"}, {"score": 0.0034899090274046014, "phrase": "bayesian_inference"}, {"score": 0.003297080148944292, "phrase": "limited_number"}, {"score": 0.0032556926399045635, "phrase": "candidate_variables"}, {"score": 0.003095257144856771, "phrase": "popular_method"}, {"score": 0.00305639530600272, "phrase": "selection_indicators"}, {"score": 0.002887450071878095, "phrase": "posterior_estimates"}, {"score": 0.0028332303659245085, "phrase": "regression_functions"}, {"score": 0.0026597250663309385, "phrase": "true_regression_model"}, {"score": 0.0025285834281312705, "phrase": "aggregated_size"}, {"score": 0.002481085648977413, "phrase": "regression_coefficients"}, {"score": 0.0024038923350585962, "phrase": "estimated_regression_functions"}, {"score": 0.0023290951071300433, "phrase": "consistent_classifiers"}, {"score": 0.002228263890334039, "phrase": "future_binary_outputs"}, {"score": 0.0021726139494880653, "phrase": "theoretical_justifications"}, {"score": 0.0021317885325570604, "phrase": "recent_empirical_successes"}, {"score": 0.0021049977753042253, "phrase": "microarray_data_analysis"}], "paper_keywords": [""], "paper_abstract": "Modern data mining and bioinformatics have presented an important playground for statistical learning techniques, where the number of input variables is possibly much larger than the sample size of the training data. In supervised learning, logistic regression or probit regression can be used to model a binary output and form perceptron classification rules based on Bayesian inference. We use a prior to select a limited number of candidate variables to enter the model, applying a popular method with selection indicators. We show that this approach can induce posterior estimates of the regression functions that are consistently estimating the truth, if the true regression model is sparse in the sense that the aggregated size of the regression coefficients are bounded. The estimated regression functions therefore can also produce consistent classifiers that are asymptotically optimal for predicting future binary outputs. These provide theoretical justifications for some recent empirical successes in microarray data analysis.", "paper_title": "On the consistency of Bayesian variable selection for high dimensional binary regression and classification", "paper_id": "WOS:000240735600008"}