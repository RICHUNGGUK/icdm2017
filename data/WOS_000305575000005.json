{"auto_keywords": [{"score": 0.029725760425679804, "phrase": "bpi_estimator"}, {"score": 0.00481495049065317, "phrase": "nonlinear_functionals_of_densities"}, {"score": 0.004627441072541506, "phrase": "k-nearest_neighbor"}, {"score": 0.004290969772225093, "phrase": "nonlinear_functions"}, {"score": 0.004240116703465252, "phrase": "probability_density"}, {"score": 0.004173244935967375, "phrase": "shannon_entropy"}, {"score": 0.004140203942496321, "phrase": "renyi_entropy"}, {"score": 0.0037485269508155516, "phrase": "previous_k-nn_estimators"}, {"score": 0.0036456298076415652, "phrase": "proposed_estimator"}, {"score": 0.0036167509502276294, "phrase": "data-splitting_and_boundary_correction"}, {"score": 0.003573859492987647, "phrase": "lower_mean_square_error"}, {"score": 0.0031845067058084583, "phrase": "k-nn_density_estimate"}, {"score": 0.003097043800165555, "phrase": "empirical_estimation"}, {"score": 0.00295264307195965, "phrase": "statistical_properties"}, {"score": 0.0029292375702719468, "phrase": "k-nn_balls"}, {"score": 0.0027267406923803367, "phrase": "sample_size"}, {"score": 0.002478297391121455, "phrase": "optimal_choice"}, {"score": 0.0024586427394152196, "phrase": "tuning_parameters"}, {"score": 0.0023626775529531486, "phrase": "mean_square_error"}, {"score": 0.0023346234464901978, "phrase": "resultant_optimized_bpi_estimator"}, {"score": 0.0022886031992739126, "phrase": "lower_mean"}, {"score": 0.0022524395393089544, "phrase": "previous_k-nn_entropy_estimators"}, {"score": 0.0021992603252793995, "phrase": "central_limit_theorem"}, {"score": 0.0021049977753042253, "phrase": "tight_asymptotic_confidence_intervals"}], "paper_keywords": ["Adaptive estimators", " bias and variance tradeoff", " bipartite k-nearest neighbor (k-NN) graphs", " concentration bounds", " convergence rates", " data-splitting estimators", " entropy estimation"], "paper_abstract": "This paper introduces a class of k-nearest neighbor (k-NN) estimators called bipartite plug-in (BPI) estimators for estimating integrals of nonlinear functions of a probability density, such as Shannon entropy and Renyi entropy. The density is assumed to be smooth, have bounded support, and be uniformly bounded from below on this set. Unlike previous k-NN estimators of nonlinear density functionals, the proposed estimator uses data-splitting and boundary correction to achieve lower mean square error. Specifically, we assume that i.i.d. samples from the density are split into two pieces of cardinality and, respectively, with samples used for computing a k-NN density estimate and the remaining samples used for empirical estimation of the integral of the density functional. By studying the statistical properties of k-NN balls, explicit rates for the bias and variance of the BPI estimator are derived in terms of the sample size, the dimension of the samples, and the underlying probability distribution. Based on these results, it is possible to specify optimal choice of tuning parameters, for maximizing the rate of decrease of the mean square error. The resultant optimized BPI estimator converges faster and achieves lower mean squared error than previous k-NN entropy estimators. In addition, a central limit theorem is established for the BPI estimator that allows us to specify tight asymptotic confidence intervals.", "paper_title": "Estimation of Nonlinear Functionals of Densities With Confidence", "paper_id": "WOS:000305575000005"}