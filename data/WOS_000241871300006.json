{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "ordinal_vc-dimension"}, {"score": 0.009516939163429132, "phrase": "inductive_inference"}, {"score": 0.00809299825892853, "phrase": "boolean_operators"}, {"score": 0.0066482613202208185, "phrase": "predictive_complexity"}, {"score": 0.004666598697538815, "phrase": "classical_notion"}, {"score": 0.004637478829869904, "phrase": "vapnik-chemovenkis"}, {"score": 0.004466519142879166, "phrase": "logical_learning_paradigms"}, {"score": 0.004369705395855237, "phrase": "numerical_learning_paradigms"}, {"score": 0.004274981117736524, "phrase": "logical_learning_paradigm"}, {"score": 0.0042085753823566125, "phrase": "set_w"}, {"score": 0.004053362743156545, "phrase": "first-order_formulas"}, {"score": 0.0037834649508375544, "phrase": "natural_topology_w"}, {"score": 0.003759835076089236, "phrase": "w._we"}, {"score": 0.0035648266669315943, "phrase": "perfect_characterization"}, {"score": 0.0031646057802503526, "phrase": "natural_interpretation"}, {"score": 0.0030862341714020694, "phrase": "logical_setting"}, {"score": 0.0029909851863915283, "phrase": "selective_complexity"}, {"score": 0.0029169007179202164, "phrase": "mind_change_complexity"}, {"score": 0.002756832447586547, "phrase": "crucial_role"}, {"score": 0.002655017467085696, "phrase": "computable_setting"}, {"score": 0.0026384167183586015, "phrase": "effective_versions"}, {"score": 0.002613709503583786, "phrase": "complexity_measures"}, {"score": 0.002447101794397243, "phrase": "effective_ordinal_vc-dimension"}, {"score": 0.0023200213317312577, "phrase": "better_note"}, {"score": 0.002262518422441041, "phrase": "effective_notions"}, {"score": 0.0021652891317252994, "phrase": "noncomputable_version"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["generalized VC-dimension", " inductive inference", " predictive complexity"], "paper_abstract": "We generalize the classical notion of Vapnik-Chemovenkis (VC) dimension to ordinal VC-dimension, in the context of logical learning paradigms. Logical learning paradigms encompass the numerical learning paradigms commonly studied in Inductive Inference. A logical learning paradigm is defined as a set W of structures over some vocabulary, and a set D of first-order formulas that represent data. The sets of models of phi in W, where phi varies over D, generate a natural topology W over W. We show that if D is closed under boolean operators, then the notion of ordinal VC-dimension offers a perfect characterization for the problem of predicting the truth of the members of D in a member of W, with an ordinal bound on the number of mistakes. This shows that the notion of VC-dimension has a natural interpretation in Inductive Inference, when cast into a logical setting. We also study the relationships between predictive complexity, selective complexity-a variation on predictive complexity-and mind change complexity. The assumptions that D is closed under boolean operators and that W is compact often play a crucial role to establish connections between these concepts. We then consider a computable setting with effective versions of the complexity measures, and show that the equivalence between ordinal VC-dimension and predictive complexity fails. More precisely, we prove that the effective ordinal VC-dimension of a paradigm can be defined when all other effective notions of complexity are undefined. On a better note, when W is compact, all effective notions of complexity are defined, though they are not related as in the noncomputable version of the framework. (C) 2006 Elsevier B.V. All rights reserved.", "paper_title": "On ordinal VC-dimension and some notions of complexity", "paper_id": "WOS:000241871300006"}