{"auto_keywords": [{"score": 0.036817037816123374, "phrase": "facial_feature_points"}, {"score": 0.00937932581394785, "phrase": "facial_expressions"}, {"score": 0.00481495049065317, "phrase": "misalignment_detection"}, {"score": 0.004723847090183984, "phrase": "talking_face"}, {"score": 0.004678940805063634, "phrase": "video_sequence"}, {"score": 0.004634459424542272, "phrase": "facial_appearance_changes"}, {"score": 0.00459039896577789, "phrase": "video_sequences"}, {"score": 0.004525088883501587, "phrase": "nonstationary_data_problem"}, {"score": 0.00409310463335204, "phrase": "fixed_appearance_models"}, {"score": 0.004034841146538891, "phrase": "target_object"}, {"score": 0.00386496062825374, "phrase": "uncontrolled_environments"}, {"score": 0.003828187766572941, "phrase": "existing_adaptive_appearance_models"}, {"score": 0.003253791343615811, "phrase": "significant_expression_changes"}, {"score": 0.0029711052657054463, "phrase": "robust_tracking"}, {"score": 0.0028187792953857957, "phrase": "stochastic_approach"}, {"score": 0.002739001004969201, "phrase": "local_minimum"}, {"score": 0.002661474607275484, "phrase": "feature_points"}, {"score": 0.0026235360404084137, "phrase": "deterministic_approach"}, {"score": 0.0025861368747446324, "phrase": "tracked_results"}, {"score": 0.002537097217899195, "phrase": "offline_learning_approach"}, {"score": 0.0024771000236931836, "phrase": "poorly_aligned_targets"}, {"score": 0.0024185182010749273, "phrase": "proposed_tracker"}, {"score": 0.002350041573006264, "phrase": "appearance_changes"}, {"score": 0.00230546852838578, "phrase": "experiment_results"}, {"score": 0.0022509365210787993, "phrase": "long_video_sequences"}, {"score": 0.0022188370388602813, "phrase": "wide_range"}, {"score": 0.002176747110119268, "phrase": "head_movement"}], "paper_keywords": ["Subspace update", " Incremental PCA", " Adaptive appearance models", " Particle filter", " Support vector machine", " Adaboost"], "paper_abstract": "Facial appearance changes in video sequences represent a nonstationary data problem, because of factors such as variation in pose, illumination and facial expressions. While most algorithm, that employ fixed appearance models of the target object, are not robust to track objects in uncontrolled environments. Existing Adaptive Appearance Models (AAMs) approaches solve this problem to an extent. However, they do not adequately track facial feature points, such as those relating to the eyes or mouth in the presence of significant expression changes. In this paper, we propose a method to combine an online and an offline learning for robust tracking of facial feature points. Our method firstly estimates facial feature points globally with a stochastic approach which allows to escape from local minimum. We then refine the feature points with a deterministic approach. The tracked results are filtered by offline learning approach to ensure rejection of poorly aligned targets. This allows the proposed tracker to significantly improves robustness against appearance changes and occlusions. Experiment results on tracking facial feature points in long video sequences with a wide range of facial expressions in head movement demonstrate the effectiveness and robustness of our tracker. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Online subspace learning using a misalignment detection for tracking a talking face in video sequence", "paper_id": "WOS:000279134100012"}