{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "deterministic_error_analysis_of_support_vector_regression"}, {"score": 0.004278807087784626, "phrase": "new_technique"}, {"score": 0.0040334640350288, "phrase": "kernel-based_regression_problems"}, {"score": 0.003500313767208306, "phrase": "machine_learning_problems"}, {"score": 0.0034185505275141077, "phrase": "penalty_terms"}, {"score": 0.0031845067058084583, "phrase": "sobolev_spaces"}, {"score": 0.0030017207229983385, "phrase": "explicit_deterministic_results"}, {"score": 0.002897109710213723, "phrase": "worst_case_behaviour"}, {"score": 0.002455009746748708, "phrase": "regularization_parameters"}, {"score": 0.0023694065215394593, "phrase": "best_possible_approximation_orders"}, {"score": 0.0021049977753042253, "phrase": "numerical_examples"}], "paper_keywords": ["sampling inequality", " radial basis functions", " approximation theory", " reproducing kernel Hilbert space", " Sobolev space"], "paper_abstract": "We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of epsilon- and nu-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples.", "paper_title": "Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods", "paper_id": "WOS:000272346100005"}