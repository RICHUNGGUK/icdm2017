{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "string_quartet"}, {"score": 0.004738472391439408, "phrase": "head_movements"}, {"score": 0.004693167092003573, "phrase": "present_study"}, {"score": 0.0046632033374901715, "phrase": "expressive_non-verbal_interaction"}, {"score": 0.0045598218623312675, "phrase": "behavioral_features"}, {"score": 0.0045162169185355, "phrase": "individual_and_group_levels"}, {"score": 0.00433200902751133, "phrase": "head_movement"}, {"score": 0.004063116219782941, "phrase": "different_performance_conditions"}, {"score": 0.003922378980031565, "phrase": "global_scale"}, {"score": 0.003872414709826833, "phrase": "local_scale"}, {"score": 0.0037024804835018373, "phrase": "ecological_setting_show"}, {"score": 0.0031845067058084583, "phrase": "alternative_interpretations"}, {"score": 0.0031540120386134058, "phrase": "music_score"}, {"score": 0.003054450075234415, "phrase": "global_data_analysis"}, {"score": 0.003025197043767572, "phrase": "discriminative_power"}, {"score": 0.002958021635132416, "phrase": "statistical_tests"}, {"score": 0.002901627749562029, "phrase": "local_data_analysis"}, {"score": 0.0028738341066290815, "phrase": "larger_amount"}, {"score": 0.0026693851900137953, "phrase": "svm_classifier"}, {"score": 0.0026438102161263567, "phrase": "binary_classification"}, {"score": 0.002495411946660959, "phrase": "global_analysis"}, {"score": 0.0024478159375394878, "phrase": "similar_way"}, {"score": 0.0024243586247372087, "phrase": "local_analysis"}, {"score": 0.002310393431454436, "phrase": "paper_demonstrate"}, {"score": 0.0022159566043745724, "phrase": "successfully_classified_examples"}, {"score": 0.0021597662797996843, "phrase": "training_phase"}, {"score": 0.002145942273032774, "phrase": "similar_results"}], "paper_keywords": ["Automated analysis of non-verbal behavior", " Head ancillary gestures", " Focus of attention", " Feature selection", " Support vector machines"], "paper_abstract": "The present study investigates expressive non-verbal interaction in the musical context starting from behavioral features extracted at individual and group levels. Four groups of features are defined, which are related to head movement and direction, and may help gaining insight on the expressivity and cohesion of the performance, discriminating between different performance conditions. Then, the features are evaluated both at a global scale and at a local scale. The findings obtained from the analysis of a string quartet recorded in an ecological setting show that using these features alone or in their combination may help in distinguishing between two types of performance: (a) a concert-like condition, where all musicians aim at performing at best, (b) a perturbed one, where the 1 violinist devises alternative interpretations of the music score without discussing them with the other musicians. In the global data analysis, the discriminative power of the features is investigated through statistical tests. Then, in the local data analysis, a larger amount of data is used to exploit more sophisticated machine learning techniques to select suitable subsets of the features, which are then used to train an SVM classifier to perform binary classification. Interestingly, the features whose discriminative power is evaluated as large (respectively, small) in the global analysis are also evaluated in a similar way in the local analysis. When used together, the 22 features that have been defined in the paper demonstrate to be efficient for classification, leading to a percentage of about 90 % successfully classified examples among the ones not used in the training phase. Similar results are obtained considering only a subset of 15 features.", "paper_title": "Expressive non-verbal interaction in a string quartet: an analysis through head movements", "paper_id": "WOS:000351472400006"}