{"auto_keywords": [{"score": 0.049593242225692294, "phrase": "speech-driven_facial_animation"}, {"score": 0.04564472491148936, "phrase": "orofacial_motion"}, {"score": 0.00481495049065317, "phrase": "acoustic_coding_models"}, {"score": 0.004622450915803531, "phrase": "thorough_experimental_comparison"}, {"score": 0.004195418010131832, "phrase": "linear_predictive_coding"}, {"score": 0.004152818376630929, "phrase": "linear_spectral_frequencies"}, {"score": 0.003986684503518077, "phrase": "speech_production_system"}, {"score": 0.0038862297330905836, "phrase": "mel_frequency"}, {"score": 0.0036740168936985314, "phrase": "perceptual_cues"}, {"score": 0.003545024951402821, "phrase": "spectral_energy"}, {"score": 0.003509006038574667, "phrase": "fundamental_frequency"}, {"score": 0.003438058620195309, "phrase": "prosodic_aspects"}, {"score": 0.0032836104153151973, "phrase": "previous_models"}, {"score": 0.0031845067058084583, "phrase": "novel_supervised_procedure"}, {"score": 0.0031360786664165093, "phrase": "fisher's_linear_discriminants"}, {"score": 0.003088384809415812, "phrase": "acoustic_information"}, {"score": 0.0030414140732789186, "phrase": "low-dimensional_subspace"}, {"score": 0.0029798924689472014, "phrase": "different_orofacial_configurations"}, {"score": 0.002889928791032578, "phrase": "speech_acoustics"}, {"score": 0.0028170314398963704, "phrase": "non-parametric_k-nearest-neighbors_procedure"}, {"score": 0.002731971103291716, "phrase": "audio-visual_mapping"}, {"score": 0.0026630475331025955, "phrase": "spatial_locality"}, {"score": 0.0025433263646920364, "phrase": "hybrid_use"}, {"score": 0.0024918548761791435, "phrase": "perceptual_and_prosodic_features"}, {"score": 0.00240426758768164, "phrase": "supervised_dimensionality-reduction_procedure"}, {"score": 0.0023197517832705297, "phrase": "individual_acoustic_model"}, {"score": 0.0021705796895687864, "phrase": "timit_compact_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["speech-driven facial animation", " audio-visual mapping", " linear discriminants analysis"], "paper_abstract": "This article presents a thorough experimental comparison of several acoustic modeling techniques by their ability to capture information related to orofacial motion. These models include (1) Linear Predictive Coding and Linear Spectral Frequencies, which model the dynamics of the speech production system, (2) Mel Frequency Cepstral Coefficients and Perceptual Critical Feature Bands, which encode perceptual cues of speech, (3) spectral energy and fundamental frequency, which capture prosodic aspects, and (4) two hybrid methods that combine information from the previous models. We also consider a novel supervised procedure based on Fisher's Linear Discriminants to project acoustic information onto a low-dimensional subspace that best discriminates different orofacial configurations. Prediction of orofacial motion from speech acoustics is performed using a non-parametric k-nearest-neighbors procedure. The sensitivity of this audio-visual mapping to coarticulation effects and spatial locality is thoroughly investigated. Our results indicate that the hybrid use of articulatory, perceptual and prosodic features of speech, combined with a supervised dimensionality-reduction procedure, is able to outperform any individual acoustic model for speech-driven facial animation. These results are validated on the 450 sentences of the TIMIT compact dataset. (C) 2005 Elsevier B.V. All rights reserved.", "paper_title": "A comparison of acoustic coding models for speech-driven facial animation", "paper_id": "WOS:000238211900002"}