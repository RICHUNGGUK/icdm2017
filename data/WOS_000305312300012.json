{"auto_keywords": [{"score": 0.04564158107788786, "phrase": "ewa"}, {"score": 0.015719709579057044, "phrase": "langevin_monte-carlo"}, {"score": 0.004631758267671427, "phrase": "deterministic_design"}, {"score": 0.0045959609346817535, "phrase": "independent_random_errors"}, {"score": 0.00447282529270722, "phrase": "sharp_pac-bayesian_type"}, {"score": 0.00440394242852293, "phrase": "exponentially_weighted_aggregate"}, {"score": 0.004302593474473502, "phrase": "expected_squared_empirical_loss"}, {"score": 0.004236321075918639, "phrase": "broad_class"}, {"score": 0.0042035670399069485, "phrase": "noise_distributions"}, {"score": 0.004171065188039856, "phrase": "presented_bound"}, {"score": 0.004090901264701185, "phrase": "temperature_parameter_b"}, {"score": 0.0038148090091532933, "phrase": "noise_variance"}, {"score": 0.003770633265791941, "phrase": "remarkable_feature"}, {"score": 0.003612971658533784, "phrase": "unbounded_regression_functions"}, {"score": 0.0035297609635321203, "phrase": "temperature_parameter"}, {"score": 0.0034618794528555063, "phrase": "noise_level"}, {"score": 0.003228101865545105, "phrase": "finite-dimensional_linear_space"}, {"score": 0.002940697344380201, "phrase": "sample_size"}, {"score": 0.002861782260525497, "phrase": "true_regression_function"}, {"score": 0.00278497898719426, "phrase": "sparse_linear_combination"}, {"score": 0.0026997176829763746, "phrase": "sparsity_scenario"}, {"score": 0.0025172781808293827, "phrase": "sparsity_oracle_inequality"}, {"score": 0.0024977819584875573, "phrase": "leading_constant_one"}, {"score": 0.002356282242320051, "phrase": "number_m"}, {"score": 0.0023380300046435187, "phrase": "aggregated_functions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Sparse learning", " Regression estimation", " Logistic regression", " Oracle inequalities", " Sparsity prior", " Langevin", " Monte-Carlo"], "paper_abstract": "We consider the problem of regression learning for deterministic design and independent random errors. We start by proving a sharp PAC-Bayesian type bound for the exponentially weighted aggregate (EWA) under the expected squared empirical loss. For a broad class of noise distributions the presented bound is valid whenever the temperature parameter B of the EWA is larger than or equal to 4 sigma(2), where sigma(2) is the noise variance. A remarkable feature of this result is that it is valid even for unbounded regression functions and the choice of the temperature parameter depends exclusively on the noise level. Next, we apply this general bound to the problem of aggregating the elements of a finite-dimensional linear space spanned by a dictionary of functions phi(1), ... , phi(M). We allow M to be much larger than the sample size n but we assume that the true regression function can be well approximated by a sparse linear combination of functions phi(j). Under this sparsity scenario, we propose an EWA with a heavy tailed prior and we show that it satisfies a sparsity oracle inequality with leading constant one. Finally, we propose several Langevin Monte-Carlo algorithms to approximately compute such an EWA when the number M of aggregated functions can be large. We discuss in some detail the convergence of these algorithms and present numerical experiments that confirm our theoretical findings. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Sparse regression learning by aggregation and Langevin Monte-Carlo", "paper_id": "WOS:000305312300012"}