{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "hybrid_bayesian_networks"}, {"score": 0.03388973147589285, "phrase": "mop_approximations"}, {"score": 0.014463701353012628, "phrase": "shenoy"}, {"score": 0.014355628656829022, "phrase": "west"}, {"score": 0.00830422329001964, "phrase": "lip_method"}, {"score": 0.0041001740903338834, "phrase": "multidimensional_functions"}, {"score": 0.0034118234596993836, "phrase": "multi-dimensional_linear_deterministic_conditionals"}, {"score": 0.0031723302551782324, "phrase": "probability_density_functions"}, {"score": 0.0031002400518251936, "phrase": "multi-dimensional_conditional_linear_gaussian_distributions"}, {"score": 0.002983700956478823, "phrase": "univariate_standard_normal_distribution"}, {"score": 0.0028715299874549245, "phrase": "taylor_series_expansion"}, {"score": 0.0028496055358577512, "phrase": "differentiable_functions"}, {"score": 0.002700738055025905, "phrase": "new_method"}, {"score": 0.0026393362602995254, "phrase": "lagrange_interpolating_polynomials"}, {"score": 0.0025892330566358503, "phrase": "chebyshev_points"}, {"score": 0.002472822097373027, "phrase": "efficient_mop_approximations"}, {"score": 0.0023981301755351607, "phrase": "conditional_linear_gaussian_pdfs"}, {"score": 0.0023525950486853937, "phrase": "conditional_log-normal_pdfs"}, {"score": 0.0022640963589007457, "phrase": "hyper-rhombus_condition"}, {"score": 0.0022382002476510573, "phrase": "hypercube_condition"}, {"score": 0.0021539955106204354, "phrase": "taylor_series_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Inference in hybrid Bayesian networks", " Mixtures of polynomials", " Conditional linear Gaussian distributions", " Lagrange interpolating polynomials", " Chebyshev points", " Conditional log-normal distributions"], "paper_abstract": "We discuss two issues in using mixtures of polynomials (MOPS) for inference in hybrid Bayesian networks. MOPs were proposed by Shenoy and West for mitigating the problem of integration in inference in hybrid Bayesian networks. First, in defining MOP for multidimensional functions, one requirement is that the pieces where the polynomials are defined are hypercubes. In this paper, we discuss relaxing this condition so that each piece is defined on regions called hyper-rhombuses. This relaxation means that MOPs are closed under transformations required for multi-dimensional linear deterministic conditionals, such as Z = X + Y, etc. Also, this relaxation allows us to construct MOP approximations of the probability density functions (PDFs) of the multi-dimensional conditional linear Gaussian distributions using a MOP approximation of the PDF of the univariate standard normal distribution. Second, Shenoy and West suggest using the Taylor series expansion of differentiable functions for finding MOP approximations of PDFs. In this paper, we describe a new method for finding MOP approximations based on Lagrange interpolating polynomials (LIP) with Chebyshev points. We describe how the LIP method can be used to find efficient MOP approximations of PDFs. We illustrate our methods using conditional linear Gaussian PDFs in one, two, and three dimensions, and conditional log-normal PDFs in one and two dimensions. We compare the efficiencies of the hyper-rhombus condition with the hypercube condition. Also, we compare the LIP method with the Taylor series method. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Two issues in using mixtures of polynomials for inference in hybrid Bayesian networks", "paper_id": "WOS:000304073300008"}