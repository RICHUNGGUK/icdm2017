{"auto_keywords": [{"score": 0.04854250213020111, "phrase": "transductive_learning_algorithms"}, {"score": 0.00481495049065317, "phrase": "new_error"}, {"score": 0.0045108906244892165, "phrase": "slack_term"}, {"score": 0.004366106816225457, "phrase": "new_bound"}, {"score": 0.004090274535243564, "phrase": "relaxed_notion"}, {"score": 0.004002242925856335, "phrase": "transductive_stability"}, {"score": 0.003115908199991072, "phrase": "novel_concentration_inequality"}, {"score": 0.0030487828775502563, "phrase": "symmetric_functions"}, {"score": 0.0027943943921568456, "phrase": "simple_sampling_technique"}, {"score": 0.0026175952455594277, "phrase": "high_probability"}, {"score": 0.0025334249463305875, "phrase": "weak_stability"}, {"score": 0.0021049977753042253, "phrase": "well_known_transductive_learning_algorithm"}], "paper_keywords": [""], "paper_abstract": "We develop a new error bound for transductive learning algorithms. The slack term in the new bound is a function of a relaxed notion of transductive stability, which measures the sensitivity of the algorithm to most pairwise exchanges of training and test set points. Our bound is based on a novel concentration inequality for symmetric functions of permutations. We also present a simple sampling technique that can estimate, with high probability, the weak stability of transductive learning algorithms with respect to a given dataset. We demonstrate the usefulness of our estimation technique on a well known transductive learning algorithm.", "paper_title": "Stable transductive learning", "paper_id": "WOS:000239587900006"}