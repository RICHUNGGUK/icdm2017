{"auto_keywords": [{"score": 0.04541703179456642, "phrase": "metadata_servers"}, {"score": 0.013570000116347545, "phrase": "hba"}, {"score": 0.004815133171259446, "phrase": "distributed"}, {"score": 0.004726382602809228, "phrase": "large_cluster-based_storage_systems"}, {"score": 0.00466102244911872, "phrase": "efficient_and_distributed_scheme"}, {"score": 0.004554082077300675, "phrase": "file_lookup"}, {"score": 0.0044495843279816075, "phrase": "metadata_management"}, {"score": 0.004228016636379321, "phrase": "novel_technique"}, {"score": 0.00418892838079239, "phrase": "hierarchical_bloom_filter_arrays"}, {"score": 0.003907013105134715, "phrase": "probabilistic_arrays"}, {"score": 0.0038173207878171725, "phrase": "bloom"}, {"score": 0.003747020072136732, "phrase": "different_levels"}, {"score": 0.003610292749660581, "phrase": "metadata_server"}, {"score": 0.0035273744062641606, "phrase": "lower_accuracy"}, {"score": 0.003414467360488913, "phrase": "entire_metadata"}, {"score": 0.003336031521101184, "phrase": "significantly_reduced_memory_overhead"}, {"score": 0.0032292292848296617, "phrase": "higher_accuracy"}, {"score": 0.0031845067058084583, "phrase": "partial_distribution_information"}, {"score": 0.003125835572347772, "phrase": "temporal_locality"}, {"score": 0.003096905353118796, "phrase": "file_access_patterns"}, {"score": 0.002942495422585405, "phrase": "fast_local_lookups"}, {"score": 0.0028615323526689582, "phrase": "extensive_trace-driven_simulations"}, {"score": 0.0028088266768503648, "phrase": "linux"}, {"score": 0.0027827907725673845, "phrase": "simulation_results"}, {"score": 0.0025832180231667853, "phrase": "file_systems"}, {"score": 0.0023867931451842087, "phrase": "petabyte_scale"}, {"score": 0.002267708463966758, "phrase": "metadata_operation_time"}, {"score": 0.002236272403824371, "phrase": "single-metadata-server_architecture"}], "paper_keywords": ["distributed file systems", " file system management", " metadata management", " Bloom filter"], "paper_abstract": "An efficient and distributed scheme for file mapping or file lookup is critical in decentralizing metadata management within a group of metadata servers. This paper presents a novel technique called Hierarchical Bloom Filter Arrays (HBA) to map filenames to the metadata servers holding their metadata. Two levels of probabilistic arrays, namely, the Bloom filter arrays with different levels of accuracies, are used on each metadata server. One array, with lower accuracy and representing the distribution of the entire metadata, trades accuracy for significantly reduced memory overhead, whereas the other array, with higher accuracy, caches partial distribution information and exploits the temporal locality of file access patterns. Both arrays are replicated to all metadata servers to support fast local lookups. We evaluate HBA through extensive trace-driven simulations and implementation in Linux. Simulation results show our HBA design to be highly effective and efficient in improving the performance and scalability of file systems in clusters with 1,000 to 10,000 nodes (or superclusters) and with the amount of data in the petabyte scale or higher. Our implementation indicates that HBA can reduce the metadata operation time of a single-metadata-server architecture by a factor of up to 43.9 when the system is configured with 16 metadata servers.", "paper_title": "HBA: Distributed metadata management for large cluster-based storage systems", "paper_id": "WOS:000255199500003"}