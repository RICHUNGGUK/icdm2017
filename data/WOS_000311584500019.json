{"auto_keywords": [{"score": 0.04060341662906797, "phrase": "magnitude_force"}, {"score": 0.015640869977860933, "phrase": "facial_expression"}, {"score": 0.013014855256672328, "phrase": "joystick_wavelength"}, {"score": 0.012916790127856683, "phrase": "visible_light_spectrum"}, {"score": 0.011472359439069815, "phrase": "emotion_value"}, {"score": 0.008642770045300558, "phrase": "mind_controller"}, {"score": 0.00481495049065317, "phrase": "haptic_feedback_to"}, {"score": 0.00458790348934943, "phrase": "facial_expressions"}, {"score": 0.004461359547152617, "phrase": "haptic_tactile_emotions"}, {"score": 0.004327271696245578, "phrase": "haptic_feedback"}, {"score": 0.004081385289516914, "phrase": "emotionvibration_mapping"}, {"score": 0.003898700951496652, "phrase": "linear_equation"}, {"score": 0.003820144626025562, "phrase": "joystick_wavelength_and_magnitude_force_data"}, {"score": 0.003639835714755229, "phrase": "linear_interpolation_method"}, {"score": 0.003548368289666932, "phrase": "complete_classification_table"}, {"score": 0.003476846303757408, "phrase": "facial_expression_synthesis"}, {"score": 0.0034328764635642313, "phrase": "action_units"}, {"score": 0.0032958629522876993, "phrase": "facial_action_coding_system"}, {"score": 0.003245900020944865, "phrase": "action_unit"}, {"score": 0.0031804552179204265, "phrase": "face_region"}, {"score": 0.003156253963939272, "phrase": "special_lighting_colour"}, {"score": 0.0030534855146887047, "phrase": "emotion_classification_table"}, {"score": 0.003022540719364359, "phrase": "emotionvibration_mapping_process"}, {"score": 0.002916675612747027, "phrase": "acoustic_effect"}, {"score": 0.0028944757265206332, "phrase": "emotional_sound"}, {"score": 0.002857849698346116, "phrase": "loudness_level"}, {"score": 0.0027718217591792644, "phrase": "haptic_device"}, {"score": 0.0027021079763086394, "phrase": "visual_and_acoustic_cues"}, {"score": 0.0025810006125483835, "phrase": "user_emotions"}, {"score": 0.0025678829712650437, "phrase": "real_time"}, {"score": 0.0024905619050479473, "phrase": "brain_activity"}, {"score": 0.0023309080951251335, "phrase": "strongly_positive_responses"}, {"score": 0.0022264006157232366, "phrase": "emotion_representation"}, {"score": 0.002175913488493802, "phrase": "high_magnitude_force"}, {"score": 0.0021211554492685442, "phrase": "low_magnitude_force"}], "paper_keywords": ["3D humanoid models", " Facial expression", " Action units", " Magnitude force"], "paper_abstract": "Most of the latest 3D humanoid models that are currently available can produce emotions only through facial expressions, gestures and voice. Only a few humanoid models are capable of manipulating haptic tactile emotions through vibrations. This study proposes a system, in which haptic feedback is integrated based on visual, acoustic, and haptic cues. This integrated framework is based on two major techniques: emotionvibration mapping and facial expression synthesis. In emotionvibration mapping, mapping is carried out by first scaling the joystick wavelength to the visible light spectrum. Then, a linear equation describing the magnitude force is created by using joystick wavelength and magnitude force data. Finally, the wavelength of visible light spectrum is used as parameter to compute the joystick wavelength by using a linear interpolation method, and then, emotions are generated and a complete classification table is stored for each emotion value. In facial expression synthesis, a combination of Action Units (A Us) is used to generate certain emotion expressions in the 3D humanoid model face based on Facial Action Coding System (FACS). Each action unit is characterized by its specific face region, and each face region has a special lighting colour to differentiate its appearance. The colour of light is obtained from the emotion classification table generated in the emotionvibration mapping process. Furthermore, the integration proceeds with rendering the facial expression, generating an acoustic effect from an emotional sound, and adjusting the loudness level according to the emotion value. Finally, the magnitude force of the haptic device, which is simultaneously adjusted after the synchronization of visual and acoustic cues, is integrated. In this study, a mind controller and a glove are used to capture user emotions in real time. The mind controller determines the type of emotion according to the brain activity of the user, whereas the glove controls the intensity of an emotion. The results from experiment show that 67% of the participants gave strongly positive responses to the system. In addition, 15 of 21 (71%) participants agreed with the classification of the magnitude force into the emotion representation. Most of the users remarked that a high magnitude force created a sensation similar to anger, whereas a low magnitude force created a more relaxing sensation.", "paper_title": "AN INTEGRATION FRAMEWORK FOR HAPTIC FEEDBACK TO IMPROVE FACIAL EXPRESSION ON VIRTUAL HUMAN", "paper_id": "WOS:000311584500019"}