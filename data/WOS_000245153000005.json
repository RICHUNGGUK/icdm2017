{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "large_dense_parallel"}, {"score": 0.004738078275822989, "phrase": "core_calculations"}, {"score": 0.004539005582217309, "phrase": "distributed_packed_storage_format"}, {"score": 0.004371660146981191, "phrase": "triangular_structure"}, {"score": 0.004301834610818624, "phrase": "dense_matrix"}, {"score": 0.0039055990919333082, "phrase": "full_storage"}, {"score": 0.0038431888954930083, "phrase": "wide_range"}, {"score": 0.0035078427258024613, "phrase": "sequential_linear_algebra_libraries"}, {"score": 0.0032536902910768957, "phrase": "packed_matrices"}, {"score": 0.0032016639006812826, "phrase": "currently_available_parallel_distributed_libraries"}, {"score": 0.003066945490483524, "phrase": "existing_scalapack_computational_kernels"}, {"score": 0.002890887778279196, "phrase": "easy_portability"}, {"score": 0.0027991418754844347, "phrase": "efficient_re-use"}, {"score": 0.0026384167183586015, "phrase": "cholesky_factorization_show"}, {"score": 0.0024868972942701582, "phrase": "scalapack_full_storage_algorithm"}, {"score": 0.002447101794397243, "phrase": "small_number"}, {"score": 0.0023694065215394593, "phrase": "larger_number"}, {"score": 0.002306543826130521, "phrase": "scalapack_full_storage_routine"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["scientific computing", " dense linear algebra", " parallel distributed algorithms", " ScaLAPACK", " packed storage format", " Cholesky factorization", " QR factorization"], "paper_abstract": "In this paper we propose a distributed packed storage format that exploits the symmetry or the triangular structure of a dense matrix. This format stores only half of the matrix while maintaining most of the efficiency compared with a full storage for a wide range of operations. This work has been motivated by the fact that, in contrast to sequential linear algebra libraries (e.g. LAPACK), there is no routine or format that handles packed matrices in the currently available parallel distributed libraries. The proposed algorithms exclusively use the existing ScaLAPACK computational kernels, which proves the generality of the approach, provides easy portability of the code and provides efficient re-use of existing software. The performance results obtained for the Cholesky factorization show that our packed format performs as good as or better than the ScaLAPACK full storage algorithm for a small number of processors. For a larger number of processors, the ScaLAPACK full storage routine performs slightly better until each processor runs out of memory. Copyright (c) 2006 John Wiley & Sons, Ltd.", "paper_title": "A distributed packed storage for large dense parallel in-core calculations", "paper_id": "WOS:000245153000005"}