{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "classifier_systems"}, {"score": 0.004771750408749566, "phrase": "pomdp_problems"}, {"score": 0.004743164872278112, "phrase": "reinforcement_learning"}, {"score": 0.004740379928311751, "phrase": "new_architecture"}, {"score": 0.0047006058139556686, "phrase": "learning_paradigm"}, {"score": 0.004586269232727432, "phrase": "partially_observable_environments"}, {"score": 0.004534136929881113, "phrase": "unknown_environment"}, {"score": 0.004439753960631346, "phrase": "rl-based_agent"}, {"score": 0.004218602458288531, "phrase": "reinforcement_signal"}, {"score": 0.003913155833767365, "phrase": "overall_environmental_reward"}, {"score": 0.0037972380063356972, "phrase": "rl_area"}, {"score": 0.0037181370924317834, "phrase": "sensory_ability"}, {"score": 0.003356675879109443, "phrase": "actual_environmental_state"}, {"score": 0.003276848046269042, "phrase": "optimal_action"}, {"score": 0.0032474028991861543, "phrase": "environmental_states"}, {"score": 0.0032085525502105836, "phrase": "extended_mechanism"}, {"score": 0.002958159666874001, "phrase": "most-used_approaches"}, {"score": 0.0029052181875265995, "phrase": "evolutionary_learning_approach"}, {"score": 0.002861822702206142, "phrase": "most-used_techniques"}, {"score": 0.0027519956065269126, "phrase": "state-action-reward_mappings"}, {"score": 0.0024842242921857705, "phrase": "novel_method"}, {"score": 0.002461883922643093, "phrase": "aliased_states"}, {"score": 0.00240328454980006, "phrase": "multiple_instances"}, {"score": 0.0022833403088699175, "phrase": "well-known_benchmark_problems"}, {"score": 0.0022289812518790824, "phrase": "best_classifier_systems"}, {"score": 0.002117717754001824, "phrase": "best_techniques"}], "paper_keywords": ["learning classifier systems", " POMDP environments", " XCS"], "paper_abstract": "Reinforcement Learning is a learning paradigm that helps the agent to learn to act optimally in an unknown environment through trial and error. An RL-based agent senses its environmental state, proposes an action, and applies it to the environment. Then a reinforcement signal, called the reward, is sent back from the environment to the agent. The agent is expected to learn how to maximize overall environmental reward through its internal mechanisms. One of the most challenging issues in the RL area arises as a result of the sensory ability of the agent, when it is not able to sense its current environmental state completely. These environments are called partially observable environments. In these environments, the agent may fail to distinguish the actual environmental state and so may fail to propose the optimal action in particular environmental states. So an extended mechanism must be added to the architecture of the agent to enable it to perform optimally in these environments. On the other hand, one of the most-used approaches to reinforcement learning is the evolutionary learning approach and one of the most-used techniques in this family is learning classifier systems. Learning classifier systems try to evolve state-action-reward mappings to model their current environment through trial and error. In this paper we propose a new architecture for learning classifier systems that is able to perform optimally in partially observable environments. This new architecture uses a novel method to detect aliased states in the environment and disambiguates them through multiple instances of classifier systems that interact with the environment in parallel. This model is applied to some well-known benchmark problems and is compared with some of the best classifier systems proposed for these environments. Our results and detailed discussion show that our approach is one of the best techniques among other learning classifier systems in partially observable environments.", "paper_title": "A new architecture for learning classifier systems to solve POMDP problems", "paper_id": "WOS:000259604800003"}