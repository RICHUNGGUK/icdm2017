{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "random_regression_graphs"}, {"score": 0.004527024978801684, "phrase": "regression_problem"}, {"score": 0.004152502093643003, "phrase": "regression_function"}, {"score": 0.0036703791815812328, "phrase": "adaptive_partition"}, {"score": 0.0033251716926666437, "phrase": "suitable_random_merges"}, {"score": 0.0030497740719977835, "phrase": "arbitrary_geometry"}, {"score": 0.002629539710839523, "phrase": "\"weak_learning_hypothesis"}, {"score": 0.0022670785913397637, "phrase": "suitable_rkhs."}, {"score": 0.002157697367447098, "phrase": "general_results"}], "paper_keywords": ["regression graph", " risk bound", " reproducing kernel Hilbert space", " weak learning hypothesis"], "paper_abstract": "We consider the regression problem and describe an algorithm approximating the regression function by estimators piecewise constant on the elements of an adaptive partition. The partitions are iteratively constructed by suitable random merges and splits, using cuts of arbitrary geometry. We give a risk bound under the assumption that a \"weak learning hypothesis\" holds, and characterize this hypothesis in terms of a suitable RKHS. Two examples illustrate the general results in two particularly interesting cases.", "paper_title": "Risk bounds for random regression graphs", "paper_id": "WOS:000249873100005"}