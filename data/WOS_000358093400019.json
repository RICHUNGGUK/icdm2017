{"auto_keywords": [{"score": 0.032056921787303426, "phrase": "proposed_algorithms"}, {"score": 0.00481495049065317, "phrase": "ensemble_regression"}, {"score": 0.0047517225918491226, "phrase": "imprecise_probabilities"}, {"score": 0.004566954456805558, "phrase": "generalized_versions"}, {"score": 0.004274766964161756, "phrase": "original_adaboost_algorithm"}, {"score": 0.004054479957160662, "phrase": "regression_methods"}, {"score": 0.003922580121023413, "phrase": "unit_simplex"}, {"score": 0.0036958398682001015, "phrase": "smaller_set"}, {"score": 0.003413705707789296, "phrase": "restricted_set"}, {"score": 0.003368816076401138, "phrase": "weighting_probabilities"}, {"score": 0.003216285378512588, "phrase": "single_parameter"}, {"score": 0.003153041030690117, "phrase": "particular_choices"}, {"score": 0.0029706504420983896, "phrase": "standard_adaboost-based_regression_algorithms"}, {"score": 0.0028360962841331634, "phrase": "main_advantage"}, {"score": 0.002707620139970122, "phrase": "basic_adaboost-based_regression_methods"}, {"score": 0.0024678216843278806, "phrase": "hard_instances"}, {"score": 0.0023096222276192194, "phrase": "better_performance"}, {"score": 0.0022641658133181115, "phrase": "proposed_regression_methods"}, {"score": 0.0021903801614787423, "phrase": "corresponding_standard_regression_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Regression", " AdaBoost algorithm", " Over-fitting", " Linear-vacuous mixture model", " Kolmogorov-Smirnov bounds"], "paper_abstract": "In this paper, generalized versions of two ensemble methods for regression based on variants of the original AdaBoost algorithm are proposed. The generalization of these regression methods consists in restricting the unit simplex for the weights of the instances to a smaller set of weighting probabilities. Various imprecise statistical models can be used to obtain a restricted set of weighting probabilities, whose sizes each depend on a single parameter. For particular choices of this parameter, the proposed algorithms reduce to standard AdaBoost-based regression algorithms or to standard regression. The main advantage of the proposed algorithms compared to the basic AdaBoost-based regression methods is that they have less tendency to over-fitting, because the weights of the hard instances are restricted. Several simulations and applications furthermore indicate a better performance of the proposed regression methods in comparison with the corresponding standard regression methods. (C) 2015 Elsevier Inc. All rights reserved.", "paper_title": "Improving over-fitting in ensemble regression by imprecise probabilities", "paper_id": "WOS:000358093400019"}