{"auto_keywords": [{"score": 0.04378641811045662, "phrase": "fusion_algorithms"}, {"score": 0.00481495049065317, "phrase": "bio-inspired_vision_data"}, {"score": 0.00476697427418509, "phrase": "simplified_high_level_scene_interpretation"}, {"score": 0.004672444473700524, "phrase": "face_motion_analysis"}, {"score": 0.0043560464825349275, "phrase": "human_visual_system"}, {"score": 0.004143196873246147, "phrase": "automatic_analysis"}, {"score": 0.004060986196931943, "phrase": "global_and_local_facial_motions"}, {"score": 0.004000396172153709, "phrase": "proposed_fusion_algorithms"}, {"score": 0.0038819041309353024, "phrase": "human_vision_models"}, {"score": 0.0038239757564863057, "phrase": "human_retina"}, {"score": 0.003785836059995376, "phrase": "primary_visual_cortex"}, {"score": 0.0037106898330901534, "phrase": "gipsa-lab"}, {"score": 0.0035827424231607784, "phrase": "low_level_bio-inspired_modules"}, {"score": 0.0034940519373446335, "phrase": "contour_detector"}, {"score": 0.0032571885279065126, "phrase": "video_data_pre-processing"}, {"score": 0.0030363332738004454, "phrase": "reliable_face_motion_interpretation"}, {"score": 0.002931569771634119, "phrase": "global_head_motion_analysis"}, {"score": 0.0028877818105020434, "phrase": "head_nods"}, {"score": 0.0028446460307503343, "phrase": "local_eye_motion_analysis"}, {"score": 0.0027602925235474317, "phrase": "local_mouth_motion_analysis"}, {"score": 0.002719055901632759, "phrase": "speech_lip_motion"}, {"score": 0.0024842242921857705, "phrase": "human_vision_model_pre-processing"}, {"score": 0.002398464540102246, "phrase": "reliable_manner"}, {"score": 0.0022925278792429553, "phrase": "traditional_video_acquisition_problems"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Bio-inspired approach", " Face motion interpretation", " Retina modelling", " V1 cortex model", " Motion detection", " Spectrum analysis", " V1 cortex model", " Yawn detection", " Blinks detection", " Non-speech periods detection", " Head approval and denial detection"], "paper_abstract": "This paper proposes to demonstrate the advantages of using certain properties of the human visual system in order to develop a set of fusion algorithms for automatic analysis and interpretation of global and local facial motions. The proposed fusion algorithms rely on information coming from human vision models such as human retina and primary visual cortex previously developed at Gipsa-lab. Starting from a set of low level bio-inspired modules (static and moving contour detector, motion event detector and spectrum analyser) which are very efficient for video data pre-processing, it is shown how to organize them together in order to achieve reliable face motion interpretation. In particular, algorithms for global head motion analysis such as head nods, for local eye motion analysis such as blinking, for local mouth motion analysis such as speech lip motion and yawning and for open/close mouth/eye state detection are proposed and their performances are assessed. Thanks to the use of human vision model pre-processing which decorrelates visual information in a reliable manner, fusion algorithms are simplified and remain robust against traditional video acquisition problems (light changes, object detection failure, etc.). (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Fusing bio-inspired vision data for simplified high level scene interpretation: Application to face motion analysis", "paper_id": "WOS:000279301000004"}