{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "critical_and_erroneous_samples"}, {"score": 0.004494570360108983, "phrase": "well-known_and_good_performance"}, {"score": 0.004195418010131832, "phrase": "machine_ensembles"}, {"score": 0.0035722806911457545, "phrase": "separated_attention"}, {"score": 0.0034911335714333507, "phrase": "sample_errors"}, {"score": 0.0032585499655442404, "phrase": "classification_border"}, {"score": 0.0027741761880779535, "phrase": "selectable_parameter"}, {"score": 0.002472822097373027, "phrase": "simple_methods"}, {"score": 0.0023079225317714815, "phrase": "better_performance"}, {"score": 0.002255431275670053, "phrase": "smaller_ensembles"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["boosting", " real adaboost", " emphasis functions", " convex combination"], "paper_abstract": "Real Adaboost is a well-known and good performance boosting method used to build machine ensembles for classification. Considering that its emphasis function can be decomposed in two factors that pay separated attention to sample errors and to their proximity to the classification border, a generalized emphasis function that combines both components by means of a selectable parameter, gimel, is presented. Experiments show that simple methods of selecting gimel frequently offer better performance and smaller ensembles. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Boosting by weighting critical and erroneous samples", "paper_id": "WOS:000235797000008"}