{"auto_keywords": [{"score": 0.026265242145147725, "phrase": "upc"}, {"score": 0.00481495049065317, "phrase": "upc_programs"}, {"score": 0.004720043307726269, "phrase": "multi-core_systems"}, {"score": 0.0045357787854931894, "phrase": "unified_parallel_c"}, {"score": 0.0033639840192975835, "phrase": "good_performance"}, {"score": 0.0032975819773787985, "phrase": "numa_multi-core_computers"}, {"score": 0.0031686717525002935, "phrase": "quantitative_performance_results"}, {"score": 0.00256993314099902, "phrase": "end-to-end_development"}, {"score": 0.0023028105026714533, "phrase": "optimized_upc_programs"}, {"score": 0.0022127018991294047, "phrase": "current_multi-core_systems"}, {"score": 0.0021049977753042253, "phrase": "vendor-optimized_libraries"}], "paper_keywords": ["UPC", " PGAS"], "paper_abstract": "The Partitioned Global Address Space (PGAS) model of Unified Parallel C (UPC) can help users express and manage application data locality on non-uniform memory access (NUMA) multi-core shared-memory systems to get good performance. First, we describe several UPC program optimization techniques that are important to achieving good performance on NUMA multi-core computers with examples and quantitative performance results. Second, we use two numerical computing kernels, parallel matrix-matrix multiplication and parallel 3-D FFT, to demonstrate the end-to-end development and optimization for UPC applications. Our results show that the optimized UPC programs achieve very good and scalable performance on current multi-core systems and can even outperform vendor-optimized libraries in some cases.", "paper_title": "Optimizing UPC programs for multi-core systems", "paper_id": "WOS:000286459500006"}