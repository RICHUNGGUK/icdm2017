{"auto_keywords": [{"score": 0.03043810258611645, "phrase": "leen"}, {"score": 0.019730792682686998, "phrase": "mapreduce"}, {"score": 0.008791255935608994, "phrase": "shuffle_phase"}, {"score": 0.006399709170701843, "phrase": "hadoop"}, {"score": 0.005657809857956901, "phrase": "different_data_nodes"}, {"score": 0.00481495049065317, "phrase": "partitioning_skew"}, {"score": 0.00474348728309194, "phrase": "leen._mapreduce"}, {"score": 0.004655640997424794, "phrase": "prominent_tool"}, {"score": 0.004620957579769604, "phrase": "big_data_processing"}, {"score": 0.004586531354697466, "phrase": "data_locality"}, {"score": 0.00453537012358554, "phrase": "key_feature"}, {"score": 0.00440170067770959, "phrase": "data-intensive_cloud_systems"}, {"score": 0.0043363441738461335, "phrase": "network_saturation"}, {"score": 0.004287961675700099, "phrase": "large_amounts"}, {"score": 0.004146015717402716, "phrase": "data_storage"}, {"score": 0.0040691889755318155, "phrase": "map_phase"}, {"score": 0.0034643747383816164, "phrase": "huge_amount"}, {"score": 0.0034385353123700885, "phrase": "data_transfer"}, {"score": 0.0033496005635637316, "phrase": "significant_unfairness"}, {"score": 0.0033121909731471787, "phrase": "reduce_input"}, {"score": 0.0031195948946937378, "phrase": "long_data_transfer"}, {"score": 0.0030388841099720843, "phrase": "computation_skew"}, {"score": 0.002993701876937243, "phrase": "reduce_phase"}, {"score": 0.0028944757265206332, "phrase": "novel_algorithm"}, {"score": 0.0026956374041441126, "phrase": "asynchronous_map"}, {"score": 0.002625866440129899, "phrase": "intermediate_keys"}, {"score": 0.0025198477493173165, "phrase": "expected_data_distribution"}, {"score": 0.0023466842270226888, "phrase": "higher_locality"}, {"score": 0.0022945073938622287, "phrase": "shuffled_data"}, {"score": 0.002235095576913575, "phrase": "fair_distribution"}, {"score": 0.0022101055924375725, "phrase": "reduce_inputs"}, {"score": 0.0021367960510499575, "phrase": "performance_improvement"}, {"score": 0.0021049977753042253, "phrase": "different_workloads"}], "paper_keywords": ["MapReduce", " Hadoop", " Cloud computing", " Skew partitioning", " Intermediate data"], "paper_abstract": "MapReduce is emerging as a prominent tool for big data processing. Data locality is a key feature in MapReduce that is extensively leveraged in data-intensive cloud systems: it avoids network saturation when processing large amounts of data by co-allocating computation and data storage, particularly for the map phase. However, our studies with Hadoop, a widely used MapReduce implementation, demonstrate that the presence of partitioning skew (Partitioning skew refers to the case when a variation in either the intermediate keys' frequencies or their distributions or both among different data nodes) causes a huge amount of data transfer during the shuffle phase and leads to significant unfairness on the reduce input among different data nodes. As a result, the applications severe performance degradation due to the long data transfer during the shuffle phase along with the computation skew, particularly in reduce phase. In this paper, we develop a novel algorithm named LEEN for locality-aware and fairness-aware key partitioning in MapReduce. LEEN embraces an asynchronous map and reduce scheme. All buffered intermediate keys are partitioned according to their frequencies and the fairness of the expected data distribution after the shuffle phase. We have integrated LEEN into Hadoop. Our experiments demonstrate that LEEN can efficiently achieve higher locality and reduce the amount of shuffled data. More importantly, LEEN guarantees fair distribution of the reduce inputs. As a result, LEEN achieves a performance improvement of up to 45 % on different workloads.", "paper_title": "Handling partitioning skew in MapReduce using LEEN", "paper_id": "WOS:000327408000005"}