{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "value_function_approximation"}, {"score": 0.00464976086880886, "phrase": "least-squares_policy_iteration_approach"}, {"score": 0.0042859292226559535, "phrase": "appropriate_basis_functions"}, {"score": 0.003950453462399077, "phrase": "gaussian_kernel"}, {"score": 0.0038148090091532933, "phrase": "popular_and_useful_choice"}, {"score": 0.0036838048761094933, "phrase": "basis_function"}, {"score": 0.003129317057177867, "phrase": "real-world_reinforcement_learning_tasks"}, {"score": 0.002817639725689939, "phrase": "new_basis_function"}, {"score": 0.002720785798036066, "phrase": "geodesic_gaussian_kernels"}, {"score": 0.002566685388128598, "phrase": "non-linear_manifold_structure"}, {"score": 0.0024496978447849835, "phrase": "markov_decision_processes"}, {"score": 0.0022841153001549193, "phrase": "proposed_method"}, {"score": 0.002154693685426657, "phrase": "simulated_robot_arm_control"}, {"score": 0.0021049977753042253, "phrase": "khepera_robot_navigation"}], "paper_keywords": ["reinforcement learning", " value function approximation", " Markov decision process", " least-squares policy iteration", " Gaussian kernel"], "paper_abstract": "The least-squares policy iteration approach works efficiently in value function approximation, given appropriate basis functions. Because of its smoothness, the Gaussian kernel is a popular and useful choice as a basis function. However, it does not allow for discontinuity which typically arises in real-world reinforcement learning tasks. In this paper, we propose a new basis function based on geodesic Gaussian kernels, which exploits the non-linear manifold structure induced by the Markov decision processes. The usefulness of the proposed method is successfully demonstrated in simulated robot arm control and Khepera robot navigation.", "paper_title": "Geodesic Gaussian kernels for value function approximation", "paper_id": "WOS:000259555600006"}