{"auto_keywords": [{"score": 0.04676932589114087, "phrase": "single_visual_word"}, {"score": 0.04502189454683372, "phrase": "existing_visual_phrases"}, {"score": 0.04107852106327227, "phrase": "visual_words"}, {"score": 0.03482636869050446, "phrase": "mvp"}, {"score": 0.00481495049065317, "phrase": "scalable_partial-duplicate_visual_search"}, {"score": 0.004720269202737973, "phrase": "multiple_visual_words"}, {"score": 0.004664352260912174, "phrase": "extra_spatial_clues"}, {"score": 0.004554488671294504, "phrase": "visual_phrase"}, {"score": 0.004518442937844318, "phrase": "better_discriminative_power"}, {"score": 0.004447201233380302, "phrase": "image_retrieval"}, {"score": 0.004257000695796238, "phrase": "obvious_shortcomings"}, {"score": 0.0036023971109946946, "phrase": "visual_word_combinations"}, {"score": 0.003573859492987647, "phrase": "visual_phrases"}, {"score": 0.0034482025128821548, "phrase": "single_visual_words"}, {"score": 0.0033269488835014583, "phrase": "multi-order_visual_phrase"}, {"score": 0.0031467250930224126, "phrase": "local_descriptor"}, {"score": 0.003109390330476147, "phrase": "image_keypoint"}, {"score": 0.0030119758165837625, "phrase": "multiple_nearby_keypoints"}, {"score": 0.0028487656293168795, "phrase": "match_confidence"}, {"score": 0.0028037753646260937, "phrase": "spatial_and_visual_consistency"}, {"score": 0.002737615043512257, "phrase": "center_visual_word_matching"}, {"score": 0.002705121052747945, "phrase": "traditional_visual_word_matching"}, {"score": 0.002662393295121998, "phrase": "neighbor_spatial_and_visual_clues"}, {"score": 0.0026099289298770023, "phrase": "discriminative_power"}, {"score": 0.0024586427394152196, "phrase": "quantization_error"}, {"score": 0.0023626775529531486, "phrase": "ukbench"}, {"score": 0.0022704789445028564, "phrase": "flickr"}, {"score": 0.0022345720876241044, "phrase": "recent_retrieval_approaches"}, {"score": 0.0021559025985688255, "phrase": "competitive_accuracy"}, {"score": 0.002104998088798757, "phrase": "mvp."}], "paper_keywords": ["Image local descriptor", " Image matching", " Large-scale image retrieval", " Visual vocabulary"], "paper_abstract": "Visual phrase considers multiple visual words and captures extra spatial clues among them. Thus, visual phrase shows better discriminative power than single visual word in image retrieval and matching. Not withstanding their success, existing visual phrases still show obvious shortcomings: (1) limited flexibility, i.e., visual phrases are considered for matching only if they contain the same number of visual words; (2) large quantization error and low repeatability, i.e., quantization errors in visual words are aggregated in visual word combinations and visual phrases, making them harder to be matched than single visual words. To avoid these issues, we propose multi-order visual phrase (MVP) which contains two complementary clues: center visual word quantized from the local descriptor of each image keypoint and the visual and spatial clues of multiple nearby keypoints. Two MVPs are flexibly matched by first matching their center visual words, then estimating a match confidence by checking the spatial and visual consistency of their neighbor keypoints. Therefore, center visual word matching equals to traditional visual word matching, but the neighbor spatial and visual clues checking significantly boosts the discriminative power. MVP does not scarify the repeatability of single visual word and is more robust to quantization error than existing visual phrases. We test our approach in three image retrieval tasks on UKbench, Oxford5K, and 1 million distractor images collected from Flickr. Comparisons with recent retrieval approaches and existing visual phrase features clearly demonstrate the competitive accuracy and significantly better efficiency of MVP.", "paper_title": "Multi-order visual phrase for scalable partial-duplicate visual search", "paper_id": "WOS:000350213300010"}