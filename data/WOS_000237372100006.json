{"auto_keywords": [{"score": 0.04933167036983738, "phrase": "incomplete_data"}, {"score": 0.015719716506582538, "phrase": "bayesian_networks"}, {"score": 0.015237830551664586, "phrase": "importance_sampling"}, {"score": 0.003832198214757852, "phrase": "imputation_proposals"}, {"score": 0.0035141291835319682, "phrase": "observed_data"}, {"score": 0.0032223742018434856, "phrase": "posterior_parameter_distribution"}, {"score": 0.003073545175760353, "phrase": "mixture_distribution"}, {"score": 0.0030017207229983385, "phrase": "tractable_number"}, {"score": 0.00281826552772148, "phrase": "different_imputation_methods"}, {"score": 0.00266693888510356, "phrase": "imputation_method"}, {"score": 0.0026045914281028473, "phrase": "gibbs_sampling"}, {"score": 0.0025436978068459565, "phrase": "data_augmentation_derivative"}, {"score": 0.00225988413662628, "phrase": "winbugs"}, {"score": 0.0022070310530794097, "phrase": "em_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Bayesian networks", " parameter learning", " incomplete data", " MCMC", " Bayesian statistics"], "paper_abstract": "We present an algorithm for learning parameters of Bayesian networks from incomplete data. By using importance sampling we are able to assign a score to imputation proposals depending on the quality of such a proposal in combination with the observed data. This in effect makes it possible to approximate the posterior parameter distribution given incomplete data by using a mixture distribution with a tractable number of components. The technique allows for different imputation methods, in particular we propose an imputation method that combines Gibbs sampling and a data augmentation derivative. We evaluate our algorithm, and we compare the results to those obtained with WinBUGS and the EM algorithm. (C) 2005 Elsevier Inc. All rights reserved.", "paper_title": "Learning parameters of Bayesian networks from incomplete data via importance sampling", "paper_id": "WOS:000237372100006"}