{"auto_keywords": [{"score": 0.04829088285990108, "phrase": "big_data"}, {"score": 0.015719716506582538, "phrase": "internet_data"}, {"score": 0.01072145040933757, "phrase": "mapreduce"}, {"score": 0.010548808419883397, "phrase": "community_detection"}, {"score": 0.004762893197005439, "phrase": "mapreduce_model"}, {"score": 0.004366106816225457, "phrase": "scientific_research"}, {"score": 0.004024072318264299, "phrase": "scientific_researchers"}, {"score": 0.0036886073959526396, "phrase": "traditional_single-host_web_spider_and_data_store_approaches"}, {"score": 0.003512308265830794, "phrase": "large_memory_requirement"}, {"score": 0.0033810134162565843, "phrase": "big_data_store-retrieve_approach"}, {"score": 0.003308195943172624, "phrase": "store-retrieve_approach"}, {"score": 0.0031500213134140953, "phrase": "distributed_processing"}, {"score": 0.0030654278282822027, "phrase": "distributed_capture_method"}, {"score": 0.0028715299874549245, "phrase": "storage_optimization_method"}, {"score": 0.0027491084541806823, "phrase": "hash_functions"}, {"score": 0.002719325177422626, "phrase": "light-weight_characteristics"}, {"score": 0.0026175952455594277, "phrase": "storage_structure"}, {"score": 0.0025611773887867255, "phrase": "data_retrieval_problems"}, {"score": 0.002533433243230496, "phrase": "dsmc"}, {"score": 0.0024788167298469455, "phrase": "high_performance"}, {"score": 0.0024519545383399773, "phrase": "large_web_data_comparison"}, {"score": 0.0023730978395564116, "phrase": "efficient_data_retrieval"}, {"score": 0.0022967713922705, "phrase": "experimental_results"}, {"score": 0.0022228943786652914, "phrase": "cloudsim_platform"}, {"score": 0.002163144760168942, "phrase": "traditional_web_spider"}, {"score": 0.002128067018576581, "phrase": "proposed_dsmc_approach"}, {"score": 0.0021049977753042253, "phrase": "better_efficiency"}], "paper_keywords": [""], "paper_abstract": "The processing of big data is a hotspot in the scientific research. Data on the Internet is very large and also very important for the scientific researchers, so the capture and store of Internet data is a priority among priorities. The traditional single-host web spider and data store approaches have some problems such as low efficiency and large memory requirement, so this paper proposes a big data store-retrieve approach DSMC (distributed store-retrieve approach using MapReduce model and community detection) based on distributed processing. Firstly, the distributed capture method using MapReduce to deduplicate big data is presented. Secondly, the storage optimization method is put forward; it uses the hash functions with light-weight characteristics and the community detection to address the storage structure and solve the data retrieval problems. DSMC has achieved the high performance of large web data comparison and storage and gets the efficient data retrieval at the same time. The experimental results show that, in the Cloudsim platform, comparing with the traditional web spider, the proposed DSMC approach shows better efficiency and performance.", "paper_title": "DSMC: A Novel Distributed Store-Retrieve Approach of Internet Data Using MapReduce Model and Community Detection in Big Data", "paper_id": "WOS:000345752700001"}