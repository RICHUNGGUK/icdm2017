{"auto_keywords": [{"score": 0.04978354449680807, "phrase": "high-end_parallel_supercomputers"}, {"score": 0.0450678547644915, "phrase": "mid"}, {"score": 0.00481495049065317, "phrase": "ultrascale_atomistic_simulations"}, {"score": 0.004339108074589307, "phrase": "high-end_chemically_reactive_and_non-reactive_molecular_dynamics"}, {"score": 0.0041825926310470616, "phrase": "microscopic_mechanisms"}, {"score": 0.004144350280534405, "phrase": "macroscopic_material_properties"}, {"score": 0.0037231102027070724, "phrase": "minimal_bandwidth_complexity"}, {"score": 0.003700371064796848, "phrase": "tight_error_control"}, {"score": 0.003666521536730407, "phrase": "edc_framework"}, {"score": 0.0036329805216306576, "phrase": "adaptive_hierarchical_simulation"}, {"score": 0.003555902292752528, "phrase": "graph-based_event_tracking"}, {"score": 0.0035233696341819437, "phrase": "tunable_hierarchical_cellular_decomposition_parallelization_framework"}, {"score": 0.003459191419478298, "phrase": "edc_algorithms"}, {"score": 0.003438058620195309, "phrase": "petaflops_computers"}, {"score": 0.0032635467579751423, "phrase": "hybrid_grid_remote_procedure_call"}, {"score": 0.0031942814007173254, "phrase": "high-end_computing_platforms"}, {"score": 0.003165099406688307, "phrase": "ibm"}, {"score": 0.003088414215675552, "phrase": "nsf"}, {"score": 0.003050750896241337, "phrase": "excellent_test_grounds"}, {"score": 0.0029405703787004824, "phrase": "unprecedented_scales"}, {"score": 0.0027741807295884172, "phrase": "edc"}, {"score": 0.002731971103291716, "phrase": "adaptive_multigrids"}, {"score": 0.0025459275963373496, "phrase": "automated_execution"}, {"score": 0.0024465972870778645, "phrase": "japan"}, {"score": 0.002217696306116023, "phrase": "performance_portability"}, {"score": 0.0021772489804440646, "phrase": "wide_range"}, {"score": 0.0021049977753042253, "phrase": "amd_opteron-based_linux_clusters"}], "paper_keywords": ["hierarchical simulation", " molecular dynamics", " reactive force field", " quantum mechanics", " density functional theory", " parallel computing", " grid computing"], "paper_abstract": "We present a de novo, hierarchical simulation framework for first-principles based predictive simulations of materials and their validation on high-end parallel supercomputers and geographically distributed clusters. In this framework, high-end chemically reactive and non-reactive molecular dynamics (MID) simulations explore a wide solution space to discover microscopic mechanisms that govern macroscopic material properties, into which highly accurate quantum mechanical (QM) simulations are embedded to validate the discovered mechanisms and quantify the uncertainty of the solution. The framework includes an embedded divide-and conquer (EDC) algorithmic framework for the design of linear-scaling simulation algorithms with minimal bandwidth complexity and tight error control. The EDC framework also enables adaptive hierarchical simulation with automated model transitioning assisted by graph-based event tracking. A tunable hierarchical cellular decomposition parallelization framework then maps the O(N) EDC algorithms onto petaflops computers, while achieving performance tunability through a hierarchy of parameterized cell data/ computation structures, as well as its implementation using hybrid grid remote procedure call + message passing + threads programming. High-end computing platforms such as IBM BlueGene/L, SGI Altix 3000 and the NSF TeraGrid provide an excellent test grounds for the framework. On these platforms, we have achieved unprecedented scales of quantum-mechanically accurate and well validated, chemically reactive atomistic simulations-1.06 billion-atom fast reactive force-field MID and 11.8 million-atom (1.04 trillion grid points) quantum-mechanical MID in the framework of the EDC density functional theory on adaptive multigrids-in addition to 134 billion-atom non-reactive space-time multiresolution MD, with the parallel efficiency as high as 0.998 on 65,536 dual-processor BlueGene/L nodes. We have also achieved an automated execution of hierarchical QM/MD simulation on a grid consisting of 6 supercomputer centers in the US and Japan (in total of 150,000 processor hours), in which the number of processors change dynamically on demand and resources are allocated and migrated dynamically in response to faults. Furthermore, performance portability has been demonstrated on a wide range of platforms such as BlueGene/L, Altix 3000, and AMD Opteron-based Linux clusters.", "paper_title": "De novo ultrascale atomistic simulations on high-end parallel supercomputers", "paper_id": "WOS:000253723600008"}