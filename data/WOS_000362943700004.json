{"auto_keywords": [{"score": 0.04962396550254858, "phrase": "hierarchical_multilabel_classification"}, {"score": 0.00481495049065317, "phrase": "bayes-optimal_hierarchical_multilabel_classification"}, {"score": 0.004612376827433774, "phrase": "multiple_class_labels"}, {"score": 0.004355415885815204, "phrase": "directed_acyclic_graph"}, {"score": 0.004212157020917929, "phrase": "popular_hierarchical_loss_functions"}, {"score": 0.003977403701783668, "phrase": "tree_hierarchies"}, {"score": 0.003300813776412652, "phrase": "loss_functions"}, {"score": 0.00311669276800628, "phrase": "hierarchical_extensions"}, {"score": 0.003072285184441679, "phrase": "hamming_loss"}, {"score": 0.003043031131345537, "phrase": "ranking_loss"}, {"score": 0.00290087451159631, "phrase": "label_hierarchy"}, {"score": 0.00276534038425712, "phrase": "general_learning_model"}, {"score": 0.0026742419973883134, "phrase": "loss_function"}, {"score": 0.002611010097359509, "phrase": "bayesian_decision_theory"}, {"score": 0.0025614999929672, "phrase": "bayes-optimal_predictions"}, {"score": 0.002512926336312949, "phrase": "corresponding_risks"}, {"score": 0.0024771000236931836, "phrase": "trained_model"}, {"score": 0.0023840346303791032, "phrase": "exhaustive_summation"}, {"score": 0.002327648608736437, "phrase": "optimal_multilabel"}, {"score": 0.0022944576894161485, "phrase": "resultant_optimization_problem"}, {"score": 0.0022188370388602813, "phrase": "greedy_algorithm"}, {"score": 0.0021976915360493628, "phrase": "experimental_results"}, {"score": 0.002145703330831668, "phrase": "real-world_data_sets"}, {"score": 0.0021049977753042253, "phrase": "proposed_bayes-optimal_classifier"}], "paper_keywords": ["Hierarchical classification", " multilabel classification", " loss function", " Bayesian decision theory"], "paper_abstract": "Hierarchical multilabel classification allows a sample to belong to multiple class labels residing on a hierarchy, which can be a tree or directed acyclic graph (DAG). However, popular hierarchical loss functions, such as the H-loss, can only be defined on tree hierarchies (but not on DAGs), and may also under-or over-penalize misclassifications near the bottom of the hierarchy. Besides, it has been relatively unexplored on how to make use of the loss functions in hierarchical multilabel classification. To overcome these deficiencies, we first propose hierarchical extensions of the Hamming loss and ranking loss which take the mistake at every node of the label hierarchy into consideration. Then, we first train a general learning model, which is independent of the loss function. Next, using Bayesian decision theory, we develop Bayes-optimal predictions that minimize the corresponding risks with the trained model. Computationally, instead of requiring an exhaustive summation and search for the optimal multilabel, the resultant optimization problem can be efficiently solved by a greedy algorithm. Experimental results on a number of real-world data sets show that the proposed Bayes-optimal classifier outperforms state-of-the-art methods.", "paper_title": "Bayes-Optimal Hierarchical Multilabel Classification", "paper_id": "WOS:000362943700004"}