{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "regression_ensembles"}, {"score": 0.004773040022691547, "phrase": "sequential_target_correction"}, {"score": 0.004690305164360866, "phrase": "ensemble_methods"}, {"score": 0.004316396683332752, "phrase": "single_decision"}, {"score": 0.0041136473765874815, "phrase": "regression_estimation"}, {"score": 0.003954837422946628, "phrase": "simple_procedure"}, {"score": 0.003903269701018361, "phrase": "target_map"}, {"score": 0.003802135020791801, "phrase": "base_learner"}, {"score": 0.003623453648863547, "phrase": "previous_step_error"}, {"score": 0.0034835017385881385, "phrase": "overall_upper_error"}, {"score": 0.003423042581201533, "phrase": "composite_hypothesis"}, {"score": 0.0033197446406648626, "phrase": "training_errors"}, {"score": 0.0032764307492734145, "phrase": "individual_hypotheses"}, {"score": 0.0031775430407770026, "phrase": "proposed_procedure_results"}, {"score": 0.003068166587650604, "phrase": "weighted_form"}, {"score": 0.0030414140732789186, "phrase": "negative_correlation"}, {"score": 0.0029886047149545025, "phrase": "previous_hypotheses"}, {"score": 0.0027863537694678094, "phrase": "highly_influential_data_points"}, {"score": 0.0026321198895843173, "phrase": "known_examples"}, {"score": 0.0025192983141887285, "phrase": "real_and_synthetic_data-sets"}, {"score": 0.002464709648714996, "phrase": "base_learners"}, {"score": 0.0023798115577749225, "phrase": "considerably_better_prediction_errors"}, {"score": 0.0021896891011033105, "phrase": "bagging_and_adaboost_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Ensemble learning", " Diversity", " Negative Correlation", " Bagging", " AdaBoost", " Regression estimation"], "paper_abstract": "Ensemble methods learn models from examples by generating a set of hypotheses, which are then combined to make a single decision. We propose an algorithm to construct an ensemble for regression estimation. Our proposal generates the hypotheses sequentially using a simple procedure whereby the target map to be learned by the base learner at each step is modified as a function of the previous step error. We state a theorem that relates the overall upper error bound of the composite hypothesis obtained within this procedure to the training errors of the individual hypotheses. We also demonstrate that the proposed procedure results in a learning functional that enforces a weighted form of Negative Correlation with respect to previous hypotheses. Additionally, we incorporate resampling to allow the ensemble to control the impact of highly influential data points, showing that this component significantly improves its ability to generalize from the known examples. We describe experiments performed to evaluate our technique on real and synthetic data-sets using neural networks as base learners. These results show that our technique exhibits considerably better prediction errors than the Negative Correlation (NC) method and that its performance is very competitive with that of the Bagging and AdaBoost algorithms for regression estimation. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Training regression ensembles by sequential target correction and resampling", "paper_id": "WOS:000303092800009"}