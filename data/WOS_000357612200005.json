{"auto_keywords": [{"score": 0.05007776290148563, "phrase": "moore-penrose_inverse_matrices"}, {"score": 0.04831370608356702, "phrase": "elm"}, {"score": 0.015244844267840884, "phrase": "qrgeninv-elm"}, {"score": 0.00637367680967332, "phrase": "tpm-elm"}, {"score": 0.005209746478021546, "phrase": "experimental_results"}, {"score": 0.004681226806682058, "phrase": "extreme_learning_machine"}, {"score": 0.004551199992546761, "phrase": "learning_algorithm"}, {"score": 0.004508661664944657, "phrase": "single-hidden_layer_feedforward_neural_networks"}, {"score": 0.004362870617347183, "phrase": "hidden_nodes"}, {"score": 0.004261617028111862, "phrase": "output_weights"}, {"score": 0.004143196873246147, "phrase": "input_weights"}, {"score": 0.004085221464783815, "phrase": "hidden_layer_biases"}, {"score": 0.003771631577951067, "phrase": "learning_time"}, {"score": 0.0035648266669315943, "phrase": "hidden_layer_output_matrix"}, {"score": 0.0034657012607680173, "phrase": "effective_computation"}, {"score": 0.0032449610741097992, "phrase": "reduced_qr_factorization"}, {"score": 0.0031695693589575916, "phrase": "geninv_elm"}, {"score": 0.0028446460307503343, "phrase": "relational_algorithm"}, {"score": 0.002752513434446804, "phrase": "relational_algorithms"}, {"score": 0.0027139447110929586, "phrase": "cholesky_factorization"}, {"score": 0.002688532127821979, "phrase": "singular_matrix_elm"}, {"score": 0.0026014425686078993, "phrase": "qr_factorization"}, {"score": 0.0025770807002632877, "phrase": "ginv_elm"}, {"score": 0.0024702398174481785, "phrase": "gram-schmidt"}, {"score": 0.002323663295811215, "phrase": "statistical_analysis"}, {"score": 0.0022064376000055764, "phrase": "geninv-elm"}, {"score": 0.0021049977753042253, "phrase": "comparable_generalization_performance"}], "paper_keywords": ["Extreme learning machine", " tensor product matrix", " cholesky factorization of singular matrix", " conjugate gram-schmidt process", " QR factorization"], "paper_abstract": "Extreme learning machine (ELM) is a learning algorithm for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. After the input weights and the hidden layer biases are chosen randomly, ELM can be simply considered a linear system. However, the learning time of ELM is mainly spent on calculating the Moore-Penrose inverse matrices of the hidden layer output matrix. This paper focuses on effective computation of the Moore-Penrose inverse matrices for ELM, several methods are proposed. They are the reduced QR factorization with column Pivoting and Geninv ELM (QRGeninv-ELM), tensor product matrix ELM (TPM-ELM). And we compare QRGeninv-ELM, TPM-ELM with the relational algorithm of Moore-Penrose inverse matrices for ELM, the relational algorithms are: Cholesky factorization of singular matrix ELM (Geninv-ELM), QR factorization and Ginv ELM (QRGinv-ELM), the conjugate Gram-Schmidt process ELM (CGS-ELM). The experimental results and the statistical analysis of the experimental results both demonstrate that QRGeninv-ELM, TPM-ELM and Geninv-ELM are faster than other kinds of ELM and can reach comparable generalization performance.", "paper_title": "Effective algorithms of the Moore-Penrose inverse matrices for extreme learning machine", "paper_id": "WOS:000357612200005"}