{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "decision_tree"}, {"score": 0.004704187604192605, "phrase": "evolutionary_training"}, {"score": 0.004631758267671427, "phrase": "imbalanced_problems"}, {"score": 0.004560438999891081, "phrase": "imbalanced_domains"}, {"score": 0.004507667881645218, "phrase": "recent_challenge"}, {"score": 0.00447282529270722, "phrase": "data_mining"}, {"score": 0.004386887374401584, "phrase": "imbalanced_classification"}, {"score": 0.003919900973086169, "phrase": "learning_task"}, {"score": 0.0036553070575015344, "phrase": "learning_process"}, {"score": 0.003408512155673968, "phrase": "majority_class"}, {"score": 0.003253279945540651, "phrase": "new_minority_examples"}, {"score": 0.003141498438356646, "phrase": "under-sampling_procedure"}, {"score": 0.0031050953733360825, "phrase": "evolutionary_algorithms"}, {"score": 0.0029867619862229853, "phrase": "decision_trees"}, {"score": 0.0029066143383279217, "phrase": "rule_sets"}, {"score": 0.002731381261399358, "phrase": "-sampling_techniques"}, {"score": 0.002658068304632523, "phrase": "new_approach"}, {"score": 0.002440192402613492, "phrase": "obtained_models"}, {"score": 0.0022141537427057363, "phrase": "non-parametric_statistical_tests"}, {"score": 0.002163089417146551, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b._v."}], "paper_keywords": ["Evolutionary algorithms", " Imbalanced classification", " Data reduction", " Training set selection", " Decision trees", " Rule induction"], "paper_abstract": "Classification in imbalanced domains is a recent challenge in data mining. We refer to imbalanced classification when data presents many examples from one class and few from the other class, and the less representative class is the one which has more interest from the point of view of the learning task. One of the most used techniques to tackle this problem consists in preprocessing the data previously to the learning process. This preprocessing could be done through under-sampling; removing examples, mainly belonging to the majority class; and over-sampling, by means of replicating or generating new minority examples. In this paper, we propose an under-sampling procedure guided by evolutionary algorithms to perform a training set selection for enhancing the decision trees obtained by the C4.5 algorithm and the rule sets obtained by PART rule induction algorithm. The proposal has been compared with other under-sampling and over-sampling techniques and the results indicate that the new approach is very competitive in terms of accuracy when comparing with over-sampling and it outperforms standard under-sampling. Moreover, the obtained models are smaller in terms of number of leaves or rules generated and they can considered more interpretable. The results have been contrasted through non-parametric statistical tests over multiple data sets. Crown Copyright (C) 2009 Published by Elsevier B. V. All rights reserved.", "paper_title": "Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems", "paper_id": "WOS:000269172700013"}