{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "genetic_programming-based_feature_extraction"}, {"score": 0.004672444473700524, "phrase": "cancer_diagnosis"}, {"score": 0.004556901872357751, "phrase": "underlying_assumption"}, {"score": 0.0043560464825349275, "phrase": "learned_classifier"}, {"score": 0.004184921610672245, "phrase": "unseen_data"}, {"score": 0.004040688646993605, "phrase": "seemingly_reasonable_assumption"}, {"score": 0.003940706575253453, "phrase": "biological_data"}, {"score": 0.00327356326590461, "phrase": "biological_variations"}, {"score": 0.0032246835734680377, "phrase": "small_differences"}, {"score": 0.002976019715894587, "phrase": "genetics-based_machine"}, {"score": 0.002902304851882577, "phrase": "feature_extraction"}, {"score": 0.0027602925235474317, "phrase": "classification_performance"}, {"score": 0.002719055901632759, "phrase": "existing_classifier"}, {"score": 0.0025989960809479104, "phrase": "different_laboratory"}, {"score": 0.002362620540317279, "phrase": "bad_behavior"}, {"score": 0.002327310963701377, "phrase": "experimental_analysis"}, {"score": 0.0023040642263522505, "phrase": "benchmark_problems"}, {"score": 0.0022582634725291225, "phrase": "real-world_problem"}, {"score": 0.00223570488212345, "phrase": "prostate_cancer_diagnosis"}, {"score": 0.0022022878843247257, "phrase": "good_behavior"}, {"score": 0.0021693692831601745, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Genetic programming", " Feature extraction", " Fractures between data", " Biological data", " Cancer diagnosis", " Different laboratories"], "paper_abstract": "There is an underlying assumption on most model building processes: given a learned classifier, it should be usable to explain unseen data from the same given problem. Despite this seemingly reasonable assumption, when dealing with biological data it tends to fail; where classifiers built out of data generated using the same protocols in two different laboratories can lead to two different, non-interchangeable, classifiers. There are usually too many uncontrollable variables in the process of generating data in the lab and biological variations, and small differences can lead to very different data distributions, with a fracture between data. This paper presents a genetics-based machine learning approach that performs feature extraction on data from a lab to help increase the classification performance of an existing classifier that was built using the data from a different laboratory which uses the same protocols, while learning about the shape of the fractures between data that motivated the bad behavior. The experimental analysis over benchmark problems together with a real-world problem on prostate cancer diagnosis show the good behavior of the proposed algorithm. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Repairing fractures between data using genetic programming-based feature extraction: A case study in cancer diagnosis", "paper_id": "WOS:000313774200054"}