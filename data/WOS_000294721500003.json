{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "supervised_learning"}, {"score": 0.004718897469501381, "phrase": "smooth_regression_functions"}, {"score": 0.0040978781692373005, "phrase": "infinitely_differentiable_regression_functions"}, {"score": 0.003704797078215222, "phrase": "lower_bounds"}, {"score": 0.003594369823549604, "phrase": "generalization_error"}, {"score": 0.003152531962718477, "phrase": "input_space"}, {"score": 0.0029672941061574375, "phrase": "non-linear_context"}, {"score": 0.002764856305713066, "phrase": "non-linear_problems"}, {"score": 0.0024742098091226203, "phrase": "serious_problem"}, {"score": 0.002282166507956957, "phrase": "counter-intuitively_that_sometimes_supervised_learning"}, {"score": 0.0021049977753042253, "phrase": "infinite_dimensionality"}], "paper_keywords": ["Supervised learning", " Van Trees", " Minimax", " Analytic function", " Nonparametric regression", " High dimensional"], "paper_abstract": "In this paper, the effect of dimensionality on the supervised learning of infinitely differentiable regression functions is analyzed. By invoking the Van Trees lower bound, we prove lower bounds on the generalization error with respect to the number of samples and the dimensionality of the input space both in a linear and non-linear context. It is shown that in non-linear problems without prior knowledge, the curse of dimensionality is a serious problem. At the same time, we speculate counter-intuitively that sometimes supervised learning becomes plausible in the asymptotic limit of infinite dimensionality.", "paper_title": "On the Curse of Dimensionality in Supervised Learning of Smooth Regression Functions", "paper_id": "WOS:000294721500003"}