{"auto_keywords": [{"score": 0.03465288598351081, "phrase": "human-based_computations"}, {"score": 0.03134441335815041, "phrase": "automan"}, {"score": 0.024586093495145706, "phrase": "human_tasks"}, {"score": 0.00481495049065317, "phrase": "integrating_human-based"}, {"score": 0.004778258911940497, "phrase": "digital_computation"}, {"score": 0.004494570360108983, "phrase": "crowdsourcing_platforms"}, {"score": 0.004460309246062286, "phrase": "amazon's_mechanical_turk"}, {"score": 0.004359078118356132, "phrase": "human-based_computational_power"}, {"score": 0.004179392391349015, "phrase": "general-purpose_computational_platform"}, {"score": 0.004068907083697369, "phrase": "complete_automation"}, {"score": 0.003976523732243618, "phrase": "complex_or_interrelated_tasks"}, {"score": 0.0038862297330905836, "phrase": "latency_costs"}, {"score": 0.0038713805573219297, "phrase": "real_money"}, {"score": 0.0031845067058084583, "phrase": "human_skills"}, {"score": 0.0030530895634491343, "phrase": "financial_incentive"}, {"score": 0.002806256131739047, "phrase": "standard_programming_language"}, {"score": 0.002784828661426853, "phrase": "ordinary_function_calls"}, {"score": 0.002700738055025905, "phrase": "traditional_functions"}, {"score": 0.0026494723416072316, "phrase": "automan_programmers"}, {"score": 0.0025892330566358503, "phrase": "automan_program"}, {"score": 0.0025596276616952516, "phrase": "confidence_level"}, {"score": 0.002530359917257626, "phrase": "overall_computation"}, {"score": 0.002472822097373027, "phrase": "automan_runtime_system"}, {"score": 0.0023525950486853937, "phrase": "quality_control"}, {"score": 0.0022382002476510573, "phrase": "desired_confidence_level"}, {"score": 0.0021049977753042253, "phrase": "human_workers"}], "paper_keywords": ["Languages", " Algorithms", " Human Factors", " Crowdsourcing", " Programming Languages", " Quality Control"], "paper_abstract": "Humans can perform many tasks with ease that remain difficult or impossible for computers. Crowdsourcing platforms like Amazon's Mechanical Turk make it possible to harness human-based computational power at an unprecedented scale. However, their utility as a general-purpose computational platform remains limited. The lack of complete automation makes it difficult to orchestrate complex or interrelated tasks. Scheduling more human workers to reduce latency costs real money, and jobs must be monitored and rescheduled when workers fail to complete their tasks. Furthermore, it is often difficult to predict the length of time and payment that should be budgeted for a given task. Finally, the results of human-based computations are not necessarily reliable, both because human skills and accuracy vary widely, and because workers have a financial incentive to minimize their effort. This paper introduces AUTOMAN, the first fully automatic crowdprogramming system. AUTOMAN integrates human-based computations into a standard programming language as ordinary function calls, which can be intermixed freely with traditional functions. This abstraction lets AUTOMAN programmers focus on their programming logic. An AUTOMAN program specifies a confidence level for the overall computation and a budget. The AUTOMAN runtime system then transparently manages all details necessary for scheduling, pricing, and quality control. AUTOMAN automatically schedules human tasks for each computation until it achieves the desired confidence level; monitors, reprices, and restarts human tasks as necessary; and maximizes parallelism across human workers while staying under budget.", "paper_title": "AUTOMAN: A Platform for Integrating Human-Based and Digital Computation", "paper_id": "WOS:000311296200037"}