{"auto_keywords": [{"score": 0.04029185245649539, "phrase": "text_files"}, {"score": 0.012677089861370668, "phrase": "large_collection"}, {"score": 0.00481495049065317, "phrase": "natural_language_content"}, {"score": 0.004592905436175987, "phrase": "natural_language_text_files"}, {"score": 0.004339867166576126, "phrase": "used_compression_method"}, {"score": 0.004218602458288531, "phrase": "compressed_text"}, {"score": 0.004120130206000757, "phrase": "attractive_property"}, {"score": 0.00406214911915987, "phrase": "search_engines"}, {"score": 0.003784209997115174, "phrase": "web_search_engines"}, {"score": 0.0037133478610481994, "phrase": "web_pages"}, {"score": 0.0036610702290043387, "phrase": "raw_text_form"}, {"score": 0.0036095259108028105, "phrase": "so-called_snippets"}, {"score": 0.003541923233726252, "phrase": "so-called_positional_ranking_functions"}, {"score": 0.0032838979278935814, "phrase": "application_logs"}, {"score": 0.0027437235598539904, "phrase": "single_files"}, {"score": 0.0026169435194789772, "phrase": "compression_algorithm"}, {"score": 0.002555761854502478, "phrase": "word-based_approach"}, {"score": 0.0021656385587456952, "phrase": "global_model"}, {"score": 0.0021351033500114735, "phrase": "proper_model"}, {"score": 0.0021049977753042253, "phrase": "single_compressed_file"}], "paper_keywords": ["natural language compression", " text compression", " byte codes", " dense codes", " search engines"], "paper_abstract": "An algorithm for very efficient compression of a set of natural language text files is presented. Not only a very good compression ratio is reached, the used compression method allows fast pattern matching in compressed text, which is an attractive property especially for search engines. Much information is stored in the form of a large collection of text files. The web search engines can store the web pages in the raw text form to build so-called snippets or to perform so-called positional ranking functions on them. Furthermore, there exist many other similar contexts such as the storage of emails, application logs or the databases of text files (literary works or technical reports). In this paper, we address the problem of the compression of a large collection of text files distributed in cluster of computers, where the single files need to be randomly accessed in very short time. The compression algorithm is based on a word-based approach and the idea of combination of two statistical models: global model (common for all the files of the set) and local model. The latter is built as a set of changes that transform the global model to the proper model of the single compressed file.", "paper_title": "Compression of a Set of Files with Natural Language Content", "paper_id": "WOS:000359139000010"}