{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "regularized_m-estimators"}, {"score": 0.04814050608144561, "phrase": "local_optima"}, {"score": 0.0390829115805425, "phrase": "statistical_precision"}, {"score": 0.004765510312881668, "phrase": "nonconvexity"}, {"score": 0.004668059458774745, "phrase": "algorithmic_theory"}, {"score": 0.004525629293824217, "phrase": "novel_theoretical_results"}, {"score": 0.004145143433523008, "phrase": "restricted_strong_convexity"}, {"score": 0.004039423410198574, "phrase": "suitable_regularity_conditions"}, {"score": 0.0038558497470953306, "phrase": "stationary_point"}, {"score": 0.0037965243835456214, "phrase": "composite_objective_function"}, {"score": 0.0036616109902631293, "phrase": "underlying_parameter_vector"}, {"score": 0.0034771228725942846, "phrase": "corrected_lasso"}, {"score": 0.003370907075333389, "phrase": "variables_linear_models"}, {"score": 0.00330189927527898, "phrase": "generalized_linear_models"}, {"score": 0.003267925215811654, "phrase": "nonconvex_penalties"}, {"score": 0.003217632602243956, "phrase": "scad"}, {"score": 0.003184539867630608, "phrase": "mcp"}, {"score": 0.002931569771634119, "phrase": "statistical_accuracy"}, {"score": 0.002769395841379498, "phrase": "prediction_error"}, {"score": 0.002740885774408238, "phrase": "stationary_points"}, {"score": 0.00257586859792573, "phrase": "simple_modification"}, {"score": 0.0025493457404304446, "phrase": "composite_gradient_descent"}, {"score": 0.0022399174232611853, "phrase": "fastest_possible_rate"}, {"score": 0.0022053994402287925, "phrase": "first-order_method"}, {"score": 0.0021601997088758957, "phrase": "simulation_studies"}], "paper_keywords": ["high-dimensional statistics", " M-estimation", " model selection", " nonconvex optimization", " nonconvex regularization"], "paper_abstract": "We provide novel theoretical results regarding local optima of regularized M-estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that any stationary point of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped 1; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the l(1)-, l(2)-, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision epsilon(stat) in log(l/epsilon(stat)) steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results.", "paper_title": "Regularized M-estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima", "paper_id": "WOS:000369886000009"}