{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "researcher_bias"}, {"score": 0.03887929413365062, "phrase": "predictive_performance"}, {"score": 0.004721162131188487, "phrase": "machine_learning"}, {"score": 0.004684157900507474, "phrase": "software_defect_prediction"}, {"score": 0.004539005582217305, "phrase": "defect-prone_software_components"}, {"score": 0.004261998281333649, "phrase": "different_techniques"}, {"score": 0.0040176151419992956, "phrase": "reliable_defect_prediction_model"}, {"score": 0.003728000957725218, "phrase": "largest_effect"}, {"score": 0.003247868216333148, "phrase": "empirical_prediction_results"}, {"score": 0.00320970209394799, "phrase": "reverse_engineering"}, {"score": 0.0031845067058084583, "phrase": "common_response_variable"}, {"score": 0.003134705849154561, "phrase": "random_effects_anova_model"}, {"score": 0.003085681391556093, "phrase": "relative_contribution"}, {"score": 0.0029781530607129653, "phrase": "researcher_group"}, {"score": 0.002943147037670826, "phrase": "model_prediction_performance"}, {"score": 0.0024842242921857705, "phrase": "high_level"}, {"score": 0.002445347925313158, "phrase": "defect_prediction_researchers"}, {"score": 0.00238816839721704, "phrase": "blind_analysis"}, {"score": 0.002332322771009122, "phrase": "reporting_protocols"}, {"score": 0.002233301265743633, "phrase": "expertise_issues"}], "paper_keywords": ["Software defect prediction", " meta-analysis", " researcher bias"], "paper_abstract": "Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.", "paper_title": "Researcher Bias: The Use of Machine Learning in Software Defect Prediction", "paper_id": "WOS:000338030200004"}