{"auto_keywords": [{"score": 0.048727646052725185, "phrase": "svr"}, {"score": 0.017078250692824277, "phrase": "rvm"}, {"score": 0.012356140237660804, "phrase": "mvc"}, {"score": 0.00481495049065317, "phrase": "multivariate_calibration_purposes"}, {"score": 0.004717485406306594, "phrase": "support_vector_regression"}, {"score": 0.004621984064678252, "phrase": "least_square_support_vector_machines"}, {"score": 0.00447316895126645, "phrase": "regression_purposes"}, {"score": 0.004329124434198367, "phrase": "advantageous_alternatives"}, {"score": 0.004276306563103443, "phrase": "existing_linear"}, {"score": 0.004241451614311598, "phrase": "nonlinear_multivariate_calibration"}, {"score": 0.004138575034269399, "phrase": "relevance_vector_machines"}, {"score": 0.003751331782523201, "phrase": "standard_svm-based_ones"}, {"score": 0.0034990853199511982, "phrase": "regression_tasks"}, {"score": 0.003414153126809511, "phrase": "arbitrary_basis_functions"}, {"score": 0.0033449479263613848, "phrase": "probability_estimates"}, {"score": 0.003250403288147613, "phrase": "excellent_sparseness_capabilities"}, {"score": 0.003158522476399068, "phrase": "simple_and_robust_model"}, {"score": 0.0030944828434567966, "phrase": "different_properties"}, {"score": 0.0029581155916843663, "phrase": "nonlinear_mvc_method"}, {"score": 0.002898127432512101, "phrase": "ill-posed_problems"}, {"score": 0.002759058732719621, "phrase": "linear_and_non-linear_solutions"}, {"score": 0.0026159017392986595, "phrase": "rvm_performance"}, {"score": 0.0025628355581338563, "phrase": "best_results"}, {"score": 0.002500571634075998, "phrase": "final_model"}, {"score": 0.0024298350266697905, "phrase": "prediction_process"}, {"score": 0.0021662983565317283, "phrase": "nonlinear_problems"}, {"score": 0.002148604369150115, "phrase": "mvc._copyright"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["relevance vector machines", " multivariate calibration", " Bayesian learning", " Kernel methods"], "paper_abstract": "The introduction of support vector regression (SVR) and least square support vector machines (LS-SVM) methods for regression purposes in the field of chemometrics has provided advantageous alternatives to the existing linear and nonlinear multivariate calibration (MVC) approaches. Relevance vector machines (RVMs) claim the advantages attributed to all the SVM-based methods over many other regression methods. Additionally, it also exhibits advantages over the standard SVM-based ones since: it is not necessary to estimate the error/margin trade-off parameter C and the insensitivity parameter epsilon in regression tasks, it is applicable to arbitrary basis functions, the algorithm gives probability estimates seamlessly and offer, additionally, excellent sparseness capabilities, which can result in a simple and robust model for the estimation of different properties. This paper presents the use of RVMs as a nonlinear MVC method capable of dealing with ill-posed problems. To study its behavior, three different chemometric benchmark datasets are considered, including both linear and non-linear solutions. RVM was compared with other calibration approaches reported in the literature. Although RVM performance is comparable with the best results obtained by LS-SVM, the final model achieved is sparser, so the prediction process is faster. Taking into account the other advantages attributed to RVMs, it can be concluded that this technique can be seen as a very promising option to solve nonlinear problems in MVC. Copyright (C) 2008 John Wiley & Sons, Ltd.", "paper_title": "Relevance vector machines for multivariate calibration purposes", "paper_id": "WOS:000262017500016"}