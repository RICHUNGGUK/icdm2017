{"auto_keywords": [{"score": 0.029947261988086258, "phrase": "response_rates"}, {"score": 0.00481495049065317, "phrase": "alert_fatigue"}, {"score": 0.004749646583168342, "phrase": "ehr-based_clinical_trial_alerts"}, {"score": 0.004669254965563445, "phrase": "randomized_controlled_study"}, {"score": 0.004637478829869904, "phrase": "objective_inadequate_participant_recruitment"}, {"score": 0.004590217765551337, "phrase": "major_problem"}, {"score": 0.004558977045730675, "phrase": "clinical_research"}, {"score": 0.004527947981009528, "phrase": "recent_studies"}, {"score": 0.004466519142879166, "phrase": "electronic_health_record"}, {"score": 0.004200197763119297, "phrase": "participant_recruitment"}, {"score": 0.00388278852285898, "phrase": "repeated_exposure"}, {"score": 0.0038170129089059013, "phrase": "declining_user_responsiveness"}, {"score": 0.003688773272422118, "phrase": "future_cta_deployments"}, {"score": 0.0035526625882388936, "phrase": "response_patterns"}, {"score": 0.0034450303951025704, "phrase": "ongoing_clinical_trial"}, {"score": 0.0033294038950375977, "phrase": "cta"}, {"score": 0.003056475095474338, "phrase": "total_number"}, {"score": 0.002963830787280652, "phrase": "time_period"}, {"score": 0.0029235593748707495, "phrase": "poisson_regression"}, {"score": 0.002903628734069691, "phrase": "results_response_rates"}, {"score": 0.002873986510042009, "phrase": "significant_downward_trend"}, {"score": 0.002767856819656175, "phrase": "advancing_time_period"}, {"score": 0.0025584115892805384, "phrase": "subgroup_analyses"}, {"score": 0.0024893297856666808, "phrase": "university-based_physicians"}, {"score": 0.002447101794397243, "phrase": "discussion_cta_responsiveness"}, {"score": 0.0024138345020219333, "phrase": "prolonged_exposure"}, {"score": 0.002285216591220303, "phrase": "notable_differences"}, {"score": 0.002238767763626708, "phrase": "university-based_users"}, {"score": 0.0021782980803068505, "phrase": "limited_literature"}, {"score": 0.002141334979696276, "phrase": "ehr-based_alert_fatigue"}, {"score": 0.0021049977753042253, "phrase": "future_tailoring"}], "paper_keywords": [""], "paper_abstract": "Objective Inadequate participant recruitment is a major problem facing clinical research. Recent studies have demonstrated that electronic health record (EHR)-based, point-of-care, clinical trial alerts (CTA) can improve participant recruitment to certain clinical research studies. Despite their promise, much remains to be learned about the use of CTAs. Our objective was to study whether repeated exposure to such alerts leads to declining user responsiveness and to characterize its extent if present to better inform future CTA deployments. Methods During a 36-week study period, we systematically documented the response patterns of 178 physician users randomized to receive CTAs for an ongoing clinical trial. Data were collected on: (1) response rates to the CTA; and (2) referral rates per physician, per time unit. Variables of interest were offset by the log of the total number of alerts received by that physician during that time period, in a Poisson regression. Results Response rates demonstrated a significant downward trend across time, with response rates decreasing by 2.7% for each advancing time period, significantly different from zero (flat) (p<0.0001). Even after 36 weeks, response rates remained in the 30%-40% range. Subgroup analyses revealed differences between community-based versus university-based physicians (p=0.0489). Discussion CTA responsiveness declined gradually over prolonged exposure, although it remained reasonably high even after 36 weeks of exposure. There were also notable differences between community-based versus university-based users. Conclusions These findings add to the limited literature on this form of EHR-based alert fatigue and should help inform future tailoring, deployment, and further study of CTAs.", "paper_title": "Evaluating alert fatigue over time to EHR-based clinical trial alerts: findings from a randomized controlled study", "paper_id": "WOS:000314151400024"}