{"auto_keywords": [{"score": 0.050073684472307214, "phrase": "gpu_memory"}, {"score": 0.03731450553819306, "phrase": "software_scheduling"}, {"score": 0.03414229704911109, "phrase": "gpu"}, {"score": 0.03303025864562151, "phrase": "gpuswap"}, {"score": 0.007772300669729129, "phrase": "gpu_kernels"}, {"score": 0.00743719384691911, "phrase": "sufficient_gpu_memory"}, {"score": 0.005119448202776078, "phrase": "gpu_applications"}, {"score": 0.0047823219739028325, "phrase": "transparent_swapping"}, {"score": 0.004606759374526927, "phrase": "cloud_computing_platforms"}, {"score": 0.0044225461990535855, "phrase": "low_cost"}, {"score": 0.004347972379250724, "phrase": "large_portion"}, {"score": 0.004303830426772906, "phrase": "cloud's_cost_advantage"}, {"score": 0.004216880725292, "phrase": "cloud_providers"}, {"score": 0.003912768486550741, "phrase": "promised_resources"}, {"score": 0.003705412219296983, "phrase": "demand_paging"}, {"score": 0.003680274684654909, "phrase": "current_gpus"}, {"score": 0.0036305081978734127, "phrase": "recent_approaches"}, {"score": 0.00356924199356768, "phrase": "gpu_memory_resort"}, {"score": 0.003426373820419672, "phrase": "significant_runtime_overhead"}, {"score": 0.0028997895009747502, "phrase": "gpu's_ability"}, {"score": 0.0028703075616629634, "phrase": "system_ram"}, {"score": 0.0028218336994432864, "phrase": "gpu's_own_memory"}, {"score": 0.0026813053666249896, "phrase": "ram"}, {"score": 0.0026449626324264275, "phrase": "memory_pressure"}, {"score": 0.002177991273791155, "phrase": "permanent_overhead"}, {"score": 0.0021049977753042253, "phrase": "native_performance"}], "paper_keywords": ["Virtualization", " Memory Overcommitment", " Oversubscription", " Swapping", " GPU"], "paper_abstract": "Over the last few years, GPUs have been finding their way into cloud computing platforms, allowing users to benefit from the performance of GPUs at low cost. However, a large portion of the cloud's cost advantage traditionally stems from oversubscription: Cloud providers rent out more resources to their customers than are actually available, expecting that the customers will not actually use all of the promised resources. For GPU memory, this oversubscription is difficult due to the lack of support for demand paging in current GPUs. Therefore, recent approaches to enabling oversubscription of GPU memory resort to software scheduling of GPU kernels - which has been shown to induce significant runtime overhead in applications even if sufficient GPU memory is available - to ensure that data is present on the GPU when referenced. In this paper, we present GPUswap, a novel approach to enabling oversubscription of GPU memory that does not rely on software scheduling of GPU kernels. GPUswap uses the GPU's ability to access system RAM directly to extend the GPU's own memory. To that end, GPUswap transparently relocates data from the GPU to system RAM in response to memory pressure. GPUswap ensures that all data is permanently accessible to the GPU and thus allows applications to submit commands to the GPU directly at any time, without the need for software scheduling. Experiments with our prototype implementation show that GPU applications can still execute even with only 20 MB of GPU memory available. In addition, while software scheduling suffers from permanent overhead even with sufficient GPU memory available, our approach executes GPU applications with native performance.", "paper_title": "GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping", "paper_id": "WOS:000361284300006"}