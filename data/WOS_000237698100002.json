{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "deep_belief_nets"}, {"score": 0.004602233424629559, "phrase": "\"complementary_priors"}, {"score": 0.004456044928556639, "phrase": "explaining-away_effects"}, {"score": 0.004286708921931472, "phrase": "densely_connected_belief_nets"}, {"score": 0.004123781267600542, "phrase": "complementary_priors"}, {"score": 0.0036710872046569532, "phrase": "top_two_layers"}, {"score": 0.0036006068766833103, "phrase": "undirected_associative_memory"}, {"score": 0.003486123304074186, "phrase": "greedy_algorithm"}, {"score": 0.003353521531649663, "phrase": "slower_learning_procedure"}, {"score": 0.003163985877966426, "phrase": "contrastive_version"}, {"score": 0.0031032109441552287, "phrase": "wake-sleep_algorithm"}, {"score": 0.0028530201960995896, "phrase": "joint_distribution"}, {"score": 0.00281635677605879, "phrase": "handwritten_digit_images"}, {"score": 0.002726740692380339, "phrase": "generative_model"}, {"score": 0.0026916956768363158, "phrase": "better_digit_classification"}, {"score": 0.002639968630788011, "phrase": "best_discriminative_learning_algorithms"}, {"score": 0.0025892330566358503, "phrase": "low-dimensional_manifolds"}, {"score": 0.0025230952890994236, "phrase": "digits_lie"}, {"score": 0.0024586427394152196, "phrase": "long_ravines"}, {"score": 0.002411383506019635, "phrase": "free-energy_landscape"}, {"score": 0.0023650305205893353, "phrase": "top-level_associative_memory"}, {"score": 0.0021742244283295986, "phrase": "directed_connections"}, {"score": 0.0021049977753042253, "phrase": "associative_memory"}], "paper_keywords": [""], "paper_abstract": "We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "paper_title": "A fast learning algorithm for deep belief nets", "paper_id": "WOS:000237698100002"}