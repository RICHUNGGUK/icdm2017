{"auto_keywords": [{"score": 0.049692788583269344, "phrase": "free-surface_flows"}, {"score": 0.031710251985384436, "phrase": "single_gpu"}, {"score": 0.00481495049065317, "phrase": "accelerating_smoothed_particle_hydrodynamics_simulations"}, {"score": 0.004737579882204798, "phrase": "multi-gpu_clusters"}, {"score": 0.004642604455537883, "phrase": "single_graphics_processing_unit"}, {"score": 0.004605497287147945, "phrase": "gpu"}, {"score": 0.004512814529823301, "phrase": "smoothed_particle_hydrodynamics"}, {"score": 0.00436890075128947, "phrase": "multi-gpu_sph_program"}, {"score": 0.004161553532799662, "phrase": "spatial_decomposition_technique"}, {"score": 0.0041112665381266315, "phrase": "different_portions"}, {"score": 0.003980106938194548, "phrase": "physical_system"}, {"score": 0.003884479830773329, "phrase": "different_gpus"}, {"score": 0.0035530197642369464, "phrase": "inter-gpu_particle_migration"}, {"score": 0.0034817084307034955, "phrase": "\"halo\"_building"}, {"score": 0.003398015125667192, "phrase": "sph_particles"}, {"score": 0.003370565320453525, "phrase": "different_sub-domains"}, {"score": 0.0032497506915502876, "phrase": "resulting_scheme"}, {"score": 0.0026423134393535265, "phrase": "current_architecture"}, {"score": 0.0025167026017217926, "phrase": "memory_constraints"}, {"score": 0.002466139325703204, "phrase": "weak_and_strong_scaling_behaviour"}, {"score": 0.0023970487263732737, "phrase": "resulting_program"}, {"score": 0.002311047934798247, "phrase": "computational_bottlenecks"}, {"score": 0.0022101055924375725, "phrase": "computational_efficiency"}, {"score": 0.0021922308351126746, "phrase": "future_versions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Smoothed particle hydrodynamics", " SPH", " CUDA", " GPU", " Multi-GPU", " Graphics processing unit", " Computational fluid dynamics", " Molecular dynamics"], "paper_abstract": "Starting from the single graphics processing unit (GPU) version of the Smoothed Particle Hydrodynamics (SPH) code DualSPHysics, a multi-GPU SPH program is developed for free-surface flows. The approach is based on a spatial decomposition technique, whereby different portions (sub-domains) of the physical system under study are assigned to different GPUs. Communication between devices is achieved with the use of Message Passing Interface (MPI) application programming interface (API) routines. The use of the sorting algorithm radix sort for inter-GPU particle migration and sub-domain \"halo\" building (which enables interaction between SPH particles of different sub-domains) is described in detail. With the resulting scheme it is possible, on the one hand, to carry out simulations that could also be performed on a single GPU, but they can now be performed even faster than on one of these devices alone. On the other hand, accelerated simulations can be performed with up to 32 million particles on the current architecture, which is beyond the limitations of a single GPU due to memory constraints. A study of weak and strong scaling behaviour, speedups and efficiency of the resulting program is presented including an investigation to elucidate the computational bottlenecks. Last, possibilities for reduction of the effects of overhead on computational efficiency in future versions of our scheme are discussed. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Towards accelerating smoothed particle hydrodynamics simulations for free-surface flows on multi-GPU clusters", "paper_id": "WOS:000325447100008"}