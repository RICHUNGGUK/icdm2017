{"auto_keywords": [{"score": 0.04911004508284347, "phrase": "nonholonomic_mobile_robot"}, {"score": 0.015719716506582538, "phrase": "epipolar_geometry"}, {"score": 0.015593050244537988, "phrase": "visual_control"}, {"score": 0.004603115982404017, "phrase": "robot_system"}, {"score": 0.004565609517511076, "phrase": "camera_information"}, {"score": 0.004509919457793852, "phrase": "challenging_task"}, {"score": 0.00447316895126645, "phrase": "unpredictable_conditions"}, {"score": 0.0044005599565517875, "phrase": "feature_point_mismatch"}, {"score": 0.004346874334971379, "phrase": "scene_illumination"}, {"score": 0.0040880724328903, "phrase": "real_world_circumstances"}, {"score": 0.004038183610525504, "phrase": "machine_learning_techniques"}, {"score": 0.003988901168289797, "phrase": "novel_intelligent_approach"}, {"score": 0.003956379470617348, "phrase": "mobile_robots"}, {"score": 0.003924121879214595, "phrase": "neural_networks"}, {"score": 0.0034990853199511982, "phrase": "direct_mapping"}, {"score": 0.003456359260921982, "phrase": "image_space"}, {"score": 0.003414153126809511, "phrase": "actuator_command"}, {"score": 0.003304097742490627, "phrase": "offline_phase"}, {"score": 0.003277140908834426, "phrase": "nn-lfd_approach"}, {"score": 0.0031714880645482496, "phrase": "feature_position"}, {"score": 0.0031327496009959464, "phrase": "image_plane"}, {"score": 0.0030944828434567966, "phrase": "angular_velocity"}, {"score": 0.0030692309090066166, "phrase": "lateral_motion_correction"}, {"score": 0.0030317376771013703, "phrase": "online_phase"}, {"score": 0.0029824560656987855, "phrase": "switching_vision"}, {"score": 0.0028510114763213596, "phrase": "nn-lfd"}, {"score": 0.0027253441561528495, "phrase": "feature_distance"}, {"score": 0.0026920404458807444, "phrase": "pre-defined_interest_area"}, {"score": 0.002510843167476253, "phrase": "optimal_solution"}, {"score": 0.0024903420152789135, "phrase": "robot_control"}, {"score": 0.002459902948098627, "phrase": "best_training_outcomes"}, {"score": 0.0024298350266697905, "phrase": "learning_algorithms"}, {"score": 0.002380534385718454, "phrase": "real_time"}, {"score": 0.0023322317017275803, "phrase": "optimal_nn_configuration"}, {"score": 0.0023131855385489764, "phrase": "robot_orientation_correction"}, {"score": 0.002229380111189829, "phrase": "structured_indoor_environment"}, {"score": 0.002202123699556382, "phrase": "excellent_performance"}, {"score": 0.002157433262687853, "phrase": "system_robustness"}, {"score": 0.0021049977753042253, "phrase": "desired_location"}], "paper_keywords": ["Neural network", " Learning from demonstration", " Epipolar geometry", " Visual control", " Mobile robot"], "paper_abstract": "The control of a robot system using camera information is a challenging task regarding unpredictable conditions, such as feature point mismatch and changing scene illumination. This paper presents a solution for the visual control of a nonholonomic mobile robot in demanding real world circumstances based on machine learning techniques. A novel intelligent approach for mobile robots using neural networks (NNs), learning from demonstration (LfD) framework, and epipolar geometry between two views is proposed and evaluated in a series of experiments. A direct mapping from the image space to the actuator command is conducted using two phases. In an offline phase, NN-LfD approach is employed in order to relate the feature position in the image plane with the angular velocity for lateral motion correction. An online phase refers to a switching vision based scheme between the epipole based linear velocity controller and NN-LfD based angular velocity controller, which selection depends on the feature distance from the pre-defined interest area in the image. In total, 18 architectures and 6 learning algorithms are tested in order to find optimal solution for robot control. The best training outcomes for each learning algorithms are then employed in real time so as to discover optimal NN configuration for robot orientation correction. Experiments conducted on a nonholonomic mobile robot in a structured indoor environment confirm an excellent performance with respect to the system robustness and positioning accuracy in the desired location.", "paper_title": "Neural network learning from demonstration and epipolar geometry for visual control of a nonholonomic mobile robot", "paper_id": "WOS:000334522300012"}