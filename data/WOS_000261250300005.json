{"auto_keywords": [{"score": 0.004728624975677976, "phrase": "highly_expressive_way"}, {"score": 0.004588158931776543, "phrase": "animated_characters"}, {"score": 0.004425072111844721, "phrase": "non-verbal_expression_call"}, {"score": 0.004242085112539693, "phrase": "important_aspect"}, {"score": 0.004191200981137508, "phrase": "non-verbal_expression"}, {"score": 0.0029707940194245216, "phrase": "data_driven_approach"}, {"score": 0.00291743479583421, "phrase": "interactive_social_behavior"}, {"score": 0.0025542617599268323, "phrase": "ail_animation_model"}, {"score": 0.0021960024679620886, "phrase": "real_human"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["Computer animation", " non-verbal behavior", " motion capture", " machine learning"], "paper_abstract": "Humans use their bodies in a highly expressive way during conversation, and animated characters that lack this form of non-verbal expression call seem stiff and unemotional. An important aspect Of non-verbal expression is that people respond to each others behavior and are highly attuned to picking up this type of response. This is particularly important for the feedback given while listening to some one speak. However, automatically generating this type of behavior is difficult as it is highly complex and subtle. This paper takes a data driven approach to generating interactive social behavior. Listening behavior is motion captured, together with the audio being listened to. These data are used to learn ail animation model of the responses of one person to the other. This allows its to create characters that respond in real-time during a conversation with a real human. Copyright (C) 2008 John Wiley & Sons, Ltd.", "paper_title": "Responsive listening behavior", "paper_id": "WOS:000261250300005"}