{"auto_keywords": [{"score": 0.02540596078500977, "phrase": "reference_image"}, {"score": 0.00481495049065317, "phrase": "unrestrained_depth_inference"}, {"score": 0.004782455247247007, "phrase": "coherent_occlusion_filling"}, {"score": 0.004750178261675602, "phrase": "traditional_depth_estimation_methods"}, {"score": 0.004623224002010226, "phrase": "internal_parameters"}, {"score": 0.004364550313102054, "phrase": "extrinsic_parameters"}, {"score": 0.0037857732912790353, "phrase": "dof"}, {"score": 0.0037093941875325862, "phrase": "defocus_blur"}, {"score": 0.0036595239003893287, "phrase": "fov"}, {"score": 0.0036347141744359442, "phrase": "camera_motion"}, {"score": 0.0036101552409969037, "phrase": "image_acquisition"}, {"score": 0.003525493807130663, "phrase": "unfettered_operation"}, {"score": 0.0034898187091432806, "phrase": "ots_camera"}, {"score": 0.0033964341293094545, "phrase": "pixel_motion"}, {"score": 0.003350679980619693, "phrase": "optical_defocus_blur"}, {"score": 0.0033167679667625667, "phrase": "captured_images"}, {"score": 0.0032610064507843776, "phrase": "depth_estimation_framework"}, {"score": 0.0032389643666649094, "phrase": "calibrated_images"}, {"score": 0.0032061793819537633, "phrase": "general_camera_motion_and_lens_parameter_variations"}, {"score": 0.0031203609829058587, "phrase": "constrained_areas"}, {"score": 0.002895986736522535, "phrase": "focus_variation"}, {"score": 0.0028569549040495163, "phrase": "parallax_and_stereo_occlusions"}, {"score": 0.002771040606604632, "phrase": "associated_challenges"}, {"score": 0.002696837864985637, "phrase": "user-defined_foreground_occluders"}, {"score": 0.002669525619637691, "phrase": "reference_depth_map"}, {"score": 0.0021776870246727233, "phrase": "missing_regions"}, {"score": 0.0021049977753042253, "phrase": "local_neighbours"}], "paper_keywords": ["Defocus blur", " Multi-view stereo", " Depth from defocus", " Belief propagation", " Inpainting"], "paper_abstract": "Traditional depth estimation methods typically exploit the effect of either the variations in internal parameters such as aperture and focus (as in depth from defocus), or variations in extrinsic parameters such as position and orientation of the camera (as in stereo). When operating off-the-shelf (OTS) cameras in a general setting, these parameters influence the depth of field (DOF) and field of view (FOV). While DOF mandates one to deal with defocus blur, a larger FOV necessitates camera motion during image acquisition. As a result, for unfettered operation of an OTS camera, it becomes inevitable to account for pixel motion as well as optical defocus blur in the captured images. We propose a depth estimation framework using calibrated images captured under general camera motion and lens parameter variations. Our formulation seeks to generalize the constrained areas of stereo and shape from defocus (SFD)/focus (SFF) by handling, in tandem, various effects such as focus variation, zoom, parallax and stereo occlusions, all under one roof. One of the associated challenges in such an unrestrained scenario is the problem of removing user-defined foreground occluders in the reference depth map and image (termed inpainting of depth and image). Inpainting is achieved by exploiting the cue from motion parallax to discover (in other images) the correspondence/color information missing in the reference image. Moreover, considering the fact that the observations could be differently blurred, it is important to ensure that the degree of defocus in the missing regions (in the reference image) is coherent with the local neighbours (defocus inpainting).", "paper_title": "Towards Unrestrained Depth Inference with Coherent Occlusion Filling", "paper_id": "WOS:000300674200003"}