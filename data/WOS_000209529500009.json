{"auto_keywords": [{"score": 0.04951738916270167, "phrase": "feature_selection"}, {"score": 0.00481495049065317, "phrase": "supervised_feature_selection"}, {"score": 0.004674952036988355, "phrase": "key_task"}, {"score": 0.004620093797504386, "phrase": "statistical_pattern_recognition"}, {"score": 0.0045122922461952805, "phrase": "selection_algorithms"}, {"score": 0.004355267258397068, "phrase": "specific_objective_functions"}, {"score": 0.0036914757355166966, "phrase": "basic_objectives"}, {"score": 0.00314708280192844, "phrase": "evaluation_metrics"}, {"score": 0.0030195184114140063, "phrase": "score_function"}, {"score": 0.002931569771634119, "phrase": "conditional_independence_structure"}, {"score": 0.002897109710213723, "phrase": "probabilistic_distributions"}, {"score": 0.0027961342030212353, "phrase": "leave-one-out_feature_selection_algorithm"}, {"score": 0.00266693888510356, "phrase": "leave-one-out_algorithm"}, {"score": 0.002620040660435039, "phrase": "conventional_greedy_backward_elimination_algorithm"}, {"score": 0.0024842242921857705, "phrase": "selection_process"}, {"score": 0.0023277282036119106, "phrase": "unified_way"}, {"score": 0.0021940118003312397, "phrase": "different_feature_evaluation_metrics"}, {"score": 0.0021049977753042253, "phrase": "popular_feature_selection_algorithms"}], "paper_keywords": ["leave-one-out", " feature selection objectives", " evaluation metrics"], "paper_abstract": "Feature selection is a key task in statistical pattern recognition. Most feature selection algorithms have been proposed based on specific objective functions which are usually intuitively reasonable but can sometimes be far from the more basic objectives of the feature selection. This paper describes how to select features such that the basic objectives, e.g., classification or clustering accuracies, can be optimized in a more direct way. The analysis requires that the contribution of each feature to the evaluation metrics can be quantitatively described by some score function. Motivated by the conditional independence structure in probabilistic distributions, the analysis uses a leave-one-out feature selection algorithm which provides an approximate solution. The leave-one-out algorithm improves the conventional greedy backward elimination algorithm by preserving more interactions among features in the selection process, so that the various feature selection objectives can be optimized in a unified way. Experiments on six real-world datasets with different feature evaluation metrics have shown that this algorithm outperforms popular feature selection algorithms in most situations.", "paper_title": "Efficient Leave-One-Out Strategy for Supervised Feature Selection", "paper_id": "WOS:000209529500009"}