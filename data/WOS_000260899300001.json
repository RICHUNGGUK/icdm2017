{"auto_keywords": [{"score": 0.039774496364856735, "phrase": "nn_rule"}, {"score": 0.008912984923052446, "phrase": "kernel_methods"}, {"score": 0.00481495049065317, "phrase": "traditional_kernels"}, {"score": 0.004329916183121203, "phrase": "pairwise_proximity_data"}, {"score": 0.004211594861013822, "phrase": "specifically_designed_kernels"}, {"score": 0.004153647187175639, "phrase": "nearest_neighbor"}, {"score": 0.0038756055612167942, "phrase": "arbitrary_proximities"}, {"score": 0.003839962121679795, "phrase": "necessary_corrections"}, {"score": 0.003616108223034427, "phrase": "local_decisions"}, {"score": 0.0034368818187904744, "phrase": "indispensable_explanation"}, {"score": 0.00309023627455429, "phrase": "accurate_classifiers"}, {"score": 0.0030196181347744372, "phrase": "training_objects"}, {"score": 0.002991823455621222, "phrase": "demand_comparisons"}, {"score": 0.002950608985750058, "phrase": "small_set"}, {"score": 0.002843450389745497, "phrase": "meaningful_dissimilarity_measures"}, {"score": 0.0028042741141125712, "phrase": "non-euclidean_and_nonmetric_ones"}, {"score": 0.002640636493247601, "phrase": "supervised_learning"}, {"score": 0.0026163210719740847, "phrase": "simple_classifiers"}, {"score": 0.0024865237976623286, "phrase": "computational_complexity"}, {"score": 0.002363150587804677, "phrase": "appealing_alternative"}, {"score": 0.0023198180380964305, "phrase": "proximity_data"}, {"score": 0.0021049977753042253, "phrase": "noisy_results"}], "paper_keywords": ["Classifier design and evaluation", " indefinite kernels", " similarity measures", " statistical learning"], "paper_abstract": "Proximity captures the degree of similarity between examples and is thereby fundamental in learning. Learning from pairwise proximity data usually relies on either kernel methods for specifically designed kernels or the nearest neighbor (NN) rule. Kernel methods are powerful, but often cannot handle arbitrary proximities without necessary corrections. The NN rule can work well in such cases, but suffers from local decisions. The aim of this paper is to provide an indispensable explanation and insights about two simple yet powerful alternatives when neither conventional kernel methods nor the NN rule can perform best. These strategies use two proximity-based representation spaces (RSs) in which accurate classifiers are trained on all training objects and demand comparisons to a small set of prototypes. They can handle all meaningful dissimilarity measures, including non-Euclidean and nonmetric ones. Practical examples illustrate that these RSs can be highly advantageous in supervised learning. Simple classifiers built there tend to outperform the NN rule. Moreover, computational complexity may be controlled. Consequently, these approaches offer an appealing alternative to learn from proximity data for which kernel methods cannot directly be applied, are too costly or impractical, while the NN rule leads to noisy results.", "paper_title": "Beyond Traditional Kernels: Classification in Two Dissimilarity-Based Representation Spaces", "paper_id": "WOS:000260899300001"}