{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "punctual_emotion_annotations"}, {"score": 0.0047051051659828275, "phrase": "smart_environment"}, {"score": 0.0045765783794477505, "phrase": "user's_affective_state"}, {"score": 0.004410629943506433, "phrase": "user's_underlying_mood"}, {"score": 0.0042506731642425275, "phrase": "automatic_punctual_emotion_recognition"}, {"score": 0.004002972723329302, "phrase": "longer-lasting_affective_states"}, {"score": 0.003787108326100196, "phrase": "emotion_recognition_algorithms"}, {"score": 0.0035994372026264478, "phrase": "known_sequence"}, {"score": 0.0035663245542174224, "phrase": "punctual_emotions"}, {"score": 0.003389557201018519, "phrase": "human_annotations"}, {"score": 0.003312121699003206, "phrase": "vam"}, {"score": 0.0032665093519968083, "phrase": "humaine."}, {"score": 0.0029234476524900794, "phrase": "emotion_space"}, {"score": 0.002843450389745497, "phrase": "emotion_annotations"}, {"score": 0.0027275289970185015, "phrase": "mood_estimation"}, {"score": 0.0026651772919894534, "phrase": "discrete_emotion_annotations"}, {"score": 0.0026042472337367015, "phrase": "video_timespan"}, {"score": 0.0025683581446365165, "phrase": "second_analysis"}, {"score": 0.0024865237976623286, "phrase": "mood_recognition"}, {"score": 0.002407290605654707, "phrase": "individual_human_coders"}, {"score": 0.0023741093119014436, "phrase": "underlying_mood"}, {"score": 0.0023091093387071593, "phrase": "moving_average_function"}, {"score": 0.0022878396720586044, "phrase": "exponential_discount"}, {"score": 0.0022355167967736326, "phrase": "mood_prediction_accuracy"}, {"score": 0.0021443262599352996, "phrase": "chance_level"}, {"score": 0.0021049977753042253, "phrase": "mutual_human_agreement"}], "paper_keywords": ["Emotion recognition", " automatic mood recognition", " affective computing", " pervasive technology"], "paper_abstract": "A smart environment designed to adapt to a user's affective state should be able to decipher unobtrusively that user's underlying mood. Great effort has been devoted to automatic punctual emotion recognition from visual input. Conversely, little has been done to recognize longer-lasting affective states, such as mood. Taking for granted the effectiveness of emotion recognition algorithms, we propose a model for estimating mood from a known sequence of punctual emotions. To validate our model experimentally, we rely on the human annotations of two well-established databases: the VAM and the HUMAINE. We perform two analyses: the first serves as a proof of concept and tests whether punctual emotions cluster around the mood in the emotion space. The results indicate that emotion annotations, continuous in time and value, facilitate mood estimation, as opposed to discrete emotion annotations scattered randomly within the video timespan. The second analysis explores factors that account for the mood recognition from emotions, by examining how individual human coders perceive the underlying mood of a person. A moving average function with exponential discount of the past emotions achieves mood prediction accuracy above 60 percent, which is higher than the chance level and higher than mutual human agreement.", "paper_title": "Predicting Mood from Punctual Emotion Annotations on Videos", "paper_id": "WOS:000356172600010"}