{"auto_keywords": [{"score": 0.040029450721470995, "phrase": "emotional_content"}, {"score": 0.00481495049065317, "phrase": "context-sensitive_learning_for_enhanced_audiovisual_emotion_classification"}, {"score": 0.004770211811866158, "phrase": "human_emotional_expression"}, {"score": 0.004638462976078815, "phrase": "structured_manner"}, {"score": 0.003994624397219346, "phrase": "emotional_display"}, {"score": 0.0039024014688338964, "phrase": "recent_emotional_displays"}, {"score": 0.0037242701420971062, "phrase": "relevant_temporal_context"}, {"score": 0.0034559594248079807, "phrase": "audio-visual_recognition"}, {"score": 0.0033761303704612734, "phrase": "improvised_emotional_interactions"}, {"score": 0.003329117824699891, "phrase": "utterance_level"}, {"score": 0.0032674478600540477, "phrase": "context-sensitive_schemes"}, {"score": 0.0030604342671868836, "phrase": "hidden_markov_model"}, {"score": 0.0028664985822266344, "phrase": "emotion_evolution"}, {"score": 0.0026105159490849364, "phrase": "long-term_temporal_context"}, {"score": 0.002562123444678208, "phrase": "emotion_recognition_systems"}, {"score": 0.0024912073108138613, "phrase": "emotional_manifestations"}, {"score": 0.00246800641971897, "phrase": "context-sensitive_approaches"}, {"score": 0.0023996890513009743, "phrase": "classification_tasks"}, {"score": 0.002344201345813952, "phrase": "valence_levels"}, {"score": 0.0022793032191981404, "phrase": "valence-activation_space"}, {"score": 0.002226592967032469, "phrase": "emotional_transitions"}, {"score": 0.0021347607855562102, "phrase": "affective_expressions"}, {"score": 0.0021049977753042253, "phrase": "potentially_useful_patterns"}], "paper_keywords": ["Audio-visual emotion recognition", " temporal context", " Hidden Markov models", " bidirectional long short term memory", " recurrent neural networks", " emotional grammars"], "paper_abstract": "Human emotional expression tends to evolve in a structured manner in the sense that certain emotional evolution patterns, i.e., anger to anger, are more probable than others, e. g., anger to happiness. Furthermore, the perception of an emotional display can be affected by recent emotional displays. Therefore, the emotional content of past and future observations could offer relevant temporal context when classifying the emotional content of an observation. In this work, we focus on audio-visual recognition of the emotional content of improvised emotional interactions at the utterance level. We examine context-sensitive schemes for emotion recognition within a multimodal, hierarchical approach: bidirectional Long Short-Term Memory (BLSTM) neural networks, hierarchical Hidden Markov Model classifiers (HMMs), and hybrid HMM/BLSTM classifiers are considered for modeling emotion evolution within an utterance and between utterances over the course of a dialog. Overall, our experimental results indicate that incorporating long-term temporal context is beneficial for emotion recognition systems that encounter a variety of emotional manifestations. Context-sensitive approaches outperform those without context for classification tasks such as discrimination between valence levels or between clusters in the valence-activation space. The analysis of emotional transitions in our database sheds light into the flow of affective expressions, revealing potentially useful patterns.", "paper_title": "Context-Sensitive Learning for Enhanced Audiovisual Emotion Classification", "paper_id": "WOS:000323627000006"}