{"auto_keywords": [{"score": 0.026705345501151934, "phrase": "visual_attention_model"}, {"score": 0.00481495049065317, "phrase": "robotic_cognition"}, {"score": 0.004658426838467567, "phrase": "cognitive_robotics_research"}, {"score": 0.004556901872357751, "phrase": "human-like_cognition"}, {"score": 0.0043764594918057915, "phrase": "action_planning"}, {"score": 0.004328494656899359, "phrase": "decision_making"}, {"score": 0.004265354774450696, "phrase": "cognitive_robotics"}, {"score": 0.004187718090913465, "phrase": "redundant_number"}, {"score": 0.00391969808162005, "phrase": "human-like_fashion"}, {"score": 0.003876719738482442, "phrase": "major_challenge"}, {"score": 0.0037505809981715024, "phrase": "enormous_amount"}, {"score": 0.003668768542734106, "phrase": "multiple_sensors"}, {"score": 0.0035887342563437935, "phrase": "information_management_skill"}, {"score": 0.0033837135367113004, "phrase": "tremendous_popularity"}, {"score": 0.0033589207694997413, "phrase": "robotic_research"}, {"score": 0.00322576184471691, "phrase": "biologically_inspired_intelligent"}, {"score": 0.0031553611552558986, "phrase": "b._webb_et_al"}, {"score": 0.003030247482854539, "phrase": "redundant_information_management"}, {"score": 0.0029100802280971065, "phrase": "visual_perception"}, {"score": 0.0028570410572695106, "phrase": "even_a_moderate_size_image"}, {"score": 0.002733646947011804, "phrase": "on-line_decision_making_process"}, {"score": 0.0026641365405034355, "phrase": "primates-like_visual_attention_mechanism"}, {"score": 0.002539689093411356, "phrase": "robotic_researchers"}, {"score": 0.00240328454980006, "phrase": "\"behaviorally_relevant\"_segment"}, {"score": 0.00224094835951312, "phrase": "ongoing_journey"}, {"score": 0.002224510044064184, "phrase": "robotics_research"}, {"score": 0.0021049977753042253, "phrase": "modern-day_robots"}], "paper_keywords": ["Human-robot interaction", " joint attention", " overt attention", " robotic cognition", " visual attention"], "paper_abstract": "The goal of the cognitive robotics research is to design robots with human-like cognition (albeit reduced complexity) in perception, reasoning, action planning, and decision making. Such a venture of cognitive robotics has developed robots with redundant number of sensors and actuators in order to perceive the world and act up on it in a human-like fashion. A major challenge to deal with these robots is managing the enormous amount of information continuously arriving through multiple sensors. The primates master this information management skill through their custom-built attention mechanism. Mimicking the attention behavior of the primates, therefore, has gained tremendous popularity in robotic research in the recent years (Bar-Cohen et al., Biologically Inspired Intelligent Robots, 2003, and B. Webb et al., Biorobotics, 2003). The difficulties of redundant information management, however, is the most severe in case of visual perception of the robots. Even a moderate size image of the natural scene generally contains enough visual information to easily overload the on-line decision making process of an autonomous robot. Modeling primates-like visual attention mechanism for the robot, therefore, is becoming more popular among the robotic researchers. A visual attention model enables the robot to selectively (and autonomously) choose a \"behaviorally relevant\" segment of visual information for further processing while relative exclusion of the others. This paper sheds light on the ongoing journey of robotics research to achieve a visual attention model which will serve as a component of cognition of the modern-day robots.", "paper_title": "Visual Attention for Robotic Cognition: A Survey", "paper_id": "WOS:000294905600011"}