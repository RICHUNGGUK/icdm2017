{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "full-multivariate_probability_distributions"}, {"score": 0.03534513334808799, "phrase": "ande"}, {"score": 0.004719012177985866, "phrase": "bayesian"}, {"score": 0.004646088032529303, "phrase": "averaged_n-dependence_estimators"}, {"score": 0.0041316803676012155, "phrase": "single_parameter"}, {"score": 0.003986684503518077, "phrase": "low-variance_high-bias_learner"}, {"score": 0.0038467574225322086, "phrase": "high-variance_low-bias_learner"}, {"score": 0.003807896759765217, "phrase": "bayes"}, {"score": 0.0036553070575015344, "phrase": "underlying_strategy"}, {"score": 0.0036181717017041387, "phrase": "averaged_one-dependence_estimators"}, {"score": 0.0034733518170738517, "phrase": "independence_assumption"}, {"score": 0.0033857878144822906, "phrase": "naive_bayes'_desirable_computational_and_theoretical_properties"}, {"score": 0.003120099687922751, "phrase": "extensive_experimental_evaluation"}, {"score": 0.0030569913145731408, "phrase": "bias-variance_trade-off"}, {"score": 0.002964706925144123, "phrase": "strong_predictive_accuracy"}, {"score": 0.0029196116767956273, "phrase": "wide_range"}, {"score": 0.002889928791032578, "phrase": "data_sets"}, {"score": 0.002831462722290752, "phrase": "training_time_linear"}, {"score": 0.0026630475331025955, "phrase": "single_pass"}, {"score": 0.002622528664187849, "phrase": "training_data"}, {"score": 0.0025826247025542213, "phrase": "incremental_learning"}, {"score": 0.0025433263646920364, "phrase": "directly_missing_values"}, {"score": 0.0023798115577749225, "phrase": "practical_utility"}, {"score": 0.0021705796895687864, "phrase": "low-bias_high-variance_generative_learners"}, {"score": 0.0021049977753042253, "phrase": "even_more_powerful_classifiers"}], "paper_keywords": ["Bayesian learning", " Classification learning", " Probabilistic learning", " Averaged one-dependence estimators", " Naive Bayes", " Semi-naive Bayesian learning", " Learning without model selection", " Ensemble learning", " Feating"], "paper_abstract": "Averaged n-Dependence Estimators (AnDE) is an approach to probabilistic classification learning that learns by extrapolation from marginal to full-multivariate probability distributions. It utilizes a single parameter that transforms the approach between a low-variance high-bias learner (Naive Bayes) and a high-variance low-bias learner with Bayes optimal asymptotic error. It extends the underlying strategy of Averaged One-Dependence Estimators (AODE), which relaxes the Naive Bayes independence assumption while retaining many of Naive Bayes' desirable computational and theoretical properties. AnDE further relaxes the independence assumption by generalizing AODE to higher-levels of dependence. Extensive experimental evaluation shows that the bias-variance trade-off for Averaged 2-Dependence Estimators results in strong predictive accuracy over a wide range of data sets. It has training time linear with respect to the number of examples, learns in a single pass through the training data, supports incremental learning, handles directly missing values, and is robust in the face of noise. Beyond the practical utility of its lower-dimensional variants, AnDE is of interest in that it demonstrates that it is possible to create low-bias high-variance generative learners and suggests strategies for developing even more powerful classifiers.", "paper_title": "Learning by extrapolation from marginal to full-multivariate probability distributions: decreasingly naive Bayesian classification", "paper_id": "WOS:000300589300003"}