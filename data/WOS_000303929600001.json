{"auto_keywords": [{"score": 0.04967913398382337, "phrase": "domain_knowledge"}, {"score": 0.039172026442386244, "phrase": "text_analysis"}, {"score": 0.00481495049065317, "phrase": "monte-carlo_framework"}, {"score": 0.004695303774351139, "phrase": "effective_performance"}, {"score": 0.004656082803151484, "phrase": "autonomous_control_systems"}, {"score": 0.004578616481036476, "phrase": "human_effort"}, {"score": 0.004408976280483648, "phrase": "control_algorithm"}, {"score": 0.004210114198411514, "phrase": "language_grounding"}, {"score": 0.00403709306806495, "phrase": "complex_control_application"}, {"score": 0.00377487157059352, "phrase": "control_performance"}, {"score": 0.0034708317974593136, "phrase": "textual_information"}, {"score": 0.0033844715014920233, "phrase": "text_segment"}, {"score": 0.0033141428467975795, "phrase": "current_game_state"}, {"score": 0.0032181231788427655, "phrase": "task-centric_predicate_structure"}, {"score": 0.003072807453368781, "phrase": "action_selection_policy"}, {"score": 0.002971249577042111, "phrase": "promising_regions"}, {"score": 0.0029340342068081247, "phrase": "action_space"}, {"score": 0.002789763110395331, "phrase": "multi-layer_neural_network"}, {"score": 0.0027203027216256013, "phrase": "latent_variables"}, {"score": 0.0026862220320327613, "phrase": "hidden_layers"}, {"score": 0.0025974077149412, "phrase": "output_layer"}, {"score": 0.002543392353000551, "phrase": "monte-carlo_search_framework"}, {"score": 0.002500987934534128, "phrase": "model_parameters"}, {"score": 0.002459288755782293, "phrase": "simulated_games"}, {"score": 0.0023879771786116228, "phrase": "complex_strategy_game"}, {"score": 0.0023779596230982234, "phrase": "civilization_ii"}, {"score": 0.0023481578019624843, "phrase": "official_game_manual"}, {"score": 0.0023187286045879643, "phrase": "text_guide"}, {"score": 0.0022609695885550058, "phrase": "linguistically-informed_game-playing_agent"}, {"score": 0.0021049977753042253, "phrase": "built-in_ai"}], "paper_keywords": [""], "paper_abstract": "Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.", "paper_title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "paper_id": "WOS:000303929600001"}