{"auto_keywords": [{"score": 0.02927436378146636, "phrase": "gaussian"}, {"score": 0.00481495049065317, "phrase": "nonlinear_system_modeling"}, {"score": 0.004766090327062509, "phrase": "random_matrices"}, {"score": 0.004717723626157272, "phrase": "echo_state_networks_revisited"}, {"score": 0.004669845451682529, "phrase": "echo_state_networks"}, {"score": 0.004529093447865156, "phrase": "novel_form"}, {"score": 0.004483120978156604, "phrase": "recurrent_neural_networks"}, {"score": 0.0043258453436258405, "phrase": "efficient_and_powerful_computational_model"}, {"score": 0.004174064076143015, "phrase": "unique_feature"}, {"score": 0.004027586827290042, "phrase": "large_number"}, {"score": 0.003525845849341026, "phrase": "bernoulli"}, {"score": 0.003438058620195309, "phrase": "large_randomly_generated_fixed_rnn"}, {"score": 0.0033173234105492895, "phrase": "nonlinear_systems"}, {"score": 0.0031042018721479385, "phrase": "random_matrix_theory"}, {"score": 0.003010496595090984, "phrase": "random_reservoirs"}, {"score": 0.002949598537993663, "phrase": "different_topologies"}, {"score": 0.002676692093594358, "phrase": "asymptotic_gap"}, {"score": 0.0026359661683151006, "phrase": "scaling_factor_bounds"}, {"score": 0.0025958582759972315, "phrase": "necessary_and_sufficient_conditions"}, {"score": 0.002530359917257626, "phrase": "echo_state_property"}, {"score": 0.002441422510910065, "phrase": "state_transition_mapping"}, {"score": 0.002392008392892289, "phrase": "high_probability"}, {"score": 0.0021705796895687864, "phrase": "echo_states"}, {"score": 0.0021375377706211686, "phrase": "spectral_radius"}, {"score": 0.0021049977753042253, "phrase": "reservoir_weight_matrix"}], "paper_keywords": ["Circular law", " concentration of measure", " echo state networks", " echo state property", " random matrix theory", " recurrent neural networks"], "paper_abstract": "Echo state networks (ESNs) are a novel form of recurrent neural networks (RNNs) that provide an efficient and powerful computational model approximating nonlinear dynamical systems. A unique feature of an ESN is that a large number of neurons (the \"reservoir\") are used, whose synaptic connections are generated randomly, with only the connections from the reservoir to the output modified by learning. Why a large randomly generated fixed RNN gives such excellent performance in approximating nonlinear systems is still not well understood. In this brief, we apply random matrix theory to examine the properties of random reservoirs in ESNs under different topologies (sparse or fully connected) and connection weights (Bernoulli or Gaussian). We quantify the asymptotic gap between the scaling factor bounds for the necessary and sufficient conditions previously proposed for the echo state property. We then show that the state transition mapping is contractive with high probability when only the necessary condition is satisfied, which corroborates and thus analytically explains the observation that in practice one obtains echo states when the spectral radius of the reservoir weight matrix is smaller than 1.", "paper_title": "Nonlinear System Modeling with Random Matrices: Echo State Networks Revisited", "paper_id": "WOS:000302703700017"}