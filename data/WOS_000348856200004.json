{"auto_keywords": [{"score": 0.04754047218347395, "phrase": "low-rank_matrix_approximation"}, {"score": 0.004605258779434384, "phrase": "gradient_method"}, {"score": 0.004361276005140776, "phrase": "computer_vision"}, {"score": 0.0043183186953102805, "phrase": "image_processing"}, {"score": 0.004212759658511567, "phrase": "conventional_low-rank_matrix_approximation_methods"}, {"score": 0.003989487818395848, "phrase": "principal_component_analysis"}, {"score": 0.003950611229920228, "phrase": "pca"}, {"score": 0.0036673959568781734, "phrase": "poor_approximation"}, {"score": 0.003371220210416809, "phrase": "negative_effect"}, {"score": 0.003068327486877065, "phrase": "robust_pca_methods"}, {"score": 0.0028203950416405563, "phrase": "heavy_computational_effort"}, {"score": 0.0027925723953701083, "phrase": "substantial_memory"}, {"score": 0.0027650234530228897, "phrase": "high-dimensional_data"}, {"score": 0.0026839922027139967, "phrase": "real-world_problems"}, {"score": 0.0024916260764216752, "phrase": "proper_projection"}, {"score": 0.002467038647952607, "phrase": "coefficient_matrices"}, {"score": 0.0024306105794630246, "phrase": "alternating_rectified_gradient_method"}, {"score": 0.002394719113232821, "phrase": "proposed_methods"}, {"score": 0.0023130151703068444, "phrase": "low-rank_matrix_approximation_problems"}, {"score": 0.0022120408738490437, "phrase": "experimental_results"}, {"score": 0.0021259846592569386, "phrase": "execution_time"}, {"score": 0.0021049977753042253, "phrase": "reconstruction_performance"}], "paper_keywords": ["Alternating rectified gradient method", " l(1)-norm", " low-rank matrix approximation", " matrix completion (MC)", " principal component analysis (PCA)", " proximal gradient method"], "paper_abstract": "Low-rank matrix approximation plays an important role in the area of computer vision and image processing. Most of the conventional low-rank matrix approximation methods are based on the l(2)-norm (Frobenius norm) with principal component analysis (PCA) being the most popular among them. However, this can give a poor approximation for data contaminated by outliers (including missing data), because the l(2)-norm exaggerates the negative effect of outliers. Recently, to overcome this problem, various methods based on the l(1)-norm, such as robust PCA methods, have been proposed for low-rank matrix approximation. Despite the robustness of the methods, they require heavy computational effort and substantial memory for high-dimensional data, which is impractical for real-world problems. In this paper, we propose two efficient low-rank factorization methods based on the l(1)-norm that find proper projection and coefficient matrices using the alternating rectified gradient method. The proposed methods are applied to a number of low-rank matrix approximation problems to demonstrate their efficiency and robustness. The experimental results show that our proposals are efficient in both execution time and reconstruction performance unlike other state-of-the-art methods.", "paper_title": "Efficient l(1)-Norm-Based Low-Rank Matrix Approximations for Large-Scale Problems Using Alternating Rectified Gradient Method", "paper_id": "WOS:000348856200004"}