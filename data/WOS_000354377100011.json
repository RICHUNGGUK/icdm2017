{"auto_keywords": [{"score": 0.04376009469235903, "phrase": "energy_landscape"}, {"score": 0.03318201413262534, "phrase": "energy_function"}, {"score": 0.01323573162165727, "phrase": "probabilistic_model"}, {"score": 0.00470325458481566, "phrase": "popular_feature_learning_models"}, {"score": 0.004424768776701965, "phrase": "efficient_inference"}, {"score": 0.004383406949799696, "phrase": "recent_work"}, {"score": 0.004104456271230213, "phrase": "negative_log-probability"}, {"score": 0.003807242767474929, "phrase": "input_space"}, {"score": 0.003581620064675766, "phrase": "training_criterion"}, {"score": 0.003401148484767927, "phrase": "restricted_boltzmann_machine"}, {"score": 0.003066945490483524, "phrase": "training_procedure"}, {"score": 0.002885066818322192, "phrase": "reconstruction_function"}, {"score": 0.002778529097886437, "phrase": "sigmoid_hidden_units"}, {"score": 0.002675914969626753, "phrase": "free_energy"}, {"score": 0.002638433015976975, "phrase": "rbm"}, {"score": 0.0024014759148123736, "phrase": "autoencoder_energy_function"}, {"score": 0.0023456351719724957, "phrase": "common_regularization_procedures"}, {"score": 0.002301896758256575, "phrase": "contractive_training"}, {"score": 0.0022378101636842296, "phrase": "dynamical_systems"}, {"score": 0.002196077907454079, "phrase": "practical_application"}, {"score": 0.0021349310877334378, "phrase": "generative_classifier"}, {"score": 0.0021049977753042253, "phrase": "class-specific_autoencoders"}], "paper_keywords": ["Autoencoders", " representation learning", " unsupervised learning", " generative classification"], "paper_abstract": "Autoencoders are popular feature learning models, that are conceptually simple, easy to train and allow for efficient inference. Recent work has shown how certain autoencoders can be associated with an energy landscape, akin to negative log-probability in a probabilistic model, which measures how well the autoencoder can represent regions in the input space. The energy landscape has been commonly inferred heuristically, by using a training criterion that relates the autoencoder to a probabilistic model such as a Restricted Boltzmann Machine (RBM). In this paper we show how most common autoencoders are naturally associated with an energy function, independent of the training procedure, and that the energy landscape can be inferred analytically by integrating the reconstruction function of the autoencoder. For autoencoders with sigmoid hidden units, the energy function is identical to the free energy of an RBM, which helps shed light onto the relationship between these two types of model. We also show that the autoencoder energy function allows us to explain common regularization procedures, such as contractive training, from the perspective of dynamical systems. As a practical application of the energy function, a generative classifier based on class-specific autoencoders is presented.", "paper_title": "The Potential Energy of an Autoencoder", "paper_id": "WOS:000354377100011"}