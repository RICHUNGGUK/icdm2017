{"auto_keywords": [{"score": 0.036624300746232616, "phrase": "local_cores"}, {"score": 0.033782804396948224, "phrase": "mc"}, {"score": 0.00481495049065317, "phrase": "distributed_memory"}, {"score": 0.004710306419574744, "phrase": "promising_memory_organization"}, {"score": 0.004669085371587273, "phrase": "future_many-core_systems"}, {"score": 0.004547564208690591, "phrase": "memory_level_parallelism"}, {"score": 0.004371160978838441, "phrase": "memory_module"}, {"score": 0.004294962474996283, "phrase": "comparable_number"}, {"score": 0.004257360656055417, "phrase": "memory_interfaces"}, {"score": 0.004220086640742953, "phrase": "on-chip_cores"}, {"score": 0.00416478405812793, "phrase": "packet-based_memory_access_model"}, {"score": 0.004128317208406523, "phrase": "pdma"}, {"score": 0.0040385352882334235, "phrase": "scalable_and_flexible_solution"}, {"score": 0.004003169429008801, "phrase": "distributed_memory_management"}, {"score": 0.003916098498242888, "phrase": "complicated_and_costly_on-chip_network_protocol_translation_and_massive_interferences"}, {"score": 0.0033868916699805224, "phrase": "local_virtualization"}, {"score": 0.003342471131048328, "phrase": "network_protocol_translation"}, {"score": 0.003226834737003501, "phrase": "remote_memory_controllers"}, {"score": 0.00307431840409833, "phrase": "local_agent"}, {"score": 0.002967931416941965, "phrase": "remote_memory"}, {"score": 0.002941913249957503, "phrase": "high-performance_intertile_communication"}, {"score": 0.0028652153774536967, "phrase": "detailed_architecture_supports"}, {"score": 0.0028276174348291923, "phrase": "ddma_model"}, {"score": 0.0027782510491938315, "phrase": "memory_interface_design"}, {"score": 0.002753891109273246, "phrase": "work_flow"}, {"score": 0.002682081928174866, "phrase": "simulation_results"}, {"score": 0.002646880721119838, "phrase": "parsec_benchmarks"}, {"score": 0.0025328377393370642, "phrase": "average_memory"}, {"score": 0.0023708980662168547, "phrase": "ddma"}, {"score": 0.0023294867572824147, "phrase": "congested_memory_traffic"}, {"score": 0.0021518875024104244, "phrase": "pdma."}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["On-chip distributed memory", " Memory interface", " Architecture", " Performance"], "paper_abstract": "On-chip distributed memory has emerged as a promising memory organization for future many-core systems, since it efficiently exploits memory level parallelism and can lighten off the load on each memory module by providing a comparable number of memory interfaces with on-chip cores. The packet-based memory access model (PDMA) has provided a scalable and flexible solution for distributed memory management, but suffers from complicated and costly on-chip network protocol translation and massive interferences among packets, which leads to unpredictable performance. In this paper we propose a direct distributed memory access (DDMA) model, in which remote memory can be directly accessed by local cores via remote-to-local virtualization, without network protocol translation. From the perspective of local cores, remote memory controllers (MC) can be directly manipulated through accessing the local agent MC, which is responsible for accessing remote memory through high-performance intertile communication. We further discuss some detailed architecture supports for the DDMA model, including the memory interface design, work flow and the protocols involved. Simulation results of executing PARSEC benchmarks show that our DDMA architecture outperforms PDMA in terms of both average memory access latency and lPC by 17.8% and 16.6% respectively on average. Besides, DDMA can better manage congested memory traffic, since a reduction of bandwidth in running memory-intensive SPEC2006 workloads only incurs 18.9% performance penalty, compared with 38.3% for PDMA. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Direct distributed memory access for CMPs", "paper_id": "WOS:000329952000008"}