{"auto_keywords": [{"score": 0.04962913836247387, "phrase": "feature_selection"}, {"score": 0.015719716506582538, "phrase": "dependency_margin"}, {"score": 0.004549734943142138, "phrase": "larger_feature_pool"}, {"score": 0.004485735417293941, "phrase": "selected_subset"}, {"score": 0.004218602458288531, "phrase": "whole_set"}, {"score": 0.004100712177044264, "phrase": "critical_preprocessing_step"}, {"score": 0.003766369160795001, "phrase": "supervised_classification"}, {"score": 0.0035925056609681194, "phrase": "class_labels"}, {"score": 0.0035587046945919788, "phrase": "traditional_greedy_search_algorithms"}, {"score": 0.003410479630045025, "phrase": "candidate_features"}, {"score": 0.0033624514778823763, "phrase": "class_label"}, {"score": 0.0032376465741070274, "phrase": "suboptimal_results"}, {"score": 0.003176986603753607, "phrase": "redundant_features"}, {"score": 0.0029454679553901613, "phrase": "subset_selection_algorithm"}, {"score": 0.0026169435194789772, "phrase": "better_alternatives"}, {"score": 0.002449208093757498, "phrase": "selection_problem"}, {"score": 0.0023030731312573246, "phrase": "selected_feature"}, {"score": 0.0022492129111567824, "phrase": "remaining_feature"}, {"score": 0.0022070310530794097, "phrase": "extensive_experiments"}, {"score": 0.002125020720324262, "phrase": "proposed_approach"}, {"score": 0.0021049977753042253, "phrase": "traditional_algorithms"}], "paper_keywords": ["Conditionally independent", " dependency margin feature selection", " forward greedy search", " redundant feature"], "paper_abstract": "Feature selection tries to find a subset of feature from a larger feature pool and the selected subset can provide the same or even better performance compared with using the whole set. Feature selection is usually a critical preprocessing step for many machine-learning applications such as clustering and classification. In this paper, we focus on feature selection for supervised classification which targets at finding features that can best predict class labels. Traditional greedy search algorithms incrementally find features based on the relevance of candidate features and the class label. However, this may lead to suboptimal results when there are redundant features that may interfere with the selection. To solve this problem, we propose a subset selection algorithm that considers both the selected and remaining features' relevances with the label. The intuition is that features, which do not have better alternatives from the feature set, should be selected first. We formulate the selection problem as maximizing the dependency margin which is measured by the difference between the selected feature set performance and the remaining feature set performance. Extensive experiments on various data sets show the superiority of the proposed approach against traditional algorithms.", "paper_title": "Feature Selection Based on Dependency Margin", "paper_id": "WOS:000354532000009"}