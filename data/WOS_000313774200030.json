{"auto_keywords": [{"score": 0.0337119789969786, "phrase": "online_spua"}, {"score": 0.00481495049065317, "phrase": "policy_update_algorithms"}, {"score": 0.004687645903373103, "phrase": "linear_continuous-time_h-infinity_state_feedback_control"}, {"score": 0.004523103062368843, "phrase": "h-infinity_state_feedback_control_problem"}, {"score": 0.004403480614319231, "phrase": "two-player_zero-sum_game"}, {"score": 0.004229927464182562, "phrase": "algebra_riccati_equation"}, {"score": 0.004009074013302036, "phrase": "simultaneous_policy_update_algorithm"}, {"score": 0.0037827603118271757, "phrase": "online_versions"}, {"score": 0.003732368382223596, "phrase": "offline_spua"}, {"score": 0.0036826452656927877, "phrase": "model-based_approach"}, {"score": 0.003459191419478298, "phrase": "lyapunov_equations"}, {"score": 0.003278447235176858, "phrase": "newton's_sequence"}, {"score": 0.0032347516831068715, "phrase": "fixed_point_equation"}, {"score": 0.003149094460617212, "phrase": "partially_model-free_approach"}, {"score": 0.0030248298759040695, "phrase": "reinforcement_learning"}, {"score": 0.00299792269946528, "phrase": "rl"}, {"score": 0.0028284924551705516, "phrase": "internal_system_dynamics"}, {"score": 0.0024620842899061614, "phrase": "comparative_simulation_studies"}, {"score": 0.0023968383387667404, "phrase": "power_system"}, {"score": 0.002162316557411786, "phrase": "existing_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Simultaneous policy update algorithm", " H-infinity state feedback control", " Algebra Riccati equation", " Lyapunov equation", " Offline", " Online"], "paper_abstract": "It is well known that the H-infinity state feedback control problem can be viewed as a two-player zero-sum game and reduced to find a solution of the algebra Riccati equation (ARE). In this paper, we propose a simultaneous policy update algorithm (SPUA) for solving the ARE, and develop offline and online versions. The offline SPUA is a model-based approach, which obtains the solution of the ARE by solving a sequence of Lyapunov equations (LEs). Its convergence is established rigorously by constructing a Newton's sequence for the fixed point equation. The online SPUA is a partially model-free approach, which takes advantage of the thought of reinforcement learning (RL) to learn the solution of the ARE online without requiring the internal system dynamics, wherein both players update their action policies simultaneously. The convergence of the online SPUA is proved by demonstrating that it is mathematically equivalent to the offline SPUA. Finally, by conducting comparative simulation studies on an F-16 aircraft plant and a power system, the results show that both the offline SPUA and the online SPUA can find the solution of the ARE, and achieve much better convergence than the existing methods. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Simultaneous policy update algorithms for learning the solution of linear continuous-time H-infinity state feedback control", "paper_id": "WOS:000313774200030"}