{"auto_keywords": [{"score": 0.036962449923098536, "phrase": "reference_speaker"}, {"score": 0.014141158058762995, "phrase": "input_representation"}, {"score": 0.008464299689703445, "phrase": "speaker_changes"}, {"score": 0.006972929093494374, "phrase": "feed-forward_neural_network"}, {"score": 0.006772958566344203, "phrase": "phonetic_classes"}, {"score": 0.0064366215373693894, "phrase": "phone_recognition_accuracy"}, {"score": 0.006176544595865164, "phrase": "corresponding_speech"}, {"score": 0.005135858235428003, "phrase": "reference_speaker_data"}, {"score": 0.00481495049065317, "phrase": "input_patterns"}, {"score": 0.004791608198919605, "phrase": "speaker_variability"}, {"score": 0.004768378525561471, "phrase": "speech_recognition_neural_networks"}, {"score": 0.004710793522479953, "phrase": "input_variability"}, {"score": 0.004564291332315995, "phrase": "speech_recognition_systems"}, {"score": 0.004460740584972504, "phrase": "inverse_network"}, {"score": 0.004253618550301311, "phrase": "single_speaker"}, {"score": 0.004141365648080381, "phrase": "input_pattern"}, {"score": 0.003935192614078078, "phrase": "speech_recognition_process"}, {"score": 0.003570349925961584, "phrase": "highest_percentage"}, {"score": 0.0034760655218366, "phrase": "representation_parameters"}, {"score": 0.0033514820577027014, "phrase": "first_method"}, {"score": 0.0033271045779740683, "phrase": "error_back-propagation_algorithm"}, {"score": 0.003278878522165818, "phrase": "optimal_point"}, {"score": 0.0032550274098078033, "phrase": "decision_region"}, {"score": 0.0031767656924644463, "phrase": "input_space"}, {"score": 0.0030778231027679434, "phrase": "corresponding_points"}, {"score": 0.0029458777184930896, "phrase": "input_signal"}, {"score": 0.002896102316712905, "phrase": "second_method"}, {"score": 0.002861062731782219, "phrase": "back-propagation_algorithm"}, {"score": 0.0028127163745564777, "phrase": "desirable_speaker_output"}, {"score": 0.002751751956416832, "phrase": "test_datasets"}, {"score": 0.0026530590435527527, "phrase": "third_method"}, {"score": 0.002576652834491814, "phrase": "speaker_information"}, {"score": 0.002539276789627705, "phrase": "phonetic_output"}, {"score": 0.002514660355894905, "phrase": "direct_network"}, {"score": 0.002320449502337321, "phrase": "final_speech_recognition_model"}, {"score": 0.0022923586577645143, "phrase": "adapted_training_data"}, {"score": 0.0022536005664642294, "phrase": "adapted_testing_data"}, {"score": 0.0022101055924375725, "phrase": "final_network_results"}, {"score": 0.002199363362666037, "phrase": "un-adapted_network"}, {"score": 0.0021780349459687622, "phrase": "highest_confidence_level"}, {"score": 0.0021049977753042253, "phrase": "clean_speech"}], "paper_keywords": ["Speaker variability", " Input adaptation", " Nonlinear normalization", " Inverse neural networks", " Speech recognition"], "paper_abstract": "The issue of input variability resulting from speaker changes is one of the most crucial factors influencing the effectiveness of speech recognition systems. A solution to this problem is adaptation or normalization of the input, in a way that all the parameters of the input representation are adapted to that of a single speaker, and a kind of normalization is applied to the input pattern against the speaker changes, before recognition. This paper proposes three such methods in which some effects of the speaker changes influencing speech recognition process is compensated. In all three methods, a feed-forward neural network is first trained for mapping the input into codes representing the phonetic classes and speakers. Then, among the 71 speakers used in training, the one who is showing the highest percentage of phone recognition accuracy is selected as the reference speaker so that the representation parameters of the other speakers are converted to the corresponding speech uttered by him. In the first method, the error back-propagation algorithm is used for finding the optimal point of every decision region relating to each phone of each speaker in the input space for all the phones and all the speakers. The distances between these points and the corresponding points related to the reference speaker are employed for offsetting the speaker change effects and the adaptation of the input signal to the reference speaker. In the second method, using the error back-propagation algorithm and maintaining the reference speaker data as the desirable speaker output, we correct all the speech signal frames, i.e., the train and the test datasets, so that they coincide with the corresponding speech of the reference speaker. In the third method, another feed-forward neural network is applied inversely for mapping the phonetic classes and speaker information to the input representation. The phonetic output retrieved from the direct network along with the reference speaker data are given to the inverse network. Using this information, the inverse network yields an estimation of the input representation adapted to the reference speaker. In all three methods, the final speech recognition model is trained using the adapted training data, and is tested by the adapted testing data. Implementing these methods and combining the final network results with un-adapted network based on the highest confidence level, an increase of 2.1, 2.6 and 3% in phone recognition accuracy on the clean speech is obtained from the three methods, respectively.", "paper_title": "Nonlinear normalization of input patterns to speaker variability in speech recognition neural networks", "paper_id": "WOS:000262506600006"}