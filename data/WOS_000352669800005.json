{"auto_keywords": [{"score": 0.04407953898448545, "phrase": "kriging"}, {"score": 0.02768727701398237, "phrase": "improved_algorithm"}, {"score": 0.008234448882736403, "phrase": "traditional_algorithm"}, {"score": 0.00481495049065317, "phrase": "computational_acceleration"}, {"score": 0.0047784273577245505, "phrase": "ordinary_kriging_interpolation"}, {"score": 0.004742179945772899, "phrase": "heavy_computation"}, {"score": 0.004652754203582873, "phrase": "kriging_interpolation_methods"}, {"score": 0.004530371221172796, "phrase": "ever-increasing_problem_size"}, {"score": 0.0044111930537670705, "phrase": "parallel_processing_techniques"}, {"score": 0.004311527998148399, "phrase": "computational_resources"}, {"score": 0.004246333442872459, "phrase": "computation-intensive_problems"}, {"score": 0.004087609837102023, "phrase": "traditional_approach"}, {"score": 0.003995226376780718, "phrase": "computation-intensive_procedure"}, {"score": 0.0039049226745413224, "phrase": "high-resolution_interpolation"}, {"score": 0.003687960013746783, "phrase": "improved_coarse-grained_parallel_algorithm"}, {"score": 0.003536526698282596, "phrase": "interpolation_task"}, {"score": 0.0034963129868796033, "phrase": "unobserved_point"}, {"score": 0.0034303002275695446, "phrase": "basic_parallel_unit"}, {"score": 0.003378385493203297, "phrase": "time_complexity"}, {"score": 0.003352722512047951, "phrase": "memory_consumption"}, {"score": 0.003314591878914152, "phrase": "large_right_hand_side_matrix"}, {"score": 0.003276893480994257, "phrase": "kriging_linear_system"}, {"score": 0.003071201732948199, "phrase": "unobserved_points"}, {"score": 0.0029450186031370245, "phrase": "homogeneous_distributed_memory_system"}, {"score": 0.0028893844749296863, "phrase": "improved_parallel_algorithm"}, {"score": 0.0028347983387988847, "phrase": "traditional_one"}, {"score": 0.0028132529128565282, "phrase": "spatial_interpolation"}, {"score": 0.0027918707809612, "phrase": "annual_average_precipitation"}, {"score": 0.002770654423586894, "phrase": "victoria"}, {"score": 0.002749644066067261, "phrase": "australia"}, {"score": 0.0024335561337016174, "phrase": "weak_scaling_efficiency"}, {"score": 0.0022719915771356354, "phrase": "experimental_results"}, {"score": 0.0021537671245615286, "phrase": "problem_size"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Interpolation", " Ordinary Kriging", " Geostatistics", " Parallel computing", " MPI"], "paper_abstract": "Heavy computation limits the use of Kriging interpolation methods in many real-time applications, especially with the ever-increasing problem size. Many researchers have realized that parallel processing techniques are critical to fully exploit computational resources and feasibly solve computation-intensive problems like Kriging. Much research has addressed the parallelization of traditional approach to Kriging, but this computation-intensive procedure may not be suitable for high-resolution interpolation of spatial data. On the basis of a more effective serial approach, we propose an improved coarse-grained parallel algorithm to accelerate ordinary Kriging interpolation. In particular, the interpolation task of each unobserved point is considered as a basic parallel unit. To reduce time complexity and memory consumption, the large right hand side matrix in the Kriging linear system is transformed and fixed at only two columns and therefore no longer directly relevant to the number of unobserved points. The MPI (Message Passing Interface) model is employed to implement our parallel programs in a homogeneous distributed memory system. Experimentally, the improved parallel algorithm performs better than the traditional one in spatial interpolation of annual average precipitation in Victoria, Australia. For example, when the number of processors is 24, the improved algorithm keeps speed-up at 20.8 while the speed-up of the traditional algorithm only reaches 93. Likewise, the weak scaling efficiency of the improved algorithm is nearly 90% while that of the traditional algorithm almost drops to 40% with 16 processors. Experimental results also demonstrate that the performance of the improved algorithm is enhanced by increasing the problem size. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "An improved coarse-grained parallel algorithm for computational acceleration of ordinary Kriging interpolation", "paper_id": "WOS:000352669800005"}