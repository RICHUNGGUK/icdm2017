{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bregman_divergences"}, {"score": 0.004262273304740847, "phrase": "particular_class"}, {"score": 0.003983016985197904, "phrase": "cost-sensitive_classifiers"}, {"score": 0.003525493807130663, "phrase": "divergence_measures"}, {"score": 0.003249966782844541, "phrase": "posterior_probabilities"}, {"score": 0.0031629800646754505, "phrase": "maximal_accuracy"}, {"score": 0.003036832649117453, "phrase": "probability_values"}, {"score": 0.002799388549023265, "phrase": "decision_boundaries"}, {"score": 0.0026157259734105, "phrase": "proposed_divergence_measures"}, {"score": 0.00237861522566195, "phrase": "decision_costs"}, {"score": 0.0023148964315569866, "phrase": "non-separable_problems"}, {"score": 0.0021049977753042253, "phrase": "separable_map_problems"}], "paper_keywords": ["Cost sensitive learning", " Bregman divergence", " Posterior class probabilities", " Maximum margin"], "paper_abstract": "This paper analyzes the application of a particular class of Bregman divergences to design cost-sensitive classifiers for multiclass problems. We show that these divergence measures can be used to estimate posterior probabilities with maximal accuracy for the probability values that are close to the decision boundaries. Asymptotically, the proposed divergence measures provide classifiers minimizing the sum of decision costs in non-separable problems, and maximizing a margin in separable MAP problems.", "paper_title": "Cost-sensitive learning based on Bregman divergences", "paper_id": "WOS:000269013000008"}