{"auto_keywords": [{"score": 0.03172845301281709, "phrase": "algorithm"}, {"score": 0.00481495049065317, "phrase": "semisupervised_least_squares_support_vector_machine"}, {"score": 0.0046256566852302256, "phrase": "support_vector_machine"}, {"score": 0.004303672490232395, "phrase": "svm"}, {"score": 0.004101088357088207, "phrase": "margin-maximization_performing_structural_risk"}, {"score": 0.003606711217432998, "phrase": "semisupervised_learning"}, {"score": 0.0032753168662834516, "phrase": "transductive_svm_idea"}, {"score": 0.003121183062764093, "phrase": "combinatorial_search"}, {"score": 0.002880196765572443, "phrase": "decision_function"}, {"score": 0.002374855916752648, "phrase": "better_generalization_capacity"}, {"score": 0.002156402793481102, "phrase": "encouraging_results"}], "paper_keywords": ["Least squares support vector machine (LS-SVM)", " semisupervised learning", " transductive learning"], "paper_abstract": "The least squares support vector machine (LS-SVM), like the SVM, is based on the margin-maximization performing structural risk and has excellent power of generalization. In this paper, we consider its use in semisupervised learning. We propose two algorithms to perform this task deduced from the transductive SVM idea. Algorithm 1 is based on combinatorial search guided by certain heuristics while Algorithm 2 iteratively builds the decision function by adding one unlabeled sample at the time. In term of complexity, Algorithm 1 is faster but Algorithm 2 yields a classifier with a better generalization capacity with only a few labeled data available. Our proposed algorithms are tested in several benchmarks and give encouraging results, confirming our approach.", "paper_title": "Semisupervised Least Squares Support Vector Machine", "paper_id": "WOS:000272484200001"}