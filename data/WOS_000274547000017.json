{"auto_keywords": [{"score": 0.012039143854965881, "phrase": "gaussians"}, {"score": 0.0047825434929393, "phrase": "gaussian_radial_basis_function_series"}, {"score": 0.004592609984331215, "phrase": "direct_summation"}, {"score": 0.004571975298172992, "phrase": "gaussian_series"}, {"score": 0.004282939206677511, "phrase": "average_separation"}, {"score": 0.004215942745756017, "phrase": "relative_inverse_width_parameter"}, {"score": 0.004031884577139605, "phrase": "long-range_interactions"}, {"score": 0.003913893297501276, "phrase": "gaussian_radial_basis_function_interpolation"}, {"score": 0.003843976215854958, "phrase": "gaussian_basis"}, {"score": 0.0038180792827473566, "phrase": "parameter_range"}, {"score": 0.0037923561535200536, "phrase": "interpolation_matrix"}, {"score": 0.0036431075260469053, "phrase": "short-range_interactions"}, {"score": 0.0034654349518263134, "phrase": "gaussian_rbf_interpolant"}, {"score": 0.003350215650147811, "phrase": "small_difference"}, {"score": 0.003320140649580988, "phrase": "exponentially_large_coefficients"}, {"score": 0.003305204504133684, "phrase": "fornberg"}, {"score": 0.0032903349377763134, "phrase": "piret"}, {"score": 0.0031381745834945024, "phrase": "recombined_basis_functions"}, {"score": 0.0031240541946545002, "phrase": "perturbed_spherical_harmonics"}, {"score": 0.0030201507728955914, "phrase": "interpolation_matrix_system"}, {"score": 0.0029997872523575683, "phrase": "preconditioned_iteration"}, {"score": 0.0029661518033534556, "phrase": "condition_numbers"}, {"score": 0.002952803147716346, "phrase": "direct_methods"}, {"score": 0.0027909132750281104, "phrase": "matrix-vector_multiply"}, {"score": 0.0026980598526314234, "phrase": "fast_summation"}, {"score": 0.002614286940320704, "phrase": "omega"}, {"score": 0.0025558607054900055, "phrase": "gaussian_rbfs"}, {"score": 0.0025328985337599707, "phrase": "rbf_species"}, {"score": 0.0024819832217370078, "phrase": "exponential_decay"}, {"score": 0.0023405579554367567, "phrase": "fast_multipole"}, {"score": 0.0023300180978349473, "phrase": "treecode_algorithms"}, {"score": 0.002278024870371009, "phrase": "underlying_difficulty"}, {"score": 0.002232221461131238, "phrase": "gaussian_rbf_basis"}, {"score": 0.0021725765174586496, "phrase": "generic_difficulty"}, {"score": 0.002143353086762981, "phrase": "almost_any_rbf_basis"}, {"score": 0.0021336993708209974, "phrase": "almost_any_fast_summation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Fast Gauss Transform", " Radial basis functions", " Fast multipole", " Treecode"], "paper_abstract": "The Fast Gauss Transform is an algorithm for summing a series of Gaussians which is sometimes much faster than direct summation. Gaussian series in d dimensions are of the form Sigma(N)(j=1) lambda(j) exp(-[alpha/h](2) parallel to x - x(j)parallel to(2)) where the x(j) are the centers, h is the average separation between centers and a is the relative inverse width parameter. We show that the speed-up of the Fast Gauss Transform is bounded by a factor Omega(alpha). When alpha << 1, Omega can be large. However, when applied to Gaussian radial basis function interpolation, it is difficult to apply the Gaussian basis in this parameter range because the interpolation matrix is expo( nentially ill-conditioned: the condition number kappa similar to (1/2) exp (pi(2)/4 alpha(2)) for a uniform, one dimensional grid, and larger still in two dimensions or when the grid is irregular. Furthermore, the Gaussian RBF interpolant is ill-conditioned for most series in the sense that the interpolant is the small difference of terms with exponentially large coefficients. Fornberg and Piret developed a \"QR-basis\" that ameliorates this difficulty for approximations on the surface of a sphere, but because the recombined basis functions are perturbed spherical harmonics, not Gaussians, the Fast Gauss Transform is no longer applicable. The solution of the interpolation matrix system by a preconditioned iteration is less sensitive to condition numbers than direct methods because iterations are self-correcting and also because the preconditioning reduces the spread of the eigenvalues. However, each iteration requires a matrix-vector multiply which is fast only if this operation can be performed by some species of Fast Summation. When alpha similar to O(1), alas, we show that Omega is not large and the Fast Gauss Transform is not accelerative. Gaussian RBFs are unusual among RBF species through the absence of long-range interactions due to the exponential decay of the Gaussians with distance from their centers; many other RBF species do have long-range interactions, and it is well-established that these can be accelerated by fast multipole and treecode algorithms. We offer a less rigorous scale analysis argument to explain why the underlying difficulty in accelerating short-range interactions is not peculiar to the Gaussian RBF basis or to the Fast Gauss Transform, but rather is likely to be a generic difficulty in accelerating the short-range interactions of almost any RBF basis with almost any Fast Summation. (C) 2009 Elsevier Inc. All rights reserved.", "paper_title": "The uselessness of the Fast Gauss Transform for summing Gaussian radial basis function series", "paper_id": "WOS:000274547000017"}