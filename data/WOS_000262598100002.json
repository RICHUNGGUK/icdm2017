{"auto_keywords": [{"score": 0.04330659241862547, "phrase": "naive_bayes"}, {"score": 0.011591917886544078, "phrase": "significant_improvement"}, {"score": 0.00481495049065317, "phrase": "instance_cloning_to"}, {"score": 0.004776669953449352, "phrase": "naive_bayes_for_ranking."}, {"score": 0.004719882047822533, "phrase": "bayes"}, {"score": 0.004553265070135597, "phrase": "significant_attention"}, {"score": 0.004517055580214742, "phrase": "related_work"}, {"score": 0.004237512575106522, "phrase": "eager_learning"}, {"score": 0.004187049198138107, "phrase": "key_idea"}, {"score": 0.004104269996872886, "phrase": "lazy_learning"}, {"score": 0.004023120753104693, "phrase": "improved_naive_bayes"}, {"score": 0.003975200317233532, "phrase": "test_instance"}, {"score": 0.0036262124965265015, "phrase": "naive_bayes'_classification_performance"}, {"score": 0.003229374325577207, "phrase": "accurate_classification"}, {"score": 0.003178147973814818, "phrase": "natural_question"}, {"score": 0.0030051602138804743, "phrase": "auc"}, {"score": 0.0026022112631368223, "phrase": "ranking_performance"}, {"score": 0.0025202582748972122, "phrase": "naive_bayes'_ranking_performance"}, {"score": 0.0024703502756471514, "phrase": "novel_lazy_method"}, {"score": 0.002363995836370621, "phrase": "different_instance_cloning_strategies"}, {"score": 0.0022712795176156536, "phrase": "experimental_results"}, {"score": 0.0021909417530145744, "phrase": "auc."}, {"score": 0.0021561514733516676, "phrase": "simple_but_effective_method"}, {"score": 0.0021049977753042253, "phrase": "accurate_ranking"}], "paper_keywords": ["Naive Bayes", " instance cloning", " ranking", " classification", " lazy learning", " similarity", " data mining"], "paper_abstract": "Improving naive Bayes (simply NB)(15,28) for classification has received significant attention. Related work can be broadly divided into two approaches: eager learning and lazy learning.(1) Different from eager learning, the key idea for extending naive Bayes using lazy learning is to learn an improved naive Bayes for each test instance. In recent years, several lazy extensions of naive Bayes have been proposed. For example, LBR,(30) SNNB,(27) and LWNB.(8) All these algorithms aim to improve naive Bayes' classification performance. Indeed, they achieve significant improvement in terms of classification, measured by accuracy. In many real-world data mining applications, however, an accurate ranking is more desirable than an accurate classification. Thus a natural question is whether they also achieve significant improvement in terms of ranking, measured by AUC (the area under the ROC curve).(2,11,17) Responding to this question, we conduct experiments on the 36 UCI data sets(18) selected by Weka(12) to investigate their ranking performance and find that they do not significantly improve the ranking performance of naive Bayes. Aiming at scaling up naive Bayes' ranking performance, we present a novel lazy method ICNB (instance cloned naive Bayes) and develop three ICNB algorithms using different instance cloning strategies. We empirically compare them with naive Bayes. The experimental results show that our algorithms achieve significant improvement in terms of AUC. Our research provides a simple but effective method for the applications where an accurate ranking is desirable.", "paper_title": "USING INSTANCE CLONING TO IMPROVE NAIVE BAYES FOR RANKING", "paper_id": "WOS:000262598100002"}