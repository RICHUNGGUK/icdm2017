{"auto_keywords": [{"score": 0.025562567223749957, "phrase": "nic"}, {"score": 0.00481495049065317, "phrase": "programmable_nics_and_multicore_platforms"}, {"score": 0.004752022662995341, "phrase": "multicore_processors"}, {"score": 0.004544127927346964, "phrase": "new_opportunities"}, {"score": 0.004508391992630655, "phrase": "efficient_network_interfaces"}, {"score": 0.00441445805214264, "phrase": "improvement_rates"}, {"score": 0.00439128062277584, "phrase": "link_bandwidths"}, {"score": 0.0043682243489937935, "phrase": "microprocessor_performance"}, {"score": 0.004311109608629196, "phrase": "important_challenges"}, {"score": 0.004265953661191515, "phrase": "high_computational_requirements"}, {"score": 0.0042212686864468805, "phrase": "traffic_volumes"}, {"score": 0.004199101289890982, "phrase": "wider_functionality"}, {"score": 0.004166067358313907, "phrase": "network_interface"}, {"score": 0.004015298544261682, "phrase": "link_bandwidth_improvement"}, {"score": 0.003941964605885606, "phrase": "application_demands"}, {"score": 0.00392125815948488, "phrase": "efficient_network_interface_architectures"}, {"score": 0.0037102752163026866, "phrase": "communication_path"}, {"score": 0.0036713893960565206, "phrase": "protocol_processing_work"}, {"score": 0.00357594216066491, "phrase": "i.e._multicore_microprocessors"}, {"score": 0.003492155652400838, "phrase": "brief_review"}, {"score": 0.00346466384521149, "phrase": "different_solutions"}, {"score": 0.003383475802364289, "phrase": "network_interfaces"}, {"score": 0.0032437952431945724, "phrase": "host_cpu_cycles"}, {"score": 0.0031929098985796814, "phrase": "communication_workload_execution"}, {"score": 0.0030772505522663612, "phrase": "general-purpose_processor"}, {"score": 0.003028969851366049, "phrase": "chip_multiprocessor"}, {"score": 0.0029735960759537813, "phrase": "symmetric_multiprocessor"}, {"score": 0.0028886123801902517, "phrase": "programmable_network_interface_cards"}, {"score": 0.002798662730881037, "phrase": "full-system_simulator"}, {"score": 0.0027620131520749167, "phrase": "fair_and_more_complete_comparison"}, {"score": 0.0026689487424251907, "phrase": "relative_improvement"}, {"score": 0.002654911400878851, "phrase": "peak_throughput"}, {"score": 0.0025790119738184807, "phrase": "application_workload"}, {"score": 0.002565446438134849, "phrase": "communication_overhead"}, {"score": 0.0024855348966965566, "phrase": "system_architecture"}, {"score": 0.00237029943579966, "phrase": "system_processor"}, {"score": 0.0023146962503449186, "phrase": "lower_latencies"}, {"score": 0.0021899641786881337, "phrase": "hybrid_network_interface"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Full-system simulation", " HDL simulation", " LAWS model", " Protocol offloading", " Network interfaces", " Simics"], "paper_abstract": "The availability of multicore processors and programmable NICs, such as TOEs (TCP/IP Off-loading Engines), provides new opportunities for designing efficient network interfaces to cope with the gap between the improvement rates of link bandwidths and microprocessor performance. This gap poses important challenges related with the high computational requirements associated to the traffic volumes and wider functionality that the network interface has to support. This way, taking into account the rate of link bandwidth improvement and the ever changing and increasing application demands, efficient network interface architectures require scalability and flexibility. An opportunity to reach these goals comes from the exploitation of the parallelism in the communication path by distributing the protocol processing work across processors which are available in the computer, i.e. multicore microprocessors and programmable NICs. Thus, after a brief review of the different solutions that have been previously proposed for speeding up network interfaces, this paper analyzes the onloading and offloading alternatives. Both strategies try to release host CPU cycles by taking advantage of the communication workload execution in other processors present in the node. Nevertheless, whereas onloading uses another general-purpose processor, either included in a chip multiprocessor (CMP) or in a symmetric multiprocessor (SMP), offloading takes advantage of processors in programmable network interface cards (NICs). From our experiments, implemented by using a full-system simulator, we provide a fair and more complete comparison between onloading and offloading. Thus, it is shown that the relative improvement on peak throughput offered by offloading and onloading depends on the rate of application workload to communication overhead, the message sizes, and on the characteristics of the system architecture, more specifically the bandwidth of the buses and the way the NIC is connected to the system processor and memory. In our implementations, offloading provides lower latencies than onloading, although the CPU utilization and interrupts are lower for onloading. Taking into account the conclusions of our experimental results, we propose a hybrid network interface that can take advantage of both, programmable NICs and multicore processors. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Network interfaces for programmable NICs and multicore platforms", "paper_id": "WOS:000275585600002"}