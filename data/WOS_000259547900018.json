{"auto_keywords": [{"score": 0.03789439700787632, "phrase": "generalization_error"}, {"score": 0.03753573727652448, "phrase": "unseen_samples"}, {"score": 0.00481495049065317, "phrase": "feature_selection"}, {"score": 0.004767826806287385, "phrase": "localized_generalization_error"}, {"score": 0.004721162131188487, "phrase": "supervised_classification_problems"}, {"score": 0.004674952036988355, "phrase": "rbfnn."}, {"score": 0.00462919213721188, "phrase": "pattern_classification_problem"}, {"score": 0.004539005582217309, "phrase": "high-dimensional_features"}, {"score": 0.004236908273106904, "phrase": "feature_reduction"}, {"score": 0.004174824849669107, "phrase": "training_accuracy"}, {"score": 0.004133940019630129, "phrase": "generalization_capability"}, {"score": 0.003974349782102708, "phrase": "novel_hybrid_filter-wrapper-type_feature_subset_selection_methodology"}, {"score": 0.0036914757355166966, "phrase": "radial_basis_function"}, {"score": 0.0036733470451885465, "phrase": "neural_network_bounds"}, {"score": 0.003445591238183763, "phrase": "training_samples"}, {"score": 0.003312486229143855, "phrase": "smallest_contribution"}, {"score": 0.0031377955308198634, "phrase": "novel_feature_selection_method"}, {"score": 0.0030614565460256897, "phrase": "sample_size"}, {"score": 0.0029576827451501956, "phrase": "experimental_results"}, {"score": 0.002899965889623787, "phrase": "proposed_method"}, {"score": 0.0028574164693918433, "phrase": "large_percentages"}, {"score": 0.0028016507334576216, "phrase": "statistically_insignificant_loss"}, {"score": 0.002538687900036229, "phrase": "feature_subsets"}, {"score": 0.0023346234464901978, "phrase": "full_set"}, {"score": 0.0021363670305843403, "phrase": "corporate_bankruptcies"}], "paper_keywords": ["feature selection", " neural network", " generalization error", " RBFNN"], "paper_abstract": "A pattern classification problem usually involves using high-dimensional features that make the classifier very complex and difficult to train. With no feature reduction, both training accuracy and generalization capability will suffer. This paper proposes a novel hybrid filter-wrapper-type feature subset selection methodology using a localized generalization error model. The localized generalization error model for a radial basis function neural network bounds from above the generalization error for unseen samples located within a neighborhood of the training samples. iteratively, the feature making the smallest contribution to the generalization error bound is removed. Moreover, the novel feature selection method is independent of the sample size and is computationally fast. The experimental results show that the proposed method consistently removes large percentages of features with statistically insignificant loss of testing accuracy for unseen samples. In the experiments for two of the datasets, the classifiers built using feature Subsets with 90% of features removed by our proposed approach yield average testing accuracies higher than those trained using the full set of features. Finally, we corroborate the efficacy of the model by using it to predict corporate bankruptcies in the US. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Feature selection using localized generalization error for supervised classification problems using RBFNN", "paper_id": "WOS:000259547900018"}