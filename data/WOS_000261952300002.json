{"auto_keywords": [{"score": 0.0461065037015248, "phrase": "pattern_vectors"}, {"score": 0.00481495049065317, "phrase": "generalized_support_vector_classification_problem"}, {"score": 0.004464326432401002, "phrase": "mutually_exclusive_sets"}, {"score": 0.004116823963855969, "phrase": "vertical_bar"}, {"score": 0.0035005841368145525, "phrase": "selected_positive_and_negative_pattern_vectors"}, {"score": 0.0031759067385562553, "phrase": "standard_support_vector_classifiers"}, {"score": 0.0030426541322472423, "phrase": "np"}, {"score": 0.0029602552900111407, "phrase": "alternative_approach"}, {"score": 0.002881255933884771, "phrase": "free_slack_concept"}, {"score": 0.0028502479839354637, "phrase": "primal_and_dual_formulations"}, {"score": 0.0027592065435732955, "phrase": "nonlinear_classification"}, {"score": 0.0026423134393535243, "phrase": "separating_hyperplane"}, {"score": 0.0025578967205807843, "phrase": "large_margin"}, {"score": 0.002530359917257626, "phrase": "iterative_elimination"}, {"score": 0.0025031188149336257, "phrase": "direct_selection_methods"}, {"score": 0.0023970487263732737, "phrase": "alternative_formulations"}, {"score": 0.002295463032589125, "phrase": "naive_method"}, {"score": 0.002270745003370529, "phrase": "simulated_data"}, {"score": 0.002234165013865669, "phrase": "iterative_elimination_method"}, {"score": 0.0021745003291048356, "phrase": "neural_data"}, {"score": 0.0021394674440767124, "phrase": "visuomotor_categorical_discrimination_task"}, {"score": 0.0021049977753042253, "phrase": "highly_cognitive_brain_activities"}], "paper_keywords": ["Classification", " Support vector machines", " Quadratic mixed 0-1programming"], "paper_abstract": "In this study we introduce a generalized support vector classification problem: Let X(i), i = 1,..., n be mutually exclusive sets of pattern vectors such that all pattern vectors x(i,k,) k = 1,..., vertical bar X(i vertical bar) have the same class label y(i). Select only one pattern vector x(i,k*) from each set X(i) such that the margin between the set of selected positive and negative pattern vectors are maximized. This problem is formulated as a quadratic mixed 0-1 programming problem, which is a generalization of the standard support vector classifiers. The quadratic mixed 0-1 formulation is shown to be NP-hard. An alternative approach is proposed with the free slack concept. Primal and dual formulations are introduced for linear and nonlinear classification. These formulations provide flexibility to the separating hyperplane to identify the pattern vectors with large margin. Iterative elimination and direct selection methods are developed to select such pattern vectors using the alternative formulations. These methods are compared with a naive method on simulated data. The iterative elimination method is also applied to neural data from a visuomotor categorical discrimination task to classify highly cognitive brain activities.", "paper_title": "Selective support vector machines", "paper_id": "WOS:000261952300002"}