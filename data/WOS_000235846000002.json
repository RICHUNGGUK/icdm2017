{"auto_keywords": [{"score": 0.037857114798916396, "phrase": "crh"}, {"score": 0.00481495049065317, "phrase": "multiprocessor_systems"}, {"score": 0.004728122501822142, "phrase": "fine-grained_data_sharing"}, {"score": 0.004625983640628956, "phrase": "specialized_architectures"}, {"score": 0.004476878957065829, "phrase": "low_latency_user-level_contention-free_access"}, {"score": 0.00430112267051817, "phrase": "local_access_latency"}, {"score": 0.003912533878506028, "phrase": "data_reduction_operations"}, {"score": 0.0037588493723080757, "phrase": "circulating_register_hardware"}, {"score": 0.0036775717816768133, "phrase": "circulating_copy"}, {"score": 0.00350742771604554, "phrase": "atomic_operations"}, {"score": 0.003381905288924267, "phrase": "two-dimensional_hierarchy"}, {"score": 0.003357343256340741, "phrase": "n_processing_elements"}, {"score": 0.003213660268958121, "phrase": "circulating_register"}, {"score": 0.00315561677959372, "phrase": "new_value"}, {"score": 0.0030426463790590445, "phrase": "crh_modules"}, {"score": 0.0029985954225814895, "phrase": "updated_value"}, {"score": 0.0029551803384781604, "phrase": "optimum_cluster_size"}, {"score": 0.002808104467715375, "phrase": "intercluster_time"}, {"score": 0.002757365657304833, "phrase": "inter-pe_time"}, {"score": 0.0024905846300979947, "phrase": "circus"}, {"score": 0.0024004805502253427, "phrase": "higher_levels"}, {"score": 0.0023570895666021664, "phrase": "expected_write-latency"}, {"score": 0.0022397131088479514, "phrase": "loop_hierarchies"}, {"score": 0.002183230464873613, "phrase": "wide_variety"}, {"score": 0.002167355210897559, "phrase": "system_architectures"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["shared-data", " fine-grained", " low-latency", " user-level", " atomic"], "paper_abstract": "The techniques for fine-grained data sharing are generally available only on specialized architectures, usually involving a shared-bus. The CIRculating Common-Update Sharing (CIRCUS) mechanism has low latency user-level contention-free access to a set of shared circulating data registers. The local access latency is near zero for both read and write operations. These operations can be mapped into more complex operations, such as arithmetic, logical, or data reduction operations such as minimum or sum to be performed by the circulating register hardware (CRH) on the circulating copy of a register. The CRH can also be used to perform atomic operations, such as fetch&add or swap. For a two-dimensional hierarchy of N processing elements (PEs), the write-latency (until the circulating register is updated with a new value) and the update-latency (when all CRH modules can see the updated value) have an optimum cluster size proportional to (N (.) I/D)(1/2), where I is the intercluster time and D is the inter-PE time, including the time between and through one node. The latencies, when optimally clustered, are proportional to (N (.) I (.) D)(1/2). Sub-microsecond write-latency is expected for up to 15,255 PEs or 660 workstations. For higher levels of hierarchy, the expected write-latency is shown to be proportional to the sum of the latencies of all loop hierarchies. CIRCUS is applicable to a wide variety of system architectures and topologies. (c) 2005 Elsevier B.V. All rights reserved.", "paper_title": "Circulating shared-registers for multiprocessor systems", "paper_id": "WOS:000235846000002"}