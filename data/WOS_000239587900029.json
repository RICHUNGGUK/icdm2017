{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "least_squares"}, {"score": 0.004278807087784626, "phrase": "optimal_rates"}, {"score": 0.0038668462001710314, "phrase": "unknown_sparsity"}, {"score": 0.0032115093493071366, "phrase": "nonparametric_regression_setting"}, {"score": 0.00305285084207725, "phrase": "main_tool"}, {"score": 0.0029020077365708966, "phrase": "novel_oracle_inequality"}, {"score": 0.0026222551419772867, "phrase": "empirical_squared_loss"}, {"score": 0.0024926348205500715, "phrase": "penalized_least_squares"}, {"score": 0.0021049977753042253, "phrase": "unknown_regression_function"}], "paper_keywords": [""], "paper_abstract": "This paper shows that near optimal rates of aggregation and adaptation to unknown sparsity can be simultaneously achieved via, penalized least squares in a nonparametric regression setting. The main tool is a novel oracle inequality on the sum between the empirical squared loss of the penalized least squares estimate and a term reflecting the sparsity of the unknown regression function.", "paper_title": "Aggregation and sparsity via l(1) penalized least squares", "paper_id": "WOS:000239587900029"}