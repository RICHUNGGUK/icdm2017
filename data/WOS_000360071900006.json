{"auto_keywords": [{"score": 0.024365862148154532, "phrase": "large_values"}, {"score": 0.00481495049065317, "phrase": "fast_transform_learning"}, {"score": 0.004647442352794694, "phrase": "new_dictionary_learning_strategy"}, {"score": 0.004329630571352367, "phrase": "-sparse_kernels"}, {"score": 0.004278807087784626, "phrase": "known_support"}, {"score": 0.00420368356873615, "phrase": "dictionary_update_step"}, {"score": 0.0040334640350288, "phrase": "non-convex_optimization_problem"}, {"score": 0.003916098498242888, "phrase": "practical_formulation"}, {"score": 0.003757480190200645, "phrase": "gauss-seidel_type_algorithm"}, {"score": 0.0036697319652557363, "phrase": "alternative_least_square_algorithm"}, {"score": 0.003541923233726252, "phrase": "search_space"}, {"score": 0.003479692058963528, "phrase": "proposed_algorithm"}, {"score": 0.00320338467974835, "phrase": "target_atom"}, {"score": 0.0027796495180095657, "phrase": "image_size"}, {"score": 0.0027307746993761035, "phrase": "larger_atoms"}, {"score": 0.0026045914281028473, "phrase": "small_patches"}, {"score": 0.0025436978068459565, "phrase": "conducted_experiments"}, {"score": 0.0024405312482831646, "phrase": "accurately_approximate_atoms"}, {"score": 0.0023415390666563177, "phrase": "sinc_functions"}, {"score": 0.002233301265743633, "phrase": "proposed_experiments"}, {"score": 0.0021049977753042253, "phrase": "global_minimum"}], "paper_keywords": ["Dictionary learning", " Matrix factorization", " Fast transform", " Sparse representation", " Global optimization", " Gauss-Seidel"], "paper_abstract": "This paper introduces a new dictionary learning strategy based on atoms obtained by translating the composition of convolutions with -sparse kernels of known support. The dictionary update step associated with this strategy is a non-convex optimization problem. We propose a practical formulation of this problem and introduce a Gauss-Seidel type algorithm referred to as alternative least square algorithm for its resolution. The search space of the proposed algorithm is of dimension , which is typically smaller than the size of the target atom and much smaller than the size of the image. Moreover, the complexity of this algorithm is linear with respect to the image size, allowing larger atoms to be learned (as opposed to small patches). The conducted experiments show that we are able to accurately approximate atoms such as wavelets, curvelets, sinc functions or cosines for large values of K. The proposed experiments also indicate that the algorithm generally converges to a global minimum for large values of and .", "paper_title": "Toward Fast Transform Learning", "paper_id": "WOS:000360071900006"}