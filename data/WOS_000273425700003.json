{"auto_keywords": [{"score": 0.04247245612782641, "phrase": "lr_pnn"}, {"score": 0.041133586303589764, "phrase": "recurrent_layer_weights"}, {"score": 0.027620001269657773, "phrase": "posterior_probabilities"}, {"score": 0.00481495049065317, "phrase": "enhanced_training_for"}, {"score": 0.004761850836420121, "phrase": "recurrent_probabilistic_neural_networks."}, {"score": 0.004683292152590589, "phrase": "present_contribution"}, {"score": 0.004580550308652044, "phrase": "integral_training_procedure"}, {"score": 0.004238307626560739, "phrase": "smoothing_factor"}, {"score": 0.004122351497440992, "phrase": "pattern_layer"}, {"score": 0.003814212645832246, "phrase": "automatic_process"}, {"score": 0.00370981613076245, "phrase": "adjustable_parameters"}, {"score": 0.003588291565092195, "phrase": "available_training_data"}, {"score": 0.0034324073240680213, "phrase": "original_lr_pnn"}, {"score": 0.0033015555065209865, "phrase": "optimum_separation"}, {"score": 0.0031933617014256676, "phrase": "training_dataset"}, {"score": 0.0030208364972341096, "phrase": "learning_rates"}, {"score": 0.002905629026254221, "phrase": "training_strategy"}, {"score": 0.002810373366887749, "phrase": "overall_classification_accuracy"}, {"score": 0.0026881926384093088, "phrase": "new_training_strategy"}, {"score": 0.0025570607111118793, "phrase": "target_class"}, {"score": 0.002445865715873089, "phrase": "non-target_classes"}, {"score": 0.0024054226746197706, "phrase": "new_fitness_function"}, {"score": 0.002152332983488664, "phrase": "integrated_training_procedure"}], "paper_keywords": ["Locally recurrent probabilistic neural networks", " particle swarm optimization", " differential evolution"], "paper_abstract": "In the present contribution we propose an integral training procedure for the Locally Recurrent Probabilistic Neural Networks (LR PNNs). Specifically, the adjustment of the smoothing factor \"sigma\" in the pattern layer of the LR PNN and the training of the recurrent layer weights are integrated in an automatic process that iteratively estimates all adjustable parameters of the LR PNN from the available training data. Furthermore, in contrast to the original LR PNN, whose recurrent layer was trained to provide optimum separation among the classes on the training dataset, while striving to keep a balance between the learning rates for all classes, here the training strategy is oriented towards optimizing the overall classification accuracy, straightforwardly. More precisely, the new training strategy directly targets at maximizing the posterior probabilities for the target class and minimizing the posterior probabilities estimated for the non-target classes. The new fitness function requires fewer computations for each evaluation, and therefore the overall computational demands for training the recurrent layer weights are reduced. The performance of the integrated training procedure is illustrated on three different speech processing tasks: emotion recognition, speaker identification and speaker verification.", "paper_title": "ENHANCED TRAINING FOR THE LOCALLY RECURRENT PROBABILISTIC NEURAL NETWORKS", "paper_id": "WOS:000273425700003"}