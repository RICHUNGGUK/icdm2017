{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "better_gp"}, {"score": 0.004081385289516914, "phrase": "community_survey"}, {"score": 0.003992385826251404, "phrase": "genetic_programming_benchmark_practices"}, {"score": 0.003820144626025562, "phrase": "broad_consensus"}, {"score": 0.0035755653799043, "phrase": "problem_selection"}, {"score": 0.0034975571855450343, "phrase": "experimental_rigor"}, {"score": 0.003030247482854539, "phrase": "large-scale_benchmark_suite"}, {"score": 0.002539689093411356, "phrase": "common_use"}, {"score": 0.0024569466759289055, "phrase": "important_flaws"}, {"score": 0.0021049977753042253, "phrase": "possible_replacement_problems"}], "paper_keywords": ["Genetic programming", " Benchmarks", " Community survey"], "paper_abstract": "We present the results of a community survey regarding genetic programming benchmark practices. Analysis shows broad consensus that improvement is needed in problem selection and experimental rigor. While views expressed in the survey dissuade us from proposing a large-scale benchmark suite, we find community support for creating a \"blacklist\" of problems which are in common use but have important flaws, and whose use should therefore be discouraged. We propose a set of possible replacement problems.", "paper_title": "Better GP benchmarks: community survey results and proposals", "paper_id": "WOS:000314723200002"}