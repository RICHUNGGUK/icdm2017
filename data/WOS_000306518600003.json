{"auto_keywords": [{"score": 0.034857340085369205, "phrase": "laplace_distribution"}, {"score": 0.00481495049065317, "phrase": "high-dimensional_statistics"}, {"score": 0.0046986841646790315, "phrase": "principled_way"}, {"score": 0.004630270043271386, "phrase": "probability_distributions"}, {"score": 0.004264074363188559, "phrase": "gaussian"}, {"score": 0.0041174821422410544, "phrase": "underdetermined_linear_regression"}, {"score": 0.003863758357292741, "phrase": "sparse_regularization"}, {"score": 0.003572766402294553, "phrase": "sparse_regularized_estimators"}, {"score": 0.0034020946201397057, "phrase": "large_problem_sizes"}, {"score": 0.0032714270066711835, "phrase": "regularized_estimators"}, {"score": 0.0032082475745123574, "phrase": "lasso"}, {"score": 0.0031767199435197243, "phrase": "basis_pursuit"}, {"score": 0.002966230968271199, "phrase": "compressed_sensing_image_reconstruction"}, {"score": 0.0029229677574293725, "phrase": "simple_corollary"}, {"score": 0.002880333728285514, "phrase": "laplace_model"}, {"score": 0.0028106505443369545, "phrase": "bayesian_map_estimation"}, {"score": 0.0027159094186967247, "phrase": "quite_the_reverse"}, {"score": 0.002573424607574822, "phrase": "nontrivial_undersampling_regions"}, {"score": 0.002535876002754739, "phrase": "simple_least-squares"}, {"score": 0.0024624103660827695, "phrase": "oracle_sparse_solution"}, {"score": 0.002299142268368128, "phrase": "simple_rules"}, {"score": 0.0021049977753042253, "phrase": "generalized_pareto_distributions"}], "paper_keywords": ["Basis pursuit", " compressed sensing", " compressible distribution", " high-dimensional statistics", " instance optimality", " Lasso", " linear inverse problems", " maximum a posteriori (MAP) estimator", " order statistics", " sparsity", " statistical regression"], "paper_abstract": "We develop a principled way of identifying probability distributions whose independent and identically distributed realizations are compressible, i.e., can be well approximated as sparse. We focus on Gaussian compressed sensing, an example of underdetermined linear regression, where compressibility is known to ensure the success of estimators exploiting sparse regularization. We prove that many distributions revolving around maximum a posteriori (MAP) interpretation of sparse regularized estimators are in fact incompressible, in the limit of large problem sizes. We especially highlight the Laplace distribution and regularized estimators such as the Lasso and basis pursuit denoising. We rigorously disprove the myth that the success of minimization for compressed sensing image reconstruction is a simple corollary of a Laplace model of images combined with Bayesian MAP estimation, and show that in fact quite the reverse is true. To establish this result, we identify nontrivial undersampling regions where the simple least-squares solution almost surely outperforms an oracle sparse solution, when the data are generated from the Laplace distribution. We also provide simple rules of thumb to characterize classes of compressible and incompressible distributions based on their second and fourth moments. Generalized Gaussian and generalized Pareto distributions serve as running examples.", "paper_title": "Compressible Distributions for High-Dimensional Statistics", "paper_id": "WOS:000306518600003"}