{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "inertial_and_vision_sensors"}, {"score": 0.004191200981137508, "phrase": "inertial_sensors"}, {"score": 0.003922011960904278, "phrase": "sensor_fusion_approach"}, {"score": 0.0037371212381934853, "phrase": "nonlinear_filtering"}, {"score": 0.0036700484729006136, "phrase": "complementary_sensors"}, {"score": 0.0035609154384221567, "phrase": "accurate_and_robust_pose_estimates"}, {"score": 0.0034550163444256386, "phrase": "primary_purpose"}, {"score": 0.0034135407979996673, "phrase": "augmented_reality_applications"}, {"score": 0.003312009953245191, "phrase": "secondary_effect"}, {"score": 0.003252542052430779, "phrase": "computation_time"}, {"score": 0.0031367803372203498, "phrase": "vision_processing"}, {"score": 0.003080449000295983, "phrase": "real-time_implementation"}, {"score": 0.0029887957779347394, "phrase": "kalman_filter"}, {"score": 0.0028823939507764238, "phrase": "dynamic_model"}, {"score": 0.002600995957987461, "phrase": "industrial_robot"}, {"score": 0.0025083651612064144, "phrase": "sensor_unit"}, {"score": 0.002318799760022913, "phrase": "ground_truth"}, {"score": 0.002222717552124615, "phrase": "objective_performance_evaluation"}, {"score": 0.0021049977753042253, "phrase": "absolute_accuracy"}], "paper_keywords": ["Pose estimation", " Sensor fusion", " Computer vision", " Inertial navigation"], "paper_abstract": "The problem of estimating and predicting position and orientation (pose) of a camera is approached by fusing measurements from inertial sensors (accelerometers and rate gyroscopes) and vision. The sensor fusion approach described in this contribution is based on nonlinear filtering of these complementary sensors. This way, accurate and robust pose estimates are available for the primary purpose of augmented reality applications, but with the secondary effect of reducing computation time and improving the performance in vision processing. A real-time implementation of a multi-rate extended Kalman filter is described, using a dynamic model with 22 states, where 12.5 Hz correspondences from vision and 100 Hz inertial measurements are processed. An example where an industrial robot is used to move the sensor unit is presented. The advantage with this configuration is that it provides ground truth for the pose, allowing for objective performance evaluation. The results show that we obtain an absolute accuracy of 2 cm in position and 1 degrees in orientation.", "paper_title": "Robust real-time tracking by fusing measurements from inertial and vision sensors", "paper_id": "WOS:000208118900007"}