{"auto_keywords": [{"score": 0.037519512308061685, "phrase": "lca"}, {"score": 0.01571964376917871, "phrase": "neural_networks"}, {"score": 0.004763565662615503, "phrase": "sparse_approximation"}, {"score": 0.004563427013973105, "phrase": "locally_competitive_algotihm"}, {"score": 0.004371660146981191, "phrase": "hopfield-style_neural_network"}, {"score": 0.004278807087784626, "phrase": "sparse_approximation_problems"}, {"score": 0.0038431888954930083, "phrase": "significant_role"}, {"score": 0.003741371936431574, "phrase": "neural_coding"}, {"score": 0.00366185681714204, "phrase": "signal_processing"}, {"score": 0.0034332736827447654, "phrase": "previous_results"}, {"score": 0.0031845067058084583, "phrase": "lca_architecture"}, {"score": 0.003050508020268648, "phrase": "desirable_convergence_properties"}, {"score": 0.002953711377604328, "phrase": "global_convergence"}, {"score": 0.002859977408815691, "phrase": "objective_function"}, {"score": 0.0027395968397212053, "phrase": "mild_conditions"}, {"score": 0.0025409639091476363, "phrase": "finite_time"}, {"score": 0.0023440589015485077, "phrase": "convergence_rate"}, {"score": 0.002174041708084857, "phrase": "analytically_bounded_convergence_rate"}], "paper_keywords": ["Exponential convergence", " global stability", " locally competitive algorithm", " Lyapunov function", " nonsmooth objective", " sparse approximation"], "paper_abstract": "We present an analysis of the Locally Competitive Algotihm (LCA), which is a Hopfield-style neural network that efficiently solves sparse approximation problems (e.g., approximating a vector from a dictionary using just a few nonzero coefficients). This class of problems plays a significant role in both theories of neural coding and applications in signal processing. However, the LCA lacks analysis of its convergence properties, and previous results on neural networks for nonsmooth optimization do not apply to the specifics of the LCA architecture. We show that the LCA has desirable convergence properties, such as stability and global convergence to the optimum of the objective function when it is unique. Under some mild conditions, the support of the solution is also proven to be reached in finite time. Furthermore, some restrictions on the problem specifics allow us to characterize the convergence rate of the system by showing that the LCA converges exponentially fast with an analytically bounded convergence rate. We support our analysis with several illustrative simulations.", "paper_title": "Convergence and Rate Analysis of Neural Networks for Sparse Approximation", "paper_id": "WOS:000308965800004"}