{"auto_keywords": [{"score": 0.032155470652842195, "phrase": "usc"}, {"score": 0.00481495049065317, "phrase": "hierarchical_binary_decision_tree_approach"}, {"score": 0.004770461993586338, "phrase": "automated_emotion_state_tracking"}, {"score": 0.004704495156558224, "phrase": "crucial_element"}, {"score": 0.0046394362688524475, "phrase": "computational_study"}, {"score": 0.004596561972944185, "phrase": "human_communication_behaviors"}, {"score": 0.0044495843279816075, "phrase": "robust_and_reliable_emotion_recognition_systems"}, {"score": 0.004347473906950474, "phrase": "real-world_applications"}, {"score": 0.004267468074128779, "phrase": "analytical_abilities"}, {"score": 0.004208427318712072, "phrase": "human_decision_making"}, {"score": 0.00413096972023037, "phrase": "human_machine_interfaces"}, {"score": 0.004073809704022463, "phrase": "efficient_communication"}, {"score": 0.00398028819691605, "phrase": "hierarchical_computational_structure"}, {"score": 0.0038708810091769856, "phrase": "proposed_structure"}, {"score": 0.003817306032290766, "phrase": "input_speech_utterance"}, {"score": 0.00372965096877929, "phrase": "multiple_emotion_classes"}, {"score": 0.0036951530348821116, "phrase": "subsequent_layers"}, {"score": 0.0036609730219354796, "phrase": "binary_classifications"}, {"score": 0.003610292749660581, "phrase": "key_idea"}, {"score": 0.0033986345279488476, "phrase": "easiest_classification_tasks"}, {"score": 0.0032898346872897383, "phrase": "error_propagation"}, {"score": 0.00321425262806329, "phrase": "classification_framework"}, {"score": 0.003096905353118796, "phrase": "aibo"}, {"score": 0.0029152570900178956, "phrase": "aibo_database"}, {"score": 0.0028482561434303886, "phrase": "balanced_recall"}, {"score": 0.0027827907725673845, "phrase": "individual_emotion_classes"}, {"score": 0.002744234034162797, "phrase": "hierarchical_structure"}, {"score": 0.0027062100658653485, "phrase": "performance_measure"}, {"score": 0.0026687115458522326, "phrase": "average_unweighted_recall"}, {"score": 0.0026317312518257803, "phrase": "evaluation_data_set"}, {"score": 0.002500456462028735, "phrase": "support_vector_machine_baseline_model"}, {"score": 0.0024543571925896073, "phrase": "usc_iemocap_database"}, {"score": 0.002397923568994474, "phrase": "absolute_improvement"}, {"score": 0.0023103100997344072, "phrase": "baseline_support_vector_machine_modeling"}, {"score": 0.002236272403824371, "phrase": "presented_hierarchical_approach"}, {"score": 0.0021848422732893926, "phrase": "emotional_utterances"}, {"score": 0.0021646022201798247, "phrase": "multiple_database_contexts"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Emotion recognition", " Hierarchical structure", " Support Vector Machine", " Bayesian Logistic Regression"], "paper_abstract": "Automated emotion state tracking is a crucial element in the computational study of human communication behaviors. It is important to design robust and reliable emotion recognition systems that are suitable for real-world applications both to enhance analytical abilities to support human decision making and to design human machine interfaces that facilitate efficient communication. We introduce a hierarchical computational structure to recognize emotions. The proposed structure maps an input speech utterance into one of the multiple emotion classes through subsequent layers of binary classifications. The key idea is that the levels in the tree are designed to solve the easiest classification tasks first, allowing us to mitigate error propagation. We evaluated the classification framework on two different emotional databases using acoustic features, the AIBO database and the USC IEMOCAP database. In the case of the AIBO database, we obtain a balanced recall on each of the individual emotion classes using this hierarchical structure. The performance measure of the average unweighted recall on the evaluation data set improves by 3.37% absolute (8.82% relative) over a Support Vector Machine baseline model. In the USC IEMOCAP database, we obtain an absolute improvement of 7.44% (14.58%) over a baseline Support Vector Machine modeling. The results demonstrate that the presented hierarchical approach is effective for classifying emotional utterances in multiple database contexts. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Emotion recognition using a hierarchical binary decision tree approach", "paper_id": "WOS:000294104000008"}