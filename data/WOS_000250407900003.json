{"auto_keywords": [{"score": 0.03217160122346607, "phrase": "sigma"}, {"score": 0.01571448160948259, "phrase": "batch_gradient_algorithm"}, {"score": 0.004568871736067739, "phrase": "sigma-pi_neural_networks"}, {"score": 0.003903269701018361, "phrase": "traditional_feed-forward_neural_networks"}, {"score": 0.003423042581201533, "phrase": "spnn_learning"}, {"score": 0.002737960968372391, "phrase": "sigma_u-sigma"}, {"score": 0.0021049977753042253, "phrase": "error_function"}], "paper_keywords": ["convergence", " sigma-Pi-Sigma neural networks", " sigma-Sigma-Pi neural networks", " sigma-Pi-Sigma-Pi neural networks", " batch gradient algorithm", " monotonicity"], "paper_abstract": "Sigma-Pi (Sigma u-Sigma a) neural networks (SPNNs) are known to provide more powerful mapping capability than traditional feed-forward neural networks. A unified convergence analysis for the batch gradient algorithm for SPNN learning is presented, covering three classes of SPNNs: Sigma u-Sigma a-Sigma u, Sigma u-Sigma u-Sigma a and Sigma u-Sigma a-Sigma u-Sigma a. The monotonicity of the error function in the iteration is also guaranteed.", "paper_title": "Convergence analysis of batch gradient algorithm for three classes of sigma-pi neural networks", "paper_id": "WOS:000250407900003"}