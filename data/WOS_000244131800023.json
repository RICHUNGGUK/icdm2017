{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "discrete-time_zero-sum_games"}, {"score": 0.004658426838467567, "phrase": "h-infinity_control"}, {"score": 0.00443308765153716, "phrase": "adaptive_critic_approximate_dynamic_programming_designs"}, {"score": 0.004218602458288531, "phrase": "discrete-time_zero-sum_game"}, {"score": 0.004081385289516914, "phrase": "state_and_action_spaces"}, {"score": 0.003820144626025562, "phrase": "forward-in-time_reinforcement_learning_algorithm"}, {"score": 0.0035461111918923117, "phrase": "corresponding_zero-sum_game"}, {"score": 0.0030808211830129304, "phrase": "riccati_equation"}, {"score": 0.0030052718739167696, "phrase": "well-known_discrete-time_h-infinity_optimal_control_problem"}, {"score": 0.002305799666781492, "phrase": "value_function"}, {"score": 0.0021049977753042253, "phrase": "h-infinity_autopilot_design"}], "paper_keywords": ["adaptive critics", " approximate dynamic programming (ADP)", " H-infinity optimal control", " policy iteration", " zero-sum game"], "paper_abstract": "In this correspondence, adaptive critic approximate dynamic programming designs are derived to solve the discrete-time zero-sum game in which the state and action spaces are continuous. This results in a forward-in-time reinforcement learning algorithm that converges to the Nash equilibrium of the corresponding zero-sum game. The results in this correspondence can be thought of as a way to solve the Riccati equation of the well-known discrete-time H-infinity optimal control problem forward in time. Two schemes are presented, namely: 1) a heuristic dynamic programming and 2) a dual-heuristic dynamic programming, to solve for the value function and the costate of the game, respectively. An H-infinity autopilot design for an F-16 aircraft is presented to-illustrate the results.", "paper_title": "Adaptive critic designs for discrete-time zero-sum games with application to H-infinity control", "paper_id": "WOS:000244131800023"}