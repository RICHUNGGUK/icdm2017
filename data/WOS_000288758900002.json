{"auto_keywords": [{"score": 0.032668893300942095, "phrase": "splitting_rule"}, {"score": 0.00481495049065317, "phrase": "bipartite_ranking"}, {"score": 0.004773692073488661, "phrase": "recursive_partitioning_methods"}, {"score": 0.004652016085325825, "phrase": "machine_learning"}, {"score": 0.004323776691055635, "phrase": "bipartite_ranking_problem"}, {"score": 0.0041773934798743405, "phrase": "treerank_approach"}, {"score": 0.0041243014667821576, "phrase": "cl"}, {"score": 0.004053362743156545, "phrase": "vayatis"}, {"score": 0.003933000051147382, "phrase": "algorithmic_learning_theory"}, {"score": 0.003623949225126944, "phrase": "tree-structured_algorithms"}, {"score": 0.003486123304074186, "phrase": "classification_data"}, {"score": 0.003441352119167824, "phrase": "main_contributions"}, {"score": 0.0033971539563683174, "phrase": "present_work"}, {"score": 0.0032538723984338615, "phrase": "treerank_algorithm"}, {"score": 0.0031435968684569112, "phrase": "crucial_issues"}, {"score": 0.0030109772626153797, "phrase": "\"right\"_size"}, {"score": 0.0029722900280316216, "phrase": "ranking_tree"}, {"score": 0.0027861630854233693, "phrase": "cost-sensitive_classification_task"}, {"score": 0.002762240719058935, "phrase": "data-dependent_cost"}, {"score": 0.0026916956768363158, "phrase": "straightforward_modifications"}, {"score": 0.0026570998719084153, "phrase": "classification_algorithm"}, {"score": 0.0025014259919355453, "phrase": "cost-complexity_pruning_method"}, {"score": 0.0023145690608400425, "phrase": "large_auc."}, {"score": 0.0022749744354271834, "phrase": "performance_bounds"}, {"score": 0.002197801146305747, "phrase": "recent_work"}, {"score": 0.0021789194955896124, "phrase": "nonparametric_model_selection"}], "paper_keywords": ["Scoring algorithms", " Ranking trees", " AUC", " ROC curve", " Recursive partitioning", " Splitting rules", " TreeRank", " Pruning", " Variable importance"], "paper_abstract": "Recursive partitioning methods are among the most popular techniques in machine learning. The purpose of this paper is to investigate how to adapt this methodology to the bipartite ranking problem. Following in the footsteps of the TreeRank approach developed in Cl,men double dagger on and Vayatis (Proceedings of the 2008 Conference on Algorithmic Learning Theory, 2008 and IEEE Trans. Inf. Theory 55(9):4316-4336, 2009), we present tree-structured algorithms designed for learning to rank instances based on classification data. The main contributions of the present work are the following: the practical implementation of the TreeRank algorithm, well-founded solutions to the crucial issues related to the splitting rule and the choice of the \"right\" size for the ranking tree. From the angle embraced in this paper, splitting is viewed as a cost-sensitive classification task with data-dependent cost. Hence, up to straightforward modifications, any classification algorithm may serve as a splitting rule. Also, we propose to implement a cost-complexity pruning method after the growing stage in order to produce a \"right-sized\" ranking sub-tree with large AUC. In particular, performance bounds are established for pruning schemes inspired by recent work on nonparametric model selection. Eventually, we propose indicators for variable importance and variable dependence, plus various simulation studies illustrating the potential of our method.", "paper_title": "Adaptive partitioning schemes for bipartite ranking", "paper_id": "WOS:000288758900002"}