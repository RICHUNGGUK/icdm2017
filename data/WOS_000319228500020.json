{"auto_keywords": [{"score": 0.049636680610423214, "phrase": "relevant_negatives"}, {"score": 0.007503615093899253, "phrase": "negative_bootstrap"}, {"score": 0.006321898749105692, "phrase": "model_compression"}, {"score": 0.00481495049065317, "phrase": "bootstrapping_visual_categorization"}, {"score": 0.004726382602809228, "phrase": "learning_classifiers"}, {"score": 0.004596561972944185, "phrase": "image_categorization"}, {"score": 0.004388035083357706, "phrase": "negative_examples"}, {"score": 0.004267468074128779, "phrase": "positive_ones"}, {"score": 0.00398028819691605, "phrase": "user-tagged_images"}, {"score": 0.003644001325962353, "phrase": "de_facto_standard"}, {"score": 0.003446353878654566, "phrase": "random_sampling"}, {"score": 0.003336031521101184, "phrase": "visual_concept"}, {"score": 0.003244275497488318, "phrase": "new_algorithm"}, {"score": 0.003068242064611088, "phrase": "small_proportion"}, {"score": 0.002956209594959227, "phrase": "meta_classifiers"}, {"score": 0.0029152570900178956, "phrase": "efficient_classification"}, {"score": 0.0028087949095629955, "phrase": "classification_time"}, {"score": 0.002744234034162797, "phrase": "ensemble_size"}, {"score": 0.0025952620596908773, "phrase": "relative_gains"}, {"score": 0.0024772998651215964, "phrase": "mean_average_precision"}, {"score": 0.0024429654627197393, "phrase": "concept_search"}, {"score": 0.0023646866829474798, "phrase": "search_time"}, {"score": 0.0021049977753042253, "phrase": "better_visual_concept_classifiers"}], "paper_keywords": ["Model compression", " negative bootstrap", " relevant negative examples", " visual categorization"], "paper_abstract": "Learning classifiers for many visual concepts are important for image categorization and retrieval. As a classifier tends to misclassify negative examples which are visually similar to positive ones, inclusion of such misclassified and thus relevant negatives should be stressed during learning. User-tagged images are abundant online, but which images are the relevant negatives remains unclear. Sampling negatives at random is the de facto standard in the literature. In this paper, we go beyond random sampling by proposing Negative Bootstrap. Given a visual concept and a few positive examples, the new algorithm iteratively finds relevant negatives. Per iteration, we learn from a small proportion of many user-tagged images, yielding an ensemble of meta classifiers. For efficient classification, we introduce Model Compression such that the classification time is independent of the ensemble size. Compared with the state of the art, we obtain relative gains of 14% and 18% on two present-day benchmarks in terms of mean average precision. For concept search in one million images, model compression reduces the search time from over 20 h to approximately 6 min. The effectiveness and efficiency, without the need of manually labeling any negatives, make negative bootstrap appealing for learning better visual concept classifiers.", "paper_title": "Bootstrapping Visual Categorization With Relevant Negatives", "paper_id": "WOS:000319228500020"}