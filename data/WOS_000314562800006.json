{"auto_keywords": [{"score": 0.036146701610938206, "phrase": "nonparametric_models"}, {"score": 0.00481495049065317, "phrase": "squared-loss_mutual_information_estimation"}, {"score": 0.00468912237497774, "phrase": "sufficient_dimension_reduction"}, {"score": 0.004639712384096822, "phrase": "supervised_learning"}, {"score": 0.004518442937844318, "phrase": "low-dimensional_subspace"}, {"score": 0.004240116703465252, "phrase": "output_values"}, {"score": 0.004173244935967375, "phrase": "input_features"}, {"score": 0.003957833102043277, "phrase": "novel_sufficient_dimension-reduction_method"}, {"score": 0.0038953958723049287, "phrase": "squared-loss_variant"}, {"score": 0.003773449694108617, "phrase": "dependency_measure"}, {"score": 0.003674738405118697, "phrase": "density-ratio_estimator"}, {"score": 0.0036167509502276294, "phrase": "squared-loss_mutual_information"}, {"score": 0.003503497128638102, "phrase": "minimum_contrast_estimator"}, {"score": 0.003270092550315648, "phrase": "appropriate_model"}, {"score": 0.003150900886669123, "phrase": "prespecified_structure"}, {"score": 0.003101153870424188, "phrase": "underlying_distributions"}, {"score": 0.0030199761431186434, "phrase": "asymptotic_bias"}, {"score": 0.0029565620771207003, "phrase": "parametric_models"}, {"score": 0.0029098743621130004, "phrase": "asymptotic_convergence_rate"}, {"score": 0.002833689453299307, "phrase": "convergence_analysis"}, {"score": 0.0026447893350441502, "phrase": "convergence_rate"}, {"score": 0.002575526825728026, "phrase": "bracketing_entropy"}, {"score": 0.0024553821147953463, "phrase": "natural_gradient_algorithm"}, {"score": 0.002416589459425828, "phrase": "grassmann_manifold"}, {"score": 0.002391067964886602, "phrase": "sufficient_subspace_search"}, {"score": 0.0023532890136269986, "phrase": "analytic_formula"}, {"score": 0.002219790619936024, "phrase": "numerical_experiments"}, {"score": 0.0021731423856364003, "phrase": "proposed_method"}, {"score": 0.0021274723597955567, "phrase": "existing_dimension-reduction_approaches"}, {"score": 0.0021049977753042253, "phrase": "artificial_and_benchmark_data_sets"}], "paper_keywords": [""], "paper_abstract": "The goal of sufficient dimension reduction in supervised learning is to find the low-dimensional subspace of input features that contains all of the information about the output values that the input features possess. In this letter, we propose a novel sufficient dimension-reduction method using a squared-loss variant of mutual information as a dependency measure. We apply a density-ratio estimator for approximating squared-loss mutual information that is formulated as a minimum contrast estimator on parametric or nonparametric models. Since cross-validation is available for choosing an appropriate model, our method does not require any prespecified structure on the underlying distributions. We elucidate the asymptotic bias of our estimator on parametric models and the asymptotic convergence rate on nonparametric models. The convergence analysis utilizes the uniform tail-bound of a U-process, and the convergence rate is characterized by the bracketing entropy of the model. We then develop a natural gradient algorithm on the Grassmann manifold for sufficient subspace search. The analytic formula of our estimator allows us to compute the gradient efficiently. Numerical experiments show that the proposed method compares favorably with existing dimension-reduction approaches on artificial and benchmark data sets.", "paper_title": "Sufficient Dimension Reduction via Squared-Loss Mutual Information Estimation", "paper_id": "WOS:000314562800006"}