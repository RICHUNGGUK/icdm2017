{"auto_keywords": [{"score": 0.025921377359192966, "phrase": "different_types"}, {"score": 0.00481495049065317, "phrase": "efficient_structured_max-margin_learning"}, {"score": 0.004779085953614184, "phrase": "high-order_mrf_models"}, {"score": 0.004434745755769205, "phrase": "pairwise_and_higher-order_models"}, {"score": 0.004256005669252138, "phrase": "tractable_subproblems"}, {"score": 0.0041151131903594445, "phrase": "dual_decomposition_principle"}, {"score": 0.003964007795174772, "phrase": "mrf_optimization"}, {"score": 0.0038471128411102914, "phrase": "max-margin_learning_method"}, {"score": 0.0036781781340563748, "phrase": "complex_high-order_mrf"}, {"score": 0.003556345965454902, "phrase": "simple_slave_mrfs"}, {"score": 0.0033496005635637316, "phrase": "solid_mathematical_principles"}, {"score": 0.0031430451507177388, "phrase": "increasing_accuracy"}, {"score": 0.003050285492466281, "phrase": "convex_relaxations"}, {"score": 0.002993701876937243, "phrase": "loss-augmented_map-mrf_inference"}, {"score": 0.0029602552900111407, "phrase": "max-margin_learning_approach"}, {"score": 0.0028090313440881937, "phrase": "special_structure"}, {"score": 0.002482363351755741, "phrase": "higher-order_mrfs"}, {"score": 0.002382124723569496, "phrase": "dissimilarity_loss_functions"}, {"score": 0.002311769781106355, "phrase": "appropriate_models"}, {"score": 0.0022688546528208133, "phrase": "vision_tasks"}, {"score": 0.0022434880661725493, "phrase": "high-order_models"}, {"score": 0.002226734412397011, "phrase": "compact_pose-invariant_shape"}, {"score": 0.0022018377054878534, "phrase": "knowledge-based_segmentation"}, {"score": 0.0021609589091422608, "phrase": "stereo_matching"}, {"score": 0.0021049977753042253, "phrase": "potts_mrfs"}], "paper_keywords": [""], "paper_abstract": "We present a very general algorithm for structured prediction learning that is able to efficiently handle discrete MRFs/CRFs (including both pairwise and higher-order models) so long as they can admit a decomposition into tractable subproblems. At its core, it relies on a dual decomposition principle that has been recently employed in the task of MRF optimization. By properly combining such an approach with a max-margin learning method, the proposed framework manages to reduce the training of a complex high-order MRF to the parallel training of a series of simple slave MRFs that are much easier to handle. This leads to a very efficient and general learning scheme that relies on solid mathematical principles. We thoroughly analyze its theoretical properties, and also show that it can yield learning algorithms of increasing accuracy since it naturally allows a hierarchy of convex relaxations to be used for loss-augmented MAP-MRF inference within a max-margin learning approach. Furthermore, it can be easily adapted to take advantage of the special structure that may be present in a given class of MRFs. We demonstrate the generality and flexibility of our approach by testing it on a variety of scenarios, including training of pairwise and higher-order MRFs, training by using different types of regularizers and/or different types of dissimilarity loss functions, as well as by learning of appropriate models for a variety of vision tasks (including high-order models for compact pose-invariant shape priors, knowledge-based segmentation, image denoising, stereo matching as well as high-order Potts MRFs).", "paper_title": "A Framework for Efficient Structured Max-Margin Learning of High-Order MRF Models", "paper_id": "WOS:000355931100010"}