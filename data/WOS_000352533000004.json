{"auto_keywords": [{"score": 0.04928833151703449, "phrase": "facial_action_units"}, {"score": 0.03815092451822489, "phrase": "proposed_model"}, {"score": 0.015110158099439022, "phrase": "spontaneously_displayed_facial_expressions"}, {"score": 0.014052494513513954, "phrase": "target_problem"}, {"score": 0.010751135122345197, "phrase": "facial_expressions"}, {"score": 0.00481495049065317, "phrase": "context-sensitive_dynamic_ordinal_regression"}, {"score": 0.0045346863560033175, "phrase": "high_variability"}, {"score": 0.00450456611645112, "phrase": "subject-specific_facial_expressiveness"}, {"score": 0.004415395176043526, "phrase": "illumination_changes"}, {"score": 0.004186105377259855, "phrase": "existing_methods"}, {"score": 0.003916098498242888, "phrase": "facial_action_unit_intensity"}, {"score": 0.003543273649210823, "phrase": "six_context_questions"}, {"score": 0.003438483411236772, "phrase": "context_questions"}, {"score": 0.0030088845918380468, "phrase": "newly_introduced_context-dependent_covariate_effects"}, {"score": 0.0028907624732186373, "phrase": "temporal_correlation"}, {"score": 0.002861961643289598, "phrase": "ordinal_outputs"}, {"score": 0.002795867549039675, "phrase": "action_units"}, {"score": 0.0027404283412788997, "phrase": "weighted_softmax-margin_learning"}, {"score": 0.0026860854754791274, "phrase": "skewed_distribution"}, {"score": 0.0026593184154220123, "phrase": "intensity_levels"}, {"score": 0.0025978919678702793, "phrase": "spontaneous_facial_data"}, {"score": 0.002537880772794665, "phrase": "intensity_estimation"}, {"score": 0.0024627509513027923, "phrase": "shoulder_pain"}, {"score": 0.002446359986258807, "phrase": "disfa"}, {"score": 0.0023190823948528953, "phrase": "target_tasks"}, {"score": 0.002198412942421744, "phrase": "traditional_learning"}, {"score": 0.0021404485085209143, "phrase": "proposed_weighted_learning_results"}, {"score": 0.0021049977753042253, "phrase": "imbalanced_intensity_data"}], "paper_keywords": ["FACS", " action unit intensity", " spontaneous facial behavior", " facial expression analysis", " ordinal regression", " conditional random fields", " context modeling"], "paper_abstract": "Modeling intensity of facial action units from spontaneously displayed facial expressions is challenging mainly because of high variability in subject-specific facial expressiveness, head-movements, illumination changes, etc. These factors make the target problem highly context-sensitive. However, existing methods usually ignore this context-sensitivity of the target problem. We propose a novel Conditional Ordinal Random Field (CORF) model for context-sensitive modeling of the facial action unit intensity, where the W5+ (who, when, what, where, why and how) definition of the context is used. While the proposed model is general enough to handle all six context questions, in this paper we focus on the context questions: who (the observed subject), how (the changes in facial expressions), and when (the timing of facial expressions and their intensity). The context questions who and how are modeled by means of the newly introduced context-dependent covariate effects, and the context question when is modeled in terms of temporal correlation between the ordinal outputs, i.e., intensity levels of action units. We also introduce a weighted softmax-margin learning of CRFs from data with skewed distribution of the intensity levels, which is commonly encountered in spontaneous facial data. The proposed model is evaluated on intensity estimation of pain and facial action units using two recently published datasets (UNBC Shoulder Pain and DISFA) of spontaneously displayed facial expressions. Our experiments show that the proposed model performs significantly better on the target tasks compared to the state-of-the-art approaches. Furthermore, compared to traditional learning of CRFs, we show that the proposed weighted learning results in more robust parameter estimation from the imbalanced intensity data.", "paper_title": "Context-Sensitive Dynamic Ordinal Regression for Intensity Estimation of Facial Action Units", "paper_id": "WOS:000352533000004"}