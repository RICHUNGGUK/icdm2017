{"auto_keywords": [{"score": 0.004771196027464407, "phrase": "visual_recognition_purposes"}, {"score": 0.004600100053651545, "phrase": "sparse_representation"}, {"score": 0.004537525607322823, "phrase": "fundamental_role"}, {"score": 0.004475798524102238, "phrase": "one-shot_learning"}, {"score": 0.004435112270935294, "phrase": "real-time_recognition"}, {"score": 0.004276016541197123, "phrase": "rgbd_images"}, {"score": 0.00393852508392789, "phrase": "computationally_efficient_way"}, {"score": 0.0038849148160362257, "phrase": "proposed_method"}, {"score": 0.0037454814403672697, "phrase": "scene_flow"}, {"score": 0.003660879039645457, "phrase": "global_histograms"}, {"score": 0.0036275736378775757, "phrase": "oriented_gradient"}, {"score": 0.003545625043276134, "phrase": "adaptive_sparse_coding"}, {"score": 0.003465521267039616, "phrase": "high-level_patterns"}, {"score": 0.0033410898465226417, "phrase": "simultaneous_on-line_video_segmentation"}, {"score": 0.00325069754015951, "phrase": "linear_svms"}, {"score": 0.0032064196784562017, "phrase": "main_contribution"}, {"score": 0.003119659433566212, "phrase": "effective_real-time_system"}, {"score": 0.003091262582401731, "phrase": "one-shot_action_modeling"}, {"score": 0.002953097497434808, "phrase": "sparse_coding_techniques"}, {"score": 0.0027954035431845344, "phrase": "benchmark_data_set"}, {"score": 0.002769950045729965, "phrase": "one-shot_action"}, {"score": 0.0025981315738078793, "phrase": "kinect_sensor"}, {"score": 0.0025744696954056404, "phrase": "complex_actions"}, {"score": 0.0025162511343171, "phrase": "small_details"}, {"score": 0.0024258208033731154, "phrase": "human-robot_interaction_purposes"}, {"score": 0.0022962183207691188, "phrase": "human-robot_interaction"}, {"score": 0.002244277924422011, "phrase": "memory_game"}, {"score": 0.0021049977753042253, "phrase": "humanoid_robot"}], "paper_keywords": ["real-time action recognition", " sparse representation", " one-shot action learning", " human robot interaction"], "paper_abstract": "Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efficient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective real-time system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, \"All Gestures You Can\", to be played against a humanoid robot.", "paper_title": "Keep It Simple And Sparse: Real-Time Action Recognition", "paper_id": "WOS:000327007400005"}