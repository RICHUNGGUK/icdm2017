{"auto_keywords": [{"score": 0.04443809788762847, "phrase": "inverted_dirichlet_mixture_models"}, {"score": 0.00481495049065317, "phrase": "inverted_dirichlet_mixtures"}, {"score": 0.004758053266234474, "phrase": "svm_kernels_generation"}, {"score": 0.004591345494188032, "phrase": "positive_data_modeling"}, {"score": 0.004100851549096203, "phrase": "outstanding_challenge"}, {"score": 0.003933609372601274, "phrase": "accurate_kernels"}, {"score": 0.0038640303886081444, "phrase": "kernels_generation_approaches"}, {"score": 0.003728529119516347, "phrase": "information_theory"}, {"score": 0.0035341028694769036, "phrase": "data_structure"}, {"score": 0.0033497809969621267, "phrase": "principled_bayesian_framework"}, {"score": 0.0031939996970411027, "phrase": "parameter_estimation"}, {"score": 0.0031561961485589633, "phrase": "bayes_factor"}, {"score": 0.003118838635447237, "phrase": "model_selection"}, {"score": 0.0029737652463012318, "phrase": "mixture's_components"}, {"score": 0.0027034948760997564, "phrase": "inverted_dirichlet_distribution"}, {"score": 0.002608583927848071, "phrase": "exponential_distributions"}, {"score": 0.0025471636425006155, "phrase": "model_parameters"}, {"score": 0.0023433334920926713, "phrase": "posterior_distributions"}, {"score": 0.0021946656099592608, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "visual_scenes"}], "paper_keywords": ["Mixture models", " SVM", " Hybrid models", " Inverted Dirichlet", " Bayesian inference", " Bayes factor", " Model selection", " Gibbs sampling", " Kernels", " Object detection", " Image databases"], "paper_abstract": "We describe approaches for positive data modeling and classification using both finite inverted Dirichlet mixture models and support vector machines (SVMs). Inverted Dirichlet mixture models are used to tackle an outstanding challenge in SVMs namely the generation of accurate kernels. The kernels generation approaches, grounded on ideas from information theory that we consider, allow the incorporation of data structure and its structural constraints. Inverted Dirichlet mixture models are learned within a principled Bayesian framework using both Gibbs sampler and Metropolis-Hastings for parameter estimation and Bayes factor for model selection (i.e., determining the number of mixture's components). Our Bayesian learning approach uses priors, which we derive by showing that the inverted Dirichlet distribution belongs to the family of exponential distributions, over the model parameters, and then combines these priors with information from the data to build posterior distributions. We illustrate the merits and the effectiveness of the proposed method with two real-world challenging applications namely object detection and visual scenes analysis and classification.", "paper_title": "Bayesian learning of inverted Dirichlet mixtures for SVM kernels generation", "paper_id": "WOS:000325026400026"}