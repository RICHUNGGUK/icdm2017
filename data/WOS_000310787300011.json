{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "scalable_low-rank_learning"}, {"score": 0.004692227830462433, "phrase": "scalability_issues"}, {"score": 0.004644014990871423, "phrase": "low-rank_matrix_learning_problems"}, {"score": 0.004456044928556639, "phrase": "nuclear_norm"}, {"score": 0.00443308765153716, "phrase": "regularized_optimization_problems"}, {"score": 0.004231699728933577, "phrase": "high_computational_complexities"}, {"score": 0.004145143433523008, "phrase": "existing_solvers"}, {"score": 0.004060350364088702, "phrase": "large-scale_settings"}, {"score": 0.0038959121068990517, "phrase": "optimal_solution_matrix"}, {"score": 0.003835972725191194, "phrase": "nnrop"}, {"score": 0.0037769520261285872, "phrase": "low_rank"}, {"score": 0.003680587852807675, "phrase": "classic_mechanism"}, {"score": 0.0036427316134070007, "phrase": "low-rank_matrix_factorization"}, {"score": 0.0034951469511975346, "phrase": "active_subspace_algorithm"}, {"score": 0.003370907075333389, "phrase": "large-scale_nnrops"}, {"score": 0.003336225354214798, "phrase": "small-scale_problems"}, {"score": 0.0031845067058084583, "phrase": "large_solution_matrix"}, {"score": 0.0030872017736888113, "phrase": "small_orthonormal_matrix"}, {"score": 0.002977419551510206, "phrase": "small_matrix"}, {"score": 0.0028715299874549245, "phrase": "nonconvex_problems"}, {"score": 0.0027837615811247963, "phrase": "suboptimal_solution"}, {"score": 0.0026986685585382347, "phrase": "augmented_lagrange_alternating_direction_method"}, {"score": 0.002602677521275444, "phrase": "candes"}, {"score": 0.0025764066752003074, "phrase": "li"}, {"score": 0.002420762293555924, "phrase": "typical_example"}, {"score": 0.0023711591828229736, "phrase": "theoretical_results"}, {"score": 0.0021049977753042253, "phrase": "computational_complexity"}], "paper_keywords": [""], "paper_abstract": "We address the scalability issues in low-rank matrix learning problems. Usually these problems resort to solving nuclear norm regularized optimization problems (NNROPs), which often suffer from high computational complexities if based on existing solvers, especially in large-scale settings. Based on the fact that the optimal solution matrix to an NNROP is often low rank, we revisit the classic mechanism of low-rank matrix factorization, based on which we present an active subspace algorithm for efficiently solving NNROPs by transforming large-scale NNROPs into small-scale problems. The transformation is achieved by factorizing the large solution matrix into the product of a small orthonormal matrix (active subspace) and another small matrix. Although such a transformation generally leads to nonconvex problems, we show that a suboptimal solution can be found by the augmented Lagrange alternating direction method. For the robust PCA (RPCA) (Candes, Li, Ma, & Wright, 2009) problem, a typical example of NNROPs, theoretical results verify the suboptimality of the solution produced by our algorithm. For the general NNROPs, we empirically show that our algorithm significantly reduces the computational complexity without loss of optimality.", "paper_title": "Active Subspace: Toward Scalable Low-Rank Learning", "paper_id": "WOS:000310787300011"}