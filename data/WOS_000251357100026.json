{"auto_keywords": [{"score": 0.04866188206860211, "phrase": "lda"}, {"score": 0.027468832365112866, "phrase": "generalized_eigenvalue_problem"}, {"score": 0.00481495049065317, "phrase": "generalized_linear_discriminant_analysis_algorithms"}, {"score": 0.004579780586951551, "phrase": "dimension_reduction_method"}, {"score": 0.004488946121663966, "phrase": "optimal_linear_transformation"}, {"score": 0.004399905285945236, "phrase": "class_separability"}, {"score": 0.004291073117901643, "phrase": "undersampled_problems"}, {"score": 0.004184921610672245, "phrase": "data_samples"}, {"score": 0.004040688646993605, "phrase": "data_space"}, {"score": 0.003785836059995376, "phrase": "scatter_matrices"}, {"score": 0.003729335910488339, "phrase": "high_dimensionality"}, {"score": 0.0033231814056233103, "phrase": "theoretical_and_algorithmic_relationships"}, {"score": 0.0031765314093444956, "phrase": "text_classification"}, {"score": 0.003066945490483524, "phrase": "practical_dimension_reduction_method"}, {"score": 0.003036333273800443, "phrase": "high_dimensional_data"}, {"score": 0.0029909851863915283, "phrase": "efficient_algorithm"}, {"score": 0.0028877818105020434, "phrase": "computational_complexity"}, {"score": 0.002830410693265704, "phrase": "competitive_prediction_accuracies"}, {"score": 0.0027602925235474317, "phrase": "nonlinear_extensions"}, {"score": 0.002719055901632759, "phrase": "lda_algorithms"}, {"score": 0.002678433668156676, "phrase": "kernel_methods"}, {"score": 0.002509284173541882, "phrase": "kernel-based_feature_space"}, {"score": 0.0024717881264735477, "phrase": "generalized_lda_algorithms"}, {"score": 0.0023507916951658455, "phrase": "nonlinear_discriminant_analysis"}, {"score": 0.0022696277881969896, "phrase": "nonlinear_discriminant_analysis_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["dimension reduction", " feature extraction", " generalized linear discriminant analysis", " kernel methods", " nonlinear discriminant analysis", " undersampled problems"], "paper_abstract": "Linear discriminant analysis (LDA) is a dimension reduction method which finds an optimal linear transformation that maximizes the class separability. However, in undersampled problems where the number of data samples is smaller than the dimension of data space, it is difficult to apply LDA due to the singularity of scatter matrices caused by high dimensionality. In order to make LDA applicable, several generalizations of LDA have been proposed recently. In this paper, we present theoretical and algorithmic relationships among several generalized LDA algorithms and compare their computational complexities and performances in text classification and face recognition. Towards a practical dimension reduction method for high dimensional data, an efficient algorithm is proposed, which reduces the computational complexity greatly while achieving competitive prediction accuracies. We also present nonlinear extensions of these LDA algorithms based on kernel methods. It is shown that a generalized eigenvalue problem can be formulated in the kernel-based feature space, and generalized LDA algorithms are applied to solve the generalized eigenvalue problem, resulting in nonlinear discriminant analysis. Performances of these linear and nonlinear discriminant analysis algorithms are compared extensively. (C) 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "A comparison of generalized linear discriminant analysis algorithms", "paper_id": "WOS:000251357100026"}