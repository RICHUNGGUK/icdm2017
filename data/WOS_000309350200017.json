{"auto_keywords": [{"score": 0.048342791832576656, "phrase": "parray"}, {"score": 0.03821066921635996, "phrase": "array_type"}, {"score": 0.00481495049065317, "phrase": "heterogeneous_parallelism"}, {"score": 0.0046834372388970405, "phrase": "programming_interface"}, {"score": 0.0045765783794477505, "phrase": "parallelizing_arrays"}, {"score": 0.004492841495994535, "phrase": "system-level_succinct_programming"}, {"score": 0.004370087541308677, "phrase": "gpu_clusters"}, {"score": 0.004309968592440261, "phrase": "current_practice"}, {"score": 0.004270347498153512, "phrase": "software_development"}, {"score": 0.004172874316207784, "phrase": "pthread"}, {"score": 0.004134560512675152, "phrase": "openmp"}, {"score": 0.004096570254094196, "phrase": "cuda"}, {"score": 0.004058846808522766, "phrase": "mpi."}, {"score": 0.003911578552479064, "phrase": "different_numbers"}, {"score": 0.0037696519417841287, "phrase": "mainstream_c_programming"}, {"score": 0.003734979290204709, "phrase": "novel_array_types"}, {"score": 0.0035498822293046884, "phrase": "tree_structure"}, {"score": 0.003468796799802612, "phrase": "memory_hierarchy"}, {"score": 0.002936997013291548, "phrase": "single-program_multiple-code_block"}, {"score": 0.0026651772919894534, "phrase": "performance-related_features"}, {"score": 0.0026284507395498897, "phrase": "deep_manual_optimization"}, {"score": 0.0025802660239538353, "phrase": "source-to-source_code_generator"}, {"score": 0.0025329623864528317, "phrase": "low-level_library_calls"}, {"score": 0.0024865237976623286, "phrase": "type_information"}, {"score": 0.0024636239664210433, "phrase": "higher-level_programming"}, {"score": 0.0024409345168575833, "phrase": "automatic_performance_optimization"}, {"score": 0.0022878396720586044, "phrase": "case_study"}, {"score": 0.002174303033371869, "phrase": "intel_cluster_mkl"}], "paper_keywords": ["Languages", " Performance", " Theory", " Parallel Programming", " Array Representation", " Heterogeneous Parallelism", " GPU Clusters"], "paper_abstract": "This paper introduces a programming interface called PARRAY (or Parallelizing ARRAYs) that supports system-level succinct programming for heterogeneous parallel systems like GPU clusters. The current practice of software development requires combining several low-level libraries like Pthread, OpenMP, CUDA and MPI. Achieving productivity and portability is hard with different numbers and models of GPUs. PARRAY extends mainstream C programming with novel array types of the following features:1)the dimensions of an array type are nested in a tree structure, conceptually reflecting the memory hierarchy; 2) the definition of an array type may contain references to other array types, allowing sophisticated array types to be created for parallelization; 3) threads also form arrays that allow programming in a Single-Program Multiple-Code block (SPMC) style to unify various sophisticated communication patterns. This leads to shorter, more portable and maintainable parallel codes, while the programmer still has control over performance-related features necessary for deep manual optimization. Although the source-to-source code generator only faithfully generates low-level library calls according to the type information,higher-level programming and automatic performance optimization are still possible through building libraries of subprograms on top of PARRAY. The case study on cluster FFT illustrates a simple 30-line code that 2x-outperforms Intel Cluster MKL on the Tianhe-1A system with 7168 Fermi GPUs and 14336 CPUs.", "paper_title": "PARRAY: A Unifying Array Representation for Heterogeneous Parallelism", "paper_id": "WOS:000309350200017"}