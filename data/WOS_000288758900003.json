{"auto_keywords": [{"score": 0.03800571030742009, "phrase": "bayes_risk"}, {"score": 0.03316377535189864, "phrase": "rate_n"}, {"score": 0.00481495049065317, "phrase": "noisy_linear_classifiers"}, {"score": 0.0047588556219929756, "phrase": "adaptive_and_selective_sampling"}, {"score": 0.004648609688625397, "phrase": "efficient_margin-based_algorithms"}, {"score": 0.004461761855323898, "phrase": "binary_classification_tasks"}, {"score": 0.004358368398332723, "phrase": "real-world_textual_data"}, {"score": 0.004134372859330047, "phrase": "popular_and_similarly_efficient_competitors"}, {"score": 0.0040385352882334235, "phrase": "so-called_mammen-tsybakov_low_noise_condition"}, {"score": 0.0038309141193444015, "phrase": "linear_label_noise"}, {"score": 0.003676811616723809, "phrase": "convergence_rate"}, {"score": 0.0035496495489801667, "phrase": "weaker_adaptive_variant"}, {"score": 0.003367077034382727, "phrase": "logarithmic_factors"}, {"score": 0.0033083236588614174, "phrase": "average_risk"}, {"score": 0.0032505921489759224, "phrase": "adaptive_sampler"}, {"score": 0.002941913249957503, "phrase": "queried_labels"}, {"score": 0.0027579362569638945, "phrase": "low_noise_condition"}, {"score": 0.0024813081174066653, "phrase": "fully_supervised_version"}, {"score": 0.0024379730432321656, "phrase": "base_selective_sampler"}, {"score": 0.0021049977753042253, "phrase": "fully-supervised_rates"}], "paper_keywords": ["Active learning", " Selective sampling", " Adaptive sampling", " Linear classification", " Low noise"], "paper_abstract": "We introduce efficient margin-based algorithms for selective sampling and filtering in binary classification tasks. Experiments on real-world textual data reveal that our algorithms perform significantly better than popular and similarly efficient competitors. Using the so-called Mammen-Tsybakov low noise condition to parametrize the instance distribution, and assuming linear label noise, we show bounds on the convergence rate to the Bayes risk of a weaker adaptive variant of our selective sampler. Our analysis reveals that, excluding logarithmic factors, the average risk of this adaptive sampler converges to the Bayes risk at rate N (-(1+alpha)(2+alpha)/2(3+alpha)) where N denotes the number of queried labels, and alpha > 0 is the exponent in the low noise condition. For all this convergence rate is asymptotically faster than the rate N (-(1+alpha)/(2+alpha)) achieved by the fully supervised version of the base selective sampler, which queries all labels. Moreover, for alpha -> a (hard margin condition) the gap between the semi- and fully-supervised rates becomes exponential.", "paper_title": "Learning noisy linear classifiers via adaptive and selective sampling", "paper_id": "WOS:000288758900003"}