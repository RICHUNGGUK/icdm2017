{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "jordan_network"}, {"score": 0.03221043383127256, "phrase": "hidden_layer_neurons"}, {"score": 0.015547084727554788, "phrase": "recurrent_constrained_learning"}, {"score": 0.01055189012941174, "phrase": "rijnrcl"}, {"score": 0.009495342481813215, "phrase": "multilayered_rnns"}, {"score": 0.004528156929805637, "phrase": "robust_initialization"}, {"score": 0.004187614134377688, "phrase": "multilayered_recurrent_neural_networks"}, {"score": 0.004049590638590415, "phrase": "novel_algorithm"}, {"score": 0.0039380388343479384, "phrase": "constrained_learning_concept"}, {"score": 0.003808209866576388, "phrase": "recurrent_sensitivity"}, {"score": 0.0037658881407605445, "phrase": "weight_convergence_analysis"}, {"score": 0.003367611042980526, "phrase": "classical_techniques"}, {"score": 0.003311604528903634, "phrase": "adaptive_learning_rate"}, {"score": 0.003256526411907174, "phrase": "adaptive_dead_zone"}, {"score": 0.003166751241099304, "phrase": "recurrent_constrained_parameter_matrix"}, {"score": 0.0030967107893195246, "phrase": "excessive_contributions"}, {"score": 0.0029945352738604742, "phrase": "weight_convergence"}, {"score": 0.0027690086059100495, "phrase": "good_response"}, {"score": 0.0026330549554033876, "phrase": "dominant_role"}, {"score": 0.0025892330566358503, "phrase": "local_minima"}, {"score": 0.0025178072071268534, "phrase": "new_rijnrcl_algorithm"}, {"score": 0.002475898613632249, "phrase": "twin_problems"}, {"score": 0.00244834685428352, "phrase": "weight_initialization"}, {"score": 0.002341164727388944, "phrase": "novel_recurrent_sensitivity_ratio_analysis"}, {"score": 0.002276566401216226, "phrase": "detailed_steps"}, {"score": 0.0021049977753042253, "phrase": "superior_generalization_performance"}], "paper_keywords": ["Generalization", " real-time rucurrent learning", " recurrent sensitivity analysis", " robust initialization", " time series prediction", " weight convergence proof"], "paper_abstract": "In this paper, we propose a robust initialization of a Jordan network with a recurrent constrained learning (RIJN-RCL) algorithm for multilayered recurrent neural networks (RNNs). This novel algorithm is based on the constrained learning concept of the Jordan network with a recurrent sensitivity and weight convergence analysis, which is used to obtain a tradeoff between the training and testing errors. In addition to using classical techniques for the adaptive learning rate and the adaptive dead zone, RIJNRCL employs a recurrent constrained parameter matrix to switch off excessive contributions from the hidden layer neurons based on weight convergence and stability conditions of the multilayered RNNs. It is well known that a good response from the hidden layer neurons and proper initialization play a dominant role in avoiding local minima in multilayered RNNs. The new RIJNRCL algorithm solves the twin problems of weight initialization and selection of the hidden layer neurons via a novel recurrent sensitivity ratio analysis. We provide the detailed steps for using RIJNRCL in a few benchmark time-series prediction problems and show that the proposed algorithm achieves superior generalization performance.", "paper_title": "Robust Initialization of a Jordan Network with Recurrent Constrained Learning", "paper_id": "WOS:000299082900028"}