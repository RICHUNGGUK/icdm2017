{"auto_keywords": [{"score": 0.031862565675836, "phrase": "eschd"}, {"score": 0.010612387000973441, "phrase": "hybrid_dissimilarity"}, {"score": 0.010518570728861783, "phrase": "soft_subspace_clustering"}, {"score": 0.00920597029620632, "phrase": "euclidean_distance"}, {"score": 0.008805568703153894, "phrase": "feature_space"}, {"score": 0.004645959941491617, "phrase": "research_efforts"}, {"score": 0.004462892860919036, "phrase": "high-dimensional_clustering"}, {"score": 0.004081385289516914, "phrase": "soft_subspace_clustering_algorithms"}, {"score": 0.003666217992454448, "phrase": "feature_space_transformations"}, {"score": 0.003601235776892337, "phrase": "enhanced_soft_subspace"}, {"score": 0.0035374012552316573, "phrase": "hybrid_dissimilarity_measure"}, {"score": 0.0034437574674180365, "phrase": "proposed_hybrid_dissimilarity_measure"}, {"score": 0.0033079039414389833, "phrase": "cosine_dissimilarity"}, {"score": 0.002971182100871581, "phrase": "new_optimization_objective_function"}, {"score": 0.002778317177178732, "phrase": "weighted_entropy"}, {"score": 0.002729029534060092, "phrase": "new_objective_function"}, {"score": 0.0026806139086152365, "phrase": "new_updating_rules"}, {"score": 0.0026095920086044145, "phrase": "clustering_procedure"}, {"score": 0.002574788013427524, "phrase": "proved_promising_convergence_property"}, {"score": 0.0024842242921857705, "phrase": "eschd._experimental_results"}, {"score": 0.002354302193867031, "phrase": "i.e._k-means"}, {"score": 0.002312539951392187, "phrase": "lac"}, {"score": 0.002291905937251069, "phrase": "ewkm."}], "paper_keywords": ["Soft subspace clustering", " dissimilarity measure", " feature weighting", " high dimensional clustering"], "paper_abstract": "Soft subspace clustering has attracted a lot of research efforts, due to the emergence of applications in high-dimensional clustering. To the best of our knowledge, most existing soft subspace clustering algorithms assign weights to features relying on Euclidean distance. Such schemes only allow soft subspace clustering algorithms to extend or shrink feature space with respect to feature weights. In this paper, we aim to break this limitation and facilitate feature space transformations directed by an enhanced soft subspace clustering algorithm through hybrid dissimilarity measure (ESCHD). The proposed hybrid dissimilarity measure assign weights to features based on Euclidean distance and Cosine dissimilarity, which can extend, shrink and rotate feature space so that the performance of soft subspace clustering can be improved. In ESCHD, a new optimization objective function is designed, based on the goal of minimizing the hybrid dissimilarity and maximizing the weighted entropy. Using the new objective function, we derive new updating rules for the iterations in the clustering procedure. The proved promising convergence property of ESCHD also ensures the performance of ESCHD. Experimental results on both synthetic and UCI datasets demonstrate that ESCHD is superior to the four existing clustering algorithms, i.e. K-means, W-k-means, LAC and EWKM. In addition, ESCHD is more robust to handle the problems with noise and missing data than the four algorithms.", "paper_title": "Enhanced soft subspace clustering through hybrid dissimilarity", "paper_id": "WOS:000364407900013"}