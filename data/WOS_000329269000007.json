{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multi-core_cpus"}, {"score": 0.039445346264425536, "phrase": "opencl_code"}, {"score": 0.0047057084749407485, "phrase": "cross-platform_parallel_programming_model"}, {"score": 0.004581373625744533, "phrase": "gpu_programming"}, {"score": 0.004494570360108983, "phrase": "large_amount"}, {"score": 0.004068907083697369, "phrase": "parallel_applications"}, {"score": 0.0035722806911457545, "phrase": "necessary_step"}, {"score": 0.003531474887552889, "phrase": "interplatform_performance_portability"}, {"score": 0.003504534343683285, "phrase": "opencl."}, {"score": 0.003438058620195309, "phrase": "iterative_tuning"}, {"score": 0.0033215617190376565, "phrase": "reasonable_openmp_implementation"}, {"score": 0.0032836104153151973, "phrase": "performance_reference"}, {"score": 0.002860546817196148, "phrase": "five_different_applications"}, {"score": 0.002806256131739047, "phrase": "rodinia_benchmark_suite"}, {"score": 0.0027635643494432365, "phrase": "equivalent_openmp"}, {"score": 0.0027424619603260837, "phrase": "opencl_implementations"}, {"score": 0.0026596472456182707, "phrase": "thorough_evaluations"}, {"score": 0.0026393362602995254, "phrase": "different_datasets"}, {"score": 0.0025596276616952516, "phrase": "opencl_performance"}, {"score": 0.002416589459425828, "phrase": "fine-grained_parallelism"}, {"score": 0.0023435920614377306, "phrase": "immature_opencl_compilers"}, {"score": 0.0022554455487460057, "phrase": "openmp"}, {"score": 0.0022296340517805125, "phrase": "better_performance"}, {"score": 0.0021705796895687864, "phrase": "good_option"}, {"score": 0.0021539955106204337, "phrase": "programming_multi-core_cpus"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["OpenCL", " Multi-core CPUs", " Performance evaluation", " Performance tuning", " OpenMP"], "paper_abstract": "Although designed as a cross-platform parallel programming model, OpenCL remains mainly used for GPU programming. Nevertheless, a large amount of applications are parallelized, implemented, and eventually optimized in OpenCL. Thus, in this paper, we focus on the potential that these parallel applications have to exploit the performance of multi-core CPUs. Specifically, we analyze the method to systematically reuse and adapt the OpenCL code from CPUs to CPUs. We claim that this work is a necessary step for enabling interplatform performance portability in OpenCL. Our method is based on iterative tuning: given an application, we choose a reasonable OpenMP implementation as a performance reference and we systematically tune the OpenCL code to reach or exceed this threshold. In the process, we identify the factors that significantly impact the performance of the OpenCL code. We apply this method for five different applications, selected from the Rodinia benchmark suite (which provides equivalent OpenMP and OpenCL implementations), and make a series of thorough evaluations with different datasets on three different multi-core platforms. We find that the OpenCL performance on CPUs is affected by typical, hard-coded GPU optimizations (unsuitable for multi-core CPUs), by the fine-grained parallelism of the model, and by the immature OpenCL compilers. Systematically fixing these issues allowed OpenCL to achieve OpenMP's or better performance, proving it can be a good option for programming multi-core CPUs. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "An application-centric evaluation of OpenCL on multi-core CPUs", "paper_id": "WOS:000329269000007"}