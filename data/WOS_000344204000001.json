{"auto_keywords": [{"score": 0.04746186469181639, "phrase": "feature_selection"}, {"score": 0.023045149268117713, "phrase": "matrix_factorization"}, {"score": 0.009956330751166222, "phrase": "feature_extraction"}, {"score": 0.00481495049065317, "phrase": "unsupervised_feature_selection"}, {"score": 0.004738325072560588, "phrase": "dimensionality_reduction"}, {"score": 0.004681653551412616, "phrase": "important_and_challenging_task"}, {"score": 0.00458869631642791, "phrase": "data_mining"}, {"score": 0.004101088357088207, "phrase": "class_labels"}, {"score": 0.0037999551521987426, "phrase": "new_unsupervised_feature_selection_criterion"}, {"score": 0.0037095162593960727, "phrase": "subspace_learning"}, {"score": 0.003606711217432998, "phrase": "matrix_factorization_problem"}, {"score": 0.003301733505307784, "phrase": "unified_framework"}, {"score": 0.0031337465656628132, "phrase": "iterative_update_algorithm"}, {"score": 0.003022465992278672, "phrase": "efficient_technique"}, {"score": 0.002974281028713475, "phrase": "high-dimensional_data"}, {"score": 0.002891793055307152, "phrase": "numeric_data"}, {"score": 0.0027667539384678814, "phrase": "discretization_process"}, {"score": 0.002711714675297321, "phrase": "new_criterion"}, {"score": 0.0026792166577171476, "phrase": "sound_foundation"}, {"score": 0.0026471070725516467, "phrase": "kernel_tricks"}, {"score": 0.0025224615049571427, "phrase": "kernel_methods"}, {"score": 0.0022904619303011097, "phrase": "clustering_results"}, {"score": 0.0022630008787375435, "phrase": "proposed_two_algorithms"}, {"score": 0.0022358683283304533, "phrase": "better_performance"}, {"score": 0.002191366809047192, "phrase": "almost_all_datasets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Machine learning", " Feature selection", " Unsupervised learning", " Matrix factorization", " Subspace distance", " Kernel method"], "paper_abstract": "Dimensionality reduction is an important and challenging task in machine learning and data mining. Feature selection and feature extraction are two commonly used techniques for decreasing dimensionality of the data and increasing efficiency of learning algorithms. Specifically, feature selection realized in the absence of class labels, namely unsupervised feature selection, is challenging and interesting. In this paper, we propose a new unsupervised feature selection criterion developed from the viewpoint of subspace learning, which is treated as a matrix factorization problem. The advantages of this work are four-fold. First, dwelling on the technique of matrix factorization, a unified framework is established for feature selection, feature extraction and clustering. Second, an iterative update algorithm is provided via matrix factorization, which is an efficient technique to deal with high-dimensional data. Third, an effective method for feature selection with numeric data is put forward, instead of drawing support from the discretization process. Fourth, this new criterion provides a sound foundation for embedding kernel tricks into feature selection. With this regard, an algorithm based on kernel methods is also proposed. The algorithms are compared with four state-of-the-art feature selection methods using six publicly available datasets. Experimental results demonstrate that in terms of clustering results, the proposed two algorithms come with better performance than the others for almost all datasets we experimented with here. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Subspace learning for unsupervised feature selection via matrix factorization", "paper_id": "WOS:000344204000001"}