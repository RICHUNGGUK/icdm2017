{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cross-validation_technique"}, {"score": 0.04197982453634864, "phrase": "dynamic_ensemble_pruning_method"}, {"score": 0.025372286719485583, "phrase": "initial_ensemble"}, {"score": 0.0045855181084455444, "phrase": "predictive_accuracy"}, {"score": 0.004534136929881113, "phrase": "ensemble_system"}, {"score": 0.00443308765153716, "phrase": "new_competitive_technique"}, {"score": 0.004399905285945236, "phrase": "ensemble_pruning"}, {"score": 0.004350691792585791, "phrase": "cross-validation"}, {"score": 0.004174451361776433, "phrase": "neural_computing_models"}, {"score": 0.0039456466422100045, "phrase": "practical_applications"}, {"score": 0.003886770770835211, "phrase": "proposed_cepcv_method"}, {"score": 0.003729335910488339, "phrase": "line_ensemble_pruning"}, {"score": 0.003687512797620773, "phrase": "full_advantage"}, {"score": 0.003659890662429693, "phrase": "potentially_valuable_information"}, {"score": 0.0033947594838968326, "phrase": "selective_competitions"}, {"score": 0.0033315236647127734, "phrase": "pruned_ensemble"}, {"score": 0.0032941471167321408, "phrase": "\"strongest\"_generalization_capability"}, {"score": 0.0030097975788892896, "phrase": "local_minima_problem"}, {"score": 0.0027811441123794427, "phrase": "ensemble_learning"}, {"score": 0.0027499252661633525, "phrase": "comparative_experiments"}, {"score": 0.0026583500966092044, "phrase": "cepcv"}, {"score": 0.002560162916190598, "phrase": "hill_climbing"}, {"score": 0.0024105326885334962, "phrase": "best_single_model"}, {"score": 0.002330233210731173, "phrase": "pruning_set"}, {"score": 0.0022610992177831643, "phrase": "network_members"}, {"score": 0.0022105951175680856, "phrase": "ten_benchmark_classification_tasks"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural networks ensemble", " Ensemble pruning", " Concept-drifting data", " Cross-validation", " Competitive learning"], "paper_abstract": "Ensemble pruning is crucial for the consideration of both efficiency and predictive accuracy of an ensemble system. This paper proposes a new Competitive technique for Ensemble Pruning based on Cross-Validation (CEPCV). The data to be learnt by neural computing models are mostly drifting with time and environment, therefore a dynamic ensemble pruning method is indispensable for practical applications, while the proposed CEPCV method is just the kind of dynamic ensemble pruning method, which can realize on-line ensemble pruning and take full advantage of potentially valuable information. The algorithm naturally inherits the predominance of cross-validation technique, which implies that those networks regarded as winners in selective competitions and chosen into the pruned ensemble have the \"strongest\" generalization capability. It is essentially based on the strategy of \"divide and rule, collect the wisdom\", and might alleviate the local minima problem of many conventional ensemble pruning approaches only at the cost of a little greater computational cost, which is acceptable to most applications of ensemble learning. The comparative experiments among the four ensemble pruning algorithms, including: CEPCV and the state-of-the-art Directed Hill Climbing Ensemble Pruning (DHCEP) algorithm and two baseline methods, i.e. BSM, which chooses the Best Single Model in the initial ensemble based on their performances on the pruning set, and ALL, which reserves all network members of the initial ensemble, on ten benchmark classification tasks, demonstrate the effectiveness and validity of CEPCV. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "A competitive ensemble pruning approach based on cross-validation technique", "paper_id": "WOS:000313761800036"}