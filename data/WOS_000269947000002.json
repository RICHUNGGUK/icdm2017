{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayesian"}, {"score": 0.004670722261414736, "phrase": "optimal_online_sensing"}, {"score": 0.004553822674247401, "phrase": "visually_guided_mobile_robot"}, {"score": 0.004395040468458808, "phrase": "online_path_planning"}, {"score": 0.0043506951029334984, "phrase": "optimal_sensing"}, {"score": 0.004285010985001827, "phrase": "mobile_robot"}, {"score": 0.003911135156992, "phrase": "time_constraints"}, {"score": 0.0037555742428544096, "phrase": "utility_function"}, {"score": 0.0036614948691735105, "phrase": "belief_state"}, {"score": 0.003587924960158317, "phrase": "finite_horizon_planning_problem"}, {"score": 0.002943502888225748, "phrase": "stochastic_planning"}, {"score": 0.0029137604783116065, "phrase": "reinforcement_learning"}, {"score": 0.002797757576619329, "phrase": "extremely_complex_problem"}, {"score": 0.002727606388863445, "phrase": "bayesian_optimization_method"}, {"score": 0.002592523232809191, "phrase": "unknown_parts"}, {"score": 0.0025533148412439166, "phrase": "policy_space"}, {"score": 0.0024516267507109753, "phrase": "current_best_solution"}, {"score": 0.0023539789234610763, "phrase": "visually-guide_mobile_robot"}, {"score": 0.00217017119491777, "phrase": "sequential_experimental_design"}, {"score": 0.002148226022654619, "phrase": "dynamic_sensing"}, {"score": 0.0021049977753042253, "phrase": "mobile_sensors"}], "paper_keywords": ["Bayesian optimization", " Online path planning", " Sequential experimental design", " Attention and gaze planning", " Active vision", " Dynamic sensor networks", " Active learning", " Policy search", " Active SLAM", " Model predictive control", " Reinforcement learning"], "paper_abstract": "We address the problem of online path planning for optimal sensing with a mobile robot. The objective of the robot is to learn the most about its pose and the environment given time constraints. We use a POMDP with a utility function that depends on the belief state to model the finite horizon planning problem. We replan as the robot progresses throughout the environment. The POMDP is high-dimensional, continuous, non-differentiable, nonlinear, non-Gaussian and must be solved in real-time. Most existing techniques for stochastic planning and reinforcement learning are therefore inapplicable. To solve this extremely complex problem, we propose a Bayesian optimization method that dynamically trades off exploration (minimizing uncertainty in unknown parts of the policy space) and exploitation (capitalizing on the current best solution). We demonstrate our approach with a visually-guide mobile robot. The solution proposed here is also applicable to other closely-related domains, including active vision, sequential experimental design, dynamic sensing and calibration with mobile sensors.", "paper_title": "A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot", "paper_id": "WOS:000269947000002"}