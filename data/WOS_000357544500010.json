{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cloud_workflow_applications"}, {"score": 0.04179959142162062, "phrase": "hadoop_mapreduce"}, {"score": 0.031023461450658384, "phrase": "cloudflow"}, {"score": 0.004784795480627637, "phrase": "modern_hpc_systems"}, {"score": 0.004521720473366683, "phrase": "real-time_processing_purpose"}, {"score": 0.004409468620826287, "phrase": "high-throughput_computing"}, {"score": 0.004381854246857709, "phrase": "htc"}, {"score": 0.004193220779864395, "phrase": "long-held_dream"}, {"score": 0.004050708996795156, "phrase": "data-aware_scheduling"}, {"score": 0.0038519872652579713, "phrase": "map_task"}, {"score": 0.003779996603264111, "phrase": "compute_node"}, {"score": 0.003744505496142733, "phrase": "corresponding_input_data_chunk"}, {"score": 0.0036171884608283047, "phrase": "one-map-to-one-reduce_framework"}, {"score": 0.0033966530720227796, "phrase": "built-in_support"}, {"score": 0.0033436458712965272, "phrase": "input_datasets"}, {"score": 0.0033018343529712876, "phrase": "multiple_applications"}, {"score": 0.0030713353414700295, "phrase": "scheduling_decisions"}, {"score": 0.0029761900030306483, "phrase": "modern_hpc_system"}, {"score": 0.0027771545344219594, "phrase": "mapreduce"}, {"score": 0.0024795356895447765, "phrase": "data_locality_purposes"}, {"score": 0.002440805709023898, "phrase": "user-defined_multiple_map-"}, {"score": 0.0023651468875880628, "phrase": "required_data-flow_logic"}, {"score": 0.002291827910789624, "phrase": "whole_scheduling_framework"}, {"score": 0.0022702776603245036, "phrase": "theoretical_analysis"}, {"score": 0.002234808936443945, "phrase": "experimental_evaluation"}, {"score": 0.0021998931213841567, "phrase": "execution_runtime_speedup"}, {"score": 0.0021655216339356693, "phrase": "traditional_mapreduce_implementation"}, {"score": 0.0021451564984201364, "phrase": "manageable_time_overhead"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Concurrency", " Data aware", " MapReduce", " HPC", " Programming model"], "paper_abstract": "Traditional High-Performance Computing (HPC) based big-data applications are usually constrained by having to move large amount of data to compute facilities for real-time processing purpose. Modern HPC systems, represented by High-Throughput Computing (HTC) and Many-Task Computing (MTC) platforms, on the other hand, intend to achieve the long-held dream of moving compute to data instead. This kind of data-aware scheduling, typically represented by Hadoop MapReduce, has been successfully implemented in its Map Phase, whereby each Map Task is sent out to the compute node where the corresponding input data chunk is located. However, Hadoop MapReduce limits itself to a one-map-to-one-reduce framework, leading to difficulties for handling complex logics, such as pipelines or workflows. Meanwhile, it lacks built-in support and optimization when the input datasets are shared among multiple applications and/or jobs. The performance can be improved significantly when the knowledge of the shared and frequently accessed data is taken into scheduling decisions. To enhance the capability of managing workflow in modern HPC system, this paper presents CloudFlow, a Hadoop MapReduce based programming model for cloud workflow applications. CloudFlow is built on top of MapReduce, which is proposed not only being data aware, but also shared-data aware. It identifies the most frequently shared data, from both task-level and job-level, replicates them to each compute node for data locality purposes. It also supports user-defined multiple Map- and Reduce functions, allowing users to orchestrate the required data-flow logic. Mathematically, we prove the correctness of the whole scheduling framework by performing theoretical analysis. Further more, experimental evaluation also shows that the execution runtime speedup exceeds 4X compared to traditional MapReduce implementation with a manageable time overhead. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "CloudFlow: A data-aware programming model for cloud workflow applications on modern HPC systems", "paper_id": "WOS:000357544500010"}