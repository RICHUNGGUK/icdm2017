{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "data-sensitive_space"}, {"score": 0.040552623246943253, "phrase": "dimensionality_reduction"}, {"score": 0.004729856080441582, "phrase": "nearest_neighbor_search"}, {"score": 0.004646258518859209, "phrase": "wide_variety"}, {"score": 0.004404188306112211, "phrase": "search_methods"}, {"score": 0.0041995784967999985, "phrase": "recent_efforts"}, {"score": 0.004052358316137446, "phrase": "better_approximate_solutions"}, {"score": 0.0035341028694769036, "phrase": "exact_nearest_neighbors"}, {"score": 0.0034922880300145283, "phrase": "high_dimensions"}, {"score": 0.0033299015941662302, "phrase": "novel_high-performance_technique"}, {"score": 0.0032709661258673206, "phrase": "exact_k-nearest_neighbors"}, {"score": 0.0032130703855586685, "phrase": "low_and_high_dimensional_spaces"}, {"score": 0.0031003255054939524, "phrase": "new_method"}, {"score": 0.002991524908432975, "phrase": "explicit_data_clustering"}, {"score": 0.0028185852354729026, "phrase": "first_time"}, {"score": 0.0027358910467781155, "phrase": "effective_reduction"}, {"score": 0.0026874405408023956, "phrase": "search_space"}, {"score": 0.002639845784408318, "phrase": "secondary_storage"}, {"score": 0.002608583927848071, "phrase": "costly_euclidean_distance_calculations"}, {"score": 0.0025471636425006155, "phrase": "efficient_processing"}, {"score": 0.0025020469669826497, "phrase": "lightweight_memory-based_filter"}, {"score": 0.002428617044669043, "phrase": "sequential_scan"}, {"score": 0.0023856341743644692, "phrase": "va"}, {"score": 0.002329412930430366, "phrase": "high-dimensional_situations"}, {"score": 0.002234252028553481, "phrase": "dynamic_loading"}, {"score": 0.0021049977753042253, "phrase": "dynamic_datasets"}], "paper_keywords": [""], "paper_abstract": "Nearest neighbor search has a wide variety of applications. Unfortunately, the majority of search methods do not scale well with dimensionality. Recent efforts have been focused on finding better approximate solutions that improve the locality of data using dimensionality reduction. However, it is possible to preserve the locality of data and find exact nearest neighbors in high dimensions without dimensionality reduction. This paper introduces a novel high-performance technique to find exact k-nearest neighbors in both low and high dimensional spaces. It relies on a new method for data-sensitive space partitioning based on explicit data clustering, which is introduced in the paper for the first time. This organization supports effective reduction of the search space before accessing secondary storage. Costly Euclidean distance calculations are reduced through efficient processing of a lightweight memory-based filter. The algorithm outperforms sequential scan and the VA-File in high-dimensional situations. Moreover, the results with dynamic loading of data show that the technique works well on dynamic datasets as well.", "paper_title": "High-dimensional similarity search using data-sensitive space partitioning", "paper_id": "WOS:000241155300072"}