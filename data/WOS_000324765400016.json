{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "semantic_and_visual_context"}, {"score": 0.041469740173514034, "phrase": "video_annotation_performance"}, {"score": 0.04094912027989201, "phrase": "semantic_context"}, {"score": 0.03162903887570056, "phrase": "visual_similarities"}, {"score": 0.004773040022691547, "phrase": "effective_video_annotation"}, {"score": 0.004669845451682529, "phrase": "new_method"}, {"score": 0.004548939310612501, "phrase": "video_annotation"}, {"score": 0.00433531459712787, "phrase": "semantic_context_mining"}, {"score": 0.0042415434489566995, "phrase": "supervised_way"}, {"score": 0.004167982855788402, "phrase": "manual_concept_labels"}, {"score": 0.0037361664435836845, "phrase": "human_intention"}, {"score": 0.0035918768194006735, "phrase": "spatial_and_temporal_context"}, {"score": 0.0035141291835319682, "phrase": "conditional_random_fields"}, {"score": 0.0034835017385881385, "phrase": "different_structures"}, {"score": 0.003423042581201533, "phrase": "existing_methods"}, {"score": 0.0033197446406648626, "phrase": "concept_relationship"}, {"score": 0.003108735847242537, "phrase": "visual_context_mining"}, {"score": 0.0030414140732789186, "phrase": "semi-supervised_way"}, {"score": 0.002962543869356265, "phrase": "video_shots"}, {"score": 0.002898378776839434, "phrase": "natural_visual_property"}, {"score": 0.002443206010337041, "phrase": "semi-supervised_learning_approach"}, {"score": 0.0023180581939456686, "phrase": "reliable_shots"}, {"score": 0.0022777800735119405, "phrase": "similar_visual_features"}, {"score": 0.0022382002476510573, "phrase": "extensive_experimental_results"}, {"score": 0.002189690405423461, "phrase": "trecvid"}, {"score": 0.0021049977753042253, "phrase": "video_annotation_accuracy"}], "paper_keywords": ["Context mining", " semantic context", " video annotation", " visual context", " video retrieval"], "paper_abstract": "We propose a new method to refine the result of video annotation by exploiting the semantic and visual context of video. On one hand, semantic context mining is performed in a supervised way, using the manual concept labels of the training set. It is very useful for boosting video annotation performance, because semantic context is learned from labels given by people, indicating human intention. In this paper, we model the spatial and temporal context in video by using conditional random fields with different structures. Comparing with existing methods, our method could more accurately capture concept relationship in video and could more effectively improve the video annotation performance. On the other hand, visual context mining is performed in a semi-supervised way based on the visual similarities among video shots. It indicates the natural visual property of video, and could be considered as the compensation to semantic context, which generally could not be perfectly modeled. In this paper, we construct a graph based on the visual similarities among shots. Then a semi-supervised learning approach is adopt based on the graph to propagate probabilities of the reliable shots to others having similar visual features with them. Extensive experimental results on the widely used TRECVID datasets exhibit the effectiveness of our method for improving video annotation accuracy.", "paper_title": "Exploiting Semantic and Visual Context for Effective Video Annotation", "paper_id": "WOS:000324765400016"}