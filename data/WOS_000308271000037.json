{"auto_keywords": [{"score": 0.00441635700580562, "phrase": "large_scale_data_sets"}, {"score": 0.004345363360869553, "phrase": "distributed_learning_scenarios"}, {"score": 0.004252469672259375, "phrase": "nonstationary_learning_paradigms"}, {"score": 0.004028796143006953, "phrase": "machine_learning_literature"}, {"score": 0.003900257249255497, "phrase": "classical_methods"}, {"score": 0.0038168424095396205, "phrase": "data_set_shifts"}, {"score": 0.0035965032173588753, "phrase": "single_layer_neural_networks"}, {"score": 0.00355782527287734, "phrase": "nonlinear_output_functions"}, {"score": 0.0032806521851034766, "phrase": "regularization_term"}, {"score": 0.0032278559855285945, "phrase": "proposed_model"}, {"score": 0.003124790943886441, "phrase": "particular_initialization"}, {"score": 0.003074495313651187, "phrase": "devised_training_algorithm"}, {"score": 0.0029602552900111407, "phrase": "real_time_systems"}, {"score": 0.002865710140187219, "phrase": "noisy_conditions"}, {"score": 0.0027295084882654917, "phrase": "previous_models"}, {"score": 0.0027001292153471202, "phrase": "special_cases"}, {"score": 0.002599763359352916, "phrase": "block_component"}, {"score": 0.0025167026017217926, "phrase": "multilayer_perceptrons"}, {"score": 0.002234165013865669, "phrase": "proposed_algorithm"}, {"score": 0.0021863046959469863, "phrase": "standard_data_sets"}, {"score": 0.0021394674440767124, "phrase": "previous_approaches"}], "paper_keywords": ["Artificial neural networks", " Incremental learning", " Nonstationary learning", " Distributed learning"], "paper_abstract": "Incremental learning of neural networks has attracted much interest in recent years due to its wide applicability to large scale data sets and to distributed learning scenarios. Moreover, nonstationary learning paradigms have also emerged as a subarea of study in Machine Learning literature due to the problems of classical methods when dealing with data set shifts. In this paper we present an algorithm to train single layer neural networks with nonlinear output functions that take into account incremental, nonstationary and distributed learning scenarios. Moreover, it is demonstrated that introducing a regularization term into the proposed model is equivalent to choosing a particular initialization for the devised training algorithm, which may be suitable for real time systems that have to work under noisy conditions. In addition, the algorithm includes some previous models as special cases and can be used as a block component to build more complex models such as multilayer perceptrons, extending the capacity of these models to incremental, nonstationary and distributed learning paradigms. In this paper, the proposed algorithm is tested with standard data sets and compared with previous approaches, demonstrating its higher accuracy. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Nonlinear single layer neural network training algorithm for incremental, nonstationary and distributed learning scenarios", "paper_id": "WOS:000308271000037"}