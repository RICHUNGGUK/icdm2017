{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "rgb-d_object_recognition"}, {"score": 0.004711085699958695, "phrase": "rgb"}, {"score": 0.004533685795068016, "phrase": "rgb_and_depth_data"}, {"score": 0.004459857205489101, "phrase": "undifferentiated_manner"}, {"score": 0.0040192915527630995, "phrase": "different_characteristics"}, {"score": 0.003868142906199082, "phrase": "shared_relationship"}, {"score": 0.003622088701900472, "phrase": "general_cnn-based_multi-modal_learning_framework"}, {"score": 0.0035826282760433466, "phrase": "rgb-d"}, {"score": 0.0034478392374444177, "phrase": "deep_cnn_layers"}, {"score": 0.0032107911131811057, "phrase": "carefully_designed_multi-modal_layer"}, {"score": 0.0028305047329728254, "phrase": "complementary_relationship"}, {"score": 0.0026942333853202556, "phrase": "multi-modal_layer"}, {"score": 0.002550481822137976, "phrase": "cnn_layers"}, {"score": 0.0024951467586005094, "phrase": "multi-modal_feature_learning"}, {"score": 0.00236199235951202, "phrase": "experimental_results"}, {"score": 0.002260591903283384, "phrase": "general_multi-modal_learning"}, {"score": 0.0021049977753042253, "phrase": "rgb-d_data"}], "paper_keywords": ["Deep learning", " large-margin feature learning", " multi-modality", " RGB-D object recognition"], "paper_abstract": "Most existing feature learning-based methods for RGB-D object recognition either combine RGB and depth data in an undifferentiated manner from the outset, or learn features from color and depth separately, which do not adequately exploit different characteristics of the two modalities or utilize the shared relationship between the modalities. In this paper, we propose a general CNN-based multi-modal learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, which are then connected with a carefully designed multi-modal layer. This layer is designed to not only discover the most discriminative features for each modality, but is also able to harness the complementary relationship between the two modalities. The results of the multi-modal layer are back-propagated to update parameters of the CNN layers, and the multi-modal feature learning and the back-propagation are iteratively performed until convergence. Experimental results on two widely used RGB-D object datasets show that our method for general multi-modal learning achieves comparable performance to state-of-the-art methods specifically designed for RGB-D data.", "paper_title": "Large-Margin Multi-Modal Deep Learning for RGB-D Object Recognition", "paper_id": "WOS:000364102400003"}