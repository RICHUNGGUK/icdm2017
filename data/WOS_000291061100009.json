{"auto_keywords": [{"score": 0.04320038033945778, "phrase": "virtual_humans"}, {"score": 0.03529624542703246, "phrase": "facial_gestures"}, {"score": 0.00481495049065317, "phrase": "multimodal_virtual_humans-real_time_speech"}, {"score": 0.0046328364646343375, "phrase": "extensive_use"}, {"score": 0.0045820747375409435, "phrase": "different_computer_devices"}, {"score": 0.0045318666714939905, "phrase": "human-computer_interaction_design"}, {"score": 0.004408729661992651, "phrase": "user_centric_interfaces"}, {"score": 0.00428892403807229, "phrase": "different_modalities"}, {"score": 0.004172360444377281, "phrase": "everyday_communication"}, {"score": 0.003536346954940084, "phrase": "automatic_speech"}, {"score": 0.0033837135367113004, "phrase": "real_time_performance"}, {"score": 0.00325555555110395, "phrase": "head_movements"}, {"score": 0.0031845067058084583, "phrase": "eyebrow_gestures"}, {"score": 0.002947790697936243, "phrase": "prosodic_information"}, {"score": 0.002883439044844259, "phrase": "speech_signal"}, {"score": 0.0027741761880779535, "phrase": "hybrid_approach-hidden_markov_models"}, {"score": 0.0025537472239953807, "phrase": "application_prototype"}, {"score": 0.0024569466759289055, "phrase": "facial_gesturing"}, {"score": 0.00241658945942583, "phrase": "virtual_presenters"}, {"score": 0.0023900527169254744, "phrase": "subjective_evaluation"}, {"score": 0.0022994427535192492, "phrase": "synthesized_facial_movements"}, {"score": 0.002200077887697072, "phrase": "underlying_speech"}, {"score": 0.0021401625519713577, "phrase": "natural_behavior"}, {"score": 0.0021049977753042253, "phrase": "whole_face"}], "paper_keywords": ["Facial gestures", " Visual prosody", " Multimodal interfaces", " Facial animation", " Speech processing", " Human-computer interaction"], "paper_abstract": "Because of extensive use of different computer devices, human-computer interaction design nowadays moves towards creating user centric interfaces. It assumes incorporating different modalities that humans use in everyday communication. Virtual humans, who look and behave believably, fit perfectly in the concept of designing interfaces in more natural, effective, as well as social oriented way. In this paper we present a novel method for automatic speech driven facial gesturing for virtual humans capable of real time performance. Facial gestures included are various nods and head movements, blinks, eyebrow gestures and gaze. A mapping from speech to facial gestures is based on the prosodic information obtained from the speech signal. It is realized using a hybrid approach-Hidden Markov Models, rules and global statistics. Further, we test the method using an application prototype-a system for speech driven facial gesturing suitable for virtual presenters. Subjective evaluation of the system confirmed that the synthesized facial movements are consistent and time aligned with the underlying speech, and thus provide natural behavior of the whole face.", "paper_title": "On creating multimodal virtual humans-real time speech driven facial gesturing", "paper_id": "WOS:000291061100009"}