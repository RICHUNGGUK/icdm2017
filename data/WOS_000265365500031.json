{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multi-class_cost-sensitive_learning"}, {"score": 0.04717487490094824, "phrase": "cost-sensitive_learning"}, {"score": 0.004308630865024719, "phrase": "different_classes"}, {"score": 0.003802135020791801, "phrase": "data_space_expansion_technique"}, {"score": 0.0037236852537136547, "phrase": "abe_et_al"}, {"score": 0.003571591012750155, "phrase": "elkan's_reduction"}, {"score": 0.0034736499231079083, "phrase": "binary_classification_tasks"}, {"score": 0.003378385493203297, "phrase": "proposed_reduction_approach"}, {"score": 0.003308649946463136, "phrase": "cost-sensitive_learning_problem"}, {"score": 0.002980916123365584, "phrase": "cost_matrix"}, {"score": 0.0028590732866152118, "phrase": "new_weighting_mechanism"}, {"score": 0.0027806165169981622, "phrase": "reduced_standard_classification_problem"}, {"score": 0.0026118503041971976, "phrase": "empirical_loss"}, {"score": 0.0024704333220467393, "phrase": "new_distribution"}, {"score": 0.0023043558058824572, "phrase": "expanded_weighted_training"}, {"score": 0.0022567398334877847, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "existing_representative_approaches"}], "paper_keywords": ["Cost-sensitive learning", " Supervised learning", " Statistical learning theory", " Classification"], "paper_abstract": "in cost-sensitive learning, misclassification costs can vary for different classes. This paper investigates an approach reducing a multi-class cost-sensitive learning to a standard classification task based on the data space expansion technique developed by Abe et al., which coincides with Elkan's reduction with respect to binary classification tasks. Using this proposed reduction approach, a cost-sensitive learning problem can be solved by considering a standard 0/1 loss classification problem on a new distribution determined by the cost matrix. We also propose a new weighting mechanism to solve the reduced standard classification problem, based on a theorem stating that the empirical loss on independently identically distributed samples from the new distribution is essentially the same as the loss on the expanded weighted training set. Experimental results on several synthetic and benchmark datasets show that our weighting approach is more effective than existing representative approaches for cost-sensitive learning. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "A closed-form reduction of multi-class cost-sensitive learning to weighted multi-class learning", "paper_id": "WOS:000265365500031"}