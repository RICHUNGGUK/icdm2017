{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "distributed_training"}, {"score": 0.02277234663612143, "phrase": "neural_network_ensembles"}, {"score": 0.0045324756541157574, "phrase": "individual_entities"}, {"score": 0.004309725894724034, "phrase": "training_sets"}, {"score": 0.003935892934907777, "phrase": "serious_privacy_concerns"}, {"score": 0.0031845067058084583, "phrase": "privacy_protection"}, {"score": 0.002907992115184249, "phrase": "privacy-preserving_distributed_algorithm"}, {"score": 0.0027648671413954455, "phrase": "adaboost"}, {"score": 0.0021917966722180132, "phrase": "uci_repository"}, {"score": 0.0021049977753042253, "phrase": "algorithm's_effectiveness"}], "paper_keywords": ["Privacy", " Distributed training", " Neural network ensemble", " Adaptive boosting"], "paper_abstract": "Distributed training allows individual entities to benefit from the training sets owned by other entities. Nevertheless, distributed training also causes serious privacy concerns. Hence, it is highly important to protect privacy in distributed training. In this paper, we study the privacy protection in distributed training of neural network ensembles. We design a privacy-preserving distributed algorithm for training neural network ensembles using AdaBoost.M2. We also analyze the security and complexity of our algorithm. Furthermore, we perform experiments on two data sets of the UCI repository to verify the algorithm's effectiveness and efficiency.", "paper_title": "A privacy-preserving algorithm for distributed training of neural network ensembles", "paper_id": "WOS:000323413300023"}