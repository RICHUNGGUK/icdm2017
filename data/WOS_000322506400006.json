{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "dimension_independent_similarity_computation"}, {"score": 0.029795339227408884, "phrase": "jaccard_similarity"}, {"score": 0.004250302819248558, "phrase": "pairwise_similarities"}, {"score": 0.0036648265911110164, "phrase": "initial_cost"}, {"score": 0.0034162643819889054, "phrase": "subsequent_operations"}, {"score": 0.0029918131347435677, "phrase": "cosine"}, {"score": 0.002945366336055944, "phrase": "dice"}, {"score": 0.0029004936383346135, "phrase": "overlap"}, {"score": 0.0026402908398883832, "phrase": "improved_version"}, {"score": 0.0024610448251906453, "phrase": "mapreduce_framework"}, {"score": 0.0023300677028248776, "phrase": "large_scale_experiments"}, {"score": 0.0022407927161620855, "phrase": "social_networking_site"}, {"score": 0.00222354913705547, "phrase": "twitter"}], "paper_keywords": ["cosine", " Jaccard", " overlap", " dice", " similarity", " MapReduce", " dimension independent"], "paper_abstract": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com.", "paper_title": "Dimension Independent Similarity Computation", "paper_id": "WOS:000322506400006"}