{"auto_keywords": [{"score": 0.049178471674397164, "phrase": "loop_transformations"}, {"score": 0.010612387000973441, "phrase": "polyhedral_optimization_space"}, {"score": 0.010059433147773343, "phrase": "complex_sequences"}, {"score": 0.004784565134881178, "phrase": "high-level_program_optimizations"}, {"score": 0.004650175539595746, "phrase": "high_performance"}, {"score": 0.004620825188916713, "phrase": "multi-core_targets"}, {"score": 0.0043235627487378595, "phrase": "data_locality"}, {"score": 0.004282680502911952, "phrase": "polyhedral_compilation_framework"}, {"score": 0.004122964978180509, "phrase": "restructuring_compute-intensive_applications"}, {"score": 0.004071055784602253, "phrase": "perfectly_and_imperfectly_nested_loops"}, {"score": 0.004019797503157808, "phrase": "arbitrarily_complex_sequences"}, {"score": 0.003956627672153024, "phrase": "unified_mathematical_framework"}, {"score": 0.003857606273328462, "phrase": "expected_effectiveness"}, {"score": 0.0038090252359678336, "phrase": "loop_optimization_stage"}, {"score": 0.0037136840686010303, "phrase": "major_challenge"}, {"score": 0.0035864603175592854, "phrase": "polyhedral_frameworks"}, {"score": 0.0035300761057370745, "phrase": "good_performance"}, {"score": 0.003496670470065125, "phrase": "wide_range"}, {"score": 0.003474575242120264, "phrase": "numerical_applications"}, {"score": 0.0033768559933238665, "phrase": "simplistic_performance_models"}, {"score": 0.0030998304961483217, "phrase": "best_polyhedral_optimizations"}, {"score": 0.0030802351224657673, "phrase": "dedicated_machine_learning_models"}, {"score": 0.003022186855664124, "phrase": "target_machine"}, {"score": 0.002927853422094123, "phrase": "high-performance_optimizations"}, {"score": 0.0028364561005358377, "phrase": "good_complex_sequences"}, {"score": 0.002713260031399583, "phrase": "candidate_optimizations"}, {"score": 0.002696101728474703, "phrase": "static_cost_models"}, {"score": 0.0026368954199603206, "phrase": "specific_high-level_optimizations"}, {"score": 0.0024669449102126518, "phrase": "high-level_complex_optimization_sequence"}, {"score": 0.0023974961635593135, "phrase": "performance-counter_characterization"}, {"score": 0.0023747827579261483, "phrase": "original_program"}, {"score": 0.0022572213326336374, "phrase": "different_machine_learning_algorithms"}, {"score": 0.0022006365282969454, "phrase": "performance_improvements"}, {"score": 0.002186712881521657, "phrase": "productions_compilers"}, {"score": 0.0021049977753042253, "phrase": "program_variants"}], "paper_keywords": ["Loop transformation", " Polyhedral optimization", " Iterative compilation", " Machine learning", " Performance counters"], "paper_abstract": "High-level program optimizations, such as loop transformations, are critical for high performance on multi-core targets. However, complex sequences of loop transformations are often required to expose parallelism (both coarse-grain and fine-grain) and improve data locality. The polyhedral compilation framework has proved to be very effective at representing these complex sequences and restructuring compute-intensive applications, seamlessly handling perfectly and imperfectly nested loops. It models arbitrarily complex sequences of loop transformations in a unified mathematical framework, dramatically increasing the expressiveness (and expected effectiveness) of the loop optimization stage. Nevertheless identifying the most effective loop transformations remains a major challenge: current state-of-the-art heuristics in polyhedral frameworks simply fail to expose good performance over a wide range of numerical applications. Their lack of effectiveness is mainly due to simplistic performance models that do not reflect the complexity today's processors (CPU, cache behavior, etc.). We address the problem of selecting the best polyhedral optimizations with dedicated machine learning models, trained specifically on the target machine. We show that these models can quickly select high-performance optimizations with very limited iterative search. We decouple the problem of selecting good complex sequences of optimizations in two stages: (1) we narrow the set of candidate optimizations using static cost models to select the loop transformations that implement specific high-level optimizations (e.g., tiling, parallelism, etc.); (2) we predict the performance of each high-level complex optimization sequence with trained models that take as input a performance-counter characterization of the original program. Our end-to-end framework is validated using numerous benchmarks on two modern multi-core platforms. We investigate a variety of different machine learning algorithms and hardware counters, and we obtain performance improvements over productions compilers ranging on average from to , by running not more than program variants from a polyhedral optimization space.", "paper_title": "Predictive Modeling in a Polyhedral Optimization Space", "paper_id": "WOS:000320285000004"}