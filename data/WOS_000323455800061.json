{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "feature_selection"}, {"score": 0.0041642891310893, "phrase": "good_empirical_performance"}, {"score": 0.0039822874870125095, "phrase": "provable_theoretical_behavior"}, {"score": 0.0031845067058084583, "phrase": "constant_probability"}, {"score": 0.003011328068286321, "phrase": "first_deterministic_feature_selection_algorithm"}, {"score": 0.002784540221128404, "phrase": "relative_error_guarantees"}, {"score": 0.0025178072071268534, "phrase": "deterministic_method"}, {"score": 0.0023021895404322767, "phrase": "structural_result"}, {"score": 0.0021049977753042253, "phrase": "dimensionality_reduction"}], "paper_keywords": ["Clustering methods", " dimensionality reduction", " unsupervised learning"], "paper_abstract": "We study feature selection for k-means clustering. Although the literature contains many methods with good empirical performance, algorithms with provable theoretical behavior have only recently been developed. Unfortunately, these algorithms are randomized and fail with, say, a constant probability. We present the first deterministic feature selection algorithm for k-means clustering with relative error guarantees. At the heart of our algorithm lies a deterministic method for decompositions of the identity and a structural result which quantifies some of the tradeoffs in dimensionality reduction.", "paper_title": "Deterministic Feature Selection for k-Means Clustering", "paper_id": "WOS:000323455800061"}