{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "language_workbenches"}, {"score": 0.03166078255783259, "phrase": "third_lwc."}, {"score": 0.004777569299319633, "phrase": "existing_results"}, {"score": 0.004184526032558792, "phrase": "computer_languages"}, {"score": 0.004087756756529841, "phrase": "annual_language_workbench_challenge"}, {"score": 0.0028771413440398614, "phrase": "ten_approaches"}, {"score": 0.0027240799841720957, "phrase": "generic_feature_model"}, {"score": 0.002366763442497185, "phrase": "existing_lwcs"}, {"score": 0.0022407927161620855, "phrase": "benchmark_problems"}, {"score": 0.002206045784326466, "phrase": "future_lwcs"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Language workbenches", " Domain-specific languages", " Questionnaire language", " Survey", " Benchmarks"], "paper_abstract": "Language workbenches are environments for simplifying the creation and use of computer languages. The annual Language Workbench Challenge (LWC) was launched in 2011 to allow the many academic and industrial researchers in this area an opportunity to quantitatively and qualitatively compare their approaches. We first describe all four LWCs to date, before focussing on the approaches used, and results generated, during the third LWC. We give various empirical data for ten approaches from the third LWC. We present a generic feature model within which the approaches can be understood and contrasted. Finally, based on our experiences of the existing LWCs, we propose a number of benchmark problems for future LWCs. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Evaluating and comparing language workbenches Existing results and benchmarks for the future", "paper_id": "WOS:000366778300003"}