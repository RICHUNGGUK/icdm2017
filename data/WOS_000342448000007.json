{"auto_keywords": [{"score": 0.04814996942162455, "phrase": "joint_positions"}, {"score": 0.00481495049065317, "phrase": "sign_language_videos"}, {"score": 0.00471700544744796, "phrase": "fully_automatic_arm"}, {"score": 0.004678384952779725, "phrase": "hand_tracker"}, {"score": 0.004583205112737672, "phrase": "continuous_sign_language_video_sequences"}, {"score": 0.0041184843463633386, "phrase": "overlaid_signer"}, {"score": 0.004018087343963485, "phrase": "background_tv_broadcast"}, {"score": 0.00388800655625083, "phrase": "layered_model"}, {"score": 0.0035369291417564606, "phrase": "random_forest_regressor"}, {"score": 0.003464890914694523, "phrase": "colour_model"}, {"score": 0.0033388871136347704, "phrase": "random_forest"}, {"score": 0.0029631248341951064, "phrase": "predicted_joint_positions"}, {"score": 0.0027514186391258263, "phrase": "imaging_conditions"}, {"score": 0.0027064605585684696, "phrase": "different_signers"}, {"score": 0.002651291854234327, "phrase": "state-of-the-art_long_term_tracker"}, {"score": 0.002629539710839523, "phrase": "buehler"}, {"score": 0.002575934917174136, "phrase": "international_journal_of_computer_vision"}, {"score": 0.00245168871716047, "phrase": "manual_annotation"}, {"score": 0.0023721974375323267, "phrase": "automatic_initialisation"}, {"score": 0.002276439636865258, "phrase": "superior_joint_localisation_results"}, {"score": 0.002220846338732873, "phrase": "pose_estimation_method"}, {"score": 0.002202943840916972, "phrase": "yang"}, {"score": 0.0021845449800440477, "phrase": "ramanan"}, {"score": 0.0021670173236817488, "phrase": "proceedings"}, {"score": 0.002139986159978777, "phrase": "ieee_conference"}, {"score": 0.0021224200236681498, "phrase": "computer_vision"}, {"score": 0.0021049977753042253, "phrase": "pattern_recognition"}], "paper_keywords": ["Sign language", " Human pose estimation", " Co-segmentation", " Random forest"], "paper_abstract": "We present a fully automatic arm and hand tracker that detects joint positions over continuous sign language video sequences of more than an hour in length. To achieve this, we make contributions in four areas: (i) we show that the overlaid signer can be separated from the background TV broadcast using co-segmentation over all frames with a layered model; (ii) we show that joint positions (shoulders, elbows, wrists) can be predicted per-frame using a random forest regressor given only this segmentation and a colour model; (iii) we show that the random forest can be trained from an existing semi-automatic, but computationally expensive, tracker; and, (iv) introduce an evaluator to assess whether the predicted joint positions are correct for each frame. The method is applied to 20 signing footage videos with changing background, challenging imaging conditions, and for different signers. Our framework outperforms the state-of-the-art long term tracker by Buehler et al. (International Journal of Computer Vision 95:180-197, 2011), does not require the manual annotation of that work, and, after automatic initialisation, performs tracking in real-time. We also achieve superior joint localisation results to those obtained using the pose estimation method of Yang and Ramanan (Proceedings of the IEEE conference on computer vision and pattern recognition, 2011).", "paper_title": "Automatic and Efficient Human Pose Estimation for Sign Language Videos", "paper_id": "WOS:000342448000007"}