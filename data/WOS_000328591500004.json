{"auto_keywords": [{"score": 0.04364011428330573, "phrase": "user_intentions"}, {"score": 0.004597622380240611, "phrase": "textual_query"}, {"score": 0.004550669123303854, "phrase": "visual_images"}, {"score": 0.004412654209742722, "phrase": "web_image_search"}, {"score": 0.0043452175611323335, "phrase": "returned_results"}, {"score": 0.0040438338011467845, "phrase": "heterogeneous_concepts"}, {"score": 0.0035382855372646164, "phrase": "social_data"}, {"score": 0.0035021127041237887, "phrase": "social_media_platforms"}, {"score": 0.0034485432592912917, "phrase": "image_search_engines"}, {"score": 0.0033096502887394233, "phrase": "returned_images"}, {"score": 0.003143849342926373, "phrase": "social_data_sparseness"}, {"score": 0.0030640804704319255, "phrase": "social_relevance"}, {"score": 0.0030327408069772293, "phrase": "visual_relevance"}, {"score": 0.002971016979295967, "phrase": "complex_social_and_visual_factors"}, {"score": 0.002895620643297197, "phrase": "community-specific_social-visual_ranking"}, {"score": 0.0027789349951704177, "phrase": "web_images"}, {"score": 0.0027363971333961967, "phrase": "current_image_search_engines"}, {"score": 0.0026945086500597304, "phrase": "svr_algorithm"}, {"score": 0.0026397235390905006, "phrase": "pagerank"}, {"score": 0.0025992391074270097, "phrase": "hybrid_image_link_graph"}, {"score": 0.002494466951433885, "phrase": "image_social-link_graph"}, {"score": 0.002456272788591051, "phrase": "image_visual-link_graph"}, {"score": 0.0024062532032402533, "phrase": "extensive_experiments"}, {"score": 0.002309242063033298, "phrase": "visual_factors"}, {"score": 0.0022856052476424344, "phrase": "social_factors"}, {"score": 0.0022161333611134806, "phrase": "social-visual_ranking_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Social image search", " Image reranking", " Social relevance"], "paper_abstract": "Many research have been focusing on how to match the textual query with visual images and their surrounding texts or tags for Web image search. The returned results are often unsatisfactory due to their deviation from user intentions, particularly for queries with heterogeneous concepts (such as \"apple\", \"jaguar\") or general (non-specific) concepts (such as \"landscape\", \"hotel\"). In this paper, we exploit social data from social media platforms to assist image search engines, aiming to improve the relevance between returned images and user intentions (i.e., social relevance). Facing the challenges of social data sparseness, the tradeoff between social relevance and visual relevance, and the complex social and visual factors, we propose a community-specific Social-Visual Ranking (SVR) algorithm to rerank the Web images returned by current image search engines. The SVR algorithm is implemented by PageRank over a hybrid image link graph, which is the combination of an image social-link graph and an image visual-link graph. By conducting extensive experiments, we demonstrated the importance of both visual factors and social factors, and the advantages of social-visual ranking algorithm for Web image search. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Social-oriented visual image search", "paper_id": "WOS:000328591500004"}