{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "divergence_estimation"}, {"score": 0.004716520738331953, "phrase": "multidimensional_densities_via_k-nearest-neighbor_distances"}, {"score": 0.004209893377338457, "phrase": "multidimensional_continuous_densities"}, {"score": 0.0033190180867511605, "phrase": "new_estimator"}, {"score": 0.0028127163745564777, "phrase": "high-dimensional_data"}, {"score": 0.002726740692380339, "phrase": "k-nn_approach"}, {"score": 0.002643386043540555, "phrase": "faster_convergence"}, {"score": 0.0025892330566358503, "phrase": "previous_algorithms"}, {"score": 0.00226322835801988, "phrase": "k-nn_method"}, {"score": 0.0021049977753042253, "phrase": "adaptive_choice"}], "paper_keywords": ["Divergence", " information measure", " Kullback-Leibler", " nearest-neighbor", " partition", " random vector", " universal estimation"], "paper_abstract": "A new universal estimator of divergence is presented for multidimensional continuous densities based on k-nearest-neighbor (k-NN) distances. Assuming independent and identically distributed (i.i.d.) samples, the new estimator is proved to be asymptotically unbiased and mean-square consistent. In experiments with high-dimensional data, the k-NN approach generally exhibits faster convergence than previous algorithms. It is also shown that the speed of convergence of the k-NN method can be further improved by an adaptive choice of k.", "paper_title": "Divergence Estimation for Multidimensional Densities Via k-Nearest-Neighbor Distances", "paper_id": "WOS:000265713000034"}