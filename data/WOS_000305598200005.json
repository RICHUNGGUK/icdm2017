{"auto_keywords": [{"score": 0.04860525980080879, "phrase": "pomdps"}, {"score": 0.00481495049065317, "phrase": "limited_reinforcement"}, {"score": 0.004750873844061641, "phrase": "bayes_risk"}, {"score": 0.00470862827326028, "phrase": "active_learning"}, {"score": 0.004267896572751978, "phrase": "challenging_task"}, {"score": 0.004229927464182562, "phrase": "especially_lithe_agent's_sensors"}, {"score": 0.004081385289516914, "phrase": "partially_observable_markov_decision_processes"}, {"score": 0.003973398317120165, "phrase": "planning_framework"}, {"score": 0.0037997077863530897, "phrase": "agent's_knowledge"}, {"score": 0.0036991458715587163, "phrase": "agent's_immediate_reward"}, {"score": 0.0035532533960012298, "phrase": "pomdp's_parameters"}, {"score": 0.003397866151093552, "phrase": "immediate_rewards"}, {"score": 0.0033376244286643176, "phrase": "desired_balance"}, {"score": 0.0030248298759040695, "phrase": "immediate_bayes_risk"}, {"score": 0.0028924840289020906, "phrase": "bayes-risk_criterion"}, {"score": 0.0028539179130824786, "phrase": "computational_intractability"}, {"score": 0.0027659126653647712, "phrase": "multi-dimensional_continuous_state_space"}, {"score": 0.0025863375961196005, "phrase": "policy_queries"}, {"score": 0.0024842242921857705, "phrase": "correct_action"}, {"score": 0.0024075916292945715, "phrase": "potential_pitfall"}, {"score": 0.002322895056238477, "phrase": "human-robot_interaction_settings"}, {"score": 0.002221192647477371, "phrase": "reward_model"}, {"score": 0.0021915573326125428, "phrase": "reward_values"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Partially observable Markov decision process", " Reinforcement learning", " Bayesian methods"], "paper_abstract": "Acting in domains where an agent must plan several steps ahead to achieve a goal can be a challenging task, especially lithe agent's sensors provide only noisy or partial information. In this setting. Partially Observable Markov Decision Processes (POMDPs) provide a planning framework that optimally trades between actions that contribute to the agent's knowledge anti actions that increase the agent's immediate reward. However, the task of specifying the POMDP's parameters is often onerous. In particular, setting the immediate rewards to achieve a desired balance between information-gathering and acting is often not intuitive. In this work, we propose an approximation based on minimizing the immediate Bayes risk for choosing actions when transition, observation, and reward models are uncertain. The Bayes-risk criterion avoids the computational intractability of solving a POMDP with a multi-dimensional continuous state space; we show it performs well in a variety of problems. We use policy queries in which we ask an expert for the correct action to infer the consequences of a potential pitfall without experiencing its effects. More important for human-robot interaction settings, policy queries allow the agent to learn the reward model without the reward values ever being specified. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs", "paper_id": "WOS:000305598200005"}