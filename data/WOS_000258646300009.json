{"auto_keywords": [{"score": 0.029180391343757647, "phrase": "hebbian"}, {"score": 0.00481495049065317, "phrase": "non-linear_component_extraction"}, {"score": 0.004695183699894267, "phrase": "generative_model"}, {"score": 0.004223708778943785, "phrase": "observed_variable"}, {"score": 0.004160319763461951, "phrase": "max_function"}, {"score": 0.003975781274672803, "phrase": "sparse_coding"}, {"score": 0.003935892934907777, "phrase": "independent_component_analysis"}, {"score": 0.003876806615011266, "phrase": "non-negative_matrix_factorization"}, {"score": 0.0037423521744765075, "phrase": "max_rule"}, {"score": 0.00363080961099728, "phrase": "non-linear_interaction"}, {"score": 0.003487222504700868, "phrase": "acoustic_and_image_data"}, {"score": 0.0034348484776384643, "phrase": "exact_maximum-likelihood_learning"}, {"score": 0.0031845067058084583, "phrase": "efficient_approximations"}, {"score": 0.002982307044758515, "phrase": "sparsely_active_hidden_causes"}, {"score": 0.002821247621250663, "phrase": "neural_network_model"}, {"score": 0.0027788478707700274, "phrase": "generalized_softmax_activation_function"}, {"score": 0.0026155094974689595, "phrase": "recent_softmax-like_neural_networks"}, {"score": 0.0025503112711036994, "phrase": "approximate_maximization"}, {"score": 0.0025119730908615104, "phrase": "data_likelihood"}, {"score": 0.0024493494346360415, "phrase": "bars_benchmark_test"}, {"score": 0.0023053339328456234, "phrase": "resulting_algorithms"}, {"score": 0.002202894673025544, "phrase": "model_parameters"}, {"score": 0.0021697677572213086, "phrase": "acoustic_and_visual_data_sets"}, {"score": 0.0021049977753042253, "phrase": "component_combinations"}], "paper_keywords": ["component extraction", " maximum likelihood", " approximate EM", " competitive learning", " neural networks"], "paper_abstract": "We study a generative model in which hidden causes combine competitively to produce observations. Multiple active causes combine to determine the value of an observed variable through a max function, in the place where algorithms such as sparse coding, independent component analysis, or non-negative matrix factorization would use a sum. This max rule can represent a more realistic model of non-linear interaction between basic components in many settings, including acoustic and image data. While exact maximum-likelihood learning of the parameters of this model proves to be intractable, we show that efficient approximations to expectation-maximization (EM) can be found in the case of sparsely active hidden causes. One of these approximations can be formulated as a neural network model with a generalized softmax activation function and Hebbian learning. Thus, we show that learning in recent softmax-like neural networks may be interpreted as approximate maximization of a data likelihood. We use the bars benchmark test to numerically verify our analytical results and to demonstrate the competitiveness of the resulting algorithms. Finally, we show results of learning model parameters to fit acoustic and visual data sets in which max-like component combinations arise naturally.", "paper_title": "Maximal causes for non-linear component extraction", "paper_id": "WOS:000258646300009"}