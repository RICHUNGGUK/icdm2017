{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mobile_devices"}, {"score": 0.004670896353428476, "phrase": "performance-based_facial_animation_system"}, {"score": 0.004476399615305783, "phrase": "real-time_frame_rates"}, {"score": 0.004238134740341528, "phrase": "novel_regression_algorithm"}, {"score": 0.0041112665381266315, "phrase": "facial_motion_parameters"}, {"score": 0.003988180913997414, "phrase": "ordinary_web_camera"}, {"score": 0.00386876598761639, "phrase": "state-of-the-art_facial_shape_regression_algorithm"}, {"score": 0.0037075456471849892, "phrase": "two-step_procedure"}, {"score": 0.003640516896245813, "phrase": "facial_animations"}, {"score": 0.0034465894339710864, "phrase": "facial_landmarks"}, {"score": 0.0032629585038738856, "phrase": "expression_coefficients"}, {"score": 0.0029965060407413898, "phrase": "one-step_approach"}, {"score": 0.0028715299874549245, "phrase": "regression_target"}, {"score": 0.0027854565462509095, "phrase": "tracking_performance"}, {"score": 0.0027184540871935284, "phrase": "tracking_accuracy"}, {"score": 0.0025892330566358503, "phrase": "training_images"}, {"score": 0.002511600086227215, "phrase": "different_lighting_environments"}, {"score": 0.002348883765159185, "phrase": "user-specific_regressor"}, {"score": 0.002264607102799332, "phrase": "lighting_changes"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Video tracking", " 3D avatars", " Facial performance", " User-specific blendshapes", " Shape regression"], "paper_abstract": "We present a performance-based facial animation system capable of running on mobile devices at real-time frame rates. A key component of our system is a novel regression algorithm that accurately infers the facial motion parameters from 2D video frames of an ordinary web camera. Compared with the state-of-the-art facial shape regression algorithm [11, which takes a two-step procedure to track facial animations (i.e., first regressing the 3D positions of facial landmarks, and then computing the head poses and expression coefficients), we directly regress the head poses and expression coefficients. This one-step approach greatly reduces the dimension of the regression target and significantly improves the tracking performance while preserving the tracking accuracy. We further propose to collect the training images of the user under different lighting environments, and make use of the data to learn a user-specific regressor, which can robustly handle lighting changes that frequently occur when using mobile devices. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Real-time facial animation on mobile devices", "paper_id": "WOS:000347018500015"}