{"auto_keywords": [{"score": 0.04701209021022985, "phrase": "multi-view_video"}, {"score": 0.00481495049065317, "phrase": "graph_cut_and_spatiotemporal_projections"}, {"score": 0.004614643339546038, "phrase": "automatic_algorithm"}, {"score": 0.004549734943142138, "phrase": "multiple_objects"}, {"score": 0.0044435677598780796, "phrase": "initial_interested_objects"}, {"score": 0.004258644418211771, "phrase": "key_view"}, {"score": 0.0041987222897385676, "phrase": "initial_frame"}, {"score": 0.004120130206000757, "phrase": "saliency_model"}, {"score": 0.004081385289516914, "phrase": "multiple_objects_segmentation"}, {"score": 0.0038746849003761024, "phrase": "energy_function"}, {"score": 0.0038382392088338784, "phrase": "binary_label_graph_cut"}, {"score": 0.003766369160795001, "phrase": "proposed_novel_energy_function"}, {"score": 0.0037133478610481994, "phrase": "color_and_depth_cues"}, {"score": 0.003626626504149897, "phrase": "data_term"}, {"score": 0.0035085965086415474, "phrase": "background_penalty"}, {"score": 0.003475582268210954, "phrase": "occlusion_reasoning"}, {"score": 0.003410479630045025, "phrase": "smoothness_term"}, {"score": 0.003378385493203297, "phrase": "foreground_contrast_enhancement"}, {"score": 0.0032838979278935814, "phrase": "moving_objects_boundary"}, {"score": 0.0031619994642255846, "phrase": "background_contrast"}, {"score": 0.0030446120560494155, "phrase": "coarse_predictions"}, {"score": 0.0029594318330247614, "phrase": "successive_frame"}, {"score": 0.0029039692582303904, "phrase": "pixel-based_disparity"}, {"score": 0.002769805263310232, "phrase": "inherent_spatiotemporal_consistency"}, {"score": 0.0027437235598539904, "phrase": "uncertain_band"}, {"score": 0.0027050595279115015, "phrase": "object_boundary"}, {"score": 0.002641823250595624, "phrase": "activity_measure"}, {"score": 0.0025922974876210194, "phrase": "graph_cut"}, {"score": 0.0023030731312573246, "phrase": "multi-view_videos"}, {"score": 0.002281376328251423, "phrase": "real_and_complex_scenes"}, {"score": 0.002259883463607508, "phrase": "excellent_subjective_results"}, {"score": 0.0021656385587456952, "phrase": "proposed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Multi-view pre-processing", " Depth reconstruction", " Saliency model", " Object extraction", " Graph cut", " Spatiotemporal protections", " Multi-view segmentation", " Multi-view video"], "paper_abstract": "In this paper, we present an automatic algorithm to segment multiple objects from multi-view video. The Initial Interested Objects (IIOs) are automatically extracted in the key view of the initial frame based on the saliency model. Multiple objects segmentation is decomposed into several sub-segmentation problems, and solved by minimizing the energy function using binary label graph cut. In the proposed novel energy function, the color and depth cues are integrated with the data term, which is then modified with background penalty with occlusion reasoning. In the smoothness term, foreground contrast enhancement is developed to strengthen the moving objects boundary, and at the same time attenuates the background contrast. To segment the multi-view video, the coarse predictions of the other views and the successive frame are projected by pixel-based disparity and motion compensation, respectively, which exploits the inherent spatiotemporal consistency. Uncertain band along the object boundary is shaped based on activity measure and refined with graph cut, resulting in a more accurate Interested Objects (IOs) layer across all views of the frames. The experiments are implemented on a couple of multi-view videos with real and complex scenes. Excellent subjective results have shown the robustness and efficiency of the proposed algorithm. (C) 2009 Elsevier Inc. All rights reserved.", "paper_title": "Multi-view video based multiple objects segmentation using graph cut and spatiotemporal projections", "paper_id": "WOS:000278970600008"}