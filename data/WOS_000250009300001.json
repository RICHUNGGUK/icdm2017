{"auto_keywords": [{"score": 0.04964702792241255, "phrase": "data_access_delay"}, {"score": 0.03142677260650649, "phrase": "dps"}, {"score": 0.00481495049065317, "phrase": "multi-processor_environments"}, {"score": 0.004707479285061122, "phrase": "major_bottleneck"}, {"score": 0.004644146045859622, "phrase": "current_high-end_computing"}, {"score": 0.004602402613916131, "phrase": "hec"}, {"score": 0.00452002148503201, "phrase": "prefetching"}, {"score": 0.0043793594257092805, "phrase": "cpu_demands"}, {"score": 0.004204904449336037, "phrase": "effective_solution"}, {"score": 0.004074016501033133, "phrase": "current_client-initiated_prefetching_strategies"}, {"score": 0.003947186611084043, "phrase": "prefetching_instructions"}, {"score": 0.0035095935622207956, "phrase": "data_access"}, {"score": 0.0034623203126093833, "phrase": "trading_computing_power"}, {"score": 0.003400274964240732, "phrase": "access_delay"}, {"score": 0.0033393377529146893, "phrase": "natural_choice"}, {"score": 0.0032061793819537633, "phrase": "server-based_data-push_approach"}, {"score": 0.0030644247579222333, "phrase": "server-push_architecture"}, {"score": 0.003023129616910419, "phrase": "dedicated_server"}, {"score": 0.0029959082125682918, "phrase": "data_push_server"}, {"score": 0.0025341792091114184, "phrase": "simplescalar_simulator"}, {"score": 0.0024774888010414206, "phrase": "dedicated_prefetching_engine"}, {"score": 0.00233594452829104, "phrase": "simulation_results"}, {"score": 0.0021531831652035482, "phrase": "superscalar_processor"}, {"score": 0.0021049977753042253, "phrase": "high_cache"}], "paper_keywords": ["performance measurement", " evaluation", " modeling", " simulation of multiple-processor system", " cache memory"], "paper_abstract": "Data access delay is a major bottleneck in utilizing current high-end computing (HEC) machines. Prefetching, where data is fetched before CPU demands for it, has been considered as an effective solution to masking data access delay. However, current client-initiated prefetching strategies, where a computing processor initiates prefetching instructions, have many limitations. They do not work well for applications with complex, non-contiguous data access patterns. While technology advances continue to increase the gap between computing and data access performance, trading computing power for reducing data access delay has become a natural choice. In this paper, we present a server-based data-push approach and discuss its associated implementation mechanisms. In the server-push architecture, a dedicated server called Data Push Server (DPS) initiates and proactively pushes data closer to the client in time. Issues, such as what data to fetch, when to fetch, and how to push are studied. The SimpleScalar simulator is modified with a dedicated prefetching engine that pushes data for another processor to test DPS based prefetching. Simulation results show that L1 Cache miss rate can be reduced by up to 97% (71% on average) over a superscalar processor for SPEC CPU2000 benchmarks that have high cache miss rates.", "paper_title": "Server-based data push architecture for multi-processor environments", "paper_id": "WOS:000250009300001"}