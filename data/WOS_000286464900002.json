{"auto_keywords": [{"score": 0.02295331367609629, "phrase": "matousek"}, {"score": 0.00481495049065317, "phrase": "lean_walsh_transforms"}, {"score": 0.004745665541429529, "phrase": "random_projection_methods"}, {"score": 0.004610058444728656, "phrase": "kxd_matrices"}, {"score": 0.004046020290832207, "phrase": "finite_set"}, {"score": 0.0033021988455635403, "phrase": "constant_probability"}, {"score": 0.002918826667294298, "phrase": "good_random_projector"}, {"score": 0.002542642270762272, "phrase": "tensor_product_matrices"}, {"score": 0.002469830206270585, "phrase": "lean_walsh"}, {"score": 0.0023644936731468252, "phrase": "lean_walsh_matrices"}, {"score": 0.0023303871145705954, "phrase": "random_projections"}, {"score": 0.002230984431056064, "phrase": "running_time"}, {"score": 0.0021049977753042253, "phrase": "comparable_assumptions"}], "paper_keywords": ["Fast random projections", " Dimension reduction", " Lean Walsh matrices", " Johnson-Lindenstrauss"], "paper_abstract": "Random projection methods give distributions over kxd matrices such that if a matrix I (chosen according to the distribution) is applied to a finite set of vectors x (i) aa\"e (d) the resulting vectors Ix (i) aa\"e (k) approximately preserve the original metric with constant probability. First, we show that any matrix (composed with a random +/- 1 diagonal matrix) is a good random projector for a subset of vectors in a\"e (d) . Second, we describe a family of tensor product matrices which we term Lean Walsh. We show that using Lean Walsh matrices as random projections outperforms, in terms of running time, the best known current result (due to Matousek) under comparable assumptions.", "paper_title": "Dense Fast Random Projections and Lean Walsh Transforms", "paper_id": "WOS:000286464900002"}