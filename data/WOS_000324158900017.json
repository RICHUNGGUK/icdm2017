{"auto_keywords": [{"score": 0.04720712072096041, "phrase": "mpi"}, {"score": 0.00481495049065317, "phrase": "efficient_distributed_memory_programming"}, {"score": 0.004777917315737992, "phrase": "multi-core_systems"}, {"score": 0.004614745158543967, "phrase": "many-core_high-performance_processors"}, {"score": 0.004388796812234033, "phrase": "programming_high-performance_computing_systems"}, {"score": 0.004338239182984759, "phrase": "distributed_memory_programming_model"}, {"score": 0.004304856672196293, "phrase": "mpi's_semantics"}, {"score": 0.0039085400187964196, "phrase": "modern_hardware"}, {"score": 0.003702702337768558, "phrase": "mpi's_communication_model"}, {"score": 0.0035076666315744525, "phrase": "shared_memory_hardware"}, {"score": 0.0034405207528624983, "phrase": "many-core_cpus"}, {"score": 0.0034008496666026585, "phrase": "communicated_data"}, {"score": 0.0033486631403337555, "phrase": "receiver's_computations"}, {"score": 0.0032341432727280707, "phrase": "message_passing"}, {"score": 0.003051887221639784, "phrase": "memory_regions"}, {"score": 0.002879872305061576, "phrase": "hybrid_mpi_implementation"}, {"score": 0.002846647336160784, "phrase": "mpi_processes"}, {"score": 0.00269662988585869, "phrase": "api"}, {"score": 0.002665465323614577, "phrase": "static_analysis_technique"}, {"score": 0.0026347074232350503, "phrase": "legacy_mpi_codes"}, {"score": 0.002429090371567499, "phrase": "ownership_passing_technique"}, {"score": 0.002345944657572499, "phrase": "standard_message"}, {"score": 0.0023099082804523044, "phrase": "state-of-the_art_multicore_systems"}, {"score": 0.0022136307563123256, "phrase": "future_development"}, {"score": 0.0021965609224818853, "phrase": "mpi-aware_optimizing_compilers"}, {"score": 0.0021796224306652326, "phrase": "multi-core_specific_optimizations"}, {"score": 0.0021049977753042253, "phrase": "current_and_next-generation_computing_platforms"}], "paper_keywords": ["Ownership Passing", " Distributed Memory", " Shared Memory", " Message Passing", " Multi-core"], "paper_abstract": "The number of cores in multi- and many-core high-performance processors is steadily increasing. MPI, the de-facto standard for programming high-performance computing systems offers a distributed memory programming model. MPI's semantics force a copy from one process' send buffer to another process' receive buffer. This makes it difficult to achieve the same performance on modern hardware than shared memory programs which are arguably harder to maintain and debug. We propose generalizing MPI's communication model to include ownership passing, which make it possible to fully leverage the shared memory hardware of multi-and many-core CPUs to stream communicated data concurrently with the receiver's computations on it. The benefits and simplicity of message passing are retained by extending MPI with calls to send (pass) ownership of memory regions, instead of their contents, between processes. Ownership passing is achieved with a hybrid MPI implementation that runs MPI processes as threads and is mostly transparent to the user. We propose an API and a static analysis technique to transform legacy MPI codes automatically and transparently to the programmer, demonstrating that this scheme is easy to use in practice. Using the ownership passing technique, we see up to 51% communication speedups over a standard message passing implementation on state-of-the art multicore systems. Our analysis and interface will lay the groundwork for future development of MPI-aware optimizing compilers and multi-core specific optimizations, which will be key for success in current and next-generation computing platforms.", "paper_title": "Ownership Passing: Efficient Distributed Memory Programming on Multi-core Systems", "paper_id": "WOS:000324158900017"}