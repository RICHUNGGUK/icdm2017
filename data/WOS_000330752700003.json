{"auto_keywords": [{"score": 0.0401018006404259, "phrase": "probe_sequence"}, {"score": 0.00481495049065317, "phrase": "human_action_recognition"}, {"score": 0.004749824776508639, "phrase": "learning-based_framework"}, {"score": 0.004397000006837371, "phrase": "optical_flow_motion"}, {"score": 0.004259418153299952, "phrase": "learning_step"}, {"score": 0.004051828608343507, "phrase": "gaussian_mixture_modeling"}, {"score": 0.003925004567341077, "phrase": "recognition_step"}, {"score": 0.003871869188502677, "phrase": "optical_flow_curves"}, {"score": 0.0035677736570659813, "phrase": "training_space"}, {"score": 0.0035194572615570977, "phrase": "probe_curves"}, {"score": 0.0034403745992576808, "phrase": "learned_curves"}, {"score": 0.0033937776228549557, "phrase": "non-metric_similarity_function"}, {"score": 0.0033326252993345685, "phrase": "longest_common_subsequence"}, {"score": 0.0031845067058084583, "phrase": "intuitive_notion"}, {"score": 0.003056819704624705, "phrase": "mean_curves"}, {"score": 0.0030017207229983385, "phrase": "canonical_time"}, {"score": 0.002855251306715901, "phrase": "learned_action"}, {"score": 0.002816556934231128, "phrase": "maximum_similarity"}, {"score": 0.002778385486855157, "phrase": "nearest_neighbor_classification_scheme"}, {"score": 0.0025951294037626174, "phrase": "time_series"}, {"score": 0.002548331088496964, "phrase": "dimensionality_reduction"}, {"score": 0.002491015208328868, "phrase": "test_phases"}, {"score": 0.0022847009758393405, "phrase": "experimental_results"}, {"score": 0.002264005677597105, "phrase": "kth"}, {"score": 0.0022434880661725493, "phrase": "ucf_sports"}, {"score": 0.0022231606165229235, "phrase": "ucf_youtube_action_databases"}, {"score": 0.0021632743048552536, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Human action recognition", " Optical flow", " Motion curves", " Gaussian mixture modeling (GMM)", " Clustering", " Dimensionality reduction", " Longest common subsequence"], "paper_abstract": "A learning-based framework for action representation and recognition relying on the description of an action by time series of optical flow motion features is presented. In the learning step, the motion curves representing each action are clustered using Gaussian mixture modeling (GMM). In the recognition step, the optical flow curves of a probe sequence are also clustered using a GMM, then each probe sequence is projected onto the training space and the probe curves are matched to the learned curves using a non-metric similarity function based on the longest common subsequence, which is robust to noise and provides an intuitive notion of similarity between curves. Alignment between the mean curves is performed using canonical time warping. Finally, the probe sequence is categorized to the learned action with the maximum similarity using a nearest neighbor classification scheme. We also present a variant of the method where the length of the time series is reduced by dimensionality reduction in both training and test phases, in order to smooth out the outliers, which are common in these type of sequences. Experimental results on KTH, UCF Sports and UCF YouTube action databases demonstrate the effectiveness of the proposed method. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Matching mixtures of curves for human action recognition", "paper_id": "WOS:000330752700003"}