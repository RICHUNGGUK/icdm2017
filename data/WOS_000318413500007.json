{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "fine_quantization"}, {"score": 0.004709567162738894, "phrase": "novel_similarity_measure"}, {"score": 0.004640590144749795, "phrase": "bag-of-words_type_large_scale_image_retrieval"}, {"score": 0.004278807087784626, "phrase": "unsupervised_manner"}, {"score": 0.004154332348800334, "phrase": "extra_space"}, {"score": 0.004063348730077936, "phrase": "standard_bag"}, {"score": 0.003945117054416623, "phrase": "words_method"}, {"score": 0.0036914757355166966, "phrase": "hamming_embedding"}, {"score": 0.0036105923401756126, "phrase": "novel_similarity_function"}, {"score": 0.003557653536102669, "phrase": "mean_average_precision"}, {"score": 0.0031845322879336213, "phrase": "oxford"}, {"score": 0.002317423416027688, "phrase": "previously_published_results"}, {"score": 0.0022005118194652704, "phrase": "large_vocabularies"}, {"score": 0.0021049977753042253, "phrase": "tf-idf_scoring_step"}], "paper_keywords": ["Image retrieval", " Vocabulary", " Feature track"], "paper_abstract": "A novel similarity measure for bag-of-words type large scale image retrieval is presented. The similarity function is learned in an unsupervised manner, requires no extra space over the standard bag-of-words method and is more discriminative than both L2-based soft assignment and Hamming embedding. The novel similarity function achieves mean average precision that is superior to any result published in the literature on the standard Oxford 5k, Oxford 105k and Paris datasets/protocols. We study the effect of a fine quantization and very large vocabularies (up to 64 million words) and show that the performance of specific object retrieval increases with the size of the vocabulary. This observation is in contradiction with previously published results. We further demonstrate that the large vocabularies increase the speed of the tf-idf scoring step.", "paper_title": "Learning Vocabularies over a Fine Quantization", "paper_id": "WOS:000318413500007"}