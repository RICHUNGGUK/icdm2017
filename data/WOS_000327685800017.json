{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "feature_selection"}, {"score": 0.03952527802954203, "phrase": "feature_quality"}, {"score": 0.026800634409058707, "phrase": "proposed_algorithm"}, {"score": 0.004719664411221952, "phrase": "important_preprocessing_step"}, {"score": 0.004657185361669812, "phrase": "machine_learning"}, {"score": 0.004595529592539951, "phrase": "pattern_recognition"}, {"score": 0.004415395176043526, "phrase": "data_mining_task"}, {"score": 0.004327981760540394, "phrase": "real-world_applications"}, {"score": 0.004270665568018583, "phrase": "feature_quality_evaluation"}, {"score": 0.004186105377259855, "phrase": "key_issue"}, {"score": 0.003942299471749811, "phrase": "classification_margin"}, {"score": 0.0034963129868796033, "phrase": "robust_loss_function"}, {"score": 0.0034270329006502328, "phrase": "brownboost"}, {"score": 0.0032058286587989234, "phrase": "optimal_feature_subsets"}, {"score": 0.0030391594477173485, "phrase": "classification_loss"}, {"score": 0.0029789104204803137, "phrase": "feature_space"}, {"score": 0.00260657976055413, "phrase": "gradient_descent"}, {"score": 0.0024545413637350765, "phrase": "uci_datasets"}, {"score": 0.002421975107313603, "phrase": "gene_expression_datasets"}, {"score": 0.0023424328921363585, "phrase": "experimental_results"}, {"score": 0.0021910825909008946, "phrase": "classification_robustness"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Feature selection", " Margin", " Robustness", " Brownboost loss", " Regularization"], "paper_abstract": "Feature selection is an important preprocessing step in machine learning and pattern recognition. It is also a data mining task in some real-world applications. Feature quality evaluation is a key issue when designing an algorithm for feature selection. The classification margin has been used widely to evaluate feature quality in recent years. In this study, we introduce a robust loss function, called Brownboost loss, which computes the feature quality and selects the optimal feature subsets to enhance robustness. We compute the classification loss in a feature space with hypothesis-margin and minimize the loss by optimizing the weights of features. An algorithm is developed based on gradient descent using L-2-norm regularization techniques. The proposed algorithm is tested using UCI datasets and gene expression datasets, respectively. The experimental results show that the proposed algorithm is effective in improving the classification robustness. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Robust feature selection based on regularized brownboost loss", "paper_id": "WOS:000327685800017"}