{"auto_keywords": [{"score": 0.03843856552198327, "phrase": "rpca"}, {"score": 0.01405457810309818, "phrase": "irpca"}, {"score": 0.008665044347931075, "phrase": "gross_corruptions"}, {"score": 0.00481495049065317, "phrase": "error_correction_problem"}, {"score": 0.004671803485255806, "phrase": "low-dimensional_subspace_structure"}, {"score": 0.00463668357590034, "phrase": "high-dimensional_observations"}, {"score": 0.004398094237005956, "phrase": "gaussian_distribution"}, {"score": 0.004365022891478405, "phrase": "principal_component_analysis"}, {"score": 0.004332357457102579, "phrase": "pca"}, {"score": 0.00409372989087314, "phrase": "canonical_pca_method"}, {"score": 0.003614124997453808, "phrase": "transductive_method"}, {"score": 0.0035198224928307854, "phrase": "new_samples"}, {"score": 0.003415047118340286, "phrase": "training_procedure"}, {"score": 0.003363830908814182, "phrase": "new_datum"}, {"score": 0.003214730261140886, "phrase": "high_computational_cost"}, {"score": 0.00307221807606505, "phrase": "fast_online_computation"}, {"score": 0.002913895605074143, "phrase": "inductive_robust_principal_component_analysis"}, {"score": 0.0028058141911641225, "phrase": "training_data"}, {"score": 0.0027119634978446895, "phrase": "original_data_matrix"}, {"score": 0.0026411384812121503, "phrase": "underlying_projection_matrix"}, {"score": 0.002543148295176345, "phrase": "possible_corruptions"}, {"score": 0.00243033541656411, "phrase": "nuclear-norm_regularized_minimization_problem"}, {"score": 0.002340147988348115, "phrase": "polynomial_time"}, {"score": 0.0023225152126545067, "phrase": "extensive_experiments"}, {"score": 0.0022963142779850562, "phrase": "benchmark_human_face_dataset"}, {"score": 0.0021451873499061633, "phrase": "new_data"}, {"score": 0.0021049977753042253, "phrase": "efficient_way"}], "paper_keywords": ["Error correction", " robust principal component analysis (PCA)", " subspace learning"], "paper_abstract": "In this paper, we address the error correction problem, that is, to uncover the low-dimensional subspace structure from high-dimensional observations, which are possibly corrupted by errors. When the errors are of Gaussian distribution, principal component analysis (PCA) can find the optimal (in terms of least-square error) low-rank approximation to high-dimensional data. However, the canonical PCA method is known to be extremely fragile to the presence of gross corruptions. Recently, Wright et al. established a so-called robust principal component analysis (RPCA) method, which can well handle the grossly corrupted data. However, RPCA is a transductive method and does not handle well the new samples, which are not involved in the training procedure. Given a new datum, RPCA essentially needs to recalculate over all the data, resulting in high computational cost. So, RPCA is inappropriate for the applications that require fast online computation. To overcome this limitation, in this paper, we propose an inductive robust principal component analysis (IRPCA) method. Given a set of training data, unlike RPCA that targets on recovering the original data matrix, IRPCA aims at learning the underlying projection matrix, which can be used to efficiently remove the possible corruptions in any datum. The learning is done by solving a nuclear-norm regularized minimization problem, which is convex and can be solved in polynomial time. Extensive experiments on a benchmark human face dataset and two video surveillance datasets show that IRPCA cannot only be robust to gross corruptions, but also handle the new data well and in an efficient way.", "paper_title": "Inductive Robust Principal Component Analysis", "paper_id": "WOS:000306598100037"}