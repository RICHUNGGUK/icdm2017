{"auto_keywords": [{"score": 0.04571345706034808, "phrase": "lda"}, {"score": 0.00481495049065317, "phrase": "data_placement_and_pipeline_processing"}, {"score": 0.004664352260912174, "phrase": "previous_methods"}, {"score": 0.004518442937844318, "phrase": "distributed_gibbs_sampling"}, {"score": 0.0041074234667687875, "phrase": "memory_or_communication_bottlenecks"}, {"score": 0.002524600918095582, "phrase": "lda."}, {"score": 0.002468450551616404, "phrase": "unparallelizable_communication_bottleneck"}, {"score": 0.0023532890136269986, "phrase": "good_load_balancing"}], "paper_keywords": ["Algorithms", " Topic models", " Gibbs sampling", " latent Dirichlet allocation", " distributed parallel computations"], "paper_abstract": "Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.", "paper_title": "PLDA+: Parallel Latent Dirichlet Allocation with Data Placement and Pipeline Processing", "paper_id": "WOS:000208617000009"}