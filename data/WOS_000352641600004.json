{"auto_keywords": [{"score": 0.04679607575467667, "phrase": "value_iteration"}, {"score": 0.04120087369549478, "phrase": "policy_iteration"}, {"score": 0.03245139145921951, "phrase": "multiple_other_agents"}, {"score": 0.00481495049065317, "phrase": "interactive_pomdps"}, {"score": 0.004785356349906913, "phrase": "generalized_and_bounded_policy_iteration"}, {"score": 0.004670081363117602, "phrase": "markov"}, {"score": 0.004527024978801684, "phrase": "quicker_convergence"}, {"score": 0.004243135792625782, "phrase": "finite_state_automaton"}, {"score": 0.004178198896815672, "phrase": "finite_state_controller"}, {"score": 0.0038442670949832523, "phrase": "controller_size"}, {"score": 0.0036477941360224435, "phrase": "local_optima"}, {"score": 0.0035808664384880213, "phrase": "policy_iteration_algorithms"}, {"score": 0.003558830226733501, "phrase": "viable_alternatives"}, {"score": 0.0033768959341690524, "phrase": "bounded_policy_iteration_technique"}, {"score": 0.003335452981547803, "phrase": "multiple_agents"}, {"score": 0.0032340499528688495, "phrase": "bounded_policy_iteration"}, {"score": 0.0032141411226372704, "phrase": "anytime_behavior"}, {"score": 0.0031551446537729107, "phrase": "interactive_pomdp_framework"}, {"score": 0.0030972277169783067, "phrase": "non-stationary_contexts"}, {"score": 0.0029937855454238507, "phrase": "decentralized_pomdps"}, {"score": 0.00284065508239492, "phrase": "non-cooperative_settings"}, {"score": 0.002678734566245688, "phrase": "others'_actions"}, {"score": 0.0025892330566358503, "phrase": "model_space"}, {"score": 0.002518227743688769, "phrase": "agent's_initial_belief"}, {"score": 0.0024190798397126924, "phrase": "large_domains"}, {"score": 0.002367315484467406, "phrase": "increased_computations"}, {"score": 0.002281134611390528, "phrase": "multiple_problem"}], "paper_keywords": ["Decision making", " Multiagent settings", " Policy iteration", " POMDP"], "paper_abstract": "Policy iteration algorithms for partially observable Markov decision processes (POMDPs) offer the benefits of quicker convergence compared to value iteration and the ability to operate directly on the solution, which usually takes the form of a finite state automaton. However, the finite state controller tends to grow quickly in size across iterations due to which its evaluation and improvement become computationally costly. Bounded policy iteration provides a way of keeping the controller size fixed while improving it monotonically until convergence, although it is susceptible to getting trapped in local optima. Despite these limitations, policy iteration algorithms are viable alternatives to value iteration, and allow POMDPs to scale. In this article, we generalize the bounded policy iteration technique to problems involving multiple agents. Specifically, we show how we may perform bounded policy iteration with anytime behavior in settings formalized by the interactive POMDP framework, which generalizes POMDPs to non-stationary contexts shared with multiple other agents. Although policy iteration has been extended to decentralized POMDPs, the context there is strictly cooperative. Its novel generalization in this article makes it useful in non-cooperative settings as well. As interactive POMDPs involve modeling other agents sharing the environment, we ascribe controllers to predict others' actions, with the benefit that the controllers compactly represent the model space. We show how we may exploit the agent's initial belief, often available, toward further improving the controller, particularly in large domains, though at the expense of increased computations, which we compensate. We extensively evaluate the approach on multiple problem domains with some that are significantly large in their dimensions, and in contexts with uncertainty about the other agent's frames and those involving multiple other agents, and demonstrate its properties and scalability.", "paper_title": "Scalable solutions of interactive POMDPs using generalized and bounded policy iteration", "paper_id": "WOS:000352641600004"}