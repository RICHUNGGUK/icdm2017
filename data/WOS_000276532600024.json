{"auto_keywords": [{"score": 0.03359643145744443, "phrase": "workload_requirements"}, {"score": 0.012589733929670884, "phrase": "system_performance"}, {"score": 0.011955006570135922, "phrase": "standard_benchmarks"}, {"score": 0.009767904660546096, "phrase": "benchmark_method"}, {"score": 0.009202201896060722, "phrase": "test_database"}, {"score": 0.00481495049065317, "phrase": "intelligent_information_integration_-_a_generic_construct-based_model"}, {"score": 0.004654542935568, "phrase": "computer_hardware_and_software_systems"}, {"score": 0.004270845182558663, "phrase": "problem_domain"}, {"score": 0.004248630241580105, "phrase": "test_results"}, {"score": 0.004171780185396856, "phrase": "possible_system_performance"}, {"score": 0.004053803248339119, "phrase": "standard_problem_domain"}, {"score": 0.004011731408358129, "phrase": "application_workload"}, {"score": 0.003959752657656169, "phrase": "standard_workload"}, {"score": 0.0038881074726931355, "phrase": "accurate_way"}, {"score": 0.0038177536203992278, "phrase": "user_problem_domain"}, {"score": 0.0037682786703039264, "phrase": "actual_problem_domain"}, {"score": 0.0035487956616920987, "phrase": "domain_boundness"}, {"score": 0.003530323168993377, "phrase": "workload_boundness"}, {"score": 0.0034664207609114302, "phrase": "ir-reproducible_performance"}, {"score": 0.0033771261386897505, "phrase": "domain-independent_and_workload-independent_benchmark_method"}, {"score": 0.0032987228516723408, "phrase": "user_requirements"}, {"score": 0.0032559521297271165, "phrase": "user-driven_workload_model"}, {"score": 0.0030185612970938724, "phrase": "actual_user_domain"}, {"score": 0.002925452606050736, "phrase": "high-level_workload_specification_scheme"}, {"score": 0.0027838524522031034, "phrase": "test_suite"}, {"score": 0.002762128010045399, "phrase": "specification_scheme"}, {"score": 0.0025740610607310017, "phrase": "test_workload"}, {"score": 0.002553969581727289, "phrase": "web_search"}, {"score": 0.0025340345255790517, "phrase": "generic_constructs"}, {"score": 0.00252083079100971, "phrase": "main_common_carriers"}, {"score": 0.0024050431370602646, "phrase": "literature_study"}, {"score": 0.0023491545128442874, "phrase": "ten_baseline_experiments"}, {"score": 0.002276646782260818, "phrase": "experimental_prototype"}, {"score": 0.0022295531401315787, "phrase": "experimental_results"}, {"score": 0.002138261955387411, "phrase": "benchmark_requirements"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["XML", " Ontology", " Intelligent information integration", " Generic construct", " Benchmark", " Workload model", " Performance measurement and evaluation"], "paper_abstract": "Benchmarks are vital tools in the performance measurement and evaluation of computer hardware and software systems. Standard benchmarks such as the TREC, TPC, SPEC, SAP, Oracle, Microsoft, IBM, Wisconsin, AS(3)AP, OO1, OO7, XOO7 benchmarks have been used to assess the system performance. These benchmarks are domain-specific in that they model typical applications and tie to a problem domain. Test results from these benchmarks are estimates of possible system performance for certain pre-determined problem types. When the user domain differs from the standard problem domain or when the application workload is divergent from the standard workload, they do not provide an accurate way to measure the system performance of the user problem domain. System performance of the actual problem domain in terms of data and transactions may vary significantly from the standard benchmarks. In this research, we address the issue of domain boundness and workload boundness which results in the ir-representative and ir-reproducible performance reading. We tackle the issue by proposing a domain-independent and workload-independent benchmark method which is developed from the perspective of the user requirements. We present a user-driven workload model to develop a benchmark in a process of workload requirements representation, transformation, and generation. We aim to create a more generalized and precise evaluation method which derives test suites from the actual user domain and application. The benchmark method comprises three main components. They are a high-level workload specification scheme, a translator of the scheme, and a set of generators to generate the test database and the test suite. The specification scheme is used to formalize the workload requirements. The translator is used to transform the specification. The generator is used to produce the test database and the test workload. In web search, the generic constructs are main common carriers we adopt to capture and compose the workload requirements. We determine the requirements via the analysis of literature study. In this study, we have conducted ten baseline experiments to validate the feasibility and validity of the benchmark method. An experimental prototype is built to execute these experiments. Experimental results demonstrate that the method is capable of modeling the standard benchmarks as well as more general benchmark requirements. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Benchmarking intelligent information integration - A generic construct-based model", "paper_id": "WOS:000276532600024"}