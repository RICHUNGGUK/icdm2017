{"auto_keywords": [{"score": 0.048506844221552735, "phrase": "continuous_domains"}, {"score": 0.02322730111336098, "phrase": "bayesian_classifiers"}, {"score": 0.022849846445563968, "phrase": "distribution_algorithms"}, {"score": 0.004541712637353326, "phrase": "evolutionary_computation_method"}, {"score": 0.004447915226010682, "phrase": "optimization_problems"}, {"score": 0.00417793854311445, "phrase": "bayesian_or_gaussian_networks"}, {"score": 0.00389164348807591, "phrase": "optimization_problem"}, {"score": 0.003685972800046028, "phrase": "next_generation"}, {"score": 0.0036097844103129043, "phrase": "evolutionary_bayesian"}, {"score": 0.0035648266669315943, "phrase": "optimization_algorithm"}, {"score": 0.0034335866344925078, "phrase": "bayesian"}, {"score": 0.0034055473906141084, "phrase": "gaussian"}, {"score": 0.003279039724748895, "phrase": "fitter_population"}, {"score": 0.0031448288291084, "phrase": "bayesian_classification_techniques"}, {"score": 0.003092690568543981, "phrase": "supervised_classification_problems"}, {"score": 0.0029291198314113608, "phrase": "paper_different_bayesian_classifiers"}, {"score": 0.002785850905791767, "phrase": "bayes"}, {"score": 0.0027167830773160203, "phrase": "naive_bayes_classifiers"}, {"score": 0.0026494723416072316, "phrase": "deep_study"}, {"score": 0.0025623050508233078, "phrase": "classical_optimiztion_problems"}, {"score": 0.002509284173541882, "phrase": "different_parameters"}, {"score": 0.0023665766694993535, "phrase": "comprehensive_overview"}, {"score": 0.00227914162528171, "phrase": "experimental_results"}, {"score": 0.0022413233627905696, "phrase": "new_method"}, {"score": 0.002194929830604162, "phrase": "art_approaches"}, {"score": 0.00216755493625594, "phrase": "evolutionary_computation_field"}, {"score": 0.00212268510072928, "phrase": "evolutionary_strategies"}, {"score": 0.002105022643489445, "phrase": "es"}], "paper_keywords": [""], "paper_abstract": "This paper introduces a evolutionary computation method that applies Bayesian classifiers to optimization problems. This approach is based on Estimation of Distribution Algorithms (EDAs) in which Bayesian or Gaussian networks are applied to the evolution of a population of individuals (i.e. potential solutions to the optimization problem) in order to improve the quality of the individuals of the next generation. Our new approach, called Evolutionary Bayesian Classifier- based Optimization Algorithm (EBCOA), employs Bayesian classifiers instead of Bayesian or Gaussian networks in order to evolve individuals to a fitter population. In brief, EBCOAs are characterized by applying Bayesian classification techniques -usually applied to supervised classification problems-to optimization in continuous domains. We propose and review in this paper different Bayesian classifiers for implementing our EBCOA method, focusing particularly on EBCOAs applying nai ve Bayes, semi-naive Bayes, and tree augmented naive Bayes classifiers. This work presents a deep study on the behavior of these algorithms with classical optimiztion problems in continuous domains. The different parameters used for tuning the performance of the algorithms are discussed, and a comprehensive overview of their influence is provided. We also present experimental results to compare this new method with other state of the art approaches of the evolutionary computation field for continuous domains such as Evolutionary Strategies (ES) and Estimation of Distribution Algorithms (EDAs).", "paper_title": "Combining Bayesian classifiers and estimation of distribution algorithms for optimization in continuous domains", "paper_id": "WOS:000251710700003"}