{"auto_keywords": [{"score": 0.04423839018992287, "phrase": "video_sequence"}, {"score": 0.004729704116022601, "phrase": "\"bag-of-words\"_representation"}, {"score": 0.0044430006374681285, "phrase": "human_actions"}, {"score": 0.004383852039913171, "phrase": "traditional_topicmodels"}, {"score": 0.004345168913192333, "phrase": "tm"}, {"score": 0.004287008195370748, "phrase": "visual_recognition"}, {"score": 0.004063186599879217, "phrase": "spatial-temporal_interest_points"}, {"score": 0.003973398317120165, "phrase": "topic_models"}, {"score": 0.0037997077863530897, "phrase": "action_category"}, {"score": 0.003601235776892337, "phrase": "action_topics"}, {"score": 0.0034902664176377943, "phrase": "previous_tm"}, {"score": 0.0033525843225012918, "phrase": "higher_order_uncertainty"}, {"score": 0.003220315882881027, "phrase": "primary_membership_function"}, {"score": 0.0030113280682863234, "phrase": "visual_word"}, {"score": 0.002957919048133256, "phrase": "specific_action_topic"}, {"score": 0.002892484028902088, "phrase": "secondary_mf"}, {"score": 0.0028032928749970026, "phrase": "primary_mf"}, {"score": 0.0025863375961196005, "phrase": "secondary_grades"}, {"score": 0.0024401411219526774, "phrase": "unequal_secondary_grades"}, {"score": 0.002291905937251069, "phrase": "efficient_message-passing_algorithms"}, {"score": 0.0022512371876580012, "phrase": "kth"}, {"score": 0.0022311610041499076, "phrase": "weizmann"}, {"score": 0.0021720199689914464, "phrase": "human_action_datasets"}], "paper_keywords": ["Human action recognition", " latent Dirichlet allocation (LDA)", " message passing", " type-2 fuzzy sets (T2 FS)"], "paper_abstract": "Following the \"bag-of-words\" representation for video sequences, we propose novel type-2 fuzzy topic models (T2 FTM) to recognize human actions. In traditional topicmodels (TM) for visual recognition, each video sequence is modeled as a \"document\" composed of spatial-temporal interest points called visual words. Topic models automatically assign a \"topic\" label to explain the action category of each word so that each video sequence becomes a mixture of action topics for recognition. Our T2 FTM differs from previous TM in that it uses type-2 fuzzy sets to encode the higher order uncertainty of each topic. We can use the primary membership function (MF) to measure the degree of uncertainty that a document or a visual word belongs to a specific action topic, and use the secondary MF to evaluate the fuzziness of the primary MF itself. In this paper, we implement two T2 FTM: 1) interval T2 FTM with all secondary grades equal one, and 2) vertical-slice T2 FTM with unequal secondary grades based on our prior knowledge. To estimate parameters in T2 FTM, we derive the efficient message-passing algorithms. Experiments on KTH, Weizmann, UCF, and Hollywood2 human action datasets demonstrate that T2 FTM performs better than other state-of-the-art topic models for human action recognition.", "paper_title": "Type-2 Fuzzy Topic Models for Human Action Recognition", "paper_id": "WOS:000362582800018"}