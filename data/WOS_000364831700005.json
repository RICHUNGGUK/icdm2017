{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sparse_gaussian_process_regression"}, {"score": 0.004675930527604869, "phrase": "efficient_optimization_algorithm"}, {"score": 0.0045143706249204905, "phrase": "training_data"}, {"score": 0.0044356867986444426, "phrase": "inducing_set"}, {"score": 0.004232475065285966, "phrase": "different_objective_functions"}, {"score": 0.0041102032028337366, "phrase": "hyperparameter_selection"}, {"score": 0.0038989120088094185, "phrase": "gradient-based_continuous_optimization"}, {"score": 0.0034673190692473903, "phrase": "discrete_input_domains"}, {"score": 0.0034068225125064586, "phrase": "kernel_functions"}, {"score": 0.002924694271730544, "phrase": "single_objective"}, {"score": 0.0027417911324611917, "phrase": "variational_free_energy"}, {"score": 0.0024379730432321656, "phrase": "large_regression_problems"}, {"score": 0.002409503258044183, "phrase": "discrete_or_continuous_domains"}, {"score": 0.002339771435910601, "phrase": "state-of-art_performance"}, {"score": 0.002129877319760485, "phrase": "test_time"}, {"score": 0.0021049977753042253, "phrase": "continuous_cases"}], "paper_keywords": ["Gaussian process regression", " low rank", " matrix factorization", " sparsity"], "paper_abstract": "We propose an efficient optimization algorithm to select a subset of training data as the inducing set for sparse Gaussian process regression. Previous methods either use different objective functions for inducing set and hyperparameter selection, or else optimize the inducing set by gradient-based continuous optimization. The former approaches are harder to interpret and suboptimal, whereas the latter cannot be applied to discrete input domains or to kernel functions that are not differentiable with respect to the input. The algorithm proposed in this work estimates an inducing set and the hyperparameters using a single objective. It can be used to optimize either the marginal likelihood or a variational free energy. Space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases, competitive prediction results as well as a favorable trade-off between training and test time in continuous cases.", "paper_title": "Efficient Optimization for Sparse Gaussian Process Regression", "paper_id": "WOS:000364831700005"}