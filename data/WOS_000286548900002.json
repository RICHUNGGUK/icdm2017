{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "incremental_approach"}, {"score": 0.0047588556219929756, "phrase": "online_inverse_reinforcement_learning"}, {"score": 0.004134372859330047, "phrase": "reward_function"}, {"score": 0.0040622852274783275, "phrase": "markov_decision_process"}, {"score": 0.0034673190692473903, "phrase": "convergence_property"}, {"score": 0.0034068225125064586, "phrase": "incremental_method"}, {"score": 0.0033473779327638322, "phrase": "irl_problem"}, {"score": 0.003138124398958758, "phrase": "learning_process"}, {"score": 0.002994178996584381, "phrase": "detailed_proof"}, {"score": 0.002924694271730544, "phrase": "online_algorithm"}, {"score": 0.0028736379396305596, "phrase": "incremental_error"}, {"score": 0.002693919110230191, "phrase": "key_idea"}, {"score": 0.002570296816631395, "phrase": "current_reward_estimate"}, {"score": 0.0025106241866273897, "phrase": "action_mismatch"}, {"score": 0.002339771435910601, "phrase": "optimal_value"}, {"score": 0.002298902547676131, "phrase": "proposed_method"}, {"score": 0.002232364384065567, "phrase": "driving_simulation_experiment"}, {"score": 0.0021049977753042253, "phrase": "adequate_reward_function"}], "paper_keywords": ["Incremental approach", " Reward recovering", " Online learning", " Inverse reinforcement learning", " Markov decision process"], "paper_abstract": "Interest in inverse reinforcement learning (IRL) has recently increased, that is, interest in the problem of recovering the reward function underlying a Markov decision process (MDP) given the dynamics of the system and the behavior of an expert. This paper deals with an incremental approach to online IRL. First, the convergence property of the incremental method for the IRL problem was investigated, and the bounds of both the mistake number during the learning process and regret were provided by using a detailed proof. Then an online algorithm based on incremental error correcting was derived to deal with the I R L problem. The key idea is to add an increment to the current reward estimate each time an action mismatch occurs. This leads to an estimate that approaches a target optimal value. The proposed method was tested in a driving simulation experiment and found to be able to efficiently recover an adequate reward function.", "paper_title": "Convergence analysis of an incremental approach to online inverse reinforcement learning", "paper_id": "WOS:000286548900002"}