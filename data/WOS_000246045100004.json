{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mpi_primitives"}, {"score": 0.04748383477648566, "phrase": "pc_clusters"}, {"score": 0.015050919878877815, "phrase": "interprocessor_communications"}, {"score": 0.013998476998573114, "phrase": "mpi"}, {"score": 0.013716422713538321, "phrase": "reconfigurable_computing"}, {"score": 0.004786821283037044, "phrase": "configurable_multiprocessors"}, {"score": 0.004635008910530911, "phrase": "widely_used_standard"}, {"score": 0.004581000737445643, "phrase": "parallel_computers"}, {"score": 0.004332894955218135, "phrase": "large_communication_latencies"}, {"score": 0.004307569753106432, "phrase": "limited_hardware_support"}, {"score": 0.004220086640742953, "phrase": "expensive_systems"}, {"score": 0.004146510711518748, "phrase": "rewarding_levels"}, {"score": 0.004074212307838049, "phrase": "programmable_parallel_systems"}, {"score": 0.00405039292112851, "phrase": "respectable_size"}, {"score": 0.0039449104968131655, "phrase": "specialized_components"}, {"score": 0.0038421645028127636, "phrase": "fpga-based_designs"}, {"score": 0.0037973594873521596, "phrase": "resulting_code"}, {"score": 0.0036660436272785476, "phrase": "performance_comparison"}, {"score": 0.003644601816390735, "phrase": "conventional_parallel_computers"}, {"score": 0.003487721546598799, "phrase": "similar_communication_libraries"}, {"score": 0.0034268702387746106, "phrase": "hardware_design"}, {"score": 0.0033968425231674262, "phrase": "directly_mpi_primitives"}, {"score": 0.0033769698684652646, "phrase": "configurable_multiprocessor_computing"}, {"score": 0.0033277936960179892, "phrase": "efficient_parallel_code_development"}, {"score": 0.0032601438574879666, "phrase": "underlying_hardware_implementation"}, {"score": 0.003175175896313016, "phrase": "mpi-based_code"}, {"score": 0.003038440237182784, "phrase": "one-sided_remote_memory_access"}, {"score": 0.0028484442403083608, "phrase": "rma."}, {"score": 0.0028069426877159664, "phrase": "universal_and_orthogonal_set"}, {"score": 0.0026624684547385718, "phrase": "low_latency"}, {"score": 0.002600661525224505, "phrase": "direct_interconnection"}, {"score": 0.0025328377393370642, "phrase": "-chip_systems"}, {"score": 0.0025180070278214613, "phrase": "experimental_results"}, {"score": 0.0024379730432321656, "phrase": "parallel_programming"}, {"score": 0.0024024377855956136, "phrase": "continuous_traffic"}, {"score": 0.002326068563512387, "phrase": "average_transmission_time"}, {"score": 0.0021805201234139475, "phrase": "exclusively_reconfigurable_multiprocessing"}, {"score": 0.002148729349266374, "phrase": "tremendous_attention"}], "paper_keywords": ["configurable system", " FPGA", " multiprocessor", " MPI"], "paper_abstract": "The Message Passing Interface (MPI) is a widely used standard for interprocessor communications in parallel computers and PC clusters. Its functions are normally implemented in software due to their enormity and complexity, thus resulting in large communication latencies. Limited hardware support for MPI is sometimes available in expensive systems. Reconfigurable computing has recently reached rewarding levels that enable the embedding of programmable parallel systems of respectable size inside one or more Field-Programmable Gate Arrays (FPGAs). Nevertheless, specialized components must be built to support interprocessor communications in these FPGA-based designs, and the resulting code may be difficult to port to other reconfigurable platforms. In addition, performance comparison with conventional parallel computers and PC clusters is very cumbersome or impossible since the latter often employ MPI or similar communication libraries. The introduction of a hardware design to implement directly MPI primitives in configurable multiprocessor computing creates a framework for efficient parallel code development involving data exchanges independently of the underlying hardware implementation. This process also supports the portability of MPI-based code developed for more conventional platforms. This paper takes advantage of the effectiveness and efficiency of one-sided Remote Memory Access (RMA) communications, and presents the design and evaluation of a coprocessor that implements a set of MPI primitives for RMA. These primitives form a universal and orthogonal set that can be used to implement any other MPI function. To evaluate the coprocessor, a router of low latency was designed as well to enable the direct interconnection of several coprocessors in cluster-on-a-chip systems. Experimental results justify the implementation of the MPI primitives in hardware to support parallel programming in reconfigurable computing. Under continuous traffic, results for a Xilinx XC2V6000 FPGA show that the average transmission time per 32-bit word is about 1.35 clock cycles. Although other computing platforms, such as PC clusters, could benefit as well from our design methodology, our focus is exclusively reconfigurable multiprocessing that has recently received tremendous attention in academia and industry. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Coprocessor design to support MPI primitives in configurable multiprocessors", "paper_id": "WOS:000246045100004"}