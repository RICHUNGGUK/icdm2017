{"auto_keywords": [{"score": 0.037728366596359864, "phrase": "weighted_distance"}, {"score": 0.03549120559414709, "phrase": "inter-prototype_relationship"}, {"score": 0.00481495049065317, "phrase": "nearest_neighbor_classification"}, {"score": 0.004764542405129454, "phrase": "cam_weighted_distance"}, {"score": 0.00471465954562773, "phrase": "nearest_neighbor"}, {"score": 0.004568108391981922, "phrase": "locally_constant_class_conditional_probabilities"}, {"score": 0.004402854153653114, "phrase": "high_dimensions"}, {"score": 0.00433386573276633, "phrase": "small_sample_set"}, {"score": 0.004133292222875435, "phrase": "novel_cam_weighted_distance"}, {"score": 0.0039006600544599537, "phrase": "existing_neighborhood-based_methods"}, {"score": 0.003799275066001196, "phrase": "small_space"}, {"score": 0.003720060615588018, "phrase": "query_sample"}, {"score": 0.0036617315529835497, "phrase": "proposed_nearest_neighbor_classification"}, {"score": 0.0034738037330064885, "phrase": "distance_measure"}, {"score": 0.0030772505522663612, "phrase": "different_surroundings"}, {"score": 0.003028969851366049, "phrase": "different_effects"}, {"score": 0.0029346624184926305, "phrase": "proposed_cam"}, {"score": 0.0027547408718334603, "phrase": "relevant_information"}, {"score": 0.002654911400878851, "phrase": "better_classification_performance"}, {"score": 0.002440071551819324, "phrase": "kappa-nearest_neighbor_classification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["classification", " nearest neighbors", " Cam distribution", " distance measure"], "paper_abstract": "Nearest neighbor (NN) classification assumes locally constant class conditional probabilities, and suffers from bias in high dimensions with a small sample set. In this paper, we propose a novel cam weighted distance to ameliorate the curse of dimensionality. Different from the existing neighborhood-based methods which only analyze a small space emanating from the query sample, the proposed nearest neighbor classification using the cam weighted distance (CamNN) optimizes the distance measure based on the analysis of inter-prototype relationship. Our motivation comes from the observation that the prototypes are not isolated. Prototypes with different Surroundings should have different effects in the classification. The proposed cam weighted distance is orientation and scale adaptive to take advantage of the relevant information of inter-prototype relationship, so that a better classification performance can be achieved. Experiments show that CamNN significantly Outperforms one nearest neighbor classification (1-NN) and kappa-nearest neighbor classification (kappa-NN) in most benchmarks, while its computational complexity is comparable with that of 1-NN classification. (c) 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.", "paper_title": "Improving nearest neighbor classification with cam weighted distance", "paper_id": "WOS:000235859200013"}