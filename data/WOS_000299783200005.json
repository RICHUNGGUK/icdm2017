{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "feature_space_and_music_genre_classification"}, {"score": 0.004717325943509345, "phrase": "music_genre_classification"}, {"score": 0.004558977045730675, "phrase": "statistical_characteristics"}, {"score": 0.004497129150551236, "phrase": "low-level_features"}, {"score": 0.004405919996432158, "phrase": "short_audio_frames"}, {"score": 0.004031495920060339, "phrase": "equally_relevant_information_loads"}, {"score": 0.003922794571908569, "phrase": "individual_frames"}, {"score": 0.0034215569542785907, "phrase": "representation_space"}, {"score": 0.003352088714126572, "phrase": "short-term_audio_features"}, {"score": 0.0032616462517547477, "phrase": "class_boundaries"}, {"score": 0.003173636240475185, "phrase": "different_processing_techniques"}, {"score": 0.0026565317818265394, "phrase": "randomized_and_unsupervised_partition"}, {"score": 0.002480826434008026, "phrase": "markov_model"}, {"score": 0.0022083264126633085, "phrase": "unsupervised_partitions"}], "paper_keywords": [""], "paper_abstract": "In music genre classification, most approaches rely on statistical characteristics of low-level features computed on short audio frames. In these methods, it is implicitly considered that frames carry equally relevant information loads and that either individual frames, or distributions thereof, somehow capture the specificities of each genre. In this paper we study the representation space defined by short-term audio features with respect to class boundaries, and compare different processing techniques to partition this space. These partitions are evaluated in terms of accuracy on two genre classification tasks, with several types of classifiers. Experiments show that a randomized and unsupervised partition of the space, used in conjunction with a Markov Model classifier lead to accuracies comparable to the state of the art. We also show that unsupervised partitions of the space tend to create less hubs.", "paper_title": "Short-term Feature Space and Music Genre Classification", "paper_id": "WOS:000299783200005"}