{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "spoken_dialogue_systems"}, {"score": 0.027891379201627115, "phrase": "original_interface"}, {"score": 0.004468942661513215, "phrase": "simple_tasks"}, {"score": 0.0044423558024348, "phrase": "current_systems"}, {"score": 0.004389653570743615, "phrase": "well-known_domains"}, {"score": 0.0041353355816051126, "phrase": "oil_ail_input_inter-face"}, {"score": 0.004061941280202248, "phrase": "semantic_analyser"}, {"score": 0.003919022096381048, "phrase": "response_generator"}, {"score": 0.0038956940500396086, "phrase": "speech_synthesiser"}, {"score": 0.003637161400331984, "phrase": "new_type"}, {"score": 0.003540714632602521, "phrase": "first_one"}, {"score": 0.003509135412622647, "phrase": "standard_speech_recogniser"}, {"score": 0.0033354158080573697, "phrase": "second_module"}, {"score": 0.0032664003336950243, "phrase": "recognised_sentence"}, {"score": 0.0032372597425374397, "phrase": "context_knowledge"}, {"score": 0.0031988083245396825, "phrase": "current_prompt"}, {"score": 0.003067778605343437, "phrase": "previously_developed_dialogue_system"}, {"score": 0.0029333222445143705, "phrase": "proposed_technique"}, {"score": 0.002907144481531093, "phrase": "experimental_results"}, {"score": 0.002746677144043828, "phrase": "new_interface"}, {"score": 0.002722160504975204, "phrase": "word_accuracy_and_sentence_understanding_rates"}, {"score": 0.0025642012084841886, "phrase": "clear_enhancement"}, {"score": 0.0024961348944169616, "phrase": "new_interface_analyses_sentences"}, {"score": 0.00231637194556478, "phrase": "real_dialogues_sentences"}, {"score": 0.002208162697208807, "phrase": "inexperienced_users"}, {"score": 0.0021367093107378847, "phrase": "system_performance"}], "paper_keywords": ["knowledge-based systems", " speech-based systems", " human-computer interactions", " dialogue systems", " automatic speech recognition", " speech understandings", " dialogue management"], "paper_abstract": "Spoken dialogue systems can be considered knowledge-based systems designed to interact with users using speech in order to provide information or carry out simple tasks. Current systems are restricted to well-known domains that provide knowledge about the words and sentences the users will likely utter. Basically. these systems rely oil ail input inter-face comprised of speech recogniser and semantic analyser. a dialogue manager, and an output interface comprised of response generator and speech synthesiser. As an attempt to enhance the performance (if the input interface, this paper proposes a technique based on a new type of speech recogniser comprised of two modules. The first one is a standard speech recogniser that receives the sentence uttered by the user and generates a graph of words. The second module analyses the graph and produces the recognised sentence using the context knowledge provided by the current prompt of the system. We evaluated the performance of two input interfaces working in a previously developed dialogue system: the original interface of the system and a new one that features the proposed technique. The experimental results show that when the sentences uttered by the users are out-of-context analysed by the new interface, the word accuracy and sentence understanding rates increase by 93.71 and 77.42% absolute, respectively. regarding the original interface. The price to pay for this clear enhancement is a little reduction in the scores when the new interface analyses sentences in-context. as they decrease by 2.05 and 3.41% absolute, respectively. in comparison with the original interface. Given that in real dialogues sentences may be out-of-context analysed. specially when they are uttered by inexperienced users. the technique can be very useful to enhance the system performance. (c) 2005 Elsevier B.V. All fights reserved.", "paper_title": "Two-level speech recognition to enhance the performance of spoken dialogue systems", "paper_id": "WOS:000239182800001"}