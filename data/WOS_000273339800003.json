{"auto_keywords": [{"score": 0.04898285994723956, "phrase": "svm"}, {"score": 0.043995232336843944, "phrase": "kernel_row_vectors"}, {"score": 0.028940589417097054, "phrase": "classification_accuracy"}, {"score": 0.00481495049065317, "phrase": "pruning_support_vector_machine_classifiers"}, {"score": 0.004526503177267299, "phrase": "high_computational_cost"}, {"score": 0.00445712058827745, "phrase": "potential_overfitting"}, {"score": 0.00415777266387528, "phrase": "one-to-one_correspondence"}, {"score": 0.0039694170761766226, "phrase": "pruning_work"}, {"score": 0.0038485929962942776, "phrase": "first_phase"}, {"score": 0.0038189638340666936, "phrase": "orthogonal_projections"}, {"score": 0.003548579306515579, "phrase": "second_phase"}, {"score": 0.0035076666315744525, "phrase": "previously_found_vectors"}, {"score": 0.0034405207528624983, "phrase": "crosswise_propagations"}, {"score": 0.0030994630902300133, "phrase": "high-dimensional_feature_space"}, {"score": 0.00297037471252423, "phrase": "local_minima"}, {"score": 0.0029361086955646625, "phrase": "different_parameters"}, {"score": 0.0028576796380512157, "phrase": "libsvm_software_platform"}, {"score": 0.0027492483892113, "phrase": "average_change"}, {"score": 0.002665465323614577, "phrase": "average_computation_time"}, {"score": 0.0025842289219860795, "phrase": "training_time"}, {"score": 0.002364172626241202, "phrase": "large_numbers"}, {"score": 0.002345944657572499, "phrase": "superabundant_svs"}, {"score": 0.0023278569001363263, "phrase": "trained_svms"}, {"score": 0.002283243905993669, "phrase": "synergistic_use"}], "paper_keywords": ["Kernels", " orthogonal projection", " pruning", " support vector machines (SVMs)"], "paper_abstract": "Support vector machine (SVM) classifiers often contain many SVs, which lead to high computational cost at runtime and potential overfitting. In this paper, a practical and effective method of pruning SVM classifiers is systematically developed. The kernel row vectors, with one-to-one correspondence to the SVs, are first organized into clusters. The pruning work is divided into two phases. In the first phase, orthogonal projections (OPs) are performed to find kernel row vectors that can be approximated by the others. In the second phase, the previously found vectors are removed, and crosswise propagations, which simply utilize the coefficients of OPs, are implemented within each cluster. The method circumvents the problem of explicitly discerning SVs in the high-dimensional feature space as the SVM formulation does, and does not involve local minima. With different parameters, 3000 experiments were run on the LibSVM software platform. After pruning 42% of the SVs, the average change in classification accuracy was only -0.7%, and the average computation time for removing one SV was 0.006 of the training time. In some scenarios, over 90% of the SVs were pruned with less than 0.1% reduction in classification accuracy. The experiments demonstrate the existence of large numbers of superabundant SVs in trained SVMs, and suggest a synergistic use of training and pruning in practice. Many SVMs already used in applications could be upgraded by pruning nearly half of their SVs.", "paper_title": "An Effective Method of Pruning Support Vector Machine Classifiers", "paper_id": "WOS:000273339800003"}