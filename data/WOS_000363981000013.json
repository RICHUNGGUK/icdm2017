{"auto_keywords": [{"score": 0.03478167905660463, "phrase": "ground_point-cloud"}, {"score": 0.00481495049065317, "phrase": "mobile_localization"}, {"score": 0.004773692073488661, "phrase": "depth_camera"}, {"score": 0.004732785510739708, "phrase": "real_scenarios"}, {"score": 0.004692227830462433, "phrase": "existing_robot_localization_methods"}, {"score": 0.004612147352416241, "phrase": "particular_characteristics"}, {"score": 0.004475266375001531, "phrase": "vertical_walls"}, {"score": 0.004361163586439822, "phrase": "loose_generality"}, {"score": 0.004123781267600542, "phrase": "domestic_environments"}, {"score": 0.0040186038781533946, "phrase": "autonomous_self-locating_robots"}, {"score": 0.003865826699096508, "phrase": "absolute_online_self-localization"}, {"score": 0.0036395944934310524, "phrase": "planar_ground"}, {"score": 0.003157174936641705, "phrase": "ground_detection_algorithm"}, {"score": 0.0031032109441552287, "phrase": "small_shifts"}, {"score": 0.00307657476125683, "phrase": "camera_orientation"}, {"score": 0.003037047249598515, "phrase": "robot_motion"}, {"score": 0.0029722900280316216, "phrase": "calibration_parameters"}, {"score": 0.0026004229197334875, "phrase": "particle_filter"}, {"score": 0.00253400005142168, "phrase": "novel_observation_model"}, {"score": 0.002437525632451029, "phrase": "ground_edges"}, {"score": 0.0024061887405214186, "phrase": "nearest_obstacles"}, {"score": 0.0023346234464901978, "phrase": "cost_function"}, {"score": 0.0022749744354271834, "phrase": "distance-to-obstacles_grid_map"}, {"score": 0.0022168460543913787, "phrase": "isr-cobot_robot"}, {"score": 0.0021601997088758957, "phrase": "different_scenarios"}, {"score": 0.0021049977753042253, "phrase": "working_hours"}], "paper_keywords": ["Kinect", " Ground detection", " Particle filter", " Point-cloud", " Indoor mobile robot", " Map-based position estimation", " RGB-depth sensor", " Boundary edges estimation", " Absolute self-localization"], "paper_abstract": "Existing robot localization methods often rely on particular characteristics of the environment, such as vertical walls. However, these approaches loose generality once the environment does not show that structure, e.g., in domestic environments, making the deployment of autonomous self-locating robots difficult. This paper addresses the problem of absolute online self-localization in a known map, where the only required structure in the environment is a planar ground. In particular, we rely on the transitions between the ground and any other non-planar structure. The approach is based on the ground point-cloud and plane model perceived by a depth-camera. The ground detection algorithm is robust to small shifts on camera orientation during the robot motion, by determining the calibration parameters on-the-fly. Then the edges of the ground point-cloud are estimated, which can be originated by obstacles in the environment. The localization is obtained using a particle filter fusing the odometry with a novel observation model reflecting the quality of the match between the ground edges and the nearest obstacles. For this purpose, a cost function was implemented based on a distance-to-obstacles grid map. Experimental results using the ISR-CoBot robot are presented, run in different scenarios, including a bookshop during working hours.", "paper_title": "Real-Time Ground-Plane Based Mobile Localization Using Depth Camera in Real Scenarios", "paper_id": "WOS:000363981000013"}