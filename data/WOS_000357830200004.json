{"auto_keywords": [{"score": 0.048000287634733727, "phrase": "text_classification"}, {"score": 0.00481495049065317, "phrase": "text_classification_using_compression-based"}, {"score": 0.004788327412856401, "phrase": "dissimilarity_measures"}, {"score": 0.0044800522651664695, "phrase": "appropriate_set"}, {"score": 0.004238307626560739, "phrase": "accurate_classification"}, {"score": 0.00392153537973826, "phrase": "careful_feature_engineering"}, {"score": 0.0038567860860362745, "phrase": "pre-processing_stage"}, {"score": 0.003668859054928118, "phrase": "emerging_context"}, {"score": 0.003628352506247126, "phrase": "massive_collections"}, {"score": 0.003588291565092195, "phrase": "electronic_texts"}, {"score": 0.0034324073240680213, "phrase": "efficient_methods"}, {"score": 0.0033384259516301223, "phrase": "information-theoretic_dissimilarity_measures"}, {"score": 0.003211145390854807, "phrase": "dissimilarity-based_representations"}, {"score": 0.003088702479841264, "phrase": "feature_design"}, {"score": 0.0029380909574836587, "phrase": "feature_space"}, {"score": 0.002905629026254221, "phrase": "universal_dissimilarity_measures"}, {"score": 0.002810373366887749, "phrase": "classical_classifiers"}, {"score": 0.0026732973536990373, "phrase": "reported_experimental_evaluation"}, {"score": 0.00262910358108176, "phrase": "proposed_methods"}, {"score": 0.002585638506308606, "phrase": "sentiment_polarity_analysis"}, {"score": 0.0021049977753042253, "phrase": "text_pre-processing"}], "paper_keywords": ["Text classidication", " text similarity measures", " relative entropy", " Ziv-Merhav method", " cross-parsing algorithm"], "paper_abstract": "Arguably, the most difficult task in text classification is to choose an appropriate set of features that allows machine learning algorithms to provide accurate classification. Most state-of-the-art techniques for this task involve careful feature engineering and a pre-processing stage, which may be too expensive in the emerging context of massive collections of electronic texts. In this paper, we propose efficient methods for text classification based on information-theoretic dissimilarity measures, which are used to define dissimilarity-based representations. These methods dispense with any feature design or engineering, by mapping texts into a feature space using universal dissimilarity measures; in this space, classical classifiers (e.g. nearest neighbor or support vector machines) can then be used. The reported experimental evaluation of the proposed methods, on sentiment polarity analysis and authorship attribution problems, reveals that it approximates, sometimes even outperforms previous state-of-the-art techniques, despite being much simpler, in the sense that they do not require any text pre-processing or feature engineering.", "paper_title": "Text Classification Using Compression-Based Dissimilarity Measures", "paper_id": "WOS:000357830200004"}