{"auto_keywords": [{"score": 0.031383332366980904, "phrase": "final_segmentation"}, {"score": 0.00481495049065317, "phrase": "solo_phrases"}, {"score": 0.004669845451682529, "phrase": "pitch-related_descriptors"}, {"score": 0.0044150318724093226, "phrase": "musical_audio_data"}, {"score": 0.0042819270655224916, "phrase": "solo_instrument_phrases"}, {"score": 0.004238452771031322, "phrase": "polyphonic_music"}, {"score": 0.004152818376630929, "phrase": "relevant_features"}, {"score": 0.003926105480202798, "phrase": "large_corpus"}, {"score": 0.0038862297330905836, "phrase": "audio_descriptors"}, {"score": 0.0037307208089736835, "phrase": "solo_and_non-solo_sections"}, {"score": 0.003581412258960711, "phrase": "five_best_features"}, {"score": 0.0034911335714333507, "phrase": "two-stage_algorithm"}, {"score": 0.0033685408148574876, "phrase": "boundary_candidates"}, {"score": 0.003334309053314909, "phrase": "local_changes"}, {"score": 0.0032172054825137866, "phrase": "fixed-length_segments"}, {"score": 0.0031521392193570846, "phrase": "desired_target_classes"}, {"score": 0.0027741761880779535, "phrase": "classical_pieces"}, {"score": 0.002718045523483502, "phrase": "full-length_recordings"}, {"score": 0.002649472341607234, "phrase": "commercially_available_audio"}, {"score": 0.002479150189700726, "phrase": "boundary_estimation"}, {"score": 0.0024414225109100672, "phrase": "new_evaluation_metrics"}, {"score": 0.002416589459425828, "phrase": "image_processing"}, {"score": 0.0023197517832705297, "phrase": "resulting_accuracy"}, {"score": 0.0022267859286766553, "phrase": "selected_features"}, {"score": 0.0021705796895687864, "phrase": "specific_task"}, {"score": 0.0021375377706211686, "phrase": "reasonable_results"}, {"score": 0.0021049977753042253, "phrase": "segmentation_problem"}], "paper_keywords": [""], "paper_abstract": "In this paper we present an algorithm for segmenting musical audio data. Our aim is to identify solo instrument phrases in polyphonic music. We extract relevant features from the audio to be input into our algorithm. A large corpus of audio descriptors was tested for its ability to discriminate between solo and non-solo sections, which resulted in a subset of five best features. We derived a two-stage algorithm that first creates a set of boundary candidates from local changes of these features and then classifies fixed-length segments according to the desired target classes. The output of the two stages is combined to derive the final segmentation and segment labels. Our system was trained and tested with excerpts from classical pieces and evaluated using full-length recordings, all taken from commercially available audio. We evaluated our algorithm by using precision and recall measurements for the boundary estimation and introduced new evaluation metrics from image processing for the final segmentation. Along with a resulting accuracy of 77%, we demonstrate that the selected features are discriminative for this specific task and achieve reasonable results for the segmentation problem.", "paper_title": "Detecting Solo Phrases in Music Using Spectral and Pitch-related Descriptors", "paper_id": "WOS:000274674900002"}