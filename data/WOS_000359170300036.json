{"auto_keywords": [{"score": 0.03804712672731131, "phrase": "original_dataset"}, {"score": 0.00481495049065317, "phrase": "neural_networks_for_sequential_data"}, {"score": 0.004713707271640319, "phrase": "approach"}, {"score": 0.0046221918332842995, "phrase": "hidden_markov_models"}, {"score": 0.004457309741979921, "phrase": "critical_role"}, {"score": 0.004417012437860011, "phrase": "unsupervised_pre-training_strategies"}, {"score": 0.00429828390000229, "phrase": "artificial_neural_networks"}, {"score": 0.0041637779492720295, "phrase": "existing_pre-training_methods"}, {"score": 0.004088807948906792, "phrase": "static_data"}, {"score": 0.003942877273973905, "phrase": "temporal_information"}, {"score": 0.00385431710959668, "phrase": "novel_approach"}, {"score": 0.0038194503801108324, "phrase": "pre-training_sequential_neural_networks"}, {"score": 0.0035677736570659813, "phrase": "learned_distribution"}, {"score": 0.0034717929086014636, "phrase": "smoothed_dataset"}, {"score": 0.0032135959043774085, "phrase": "connection_weights"}, {"score": 0.003170060683860338, "phrase": "better_region"}, {"score": 0.0031271133933467575, "phrase": "parameter_space"}, {"score": 0.0029076695661972114, "phrase": "novel_pre-training_approach"}, {"score": 0.002765776659294382, "phrase": "different_network_architectures"}, {"score": 0.0026913130726432645, "phrase": "proposed_method"}, {"score": 0.0025137856138000014, "phrase": "prediction_task"}, {"score": 0.002468450551616404, "phrase": "polyphonic_music"}, {"score": 0.002401972657236928, "phrase": "proposed_strategy"}, {"score": 0.0022231606165229235, "phrase": "different_hyperparameters"}, {"score": 0.0021632743048552536, "phrase": "proposed_pre-training_strategy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Sequential Data", " Hidden Markov Model", " Recurrent Neural Networks", " Pre-Training", " Curriculum learning", " RNNRBM"], "paper_abstract": "In the last few years, research highlighted the critical role of unsupervised pre-training strategies to improve the performance of artificial neural networks. However, the scope of existing pre-training methods is limited to static data, whereas many learning tasks require to deal with temporal information. We propose a novel approach to pre-training sequential neural networks that exploits a simpler, first-order Hidden Markov Model to generate an approximate distribution of the original dataset. The learned distribution is used to generate a smoothed dataset that is used for pre-training. In this way, it is possible to drive the connection weights in a better region of the parameter space, where subsequent fine-tuning on the original dataset can be more effective. This novel pre-training approach is model-independent and can be readily applied to different network architectures. The benefits of the proposed method, both in terms of accuracy and training times, are demonstrated on a prediction task using four datasets of polyphonic music. The flexibility of the proposed strategy is shown by applying it to two different recurrent neural network architectures, and we also empirically investigate the impact of different hyperparameters on the performance of the proposed pre-training strategy. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Neural Networks for Sequential Data: a Pre-training Approach based on Hidden Markov Models", "paper_id": "WOS:000359170300036"}