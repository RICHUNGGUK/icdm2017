{"auto_keywords": [{"score": 0.04968898347890439, "phrase": "irregular_algorithms"}, {"score": 0.00481495049065317, "phrase": "concurrent_schedulers"}, {"score": 0.004418601451254075, "phrase": "important_concern"}, {"score": 0.004382591802405206, "phrase": "parallel_programming"}, {"score": 0.004224130367280183, "phrase": "static_scheduling"}, {"score": 0.004121672175779706, "phrase": "dependence_graph"}, {"score": 0.0040052617265655035, "phrase": "dynamic_scheduling"}, {"score": 0.003972607172518598, "phrase": "independent_loop_iterations"}, {"score": 0.0039080939753109965, "phrase": "openmp."}, {"score": 0.003782174375003052, "phrase": "complex_functions"}, {"score": 0.003751331782523201, "phrase": "runtime_values"}, {"score": 0.0036156119373414067, "phrase": "-time_analysis"}, {"score": 0.0035278613436978933, "phrase": "independent_activities"}, {"score": 0.0033449479263613848, "phrase": "scheduling_policy"}, {"score": 0.0030818310926031024, "phrase": "parallel_programming_problem"}, {"score": 0.0029581155916843663, "phrase": "flexible_and_efficient_approach"}, {"score": 0.002898127432512101, "phrase": "scheduling_policies"}, {"score": 0.0028161764476743257, "phrase": "simple_compositional_specification_language"}, {"score": 0.002583931950389087, "phrase": "efficient_parallel_schedulers"}, {"score": 0.0024903420152789135, "phrase": "five_irregular_algorithms"}, {"score": 0.002275557501858879, "phrase": "right_scheduling_policy"}, {"score": 0.0021049977753042253, "phrase": "fixed-function_schedulers"}], "paper_keywords": ["Algorithms", " Performance", " Amorphous data-parallelism", " Irregular programs", " Multicore processors", " Optimistic parallelization", " Program synthesis", " Scheduling"], "paper_abstract": "Scheduling is the assignment of tasks or activities to processors for execution, and it is an important concern in parallel programming. Most prior work on scheduling has focused either on static scheduling of applications in which the dependence graph is known at compile-time or on dynamic scheduling of independent loop iterations such as in OpenMP. In irregular algorithms, dependences between activities are complex functions of runtime values so these algorithms are not amenable to compile-time analysis nor do they consist of independent activities. Moreover, the amount of work can vary dramatically with the scheduling policy. To handle these complexities, implementations of irregular algorithms employ carefully handcrafted, algorithm-specific schedulers but these schedulers are themselves parallel programs, complicating the parallel programming problem further. In this paper, we present a flexible and efficient approach for specifying and synthesizing scheduling policies for irregular algorithms. We develop a simple compositional specification language and show how it can concisely encode scheduling policies in the literature. Then, we show how to synthesize efficient parallel schedulers from these specifications. We evaluate our approach for five irregular algorithms on three multicore architectures and show that (1) the performance of some algorithms can improve by orders of magnitude with the right scheduling policy, and (2) for the same policy, the overheads of our synthesized schedulers are comparable to those of fixed-function schedulers.", "paper_title": "Synthesizing Concurrent Schedulers for Irregular Algorithms", "paper_id": "WOS:000290854400027"}