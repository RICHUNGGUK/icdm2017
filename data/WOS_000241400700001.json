{"auto_keywords": [{"score": 0.040834622919398475, "phrase": "marcus"}, {"score": 0.00481495049065317, "phrase": "neural_networks"}, {"score": 0.004744018246633455, "phrase": "near-identity_relation"}, {"score": 0.004707114206059166, "phrase": "shultz"}, {"score": 0.0046741260449047976, "phrase": "simple_syntactic_forms"}, {"score": 0.004628101576367193, "phrase": "computer_simulations"}, {"score": 0.004537401559396022, "phrase": "unstructured_neural-network_model"}, {"score": 0.004448471107649881, "phrase": "t._r."}, {"score": 0.004069280130346136, "phrase": "essential_features"}, {"score": 0.004029187256595917, "phrase": "infant_learning"}, {"score": 0.003989487818395848, "phrase": "simple_grammars"}, {"score": 0.00393066798511457, "phrase": "artificial_language"}, {"score": 0.003853583126988071, "phrase": "g._f."}, {"score": 0.003815607281455725, "phrase": "vijayan"}, {"score": 0.0037407704200033607, "phrase": "bandi_rao"}, {"score": 0.0031609252620168446, "phrase": "training_sentences"}, {"score": 0.0031297538950335233, "phrase": "knowledge-representation_analyses"}, {"score": 0.003023047748772801, "phrase": "duplicate_words"}, {"score": 0.002670719148310961, "phrase": "familiar_grammar"}, {"score": 0.0024306105794630246, "phrase": "vilcu"}, {"score": 0.0021793698531006197, "phrase": "syntactic_types"}, {"score": 0.002157857111130435, "phrase": "speech_sounds"}, {"score": 0.0021049977753042253, "phrase": "standard_statistical_tests"}], "paper_keywords": ["artificial grammars", " cascade-correlation", " connectionism", " generalization", " neural networks", " representation", " sonority", " syllables"], "paper_abstract": "Computer simulations show that an unstructured neural-network model [Shultz, T. R., & Bale, A. C. (2001). Infancy, 2, 501-536] covers the essential features of infant learning of simple grammars in an artificial language [Marcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M. (1999). Science, 283, 77-80], and generalizes to examples both outside and inside of the range of training sentences. Knowledge-representation analyses confirm that these networks discover that duplicate words in the sentences are nearly identical and that they use this near-identity relation to distinguish sentences that are consistent or inconsistent with a familiar grammar. Recent simulations that were claimed to show that this model did not really learn these grammars [Vilcu, M., & Hadley, R. F. (2005). Minds and Machines, 15, 359-382] confounded syntactic types with speech sounds and did not perform standard statistical tests of results.", "paper_title": "Neural networks discover a near-identity relation to distinguish simple syntactic forms", "paper_id": "WOS:000241400700001"}