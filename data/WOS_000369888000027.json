{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "global_convergence_of_online_limited_memory_bfgs."}, {"score": 0.004762202897268383, "phrase": "global_convergence"}, {"score": 0.004360412703519539, "phrase": "memory_version"}, {"score": 0.0040366412779632085, "phrase": "quasi-newton_method"}, {"score": 0.0039053194919285725, "phrase": "optimization_problems"}, {"score": 0.003820144626025562, "phrase": "stochastic_objectives"}, {"score": 0.0036553070575015344, "phrase": "large_scale_machine_learning"}, {"score": 0.0034975571855450343, "phrase": "lower_and_upper_bounds"}, {"score": 0.0033837135367113004, "phrase": "hessian_eigenvalues"}, {"score": 0.00327356326590461, "phrase": "sample_functions"}, {"score": 0.0029641011108505785, "phrase": "curvature_approximation_matrices"}, {"score": 0.0024842242921857705, "phrase": "optimal_arguments"}, {"score": 0.0022492129111567824, "phrase": "search_engine_advertising_problem_showcase_reductions"}, {"score": 0.0021049977753042253, "phrase": "stochastic_gradient_descent_algorithms"}], "paper_keywords": ["quasi-Newton methods", " large-scale optimization", " stochastic optimization"], "paper_abstract": "Global convergence of an online (stochastic) limited memory version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method for solving optimization problems with stochastic objectives that arise in large scale machine learning is established. Lower and upper bounds on the Hessian eigenvalues of the sample functions are shown to suffice to guarantee that the curvature approximation matrices have bounded determinants and traces, which, in turn, permits establishing convergence to optimal arguments with probability 1. Experimental evaluation on a search engine advertising problem showcase reductions in convergence time relative to stochastic gradient descent algorithms.", "paper_title": "Global Convergence of Online Limited Memory BFGS", "paper_id": "WOS:000369888000027"}