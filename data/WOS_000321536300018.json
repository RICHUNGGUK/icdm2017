{"auto_keywords": [{"score": 0.047274671926607295, "phrase": "mutual_information"}, {"score": 0.04678589180942574, "phrase": "mi"}, {"score": 0.03337193094180301, "phrase": "mse"}, {"score": 0.0278383532651973, "phrase": "optimal_number"}, {"score": 0.00481495049065317, "phrase": "new_histogram-based_estimation_technique"}, {"score": 0.004583579470330216, "phrase": "error_minimization"}, {"score": 0.004153520149706423, "phrase": "random_variables"}, {"score": 0.0040192915527630995, "phrase": "different_signal"}, {"score": 0.003975515793563275, "phrase": "image_processing_applications"}, {"score": 0.0038893838564184107, "phrase": "mi_estimation_techniques"}, {"score": 0.0037636590459094762, "phrase": "large_bias"}, {"score": 0.003702323283528696, "phrase": "high_mean"}, {"score": 0.0027691111795848183, "phrase": "error_accumulation_problem"}, {"score": 0.002738914205394527, "phrase": "traditional_methods"}, {"score": 0.0024276590297034064, "phrase": "speech_recognition_problem"}, {"score": 0.002388043660169452, "phrase": "computer_aided_diagnosis_problem"}, {"score": 0.0023107372754933887, "phrase": "proposed_approach"}, {"score": 0.0022359278646792153, "phrase": "selected_features"}, {"score": 0.00221153232497724, "phrase": "enhanced_classification_results"}, {"score": 0.002175436093794315, "phrase": "existing_approaches"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": [""], "paper_abstract": "Mutual Information (MI) has extensively been used as a measure of similarity or dependence between random variables (or parameters) in different signal and image processing applications. However, MI estimation techniques are known to exhibit a large bias, a high Mean Squared Error (MSE), and can computationally be very costly. In order to overcome these drawbacks, we propose here a novel fast and low MSE histogram-based estimation technique for the computation of entropy and the mutual information. By minimizing the MSE, the estimation avoids the error accumulation problem of traditional methods. We derive an expression for the optimal number of bins to estimate the MI for both continuous and discrete random variables. Experimental results from a speech recognition problem and a computer aided diagnosis problem show the power of the proposed approach in estimating the optimal number of selected features with enhanced classification results compared to existing approaches. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "A new histogram-based estimation technique of entropy and mutual information using mean squared error minimization", "paper_id": "WOS:000321536300018"}