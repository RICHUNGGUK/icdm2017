{"auto_keywords": [{"score": 0.04015450647819761, "phrase": "support_vectors"}, {"score": 0.004815251518020085, "phrase": "svm"}, {"score": 0.004459334186889869, "phrase": "art_kernel_learning_methods"}, {"score": 0.003986103192038901, "phrase": "unnecessarily_liberal_use"}, {"score": 0.003939296259436904, "phrase": "basis_functions"}, {"score": 0.002897117039227939, "phrase": "rvm"}, {"score": 0.0028630535573013686, "phrase": "regression_problems"}, {"score": 0.0022598838001173095, "phrase": "svr."}, {"score": 0.002130063060904761, "phrase": "svr"}, {"score": 0.0021049977753042253, "phrase": "rvr."}], "paper_keywords": ["Support vector machine (SVM)", " support vector regression (SVR)", " relevance vector machine (RVM)", " relevance vector regression (RVR)", " sparseness", " robustness", " outlier"], "paper_abstract": "Support vector machine (SVM) and relevance vector machine (RVM) are two state of the art kernel learning methods. But both methods have some disadvantages: although SVM is very robust against outliers, it makes unnecessarily liberal use of basis functions since the number of support vectors required typically grows linearly with the size of the training set; on the other hand the solution of RVM is astonishingly sparse, but its performance deteriorates significantly when the observations are contaminated by outliers. In this paper, we present a combination of SVM and RVM for regression problems, in which the two methods are concatenated: firstly, we train a support vector regression (SVR) machine on the full training set: then a relevance vector regression (RVR) machine is trained only on a subset consisting of support vectors, but whose target values are replaced by the predictions of SVR. Using this combination, we overcome the drawbacks of SVR and RVR. Experiments demonstrate SVR+RVR is both very sparse and robust.", "paper_title": "SVR plus RVR: A ROBUST SPARSE KERNEL METHOD FOR REGRESSION", "paper_id": "WOS:000283074200003"}