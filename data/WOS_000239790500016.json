{"auto_keywords": [{"score": 0.040940350360074255, "phrase": "evolutionary_computation"}, {"score": 0.03600243304416854, "phrase": "performance_evaluation"}, {"score": 0.00481495049065317, "phrase": "metaheuristics_design"}, {"score": 0.004649302313891199, "phrase": "methodological_papers"}, {"score": 0.004489327204473076, "phrase": "thorough_revision"}, {"score": 0.00444241196374042, "phrase": "research_methodology"}, {"score": 0.0040274132322789275, "phrase": "eiben"}, {"score": 0.003999292102785413, "phrase": "jelasity"}, {"score": 0.003794529932139987, "phrase": "hooker"}, {"score": 0.0036382660173943393, "phrase": "rardin"}, {"score": 0.0036128527616772848, "phrase": "uzsoy"}, {"score": 0.003587616378156851, "phrase": "j._heuristics"}, {"score": 0.0033919477901198716, "phrase": "nondeterministic_methods"}, {"score": 0.0033564619316337634, "phrase": "widely_studied_metaheuristics"}, {"score": 0.00329813918116714, "phrase": "ant_colony"}, {"score": 0.003218180455135933, "phrase": "new_experimental_protocols"}, {"score": 0.0031845067058084583, "phrase": "careful_and_thorough_analysis"}, {"score": 0.003096417317829492, "phrase": "strong_similarities"}, {"score": 0.0030002172602367682, "phrase": "learning_methods"}, {"score": 0.0028166655005289073, "phrase": "machine_learning"}, {"score": 0.002598333022293788, "phrase": "testing_datasets"}, {"score": 0.0025264171945423254, "phrase": "metaheuristics_evaluation"}, {"score": 0.0024137645015872952, "phrase": "experimental_practice"}, {"score": 0.0023387234705715154, "phrase": "optimization_algorithms"}, {"score": 0.002281971169774711, "phrase": "clear_separation"}, {"score": 0.002149799391146313, "phrase": "actual_evaluation"}, {"score": 0.0021049977753042253, "phrase": "proper_assessment"}], "paper_keywords": [""], "paper_abstract": "A number of methodological papers published during the last years testify that a need for a thorough revision of the research methodology is felt by the operations research community - see, for example, [Barr et al., J. Heuristics 1 (1995) 9-32; Eiben and Jelasity, Proceedings of the 2002 Congress on Evolutionary Computation (CEC'2002) 582-587; Hooker, J. Heuristics 1 (1995) 33-42; Rardin and Uzsoy, J. Heuristics 7 (2001) 261-304]. In particular, the performance evaluation of nondeterministic methods, including widely studied metaheuristics such as evolutionary computation and ant colony optimization, requires the definition of new experimental protocols. A careful and thorough analysis of the problem of evaluating metaheuristics reveals strong similarities between this problem and the problem of evaluating learning methods in the machine learning field. In this paper, we show that several conceptual tools commonly used in machine learning - such as, for example, the probabilistic notion of class of instances and the separation between the training and the testing datasets - fit naturally in the context of metaheuristics evaluation. Accordingly, we propose and discuss some principles inspired by the experimental practice in machine learning for guiding the performance evaluation of optimization algorithms. Among these principles, a clear separation between the instances that are used for tuning algorithms and those that are used in the actual evaluation is particularly important for a proper assessment.", "paper_title": "Towards a theory of practice in metaheuristics design: A machine learning perspective", "paper_id": "WOS:000239790500016"}