{"auto_keywords": [{"score": 0.04193874382817036, "phrase": "validity_windows"}, {"score": 0.027740271023772693, "phrase": "posterior_distribution"}, {"score": 0.008351832725773806, "phrase": "algebraic_tails"}, {"score": 0.00481495049065317, "phrase": "inference_from_aging_information"}, {"score": 0.004652016085325825, "phrase": "data_collection"}, {"score": 0.004533427352547707, "phrase": "time_scale"}, {"score": 0.0044369056710423065, "phrase": "underlying_data_distribution"}, {"score": 0.004141575435071002, "phrase": "ad_hoc_methods"}, {"score": 0.003933000051147382, "phrase": "learning_machine"}, {"score": 0.0038326698261576023, "phrase": "old_data"}, {"score": 0.0035162927451540065, "phrase": "new_adaptive_bayesian"}, {"score": 0.0034118234596993836, "phrase": "drifting_concepts"}, {"score": 0.003267925215811654, "phrase": "adaptive_bayesian_way"}, {"score": 0.0031845067058084583, "phrase": "data_distribution"}, {"score": 0.0030898642396380662, "phrase": "theoretical_approach"}, {"score": 0.003050166508545763, "phrase": "information_geometry"}, {"score": 0.0030109772626153797, "phrase": "classification_problem"}, {"score": 0.0028715299874549245, "phrase": "appropriate_size"}, {"score": 0.0028346293790895024, "phrase": "memory_windows"}, {"score": 0.0027622407190589326, "phrase": "bayesian_manner"}, {"score": 0.002668582249630492, "phrase": "adaptive_window_size"}, {"score": 0.0021601997088758957, "phrase": "evolving_environment"}, {"score": 0.0021049977753042253, "phrase": "local_traps"}], "paper_keywords": ["Online Bayesian algorithms", " pattern classification", " time-varying environment"], "paper_abstract": "For many learning tasks the duration of the data collection can be greater than the time scale for changes of the underlying data distribution. The question we ask is how to include the information that data are aging. Ad hoc methods to achieve this include the use of validity windows that prevent the learning machine from making inferences based on old data. This introduces the problem of how to define the size of validity windows. In this brief, a new adaptive Bayesian inspired algorithm is presented for learning drifting concepts. It uses the analogy of validity windows in an adaptive Bayesian way to incorporate changes in the data distribution over time. We apply a theoretical approach based on information geometry to the classification problem and measure its performance in simulations. The uncertainty about the appropriate size of the memory windows is dealt with in a Bayesian manner by integrating over the distribution of the adaptive window size. Thus, the posterior distribution of the weights may develop algebraic tails. The learning algorithm results from tracking the mean and variance of the posterior distribution of the weights. It was found that the algebraic tails of this posterior distribution give the learning algorithm the ability to cope with an evolving environment by permitting the escape from local traps.", "paper_title": "Inference From Aging Information", "paper_id": "WOS:000278537600013"}