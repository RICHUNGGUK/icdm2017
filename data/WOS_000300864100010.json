{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "limited_rank_matrix_learning"}, {"score": 0.004386063799793622, "phrase": "recently_introduced_generalized_matrix_learning_vector_quantization_algorithm"}, {"score": 0.004270665568018583, "phrase": "original_scheme"}, {"score": 0.00421410521089045, "phrase": "adaptive_square_matrices"}, {"score": 0.004158290806404304, "phrase": "relevance_factors"}, {"score": 0.004075946554498088, "phrase": "discriminative_distance_measure"}, {"score": 0.0038385317554884713, "phrase": "limited_rank"}, {"score": 0.0037624955853577786, "phrase": "low-dimensional_representations"}, {"score": 0.003543273649210823, "phrase": "prior_knowledge"}, {"score": 0.0034730657554172405, "phrase": "intrinsic_dimension"}, {"score": 0.003314591878914152, "phrase": "adaptive_parameters"}, {"score": 0.0029789104204803137, "phrase": "computation_time"}, {"score": 0.0029394074636168435, "phrase": "memory_requirements"}, {"score": 0.002842920225545139, "phrase": "two-_or_three-dimensional_representations"}, {"score": 0.0027131212233443137, "phrase": "labeled_data_sets"}, {"score": 0.00260657976055413, "phrase": "suitable_projection"}, {"score": 0.0025042115651986332, "phrase": "pre-processing_step"}, {"score": 0.002438203996977464, "phrase": "integral_part"}, {"score": 0.002389839896897424, "phrase": "supervised_training"}, {"score": 0.0021910825909008946, "phrase": "suggested_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Learning Vector Quantization", " Classification", " Visualization", " Adaptive metrics", " Dimension reduction"], "paper_abstract": "We present an extension of the recently introduced Generalized Matrix Learning Vector Quantization algorithm. In the original scheme, adaptive square matrices of relevance factors parameterize a discriminative distance measure. We extend the scheme to matrices of limited rank corresponding to low-dimensional representations of the data. This allows to incorporate prior knowledge of the intrinsic dimension and to reduce the number of adaptive parameters efficiently. In particular, for very large dimensional data, the limitation of the rank can reduce computation time and memory requirements significantly. Furthermore, two- or three-dimensional representations constitute an efficient visualization method for labeled data sets. The identification of a suitable projection is not treated as a pre-processing step but as an integral part of the supervised training. Several real world data sets serve as an illustration and demonstrate the usefulness of the suggested method. (c) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Limited Rank Matrix Learning, discriminative dimension reduction and visualization", "paper_id": "WOS:000300864100010"}