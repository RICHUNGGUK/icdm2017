{"auto_keywords": [{"score": 0.04423467024884345, "phrase": "expected_approximation_ratio"}, {"score": 0.00481495049065317, "phrase": "bad_instance"}, {"score": 0.004440277230269978, "phrase": "seeding_technique"}, {"score": 0.004333642054036135, "phrase": "k-means_method"}, {"score": 0.003289534854212415, "phrase": "upper_bound"}, {"score": 0.002936331371903168, "phrase": "constant_approximation"}, {"score": 0.002751751956416832, "phrase": "constant_probability"}, {"score": 0.002599763359352916, "phrase": "present_instances"}, {"score": 0.0024761702541879213, "phrase": "approximation_ratio"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Clustering", " k-means", " k-means plus"], "paper_abstract": "k-means++ is a seeding technique for the k-means method with an expected approximation ratio of O(log k), where k denotes the number of clusters. Examples are known on which the expected approximation ratio of k-means++ is Omega(log k), showing that the upper bound is asymptotically tight. However, it remained open whether k-means++ yields a constant approximation with probability 1/poly(k) or even with constant probability. We settle this question and present instances on which k-means++ achieves an approximation ratio no better than (2/3 - epsilon) . log k with probability exponentially close to 1. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "A bad instance for k-means plus", "paper_id": "WOS:000325905200004"}