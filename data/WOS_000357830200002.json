{"auto_keywords": [{"score": 0.0495648351427175, "phrase": "elite_samples"}, {"score": 0.046345717877726705, "phrase": "deep_architectures"}, {"score": 0.040918691207282594, "phrase": "training_data"}, {"score": 0.00481495049065317, "phrase": "deep_belief_network_training_improvement"}, {"score": 0.004392565073466714, "phrase": "machine_learning"}, {"score": 0.004345363360869553, "phrase": "deep_belief_networks"}, {"score": 0.004116823963855969, "phrase": "restricted_boltzmann_machines"}, {"score": 0.003964007795174772, "phrase": "powerful_generative_model"}, {"score": 0.0036553070575015344, "phrase": "common_method"}, {"score": 0.0035386420095120706, "phrase": "training_rbms"}, {"score": 0.0034817084307034955, "phrase": "new_method"}, {"score": 0.0034442607586033657, "phrase": "free_energy"}, {"score": 0.003298441494378465, "phrase": "generative_model"}, {"score": 0.003107935254560724, "phrase": "log_probability"}, {"score": 0.0029126002350975634, "phrase": "error_rate"}, {"score": 0.0028195727965553367, "phrase": "mnist_test_set"}, {"score": 0.002714779217561487, "phrase": "proposed_method"}, {"score": 0.002613870237087035, "phrase": "first_paper"}, {"score": 0.0024232901435244955, "phrase": "svm"}, {"score": 0.002198173002125907, "phrase": "isolet_dataset"}, {"score": 0.0021745003291048356, "phrase": "letter_classification_error"}], "paper_keywords": ["Deep belief network", " restricted Boltzmann machine", " Gibbs sampling", " contrastive divergence (CD)", " persistent contrastive divergence (PCD)", " free energy"], "paper_abstract": "Nowadays, it is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBMs) to create a powerful generative model using the training data. In this paper, we present an improvement in a common method that is usually used in training RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can compute gradient of log probability of training data more accurately. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in the papers using this dataset.", "paper_title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy", "paper_id": "WOS:000357830200002"}