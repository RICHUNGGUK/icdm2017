{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "visual_perceptual_components"}, {"score": 0.004658426838467567, "phrase": "implementation_details"}, {"score": 0.0045820747375409435, "phrase": "patient_status_awareness"}, {"score": 0.0045318666714939905, "phrase": "human_activity_interpretation"}, {"score": 0.004482206282876944, "phrase": "emergency_detection"}, {"score": 0.00433645229352341, "phrase": "personal_health"}, {"score": 0.004081385289516914, "phrase": "proposed_system"}, {"score": 0.003820144626025562, "phrase": "patient's_body"}, {"score": 0.003778253716938922, "phrase": "appropriate_body_sensors"}, {"score": 0.003716273882758735, "phrase": "surrounding_environment"}, {"score": 0.0036553070575015344, "phrase": "overhead_cameras"}, {"score": 0.003615217162972726, "phrase": "microphone_arrays"}, {"score": 0.0035755653799043, "phrase": "appropriate_tracking_techniques"}, {"score": 0.003478321590910367, "phrase": "visual_perceptual_component"}, {"score": 0.003421245054271364, "phrase": "trajectory_tracking"}, {"score": 0.003309877074892399, "phrase": "audio_data_processing"}, {"score": 0.00327356326590461, "phrase": "sound_directionality_analysis"}, {"score": 0.0032021226849872054, "phrase": "motion_information"}, {"score": 0.0031669873302916, "phrase": "subject's_visual_location"}, {"score": 0.0030470127429039497, "phrase": "emergency_event"}, {"score": 0.002996992362027833, "phrase": "postfall_visual_and_motion_behavior"}, {"score": 0.0025678829712650437, "phrase": "semantic_representation"}, {"score": 0.0025257081555100556, "phrase": "patient's_status"}, {"score": 0.002470547930651519, "phrase": "rules-based_evaluation"}, {"score": 0.0024299678474208023, "phrase": "advanced_classification"}, {"score": 0.002363806685815014, "phrase": "advanced_classification_techniques"}, {"score": 0.0021049977753042253, "phrase": "emergency_situation"}], "paper_keywords": ["Activity recognition", " assisted living environments", " body sensors", " context awareness", " event detection", " human safety", " patient telemonitoring"], "paper_abstract": "This paper presents the implementation details of a patient status awareness enabling human activity interpretation and emergency detection in cases, where the personal health is threatened like elder falls or patient collapses. The proposed system utilizes video, audio, and motion data captured from the patient's body using appropriate body sensors and the surrounding environment, using overhead cameras and microphone arrays. Appropriate tracking techniques are applied to the visual perceptual component enabling the trajectory tracking of persons, while proper audio data processing and sound directionality analysis in conjunction to motion information and subject's visual location can verify fall and indicate an emergency event. The postfall visual and motion behavior of the subject, which indicates the severity of the fall (e.g., if the person remains unconscious or patient recovers) is performed through a semantic representation of the patient's status, context and rules-based evaluation, and advanced classification. A number of advanced classification techniques have been examined in the framework of this study and their corresponding performance in terms of accuracy and efficiency in detecting an emergency situation has been thoroughly assessed.", "paper_title": "Emergency Fall Incidents Detection in Assisted Living Environments Utilizing Motion, Sound, and Visual Perceptual Components", "paper_id": "WOS:000288082200013"}