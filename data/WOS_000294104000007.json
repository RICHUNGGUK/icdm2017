{"auto_keywords": [{"score": 0.04612581829700839, "phrase": "emotion_recognition"}, {"score": 0.00481495049065317, "phrase": "parent-infant_interaction_analysis"}, {"score": 0.004612376827433774, "phrase": "infant-directed_speech_discrimination_system"}, {"score": 0.004460703838987562, "phrase": "different_feature_sets"}, {"score": 0.0041522059428827345, "phrase": "classification_experiments"}, {"score": 0.004054169947971483, "phrase": "short_pre-segmented_adult-directed_speech"}, {"score": 0.0038834787946979863, "phrase": "real-life_family_home_movies"}, {"score": 0.0037199473428123175, "phrase": "experimental_results"}, {"score": 0.003597514862052876, "phrase": "supervised_learning"}, {"score": 0.0035632774896594524, "phrase": "spectral_features"}, {"score": 0.0035125292519354724, "phrase": "major_role"}, {"score": 0.0034625012596827334, "phrase": "infant-directed_speech_discrimination"}, {"score": 0.0033806943112103397, "phrase": "major_difficulty"}, {"score": 0.003332537728389913, "phrase": "natural_corpora"}, {"score": 0.00326939082861424, "phrase": "annotation_process"}, {"score": 0.0030140547908298404, "phrase": "acted_speech"}, {"score": 0.0029569247903654477, "phrase": "interlabeler_agreement_and_annotation_label_confidences"}, {"score": 0.002752139271586703, "phrase": "new_semi-supervised_approach"}, {"score": 0.002699960312990493, "phrase": "standard_co-training_algorithm"}, {"score": 0.002512926336312949, "phrase": "supervised_classifiers"}, {"score": 0.0024771000236931836, "phrase": "different_features"}, {"score": 0.0024417832296664698, "phrase": "proposed_dynamic_weighted_co-training_approach"}, {"score": 0.0022725931637071852, "phrase": "different_views"}, {"score": 0.0021354538918029286, "phrase": "real-life_corpus"}, {"score": 0.0021049977753042253, "phrase": "home_movies"}], "paper_keywords": ["Infant-directed speech", " Emotion recognition", " Face-to-face interaction", " Data fusion", " Semi-supervised learning"], "paper_abstract": "This paper describes the development of an infant-directed speech discrimination system for parent infant interaction analysis. Different feature sets for emotion recognition were investigated using two classification techniques: supervised and semi-supervised. The classification experiments were carried out with short pre-segmented adult-directed speech and infant-directed speech segments extracted from real-life family home movies (with durations typically between 0.5 s and 4 s). The experimental results show that in the case of supervised learning, spectral features play a major role in the infant-directed speech discrimination. However, a major difficulty of using natural corpora is that the annotation process is time-consuming, and the expression of emotion is much more complex than in acted speech. Furthermore, interlabeler agreement and annotation label confidences are important issues to address. To overcome these problems, we propose a new semi-supervised approach based on the standard co-training algorithm exploiting labelled and unlabelled data. It offers a framework to take advantage of supervised classifiers trained by different features. The proposed dynamic weighted co-training approach combines various features and classifiers usually used in emotion recognition in order to learn from different views. Our experiments demonstrate the validity and effectiveness of this method for a real-life corpus such as home movies. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Supervised and semi-supervised infant-directed speech classification for parent-infant interaction analysis", "paper_id": "WOS:000294104000007"}