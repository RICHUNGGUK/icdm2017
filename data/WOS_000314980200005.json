{"auto_keywords": [{"score": 0.0455567754671588, "phrase": "high_confidence"}, {"score": 0.03744282073700833, "phrase": "confidence_annotator"}, {"score": 0.03251720662140616, "phrase": "unlabeled_data"}, {"score": 0.00481495049065317, "phrase": "semi-supervised_learning"}, {"score": 0.004731638279336065, "phrase": "unlabeled_data_with_symmetrical_distribution_and_high_confidence._current_existing_representative_works"}, {"score": 0.004310949776826717, "phrase": "model_retraining"}, {"score": 0.004090901264701185, "phrase": "classification_performance"}, {"score": 0.003837089839193524, "phrase": "data_selection"}, {"score": 0.003726967166630923, "phrase": "erroneous_estimate"}, {"score": 0.003662410853488772, "phrase": "true_distribution"}, {"score": 0.0032978134802348433, "phrase": "new_semi-supervised_incremental_learning_algorithm"}, {"score": 0.0031660031870090434, "phrase": "high_confidence_unlabeled_instances"}, {"score": 0.00278497898719426, "phrase": "expectation_maximization_algorithm"}, {"score": 0.0025967927629150715, "phrase": "gaussian_function"}, {"score": 0.002507511169156787, "phrase": "data_distribution"}, {"score": 0.0024496978447849835, "phrase": "selected_unlabeled_data"}, {"score": 0.0023932142697579506, "phrase": "retraining_model"}, {"score": 0.0023654615987859402, "phrase": "classifier_algorithm"}, {"score": 0.002324433454350402, "phrase": "experimental_results"}, {"score": 0.0022708315711029423, "phrase": "large_number"}, {"score": 0.002244494905883452, "phrase": "uci_data_sets"}, {"score": 0.0021049977753042253, "phrase": "learning_performance"}], "paper_keywords": ["Semi-supervised learning", " incremental learning", " unlabeled data", " high confidence", " symmetrical distribution"], "paper_abstract": "Current existing representative works to semi-supervised incremental learning prefer to select unlabeled instances predicted with high confidence for model retraining. However, this strategy may degrade the classification performance rather than improve it, because relying on high confidence for data selection can lead to an erroneous estimate to the true distribution, especially when the confidence annotator is highly correlated with the confidence annotator. In this paper, a new semi-supervised incremental learning algorithm was proposed, which selected the high confidence unlabeled instances with symmetrical distribution from unlabeled data, it can reduce the bias in the estimation in some degree. In detail, expectation maximization algorithm was used to estimate the confidence of each instance, and Gaussian function was used to calculate the data distribution, then the selected unlabeled data was used for retraining model with classifier algorithm. The experimental results based on a large number of UCI data sets show that our algorithm can effectively exploit unlabeled data to enhance the learning performance.", "paper_title": "SEMI-SUPERVISED LEARNING: EXPLOITING UNLABELED DATA WITH SYMMETRICAL DISTRIBUTION AND HIGH CONFIDENCE", "paper_id": "WOS:000314980200005"}