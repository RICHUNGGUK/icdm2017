{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "expert_judgments"}, {"score": 0.00770840152612731, "phrase": "parameter_estimation"}, {"score": 0.00473509474652851, "phrase": "hardest_challenges"}, {"score": 0.0045946617913925755, "phrase": "node_probability_tables"}, {"score": 0.00447331713604276, "phrase": "fixed_predefined_model_structure"}, {"score": 0.004413846562278824, "phrase": "relevant_data"}, {"score": 0.004384407114103125, "phrase": "machine_learning_methods"}, {"score": 0.004311660976562745, "phrase": "great_accuracy"}, {"score": 0.0042543303585113965, "phrase": "ground_truth"}, {"score": 0.004197758831410097, "phrase": "npt_entries"}, {"score": 0.0039655692352351625, "phrase": "learning_process"}, {"score": 0.003899745240916238, "phrase": "multinomial_parameter_learning_method"}, {"score": 0.0037461743696712833, "phrase": "parameter_learning_process"}, {"score": 0.003671663469605366, "phrase": "auxiliary_bn_model"}, {"score": 0.0035270425911920595, "phrase": "continuous_variables"}, {"score": 0.003399466007862412, "phrase": "iterative_discretization_technique"}, {"score": 0.003013237619840709, "phrase": "well-known_sample_bn_models"}, {"score": 0.0029731325085329075, "phrase": "asia"}, {"score": 0.00287514090358681, "phrase": "real-world_software_defects_prediction_bn_model"}, {"score": 0.0027525658515943985, "phrase": "state-of-the-art_machine"}, {"score": 0.0026088355715432523, "phrase": "software_defects"}, {"score": 0.0025740882809779913, "phrase": "sample_size"}, {"score": 0.0024396547357855777, "phrase": "small_number"}, {"score": 0.00242335068996852, "phrase": "real_expert_constraints"}, {"score": 0.002213596068643924, "phrase": "standard_machine_learning_method"}, {"score": 0.00216950156199795, "phrase": "directly_competing_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Bayesian networks", " Multinomial parameter learning", " Expert judgments"], "paper_abstract": "One of the hardest challenges in building a realistic Bayesian Network (BN) model is to construct the node probability tables (NPTs). Even with a fixed predefined model structure and very large amounts of relevant data, machine learning methods do not consistently achieve great accuracy compared to the ground truth when learning the NPT entries (parameters). Hence, it is widely believed that incorporating expert judgments can improve the learning process. We present a multinomial parameter learning method, which can easily incorporate both expert judgments and data during the parameter learning process. This method uses an auxiliary BN model to learn the parameters of a given BN. The auxiliary BN contains continuous variables and the parameter estimation amounts to updating these variables using an iterative discretization technique. The expert judgments are provided in the form of constraints on parameters divided into two categories: linear inequality constraints and approximate equality constraints. The method is evaluated with experiments based on a number of well-known sample BN models (such as Asia, Alarm and Hailfinder) as well as a real-world software defects prediction BN model. Empirically, the new method achieves much greater learning accuracy (compared to both state-of-the-art machine learning techniques and directly competing methods) with much less data. For example, in the software defects BN for a sample size of 20 (which would be considered difficult to collect in practice) when a small number of real expert constraints are provided, our method achieves a level of accuracy in parameter estimation that can only be matched by other methods with much larger sample sizes (320 samples required for the standard machine learning method, and 105 for the directly competing method with constraints). (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Bayesian network approach to multinomial parameter learning using data and expert judgments", "paper_id": "WOS:000336712800009"}