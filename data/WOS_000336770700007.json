{"auto_keywords": [{"score": 0.03579084835283085, "phrase": "task-level_memoization"}, {"score": 0.01325647462080072, "phrase": "hadoop"}, {"score": 0.004815007698069039, "phrase": "mapreduce"}, {"score": 0.004744437370338926, "phrase": "important_property"}, {"score": 0.004698000536608171, "phrase": "today's_big_data_processing"}, {"score": 0.004342430078306461, "phrase": "web_and_social_network_data"}, {"score": 0.004257806363128601, "phrase": "full_computation"}, {"score": 0.004195418010131832, "phrase": "entire_datasets"}, {"score": 0.0041136473765874815, "phrase": "distributed_computing_frameworks"}, {"score": 0.003584025547693836, "phrase": "conventional_mapreduce_algorithms"}, {"score": 0.003428665921276366, "phrase": "similar_goal"}, {"score": 0.003200230621987751, "phrase": "coarse-grained_level"}, {"score": 0.0030464123943266673, "phrase": "hadup"}, {"score": 0.002899965889623787, "phrase": "fine-grained_level"}, {"score": 0.0028574164693918433, "phrase": "deduplication-based_snapshot_differential_algorithm"}, {"score": 0.002640781900605126, "phrase": "high_performance"}, {"score": 0.0024405312482831646, "phrase": "extra_programming_cost"}, {"score": 0.0022005118194652704, "phrase": "hadup_applications"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Big data processing", " Incremental processing", " MapReduce", " Hadoop", " Data deduplication"], "paper_abstract": "An important property of today's big data processing is that the same computation is often repeated on datasets evolving over time, such as web and social network data. While repeating full computation of the entire datasets is feasible with distributed computing frameworks such as Hadoop, it is obviously inefficient and wastes resources. In this paper, we present HadUP (Hadoop with Update Processing), a modified Hadoop architecture tailored to large-scale incremental processing with conventional MapReduce algorithms. Several approaches have been proposed to achieve a similar goal using task-level memoization. However, task-level memoization detects the change of datasets at a coarse-grained level, which often makes such approaches ineffective. Instead, HadUP detects and computes the change of datasets at a fine-grained level using a deduplication-based snapshot differential algorithm (D-SD) and update propagation. As a result, it provides high performance, especially in an environment where task-level memoization has no benefit. HadUP requires only a small amount of extra programming cost because it can reuse the code for the map and reduce functions of Hadoop. Therefore, the development of HadUP applications is quite easy. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Large-scale incremental processing with MapReduce", "paper_id": "WOS:000336770700007"}