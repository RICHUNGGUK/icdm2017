{"auto_keywords": [{"score": 0.047678689072811614, "phrase": "facs"}, {"score": 0.00481495049065317, "phrase": "facial_action_units"}, {"score": 0.004782455247247007, "phrase": "facial_action_coding_system"}, {"score": 0.00468627326602559, "phrase": "de_facto_standard"}, {"score": 0.004607594060491493, "phrase": "facial_expressions"}, {"score": 0.004409127897028587, "phrase": "atomic_units"}, {"score": 0.0043793594257092805, "phrase": "action_units"}, {"score": 0.004247858437831649, "phrase": "au_intensity"}, {"score": 0.004176509019758331, "phrase": "nonlinear_scale"}, {"score": 0.00414830454375092, "phrase": "five_grades"}, {"score": 0.004078620439629185, "phrase": "significant_progress"}, {"score": 0.0039028497942174777, "phrase": "companion_problem"}, {"score": 0.003850299959557491, "phrase": "au_strengths"}, {"score": 0.003684332524184009, "phrase": "novel_au_intensity_estimation_scheme"}, {"score": 0.0035374660789922715, "phrase": "selected_image_features"}, {"score": 0.0033393377529146893, "phrase": "au_detection_algorithm"}, {"score": 0.0032061793819537633, "phrase": "sole_purpose"}, {"score": 0.0031845067058084583, "phrase": "intensity_estimation"}, {"score": 0.003036832649117453, "phrase": "mean_curvature"}, {"score": 0.0030163012350270025, "phrase": "gaussian_curvature"}, {"score": 0.0029959082125682918, "phrase": "shape_index"}, {"score": 0.002895986736522535, "phrase": "feature_selection"}, {"score": 0.002866663406866159, "phrase": "initial_plethora"}, {"score": 0.0028472791870440065, "phrase": "gabor_moments"}, {"score": 0.0027616550517346066, "phrase": "au_intensity_predictions"}, {"score": 0.0025370474510475628, "phrase": "state-of-the-art_method"}, {"score": 0.0024440836179156593, "phrase": "au_detection"}, {"score": 0.0023148964315569866, "phrase": "relative_merits"}, {"score": 0.0022992346264367374, "phrase": "upper_face"}, {"score": 0.0022836785412390544, "phrase": "lower_face"}, {"score": 0.0022074592164608134, "phrase": "overall_improvement"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action unit intensity estimation", " 3D facial expression recognition", " Facial Action Coding System", " Feature selection", " AdaBoost.RT", " SVM regression"], "paper_abstract": "Facial Action Coding System (FACS) is the de facto standard in the analysis of facial expressions. FACS describes expressions in terms of the configuration and strength of atomic units called Action Units: AUs. FACS defines 44 AUs and each AU intensity is defined on a nonlinear scale of five grades. There has been significant progress in the literature on the detection of AUs. However, the companion problem of estimating the AU strengths has not been much investigated. In this work we propose a novel AU intensity estimation scheme applied to 2D luminance and/or 3D surface geometry images. Our scheme is based on regression of selected image features. These features are either non-specific, that is, those inherited from the AU detection algorithm, or are specific in that they are selected for the sole purpose of intensity estimation. For thoroughness, various types of local 3D shape indicators have been considered, such as mean curvature, Gaussian curvature, shape index and curvedness, as well as their fusion. The feature selection from the initial plethora of Gabor moments is instrumented via a regression that optimizes the AU intensity predictions. Our AU intensity estimator is person-independent and when tested on 25 AUs that appear singly or in various combinations, it performs significantly better than the state-of-the-art method which is based on the margins of SVMs designed for AU detection. When evaluated comparatively, one can see that the 2D and 3D modalities have relative merits per upper face and lower face AUs, respectively, and that there is an overall improvement if 2D and 3D intensity estimations are used in fusion. (c) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Regression-based intensity estimation of facial action units", "paper_id": "WOS:000310389100009"}