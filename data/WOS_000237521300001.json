{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "generalized_adalines"}, {"score": 0.0469306417943247, "phrase": "data_driven_function_approximation"}, {"score": 0.03943138540816243, "phrase": "generative_component"}, {"score": 0.02482967141164545, "phrase": "proposed_learning_method"}, {"score": 0.004680624753255133, "phrase": "neural_organization"}, {"score": 0.004373267443572639, "phrase": "threshold_function"}, {"score": 0.004203343568960231, "phrase": "k-state_transfer_function"}, {"score": 0.004062937864702115, "phrase": "unitary_vector"}, {"score": 0.0040171817997972335, "phrase": "k_binary_values"}, {"score": 0.003817548980308163, "phrase": "receptive_field"}, {"score": 0.0036691380448651443, "phrase": "k-state_activation"}, {"score": 0.003546513036085932, "phrase": "k_posterior_independent_normal_variables"}, {"score": 0.003447451122274614, "phrase": "stochastic_predictor-oriented_target_generation"}, {"score": 0.0031308163228919773, "phrase": "mixed_integer"}, {"score": 0.0030955250132409964, "phrase": "linear_programming"}, {"score": 0.003009022147559257, "phrase": "continuous_and_discrete_variables"}, {"score": 0.002958281631770322, "phrase": "mathematical_framework"}, {"score": 0.002827105825163379, "phrase": "mean_field"}, {"score": 0.0027794243746094905, "phrase": "gradient_descent_methods"}, {"score": 0.0027170943110836425, "phrase": "leave-one-out_learning_strategy"}, {"score": 0.0025673005153462707, "phrase": "multiple_generative_components"}, {"score": 0.0025239898026543964, "phrase": "learning_result"}, {"score": 0.0024395426929046415, "phrase": "deterministic_gadaline_network"}, {"score": 0.0024120247307686084, "phrase": "function_approximation"}, {"score": 0.0023848164282879885, "phrase": "numerical_simulations"}, {"score": 0.002305014991156598, "phrase": "paired_data"}, {"score": 0.0022278779300543548, "phrase": "target_functions"}, {"score": 0.0021049977753042253, "phrase": "mlp_and_rbf_learning_methods"}], "paper_keywords": ["adalines", " generative models", " mean field annealing", " perceptron", " postnonlinear projection", " potts encoding", " supervised learning"], "paper_abstract": "This paper proposes neural organization of generalized adalines (gadalines) for data driven function approximation. By generalizing the threshold function of adalines, we achieve the K-state transfer function of gadalines which responds a unitary vector of K binary values to the projection of a predictor on a receptive field. A generative component that uses the K-state activation of a gadaline to trigger K posterior independent normal variables is employed to emulate stochastic predictor-oriented target generation. The fitness or a generative component to a set or paired data mathematically translates to a mixed integer and linear programming. Since consisting of continuous and discrete variables. the mathematical framework is resolved by a hybrid of the mean field annealing and gradient descent methods. Following the leave-one-out learning strategy, the obtained learning method is extended for optimizing multiple generative components. The learning result leads to parameters of a deterministic gadaline network for function approximation. Numerical simulations further test the proposed learning method with paired data oriented from a variety of target functions. The result shows that the proposed learning method outperforms the MLP and RBF learning methods for data driven function approximation.", "paper_title": "Function approximation using generalized adalines", "paper_id": "WOS:000237521300001"}