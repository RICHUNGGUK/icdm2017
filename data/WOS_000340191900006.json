{"auto_keywords": [{"score": 0.03843856552198327, "phrase": "coding_theory"}, {"score": 0.03136706202255777, "phrase": "generalization_error"}, {"score": 0.030868998874318338, "phrase": "proposed_algorithm"}, {"score": 0.00481495049065317, "phrase": "information_bottleneck"}, {"score": 0.004621043537046209, "phrase": "multi-view_features"}, {"score": 0.004416713079266374, "phrase": "communication_system"}, {"score": 0.004380540407926982, "phrase": "multiple_senders"}, {"score": 0.004135458387200662, "phrase": "precise_components"}, {"score": 0.004067977399245212, "phrase": "multiple_information_sources"}, {"score": 0.003952513974962462, "phrase": "margin_maximization_approach"}, {"score": 0.0037312893857519803, "phrase": "code_distance"}, {"score": 0.0036104596826132965, "phrase": "resulting_algorithm"}, {"score": 0.0035224030823976186, "phrase": "ib_principle"}, {"score": 0.0034083145672246067, "phrase": "existing_algorithms"}, {"score": 0.0031910685060552485, "phrase": "multi-view_model"}, {"score": 0.0031260524791486347, "phrase": "encoded_multi-view_data"}, {"score": 0.0028553159147435424, "phrase": "specific_properties"}, {"score": 0.002831894638104635, "phrase": "multi-view_learning"}, {"score": 0.0025338369486914364, "phrase": "empirical_rademacher_complexity"}, {"score": 0.0025027144548626975, "phrase": "objective_function"}, {"score": 0.002314271047073693, "phrase": "resulting_objective_function"}, {"score": 0.0022577559526646904, "phrase": "alternating_direction_method"}, {"score": 0.002239225269052105, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "practical_applications"}], "paper_keywords": ["Multi-view learning", " large-margin learning", " information bottleneck"], "paper_abstract": "In this paper, we extend the theory of the information bottleneck (IB) to learning from examples represented by multi-view features. We formulate the problem as one of encoding a communication system with multiple senders, each of which represents one view of the data. Based on the precise components filtered out from multiple information sources through a \"bottleneck\", a margin maximization approach is then used to strengthen the discrimination of the encoder by improving the code distance within the frame of coding theory. The resulting algorithm therefore inherits all the merits of the IB principle and coding theory. It has two distinct advantages over existing algorithms, namely, that our method finds a tradeoff between the accuracy and complexity of the multi-view model, and that the encoded multi-view data retains sufficient discrimination for classification. We also derive the robustness and generalization error bound of the proposed algorithm, and reveal the specific properties of multi-view learning. First, the complementarity of multi-view features guarantees the robustness of the algorithm. Second, the consensus of multi-view features reduces the empirical Rademacher complexity of the objective function, enhances the accuracy of the solution, and improves the generalization error bound of the algorithm. The resulting objective function is solved efficiently using the alternating direction method. Experimental results on annotation, classification and recognition tasks demonstrate that the proposed algorithm is promising for practical applications.", "paper_title": "Large-Margin Multi-View Information Bottleneck", "paper_id": "WOS:000340191900006"}