{"auto_keywords": [{"score": 0.04013856515910984, "phrase": "gpus_cluster"}, {"score": 0.033202430092532126, "phrase": "occlusion_query"}, {"score": 0.031158790934142384, "phrase": "data_parallelism_strategies"}, {"score": 0.03092651748836349, "phrase": "functionality_parallelism_strategies"}, {"score": 0.00481495049065317, "phrase": "parallel_rendering"}, {"score": 0.004633429997452688, "phrase": "key_techniques"}, {"score": 0.004562743714811391, "phrase": "rendering_performance"}, {"score": 0.004527804152066942, "phrase": "large_geometric_data_sets"}, {"score": 0.0039274105882136775, "phrase": "multiple_computing_nodes"}, {"score": 0.0038674532861321864, "phrase": "cpus_cluster"}, {"score": 0.0034194902384248006, "phrase": "novel_occlusion"}, {"score": 0.00334144503360907, "phrase": "occlusion_query_functionality"}, {"score": 0.0033030907500120367, "phrase": "current_gpu."}, {"score": 0.0032526334220034326, "phrase": "visibility_prediction_technique"}, {"score": 0.003215295260684715, "phrase": "temporal_coherence"}, {"score": 0.003093893243443715, "phrase": "different_strategies"}, {"score": 0.002864628676257225, "phrase": "data_sets"}, {"score": 0.002820850247525606, "phrase": "disjoint_parts"}, {"score": 0.0027564303939865476, "phrase": "different_cluster_nodes"}, {"score": 0.002735284807307981, "phrase": "parallel_execution"}, {"score": 0.0026421139329830755, "phrase": "multiple_cluster_nodes"}, {"score": 0.002493810638588771, "phrase": "special_issues"}, {"score": 0.002372015603684862, "phrase": "data_dependency"}, {"score": 0.0022911901892022847, "phrase": "experimental_results"}, {"score": 0.0022388390432214415, "phrase": "proposed_parallelism_strategies"}, {"score": 0.0021625417261202603, "phrase": "visibility_predictor"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["GPUs cluster", " occlusion culling", " parallel rendering", " cluster rendering", " tiled display", " immersive environments"], "paper_abstract": "Parallel rendering, visibility culling and level-of-detail are key techniques to improve the rendering performance for large geometric data sets. Although each of these techniques has been researched extensively and some systems have been developed to combine them together. However, parallel occlusion culling, that is distributing the computation of occlusion culling to multiple computing nodes such as a CPUs cluster or a GPUs cluster, has rarely been touched. This is because most existing occlusion culling algorithms are difficult to parallelize or do not scale well when parallelized. We first introduce a novel occlusion culling algorithm that uses the occlusion query functionality provided by current GPU. We employ a visibility prediction technique based on temporal coherence to reduce the times of occlusion query. Furthermore, different strategies of parallelizing the occlusion culling algorithm on a GPUs cluster are proposed including data parallelism strategies and functionality parallelism strategies. Data parallelism strategies decompose the data sets for occlusion query into disjoint parts and map these queries on different cluster nodes for parallel execution while functionality parallelism strategies assemble an occlusion culling pipeline with multiple cluster nodes which outputs image stream steadily. We propose a number of solutions to some special issues on parallelizing this occlusion culling algorithm, such as the transferring of data dependency and the load-balancing of occlusion culling pipeline. Experimental results demonstrate the efficiency of the proposed parallelism strategies of the occlusion culling algorithm based on the visibility predictor. Copyright (c) 2007 John Wiley & Sons, Ltd.", "paper_title": "Parallel strategies of occlusion culling on cluster of GPUs", "paper_id": "WOS:000247701400003"}