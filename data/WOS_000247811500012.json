{"auto_keywords": [{"score": 0.049270547314558835, "phrase": "multiagent_learning"}, {"score": 0.04116304647850638, "phrase": "regret_minimization"}, {"score": 0.00481495049065317, "phrase": "prescriptive_goals"}, {"score": 0.0046141941030386525, "phrase": "great_deal"}, {"score": 0.004536244367689415, "phrase": "theoretical_effort"}, {"score": 0.004201381218977942, "phrase": "inherent_symmetry"}, {"score": 0.002475755804232436, "phrase": "utility_guarantees"}, {"score": 0.002253826965566618, "phrase": "game-theoretic_equilibrium"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": [""], "paper_abstract": "A great deal of theoretical effort in multiagent learning involves either embracing or avoiding the inherent symmetry between the problem and the solution. Regret minimization is an approach to the prescriptive, non-cooperative goal that explicitly breaks this symmetry, but, since it makes no assumptions about the adversary, it achieves only limited guarantees. In this paper, we consider a hierarchy of goals that begins with the basics of regret minimization and moves towards the utility guarantees achievable by agents that could also guarantee converging to a game-theoretic equilibrium. (c) 2007 Published by Elsevier B.V.", "paper_title": "A hierarchy of prescriptive goals for multiagent learning", "paper_id": "WOS:000247811500012"}