{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "object_affordances"}, {"score": 0.04950097558699653, "phrase": "human_demonstration"}, {"score": 0.004614483128885678, "phrase": "object_categorization"}, {"score": 0.00386876598761639, "phrase": "different_types"}, {"score": 0.0037529131348732715, "phrase": "intended_application"}, {"score": 0.00342568771304873, "phrase": "household_tasks"}, {"score": 0.003051713660488494, "phrase": "manipulated_objects"}, {"score": 0.0030147970456037274, "phrase": "human_manipulation_actions"}, {"score": 0.002735052488172277, "phrase": "human_hand_actions"}, {"score": 0.0024511688853030168, "phrase": "initial_step"}, {"score": 0.0023776656246300063, "phrase": "demonstration_method"}, {"score": 0.0022923586577645143, "phrase": "contextual_information"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Object recognition", " Action recognition", " Contextual recognition", " Object affordances", " Learning from demonstration"], "paper_abstract": "This paper investigates object categorization according to function, i.e., learning the affordances of objects from human demonstration. Object affordances (functionality) are inferred from observations of humans using the objects in different types of actions. The intended application is learning from demonstration, in which a robot learns to employ objects in household tasks, from observing a human performing the same tasks with the objects. We present a method for categorizing manipulated objects and human manipulation actions in context of each other. The method is able to simultaneously segment and classify human hand actions, and detect and classify the objects involved in the action. This can serve as an initial step in a learning from demonstration method. Experiments show that the contextual information improves the classification of both objects and actions. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "Visual object-action recognition: Inferring object affordances from human demonstration", "paper_id": "WOS:000285275200008"}