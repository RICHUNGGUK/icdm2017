{"auto_keywords": [{"score": 0.04179897773189256, "phrase": "possibility_distributions"}, {"score": 0.03898843416319269, "phrase": "probability_distributions"}, {"score": 0.028550848759891546, "phrase": "possibilistic_specificity"}, {"score": 0.00481495049065317, "phrase": "probability_measures"}, {"score": 0.004776484866276242, "phrase": "possibility_theory"}, {"score": 0.004681653551412616, "phrase": "relative_peakedness"}, {"score": 0.004251912075877877, "phrase": "well-established_information_measures"}, {"score": 0.00401961206251868, "phrase": "relative_specificity"}, {"score": 0.0038925903520043623, "phrase": "fuzzy_set_inclusion"}, {"score": 0.0035350218469335573, "phrase": "numerical_index"}, {"score": 0.0034926916198124484, "phrase": "natural_partial_ordering"}, {"score": 0.003437036036889996, "phrase": "relative_\"peakedness"}, {"score": 0.0033958751452139984, "phrase": "probability_functions"}, {"score": 0.00321019328631004, "phrase": "close_connection"}, {"score": 0.003108669771132255, "phrase": "standard_specificity_ordering"}, {"score": 0.002974281028713475, "phrase": "known_probability-possibility_transformation"}, {"score": 0.0029034358982402346, "phrase": "direct_proof"}, {"score": 0.0027890803075309926, "phrase": "probabilistic_entropy"}, {"score": 0.0025224615049571427, "phrase": "related_work"}, {"score": 0.002442634456448208, "phrase": "convex_functions"}, {"score": 0.002253920389285441, "phrase": "machine_learning"}, {"score": 0.0021738146596180404, "phrase": "decision_forests"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["probability distributions", " possibility distributions", " probability-possibility transformation", " entropy", " dispersion", " stochastic dominance", " specificity", " machine learning", " recursive partitioning", " decision forest"], "paper_abstract": "Deciding whether one probability distribution is more informative (in the sense of representing a less indeterminate situation) than another one is typically done using well-established information measures such as, e.g., the Shannon entropy or other dispersion indices. In contrast, the relative specificity of possibility distributions is evaluated by means of fuzzy set inclusion. In this paper, we propose a technique for comparing probability distributions from the point of view of their relative dispersion without resorting to a numerical index. A natural partial ordering in terms of relative \"peakedness\" of probability functions is proposed which is closely related to order-1 stochastic dominance. There is also a close connection between this ordering on probability distributions and the standard specificity ordering on possibility distributions that can be derived by means of a known probability-possibility transformation. The paper proposes a direct proof showing that the (total) preordering on probability measures defined by probabilistic entropy refines the (partial) ordering defined by possibilistic specificity. This result, also valid for other dispersion indices, is discussed against the background of related work in statistics, mathematics (inequalities on convex functions), and the social sciences. Finally, an application of the possibilistic specificity ordering in the field of machine learning or, more specifically, the induction of decision forests is proposed. (C) 2006 Elsevier Inc. All rights reserved.", "paper_title": "Comparing probability measures using possibility theory: A notion of relative peakedness", "paper_id": "WOS:000247920700011"}