{"auto_keywords": [{"score": 0.04795066700982643, "phrase": "face_recognition"}, {"score": 0.014485609560573565, "phrase": "image_representation"}, {"score": 0.00481495049065317, "phrase": "learning_deep_supervised_autoencoders"}, {"score": 0.0046970764288494764, "phrase": "robust_image_representation"}, {"score": 0.004658426838467567, "phrase": "single_training_sample"}, {"score": 0.00446987606516801, "phrase": "deep_learning"}, {"score": 0.004342430078306461, "phrase": "supervised_autoencoder"}, {"score": 0.004253618550301311, "phrase": "new_type"}, {"score": 0.004218602458288531, "phrase": "building_block"}, {"score": 0.00418387341007444, "phrase": "deep_architectures"}, {"score": 0.004047781270412204, "phrase": "standard_autoencoder"}, {"score": 0.003804381587788162, "phrase": "canonical_face"}, {"score": 0.0036352070472201086, "phrase": "neutral_expression"}, {"score": 0.0036052633308076933, "phrase": "normal_illumination"}, {"score": 0.0029437270961011077, "phrase": "deep_architecture"}, {"score": 0.0028243822059448266, "phrase": "experimental_results"}, {"score": 0.002721103874746693, "phrase": "cmu-pie"}, {"score": 0.002687520775504511, "phrase": "multi-pie"}, {"score": 0.002676418520290942, "phrase": "data_sets"}, {"score": 0.0025999744005191713, "phrase": "commonly_used_sparse_representation-based_classification"}, {"score": 0.0024842242921857705, "phrase": "single_sample"}, {"score": 0.00240328454980006, "phrase": "higher_recognition_accuracy"}, {"score": 0.0023346234464901978, "phrase": "deep_lambertian_network"}, {"score": 0.0022492129111567824, "phrase": "domain_information"}, {"score": 0.0021669202726173928, "phrase": "face_verification"}, {"score": 0.0021049977753042253, "phrase": "face_representation"}], "paper_keywords": ["Single training sample per person", " face recognition", " supervised auto-encoder", " deep architecture"], "paper_abstract": "This paper targets learning robust image representation for single training sample per person face recognition. Motivated by the success of deep learning in image representation, we propose a supervised autoencoder, which is a new type of building block for deep architectures. There are two features distinct our supervised autoencoder from standard autoencoder. First, we enforce the faces with variants to be mapped with the canonical face of the person, for example, frontal face with neutral expression and normal illumination; Second, we enforce features corresponding to the same person to be similar. As a result, our supervised autoencoder extracts the features which are robust to variances in illumination, expression, occlusion, and pose, and facilitates the face recognition. We stack such supervised autoencoders to get the deep architecture and use it for extracting features in image representation. Experimental results on the AR, Extended Yale B, CMU-PIE, and Multi-PIE data sets demonstrate that by coupling with the commonly used sparse representation-based classification, our stacked supervised autoencoders-based face representation significantly outperforms the commonly used image representations in single sample per person face recognition, and it achieves higher recognition accuracy compared with other deep learning models, including the deep Lambertian network, in spite of much less training data and without any domain information. Moreover, supervised autoencoder can also be used for face verification, which further demonstrates its effectiveness for face representation.", "paper_title": "Single Sample Face Recognition via Learning Deep Supervised Autoencoders", "paper_id": "WOS:000360891300007"}