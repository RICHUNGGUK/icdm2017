{"auto_keywords": [{"score": 0.04904273003005848, "phrase": "sparse_coding"}, {"score": 0.04370000843795686, "phrase": "signal_processing"}, {"score": 0.00481495049065317, "phrase": "online_learning"}, {"score": 0.004762550320451225, "phrase": "matrix_factorization"}, {"score": 0.004508942069742167, "phrase": "data_vectors"}, {"score": 0.004459857205489101, "phrase": "sparse_linear_combinations"}, {"score": 0.00441130431691048, "phrase": "basis_elements"}, {"score": 0.004268780663872836, "phrase": "machine_learning"}, {"score": 0.003953806321226277, "phrase": "large-scale_matrix_factorization_problem"}, {"score": 0.003661987063211563, "phrase": "specific_data"}, {"score": 0.0030229485340586473, "phrase": "new_online_optimization_algorithm"}, {"score": 0.0029573932605287947, "phrase": "stochastic_approximations"}, {"score": 0.0028460640294160383, "phrase": "large_data_sets"}, {"score": 0.0027843339377323878, "phrase": "training_samples"}, {"score": 0.002592784882557155, "phrase": "wide_range"}, {"score": 0.002564505790176378, "phrase": "learning_problems"}, {"score": 0.002388043660169452, "phrase": "natural_images"}, {"score": 0.002361992359512022, "phrase": "genomic_data"}, {"score": 0.0021049977753042253, "phrase": "small_and_large_data_sets"}], "paper_keywords": ["basis pursuit", " dictionary learning", " matrix factorization", " online learning", " sparse coding", " sparse principal component analysis", " stochastic approximations", " stochastic optimization", " non-negative matrix factorization"], "paper_abstract": "Sparse coding-that is, modelling data vectors as sparse linear combinations of basis elements-is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.", "paper_title": "Online Learning for Matrix Factorization and Sparse Coding", "paper_id": "WOS:000277186400002"}