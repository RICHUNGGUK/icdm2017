{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "low-rank_subspace_recovery_models"}, {"score": 0.004755054742217193, "phrase": "intrinsic_low-dimensional_subspaces"}, {"score": 0.004598933498016774, "phrase": "key_preprocessing_step"}, {"score": 0.0043560464825349275, "phrase": "subspace_recovery"}, {"score": 0.004319830179921363, "phrase": "low-rank_minimization_problems"}, {"score": 0.00421297029755273, "phrase": "representative_models"}, {"score": 0.004143196873246147, "phrase": "robust_principal_component_analysis"}, {"score": 0.0040238512275833265, "phrase": "robust_low-rank_representation"}, {"score": 0.00389164348807591, "phrase": "robust_latent_low-rank_representation"}, {"score": 0.003334309053314909, "phrase": "closed-form_formulations"}, {"score": 0.0029909851863915283, "phrase": "solid_theoretical_foundation"}, {"score": 0.0028926147550022607, "phrase": "globally_optimal_solutions"}, {"score": 0.002856563331151566, "phrase": "low-rank_models"}, {"score": 0.0028209599545587745, "phrase": "overwhelming_probability"}, {"score": 0.002671722386856879, "phrase": "significantly_faster_algorithms"}, {"score": 0.0025623050508233078, "phrase": "computation_cost"}, {"score": 0.0024883835349005863, "phrase": "low-complexity_randomized_algorithms"}, {"score": 0.0021049977753042253, "phrase": "alternating_direction_method"}], "paper_keywords": [""], "paper_abstract": "Recovering intrinsic low-dimensional subspaces from data distributed on them is a key preprocessing step to many applications. In recent years, a lot of work has modeled subspace recovery as low-rank minimization problems. We find that some representative models, such as robust principal component analysis (R-PCA), robust low-rank representation (R-LRR), and robust latent low-rank representation (R-LatLRR), are actually deeply connected. More specifically, we discover that once a solution to one of the models is obtained, we can obtain the solutions to other models in closed-form formulations. Since R-PCA is the simplest, our discovery makes it the center of low-rank subspace recovery models. Our work has two important implications. First, R-PCA has a solid theoretical foundation. Under certain conditions, we could find globally optimal solutions to these low-rank models at an overwhelming probability, although these models are nonconvex. Second, we can obtain significantly faster algorithms for these models by solving R-PCA first. The computation cost can be further cut by applying low-complexity randomized algorithms, for example, our novel l(2,1) filtering algorithm, to R-PCA. Although for the moment the formal proof of our l(2,1) filtering algorithm is not yet available, experiments verify the advantages of our algorithm over other state-of-the-art methods based on the alternating direction method.", "paper_title": "Relations Among Some Low-Rank Subspace Recovery Models", "paper_id": "WOS:000360091800005"}