{"auto_keywords": [{"score": 0.05007830243554293, "phrase": "gmm_mean_supervector_space"}, {"score": 0.032070541335258995, "phrase": "lpp"}, {"score": 0.004285489630701221, "phrase": "speaking_style"}, {"score": 0.004177631185971939, "phrase": "time-frequency_structure"}, {"score": 0.004132237700479874, "phrase": "speaker's_voice"}, {"score": 0.003955523458325766, "phrase": "speech_systems"}, {"score": 0.003912533878506028, "phrase": "speech_indexing"}, {"score": 0.0038140261371648744, "phrase": "intrinsic_variations"}, {"score": 0.003786337629480068, "phrase": "speech_production"}, {"score": 0.003745179888113379, "phrase": "clustering_techniques"}, {"score": 0.0035980453088879393, "phrase": "acoustic_space_differences"}, {"score": 0.003284718806701356, "phrase": "membership_degrees"}, {"score": 0.0031099353574309606, "phrase": "subspace_learning_methods"}, {"score": 0.00296597499768551, "phrase": "plda."}, {"score": 0.002838993542880416, "phrase": "initial_clusters"}, {"score": 0.0027978827655799495, "phrase": "full_dimension_supervectors"}, {"score": 0.0026878636586288363, "phrase": "plda_subspace"}, {"score": 0.00250789768689397, "phrase": "average_clustering_accuracy"}, {"score": 0.002453602802736223, "phrase": "hierarchical_baseline"}, {"score": 0.0023657078783304677, "phrase": "plda"}, {"score": 0.002339953069296797, "phrase": "increases_accuracy"}, {"score": 0.002289285767792039, "phrase": "k-means_baseline"}, {"score": 0.002247900207250435, "phrase": "novel_techniques"}, {"score": 0.002223428059261574, "phrase": "model_formulation"}, {"score": 0.0022072611570187334, "phrase": "speech_applications"}, {"score": 0.0021912115487610286, "phrase": "speaker_id"}, {"score": 0.0021752783872387173, "phrase": "audio_search"}, {"score": 0.0021515951441422082, "phrase": "audio_content_analysis"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Speaker clustering", " Singing", " Speaking styles", " Subspace learning"], "paper_abstract": "In this study, we propose algorithms based on subspace learning in the GMM mean supervector space to improve performance of speaker clustering with speech from both reading and singing. As a speaking style, singing introduces changes in the time-frequency structure of a speaker's voice. The purpose of this study is to introduce advancements for speech systems such as speech indexing and retrieval which improve robustness to intrinsic variations in speech production. Speaker clustering techniques such as k-means and hierarchical are explored for analysis of acoustic space differences of a corpus consisting of reading and singing of lyrics for each speaker. Furthermore, a distance based on fuzzy c-means membership degrees is proposed to more accurately measure clustering difficulty or speaker confusability. Two categories of subspace learning methods are studied: unsupervised based on LPP, and supervised based on PLDA. Our proposed clustering method based on PLDA is a two stage algorithm: where first, initial clusters are obtained using full dimension supervectors, and next, each cluster is refined in a PLDA subspace resulting in a more speaker dependent representation that is less sensitive to speaking style. It is shown that LPP improves average clustering accuracy by 5.1% absolute versus a hierarchical baseline for a mixture of reading and singing, and PLDA based clustering increases accuracy by 9.6% absolute versus a k-means baseline. The advancements offer novel techniques to improve model formulation for speech applications including speaker ID, audio search, and audio content analysis. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Singing speaker clustering based on subspace learning in the GMM mean supervector space", "paper_id": "WOS:000318744800006"}