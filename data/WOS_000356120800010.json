{"auto_keywords": [{"score": 0.046737172310030196, "phrase": "human_actions"}, {"score": 0.007244236391201885, "phrase": "large_number"}, {"score": 0.0062356866927737115, "phrase": "marker-based_motion_capture_system"}, {"score": 0.005476343052699626, "phrase": "large_datasets"}, {"score": 0.00481495049065317, "phrase": "human_poses"}, {"score": 0.0047872135686404275, "phrase": "video_sequences"}, {"score": 0.004691380685111258, "phrase": "automated_recognition"}, {"score": 0.004664352260912174, "phrase": "human_activities"}, {"score": 0.0045709675813472884, "phrase": "specific_action_class"}, {"score": 0.004415189385015778, "phrase": "complex_and_varied_human_actions"}, {"score": 0.004371684442911149, "phrase": "action_configurations"}, {"score": 0.004364446928620496, "phrase": "automated_surveillance"}, {"score": 0.003944506079554075, "phrase": "training_data"}, {"score": 0.003910441483033426, "phrase": "action_classifiers"}, {"score": 0.0037121208432871118, "phrase": "capture_system"}, {"score": 0.0036649041080693383, "phrase": "action_database"}, {"score": 0.0036167509502276294, "phrase": "marker-based_systems"}, {"score": 0.003463196032143783, "phrase": "intensive_use"}, {"score": 0.0034332736827447654, "phrase": "motion_capture_system"}, {"score": 0.003156984631072011, "phrase": "motion_capture_systems"}, {"score": 0.003066945490483524, "phrase": "video_segments"}, {"score": 0.0030404363386284756, "phrase": "correct_human_action_category"}, {"score": 0.0029708567739251254, "phrase": "action_categories"}, {"score": 0.0029281780903782284, "phrase": "human_postures"}, {"score": 0.002861160152640875, "phrase": "new_concept"}, {"score": 0.002820052889643225, "phrase": "human_whole_body_actions"}, {"score": 0.0026691455928407022, "phrase": "body_parts"}, {"score": 0.0026307897899760383, "phrase": "silhouette_images"}, {"score": 0.0025557195732514915, "phrase": "pose_descriptors"}, {"score": 0.002526293222350046, "phrase": "regression_model"}, {"score": 0.0024756080101497086, "phrase": "pose_descriptor"}, {"score": 0.0022630645070078554, "phrase": "action_classification_tasks"}, {"score": 0.0022499947003073654, "phrase": "human_body_posture_inference_tasks"}, {"score": 0.0021543421286324945, "phrase": "action_classification"}, {"score": 0.002141898874982106, "phrase": "posture_inference"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action database", " Action classification", " Hidden Markov model", " Gaussian process regression"], "paper_abstract": "One of the difficulties in automated recognition of human activities is classifying a video into a specific action class by selecting among a large number of human actions. Technology for understanding complex and varied human actions is necessary for automated surveillance, sports training, computer games, and human-robot interactions. The difficulty of classification comes from a dearth of datasets of human actions that are manually categorized and suitable for use as training data for designing action classifiers. A marker-based motion capture system enables precise measurement of human actions for the purpose of analysis. This type of capture system has several drawbacks, however; in particular, marker-based systems are expensive, intrusive, and complex to use. Despite this, the intensive use of a motion capture system can provide large datasets of human actions, and the datasets can be used to facilitate handling the variety of actions to be classified. Large datasets of human actions measured by motion capture systems are expected to be suitable for use in classifying video segments into the correct human action category, selecting from among a large number of action categories, and for inferring human postures from video. This paper proposes a new concept for a database of human whole body actions and an application to understanding human actions from video. The database contains action configurations, such as positions of body parts, pose descriptors from silhouette images, a stochastic model encoding each sequence of the pose descriptors, and a regression model for predicting the configuration from the pose descriptor. The action configurations are recorded in advance of use by measuring many human actions with a marker-based motion capture system, and silhouette images are created from these configurations. We tested the action database on action classification tasks and human body posture inference tasks. The experimental results show that the action database is suitable for use in both action classification and posture inference. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Action database for categorizing and inferring human poses from video sequences", "paper_id": "WOS:000356120800010"}