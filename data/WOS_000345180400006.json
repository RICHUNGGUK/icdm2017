{"auto_keywords": [{"score": 0.0477034884432716, "phrase": "statistical_relational_learning"}, {"score": 0.00481495049065317, "phrase": "logical_and_relational_learning"}, {"score": 0.004518442937844318, "phrase": "standard_approaches"}, {"score": 0.004480307790394296, "phrase": "klog"}, {"score": 0.0043863636475983845, "phrase": "probability_distribution"}, {"score": 0.004204320052179726, "phrase": "kernel-based_learning"}, {"score": 0.00416882434439484, "phrase": "expressive_logical_and_relational_representations"}, {"score": 0.003928572641266562, "phrase": "simple_but_powerful_concepts"}, {"score": 0.0035634522350563107, "phrase": "rich_representation"}, {"score": 0.003400984514637656, "phrase": "relational_representation"}, {"score": 0.0031643005824589917, "phrase": "feature_space"}, {"score": 0.0031110402375026016, "phrase": "mixed_numerical_and_symbolic_data"}, {"score": 0.0030457198309616694, "phrase": "background_knowledge"}, {"score": 0.0029818253737139092, "phrase": "prolog"}, {"score": 0.0029565867133766566, "phrase": "datalog"}, {"score": 0.002906788114295269, "phrase": "inductive_logic_programming_systems"}, {"score": 0.002870006736445581, "phrase": "klog_framework"}, {"score": 0.0026140878612307536, "phrase": "multitask_learning"}, {"score": 0.0025810006125483835, "phrase": "collective_classification"}, {"score": 0.0025160740422447837, "phrase": "empirical_comparisons"}, {"score": 0.002301405771795899, "phrase": "tilde"}, {"score": 0.002281933498632781, "phrase": "alchemy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Logical and relational learning", " Statistical relational learning", " Kernel methods", " Prolog", " Deductive databases"], "paper_abstract": "We introduce kLog, a novel approach to statistical relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. It is rather a language to perform kernel-based learning on expressive logical and relational representations. kLog allows users to specify learning problems declaratively. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases. Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph - in particular, a grounded entity/relationship diagram. Subsequently, a choice of graph kernel defines the feature space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems. The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification. We also report about empirical comparisons, showing that kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at http://klog.dinfo.unifi.it along with tutorials. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "kLog: A language for logical and relational learning with kernels", "paper_id": "WOS:000345180400006"}