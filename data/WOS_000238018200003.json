{"auto_keywords": [{"score": 0.033457925072458865, "phrase": "multivariate_search_techniques"}, {"score": 0.00481495049065317, "phrase": "multivariate_feature_selection"}, {"score": 0.004744123493964429, "phrase": "growing_number"}, {"score": 0.0047090997998449095, "phrase": "domains_data"}, {"score": 0.004691684576246597, "phrase": "captured_encapsulates"}, {"score": 0.004487635647709636, "phrase": "classical_pattern_recognition_techniques"}, {"score": 0.00418247778205288, "phrase": "classical_pattern_recognition_methods"}, {"score": 0.004120915812750583, "phrase": "small_sample_size"}, {"score": 0.00407533728096378, "phrase": "robust_classification_techniques"}, {"score": 0.003897989308486044, "phrase": "feature_space"}, {"score": 0.0038263829059336564, "phrase": "informative_features"}, {"score": 0.003784049616066444, "phrase": "essential_step"}, {"score": 0.0037421829222825964, "phrase": "classification_task"}, {"score": 0.003474624627033273, "phrase": "multivariate_manner"}, {"score": 0.003448940584469061, "phrase": "univariate_approaches"}, {"score": 0.0033110234463504125, "phrase": "possible_correlation"}, {"score": 0.0030514638431036714, "phrase": "multivariate_searches"}, {"score": 0.00265987582861194, "phrase": "large_feature_space"}, {"score": 0.0026109540617641593, "phrase": "new_multivariate_search_technique"}, {"score": 0.0023184226291364097, "phrase": "ground_truth_information"}, {"score": 0.0022757667100089243, "phrase": "real_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["feature selection", " random subspace method", " small sample size problem"], "paper_abstract": "In a growing number of domains data captured encapsulates as many features as possible. This poses a challenge to classical pattern recognition techniques, since the number of samples often still is limited with respect to the number of features. Classical pattern recognition methods suffer from the small sample size, and robust classification techniques are needed. In order to reduce the dimensionality of the feature space, the selection of informative features becomes an essential step towards the classification task. The relevance of the features can be evaluated either individually (univariate approaches), or in a multivariate manner. Univariate approaches are simple and fast, therefore appealing. However, possible correlation and dependencies between the features are not considered. Therefore, multivariate search techniques may be helpful. Several limitations restrict the use of multivariate searches. First, they are prone to overtraining, especially in p >> n (many features and few samples) settings. Secondly, they can be computationally too expensive when dealing with a large feature space. We introduce a new multivariate search technique, that is less sensitive to the noise in the data and computationally feasible as well. We compare our approach with several multivariate and univariate feature selection techniques, on an artificial dataset which provides us with ground truth information, and on a real dataset. The results show the importance of multivariate search techniques and the robustness and reliability of our novel multivariate feature selection method. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Random subspace method for multivariate feature selection", "paper_id": "WOS:000238018200003"}