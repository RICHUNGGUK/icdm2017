{"auto_keywords": [{"score": 0.03870166960129152, "phrase": "theoretical_problems"}, {"score": 0.015719716506582538, "phrase": "time_series_predictor_evaluation"}, {"score": 0.004615202066192795, "phrase": "model_selection_procedure"}, {"score": 0.004486640769041646, "phrase": "traditional_forecasting_procedures"}, {"score": 0.004346266306175988, "phrase": "machine_learning_techniques"}, {"score": 0.004255121395629569, "phrase": "traditional_forecasting"}, {"score": 0.004195418010131832, "phrase": "common_practice"}, {"score": 0.003760137090849196, "phrase": "full_use"}, {"score": 0.003629557750546267, "phrase": "temporal_evolutionary_effects"}, {"score": 0.0035159043158639633, "phrase": "practical_problems"}, {"score": 0.0034911335714333507, "phrase": "missing_values"}, {"score": 0.003195787852087248, "phrase": "fundamental_assumptions"}, {"score": 0.0030521898646210413, "phrase": "different_model_selection_procedures"}, {"score": 0.0029670381308395505, "phrase": "rigorous_and_extensive_empirical_study"}, {"score": 0.002946122903079259, "phrase": "six_different_model_selection_procedures"}, {"score": 0.0027741761880779535, "phrase": "series'_last_part"}, {"score": 0.0026494723416072316, "phrase": "synthetic_and_real-world_time_series"}, {"score": 0.0025846562485378247, "phrase": "theoretical_flaws"}, {"score": 0.0024947959375982614, "phrase": "cross-validation_techniques"}, {"score": 0.0022835460368487233, "phrase": "blocked_form"}, {"score": 0.0022434880661725493, "phrase": "series_evaluation"}, {"score": 0.002219790619936024, "phrase": "standard_procedure"}, {"score": 0.002180848602315088, "phrase": "available_information"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Cross-validation", " Time series", " Predictor evaluation", " Error measures", " Machine learning", " Regression"], "paper_abstract": "In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the \"best of both worlds\", we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "On the use of cross-validation for time series predictor evaluation", "paper_id": "WOS:000302510400013"}