{"auto_keywords": [{"score": 0.034887398305923065, "phrase": "truncated_approach"}, {"score": 0.02864039936811681, "phrase": "truncated_em_approach"}, {"score": 0.02661395956257378, "phrase": "factored_variational_approach"}, {"score": 0.00481495049065317, "phrase": "spike-and-slab_sparse_coding"}, {"score": 0.004637478829869904, "phrase": "sparse_coding_model"}, {"score": 0.00450020161335541, "phrase": "standard_sparse_coding"}, {"score": 0.004399905285945236, "phrase": "independent_latent_sources"}, {"score": 0.004301834610818624, "phrase": "data_points"}, {"score": 0.004081385289516914, "phrase": "laplace_distribution"}, {"score": 0.003828770052266183, "phrase": "source's_contribution"}, {"score": 0.0035514484574435574, "phrase": "sparse_coding"}, {"score": 0.003344076203780408, "phrase": "standard_factored_variational_distributions"}, {"score": 0.003232779373179631, "phrase": "variational_approach"}, {"score": 0.0032085525502105836, "phrase": "truncated_posteriors"}, {"score": 0.0031845067058084583, "phrase": "variational_distributions"}, {"score": 0.003125175084962886, "phrase": "source_separation"}, {"score": 0.002920561105784389, "phrase": "standard_benchmarks"}, {"score": 0.002833962854197749, "phrase": "'spike-and-slab'_priors"}, {"score": 0.0026186324161493225, "phrase": "source_separation_tasks"}, {"score": 0.002531418327080538, "phrase": "posterior_independence"}, {"score": 0.002447101794397243, "phrase": "standard_benchmark"}, {"score": 0.0024287485834314027, "phrase": "image_denoising"}, {"score": 0.0022781884538102264, "phrase": "factored_approach"}, {"score": 0.002252602622785975, "phrase": "increasing_numbers"}, {"score": 0.00223570488212345, "phrase": "hidden_dimensions"}, {"score": 0.0021049977753042253, "phrase": "higher_noise_levels"}], "paper_keywords": ["sparse coding", " spike-and-slab distributions", " approximate EM", " variational Bayes", " unsupervised learning", " source separation", " denoising"], "paper_abstract": "We study inference and learning based on a sparse coding model with 'spike-and-slab' prior. As in standard sparse coding, the model used assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse prior such as a Laplace distribution, we study the application of a more flexible 'spike-and-slab' distribution which models the absence or presence of a source's contribution independently of its strength if it contributes. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: a novel truncated EM approach and, for comparison, an approach based on standard factored variational distributions. The truncated approach can be regarded as a variational approach with truncated posteriors as variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of 'spike-and-slab' priors for the corresponding data domains. Furthermore, we find that the truncated EM approach improves on the standard factored approach in source separation tasks-which hints to biases introduced by assuming posterior independence in the factored variational approach. Likewise, on a standard benchmark for image denoising, we find that the truncated EM approach improves on the factored variational approach. While the performance of the factored approach saturates with increasing numbers of hidden dimensions, the performance of the truncated approach improves the state-of-the-art for higher noise levels.", "paper_title": "A Truncated EM Approach for Spike-and-Slab Sparse Coding", "paper_id": "WOS:000344638600002"}