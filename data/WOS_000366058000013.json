{"auto_keywords": [{"score": 0.049774466498282044, "phrase": "rotation_forest"}, {"score": 0.03612455554828082, "phrase": "random_forest"}, {"score": 0.009362661432125021, "phrase": "highly_imbalanced_datasets"}, {"score": 0.005932198186383373, "phrase": "bagging"}, {"score": 0.005627531370562546, "phrase": "hellinger_distance"}, {"score": 0.005548193803737982, "phrase": "splitting_criteria"}, {"score": 0.005357021538397223, "phrase": "gini_index"}, {"score": 0.00532297358867729, "phrase": "information_gain_ratio"}, {"score": 0.004723672884600079, "phrase": "decision_tree"}, {"score": 0.004678681971460014, "phrase": "simple_and_effective_method"}, {"score": 0.004575354958479747, "phrase": "ensemble_methods"}, {"score": 0.004197660328621118, "phrase": "boosting"}, {"score": 0.004014196517907476, "phrase": "feature_space"}, {"score": 0.0039690218441529425, "phrase": "splitting_criterion"}, {"score": 0.0036243109044803924, "phrase": "class_imbalance"}, {"score": 0.0035555237584957455, "phrase": "minority_class_concept"}, {"score": 0.003532885029484907, "phrase": "hellinger_distance_decision_tree"}, {"score": 0.0034547749922175567, "phrase": "chawla"}, {"score": 0.0033354927389810207, "phrase": "unpruned_hddt"}, {"score": 0.0032721691785599833, "phrase": "effective_way"}, {"score": 0.003230620599529342, "phrase": "highly_imbalanced_problem"}, {"score": 0.0031794236141757, "phrase": "bootstrap_sampling"}, {"score": 0.0030892986597489485, "phrase": "low_diversity"}, {"score": 0.002954140458190344, "phrase": "hddt"}, {"score": 0.002718581874516321, "phrase": "experimental_framework"}, {"score": 0.002675477598833661, "phrase": "wide_range"}, {"score": 0.0024937735547989445, "phrase": "decision_trees"}, {"score": 0.0024230370118191267, "phrase": "balanced_random_forest"}, {"score": 0.002324381111188894, "phrase": "class_imbalance_problem"}, {"score": 0.0023021895404322767, "phrase": "experimental_results"}, {"score": 0.0022656723439661163, "phrase": "nonparametric_statistical_tests"}, {"score": 0.002187356046251648, "phrase": "individual_decision_tree"}, {"score": 0.0021049977753042253, "phrase": "highly_imbalanced_classification"}], "paper_keywords": ["Random Forest", " Rotation Forest", " Hellinger distance", " Hellinger distance decision tree (HDDT)", " highly imbalanced datasets"], "paper_abstract": "Decision tree is a simple and effective method and it can be supplemented with ensemble methods to improve its performance. Random Forest and Rotation Forest are two approaches which are perceived as \"classic\" at present. They can build more accurate and diverse classifiers than Bagging and Boosting by introducing the diversities namely randomly chosen a subset of features or rotated feature space. However, the splitting criteria used for constructing each tree in Random Forest and Rotation Forest are Gini index and information gain ratio respectively, which are skew-sensitive. When learning from highly imbalanced datasets, class imbalance impedes their ability to learn the minority class concept. Hellinger distance decision tree (HDDT) was proposed by Chawla, which is skew-insensitive. Especially, bagged unpruned HDDT has proven to be an effective way to deal with highly imbalanced problem. Nevertheless, the bootstrap sampling used in Bagging can lead to ensembles of low diversity compared to Random Forest and Rotation Forest. In order to combine the skew-insensitivity of HDDT and the diversities of Random Forest and Rotation Forest, we use Hellinger distance as the splitting criterion for building each tree in Random Forest and Rotation Forest respectively. An experimental framework is performed across a wide range of highly imbalanced datasets to investigate the effectiveness of Hellinger distance, information gain ratio and Gini index which are used as the splitting criteria in ensembles of decision trees including Bagging, Boosting, Random Forest and Rotation Forest. In addition, Balanced Random Forest is also included in the experiment since it is designed to tackle class imbalance problem. The experimental results, which contrasted through nonparametric statistical tests, demonstrate that using Hellinger distance as the splitting criterion to build individual decision tree in forest can improve the performances of Random Forest and Rotation Forest for highly imbalanced classification.", "paper_title": "Improving Random Forest and Rotation Forest for highly imbalanced datasets", "paper_id": "WOS:000366058000013"}