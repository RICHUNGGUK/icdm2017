{"auto_keywords": [{"score": 0.023895726890989653, "phrase": "fixed_point_arithmetic"}, {"score": 0.00481495049065317, "phrase": "random_vector_functional-link_networks"}, {"score": 0.004731754442072492, "phrase": "renewed_interest"}, {"score": 0.004698878021540436, "phrase": "single_layer_feedforward_neural_network"}, {"score": 0.004553725265085244, "phrase": "hidden_layer_coefficients"}, {"score": 0.004459445258869177, "phrase": "output_coefficients"}, {"score": 0.004382364826200616, "phrase": "least_square_algorithm"}, {"score": 0.004306610946695577, "phrase": "random_coefficient_initialization"}, {"score": 0.0042617859335771615, "phrase": "main_advantages"}, {"score": 0.004217425501814589, "phrase": "learning_models"}, {"score": 0.004101366149219787, "phrase": "multiple_iterations"}, {"score": 0.004030449969901066, "phrase": "initial_coefficient_definition"}, {"score": 0.003919515745050147, "phrase": "multilayer_perceptron"}, {"score": 0.0038116231917604054, "phrase": "real_time_operation"}, {"score": 0.0037719307016238998, "phrase": "fast_online_training"}, {"score": 0.003444732262403694, "phrase": "large_resource_usage"}, {"score": 0.0034207673889407003, "phrase": "low_speed"}, {"score": 0.003314965171168532, "phrase": "hardware_implementation"}, {"score": 0.0032689949867583633, "phrase": "embedded_systems"}, {"score": 0.003212424802083714, "phrase": "real_time_systems"}, {"score": 0.003080606033950419, "phrase": "desktop_computers"}, {"score": 0.0030062165552187086, "phrase": "rvfln"}, {"score": 0.002974886152979996, "phrase": "wide_number"}, {"score": 0.00292339061443511, "phrase": "high_computational_burden"}, {"score": 0.00290304212971385, "phrase": "high_dimension_matrices"}, {"score": 0.002842840150239715, "phrase": "intensive_algorithms"}, {"score": 0.0027838831266525773, "phrase": "output_layer_coefficient_values"}, {"score": 0.002754863452181364, "phrase": "neural_network"}, {"score": 0.0026602925366283952, "phrase": "algorithm_implementation"}, {"score": 0.002586972610351776, "phrase": "embedded_hardware_system_requirements"}, {"score": 0.0025510720021028107, "phrase": "parameterizable_model"}, {"score": 0.002524473047010871, "phrase": "different_applications"}, {"score": 0.002420815160284086, "phrase": "fuzzy_activation_functions"}, {"score": 0.0023540798554571745, "phrase": "exhaustive_analysis"}, {"score": 0.00227323596233465, "phrase": "classification_results"}, {"score": 0.0022105600003496225, "phrase": "matlab_floating_point_results"}, {"score": 0.00217987162651941, "phrase": "hardware_related_analysis"}, {"score": 0.0021346343252001488, "phrase": "bit-length_accuracy"}, {"score": 0.0021049977753042253, "phrase": "logic_resource_occupation"}], "paper_keywords": ["Random Vector Functional-Link Networks", " Fast learning", " Matrix inversion", " Neural network training", " VHDL", " Embedded and real-time systems"], "paper_abstract": "Recently appeared a renewed interest for Single Layer Feedforward Neural Network (SLF-NN) models where the hidden layer coefficients are randomly assigned and the output coefficients are calculated by a least square algorithm. In addition to random coefficient initialization, the main advantages for these learning models are the speed of training (no multiple iterations required) and no initial coefficient definition (e.g. no adaptation constant as in multilayer perceptron). These features are adequate for real time operation since a fast online training can be achieved, benefiting to applications (industrial, automotive, portable systems) where other neural networks learning approaches could not be used due to large resource usage, low speed and lack of flexibility. Thus, targeting hardware implementation allows its use in embedded systems, expanding its application areas to real time systems and, in general, those applications where the use of desktop computers is not possible. Typically, RVFLN demands a wide number of resources and a high computational burden; high dimension matrices are involved, and computation intensive algorithms are required to obtain the output layer coefficient values for the neural network, especially matrix inversion. This work describes the algorithm implementation and optimization of these models to fit embedded hardware system requirements together with a parameterizable model, allowing different applications to benefit from it. The proposal includes the use of fuzzy activation functions in neurons to reduce computations. An exhaustive analysis of three proposed different computation architectures for the learning algorithm is done. Classification results for three standard datasets and fixed point arithmetic are compared to Matlab floating point results, together with hardware related analysis as speed of operation, bit-length accuracy in fixed point arithmetic and logic resource occupation.", "paper_title": "Hardware implementation methods in Random Vector Functional-Link Networks", "paper_id": "WOS:000338214100012"}