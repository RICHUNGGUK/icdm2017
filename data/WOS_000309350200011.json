{"auto_keywords": [{"score": 0.04878556281654673, "phrase": "analysis"}, {"score": 0.004928673163393792, "phrase": "gpu"}, {"score": 0.00474946741204285, "phrase": "inclusion-based_points"}, {"score": 0.004642292792558686, "phrase": "graphics_processing_units"}, {"score": 0.004496280772084575, "phrase": "powerful_accelerators"}, {"score": 0.004374772264092067, "phrase": "dense_arrays"}, {"score": 0.004066498558415824, "phrase": "highly_irregular_algorithms"}, {"score": 0.0039928721865882, "phrase": "pointer-based_data_structures"}, {"score": 0.0037626342268042997, "phrase": "gpu_implementations"}, {"score": 0.0037284065559949064, "phrase": "graph_analysis_algorithms"}, {"score": 0.0034813964296826973, "phrase": "breadth-first_search"}, {"score": 0.003449718244515019, "phrase": "strongly-connected_components"}, {"score": 0.003310684108128973, "phrase": "high-performance_gpu_implementation"}, {"score": 0.003265591844292821, "phrase": "important_graph_algorithm"}, {"score": 0.0031497548858180043, "phrase": "llvm"}, {"score": 0.0028082176224611542, "phrase": "extensive_modifications"}, {"score": 0.0027322025124213566, "phrase": "underlying_graph"}, {"score": 0.0026949679939994226, "phrase": "relatively_little_computation"}, {"score": 0.002493333098814506, "phrase": "average_speedup"}, {"score": 0.0024258208033731154, "phrase": "sequential_cpu_implementation"}, {"score": 0.0023818288346547692, "phrase": "parallel_implementation"}, {"score": 0.002275299774554847, "phrase": "general_insights"}, {"score": 0.0022238315004700607, "phrase": "high-performance_gpu_implementations"}, {"score": 0.0021438877099090262, "phrase": "key_differences"}, {"score": 0.0021243539405631866, "phrase": "optimizing_parallel_programs"}, {"score": 0.0021049977753042253, "phrase": "multicore_cpus"}], "paper_keywords": ["Algorithms", " Languages", " Performance", " Inclusion-based Points-to Analysis", " Irregular Programs", " Graph Algorithms", " GPU", " CUDA"], "paper_abstract": "Graphics Processing Units (GPUs) have emerged as powerful accelerators for many regular algorithms that operate on dense arrays and matrices. In contrast, we know relatively little about using GPUs to accelerate highly irregular algorithms that operate on pointer-based data structures such as graphs. For the most part, research has focused on GPU implementations of graph analysis algorithms that do not modify the structure of the graph, such as algorithms for breadth-first search and strongly-connected components. In this paper, we describe a high-performance GPU implementation of an important graph algorithm used in compilers such as gcc and LLVM: Andersen-style inclusion-based points-to analysis. This algorithm is challenging to parallelize effectively on GPUs because it makes extensive modifications to the structure of the underlying graph and performs relatively little computation. In spite of this, our program, when executed on a 14 Streaming Multiprocessor GPU, achieves an average speedup of 7x compared to a sequential CPU implementation and outperforms a parallel implementation of the same algorithm running on 16 CPU cores. Our implementation provides general insights into how to produce high-performance GPU implementations of graph algorithms, and it highlights key differences between optimizing parallel programs for multicore CPUs and for GPUs.", "paper_title": "A GPU Implementation of Inclusion-based Points-to Analysis", "paper_id": "WOS:000309350200011"}