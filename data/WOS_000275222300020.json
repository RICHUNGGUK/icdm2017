{"auto_keywords": [{"score": 0.0383005943678633, "phrase": "existing_algorithms"}, {"score": 0.00481495049065317, "phrase": "adaptive_markov_reward_processes."}, {"score": 0.004403480614319231, "phrase": "stochastic_optimization_problem"}, {"score": 0.004306205029641728, "phrase": "adaptive_markov_reward_processes"}, {"score": 0.0039380388343479384, "phrase": "adjustable_parameters"}, {"score": 0.003808209866576388, "phrase": "unknown_constant_parameters"}, {"score": 0.0031845067058084583, "phrase": "novel_two_time-scale_gradient_approximation_algorithm"}, {"score": 0.0028795712616923462, "phrase": "small_sample_path_variation"}, {"score": 0.002815864555488112, "phrase": "low_computational_cost"}, {"score": 0.00269263690712392, "phrase": "mild_assumptions"}, {"score": 0.0024346858809238766, "phrase": "proposed_algorithm"}, {"score": 0.0022013916728990564, "phrase": "numerical_examples"}], "paper_keywords": ["Adaptive Markov reward processes", " Two time-scale", " Gradient approximation"], "paper_abstract": "In this paper, we study the stochastic optimization problem of adaptive Markov reward processes parameterized by two sets of parameters, including adjustable parameters, and unknown constant parameters. As the existing algorithms do not work well for this problem, we propose a novel two time-scale gradient approximation algorithm. This new algorithm yields fast convergence, small sample path variation and low computational cost. Under some mild assumptions, we theoretically prove the convergence of the proposed algorithm, and compare it with the existing algorithms through numerical examples, which confirms its superiority.", "paper_title": "TWO TIME-SCALE GRADIENT APPROXIMATION ALGORITHM FOR ADAPTIVE MARKOV REWARD PROCESSES", "paper_id": "WOS:000275222300020"}