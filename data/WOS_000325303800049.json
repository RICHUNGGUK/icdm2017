{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "genetic_algorithm"}, {"score": 0.049631680892871025, "phrase": "simulated_annealing"}, {"score": 0.048313706083567, "phrase": "ensemble_learning"}, {"score": 0.04015970079348806, "phrase": "generalization_ability"}, {"score": 0.004725385523804256, "phrase": "automatic_neural_network_ensemble_development"}, {"score": 0.004508661664944657, "phrase": "valuable_strategy"}, {"score": 0.00444559516711374, "phrase": "computational_intelligence_modeling"}, {"score": 0.00440403952059725, "phrase": "machine_learning_community"}, {"score": 0.004261617028111862, "phrase": "multiple_models"}, {"score": 0.004047020753789236, "phrase": "different_data"}, {"score": 0.0039716833701918365, "phrase": "prediction_performance"}, {"score": 0.003684048704119722, "phrase": "key_factors"}, {"score": 0.003649585593445665, "phrase": "ensemble_systems"}, {"score": 0.003531474887552889, "phrase": "ensemble_members"}, {"score": 0.0034657012607680173, "phrase": "ensemble_system_performance"}, {"score": 0.003369322866499541, "phrase": "unified_procedure"}, {"score": 0.0031695693589575916, "phrase": "based_approaches"}, {"score": 0.003125175084962886, "phrase": "automatic_development"}, {"score": 0.003095923814666088, "phrase": "neural_network_ensembles"}, {"score": 0.003066945490483524, "phrase": "regression_problems"}, {"score": 0.0030239842475439814, "phrase": "main_contribution"}, {"score": 0.002912331664922248, "phrase": "optimization_techniques"}, {"score": 0.0028580564897994175, "phrase": "best_subset"}, {"score": 0.0024014759148123736, "phrase": "proposed_methodologies"}, {"score": 0.002301896758256575, "phrase": "simple_bagging"}, {"score": 0.002280333649167796, "phrase": "negative_correlation_learning"}, {"score": 0.0022064385857118655, "phrase": "gasen"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Ensemble learning", " Neural network", " Genetic algorithm", " Simulated annealing"], "paper_abstract": "In the last decades ensemble learning has established itself as a valuable strategy within the computational intelligence modeling and machine learning community. Ensemble learning is a paradigm where multiple models combine in some way their decisions, or their learning algorithms, or different data to improve the prediction performance. Ensemble learning aims at improving the generalization ability and the reliability of the system. Key factors of ensemble systems are diversity, training and combining ensemble members to improve the ensemble system performance. Since there is no unified procedure to address all these issues, this work proposes and compares Genetic Algorithm and Simulated Annealing based approaches for the automatic development of Neural Network Ensembles for regression problems. The main contribution of this work is the development of optimization techniques that selects the best subset of models to be aggregated taking into account all the key factors of ensemble systems (e.g., diversity, training ensemble members and combination strategy). Experiments on two well-known data sets are reported to evaluate the effectiveness of the proposed methodologies. Results show that these outperform other approaches including Simple Bagging, Negative Correlation Learning (NCL), AdaBoost and GASEN in terms of generalization ability. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Comparison of a genetic algorithm and simulated annealing for automatic neural network ensemble development", "paper_id": "WOS:000325303800049"}