{"auto_keywords": [{"score": 0.041969999861445444, "phrase": "multiclass_margin"}, {"score": 0.00481495049065317, "phrase": "large-margin_multiclass_loss-based_boosting"}, {"score": 0.0046739596755881185, "phrase": "scalable_and_effective_classification_model"}, {"score": 0.004510184935928004, "phrase": "multiclass_classification_problems"}, {"score": 0.004430452655732319, "phrase": "direct_formulation"}, {"score": 0.004378078970574258, "phrase": "multiclass_boosting"}, {"score": 0.003933609372601274, "phrase": "major_problem"}, {"score": 0.0036408354779365643, "phrase": "real-world_problems"}, {"score": 0.003450966222451678, "phrase": "scalable_and_simple_stagewise_multiclass_boosting_method"}, {"score": 0.0028865314258560214, "phrase": "training_time"}, {"score": 0.0027522340188241446, "phrase": "classification_accuracy"}, {"score": 0.0026714812866037584, "phrase": "traditional_adaboost"}, {"score": 0.0024577274522206436, "phrase": "excellent_generalization_performance"}, {"score": 0.002428617044669043, "phrase": "experimental_results"}, {"score": 0.002399850604120431, "phrase": "challenging_multiclass_machine_learning"}, {"score": 0.0023018188296933923, "phrase": "proposed_approach"}, {"score": 0.0022476051902301187, "phrase": "convergence_rate"}, {"score": 0.002181626277903721, "phrase": "final_visual_detector"}, {"score": 0.002142970278347126, "phrase": "additional_computational_cost"}, {"score": 0.0021049977753042253, "phrase": "existing_multiclass_boosting"}], "paper_keywords": ["Boosting", " column generation", " convex optimization", " multiclass classification"], "paper_abstract": "We present a scalable and effective classification model to train multiclass boosting for multiclass classification problems. A direct formulation of multiclass boosting had been introduced in the past in the sense that it directly maximized the multiclass margin. The major problem of that approach is its high computational complexity during training, which hampers its application to real-world problems. In this brief, we propose a scalable and simple stagewise multiclass boosting method which also directly maximizes the multiclass margin. Our approach offers the following advantages: 1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy and 2) like traditional AdaBoost, it is less sensitive to the choice of parameters and empirically demonstrates excellent generalization performance. Experimental results on challenging multiclass machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost compared to existing multiclass boosting.", "paper_title": "A Scalable Stagewise Approach to Large-Margin Multiclass Loss-Based Boosting", "paper_id": "WOS:000334738400013"}