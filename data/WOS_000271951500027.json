{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cross_validation"}, {"score": 0.004621984064678252, "phrase": "n-dimensional_real-valued_vector_x"}, {"score": 0.004189698902282913, "phrase": "approximate_sparsity_level"}, {"score": 0.004054745478848129, "phrase": "theoretical_guarantee"}, {"score": 0.0040052617265655035, "phrase": "quality_assurance"}, {"score": 0.003956379470617348, "phrase": "resulting_cs_estimate"}, {"score": 0.0038603905208013482, "phrase": "underlying_sparsity"}, {"score": 0.003813269577615793, "phrase": "signal_x"}, {"score": 0.0036903962975894634, "phrase": "cs_estimate"}, {"score": 0.0031199418866663543, "phrase": "almost_no_effort"}, {"score": 0.0030193416820392554, "phrase": "maximum_number"}, {"score": 0.0027931892160452513, "phrase": "possible_estimates"}, {"score": 0.0023322317017275803, "phrase": "high_probability"}, {"score": 0.002275557501858879, "phrase": "numerical_upper_and_lower_bounds"}, {"score": 0.002202123699556382, "phrase": "best_k-term_approximation"}, {"score": 0.0021398115287338693, "phrase": "p_values"}, {"score": 0.0021049977753042253, "phrase": "almost_no_cost"}], "paper_keywords": ["Best k-term approximation", " compressed sensing (CS)", " cross validation", " encoding/decoding", " error estimates", " Johnson-Lindenstrauss (JL) lemma", " measurements"], "paper_abstract": "Compressed sensing (CS) decoding algorithms can efficiently recover an N-dimensional real-valued vector x to within a factor of its best k-term approximation by taking m = O(k log N/k) measurements y = Phi x. If the sparsity or approximate sparsity level of x were known, then this theoretical guarantee would imply quality assurance of the resulting CS estimate. However, because the underlying sparsity of the signal x is unknown, the quality of a CS estimate (x) over cap using m measurements is not assured. It is nevertheless shown in this paper that sharp bounds on the error parallel to x - (x) over cap parallel to(l2N) can be achieved with almost no effort. More precisely, suppose that a maximum number of measurements m is preimposed. One can reserve 10 log p of these m measurements and compute a sequence of possible estimates ((x) over cap (j))(j=1)(p) to x from the m - 10 log p remaining measurements; the errors parallel to x - (x) over cap parallel to(l2N) for j = 1, ..., p can then be bounded with high probability. As a consequence, numerical upper and lower bounds on the error between x and the best k-term approximation to x can be estimated for p values of k with almost no cost. This observation has applications outside CS as well.", "paper_title": "Compressed Sensing With Cross Validation", "paper_id": "WOS:000271951500027"}