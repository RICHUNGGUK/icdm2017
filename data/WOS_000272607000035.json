{"auto_keywords": [{"score": 0.04015555003324516, "phrase": "data_sets"}, {"score": 0.026051181810325553, "phrase": "class_labels"}, {"score": 0.00481495049065317, "phrase": "feature_selection"}, {"score": 0.004760765880314089, "phrase": "important_role"}, {"score": 0.004724980378009193, "phrase": "pattern_classification"}, {"score": 0.0046192222897398685, "phrase": "redundant_features"}, {"score": 0.004464987136420614, "phrase": "useless_features"}, {"score": 0.0040323765107766335, "phrase": "new_and_emerging_techniques"}, {"score": 0.0039124037252916055, "phrase": "larger_and_larger_and_many_irrelevant_features"}, {"score": 0.0037249853525936428, "phrase": "great_challenges"}, {"score": 0.0036969566358089644, "phrase": "traditional_learning_algorithms"}, {"score": 0.003641528016135388, "phrase": "low_efficiency"}, {"score": 0.003467040410632942, "phrase": "efficient_technique"}, {"score": 0.00340217066797559, "phrase": "redundant_or_irrelevant_features"}, {"score": 0.003026127736667187, "phrase": "general_scheme"}, {"score": 0.0029920129598858545, "phrase": "feature_selection_method"}, {"score": 0.0029249294679606656, "phrase": "primary_characteristic"}, {"score": 0.002795228413298423, "phrase": "salient_features"}, {"score": 0.0026511422582578027, "phrase": "candidate_feature"}, {"score": 0.002572158343938064, "phrase": "information_criteria"}, {"score": 0.002543148295176345, "phrase": "feature_selector"}, {"score": 0.002495521662819317, "phrase": "relevant_degree"}, {"score": 0.0023848164282879885, "phrase": "selected_feature_subset"}, {"score": 0.0023137485947956732, "phrase": "simulation_studies"}, {"score": 0.002279011030331704, "phrase": "uci_data_sets"}, {"score": 0.0022447938263410005, "phrase": "classification_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Classification", " Boosting", " Feature selection", " Information metric", " Filter model"], "paper_abstract": "Feature selection plays an important role in pattern classification. Its purpose is to remove redundant features from data set as many as possible. The presence of useless features may not only deteriorate the performance of learning algorithms, but also obscure important information (e.g., intrinsic structure) behind data. Along with new and emerging techniques, data sets in many domains are becoming larger and larger and many irrelevant features are often prevailing in these data sets. This, however, poses great challenges to traditional learning algorithms, such as low efficiency and over-fitting. Thus, it becomes apparent that an efficient technique is needed to eliminate redundant or irrelevant features from the data sets. Currently, many endeavors to cope with this problem have been attempted and various outstanding feature selection methods have been proposed. Unlike other selection methods, in this paper we propose a general scheme of boosting feature selection method using information metric. The primary characteristic of our method is that it exploits weight of data to select salient features. Furthermore, the weight of data will be dynamically changed after each candidate feature has been selected. Thus, the information criteria used in feature selector can exactly represent the relevant degree between features and the class labels. As a result, the selected feature subset has maximal relevance to the class labels. Simulation studies carried out on UCI data sets show that the classification performance achieved by our proposed method is better than those of other selection methods in most cases. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Boosting feature selection using information metric for classification", "paper_id": "WOS:000272607000035"}