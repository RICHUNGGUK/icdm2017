{"auto_keywords": [{"score": 0.03835049443893983, "phrase": "observed_behavior"}, {"score": 0.008826526056243431, "phrase": "behavior_acquisition"}, {"score": 0.00481495049065317, "phrase": "shared_values"}, {"score": 0.004780784194987702, "phrase": "neurophysiology"}, {"score": 0.0046797114400479135, "phrase": "mirror_neurons"}, {"score": 0.0045971138571344345, "phrase": "macaque_monkeys"}, {"score": 0.003944112858235306, "phrase": "others'_behavior"}, {"score": 0.0034567253581991226, "phrase": "reinforcement_learning_scheme"}, {"score": 0.00335950642866031, "phrase": "observed_agent"}, {"score": 0.0032418062548293745, "phrase": "precise_world_model"}, {"score": 0.0032073043728420843, "phrase": "coordinate_transformation_system"}, {"score": 0.003150612723641925, "phrase": "view_difference"}, {"score": 0.0031170783606525856, "phrase": "different_viewpoints"}, {"score": 0.002912803994461996, "phrase": "precise_object_trajectory"}, {"score": 0.0028207566216121856, "phrase": "estimated_utility_transition"}, {"score": 0.002570817338292482, "phrase": "recognition_performance"}, {"score": 0.0025434384320561403, "phrase": "state_value_updates"}, {"score": 0.0024718415971488627, "phrase": "real_trial"}, {"score": 0.0024281181691350085, "phrase": "learned_values"}, {"score": 0.0024022553322977165, "phrase": "recognition_system"}, {"score": 0.0023180144105895257, "phrase": "state_value"}, {"score": 0.0022447206386952126, "phrase": "proposed_method"}, {"score": 0.002181514206602054, "phrase": "dynamic_environment"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Behavior recognition", " Value system", " Learning by observation"], "paper_abstract": "Neurophysiology has revealed the existence of mirror neurons in the brain of macaque monkeys that activate when the monkey executes a goal directed behavior and also when it observes the same behavior performed by another. The concept of the mirror neurons/systems (Oztop and Kawato, 2006) [19] suggests that behavior acquisition and understanding of others' behavior are related. We propose a method not only to learn and execute a variety of behaviors but also to understand observed behavior, supposing that the observer has already acquired the utilities (state values in reinforcement learning scheme) of all kinds of behavior the observed agent can do. The method does not need a precise world model or a coordinate transformation system to deal with a view difference caused by different viewpoints. This paper shows that an observer can understand/recognize behaviors shown by a demonstrator based not on a precise object trajectory in allocentric/egocentric coordinate space but rather on an estimated utility transition during the observed behavior. Furthermore, it is shown that the loop of the behavior acquisition and recognition of observed behavior accelerates the learning and improves the recognition performance. The state value updates can be accelerated by the observation without real trial and error while the learned values enrich the recognition system, since it is based on estimation of the state value of the observed behavior. The validity of the proposed method is shown by applying it to a dynamic environment where two robots play soccer. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Emulation and behavior understanding through shared values", "paper_id": "WOS:000279573100006"}