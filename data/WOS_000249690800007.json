{"auto_keywords": [{"score": 0.042150781020745584, "phrase": "decision_tree_size"}, {"score": 0.03441652477229554, "phrase": "decision_tree"}, {"score": 0.00481495049065317, "phrase": "monotone_decision_trees"}, {"score": 0.004773904938276846, "phrase": "polynomial_time"}, {"score": 0.004593467306126913, "phrase": "monotone_boolean_function"}, {"score": 0.004270960586542813, "phrase": "constant_accuracy"}, {"score": 0.0041984140090404985, "phrase": "uniform_distribution"}, {"score": 0.003920295222165263, "phrase": "first_algorithm"}, {"score": 0.00385368199735867, "phrase": "arbitrary_monotone_boolean_functions"}, {"score": 0.0037558704408683905, "phrase": "random_examples"}, {"score": 0.003629291866667764, "phrase": "reasonable_measure"}, {"score": 0.003522027550662857, "phrase": "key_ingredient"}, {"score": 0.003359816816853762, "phrase": "average_sensitivity"}, {"score": 0.003316884515934182, "phrase": "monotone_function"}, {"score": 0.0030705342386533083, "phrase": "independent_utility"}, {"score": 0.0030054007508185858, "phrase": "decision_tree_complexity"}, {"score": 0.0029797347088361056, "phrase": "o._schramm"}, {"score": 0.002954287204671032, "phrase": "r._o'donnell"}, {"score": 0.002929056388593134, "phrase": "m._saks"}, {"score": 0.0028916124463906983, "phrase": "r._servedio"}, {"score": 0.0028181504086563967, "phrase": "influential_variable"}, {"score": 0.0027826459015330694, "phrase": "proceedings"}, {"score": 0.0027230880516264685, "phrase": "ieee_computer_society"}, {"score": 0.0026998264237910884, "phrase": "los_alamitos"}, {"score": 0.0026774840370292394, "phrase": "ca"}, {"score": 0.0025315664299815537, "phrase": "basic_inequality"}, {"score": 0.002404522326122611, "phrase": "partition_size"}, {"score": 0.002244969789802093, "phrase": "boolean_cube"}, {"score": 0.002105139765368486, "phrase": "boolean"}], "paper_keywords": ["learning", " monotone", " decision trees"], "paper_abstract": "We give an algorithm that learns any monotone Boolean function f : {-1, 1}(n) -> {-1, 1} to any constant accuracy, under the uniform distribution, in time polynomial in n and in the decision tree size of f. This is the first algorithm that can learn arbitrary monotone Boolean functions to high accuracy, using random examples only, in time polynomial in a reasonable measure of the complexity of f. A key ingredient of the result is a new bound showing that the average sensitivity of any monotone function computed by a decision tree of size s must be at most v log s. This bound has proved to be of independent utility in the study of decision tree complexity [ O. Schramm, R. O'Donnell, M. Saks, and R. Servedio, Every decision tree has an influential variable, in Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science, IEEE Computer Society, Los Alamitos, CA, 2005, pp. 31-39]. We generalize the basic inequality and learning result described above in various ways-specifically, to partition size (a stronger complexity measure than decision tree size), p-biased measures over the Boolean cube (rather than just the uniform distribution), and real-valued (rather than just Boolean-valued) functions.", "paper_title": "Learning monotone decision trees in polynomial time", "paper_id": "WOS:000249690800007"}