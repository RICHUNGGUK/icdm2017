{"auto_keywords": [{"score": 0.04934553301452924, "phrase": "gaussian_process_regression"}, {"score": 0.00481495049065317, "phrase": "evaluating_approximation_methods"}, {"score": 0.004388796812234033, "phrase": "machine_learning"}, {"score": 0.0042552618754756934, "phrase": "even_a_straightforward_implementation"}, {"score": 0.004125791474525642, "phrase": "gpr"}, {"score": 0.0032972747743006603, "phrase": "relative_merits"}, {"score": 0.0032216622984027558, "phrase": "different_approximations"}, {"score": 0.002675797331220082, "phrase": "compute_time"}, {"score": 0.002554406040896888, "phrase": "standard_baselines"}, {"score": 0.00238254295201662, "phrase": "fitc"}, {"score": 0.0021049977753042253, "phrase": "future_comparisons"}], "paper_keywords": ["Gaussian process regression", " subset of data", " FITC", " local GP"], "paper_abstract": "Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n(2)) space and O(n(3)) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e. g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons.", "paper_title": "A Framework for Evaluating Approximation Methods for Gaussian Process Regression", "paper_id": "WOS:000315981900002"}