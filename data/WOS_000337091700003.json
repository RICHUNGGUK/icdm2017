{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "domain_adaptation"}, {"score": 0.00466016686289196, "phrase": "visual_domain_adaptation"}, {"score": 0.004595356579614969, "phrase": "object_models"}, {"score": 0.004510336430422002, "phrase": "visual_domain"}, {"score": 0.00436530211426514, "phrase": "unified_flexible_model"}, {"score": 0.0042446888127531945, "phrase": "-supervised_learning"}, {"score": 0.003570889780957037, "phrase": "metric_learning_methods"}, {"score": 0.0033919477901198716, "phrase": "domain_shift"}, {"score": 0.0033447157263095223, "phrase": "final_classifier"}, {"score": 0.0030604342671868836, "phrase": "independent_transformations"}, {"score": 0.0029343128039167185, "phrase": "target_domain"}, {"score": 0.0028664985822266344, "phrase": "labeled_examples"}, {"score": 0.0027612315954598085, "phrase": "target_and_source_features"}, {"score": 0.00263505298504827, "phrase": "joint_learning_framework"}, {"score": 0.0026105159490849364, "phrase": "adaptive_classifiers"}, {"score": 0.002562123444678208, "phrase": "competing_methods"}, {"score": 0.002514625755809045, "phrase": "multi-class_accuracy"}, {"score": 0.00237733844863506, "phrase": "object_recognition_models"}, {"score": 0.0022793032191981404, "phrase": "differing_imaging_conditions"}, {"score": 0.0022580713471027996, "phrase": "feature_types"}, {"score": 0.0021447748132703078, "phrase": "previous_approaches"}, {"score": 0.0021049977753042253, "phrase": "large-scale_scenarios"}], "paper_keywords": ["Object recognition", " Domain adaptation", " Transformation learning"], "paper_abstract": "We address the problem of visual domain adaptation for transferring object models from one dataset or visual domain to another. We introduce a unified flexible model for both supervised and semi-supervised learning that allows us to learn transformations between domains. Additionally, we present two instantiations of the model, one for general feature adaptation/alignment, and one specifically designed for classification. First, we show how to extend metric learning methods for domain adaptation, allowing for learning metrics independent of the domain shift and the final classifier used. Furthermore, we go beyond classical metric learning by extending the method to asymmetric, category independent transformations. Our framework can adapt features even when the target domain does not have any labeled examples for some categories, and when the target and source features have different dimensions. Finally, we develop a joint learning framework for adaptive classifiers, which outperforms competing methods in terms of multi-class accuracy and scalability. We demonstrate the ability of our approach to adapt object recognition models under a variety of situations, such as differing imaging conditions, feature types, and codebooks. The experiments show its strong performance compared to previous approaches and its applicability to large-scale scenarios.", "paper_title": "Asymmetric and Category Invariant Feature Transformations for Domain Adaptation", "paper_id": "WOS:000337091700003"}