{"auto_keywords": [{"score": 0.02520807633363263, "phrase": "kwknn"}, {"score": 0.00481495049065317, "phrase": "k-nearest_neighbours"}, {"score": 0.004766090327062509, "phrase": "time_series_modelling"}, {"score": 0.004717723626157272, "phrase": "least_squares"}, {"score": 0.004669845451682529, "phrase": "vector_machines"}, {"score": 0.004552255360140208, "phrase": "gaussian_kernel"}, {"score": 0.004392565073466714, "phrase": "kernel_methods"}, {"score": 0.004260134699483774, "phrase": "regression_and_time_series_prediction"}, {"score": 0.0041316803676012155, "phrase": "good_behaviour"}, {"score": 0.0037498152007206815, "phrase": "feature_selection"}, {"score": 0.003563172322934695, "phrase": "training_samples"}, {"score": 0.0035269696779744266, "phrase": "time_series_prediction"}, {"score": 0.00342054627009158, "phrase": "regression_problem"}, {"score": 0.0032172054825137866, "phrase": "traditional_approach"}, {"score": 0.0031360786664165093, "phrase": "typical_recursive_or_direct_strategies"}, {"score": 0.0030569913145731408, "phrase": "long-term_prediction"}, {"score": 0.002676692093594358, "phrase": "large_scale_prediction"}, {"score": 0.0026359661683151006, "phrase": "simple_methodology"}, {"score": 0.0026091594819333654, "phrase": "kernel_creation"}, {"score": 0.002530359917257626, "phrase": "time_series_data"}, {"score": 0.002441422510910065, "phrase": "lssvm_models"}, {"score": 0.002416589459425828, "phrase": "lower_computational_cost"}, {"score": 0.0023079225317714815, "phrase": "function_approximation"}, {"score": 0.002272794656458419, "phrase": "parallel_version"}, {"score": 0.0021705796895687864, "phrase": "large_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Kernel methods", " Time series modelling", " Specific to problem kernels", " Kernel weighted K-Nearest Neighbours", " Least squares support vector machines", " Parallelization of kernel methods"], "paper_abstract": "Least squares support vector machines (LSSVM) with Gaussian kernel represent the most used of the kernel methods existing in the literature for regression and time series prediction. These models have a good behaviour for these types of problems due to their generalization capabilities and their smooth interpolation, but they are very dependent on the feature selection performed and their computational cost notably increases with the number of training samples. Time series prediction can be tackled as a regression problem by constructing a set of input/output data from the series; this traditional approach and the use of typical recursive or direct strategies present serious drawbacks in long-term prediction. This paper presents an alternative based on the settings of specific-to-problem kernels to be applied to time series prediction focusing on large scale prediction. A simple methodology for kernel creation based on the periodicities in time series data is proposed. An alternative to LSSVM models with lower computational cost, the Kernel Weighted K-Nearest Neighbours (KWKNN) is described for function approximation. A parallel version of KWKNN is also presented to deal with large data sets. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Design of specific-to-problem kernels and use of kernel weighted K-nearest neighbours for time series modelling", "paper_id": "WOS:000279134100047"}