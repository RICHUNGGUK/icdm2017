{"auto_keywords": [{"score": 0.025201051895757488, "phrase": "kinect"}, {"score": 0.00481495049065317, "phrase": "natural_interaction"}, {"score": 0.004780926543714981, "phrase": "smart_spaces"}, {"score": 0.004730338746443662, "phrase": "key_issue"}, {"score": 0.004549366134876482, "phrase": "personalized_interaction_methods"}, {"score": 0.004406436661912347, "phrase": "physical_and_virtual_resources"}, {"score": 0.004298367681427065, "phrase": "multiple_interaction_approaches"}, {"score": 0.0041485458333515, "phrase": "gesture-based_ones"}, {"score": 0.004075596668736259, "phrase": "great_potential"}, {"score": 0.003933508945770709, "phrase": "gesture-based_interaction_method"}, {"score": 0.0038780652371817447, "phrase": "specific_grammar"}, {"score": 0.00376950244909, "phrase": "smart_space"}, {"score": 0.0037295765964056255, "phrase": "method's_grammar"}, {"score": 0.0035236476185949565, "phrase": "individuation_gesture"}, {"score": 0.0033647036658823798, "phrase": "action_gesture"}, {"score": 0.003201516237269705, "phrase": "second_object"}, {"score": 0.0029925953740661754, "phrase": "action_gestures"}, {"score": 0.0027972697690729453, "phrase": "interaction_cues"}, {"score": 0.002699627613420169, "phrase": "main_component"}, {"score": 0.002642681666526916, "phrase": "gesture_recognition_module"}, {"score": 0.002605384872894451, "phrase": "adapted_dynamic_time_warping_algorithm"}, {"score": 0.0025144238077014114, "phrase": "data_inputs"}, {"score": 0.002435270624076196, "phrase": "infrastructure-based_solutions"}, {"score": 0.002308834230545883, "phrase": "average_recognition_rate"}, {"score": 0.0022762383517229957, "phrase": "smartphone-based_recognition"}, {"score": 0.0022441016229967025, "phrase": "kinect-based_one"}, {"score": 0.0021734395455430167, "phrase": "architecture_and_software_tools"}, {"score": 0.0021427510493208864, "phrase": "interaction_method"}, {"score": 0.0021049977753042253, "phrase": "real_environment"}], "paper_keywords": ["Natural interaction", " mobile applications", " smart objects", " smart spaces", " pattern recognition", " gesture recognition"], "paper_abstract": "A key issue when making spaces smart is the availability of satisfying and personalized interaction methods, for the user to comfortably manage the physical and virtual resources in the environment. Among the multiple interaction approaches that are nowadays being explored with this objective, gesture-based ones seem to have a great potential. In this line, this research describes a gesture-based interaction method that uses a specific grammar to control and network objects in a smart space. The method's grammar establishes that the user has to identify the object to interact with by performing an individuation gesture primitive (the object's initial letter), then using an action gesture primitive to indicate the action to perform. The action may involve a second object, which will be identified as the first one. The user will be able to personalize the action gestures in the vocabulary, and to configure the commands assigned to gestures, and the system will provide interaction cues for the user not to feel lost. The main component of the system is a gesture recognition module based on an adapted Dynamic Time Warping algorithm. This module works sharply on acceleration or position data inputs, being suitable to deploy device instrumented or infrastructure-based solutions for gesture recognition (i.e. smartphone or Kinect-based ones). The average recognition rate is 93.63% for smartphone-based recognition and 98.64% for Kinect-based one, respectively. The paper also details the architecture and software tools that enables the interaction method to work in a real environment.", "paper_title": "A gesture-based method for natural interaction in smart spaces", "paper_id": "WOS:000357989200009"}