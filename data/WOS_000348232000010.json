{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "parallel_applications"}, {"score": 0.0047283116134926645, "phrase": "device-level_memory_abstraction"}, {"score": 0.004518442937844318, "phrase": "increasingly_important_role"}, {"score": 0.004437115547184535, "phrase": "high-performance_computing"}, {"score": 0.004278807087784626, "phrase": "naive_code"}, {"score": 0.004088807948906792, "phrase": "massively_parallel_applications"}, {"score": 0.004015182362035361, "phrase": "deep_understanding"}, {"score": 0.003907212558276185, "phrase": "underlying_architecture"}, {"score": 0.0036663981146086103, "phrase": "complex_index_calculations"}, {"score": 0.003600351201298387, "phrase": "manual_memory_transfers"}, {"score": 0.0034403745992576808, "phrase": "memory_access_patterns"}, {"score": 0.0032282396745967504, "phrase": "berkeley's_parallel_\"dwarfs"}, {"score": 0.0030291453183733897, "phrase": "maps_framework"}, {"score": 0.002765776659294382, "phrase": "memory_access"}, {"score": 0.0026427848602443267, "phrase": "complex_indexing"}, {"score": 0.0021436720821474973, "phrase": "carefully_optimized_implementations"}, {"score": 0.0021049977753042253, "phrase": "real-world_applications"}], "paper_keywords": ["Parallelism", " Abstraction", " Performance", " GPGPU", " memory abstraction", " heterogeneous computing architectures", " memory access patterns"], "paper_abstract": "GPUs play an increasingly important role in high-performance computing. While developing naive code is straightforward, optimizing massively parallel applications requires deep understanding of the underlying architecture. The developer must struggle with complex index calculations and manual memory transfers. This article classifies memory access patterns used in most parallel algorithms, based on Berkeley's Parallel \"Dwarfs.\" It then proposes the MAPS framework, a device-level memory abstraction that facilitates memory access on GPUs, alleviating complex indexing using on-device containers and iterators. This article presents an implementation of MAPS and shows that its performance is comparable to carefully optimized implementations of real-world applications.", "paper_title": "MAPS: Optimizing Massively Parallel Applications Using Device-Level Memory Abstraction", "paper_id": "WOS:000348232000010"}