{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "general_object_recognition"}, {"score": 0.04965786117733892, "phrase": "image_understanding"}, {"score": 0.036413345484819205, "phrase": "large_image"}, {"score": 0.0295049584209137, "phrase": "salient_objects"}, {"score": 0.004586531354697466, "phrase": "dramatic_goal"}, {"score": 0.004546174820239776, "phrase": "computer_vision"}, {"score": 0.004506171772318298, "phrase": "multimedia_retrieval"}, {"score": 0.004388252997694986, "phrase": "great_efforts"}, {"score": 0.004217110862974562, "phrase": "open_problem"}, {"score": 0.004052616113326177, "phrase": "selective_attention-driven_model"}, {"score": 0.003929107176349023, "phrase": "gorium"}, {"score": 0.0037260386566859197, "phrase": "key_idea"}, {"score": 0.003628460216207393, "phrase": "recurring_visual_objects"}, {"score": 0.0035965032173588753, "phrase": "selective_attention_modeling"}, {"score": 0.0033955105098235345, "phrase": "unsupervised_manner"}, {"score": 0.0032199307816384577, "phrase": "four-layer_bottomup_model"}, {"score": 0.003121719509312159, "phrase": "automatic_object"}, {"score": 0.002999823428906084, "phrase": "multi-task_learning_methods"}, {"score": 0.0028699415886017468, "phrase": "top-down_factors"}, {"score": 0.002832081745025006, "phrase": "lowest_layer"}, {"score": 0.0027094427948305515, "phrase": "second_layer"}, {"score": 0.0026501241093379786, "phrase": "effective_learning_approach"}, {"score": 0.002436289099444074, "phrase": "complex_scene"}, {"score": 0.0023935128386292966, "phrase": "third_layer"}, {"score": 0.002320449502337321, "phrase": "unsupervised_approach"}, {"score": 0.002279702500235741, "phrase": "general_objects"}, {"score": 0.002200337772403983, "phrase": "local_invariant_features"}, {"score": 0.002161695357601592, "phrase": "visual_dictionary_construction"}], "paper_keywords": ["object recognition", " image understanding", " visual saliency", " salient object segmentation", " visual dictionary"], "paper_abstract": "General object recognition and image understanding is recognized as a dramatic goal for computer vision and multimedia retrieval. In spite of the great efforts devoted in the last two decades, it still remains an open problem. In this paper, we propose a selective attention-driven model for general image understanding, named GORIUM (general object recognition and image understanding model). The key idea of our model is to discover recurring visual objects by selective attention modeling and pairwise local invariant features matching on a large image set in an unsupervised manner. Towards this end, it can be formulated as a four-layer bottomup model, i.e., salient region detection, object segmentation, automatic object discovering and visual dictionary construction. By exploiting multi-task learning methods to model visual saliency simultaneously with the bottom-up and top-down factors, the lowest layer can effectively detect salient objects in an image. The second layer exploits a simple yet effective learning approach to generate two complementary maps from several raw saliency maps, which then can be utilized to segment the salient objects precisely from a complex scene. For the third layer, we have also implemented an unsupervised approach to automatically discover general objects from large image set by pairwise matching with local invariant features. Afterwards, visual dictionary construction can be implemented by using many state-of-the-art algorithms and tools available nowadays.", "paper_title": "Salient region detection and segmentation for general object recognition and image understanding", "paper_id": "WOS:000297709400002"}