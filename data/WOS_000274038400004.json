{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "collective_annotations"}, {"score": 0.04616065522315394, "phrase": "argumentative_debates"}, {"score": 0.0042373381286051354, "phrase": "highly_cognitive_task"}, {"score": 0.00402610372759447, "phrase": "group's_global_opinion"}, {"score": 0.0031981188104809994, "phrase": "global_confirmation"}, {"score": 0.0028139168128067343, "phrase": "prominent_information_systems"}, {"score": 0.002539978170928796, "phrase": "social_validation_measure"}, {"score": 0.002433843373780471, "phrase": "online_study"}], "paper_keywords": [""], "paper_abstract": "People taking part in argumentative debates through collective annotations face a highly cognitive task when trying to estimate the group's global opinion. In order to reduce this effort, we propose in this paper to model such debates prior to evaluating their \"social validation.\" Computing the degree of global confirmation (or refutation) enables the identification of consensual (or controversial) debates. Readers as well as prominent information systems may thus benefit from this information. The accuracy of the social validation measure was tested through an online study conducted with 121 participants. We compared their human perception of consensus in argumentative debates with the results of the three proposed social validation algorithms. Their efficiency in synthesizing opinions was demonstrated by the fact that they achieved an accuracy of up to 84%.", "paper_title": "Social Validation of Collective Annotations: Definition and Experiment", "paper_id": "WOS:000274038400004"}