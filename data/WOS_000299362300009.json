{"auto_keywords": [{"score": 0.046339294991504736, "phrase": "search_engines"}, {"score": 0.04457074599735022, "phrase": "search_engine"}, {"score": 0.015719716506582538, "phrase": "web_search_engines"}, {"score": 0.01373314442719836, "phrase": "ask.com"}, {"score": 0.013337317650881628, "phrase": "best_performance"}, {"score": 0.004744437370338926, "phrase": "metasearch_engines"}, {"score": 0.004390764654147291, "phrase": "reported_study"}, {"score": 0.004139121616131771, "phrase": "google"}, {"score": 0.003844478919573134, "phrase": "weighted_average"}, {"score": 0.003816197777388722, "phrase": "similarity_degrees"}, {"score": 0.003466865016432561, "phrase": "similarity_degree"}, {"score": 0.0031377955308198634, "phrase": "results_presentation"}, {"score": 0.003035177667751495, "phrase": "retrieval_effectiveness"}, {"score": 0.0027469703311742647, "phrase": "human-based_ones"}, {"score": 0.002726740692380339, "phrase": "findings_-_google"}, {"score": 0.002657106199831846, "phrase": "bing"}, {"score": 0.0026084451971464867, "phrase": "significant_degrees"}, {"score": 0.0025230952890994236, "phrase": "automatic_and_human-based_approaches"}, {"score": 0.0023781827711414107, "phrase": "truly_effective_search_engine"}, {"score": 0.002120624523437494, "phrase": "valuable_experimental_results"}], "paper_keywords": ["Search engines", " Performance evaluation", " Metasearch engines", " Information retrieval", " Automation", " Information searches", " Function evaluation"], "paper_abstract": "Purpose - The purpose of this paper is to introduce two new automatic methods for evaluating the performance of search engines. The reported study uses the methods to experimentally investigate which search engine among three popular search engines (Ask.com, Bing and Google) gives the best performance. Design/methodology/approach - The study assesses the performance of three search engines. For each one the weighted average of similarity degrees between its ranked result list and those of its metasearch engines is measured. Next these measures are compared to establish which search engine gives the best performance. To compute the similarity degree between the lists two measures called the \"tendency degree\" and \"coverage degree\" are introduced; the former assesses a search engine in terms of results presentation and the latter evaluates it in terms of retrieval effectiveness. The performance of the search engines is experimentally assessed based on the 50 topics of the 2002 TREC web track. The effectiveness of the methods is also compared with human-based ones. Findings - Google outperformed the others, followed by Bing and Ask.com. Moreover significant degrees of consistency - 92.87 percent and 91.93 percent - were found between automatic and human-based approaches. Practical implications - The findings of this work could help users to select a truly effective search engine. The results also provide motivation for the vendors of web search engines to improve their technology. Originality/value - The paper focuses on two novel automatic methods to evaluate the performance of search engines and provides valuable experimental results on three popular ones.", "paper_title": "Automatic performance evaluation of web search engines using judgments of metasearch engines", "paper_id": "WOS:000299362300009"}