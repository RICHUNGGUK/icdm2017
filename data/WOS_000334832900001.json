{"auto_keywords": [{"score": 0.05007712399672984, "phrase": "file_deduplication_framework"}, {"score": 0.026241804536545506, "phrase": "hadoop"}, {"score": 0.004749106104067268, "phrase": "hdfs._file_systems"}, {"score": 0.0041666157975446564, "phrase": "file_contents"}, {"score": 0.004109600767747015, "phrase": "file_systems"}, {"score": 0.004025531820226182, "phrase": "duplicate_copies"}, {"score": 0.003916098498242888, "phrase": "redundant_consumptions"}, {"score": 0.0038624981944755813, "phrase": "storage_space"}, {"score": 0.003809628721278519, "phrase": "network_bandwidth"}, {"score": 0.0036553070575015344, "phrase": "complex_and_challenging_issue"}, {"score": 0.003531474887552889, "phrase": "deduplication_technologies"}, {"score": 0.0033651019362620866, "phrase": "storage_efficiency"}, {"score": 0.003097865379563127, "phrase": "offline_solutions"}, {"score": 0.0030554300179552415, "phrase": "primary_storages"}, {"score": 0.0030135741897058844, "phrase": "backup_systems"}, {"score": 0.0029518598926717332, "phrase": "subfile_or_whole-file_level"}, {"score": 0.0027741761880779535, "phrase": "file_servers"}, {"score": 0.0027361626349903744, "phrase": "database_systems"}, {"score": 0.0026252106881752067, "phrase": "cloud_file_system_deduplication_technologies"}, {"score": 0.0021789194955896102, "phrase": "distributed_file_system"}, {"score": 0.0021490449563443025, "phrase": "cloud_application_developers"}], "paper_keywords": [""], "paper_abstract": "File systems are designed to control how files are stored and retrieved. Without knowing the context and semantics of file contents, file systems often contain duplicate copies and result in redundant consumptions of storage space and network bandwidth. It has been a complex and challenging issue for enterprises to seek deduplication technologies to reduce cost and increase the storage efficiency. To solve such problem, researchers proposed in-line or offline solutions for primary storages or backup systems at the subfile or whole-file level. Some of the technologies are used for file servers and database systems. Fewer studies focus on the cloud file system deduplication technologies at the application level, especially for the Hadoop distributed file system. It is the goal of this paper to design a file deduplication framework on Hadoop distributed file system for cloud application developers. The architecture, interface, and implementation experiences are also shared in this paper.", "paper_title": "Design and Implementation of File Deduplication Framework on HDFS", "paper_id": "WOS:000334832900001"}