{"auto_keywords": [{"score": 0.03917269004419755, "phrase": "unsupervised_models"}, {"score": 0.0351970340194402, "phrase": "supervised_models"}, {"score": 0.025299381701068747, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "multiple_supervised"}, {"score": 0.004732444672509326, "phrase": "ensemble_learning"}, {"score": 0.004651346011959237, "phrase": "powerful_method"}, {"score": 0.004603352344461032, "phrase": "multiple_models"}, {"score": 0.004571630734130793, "phrase": "well-known_methods"}, {"score": 0.004431554974667461, "phrase": "model_averaging"}, {"score": 0.004266141613620429, "phrase": "single_models"}, {"score": 0.004164094489641152, "phrase": "high_costs"}, {"score": 0.004135387221172467, "phrase": "manual_labeling"}, {"score": 0.004036455094603843, "phrase": "sufficient_and_reliable_labeled_data"}, {"score": 0.003912712778141845, "phrase": "unlabeled_data"}, {"score": 0.0037665925842197967, "phrase": "multiple_unsupervised_models"}, {"score": 0.0036510928952124522, "phrase": "class_label_prediction"}, {"score": 0.0035637052288771706, "phrase": "useful_constraints"}, {"score": 0.0035268943257252224, "phrase": "joint_predictions"}, {"score": 0.003466382911409222, "phrase": "related_objects"}, {"score": 0.003290986600659723, "phrase": "better_prediction_performance"}, {"score": 0.0031461489526079236, "phrase": "multiple_supervised_and_unsupervised_models"}, {"score": 0.002986907667028281, "phrase": "classification_solution"}, {"score": 0.002915370532555375, "phrase": "supervised_predictions"}, {"score": 0.002895246938820814, "phrase": "unsupervised_constraints"}, {"score": 0.002845541839157798, "phrase": "ensemble_task"}, {"score": 0.0028161281608563267, "phrase": "optimization_problem"}, {"score": 0.0027870176748502045, "phrase": "bipartite_graph"}, {"score": 0.0027486699590262343, "phrase": "objective_function"}, {"score": 0.0025914765323897604, "phrase": "initial_labeling"}, {"score": 0.0024945743145462144, "phrase": "iterative_propagation"}, {"score": 0.0024773480858940504, "phrase": "probability_estimates"}, {"score": 0.002460240519198386, "phrase": "neighboring_nodes"}, {"score": 0.002319503690965857, "phrase": "constrained_embedding"}, {"score": 0.0022955150243239623, "phrase": "transformed_space"}, {"score": 0.002225024057634929, "phrase": "experimental_results"}, {"score": 0.0022096550791379033, "phrase": "different_applications"}, {"score": 0.0021943920257952174, "phrase": "heterogeneous_data_sources"}, {"score": 0.0021343846011837204, "phrase": "existing_alternatives"}], "paper_keywords": ["Knowledge integration", " ensemble learning", " clustering ensemble", " semi-supervised learning"], "paper_abstract": "Ensemble learning has emerged as a powerful method for combining multiple models. Well-known methods, such as bagging, boosting, and model averaging, have been shown to improve accuracy and robustness over single models. However, due to the high costs of manual labeling, it is hard to obtain sufficient and reliable labeled data for effective training. Meanwhile, lots of unlabeled data exist in these sources, and we can readily obtain multiple unsupervised models. Although unsupervised models do not directly generate a class label prediction for each object, they provide useful constraints on the joint predictions for a set of related objects. Therefore, incorporating these unsupervised models into the ensemble of supervised models can lead to better prediction performance. In this paper, we study ensemble learning with outputs from multiple supervised and unsupervised models, a topic where little work has been done. We propose to consolidate a classification solution by maximizing the consensus among both supervised predictions and unsupervised constraints. We cast this ensemble task as an optimization problem on a bipartite graph, where the objective function favors the smoothness of the predictions over the graph, but penalizes the deviations from the initial labeling provided by the supervised models. We solve this problem through iterative propagation of probability estimates among neighboring nodes and prove the optimality of the solution. The proposed method can be interpreted as conducting a constrained embedding in a transformed space, or a ranking on the graph. Experimental results on different applications with heterogeneous data sources demonstrate the benefits of the proposed method over existing alternatives. (More information, data, and code are available at http://www.cse.buffalo.edu/similar to jing/integrate.htm.)", "paper_title": "A Graph-Based Consensus Maximization Approach for Combining Multiple Supervised and Unsupervised Models", "paper_id": "WOS:000312890500002"}