{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "neural_probabilistic_language_model"}, {"score": 0.004732785510739708, "phrase": "previous_work_oil_statistical_language_modeling"}, {"score": 0.004342430078306461, "phrase": "feedforward_neural_network"}, {"score": 0.003949974260471287, "phrase": "significant_error_reduction"}, {"score": 0.003816197777388722, "phrase": "standard_baseline_models"}, {"score": 0.003471135453226184, "phrase": "neural_network_model"}, {"score": 0.003382547312072468, "phrase": "maximum-likelihood_criterion"}, {"score": 0.0028715299874549245, "phrase": "adaptive_importance_sampling"}, {"score": 0.002294686545883412, "phrase": "neural_network"}, {"score": 0.0021049977753042253, "phrase": "standard_problems"}], "paper_keywords": ["energy-based models", " fast training", " importance sampling", " language modeling", " Monte Carlo methods", " probabilistic neural networks"], "paper_abstract": "Previous work oil statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based oil n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use in adaptive,n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup call be obtained on standard problems.", "paper_title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "paper_id": "WOS:000258766700012"}