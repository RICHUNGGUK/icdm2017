{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "summarizing_data_succinctly"}, {"score": 0.004745665541429529, "phrase": "knowledge_discovery"}, {"score": 0.004677372885331908, "phrase": "inherently_iterative_process"}, {"score": 0.004312622975366676, "phrase": "new_knowledge"}, {"score": 0.004129017936622578, "phrase": "redundant_results"}, {"score": 0.0041051328120736575, "phrase": "knowledge_discovery_algorithms"}, {"score": 0.003941746077126688, "phrase": "well-founded_approach"}, {"score": 0.0038179283509963695, "phrase": "probabilistic_maximum_entropy_model"}, {"score": 0.003399466007862412, "phrase": "maximum_entropy_principle"}, {"score": 0.0033699898113395328, "phrase": "unbiased_probabilistic_models"}, {"score": 0.0032263862712733934, "phrase": "current_model"}, {"score": 0.0030267301404032893, "phrase": "mtv"}, {"score": 0.002974448752618528, "phrase": "top-k_most_informative_itemsets"}, {"score": 0.0026175952455594277, "phrase": "one-phase_algorithm"}, {"score": 0.0023921371117388143, "phrase": "efficient_method"}, {"score": 0.0023644936731468252, "phrase": "maximum_entropy_distribution"}, {"score": 0.0023507916951658455, "phrase": "quick_inclusion-exclusion"}, {"score": 0.0022702274881448692, "phrase": "real_data"}, {"score": 0.002237477619351199, "phrase": "discovered_summaries"}, {"score": 0.0021860555252905876, "phrase": "key_patterns"}, {"score": 0.00212343307513777, "phrase": "high_likelihoods"}, {"score": 0.0021049977753042253, "phrase": "inspection_shows"}], "paper_keywords": ["Frequent itemsets", " pattern sets", " summarization", " maximum entropy", " minimum description length principle", " MDL", " BIC", " inclusion-exclusion"], "paper_abstract": "Knowledge discovery from data is an inherently iterative process. That is, what we know about the data greatly determines our expectations, and therefore, what results we would find interesting and/or surprising. Given new knowledge about the data, our expectations will change. Hence, in order to avoid redundant results, knowledge discovery algorithms ideally should follow such an iterative updating procedure. With this in mind, we introduce a well-founded approach for succinctly summarizing data with the most informative itemsets; using a probabilistic maximum entropy model, we iteratively find the itemset that provides us the most novel information-that is, for which the frequency in the data surprises us the most-and in turn we update our model accordingly. As we use the maximum entropy principle to obtain unbiased probabilistic models, and only include those itemsets that are most informative with regard to the current model, the summaries we construct are guaranteed to be both descriptive and nonredundant. The algorithm that we present, called MTV, can either discover the top-k most informative itemsets, or we can employ either the Bayesian Information Criterion (BIC) or the Minimum Description Length (MDL) principle to automatically identify the set of itemsets that together summarize the data well. In other words, our method will \"tell you what you need to know\" about the data. Importantly, it is a one-phase algorithm: rather than picking itemsets from a user-provided candidate set, itemsets and their supports are mined on-the-fly. To further its applicability, we provide an efficient method to compute the maximum entropy distribution using Quick Inclusion-Exclusion. Experiments on our method, using synthetic, benchmark, and real data, show that the discovered summaries are succinct, and correctly identify the key patterns in the data. The models they form attain high likelihoods, and inspection shows that they summarize the data well with increasingly specific, yet nonredundant itemsets.", "paper_title": "Summarizing Data Succinctly with the Most Informative ltemsets", "paper_id": "WOS:000312924700003"}