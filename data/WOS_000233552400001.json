{"auto_keywords": [{"score": 0.0496729815270419, "phrase": "data_fusion"}, {"score": 0.013735732053183231, "phrase": "automatic_ranking"}, {"score": 0.00481495049065317, "phrase": "information_retrieval_systems"}, {"score": 0.004673530058379246, "phrase": "information_retrieval"}, {"score": 0.004421770937919324, "phrase": "search_quality"}, {"score": 0.0043842558719324526, "phrase": "dynamic_environments"}, {"score": 0.0042373381286051354, "phrase": "new_methods"}, {"score": 0.0041657281535389615, "phrase": "retrieval_systems"}, {"score": 0.004008981656795115, "phrase": "retrieval_results"}, {"score": 0.00397495478332679, "phrase": "multiple_systems"}, {"score": 0.00387459052649752, "phrase": "top-ranked_documents"}, {"score": 0.0038253590626087237, "phrase": "merged_result"}, {"score": 0.003527712410706358, "phrase": "text_retrieval_conference"}, {"score": 0.0034386016613475335, "phrase": "statistically_significant_strong_correlations"}, {"score": 0.003409399407390298, "phrase": "human-based_assessments"}, {"score": 0.0030908254851076005, "phrase": "ordinary_systems"}, {"score": 0.003025597247856977, "phrase": "better_discrimination"}, {"score": 0.002742787059497186, "phrase": "new_method"}, {"score": 0.0025291601927956765, "phrase": "bias_concept"}, {"score": 0.002342110338541984, "phrase": "higher_bias"}, {"score": 0.0023123057299992587, "phrase": "data_fusion_process"}, {"score": 0.0022634699331607615, "phrase": "even_higher_correlations"}, {"score": 0.0022346638243276717, "phrase": "human-based_results"}, {"score": 0.002159623311924437, "phrase": "previously_proposed_automatic_ranking_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["data fusion", " experimentation", " information retrieval", " performance evaluation", " rank aggregation"], "paper_abstract": "Measuring effectiveness of information retrieval (IR) systems is essential for research and development and for monitoring search quality in dynamic environments. In this study, we employ new methods for automatic ranking of retrieval systems. In these methods, we merge the retrieval results of multiple systems using various data fusion algorithms, use the top-ranked documents in the merged result as the \"(pseudo) relevant documents,\" and employ these documents to evaluate and rank the systems. Experiments using Text REtrieval Conference (TREC) data provide statistically significant strong correlations with human-based assessments of the same systems. We hypothesize that the selection of systems that would return documents different from the majority could eliminate the ordinary systems from data fusion and provide better discrimination among the documents and systems. This could improve the effectiveness of automatic ranking. Based on this intuition, we introduce a new method for the selection of systems to be used for data fusion. For this purpose, we use the bias concept that measures the deviation of a system from the norm or majority and employ the systems with higher bias in the data fusion process. This approach provides even higher correlations with the human-based results. We demonstrate that our approach outperforms the previously proposed automatic ranking methods. (c) 2005 Elsevier Ltd. All rights reserved.", "paper_title": "Automatic ranking of information retrieval systems using data fusion", "paper_id": "WOS:000233552400001"}