{"auto_keywords": [{"score": 0.04849105041601803, "phrase": "cholesky"}, {"score": 0.00481495049065317, "phrase": "statically_partitioned_hypermatrix_sparse_cholesky_factorization"}, {"score": 0.004500891637057374, "phrase": "large_matrices"}, {"score": 0.00413684709497912, "phrase": "sparse_hypermatrix_storage_scheme"}, {"score": 0.003899612025233852, "phrase": "sparse_matrix"}, {"score": 0.0036759314172445934, "phrase": "dense_matrices"}, {"score": 0.003266201217790818, "phrase": "sparse_matrices"}, {"score": 0.00305285084207725, "phrase": "dense_blocks"}, {"score": 0.0022906498741695094, "phrase": "sparse_cholesky_factorization"}, {"score": 0.0022145052062614514, "phrase": "hypermatrix_storage_structure"}], "paper_keywords": [""], "paper_abstract": "The sparse Cholesky factorization of some large matrices can require a two dimensional partitioning of the matrix. The sparse hypermatrix storage scheme produces a recursive 2D partitioning of a sparse matrix. The subblocks are stored as dense matrices so BLAS3 routines can be used. However, since we are dealing with sparse matrices some zeros may be stored in those dense blocks. The overhead introduced by the operations on zeros can become large and considerably degrade performance. In this paper we present an improvement to our sequential in-core implementation of a sparse Cholesky factorization based on a hypermatrix storage structure. We compare its performance with several codes and analyze the results.", "paper_title": "Optimization of a statically partitioned hypermatrix sparse Cholesky factorization", "paper_id": "WOS:000237003200096"}