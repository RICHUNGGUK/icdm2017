{"auto_keywords": [{"score": 0.05004048994862916, "phrase": "trace_transform"}, {"score": 0.043825904344572245, "phrase": "human_action_recognition"}, {"score": 0.04103303975588275, "phrase": "trace"}, {"score": 0.0047707093928203, "phrase": "robust_human_action_recognition"}, {"score": 0.0047268728636719725, "phrase": "machine_based_human_action_recognition"}, {"score": 0.004555499634179394, "phrase": "automatic_unattended_surveillance_systems"}, {"score": 0.0040215052006051235, "phrase": "known_transform"}, {"score": 0.003734979290204709, "phrase": "specific_transform"}, {"score": 0.003683565150278339, "phrase": "first_method"}, {"score": 0.0036496814103475174, "phrase": "trace_transforms"}, {"score": 0.003616108223034427, "phrase": "binarized_silhouettes"}, {"score": 0.0035663245542174224, "phrase": "different_stages"}, {"score": 0.003517223847512266, "phrase": "single_action_period"}, {"score": 0.003468796799802612, "phrase": "final_history_template"}, {"score": 0.0033428820402140683, "phrase": "whole_sequence"}, {"score": 0.0032665093519968083, "phrase": "valuable_spatiotemporal_information"}, {"score": 0.0032066653632708965, "phrase": "human_action"}, {"score": 0.0030056887638095883, "phrase": "invariant_features"}, {"score": 0.002950608985750058, "phrase": "action_sequence"}, {"score": 0.0028303313661029597, "phrase": "video_capturing"}, {"score": 0.0027913353347879507, "phrase": "specific_method"}, {"score": 0.0027275289970185015, "phrase": "natural_specifications"}, {"score": 0.002640636493247601, "phrase": "noise_robust_features"}, {"score": 0.0023198194192868143, "phrase": "weizmann"}, {"score": 0.0022878396720586044, "phrase": "radial_basis_function"}, {"score": 0.0022458849724980904, "phrase": "kernel_svm"}, {"score": 0.002164264600379492, "phrase": "proposed_techniques"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Human action recognition", " Motion analysis", " Action classification", " Trace transform"], "paper_abstract": "Machine based human action recognition has become very popular in the last decade. Automatic unattended surveillance systems, interactive video games, machine learning and robotics are only few of the areas that involve human action recognition. This paper examines the capability of a known transform, the so-called Trace, for human action recognition and proposes two new feature extraction methods based on the specific transform. The first method extracts Trace transforms from binarized silhouettes, representing different stages of a single action period. A final history template composed from the above transforms, represents the whole sequence containing much of the valuable spatiotemporal information contained in a human action. The second, involves Trace for the construction of a set of invariant features that represent the action sequence and can cope with variations usually appeared in video capturing. The specific method takes advantage of the natural specifications of the Trace transform, to produce noise robust features that are invariant to translation, rotation, scaling and are effective, simple and fast to create. Classification experiments performed on two well known and challenging action datasets (KTH and Weizmann) using Radial Basis Function (RBF) Kernel SVM provided very competitive results indicating the potentials of the proposed techniques. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Exploring trace transform for robust human action recognition", "paper_id": "WOS:000323804100007"}