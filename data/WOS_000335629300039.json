{"auto_keywords": [{"score": 0.03988311462256843, "phrase": "anomaly_detection"}, {"score": 0.00481495049065317, "phrase": "statistical_anomaly_detection"}, {"score": 0.004777034835192062, "phrase": "anomaly_detection_accuracy"}, {"score": 0.004702092660370367, "phrase": "serious_limitation"}, {"score": 0.0046650615456476155, "phrase": "commercial_ads_deployments"}, {"score": 0.004610058444728656, "phrase": "main_reason"}, {"score": 0.004344577216137524, "phrase": "extremely_low_computational_complexity"}, {"score": 0.00425951071471662, "phrase": "low_computational_cost"}, {"score": 0.004110549138321181, "phrase": "cheap_high-performance_platforms"}, {"score": 0.003828012660311739, "phrase": "current_adss"}, {"score": 0.0037679023889496885, "phrase": "aggregate_feature_spaces"}, {"score": 0.0037234376132464463, "phrase": "large_volumes"}, {"score": 0.0036217089991592275, "phrase": "-benign_feature_instances"}, {"score": 0.0035648266669315943, "phrase": "feature_space"}, {"score": 0.003508834578859094, "phrase": "low_accuracies"}, {"score": 0.0031035949505887083, "phrase": "higher_computational_resource_utilization"}, {"score": 0.00303072749758085, "phrase": "existing_adss"}, {"score": 0.0029595657769783314, "phrase": "better_computational_platforms"}, {"score": 0.0029246121262946384, "phrase": "higher_accuracies"}, {"score": 0.0028110458014332187, "phrase": "fundamental_accuracy"}, {"score": 0.0027668603377225564, "phrase": "statistical_network"}, {"score": 0.0027450280220853137, "phrase": "host-based_adss"}, {"score": 0.002546002312015341, "phrase": "statistical_ads'_feature_space"}, {"score": 0.0023613726075968986, "phrase": "generic_information-theoretic_methods"}, {"score": 0.002278639906498682, "phrase": "appropriate_number"}, {"score": 0.0022339335462778437, "phrase": "statistical_ads._performance_evaluation"}, {"score": 0.0021471293746360026, "phrase": "enhanced_adss"}, {"score": 0.0021049977753042253, "phrase": "dramatic_improvements"}], "paper_keywords": ["Feature slicing", " Conditional entropy", " Information content", " Clustering", " Statistical anomaly detection"], "paper_abstract": "Anomaly detection accuracy has been a serious limitation in commercial ADS deployments. A main reason for this limitation is the expectation that an ADS should achieve very high accuracy while having extremely low computational complexity. The constraint of low computational cost has recently been relaxed with the emergence of cheap high-performance platforms (e.g., multi-core, CPU, SCC, etc.). Moreover, current ADSs perform anomaly detection on aggregate feature spaces, with large volumes of benign and close-to-benign feature instances that overwhelm the feature space and hence yield low accuracies. In this paper, we ask and address the following question: Can the accuracy of an ADS be improved if we slice ADS feature space at the cost of higher computational resource utilization? We first observe that existing ADSs are not designed to exploit better computational platforms to achieve higher accuracies. To mitigate this problem, we identify the fundamental accuracy limiting factors for statistical network and host-based ADSs. We then show that these bottlenecks can be alleviated by our proposed feature space slicing framework. Our framework slices a statistical ADS' feature space into multiple disjoint subspaces and then performs anomaly detection separately on each subspace by utilizing more computational resources. We propose generic information-theoretic methods for feature space slicing and for determining the appropriate number of subspaces for any statistical ADS. Performance evaluation on three independently-collected attack datasets and multiple ID algorithms shows that the enhanced ADSs are able to achieve dramatic improvements in detection (up to 75%) and false alarm (up to 99%) rates. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Information theoretic feature space slicing for statistical anomaly detection", "paper_id": "WOS:000335629300039"}