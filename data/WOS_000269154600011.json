{"auto_keywords": [{"score": 0.02854123585988545, "phrase": "state_transitions"}, {"score": 0.00481495049065317, "phrase": "multichannel_opportunistic_access"}, {"score": 0.004709193603747991, "phrase": "opportunistic_communication"}, {"score": 0.004667541072056214, "phrase": "multiple_channels"}, {"score": 0.0040309035566691645, "phrase": "limited_channel"}, {"score": 0.0037043953071235155, "phrase": "sensing_result"}, {"score": 0.0036390887584943723, "phrase": "time_slot"}, {"score": 0.0034807976670686628, "phrase": "\"good\"_channel"}, {"score": 0.003359119985242403, "phrase": "channel_selection_policy"}, {"score": 0.003299880298525127, "phrase": "expected_total"}, {"score": 0.0031845067058084583, "phrase": "finite_or_infinite_horizon"}, {"score": 0.0030595112110658675, "phrase": "partially_observed_markov_decision_process"}, {"score": 0.003032442109419577, "phrase": "pomdp"}, {"score": 0.0029789104204803137, "phrase": "restless_multiarmed_bandit_process"}, {"score": 0.0029263562233195423, "phrase": "optimal_solutions"}, {"score": 0.0027989800906333784, "phrase": "myopic_policy"}, {"score": 0.0027495914857929584, "phrase": "immediate_one-step_reward"}, {"score": 0.0021910825909008946, "phrase": "opportunistic_transmission_scheduling"}, {"score": 0.0021620042653528846, "phrase": "fading_environment"}, {"score": 0.0021049977753042253, "phrase": "spectrum_overlay"}], "paper_keywords": ["Cognitive radio", " Gittins index", " myopic policy", " opportunistic access", " partially observed Markov decision process (POMDP)", " restless bandit", " Whittle's index"], "paper_abstract": "This paper considers opportunistic communication over multiple channels where the state (\"good\" or \"bad\") of each channel evolves as independent and identically distributed (i.i.d.) Markov processes. A user, with limited channel sensing capability, chooses one channel to sense and decides whether to use the channel (based on the sensing result) in each time slot. A reward is obtained whenever the user senses and accesses a \"good\" channel. The objective is to design a channel selection policy that maximizes the expected total (discounted or average) reward accrued over a finite or infinite horizon. This problem can be cast as a partially observed Markov decision process (POMDP) or a restless multiarmed bandit process, to which optimal solutions are often intractable. This paper shows that a myopic policy that maximizes the immediate one-step reward is optimal when the state transitions are positively correlated over time. When the state transitions are negatively correlated, we show that the same policy is optimal when the number of channels is limited to two or three, while presenting a counterexample for the case of four channels. This result finds applications in opportunistic transmission scheduling in a fading environment, cognitive radio networks for spectrum overlay, and resource-constrained jamming and antijamming.", "paper_title": "Optimality of Myopic Sensing in Multichannel Opportunistic Access", "paper_id": "WOS:000269154600011"}