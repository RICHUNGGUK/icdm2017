{"auto_keywords": [{"score": 0.04678601157753345, "phrase": "msda"}, {"score": 0.015719713042815744, "phrase": "restricted_gaussian_model"}, {"score": 0.010635986879418786, "phrase": "discriminant_subspace"}, {"score": 0.004583579470330216, "phrase": "theoretical_link"}, {"score": 0.004533685795068016, "phrase": "mixture_subclass_discriminant_analysis"}, {"score": 0.003702323283528696, "phrase": "appropriate_gaussian_model"}, {"score": 0.0035826234811610316, "phrase": "new_da_method"}, {"score": 0.003524227590204673, "phrase": "expectation_maximization"}, {"score": 0.00319324425647735, "phrase": "maximum_likelihood_estimates"}, {"score": 0.0029736479837154843, "phrase": "conventional_da._fsmsda"}, {"score": 0.002925149105307855, "phrase": "subclass_separation_problem"}, {"score": 0.0026213749921120623, "phrase": "inter-between-subclass_scatter_matrix"}, {"score": 0.002522662955545598, "phrase": "appropriate_weighting_scheme"}, {"score": 0.0024410093024417527, "phrase": "iterative_algorithm"}, {"score": 0.0024011766029754006, "phrase": "useful_discriminant_directions"}, {"score": 0.002323446065342625, "phrase": "kmsda"}, {"score": 0.002285527382381719, "phrase": "kernel_trick"}, {"score": 0.002260591903283384, "phrase": "separate_data"}, {"score": 0.0022359278646792153, "phrase": "nonlinearly_separable_subclass_structure"}, {"score": 0.0021516991383785985, "phrase": "proposed_methods"}, {"score": 0.002128220632893247, "phrase": "conventional_msda"}], "paper_keywords": ["Classification", " clustering", " discriminant analysis", " feature extraction", " machine learning", " mixture of gaussians", " pattern recognition", " probabilistic algorithms"], "paper_abstract": "In this paper, a theoretical link between mixture subclass discriminant analysis (MSDA) and a restricted Gaussian model is first presented. Then, two further discriminant analysis (DA) methods, i.e., fractional step MSDA (FSMSDA) and kernel MSDA (KMSDA) are proposed. Linking MSDA to an appropriate Gaussian model allows the derivation of a new DA method under the expectation maximization (EM) framework (EM-MSDA), which simultaneously derives the discriminant subspace and the maximum likelihood estimates. The two other proposed methods generalize MSDA in order to solve problems inherited from conventional DA. FSMSDA solves the subclass separation problem, that is, the situation in which the dimensionality of the discriminant subspace is strictly smaller than the rank of the inter-between-subclass scatter matrix. This is done by an appropriate weighting scheme and the utilization of an iterative algorithm for preserving useful discriminant directions. On the other hand, KMSDA uses the kernel trick to separate data with nonlinearly separable subclass structure. Extensive experimentation shows that the proposed methods outperform conventional MSDA and other linear discriminant analysis variants.", "paper_title": "Mixture Subclass Discriminant Analysis Link to Restricted Gaussian Model and Other Generalizations", "paper_id": "WOS:000312844800002"}