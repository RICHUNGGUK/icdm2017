{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mobile_videos"}, {"score": 0.044585905779495894, "phrase": "domain_knowledge"}, {"score": 0.03878676535760206, "phrase": "sport_type"}, {"score": 0.03433183203055726, "phrase": "auxiliary_sensor_data"}, {"score": 0.004747997855358171, "phrase": "recent_proliferation"}, {"score": 0.004703878334423852, "phrase": "mobile_video_content"}, {"score": 0.004510336430422002, "phrase": "automatic_organization"}, {"score": 0.004468415400599571, "phrase": "automatic_editing"}, {"score": 0.004108161406159407, "phrase": "semantic_information"}, {"score": 0.0040133278382101885, "phrase": "challenging_task"}, {"score": 0.0038301525703886585, "phrase": "sport_events"}, {"score": 0.0037768417338850274, "phrase": "multiple_users"}, {"score": 0.0036213041338584756, "phrase": "american_football"}, {"score": 0.0033919477901198716, "phrase": "multi-user_and_multimodal_approach"}, {"score": 0.0032827577713351336, "phrase": "audio-visual_content"}, {"score": 0.002989714021608139, "phrase": "analysis_results"}, {"score": 0.002622755848331379, "phrase": "fusion_process"}, {"score": 0.002514625755809045, "phrase": "input_data"}, {"score": 0.00246800641971897, "phrase": "extensive_experiments"}, {"score": 0.002410942838102255, "phrase": "public_sport_events"}, {"score": 0.002333258365963461, "phrase": "different_combinations"}, {"score": 0.002289993730291138, "phrase": "fusion_methods"}, {"score": 0.002195552442011313, "phrase": "multi-user_data"}, {"score": 0.002154835714714195, "phrase": "adaptive_fusion"}, {"score": 0.0021247934141234988, "phrase": "classification_accuracies"}], "paper_keywords": ["Fusion", " mobile", " sport", " video"], "paper_abstract": "The recent proliferation of mobile video content has emphasized the need for applications such as automatic organization and automatic editing of videos. These applications could greatly benefit from domain knowledge about the content. However, extracting semantic information from mobile videos is a challenging task, due to their unconstrained nature. We extract domain knowledge about sport events recorded by multiple users, by classifying the sport type into soccer, American football, basketball, tennis, ice-hockey, or volleyball. We adopt a multi-user and multimodal approach, where each user simultaneously captures audio-visual content and auxiliary sensor data (from magnetometers and accelerometers). Firstly, each modality is separately analyzed; then, analysis results are fused for obtaining the sport type. The auxiliary sensor data is used for extracting more discriminative spatio-temporal visual features and efficient camera motion features. The contribution of each modality to the fusion process is adapted according to the quality of the input data. We performed extensive experiments on data collected at public sport events, showing the merits of using different combinations of modalities and fusion methods. The results indicate that analyzing multimodal and multi-user data, coupled with adaptive fusion, improves classification accuracies in most tested cases, up to 95.45%.", "paper_title": "Sport Type Classification of Mobile Videos", "paper_id": "WOS:000337955800003"}