{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "novel_approach"}, {"score": 0.004750527930368202, "phrase": "efficient_navigation_policies"}, {"score": 0.004708056866527125, "phrase": "mobile_robots"}, {"score": 0.004645057834614705, "phrase": "visual_features"}, {"score": 0.004541918674719973, "phrase": "fast_movements"}, {"score": 0.004481132747216135, "phrase": "mobile_robot"}, {"score": 0.004421156723819163, "phrase": "inherent_motion_blur"}, {"score": 0.0043619799095455415, "phrase": "acquired_images"}, {"score": 0.003916098498242888, "phrase": "navigation_task"}, {"score": 0.0037948184359488284, "phrase": "robot's_pose_estimate"}, {"score": 0.003628024127607343, "phrase": "reinforcement_learning_approach"}, {"score": 0.003563370084662192, "phrase": "navigation_policy"}, {"score": 0.003141830113006653, "phrase": "localization_accuracy"}, {"score": 0.003044455555031339, "phrase": "motion_blur"}, {"score": 0.0028586409810976367, "phrase": "learned_policy"}, {"score": 0.002820319063320601, "phrase": "clustering_approach"}, {"score": 0.0026962411675196213, "phrase": "policy_representation"}, {"score": 0.002543043781904587, "phrase": "memory-constrained_systems"}, {"score": 0.0025202582748972122, "phrase": "extensive_simulated_and_real-world_experiments"}, {"score": 0.0023557346035701096, "phrase": "constant_velocity"}, {"score": 0.0022118619998034742, "phrase": "different_indoor_and_outdoor_scenarios"}, {"score": 0.002124036931047162, "phrase": "navigation_tasks"}, {"score": 0.0021049977753042253, "phrase": "different_complexity"}], "paper_keywords": ["Navigation", " Reinforcement learning", " Vision", " Motion blur"], "paper_abstract": "In this article, we present a novel approach to learning efficient navigation policies for mobile robots that use visual features for localization. As fast movements of a mobile robot typically introduce inherent motion blur in the acquired images, the uncertainty of the robot about its pose increases in such situations. As a result, it cannot be ensured anymore that a navigation task can be executed efficiently since the robot's pose estimate might not correspond to its true location. We present a reinforcement learning approach to determine a navigation policy to reach the destination reliably and, at the same time, as fast as possible. Using our technique, the robot learns to trade off velocity against localization accuracy and implicitly takes the impact of motion blur on observations into account. We furthermore developed a method to compress the learned policy via a clustering approach. In this way, the size of the policy representation is significantly reduced, which is especially desirable in the context of memory-constrained systems. Extensive simulated and real-world experiments carried out with two different robots demonstrate that our learned policy significantly outperforms policies using a constant velocity and more advanced heuristics. We furthermore show that the policy is generally applicable to different indoor and outdoor scenarios with varying landmark densities as well as to navigation tasks of different complexity.", "paper_title": "Efficient vision-based navigation", "paper_id": "WOS:000278810500001"}