{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "online_modeling"}, {"score": 0.0047292449487169345, "phrase": "realtime_facial_animation"}, {"score": 0.004521566043596533, "phrase": "new_algorithm"}, {"score": 0.004441059405327222, "phrase": "realtime_face_tracking"}, {"score": 0.0043619799095455415, "phrase": "commodity_rgb-d_sensing_devices"}, {"score": 0.004133055661437163, "phrase": "user-specific_training"}, {"score": 0.00384633057714983, "phrase": "manual_assistance"}, {"score": 0.003611751858608048, "phrase": "new_applications"}, {"score": 0.0035473867669654174, "phrase": "performance-based_facial_animation"}, {"score": 0.003361069456060196, "phrase": "consumer_level"}, {"score": 0.0032716011062067286, "phrase": "key_novelty"}, {"score": 0.0031277316566463978, "phrase": "optimization_algorithm"}, {"score": 0.0028076592725295646, "phrase": "corresponding_dynamic_tracking_parameters"}, {"score": 0.0026841368358805407, "phrase": "robust_computations"}, {"score": 0.0025660347613972573, "phrase": "novel_subspace_parameterization"}, {"score": 0.00249767641333441, "phrase": "dynamic_facial_expression_space"}, {"score": 0.002387758958649797, "phrase": "detailed_evaluation"}, {"score": 0.0022218413146898887, "phrase": "performance_capture_workflow"}, {"score": 0.002143247921132179, "phrase": "accurate_facial_tracking"}, {"score": 0.0021049977753042253, "phrase": "realtime_applications"}], "paper_keywords": ["markerless performance capture", " face animation", " realtime tracking", " blendshape animation"], "paper_abstract": "We present a new algorithm for realtime face tracking on commodity RGB-D sensing devices. Our method requires no user-specific training or calibration, or any other form of manual assistance, thus enabling a range of new applications in performance-based facial animation and virtual interaction at the consumer level. The key novelty of our approach is an optimization algorithm that jointly solves for a detailed 3D expression model of the user and the corresponding dynamic tracking parameters. Realtime performance and robust computations are facilitated by a novel subspace parameterization of the dynamic facial expression space. We provide a detailed evaluation that shows that our approach significantly simplifies the performance capture workflow, while achieving accurate facial tracking for realtime applications.", "paper_title": "Online Modeling For Realtime Facial Animation", "paper_id": "WOS:000321840100009"}