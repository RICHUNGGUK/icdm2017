{"auto_keywords": [{"score": 0.04785254869208752, "phrase": "student's_response"}, {"score": 0.00481495049065317, "phrase": "intelligent_tutoring_systems"}, {"score": 0.00469159356595623, "phrase": "new_method"}, {"score": 0.004531997388646422, "phrase": "automated_tutor's_question"}, {"score": 0.004228839109569368, "phrase": "finer-grained_analysis"}, {"score": 0.004102639724186862, "phrase": "current_tutoring_systems"}, {"score": 0.0040672766251859975, "phrase": "entailment_databases"}, {"score": 0.003997458586709491, "phrase": "new_representation"}, {"score": 0.003962998498942108, "phrase": "reference_answers"}, {"score": 0.0038115535592184438, "phrase": "detailed_facets"}, {"score": 0.0037138072936226, "phrase": "student's_answer"}, {"score": 0.0036500346000495317, "phrase": "human_annotation"}, {"score": 0.0036029221645629225, "phrase": "detailed_level"}, {"score": 0.0035410467987580484, "phrase": "substantial_interannotator_agreement"}, {"score": 0.0034353020860809404, "phrase": "kappa_statistic"}, {"score": 0.00327545462323687, "phrase": "student_answers"}, {"score": 0.0032191849045822415, "phrase": "training_machine"}, {"score": 0.0031230216871739776, "phrase": "dependency_parses"}, {"score": 0.00308269056870936, "phrase": "reference_answer"}, {"score": 0.002990592465417758, "phrase": "domain-independent_lexical_statistics"}, {"score": 0.0026603411931700556, "phrase": "significant_contribution"}, {"score": 0.002558549084795168, "phrase": "significant_step"}, {"score": 0.0024928553725344933, "phrase": "domain-independent_semantic_assessment"}, {"score": 0.0024393978073660757, "phrase": "prior_work"}, {"score": 0.002387083859537412, "phrase": "tutoring_or_educational_assessment"}, {"score": 0.002227083991228041, "phrase": "learner_answers"}, {"score": 0.0021982976170703884, "phrase": "new_question"}, {"score": 0.0021049977753042253, "phrase": "hand-craft_information_extraction_templates"}], "paper_keywords": [""], "paper_abstract": "This paper describes a new method for recognizing whether a student's response to an automated tutor's question entails that they understand the concepts being taught. We demonstrate the need for a finer-grained analysis of answers than is supported by current tutoring systems or entailment databases and describe a new representation for reference answers that addresses these issues, breaking them into detailed facets and annotating their entailment relationships to the student's answer more precisely. Human annotation at this detailed level still results in substantial interannotator agreement (86.2%), with a kappa statistic of 0.728. We also present our current efforts to automatically assess student answers, which involves training machine learning classifiers on features extracted from dependency parses of the reference answer and student's response and features derived from domain-independent lexical statistics. Our system's performance, as high as 75.5% accuracy within domain and 68.3% out of domain, is very encouraging and confirms the approach is feasible. Another significant contribution of this work is that it represents a significant step in the direction of providing domain-independent semantic assessment of answers. No prior work in the area of tutoring or educational assessment has attempted to build such domain-independent systems. They have virtually all required hundreds of examples of learner answers for each new question in order to train aspects of their systems or to hand-craft information extraction templates.", "paper_title": "Recognizing entailment in intelligent tutoring systems", "paper_id": "WOS:000278491100002"}