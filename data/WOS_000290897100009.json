{"auto_keywords": [{"score": 0.047089431616928495, "phrase": "mutual_information"}, {"score": 0.004814954781473925, "phrase": "finite-sample"}, {"score": 0.004712726613162057, "phrase": "distribution-free"}, {"score": 0.004612662942073808, "phrase": "probabilistic_lower_bound"}, {"score": 0.004324985524216987, "phrase": "memoryless_communication_channel"}, {"score": 0.004187917910692495, "phrase": "binary-valued_input"}, {"score": 0.004055176557424983, "phrase": "one-dimensional_real-valued_output"}, {"score": 0.0035648266669315943, "phrase": "empirical_observations"}, {"score": 0.003167441161387607, "phrase": "dvoretzky-kiefer-wolfowitz_inequality"}, {"score": 0.002937879003671346, "phrase": "quadratic_time_algorithm"}, {"score": 0.0025003054002226965, "phrase": "existing_techniques"}, {"score": 0.002174041708084857, "phrase": "fano's_inequality"}, {"score": 0.0021049977753042253, "phrase": "continuous_random_variable"}], "paper_keywords": [""], "paper_abstract": "For any memoryless communication channel with a binary-valued input and a one-dimensional real-valued output, we introduce a probabilistic lower bound on the mutual information given empirical observations on the channel. The bound is built on the Dvoretzky-Kiefer-Wolfowitz inequality and is distribution free. A quadratic time algorithm is described for computing the bound and its corresponding class-conditional distribution functions. We compare our approach to existing techniques and show the superiority of our bound to a method inspired by Fano's inequality where the continuous random variable is discretized.", "paper_title": "A Finite-Sample, Distribution-Free, Probabilistic Lower Bound on Mutual Information", "paper_id": "WOS:000290897100009"}