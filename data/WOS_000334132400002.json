{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "sample_width"}, {"score": 0.004695900553353526, "phrase": "finite_metric_spaces"}, {"score": 0.004579780586951551, "phrase": "recent_paper"}, {"score": 0.004494570360108983, "phrase": "j._ratsaby"}, {"score": 0.004438794956203309, "phrase": "maximal"}, {"score": 0.004328856052035968, "phrase": "binary_functions"}, {"score": 0.004274981117736524, "phrase": "theoretical_computer_science"}, {"score": 0.003916098498242888, "phrase": "binary_classifiers"}, {"score": 0.003819187440994939, "phrase": "real_line"}, {"score": 0.0028986672783671147, "phrase": "finite_metric_space"}, {"score": 0.0028091907694771613, "phrase": "learning_problem"}, {"score": 0.002722468694023035, "phrase": "domination_numbers"}, {"score": 0.002371531125700046, "phrase": "underlying_metric_space"}], "paper_keywords": [""], "paper_abstract": "In a recent paper [M. Anthony, J. Ratsaby, Maximal width learning of binary functions, Theoretical Computer Science 411 (2010) 138-147] the notion of sample width for binary classifiers mapping from the real line was introduced, and it was shown that the performance of such classifiers could be quantified in terms of this quantity. This paper considers how to generalize the notion of sample width so that we can apply it where the classifiers map from some finite metric space. By relating the learning problem to one involving the domination numbers of certain graphs, we obtain generalization error bounds that depend on the sample width and on certain measures of 'density' of the underlying metric space. We also discuss how to employ a greedy set-covering heuristic to bound generalization error.(C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Learning bounds via sample width for classifiers on finite metric spaces", "paper_id": "WOS:000334132400002"}