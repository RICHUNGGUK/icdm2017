{"auto_keywords": [{"score": 0.040927564116666496, "phrase": "gait_traits"}, {"score": 0.04017990702934365, "phrase": "arbitrary_view"}, {"score": 0.03901080897702869, "phrase": "target_views"}, {"score": 0.029974031834022252, "phrase": "avtm"}, {"score": 0.00481495049065317, "phrase": "arbitrary_view_transformation_model"}, {"score": 0.0047057084749407485, "phrase": "useful_biometric_trait"}, {"score": 0.004669845451682529, "phrase": "person_authentication"}, {"score": 0.0045464538515249085, "phrase": "low_image_resolution"}, {"score": 0.004426308135681733, "phrase": "view_change"}, {"score": 0.0042928648367867835, "phrase": "view_transformation_models"}, {"score": 0.0037834649508375544, "phrase": "real_situation"}, {"score": 0.0036413370053909886, "phrase": "discrete_training_views"}, {"score": 0.003585986761045987, "phrase": "recognition_accuracy_degradation"}, {"score": 0.0035179764329354877, "phrase": "arbitrary_vtm"}, {"score": 0.0031240868074933078, "phrase": "test_subjects"}, {"score": 0.003088384809415812, "phrase": "target_scene"}, {"score": 0.0029951555610569225, "phrase": "training_subjects"}, {"score": 0.002806256131739047, "phrase": "gait_features"}, {"score": 0.0026494723416072316, "phrase": "part-dependent_view_selection_scheme"}, {"score": 0.0025694584194998356, "phrase": "gait_feature"}, {"score": 0.0025110337774260773, "phrase": "part-dependent_destination_views"}, {"score": 0.0024633601573822114, "phrase": "appropriate_destination_views"}, {"score": 0.0024258721867522707, "phrase": "different_body_parts"}, {"score": 0.0023981301755351607, "phrase": "part-dependent_destination_view_selection"}, {"score": 0.002325689073229477, "phrase": "increased_recognition_accuracy"}, {"score": 0.002290291401956985, "phrase": "data_sets"}, {"score": 0.0022640963589007457, "phrase": "different_settings"}, {"score": 0.0021872912745910127, "phrase": "cross-view_matching"}, {"score": 0.0021539955106204354, "phrase": "avtm_pdvs"}], "paper_keywords": ["Gait recognition"], "paper_abstract": "Gait recognition is a useful biometric trait for person authentication because it is usable even with low image resolution. One challenge is robustness to a view change (cross-view matching); view transformation models (VTMs) have been proposed to solve this. The VTMs work well if the target views are the same as their discrete training views. However, the gait traits are observed from an arbitrary view in a real situation. Thus, the target views may not coincide with discrete training views, resulting in recognition accuracy degradation. We propose an arbitrary VTM (AVTM) that accurately matches a pair of gait traits from an arbitrary view. To realize an AVTM, we first construct 3D gait volume sequences of training subjects, disjoint from the test subjects in the target scene. We then generate 2D gait silhouette sequences of the training subjects by projecting the 3D gait volume sequences onto the same views as the target views, and train the AVTM with gait features extracted from the 2D sequences. In addition, we extend our AVTM by incorporating a part-dependent view selection scheme (AVTM_PdVS), which divides the gait feature into several parts, and sets part-dependent destination views for transformation. Because appropriate destination views may differ for different body parts, the part-dependent destination view selection can suppress transformation errors, leading to increased recognition accuracy. Experiments using data sets collected in different settings show that the AVTM improves the accuracy of cross-view matching and that the AVTM_PdVS further improves the accuracy in many cases, in particular, verification scenarios.", "paper_title": "Gait-Based Person Recognition Using Arbitrary View Transformation Model", "paper_id": "WOS:000346595500001"}