{"auto_keywords": [{"score": 0.043216159049874636, "phrase": "classification_accuracy"}, {"score": 0.00481495049065317, "phrase": "numerical_and_nominal_features_based_on_probabilistic_dependence_between_features._data_classification_tasks"}, {"score": 0.0043953478557600565, "phrase": "difficult_computational_problem"}, {"score": 0.004345525254560329, "phrase": "feature_selection_techniques"}, {"score": 0.004058180509184539, "phrase": "michalak"}, {"score": 0.00401216440036853, "phrase": "kwasnicka"}, {"score": 0.0038114397487574838, "phrase": "feature_selection_strategy"}, {"score": 0.003641433168173135, "phrase": "pairwise_manner"}, {"score": 0.003559284776569813, "phrase": "assessed_level"}, {"score": 0.0033619033669956317, "phrase": "numerical_features"}, {"score": 0.0031573626285069157, "phrase": "linear_correlation_coefficients"}, {"score": 0.0030337085953063125, "phrase": "feature_selection_problem"}, {"score": 0.0028653893877383188, "phrase": "nominal_and_numerical_features"}, {"score": 0.0028167336373711494, "phrase": "feature_similarity_measure"}, {"score": 0.002675654549462983, "phrase": "probabilistic_dependence"}, {"score": 0.0026003455833291124, "phrase": "similarity_function"}, {"score": 0.002527150892127982, "phrase": "iterative_feature_selection_procedure"}, {"score": 0.0022933216359142736, "phrase": "probabilistic_dependence_similarity_function"}, {"score": 0.00224151735457093, "phrase": "presented_feature_selection_procedure"}, {"score": 0.0022034318550547866, "phrase": "computation_speed"}, {"score": 0.0021049977753042253, "phrase": "mixed_nominal_and_numerical_features"}], "paper_keywords": [""], "paper_abstract": "Data classification tasks often concern objects described by tens or even hundreds of features. Classification of such high-dimensional data is a difficult computational problem. Feature selection techniques help reduce the number of computations and improve classification accuracy. In Michalak and Kwasnicka (2006a, b) we proposed a feature selection strategy that selects features in an individual or pairwise manner based on the assessed level of dependence between features. In the case of numerical features, this level of dependence can be expressed numerically using linear correlation coefficients. In this paper, the feature selection problem is addressed in the case of a mixture of nominal and numerical features. The feature similarity measure used in this case is based on the probabilistic dependence between features. This similarity function is used in an iterative feature selection procedure, which we proposed for selecting features prior to classification. Experiments prove that using the probabilistic dependence similarity function along with the presented feature selection procedure can improve computation speed while preserving classification accuracy in the case of mixed nominal and numerical features.", "paper_title": "SELECTION OF NUMERICAL AND NOMINAL FEATURES BASED ON PROBABILISTIC DEPENDENCE BETWEEN FEATURES", "paper_id": "WOS:000295320600005"}