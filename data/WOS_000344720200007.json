{"auto_keywords": [{"score": 0.0245708430292824, "phrase": "jaffe"}, {"score": 0.00481495049065317, "phrase": "facial_expression_analysis"}, {"score": 0.004760394308239882, "phrase": "automatic_facial_expression_analysis_systems"}, {"score": 0.004574247691703376, "phrase": "computer_vision_techniques"}, {"score": 0.004522406417999492, "phrase": "human_computer_interaction"}, {"score": 0.00447115003744675, "phrase": "emotion_analysis"}, {"score": 0.004395347855760052, "phrase": "even_medical_care"}, {"score": 0.0043208252177835815, "phrase": "space_mapping"}, {"score": 0.004247560712327667, "phrase": "continuous_emotion"}, {"score": 0.004128192012388041, "phrase": "discrete_expression_categories"}, {"score": 0.004058180509184539, "phrase": "main_difficulty"}, {"score": 0.0039216854691927865, "phrase": "inherent_problem"}, {"score": 0.003877211065759289, "phrase": "facial_alignment"}, {"score": 0.0038114397487574838, "phrase": "person-specific_appearance"}, {"score": 0.003725470458263889, "phrase": "facial_representation_problem"}, {"score": 0.0033427754823819157, "phrase": "different_contexts"}, {"score": 0.0032302647179383915, "phrase": "variable_factors"}, {"score": 0.0031037644581516973, "phrase": "prototype-based_model"}, {"score": 0.002999274091205852, "phrase": "sift-flow_registration"}, {"score": 0.0029148831519944358, "phrase": "prototype_facial_expression_models"}, {"score": 0.0028328599839713866, "phrase": "reference_space"}, {"score": 0.002753138528582461, "phrase": "face_images"}, {"score": 0.0026302120935562568, "phrase": "registered_faces"}, {"score": 0.0025561788724870974, "phrase": "facial_expression_appearance"}, {"score": 0.002527150892127982, "phrase": "oriented_gradients"}, {"score": 0.0024560114164181765, "phrase": "registered_image"}, {"score": 0.002386869729952315, "phrase": "best_results"}, {"score": 0.0023329572140896237, "phrase": "person-independent_evaluation_strategy"}, {"score": 0.0021413855297794946, "phrase": "complex_setting"}, {"score": 0.0021049977753042253, "phrase": "gemep-fera_database"}], "paper_keywords": ["Facial expression recognition", " HOG", " prototype facial expression models", " registration", " SIFT-flow"], "paper_abstract": "Automatic facial expression analysis systems are aiming towards the application of computer vision techniques in human computer interaction, emotion analysis, and even medical care via a space mapping between the continuous emotion and a set of discrete expression categories. The main difficulty with these systems is the inherent problem of facial alignment due to person-specific appearance. Beside the facial representation problem, the same displayed facial expression may vary differently across humans; this can be true even for the same person in different contexts. To cope with these variable factors, we introduce the concept of prototype-based model as anchor modeling through a SIFT-flow registration. A set of prototype facial expression models is generated as a reference space of emotions on which face images are projected to generate a set of registered faces. To characterize the facial expression appearance, oriented gradients are processed on each registered image. We obtained the best results 87% with the person-independent evaluation strategy on JAFFE dataset (7-class expression recognition problem), and 83% on the complex setting of the GEMEP-FERA database (5-class problem).", "paper_title": "Prototype-Based Modeling for Facial Expression Analysis", "paper_id": "WOS:000344720200007"}