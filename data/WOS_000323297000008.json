{"auto_keywords": [{"score": 0.04791786215360241, "phrase": "dimension_reduction"}, {"score": 0.0413995704929081, "phrase": "sliced_inverse_regression"}, {"score": 0.03478816685509988, "phrase": "regression_function"}, {"score": 0.004822331688916576, "phrase": "entropy"}, {"score": 0.004349955692802962, "phrase": "available_data"}, {"score": 0.0042506731642425275, "phrase": "appropriate_dimension_reduction_method"}, {"score": 0.0041154571996523505, "phrase": "computational_time"}, {"score": 0.0040215052006051235, "phrase": "intrinsic_structure"}, {"score": 0.00398452530840582, "phrase": "complex_data"}, {"score": 0.0038935506745975835, "phrase": "well-known_dimension_reduction_method"}, {"score": 0.0037696519417841287, "phrase": "elliptical_distribution"}, {"score": 0.0037177622939493084, "phrase": "explanatory_variable"}, {"score": 0.0035335154419206634, "phrase": "simple_eigenvalue_problem"}, {"score": 0.0034210342328868017, "phrase": "strong_assumptions"}, {"score": 0.0033739270918502285, "phrase": "data_distribution"}, {"score": 0.002991823455621222, "phrase": "inverse_regression_method"}, {"score": 0.002714943351733626, "phrase": "existing_methods"}, {"score": 0.0026775328564629577, "phrase": "dimension_reduction_method"}, {"score": 0.0025802660239538353, "phrase": "conditional_entropy_minimization"}, {"score": 0.002407290605654707, "phrase": "low_dimensional_subspace"}, {"score": 0.002341384304635523, "phrase": "specific_distribution"}, {"score": 0.0022772782536385717, "phrase": "proposed_method"}, {"score": 0.002174303033371869, "phrase": "conventional_methods"}, {"score": 0.0021344259291927914, "phrase": "artificial_and_real-world_datasets"}], "paper_keywords": ["Sliced inverse regression", " Dimension reduction", " Entropy"], "paper_abstract": "The importance of dimension reduction has been increasing according to the growth of the size of available data in many fields. An appropriate dimension reduction method of raw data helps to reduce computational time and to expose the intrinsic structure of complex data. Sliced inverse regression is a well-known dimension reduction method for regression, which assumes an elliptical distribution for the explanatory variable, and ingeniously reduces the problem of dimension reduction to a simple eigenvalue problem. Sliced inverse regression is based on the strong assumptions on the data distribution and the form of regression function, and there are a number of methods to relax or remove these assumptions to extend the applicability of the inverse regression method. However, each method is known to have its drawbacks either theoretically or empirically. To alleviate drawbacks in the existing methods, a dimension reduction method for regression based on the notion of conditional entropy minimization is proposed. Using entropy as a measure of dispersion of data, a low dimensional subspace is estimated without assuming any specific distribution nor any regression function. The proposed method is shown to perform comparable or superior to the conventional methods through experiments using artificial and real-world datasets. (c) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Entropy-based sliced inverse regression", "paper_id": "WOS:000323297000008"}