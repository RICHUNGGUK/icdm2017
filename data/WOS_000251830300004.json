{"auto_keywords": [{"score": 0.044141882498322, "phrase": "australia"}, {"score": 0.013149279436651385, "phrase": "ict"}, {"score": 0.00469445816161498, "phrase": "journal_outputs"}, {"score": 0.004507882471601552, "phrase": "quantitative_indicators"}, {"score": 0.004285010985001827, "phrase": "university_sector"}, {"score": 0.0038520615699124123, "phrase": "social_science"}, {"score": 0.0037746771370555546, "phrase": "science_disciplines"}, {"score": 0.003445174898115646, "phrase": "initial_results"}, {"score": 0.0032415934630668484, "phrase": "prestige_tiers"}, {"score": 0.0030811340338342454, "phrase": "best_practice"}, {"score": 0.0029584874121320165, "phrase": "five-stage_process"}, {"score": 0.0028551716387011637, "phrase": "preliminary_ranking"}, {"score": 0.002769483748975581, "phrase": "extensive_consultation"}, {"score": 0.002727606388863445, "phrase": "performance_measures"}, {"score": 0.0026457367614424756, "phrase": "'live'_data"}, {"score": 0.0024516267507109753, "phrase": "ranking_classification"}, {"score": 0.002426842369975213, "phrase": "publication_outlets"}, {"score": 0.002226017732566121, "phrase": "australian_experience"}, {"score": 0.0021049977753042253, "phrase": "alternative_metrics"}], "paper_keywords": [""], "paper_abstract": "There are increasing moves to deploy quantitative indicators in the assessment of research, particularly in the university sector. In Australia, discussions surrounding their use have long acknowledged the unsuitability of many standard quantitative measures for most humanities, arts, social science, and applied science disciplines. To fill this void, several projects are running concurrently. This paper details the methodology and initial results for one of the projects that aims to rank conferences into prestige tiers, and which is fast gaining a reputation for best practice in such exercises. The study involves a five-stage process: identifying conferences; constructing a preliminary ranking of these; engaging in extensive consultation; testing performance measures based on the rankings on 'live' data; and assessing the measures. In the past, many similar attempts to develop a ranking classification for publication outlets have faltered due to the inability of researchers to agree on a hierarchy. However the Australian experience suggests that when researchers are faced with the imposition of alternative metrics that are far less palatable, consensus is more readily achieved.", "paper_title": "ICT assessment: Moving beyond journal outputs", "paper_id": "WOS:000251830300004"}