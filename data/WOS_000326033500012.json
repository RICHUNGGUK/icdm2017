{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "globally_linearly_convergent_algorithm"}, {"score": 0.0047402438279438765, "phrase": "long-existing_idea"}, {"score": 0.004691081414630302, "phrase": "nice_smooth_function"}, {"score": 0.004618287731422781, "phrase": "nondifferentiable_objective_function"}, {"score": 0.00455848598917637, "phrase": "sparse_optimization"}, {"score": 0.003778122295696292, "phrase": "nuclear_and_frobenius_norms"}, {"score": 0.0036712368673314177, "phrase": "sparse_vectors"}, {"score": 0.003652129369208894, "phrase": "low-rank_matrices"}, {"score": 0.0035580679938252116, "phrase": "exact_and_stable_recovery_guarantees"}, {"score": 0.003307343959522875, "phrase": "sensing_operator"}, {"score": 0.003197000180225272, "phrase": "section_property"}, {"score": 0.003172061898951532, "phrase": "\"ripless\"_property"}, {"score": 0.0025011537516782104, "phrase": "linearized_bregman_algorithm"}, {"score": 0.0023308144086659948, "phrase": "global_linear_convergence"}, {"score": 0.002300564468380128, "phrase": "nonzero_solution"}, {"score": 0.002258871472099473, "phrase": "explicit_rate"}, {"score": 0.0022295531401315787, "phrase": "convergence_property"}, {"score": 0.0022006144972018526, "phrase": "sparse_solution"}, {"score": 0.0021160280592526764, "phrase": "best_known_global_convergence_result"}, {"score": 0.0021049977753042253, "phrase": "first-order_sparse_optimization_algorithms"}], "paper_keywords": ["sparse optimization", " global linear convergence", " compressed sensing", " low-rank matrix", " matrix completion", " exact regularization"], "paper_abstract": "This paper studies the long-existing idea of adding a nice smooth function to \"smooth\" a nondifferentiable objective function in the context of sparse optimization, in particular, the minimization of parallel to x parallel to(1) + 1/2 alpha parallel to x parallel to(2)(2), where x is a vector, as well as the minimization of parallel to X parallel to(*) + 1/2 alpha parallel to X parallel to(2)(F), where X is a matrix and parallel to X parallel to(*) and parallel to X parallel to(F) are the nuclear and Frobenius norms of X, respectively. We show that they let sparse vectors and low-rank matrices be efficiently recovered. In particular, they enjoy exact and stable recovery guarantees similar to those known for the minimization of parallel to x parallel to(1) and parallel to X parallel to(*) under the conditions on the sensing operator such as its null-space property, restricted isometry property (RIP), spherical section property, or \"RIPless\" property. To recover a (nearly) sparse vector x(0), minimizing parallel to x parallel to(1) + 1/2 alpha parallel to x parallel to(2)(2) returns (nearly) the same solution as minimizing parallel to x parallel to(1) whenever alpha >= 10 parallel to x(0)parallel to(infinity). The same relation also holds between minimizing parallel to X parallel to(*) + 1/2 alpha parallel to X parallel to(2)(F) and minimizing parallel to X parallel to(*) for recovering a (nearly) low-rank matrix X-0 if alpha >= 10 parallel to X-0 parallel to 2. Furthermore, we show that the linearized Bregman algorithm, as well as its two fast variants, for minimizing parallel to x parallel to(1) + 1/2 alpha parallel to x parallel to(2)(2) subject to Ax = b enjoys global linear convergence as long as a nonzero solution exists, and we give an explicit rate of convergence. The convergence property does not require a sparse solution or any properties on A. To the best of our knowledge, this is the best known global convergence result for first-order sparse optimization algorithms.", "paper_title": "Augmented l(1) and Nuclear-Norm Models with a Globally Linearly Convergent Algorithm", "paper_id": "WOS:000326033500012"}