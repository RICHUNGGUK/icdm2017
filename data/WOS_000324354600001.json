{"auto_keywords": [{"score": 0.04039655347091129, "phrase": "lbt"}, {"score": 0.010612336161997261, "phrase": "cross-domain_document_classification"}, {"score": 0.00589010585930824, "phrase": "shared_topics"}, {"score": 0.004722898398399427, "phrase": "labeled_data"}, {"score": 0.004650519511749445, "phrase": "related_domain"}, {"score": 0.004526503177267299, "phrase": "effective_knowledge_transformation"}, {"score": 0.004474366340935036, "phrase": "target_domain"}, {"score": 0.0039694170761766226, "phrase": "novel_cross-domain_document_classification_approach"}, {"score": 0.00393886140366453, "phrase": "link-bridged_topic"}, {"score": 0.003702702337768558, "phrase": "auxiliary_link_network"}, {"score": 0.0036459007842756983, "phrase": "direct_or_indirect_co-citation_relationship"}, {"score": 0.003562322256892338, "phrase": "background_knowledge"}, {"score": 0.0035212516951180946, "phrase": "graph_kernel"}, {"score": 0.0034806529881619454, "phrase": "mined_co-citation_relationship"}, {"score": 0.003374655868348244, "phrase": "different_domains"}, {"score": 0.0032718761046588835, "phrase": "content_information"}, {"score": 0.0031968441980848436, "phrase": "unified_latent_topic_model"}, {"score": 0.00297037471252423, "phrase": "common_topics"}, {"score": 0.002879872305061576, "phrase": "content_information_and_link_structure"}, {"score": 0.00282470997972535, "phrase": "domains_data"}, {"score": 0.002792119634261638, "phrase": "latent_topic_spaces"}, {"score": 0.0027175263159330523, "phrase": "domain_commonality"}, {"score": 0.002644920521740564, "phrase": "associated_differential_probabilities"}, {"score": 0.0026143990748127253, "phrase": "learned_latent_topics"}, {"score": 0.002345944657572499, "phrase": "knowledge_transfer"}, {"score": 0.0022481685132781626, "phrase": "different_types"}, {"score": 0.002171202123281881, "phrase": "generalization_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Cross-domain", " Document classification", " Transfer learning", " Auxiliary link network"], "paper_abstract": "Transfer learning utilizes labeled data available from some related domain (source domain) for achieving effective knowledge transformation to the target domain. However, most state-of-the-art cross-domain classification methods treat documents as plain text and ignore the hyperlink (or citation) relationship existing among the documents. In this paper, we propose a novel cross-domain document classification approach called Link-Bridged Topic model (LBT). LBT consists of two key steps. Firstly, LBT utilizes an auxiliary link network to discover the direct or indirect co-citation relationship among documents by embedding the background knowledge into a graph kernel. The mined co-citation relationship is leveraged to bridge the gap across different domains. Secondly, LBT simultaneously combines the content information and link structures into a unified latent topic model. The model is based on an assumption that the documents of source and target domains share some common topics from the point of view of both content information and link structure. By mapping both domains data into the latent topic spaces, LBT encodes the knowledge about domain commonality and difference as the shared topics with associated differential probabilities. The learned latent topics must be consistent with the source and target data, as well as content and link statistics. Then the shared topics act as the bridge to facilitate knowledge transfer from the source to the target domains. Experiments on different types of datasets show that our algorithm significantly improves the generalization performance of cross-domain document classification. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "A link-bridged topic model for cross-domain document classification", "paper_id": "WOS:000324354600001"}