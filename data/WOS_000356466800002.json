{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "object_recognition"}, {"score": 0.004886387251534559, "phrase": "distributional_similarity"}, {"score": 0.004757888314764533, "phrase": "attribute_nomination"}, {"score": 0.004278807087784626, "phrase": "object_categories"}, {"score": 0.0041662189933067, "phrase": "useful_intermediate_representation"}, {"score": 0.00405658135587089, "phrase": "zero-shot_learning"}, {"score": 0.003980032599437366, "phrase": "object_appearance"}, {"score": 0.003919830848124645, "phrase": "manual_selection"}, {"score": 0.0038900709793274484, "phrase": "relevant_attributes"}, {"score": 0.003831224748279043, "phrase": "potential_candidates"}, {"score": 0.003659954070354848, "phrase": "mining_attributes"}, {"score": 0.0034963129868796033, "phrase": "nominating_attributes"}, {"score": 0.003378385493203297, "phrase": "candidate_attributes"}, {"score": 0.003314591878914152, "phrase": "first_technique"}, {"score": 0.0032894118890753805, "phrase": "attribute_nomination_estimates"}, {"score": 0.0031663432942179853, "phrase": "multiple_levels"}, {"score": 0.003094716540812336, "phrase": "second_technique"}, {"score": 0.0030595112110658675, "phrase": "linguistic_concept"}, {"score": 0.0029789104204803137, "phrase": "estimated_qualities"}, {"score": 0.0026567830878420347, "phrase": "useful_sources"}, {"score": 0.002499438189264537, "phrase": "mined_attributes"}, {"score": 0.0024615764782448214, "phrase": "zero-shot_learning_settings"}, {"score": 0.0023966900120342364, "phrase": "selected_attributes"}, {"score": 0.0023157656212149367, "phrase": "supervised_case"}, {"score": 0.0021952683716334478, "phrase": "zero-shot_scenario"}, {"score": 0.0021702728407945976, "phrase": "accurate_predictions"}, {"score": 0.0021537671245615286, "phrase": "previous_automated_techniques"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Object recognition", " Zero-shot learning", " Attribute mining", " Attribute-based classification"], "paper_abstract": "Attributes of objects such as \"square\", \"metallic\", and \"red\" allow a way for humans to explain or discriminate object categories. These attributes also provide a useful intermediate representation for object recognition, including support for zero-shot learning from textual descriptions of object appearance. However, manual selection of relevant attributes among thousands of potential candidates is labor intensive. Hence, there is increasing interest in mining attributes for object recognition. In this paper, we introduce two novel techniques for nominating attributes and a method for assessing the suitability of candidate attributes for object recognition. The first technique for attribute nomination estimates attribute qualities based on their ability to discriminate objects at multiple levels of the taxonomy. The second technique leverages the linguistic concept of distributional similarity to further refine the estimated qualities. Attribute nomination is followed by our attribute assessment procedure, which assesses the quality of the candidate attributes based on their performance in object recognition. Our evaluations demonstrate that both taxonomy and distributional similarity serve as useful sources of information for attribute nomination, and our methods can effectively exploit them. We use the mined attributes in supervised and zero-shot learning settings to show the utility of the selected attributes in object recognition. Our experimental results show that in the supervised case we can improve on a state of the art classifier while in the zero-shot scenario we make accurate predictions outperforming previous automated techniques. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Identifying visual attributes for object recognition from text and taxonomy", "paper_id": "WOS:000356466800002"}