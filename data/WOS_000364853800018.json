{"auto_keywords": [{"score": 0.04964635174824772, "phrase": "controlled_redundancy"}, {"score": 0.014433610484275352, "phrase": "correlated_features"}, {"score": 0.00481495049065317, "phrase": "unsupervised_feature_selection"}, {"score": 0.004518442937844318, "phrase": "redundant_or_correlated_features"}, {"score": 0.0042209023469249205, "phrase": "decision_making_cost"}, {"score": 0.004015182362035361, "phrase": "measurement_errors"}, {"score": 0.003942877273973905, "phrase": "selection_schemes"}, {"score": 0.003600351201298387, "phrase": "novel_unsupervised_feature_selection_scheme"}, {"score": 0.0034560481756572632, "phrase": "irrelevant_features"}, {"score": 0.0032429499567088113, "phrase": "selected_features"}, {"score": 0.0031129269142224194, "phrase": "objective_function"}, {"score": 0.003015401937358271, "phrase": "specified_number"}, {"score": 0.002934237411711622, "phrase": "controlled_level"}, {"score": 0.0028037753646260937, "phrase": "original_data"}, {"score": 0.0027159094186967247, "phrase": "reduced_dimension"}, {"score": 0.0026307897899760383, "phrase": "sammon's_error"}, {"score": 0.002401972657236928, "phrase": "relevant_features"}, {"score": 0.0022231606165229235, "phrase": "comparative_study"}, {"score": 0.002203016940021265, "phrase": "five_unsupervised_feature_selection_methods"}, {"score": 0.002133937537670314, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "useful_features"}], "paper_keywords": ["Unsupervised feature selection", " dimensionality reduction", " redundancy control", " Sammon's error", " gradient descent technique"], "paper_abstract": "Features selected by a supervised/unsupervised technique often include redundant or correlated features. While use of correlated features may result in an increase in the design and decision making cost, removing redundancy completely can make the system vulnerable to measurement errors. Most feature selection schemes do not account for redundancy at all, while a few supervised methods try to discard correlated features. We propose a novel unsupervised feature selection scheme (UFeSCoR), which not only discards irrelevant features, but also selects features with controlled redundancy. Here, the number of selected features can also be directed. Our algorithm optimizes an objective function, which tries to select a specified number of features, with a controlled level of redundancy, such that the topology of the original data set can be maintained in the reduced dimension. Here, we have used Sammon's error as a measure of preservation of topology. We demonstrate the effectiveness of the algorithm in terms of choosing relevant features, controlling redundancy, and selecting a given number of features using several data sets. We make a comparative study with five unsupervised feature selection methods. Our results reveal that the proposed method can select useful features with controlled redundancy.", "paper_title": "Unsupervised Feature Selection with Controlled Redundancy (UFeSCoR)", "paper_id": "WOS:000364853800018"}