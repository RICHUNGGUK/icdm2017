{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "deep_neural_nets"}, {"score": 0.049124089120586543, "phrase": "quantitative_structure-activity_relationships"}, {"score": 0.004301046274139699, "phrase": "large_problems"}, {"score": 0.0039677715255034595, "phrase": "support_vector_machine"}, {"score": 0.003936139221574909, "phrase": "svm"}, {"score": 0.003616189733352126, "phrase": "neural_networks"}, {"score": 0.003501322813520807, "phrase": "new_methods"}, {"score": 0.0033628392198449134, "phrase": "computer_hardware"}, {"score": 0.0031271827534050493, "phrase": "great_successes"}, {"score": 0.0030523478413779686, "phrase": "computer_vision"}, {"score": 0.0030278017562213265, "phrase": "natural_language_processing"}, {"score": 0.002907992115184249, "phrase": "better_prospective_predictions"}, {"score": 0.002826949449085849, "phrase": "large_diverse_qsar_data_sets"}, {"score": 0.0027704444816096484, "phrase": "mercks_drug_discovery_effort"}, {"score": 0.0027150658583211746, "phrase": "adjustable_parameters"}, {"score": 0.002504374718975704, "phrase": "individual_data_sets"}, {"score": 0.0024642355970277497, "phrase": "single_set"}, {"score": 0.0024444073418288703, "phrase": "recommended_parameters"}, {"score": 0.0024149629835339926, "phrase": "better_performance"}, {"score": 0.002347628167175859, "phrase": "data_sets"}, {"score": 0.0022365262201954643, "phrase": "additional_data_sets"}, {"score": 0.002165388556451298, "phrase": "training_dnns"}, {"score": 0.0021049977753042253, "phrase": "graphical_processing_units"}], "paper_keywords": [""], "paper_abstract": "Neural networks were widely used for quantitative structure-activity relationships (QSAR) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine (SVM) and random forest (RF), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets (DNNs), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that DNNs can routinely make better prospective predictions than RF on a set of large diverse QSAR data sets that are taken from Mercks drug discovery effort. The number of adjustable parameters needed for DNNs is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than RF for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training DNNs is still computationally intensive, using graphical processing units (GPUs) can make this issue manageable.", "paper_title": "Deep Neural Nets as a Method for Quantitative Structure-Activity Relationships", "paper_id": "WOS:000349943100007"}