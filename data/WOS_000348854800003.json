{"auto_keywords": [{"score": 0.0403190888165153, "phrase": "generalization_capability"}, {"score": 0.01922285491085674, "phrase": "elm"}, {"score": 0.008118665358119139, "phrase": "fnn"}, {"score": 0.006194839948175813, "phrase": "elm."}, {"score": 0.00481495049065317, "phrase": "extreme_learning_machine_feasible"}, {"score": 0.004647013794908825, "phrase": "extreme_learning_machine"}, {"score": 0.004513957235071457, "phrase": "two-stage_feed-forward_neural_network"}, {"score": 0.00447244551063032, "phrase": "gaussian_kernel"}, {"score": 0.0043144799365586375, "phrase": "hidden_neurons"}, {"score": 0.004272890231368899, "phrase": "first_stage"}, {"score": 0.004190904630161064, "phrase": "output_neurons"}, {"score": 0.00415050117148714, "phrase": "second_stage"}, {"score": 0.00409723267214392, "phrase": "elm_training"}, {"score": 0.004044645056447888, "phrase": "linear_learning_problem"}, {"score": 0.003967021436047945, "phrase": "computational_burden"}, {"score": 0.003941478054422721, "phrase": "numerous_applications"}, {"score": 0.003531474887552889, "phrase": "theoretical_feasibility"}, {"score": 0.0033861930947608207, "phrase": "previous_part"}, {"score": 0.003278504512223807, "phrase": "appropriately_selected_activation_functions"}, {"score": 0.003033776234384367, "phrase": "different_direction"}, {"score": 0.00281635677605879, "phrase": "additional_uncertainty_problem"}, {"score": 0.0026314443645799913, "phrase": "activation_functions"}, {"score": 0.002597620940221604, "phrase": "corresponding_elm"}, {"score": 0.0022384684697329097, "phrase": "well-developed_coefficient_regularization_technique"}, {"score": 0.002181270821530302, "phrase": "obtained_results"}, {"score": 0.0021601997088758957, "phrase": "essential_characteristic"}, {"score": 0.0021049977753042253, "phrase": "theoretical_guidance"}], "paper_keywords": ["Extreme learning machine (ELM)", " Gaussian kernel", " generalization capability", " neural networks"], "paper_abstract": "An extreme learning machine (ELM) can be regarded as a two-stage feed-forward neural network (FNN) learning system that randomly assigns the connections with and within hidden neurons in the first stage and tunes the connections with output neurons in the second stage. Therefore, ELM training is essentially a linear learning problem, which significantly reduces the computational burden. Numerous applications show that such a computation burden reduction does not degrade the generalization capability. It has, however, been open that whether this is true in theory. The aim of this paper is to study the theoretical feasibility of ELM by analyzing the pros and cons of ELM. In the previous part of this topic, we pointed out that via appropriately selected activation functions, ELM does not degrade the generalization capability in the sense of expectation. In this paper, we launch the study in a different direction and show that the randomness of ELM also leads to certain negative consequences. On one hand, we find that the randomness causes an additional uncertainty problem of ELM, both in approximation and learning. On the other hand, we theoretically justify that there also exist activation functions such that the corresponding ELM degrades the generalization capability. In particular, we prove that the generalization capability of ELM with Gaussian kernel is essentially worse than that of FNN with Gaussian kernel. To facilitate the use of ELM, we also provide a remedy to such a degradation. We find that the well-developed coefficient regularization technique can essentially improve the generalization capability. The obtained results reveal the essential characteristic of ELM in a certain sense and give theoretical guidance concerning how to use ELM.", "paper_title": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II)", "paper_id": "WOS:000348854800003"}