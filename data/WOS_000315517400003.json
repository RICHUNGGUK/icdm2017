{"auto_keywords": [{"score": 0.04823115624486472, "phrase": "fcrn"}, {"score": 0.008067794247861637, "phrase": "energy_function"}, {"score": 0.00481495049065317, "phrase": "relaxation_neural_network"}, {"score": 0.004674952036988355, "phrase": "fully_complex-valued_relaxation_network"}, {"score": 0.004428728205625129, "phrase": "single_hidden_layer_network"}, {"score": 0.004363846362238565, "phrase": "gaussian-like_sech_activation_function"}, {"score": 0.004216112320418982, "phrase": "exponential_activation_function"}, {"score": 0.004154332348800334, "phrase": "output_layer"}, {"score": 0.0040334640350288, "phrase": "hidden_neurons"}, {"score": 0.003974349782102708, "phrase": "input_weights"}, {"score": 0.003858697684681798, "phrase": "output_weights"}, {"score": 0.0037463983616303786, "phrase": "nonlinear_logarithmic_function"}, {"score": 0.003496868378204654, "phrase": "projection-based_learning_algorithm"}, {"score": 0.003445591238183763, "phrase": "optimal_output_weights"}, {"score": 0.0032639039638211347, "phrase": "nonlinear_programming_problem"}, {"score": 0.0031377955308198634, "phrase": "simultaneous_linear_algebraic_equations"}, {"score": 0.003091767402370757, "phrase": "resultant_fcrn"}, {"score": 0.0030464123943266673, "phrase": "desired_output"}, {"score": 0.0029722900280316216, "phrase": "lower_computational_effort"}, {"score": 0.002928682556953258, "phrase": "classification_ability"}, {"score": 0.0028016507334576216, "phrase": "real-valued_benchmark_classification_problems"}, {"score": 0.00266693888510356, "phrase": "circular_transformation"}, {"score": 0.0025892330566358503, "phrase": "real-valued_input_features"}, {"score": 0.0022113879114507577, "phrase": "performance_results"}, {"score": 0.0021049977753042253, "phrase": "fcrn."}], "paper_keywords": ["Adaptive beamforming", " classification", " complex-valued neural network", " energy function", " quadrature amplitude modulation (QAM)"], "paper_abstract": "This paper presents a fully complex-valued relaxation network (FCRN) with its projection-based learning algorithm. The FCRN is a single hidden layer network with a Gaussian-like sech activation function in the hidden layer and an exponential activation function in the output layer. For a given number of hidden neurons, the input weights are assigned randomly and the output weights are estimated by minimizing a nonlinear logarithmic function (called as an energy function) which explicitly contains both the magnitude and phase errors. A projection-based learning algorithm determines the optimal output weights corresponding to the minima of the energy function by converting the nonlinear programming problem into that of solving a set of simultaneous linear algebraic equations. The resultant FCRN approximates the desired output more accurately with a lower computational effort. The classification ability of FCRN is evaluated using a set of real-valued benchmark classification problems from the University of California, Irvine machine learning repository. Here, a circular transformation is used to transform the real-valued input features to the complex domain. Next, the FCRN is used to solve three practical problems: a quadrature amplitude modulation channel equalization, an adaptive beamforming, and a mammogram classification. Performance results from this paper clearly indicate the superior classification/approximation performance of the FCRN.", "paper_title": "Projection-Based Fast Learning Fully Complex-Valued Relaxation Neural Network", "paper_id": "WOS:000315517400003"}