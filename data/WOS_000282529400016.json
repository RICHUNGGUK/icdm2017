{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "existing_images"}, {"score": 0.0038404642251010797, "phrase": "first_estimating_depth"}, {"score": 0.0034162643819889054, "phrase": "brush-based_interface"}, {"score": 0.002422890839801611, "phrase": "perceptually_plausible_results"}, {"score": 0.0022939396079753463, "phrase": "imperfect_depth"}, {"score": 0.0022583704173318123, "phrase": "lighting_information"}, {"score": 0.002223351523626989, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Picture/image generation", " Scene analysis"], "paper_abstract": "We demonstrate the feasibility of rendering fur directly into existing images, without the need to either painstakingly paint over all pixels, or to supply 3D geometry and lighting. We add fur to objects depicted in images by first estimating depth and lighting information and then re-rendering the resulting 2.5D geometry with fur. A brush-based interface is provided, allowing the user to control the positioning and appearance of fur, while all the interaction takes place in a 2D pipeline. The novelty of this approach lies in the fact that a complex, high-level image edit such as the addition of fur can yield perceptually plausible results, even in the presence of imperfect depth or lighting information. Crown Copyright (C) 2010 Published by Elsevier Ltd. All rights reserved.", "paper_title": "Rendering fur directly into images", "paper_id": "WOS:000282529400016"}