{"auto_keywords": [{"score": 0.045721899155334936, "phrase": "youtube_videos"}, {"score": 0.00481495049065317, "phrase": "spatiotemporal_video_patterns"}, {"score": 0.00476697427418509, "phrase": "robust_action_retrieval"}, {"score": 0.004556901872357751, "phrase": "spatiotemporal_co-location_video_pattern_mining_approach"}, {"score": 0.004227064760262924, "phrase": "attention_shift_scheme"}, {"score": 0.004101886428528311, "phrase": "focused_human_actions"}, {"score": 0.00392100783796804, "phrase": "visual_saliency"}, {"score": 0.00354700018079049, "phrase": "segmented_spatiotemporal_human_action_regions"}, {"score": 0.003339886795391701, "phrase": "interest_points"}, {"score": 0.0032900200522703923, "phrase": "reference_youtube_videos"}, {"score": 0.003129096013910626, "phrase": "individual_interest_point"}, {"score": 0.0030823667846259836, "phrase": "word_identity"}, {"score": 0.0030363332738004454, "phrase": "aprior_based_frequent_itemset_mining_scheme"}, {"score": 0.0029463123778313196, "phrase": "spatiotemporal_co-located_words"}, {"score": 0.002902304851882577, "phrase": "co-location_video_patterns"}, {"score": 0.0028021527740710508, "phrase": "visual_words"}, {"score": 0.002719055901632759, "phrase": "boosting_based_feature_selection"}, {"score": 0.0025730425700096365, "phrase": "ranking_distortion"}, {"score": 0.002534596206835737, "phrase": "conjunctive_queries"}, {"score": 0.002496722870659199, "phrase": "boosting_objective"}, {"score": 0.0024348510123716424, "phrase": "quantitative_evaluations"}, {"score": 0.002398464540102246, "phrase": "kth_human_motion_benchmark"}, {"score": 0.002281086182920238, "phrase": "youtube"}, {"score": 0.0021802872319258977, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Video search", " Spatiotemporal descriptor", " Visual vocabulary", " Visual pattern mining", " Social media", " Scalable multimedia representation"], "paper_abstract": "In this paper, we present a spatiotemporal co-location video pattern mining approach with application to robust action retrieval in YouTube videos. First, we introduce an attention shift scheme to detect and partition the focused human actions from YouTube videos, which is based upon the visual saliency [13] modeling together with both the face [35] and body [32] detectors. From the segmented spatiotemporal human action regions, we extract 3D-SIFT [17] detector. Then, we quantize all detected interest points from the reference YouTube videos into a vocabulary, based on which assign each individual interest point with a word identity. An APrior based frequent itemset mining scheme is then deployed over the spatiotemporal co-located words to discover co-location video patterns. Finally, we fuse both visual words and patterns and leverage a boosting based feature selection to output the final action descriptors, which incorporates the ranking distortion of the conjunctive queries into the boosting objective. We carried out quantitative evaluations over both KTH human motion benchmark [26], as well as over 60-hour YouTube videos, with comparisons to the state-of-the-arts. Crown Copyright (C) 2012 Published by Elsevier B.V. All rights reserved.", "paper_title": "Mining spatiotemporal video patterns towards robust action retrieval", "paper_id": "WOS:000317091700009"}