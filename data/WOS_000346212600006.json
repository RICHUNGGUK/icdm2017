{"auto_keywords": [{"score": 0.03600109886723128, "phrase": "reduce_tasks"}, {"score": 0.00481495049065317, "phrase": "reduce_start_time"}, {"score": 0.004581660953729745, "phrase": "large-scale_data-intensive_cloud_computing_platforms"}, {"score": 0.004339979174946844, "phrase": "key_problems"}, {"score": 0.004262273304740847, "phrase": "mapreduce_performance"}, {"score": 0.004204904449336037, "phrase": "existing_implementations"}, {"score": 0.003947186611084043, "phrase": "map_tasks"}, {"score": 0.0038070472635126276, "phrase": "mapreduce_scheduling_algorithm"}, {"score": 0.003638823482920949, "phrase": "current_mapreduce_scheduling_mechanism"}, {"score": 0.0035095935622207956, "phrase": "system_slot_resources_waste"}, {"score": 0.003309278541665552, "phrase": "optimal_reduce_scheduling_policy"}, {"score": 0.0030783143919343972, "phrase": "hadoop_platform"}, {"score": 0.0029959082125682918, "phrase": "start_time_point"}, {"score": 0.002955533639862811, "phrase": "reduce_task"}, {"score": 0.002889444701599236, "phrase": "job_context"}, {"score": 0.0028376361457661415, "phrase": "task_completion_time"}, {"score": 0.0027741761880779535, "phrase": "map_output"}, {"score": 0.002724428781940696, "phrase": "job_completion_time"}, {"score": 0.002687702958515561, "phrase": "completion_time"}, {"score": 0.0025804616931593897, "phrase": "experimental_results"}, {"score": 0.002466303439899648, "phrase": "reduce_completion_time"}, {"score": 0.0023465400668244386, "phrase": "average_response_time"}, {"score": 0.0022631003204259224, "phrase": "sars_algorithm"}, {"score": 0.0022124605840026313, "phrase": "traditional_job_scheduling_algorithms_fifo"}, {"score": 0.0021925226977381244, "phrase": "fairscheduler"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Big data", " Hadoop", " MapReduce", " Reduce", " Self-adaptive", " Task scheduling"], "paper_abstract": "MapReduce is by far one of the most successful realizations of large-scale data-intensive cloud computing platforms. When to start the reduce tasks is one of the key problems to advance the MapReduce performance. The existing implementations may result in a block of reduce tasks. When the output of map tasks become large, the performance of a MapReduce scheduling algorithm will be influenced seriously. Through analysis for the current MapReduce scheduling mechanism, this paper illustrates the reasons of system slot resources waste, which results in the reduce tasks waiting around, and proposes an optimal reduce scheduling policy called SARS (Self Adaptive Reduce Scheduling) for reduce tasks' start times in the Hadoop platform. It can decide the start time point of each reduce task dynamically according to each job context, including the task completion time and the size of map output. Through estimating job completion time, reduce completion time, and system average response time, the experimental results illustrate that, when comparing with other algorithms, the reduce completion time is decreased sharply. It is also proved that the average response time is decreased by 11% to 29%, when the SARS algorithm is applied to the traditional job scheduling algorithms FIFO, FairScheduler, and CapacityScheduler. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A self-adaptive scheduling algorithm for reduce start time", "paper_id": "WOS:000346212600006"}