{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "fpt_problems"}, {"score": 0.004720043307726269, "phrase": "algorithmic_methods"}, {"score": 0.004446349752828045, "phrase": "fixed-parameter_tractability"}, {"score": 0.004272724011826495, "phrase": "powerful_computational_platforms"}, {"score": 0.0041469500929423595, "phrase": "systematic_attacks"}, {"score": 0.004065156038847781, "phrase": "combinatorial_problems"}, {"score": 0.003643185395780667, "phrase": "np-hard_vertex_cover_problem"}, {"score": 0.0031061129850110994, "phrase": "parallel_algorithms"}, {"score": 0.002701361707441567, "phrase": "balanced_decomposition"}, {"score": 0.0026217213613095322, "phrase": "search_space"}, {"score": 0.0023727863021828547, "phrase": "target_problems"}, {"score": 0.0021049977753042253, "phrase": "high_throughput_computational_biology"}], "paper_keywords": ["algorithm design", " parallel computing", " optimization", " load balancing", " applications"], "paper_abstract": "Algorithmic methods based on the theory of fixed-parameter tractability are combined with powerful computational platforms to launch systematic attacks on combinatorial problems of significance. As a case study, optimal solutions to very large instances of the NP-hard vertex cover problem are computed. To accomplish this, an efficient sequential algorithm and various forms of parallel algorithms are devised, implemented, and compared. The importance of maintaining a balanced decomposition of the search space is shown to be critical to achieving scalability. Target problems need only be amenable to reduction and decomposition. Applications in high throughput computational biology are also discussed.", "paper_title": "Scalable parallel algorithms for FPT problems", "paper_id": "WOS:000238153600002"}