{"auto_keywords": [{"score": 0.04203576937954651, "phrase": "introduced_estimators"}, {"score": 0.00481495049065317, "phrase": "new_entropy_estimators"}, {"score": 0.004225950334493496, "phrase": "absolutely_continuous_random_variables"}, {"score": 0.003907586297783078, "phrase": "existing_entropy_estimators"}, {"score": 0.003613119346373748, "phrase": "dimitriev"}, {"score": 0.0035507457285983268, "phrase": "tarasenko"}, {"score": 0.0034291991359158827, "phrase": "estimation_functions"}, {"score": 0.003340768341189764, "phrase": "probability_density"}, {"score": 0.0027104531025934865, "phrase": "goodness-of-fit_tests"}, {"score": 0.0025723630851105304, "phrase": "introduced_entropy_estimators"}, {"score": 0.0021049977753042253, "phrase": "testing_normality"}], "paper_keywords": ["information theory", " entropy estimator", " testing normality"], "paper_abstract": "In this paper, we first introduce two new estimators for estimating the entropy of absolutely continuous random variables. We then compare the introduced estimators with the existing entropy estimators, including the first of such estimators proposed by Dimitriev and Tarasenko [On the estimation functions of the probability density and its derivatives, Theory Probab. Appl. 18 (1973), pp. 628-633]. We next propose goodness-of-fit tests for normality based on the introduced entropy estimators and compare their powers with the powers of other entropy-based tests for normality. Our simulation results show that the introduced estimators perform well in estimating entropy and testing normality.", "paper_title": "Testing normality based on new entropy estimators", "paper_id": "WOS:000310309700010"}