{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "random_projection"}, {"score": 0.0047267100475487595, "phrase": "simple_technique"}, {"score": 0.004471530397729526, "phrase": "algorithm_design"}, {"score": 0.00430907756010007, "phrase": "machine_learning"}, {"score": 0.003738973626996355, "phrase": "large_margin"}, {"score": 0.0030309961956273028, "phrase": "simple_learning_algorithms"}, {"score": 0.002678734566245688, "phrase": "black-box_function"}, {"score": 0.0025027144548626975, "phrase": "explicit_small_feature_space"}, {"score": 0.0022670785913397637, "phrase": "large_part"}, {"score": 0.0021311850321045767, "phrase": "nina_balcan"}, {"score": 0.0021049977753042253, "phrase": "santosh_vempala"}], "paper_keywords": [""], "paper_abstract": "Random projection is a simple technique that has had a number of applications in algorithm design. In the context of machine learning, it can provide insight into questions such as \"why is a learning problem easier if data is separable by a large margin?\" and \"in what sense is choosing a kernel much like choosing a set of features?\" This talk is intended to provide an introduction to random projection and to survey some simple learning algorithms and other applications to learning based on it. I will also discuss how, given a kernel as a black-box function, we can use various forms of random projection to extract an explicit small feature space that captures much of what the kernel is doing. This talk is based in large part on work in [BB05, BBV04] joint with Nina Balcan and Santosh Vempala.", "paper_title": "Random projection, margins, kernels, and feature-selection", "paper_id": "WOS:000238094700003"}