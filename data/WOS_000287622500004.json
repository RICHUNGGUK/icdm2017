{"auto_keywords": [{"score": 0.03981238870202312, "phrase": "probabilistic_kernels"}, {"score": 0.035826256567006345, "phrase": "special_case"}, {"score": 0.00481495049065317, "phrase": "finite_liouville_mixture_models"}, {"score": 0.004740476936977635, "phrase": "hybrid_generative_discriminative_approaches"}, {"score": 0.004618893461411858, "phrase": "efficient_knowledge_representation"}, {"score": 0.004571133708117502, "phrase": "data_classification_engine"}, {"score": 0.004206338218873218, "phrase": "especially_proportional_vectors"}, {"score": 0.0039519004610213595, "phrase": "true_structure"}, {"score": 0.0037516215763875225, "phrase": "generative_mixture_models"}, {"score": 0.003693533648584145, "phrase": "liouville_family"}, {"score": 0.0035614563596982306, "phrase": "beta-liouville_distribution"}, {"score": 0.0034519997658752598, "phrase": "well-known_dirichlet"}, {"score": 0.003243688831359012, "phrase": "dirichlet"}, {"score": 0.0030151432267065815, "phrase": "principled_purely_bayesian_approach"}, {"score": 0.0028921584725218642, "phrase": "support_vector_machine"}, {"score": 0.0028623812600980267, "phrase": "svm"}, {"score": 0.0027886573969044042, "phrase": "information_divergence"}, {"score": 0.0026471739851621143, "phrase": "closed-form_expressions"}, {"score": 0.0025792274347474576, "phrase": "renyi"}, {"score": 0.0024103044278902916, "phrase": "extensive_simulations"}, {"score": 0.002324007171619245, "phrase": "synthetic_data"}, {"score": 0.002299921909935613, "phrase": "visual_scenes"}, {"score": 0.0022760856906838814, "phrase": "texture_images"}, {"score": 0.002171836484200008, "phrase": "proposed_approaches"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Liouville family of distributions", " Generative models", " Discriminative models", " Mixture models", " SVM", " Bayesian inference", " Exponential family", " Conjugate prior", " Gibbs sampling", " Bayes factor", " Image classification", " Texture modeling"], "paper_abstract": "Recently hybrid generative discriminative approaches have emerged as an efficient knowledge representation and data classification engine. However, little attention has been devoted to the modeling and classification of non-Gaussian and especially proportional vectors. Our main goal, in this paper, is to discover the true structure of this kind of data by building probabilistic kernels from generative mixture models based on Liouville family, from which we develop the Beta-Liouville distribution, and which includes the well-known Dirichlet as a special case. The Beta-Liouville has a more general covariance structure than the Dirichlet which makes it more practical and useful. Our learning technique is based on a principled purely Bayesian approach which resulted models are used to generate support vector machine (SVM) probabilistic kernels based on information divergence. In particular, we show the existence of closed-form expressions of the Kullback-Leibler and Renyi divergences between two Beta-Liouville distributions and then between two Dirichlet distributions as a special case. Through extensive simulations and a number of experiments involving synthetic data, visual scenes and texture images classification, we demonstrate the effectiveness of the proposed approaches. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Bayesian hybrid generative discriminative learning based on finite Liouville mixture models", "paper_id": "WOS:000287622500004"}