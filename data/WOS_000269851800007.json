{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "nonlinear_dimension_reduction"}, {"score": 0.03775114821059378, "phrase": "kernel_sir"}, {"score": 0.004681971826924847, "phrase": "inverse_regression"}, {"score": 0.004616859702338596, "phrase": "sliced_inverse_regression"}, {"score": 0.004426882270402403, "phrase": "renowned_dimension_reduction_method"}, {"score": 0.004304574865291508, "phrase": "effective_low-dimensional_linear_subspace"}, {"score": 0.0040133278382101885, "phrase": "nonlinear_setting"}, {"score": 0.003929843413153981, "phrase": "\"kernel_trick"}, {"score": 0.003821215658570578, "phrase": "main_purpose"}, {"score": 0.0035129562812850784, "phrase": "reproducing_kernel_hilbert_space"}, {"score": 0.003275093899046286, "phrase": "second_focus"}, {"score": 0.0031845067058084583, "phrase": "implementation_algorithm"}, {"score": 0.003096417317829492, "phrase": "fast_computation"}, {"score": 0.0030532878377173885, "phrase": "numerical_stability"}, {"score": 0.002948066398723126, "phrase": "low-rank_approximation"}, {"score": 0.0028664985822266344, "phrase": "huge_and_dense_full_kernel_covariance_matrix"}, {"score": 0.0028068030356445894, "phrase": "reduced_singular_value_decomposition_technique"}, {"score": 0.0027483472371835865, "phrase": "kernel_sir_directions"}, {"score": 0.00263505298504827, "phrase": "sir's_ability"}, {"score": 0.002456486927713543, "phrase": "multiresponse_regression"}, {"score": 0.002422249273477746, "phrase": "numerical_experiments"}, {"score": 0.0023387234705715154, "phrase": "effective_kernel_tool"}, {"score": 0.0021347607855562102, "phrase": "powerful_toolkit"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_data_analysis"}], "paper_keywords": ["Dimension reduction", " eigenvalue decomposition", " kernel", " reproducing kernel Hilbert space", " singular value decomposition", " sliced inverse regression", " support vector machines"], "paper_abstract": "Sliced inverse regression (SIR) is a renowned dimension reduction method for finding an effective low-dimensional linear subspace. Like many other linear methods, SIR can be extended to nonlinear setting via the \"kernel trick.\" The main purpose of this paper is two-fold. We build kernel SIR in a reproducing kernel Hilbert space rigorously for a more intuitive model explanation and theoretical development. The second focus is on the implementation algorithm of kernel SIR for fast computation and numerical stability. We adopt a low-rank approximation to approximate the huge and dense full kernel covariance matrix and a reduced singular value decomposition technique for extracting kernel SIR directions. We also explore kernel SIR's ability to combine with other linear learning algorithms for classification and regression including multiresponse regression. Numerical experiments show that kernel SIR is an effective kernel tool for nonlinear dimension reduction and it can easily combine with other linear algorithms to form a powerful toolkit for nonlinear data analysis.", "paper_title": "Nonlinear Dimension Reduction with Kernel Sliced Inverse Regression", "paper_id": "WOS:000269851800007"}