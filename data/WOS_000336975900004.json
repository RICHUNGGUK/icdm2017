{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "pca"}, {"score": 0.008635029170433078, "phrase": "large_data_sets"}, {"score": 0.006261249954165922, "phrase": "dbms"}, {"score": 0.005294065350681945, "phrase": "linear_speedup"}, {"score": 0.0047283116134926645, "phrase": "parallel_data_summarization"}, {"score": 0.004685575787741084, "phrase": "parallel_processing"}, {"score": 0.00460125407969429, "phrase": "large-scale_analytics"}, {"score": 0.004559661374976129, "phrase": "principal_component_analysis"}, {"score": 0.004437115547184535, "phrase": "well_known_model"}, {"score": 0.004397000006837371, "phrase": "dimensionality_reduction"}, {"score": 0.0043950651547602125, "phrase": "multicore_cpus"}, {"score": 0.004357245559948974, "phrase": "statistical_analysis"}, {"score": 0.004259418153299952, "phrase": "demanding_number"}, {"score": 0.0038895008852940323, "phrase": "previous_sequential_method"}, {"score": 0.0038368442918503072, "phrase": "highly_parallel_algorithm"}, {"score": 0.003683097597914604, "phrase": "large_data_set"}, {"score": 0.0036332251283046997, "phrase": "summarization_matrices"}, {"score": 0.0033175095965171674, "phrase": "parallel_data"}, {"score": 0.003257727050827128, "phrase": "user-defined_aggregations"}, {"score": 0.0031990227490602276, "phrase": "mkl"}, {"score": 0.0031271133933467575, "phrase": "lapack_library"}, {"score": 0.0030847461428168614, "phrase": "singular_value_decomposition"}, {"score": 0.002855251306715901, "phrase": "data_size"}, {"score": 0.0027407679668674215, "phrase": "ram"}, {"score": 0.0025367855339394284, "phrase": "svd"}, {"score": 0.0025137856138000014, "phrase": "cubic_time_complexity"}, {"score": 0.002326669201839705, "phrase": "r_statistical_package"}, {"score": 0.0022537212268979507, "phrase": "sql_queries"}, {"score": 0.0021436720821474973, "phrase": "multiple_nodes"}, {"score": 0.0021049977753042253, "phrase": "linear_scalability"}], "paper_keywords": ["PCA", " SVD", " Matrices", " DBMS", " Speedup", " LAPACK"], "paper_abstract": "Parallel processing is essential for large-scale analytics. Principal Component Analysis (PCA) is a well known model for dimensionality reduction in statistical analysis, which requires a demanding number of I/O and CPU operations. In this paper, we study how to compute PCA in parallel. We extend a previous sequential method to a highly parallel algorithm that can compute PCA in one pass on a large data set based on summarization matrices. We also study how to integrate our algorithm with a DBMS; our solution is based on a combination of parallel data set summarization via user-defined aggregations and calling the MKL parallel variant of the LAPACK library to solve Singular Value Decomposition (SVD) in RAM. Our algorithm is theoretically shown to achieve linear speedup, linear scalability on data size, quadratic time on dimensionality (but in RAM), spending most of the time on data set summarization, despite the fact that SVD has cubic time complexity on dimensionality. Experiments with large data sets on multicore CPUs show that our solution is much faster than the R statistical package as well as solving PCA with SQL queries. Benchmarking on multicore CPUs and a parallel DBMS running on multiple nodes confirms linear speedup and linear scalability.", "paper_title": "PCA for large data sets with parallel data summarization", "paper_id": "WOS:000336975900004"}