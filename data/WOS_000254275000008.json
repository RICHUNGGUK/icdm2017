{"auto_keywords": [{"score": 0.032274328647462026, "phrase": "smooth_images"}, {"score": 0.00481495049065317, "phrase": "multiscale_recurrent_patterns"}, {"score": 0.004783483480147339, "phrase": "adaptive_probability_model"}, {"score": 0.00462919213721188, "phrase": "multidimensional_multiscale_parser"}, {"score": 0.004598933498016774, "phrase": "mmip"}, {"score": 0.004195418010131832, "phrase": "ecg_signals"}, {"score": 0.004154332348800334, "phrase": "mnip"}, {"score": 0.0041001740903338834, "phrase": "approximate_multiscale_pattern_matching"}, {"score": 0.004020252310319777, "phrase": "input_signal"}, {"score": 0.003703611021999795, "phrase": "previously_encoded_blocks"}, {"score": 0.0036433462598549956, "phrase": "mmp"}, {"score": 0.003584025547693836, "phrase": "input_data"}, {"score": 0.003400640983719424, "phrase": "universal_flavor"}, {"score": 0.0033452741392850523, "phrase": "flexible_structure"}, {"score": 0.003269266730193333, "phrase": "data-specific_extensions"}, {"score": 0.0032372213689015844, "phrase": "base_algorithm"}, {"score": 0.0031121406603482112, "phrase": "narrow_class"}, {"score": 0.002649472341607234, "phrase": "good_context_models"}, {"score": 0.0025220591847502356, "phrase": "smoothness_constraints"}, {"score": 0.0025055391159787134, "phrase": "causal_block_boundaries"}, {"score": 0.0024405312482831646, "phrase": "obtained_probability_models"}, {"score": 0.0024007585788965655, "phrase": "existing_knowledge"}, {"score": 0.002377206033025253, "phrase": "original_scale"}, {"score": 0.0023538840032613535, "phrase": "included_blocks"}, {"score": 0.002330790244117364, "phrase": "dictionary_updating_process"}, {"score": 0.0023155201226300955, "phrase": "simulation_results"}, {"score": 0.0022628564592874147, "phrase": "significant_improvements"}, {"score": 0.0022406537579986842, "phrase": "original_mmp"}, {"score": 0.0021049977753042253, "phrase": "mmp's_universal_character"}], "paper_keywords": ["adaptive probability model", " image compression", " multiscale recurrent patterns", " side-match", " vector quantization"], "paper_abstract": "In this work, we further develop the multidimensional multiscale parser (MMIP) algorithm, a recently proposed universal lossy compression method which has been successfully applied to images as well as other types of data, as video and ECG signals. The MNIP is based on approximate multiscale pattern matching, encoding blocks of an input signal using expanded and contracted versions of patterns stored in a dictionary. The dictionary is updated using expanded and contracted versions of concatenations of previously encoded blocks. This implies that MMP builds its own dictionary while the input data is being encoded, using segments of the input itself, which lends it a universal flavor. It presents a flexible structure, which allows for easily adding data-specific extensions to the base algorithm. Often, the signals to be encoded belong to a narrow class, as the one of smooth images. In these cases, one expects that some improvement can be achieved by introducing some knowledge about the source to be encoded. In this paper, we use the assumption about the smoothness of the source in order to create good context models for the probability of blocks in the dictionary. Such probability models are estimated by considering smoothness constraints around causal block boundaries. In addition, we refine the obtained probability models by also exploiting the existing knowledge about the original scale of the included blocks during the dictionary updating process. Simulation results have shown that these developments allow significant improvements over the original MMP for smooth images, while keeping its state-of-the-art performance for more complex, less smooth ones, thus improving MMP's universal character.", "paper_title": "Universal image compression using multiscale recurrent patterns with adaptive probability model", "paper_id": "WOS:000254275000008"}