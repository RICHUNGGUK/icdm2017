{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "information_theory"}, {"score": 0.004635881163634672, "phrase": "privacy_model"}, {"score": 0.004497410940849537, "phrase": "data_anonymization"}, {"score": 0.004396267325338022, "phrase": "data_set"}, {"score": 0.0038644521996539466, "phrase": "key_attributes"}, {"score": 0.0036093693971289754, "phrase": "confidential_attribute"}, {"score": 0.0032951702142329357, "phrase": "entire_data"}, {"score": 0.0031246358910995316, "phrase": "threshold_t."}, {"score": 0.002985484575498061, "phrase": "privacy_measure"}, {"score": 0.002413531395475291, "phrase": "postrandomization_method"}, {"score": 0.0022711961447468114, "phrase": "discrete_case"}, {"score": 0.002153540857619362, "phrase": "noise_addition"}, {"score": 0.0021049977753042253, "phrase": "general_case"}], "paper_keywords": ["t-Closeness", " microdata anonymization", " information theory", " rate-distortion theory", " PRAM", " noise addition"], "paper_abstract": "Closeness is a privacy model recently defined for data anonymization. A data set is said to satisfy t-closeness if, for each group of records sharing a combination of key attributes, the distance between the distribution of a confidential attribute in the group and the distribution of the attribute in the entire data set is no more than a threshold t. Here, we define a privacy measure in terms of information theory, similar to t-closeness. Then, we use the tools of that theory to show that our privacy measure can be achieved by the postrandomization method (PRAM) for masking in the discrete case, and by a form of noise addition in the general case.", "paper_title": "From t-Closeness-Like Privacy to Postrandomization via Information Theory", "paper_id": "WOS:000281989800011"}