{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "tensor_deep_stacking_networks"}, {"score": 0.00474946741204285, "phrase": "novel_deep_architecture"}, {"score": 0.0046848707075530256, "phrase": "tensor_deep_stacking_network"}, {"score": 0.00419861271409079, "phrase": "bilinear_mapping"}, {"score": 0.004103817210338686, "phrase": "output_layer"}, {"score": 0.004029517895918228, "phrase": "weight_tensor"}, {"score": 0.003974673997871831, "phrase": "higher_order_statistics"}, {"score": 0.003920573616298175, "phrase": "hidden_binary"}, {"score": 0.0037284065559949064, "phrase": "learning_algorithm"}, {"score": 0.0036776458116294986, "phrase": "t-dsn's_weight_matrices"}, {"score": 0.0035133644847622383, "phrase": "main_parameter_estimation_burden"}, {"score": 0.003433987036569199, "phrase": "convex_subproblem"}, {"score": 0.0033872210664940817, "phrase": "closed-form_solution"}, {"score": 0.003325852346387272, "phrase": "efficient_and_scalable_parallel_implementation"}, {"score": 0.0031339552246625463, "phrase": "increasing_order"}, {"score": 0.003091262582401731, "phrase": "data_size"}, {"score": 0.003063123420705719, "phrase": "handwritten_digit_recognition"}, {"score": 0.0030352852357748, "phrase": "mnist"}, {"score": 0.0028600670228589547, "phrase": "timit"}, {"score": 0.0026704265616705023, "phrase": "experimental_results"}, {"score": 0.002527788827701828, "phrase": "associated_learning_methods"}, {"score": 0.002493333098814506, "phrase": "consistent_manner"}, {"score": 0.0024258208033731154, "phrase": "sufficient_depth"}, {"score": 0.0022962183207691188, "phrase": "t-dsn_block"}, {"score": 0.002234031373581961, "phrase": "softmax_layer"}, {"score": 0.0021049977753042253, "phrase": "low_error_rates"}], "paper_keywords": ["Deep learning", " stacking networks", " tensor", " bilinear models", " handwriting image classification", " phone classification and recognition", " MNIST", " TIMIT", " WSJ"], "paper_abstract": "A novel deep architecture, the tensor deep stacking network (T-DSN), is presented. The T-DSN consists of multiple, stacked blocks, where each block contains a bilinear mapping from two hidden layers to the output layer, using a weight tensor to incorporate higher order statistics of the hidden binary ([0, 1]) features. A learning algorithm for the T-DSN's weight matrices and tensors is developed and described in which the main parameter estimation burden is shifted to a convex subproblem with a closed-form solution. Using an efficient and scalable parallel implementation for CPU clusters, we train sets of T-DSNs in three popular tasks in increasing order of the data size: handwritten digit recognition using MNIST (60k), isolated state/phone classification and continuous phone recognition using TIMIT (1.1 m), and isolated phone classification using WSJ0 (5.2 m). Experimental results in all three tasks demonstrate the effectiveness of the T-DSN and the associated learning methods in a consistent manner. In particular, a sufficient depth of the T-DSN, a symmetry in the two hidden layers structure in each T-DSN block, our model parameter learning algorithm, and a softmax layer on top of T-DSN are shown to have all contributed to the low error rates observed in the experiments for all three tasks.", "paper_title": "Tensor Deep Stacking Networks", "paper_id": "WOS:000320381400010"}