{"auto_keywords": [{"score": 0.0407346512262469, "phrase": "shared_memory"}, {"score": 0.01475510885196895, "phrase": "smp_nodes"}, {"score": 0.00481495049065317, "phrase": "hybrid_mpi-thread_programming"}, {"score": 0.00464976086880886, "phrase": "parallel_sparse_direct_solver"}, {"score": 0.004310949776826717, "phrase": "supercomputer_architectures"}, {"score": 0.0034956560177000656, "phrase": "mpi_implementations"}, {"score": 0.0025967927629150715, "phrase": "high_performance"}, {"score": 0.0022841153001549193, "phrase": "hybrid_mpi-thread_implementation"}, {"score": 0.002244494905883452, "phrase": "parallel_direct_solver"}, {"score": 0.0021049977753042253, "phrase": "memory_and_run-time_performances"}], "paper_keywords": [""], "paper_abstract": "Since the last decade, most of the supercomputer architectures are based on clusters of SMP nodes. In those architectures the exchanges between processors are made through shared memory when the processors are located on a same SMP node and through the network otherwise. Generally, the MPI implementations provided by the constructor on those machines are adapted to this situation and take advantage of the shared memory to treat messages between processors in a same SMP node. Nevertheless, this transparent approach to exploit shared memory does not avoid the storage of the extra-structures needed to manage efficiently the communications between processors. For high performance parallel direct solvers, the storage of these extra-structures can become a bottleneck. In this paper, we propose an hybrid MPI-thread implementation of a parallel direct solver and analyse the benefits of this approach in terms of memory and run-time performances.", "paper_title": "On using an hybrid MPI-thread programming for the implementation of a parallel sparse direct solver on a network of SMP nodes", "paper_id": "WOS:000238107100127"}