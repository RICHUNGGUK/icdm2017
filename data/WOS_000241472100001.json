{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "regression_problems"}, {"score": 0.013911401308480153, "phrase": "new_features"}, {"score": 0.012425044389866493, "phrase": "feature_space"}, {"score": 0.004462663711734448, "phrase": "supervised_learning"}, {"score": 0.004136044939878838, "phrase": "original_features"}, {"score": 0.00369022262431101, "phrase": "better_performance"}, {"score": 0.003355515315708125, "phrase": "independent_component_analysis"}, {"score": 0.002854505078275351, "phrase": "general_ica_algorithms"}, {"score": 0.0026705669656375197, "phrase": "feature_extraction"}, {"score": 0.002522344964242888, "phrase": "joint_mutual_information"}, {"score": 0.0024747842618794255, "phrase": "target_variable_and_new_features"}, {"score": 0.0021049977753042253, "phrase": "regression_performance"}], "paper_keywords": [""], "paper_abstract": "In manipulating data such as in supervised learning, we often extract new features from the original features for the purpose of reducing the dimensions of feature space and achieving better performance. In this paper, we show how standard algorithms for independent component analysis (ICA) can be applied to extract features for regression problems. The advantage is that general ICA algorithms become available to a task of feature extraction for regression problems by maximizing the joint mutual information between target variable and new features. Using the new features, we can greatly reduce the dimension of feature space without degrading the regression performance.", "paper_title": "Dimensionality reduction based on ICA for regression problems", "paper_id": "WOS:000241472100001"}