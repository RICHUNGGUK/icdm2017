{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "effective_appearance_model"}, {"score": 0.04954517773618335, "phrase": "sparse_coding"}, {"score": 0.03032744492498588, "phrase": "probability_distribution"}, {"score": 0.004725787037228084, "phrase": "intelligent_video_surveillance"}, {"score": 0.004434745755769205, "phrase": "video_data"}, {"score": 0.00436890075128947, "phrase": "large_number"}, {"score": 0.0043363441738461335, "phrase": "surveillance_cameras"}, {"score": 0.004271953920234395, "phrase": "key_step"}, {"score": 0.004224286804379592, "phrase": "intelligent_surveillance_system"}, {"score": 0.004192803336288522, "phrase": "robust_visual_tracking"}, {"score": 0.0041151131903594445, "phrase": "computer_vision"}, {"score": 0.004008749730113699, "phrase": "basic_functionality"}, {"score": 0.003964007795174772, "phrase": "human_visual_system"}, {"score": 0.003890540531028228, "phrase": "psychophysical_findings"}, {"score": 0.0038184296585478724, "phrase": "receptive_fields"}, {"score": 0.0037899595175206143, "phrase": "simple_cells"}, {"score": 0.0037476503270386903, "phrase": "visual_cortex"}, {"score": 0.0031904741237384106, "phrase": "visual_tracking"}, {"score": 0.0030847461428168614, "phrase": "general_basis_functions"}, {"score": 0.0030502854924662837, "phrase": "independent_component_analysis"}, {"score": 0.0030162086479223506, "phrase": "large_set"}, {"score": 0.002993701876937243, "phrase": "natural_image_patches"}, {"score": 0.0028944757265206332, "phrase": "tracked_target"}, {"score": 0.0027159094186967247, "phrase": "partial_occlusion"}, {"score": 0.0026956374041441126, "phrase": "camouflage_environments"}, {"score": 0.0026357223464279983, "phrase": "illumination_changes"}, {"score": 0.0024916819803100635, "phrase": "entropy-gain_criterion"}, {"score": 0.0023291617361832157, "phrase": "related_features"}, {"score": 0.0023031224517224757, "phrase": "target_search"}, {"score": 0.002251911996999611, "phrase": "matusita_distance"}, {"score": 0.0022018377054878534, "phrase": "target_model"}, {"score": 0.0021609589091422608, "phrase": "newton-style_iterations"}, {"score": 0.0021367960510499575, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["Algorithms", " Intelligent visual surveillance", " appearance model", " sparse coding"], "paper_abstract": "Intelligent video surveillance is currently one of the most active research topics in computer vision, especially when facing the explosion of video data captured by a large number of surveillance cameras. As a key step of an intelligent surveillance system, robust visual tracking is very challenging for computer vision. However, it is a basic functionality of the human visual system (HVS). Psychophysical findings have shown that the receptive fields of simple cells in the visual cortex can be characterized as being spatially localized, oriented, and bandpass, and it forms a sparse, distributed representation of natural images. In this article, motivated by these findings, we propose an effective appearance model based on sparse coding and apply it in visual tracking. Specifically, we consider the responses of general basis functions extracted by independent component analysis on a large set of natural image patches as features and model the appearance of the tracked target as the probability distribution of these features. In order to make the tracker more robust to partial occlusion, camouflage environments, pose changes, and illumination changes, we further select features that are related to the target based on an entropy-gain criterion and ignore those that are not. The target is finally represented by the probability distribution of those related features. The target search is performed by minimizing the Matusita distance between the distributions of the target model and a candidate using Newton-style iterations. The experimental results validate that the proposed method is more robust and effective than three state-of-the-art methods.", "paper_title": "Robust Visual Tracking Using an Effective Appearance Model Based on Sparse Coding", "paper_id": "WOS:000313763400005"}