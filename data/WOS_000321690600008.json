{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "recursive_partitioning"}, {"score": 0.04967913398382337, "phrase": "generalized_linear_models"}, {"score": 0.04247015589711413, "phrase": "dependent_and_independent_variables"}, {"score": 0.004734853555630616, "phrase": "recursive_partitioning_algorithms"}, {"score": 0.004675652357253327, "phrase": "feature_space"}, {"score": 0.004578616481036476, "phrase": "disjoint_rectangles"}, {"score": 0.0042455945330942746, "phrase": "simple_and_intuitive_approach"}, {"score": 0.003696460568091852, "phrase": "candidate_variables"}, {"score": 0.003574364535717202, "phrase": "different_model_parameter_values"}, {"score": 0.003427406588505264, "phrase": "glm"}, {"score": 0.0033280910751441522, "phrase": "enhanced_interpretability"}, {"score": 0.00330025288273863, "phrase": "classical_trees"}, {"score": 0.0032181231788427655, "phrase": "explorative_way"}, {"score": 0.0031645050772280033, "phrase": "candidate_variable's_influence"}, {"score": 0.0031248767378106663, "phrase": "parametric_model"}, {"score": 0.0028015105858516702, "phrase": "parameter_instability"}, {"score": 0.002543392353000551, "phrase": "highest_instability"}, {"score": 0.0024489727828322693, "phrase": "terminal_node"}, {"score": 0.002398038618483724, "phrase": "glm."}, {"score": 0.0023481578019624843, "phrase": "method's_versatility"}, {"score": 0.002299313860419974, "phrase": "additional_insight"}, {"score": 0.002204646157481869, "phrase": "modelling_voting_behaviour"}, {"score": 0.0021770116505684394, "phrase": "failure_model"}, {"score": 0.002158780936704905, "phrase": "debt_amortization"}, {"score": 0.0021049977753042253, "phrase": "alternative_approaches"}], "paper_keywords": ["model-based recursive partitioning", " generalized linear models", " model trees", " functional trees", " parameter instability", " maximum likelihood"], "paper_abstract": "Recursive partitioning algorithms separate a feature space into a set of disjoint rectangles. Then, usually, a constant in every partition is fitted. While this is a simple and intuitive approach, it may still lack interpretability as to how a specific relationship between dependent and independent variables may look. Or it may be that a certain model is assumed or of interest and there is a number of candidate variables that may non-linearly give rise to different model parameter values. We present an approach that combines generalized linear models (GLM) with recursive partitioning that offers enhanced interpretability of classical trees as well as providing an explorative way to assess a candidate variable's influence on a parametric model. This method conducts recursive partitioning of a GLM by (1) fitting the model to the data set, (2) testing for parameter instability over a set of partitioning variables, (3) splitting the data set with respect to the variable associated with the highest instability. The outcome is a tree where each terminal node is associated with a GLM. We will show the method's versatility and suitability to gain additional insight into the relationship of dependent and independent variables by two examples, modelling voting behaviour and a failure model for debt amortization, and compare it to alternative approaches.", "paper_title": "Gaining insight with recursive partitioning of generalized linear models", "paper_id": "WOS:000321690600008"}