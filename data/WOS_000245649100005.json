{"auto_keywords": [{"score": 0.04712665802421912, "phrase": "tracking_forecast"}, {"score": 0.005427189118206321, "phrase": "best_response"}, {"score": 0.00481495049065317, "phrase": "simple_learning_process"}, {"score": 0.004330941610682453, "phrase": "recent_outcomes"}, {"score": 0.004081385289516914, "phrase": "significantly_reduced_memory_requirements"}, {"score": 0.00406862420013284, "phrase": "nash_equilibrium"}, {"score": 0.0033865859510157238, "phrase": "binary_outcomes"}, {"score": 0.003287482882222731, "phrase": "internal_state_assumptions"}, {"score": 0.0031912706211861324, "phrase": "repeated_strategic_game"}, {"score": 0.0030457198309616694, "phrase": "opponent_actions"}, {"score": 0.0027390682208834013, "phrase": "standard_learning_algorithm"}, {"score": 0.0026929458952693465, "phrase": "exponential_regret"}, {"score": 0.0024842242921857705, "phrase": "actual_play"}, {"score": 0.002281930100792185, "phrase": "game_matrix"}, {"score": 0.002205692205167157, "phrase": "positive_probability"}, {"score": 0.002177762848946681, "phrase": "larger_class"}], "paper_keywords": ["learning in games", " forecasting", " calibration", " fictitious play", " prediction of universal sequences", " stochastic approximation", " The ODE method"], "paper_abstract": "We provide a simple learning process that enables an agent to forecast a sequence of outcomes. Our forecasting scheme, termed tracking forecast, is based on tracking the past observations while emphasizing recent outcomes. As opposed to other forecasting schemes, we sacrifice universality in favor of a significantly reduced memory requirements. We show that if the sequence of outcomes has certain properties-it has some internal (hidden) state that does not change too rapidly-then the tracking forecast is weakly calibrated so that the forecast appears to be correct most of the time. For binary outcomes, this result holds without any internal state assumptions. We consider learning in a repeated strategic game where each player attempts to compute some forecast of the opponent actions and play a best response to it. We show that if one of the players uses a tracking forecast, while the other player uses a standard learning algorithm (such as exponential regret matching or smooth fictitious play), then the player using the tracking forecast obtains the best response to the actual play of the other players. We further show that if both players use tracking forecast, then under certain conditions on the game matrix, convergence to a Nash equilibrium is possible with positive probability for a larger class of games than the class of games for which smooth fictitious play converges to a Nash equilibrium.", "paper_title": "Online calibrated forecasts: Memory efficiency versus universality for learning in games", "paper_id": "WOS:000245649100005"}