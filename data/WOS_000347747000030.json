{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dnrf"}, {"score": 0.004634254474150777, "phrase": "randomly_trained_disjunctive_normal_decision_trees"}, {"score": 0.004195418010131832, "phrase": "decision_tree"}, {"score": 0.0041001740903338834, "phrase": "random_forest"}, {"score": 0.003769006950727621, "phrase": "boolean_functions"}, {"score": 0.00346449481422526, "phrase": "differentiable_function"}, {"score": 0.0033599501775954024, "phrase": "learning_process"}, {"score": 0.0032836104153151973, "phrase": "risk_minimization_problem"}, {"score": 0.0031845067058084613, "phrase": "classification_error"}, {"score": 0.0031121406603482112, "phrase": "single_global_objective_function"}, {"score": 0.0030414140732789186, "phrase": "minimization_problem"}, {"score": 0.002949598537993663, "phrase": "gradient_descent"}, {"score": 0.0027955219242123013, "phrase": "complex_decision_boundaries"}, {"score": 0.002731971103291716, "phrase": "low_generalization_error"}, {"score": 0.002649472341607234, "phrase": "experimental_results"}, {"score": 0.0025892330566358503, "phrase": "improved_performance"}, {"score": 0.0024728220973730293, "phrase": "conventional_decision_trees"}, {"score": 0.002435190484324197, "phrase": "random_forests"}, {"score": 0.002325689073229477, "phrase": "superior_performance"}, {"score": 0.002255431275670053, "phrase": "state-of-the-art_classification_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Random forest", " Decision tree", " Classifier", " Supervised learning", " Disjunctive normal form"], "paper_abstract": "We develop a novel supervised learning/classification method, called disjunctive normal random forest (DNRF). A DNRF is an ensemble of randomly trained disjunctive normal decision trees (DNDT). To construct a DNDT, we formulate each decision tree in the random forest as a disjunction of rules, which are conjunctions of Boolean functions. We then approximate this disjunction of conjunctions with a differentiable function and approach the learning process as a risk minimization problem that incorporates the classification error into a single global objective function. The minimization problem is solved using gradient descent. DNRFs are able to learn complex decision boundaries and achieve low generalization error. We present experimental results demonstrating the improved performance of DNDTs and DNRFs over conventional decision trees and random forests. We also show the superior performance of DNRFs over state-of-the-art classification methods on benchmark datasets. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Disjunctive normal random forests", "paper_id": "WOS:000347747000030"}