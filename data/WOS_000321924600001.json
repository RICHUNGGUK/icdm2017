{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "linear_dimensionality_reduction"}, {"score": 0.004741167616693272, "phrase": "manifold_learning"}, {"score": 0.00456159659731398, "phrase": "machine_learning"}, {"score": 0.004491678520807451, "phrase": "pattern_recognition"}, {"score": 0.0032216622984027558, "phrase": "adjacency_graph"}, {"score": 0.003123527559542239, "phrase": "intraclass_variation"}, {"score": 0.0026965813247045427, "phrase": "discriminant_objective_function"}, {"score": 0.0025151755329660837, "phrase": "orthogonal_constraint"}, {"score": 0.0024574537875026634, "phrase": "basis_vectors"}, {"score": 0.002382541887825168, "phrase": "orthogonal_algorithm"}, {"score": 0.0022394839933089074, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "proposed_dimensionality_reduction_approach"}], "paper_keywords": ["Discriminant analysis", " diversity", " face recognition", " manifold learning", " similarity"], "paper_abstract": "Manifold learning is widely used in machine learning and pattern recognition. However, manifold learning only considers the similarity of samples belonging to the same class and ignores the within-class variation of data, which will impair the generalization and stableness of the algorithms. For this purpose, we construct an adjacency graph to model the intraclass variation that characterizes the most important properties, such as diversity of patterns, and then incorporate the diversity into the discriminant objective function for linear dimensionality reduction. Finally, we introduce the orthogonal constraint for the basis vectors and propose an orthogonal algorithm called stable orthogonal local discriminate embedding. Experimental results on several standard image databases demonstrate the effectiveness of the proposed dimensionality reduction approach.", "paper_title": "Stable Orthogonal Local Discriminant Embedding for Linear Dimensionality Reduction", "paper_id": "WOS:000321924600001"}