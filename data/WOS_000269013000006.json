{"auto_keywords": [{"score": 0.03399273535727023, "phrase": "policy_iteration"}, {"score": 0.015719716506582538, "phrase": "approximate_policy_evaluation"}, {"score": 0.004385555408850022, "phrase": "target_value_function"}, {"score": 0.004261268957001927, "phrase": "specific_criterion"}, {"score": 0.0042004472903194616, "phrase": "different_algorithms"}, {"score": 0.004140490129683107, "phrase": "different_choices"}, {"score": 0.004052148727852149, "phrase": "optimization_criterion"}, {"score": 0.003798231095121328, "phrase": "bellman_residual_method"}, {"score": 0.003664101941977768, "phrase": "bellman"}, {"score": 0.0035346515636097533, "phrase": "fixed_point_method"}, {"score": 0.00333697550984782, "phrase": "bellman_residual"}, {"score": 0.003150319576043518, "phrase": "fixed_point_algorithm"}, {"score": 0.003038983069081333, "phrase": "better_performing_policies"}, {"score": 0.0029740732099993706, "phrase": "bellman_residual_algorithm"}, {"score": 0.0025021765811679446, "phrase": "analytical_and_geometric_interpretation"}, {"score": 0.0023451553255837317, "phrase": "simple_problem"}, {"score": 0.0023116191163488824, "phrase": "experimental_results"}, {"score": 0.002262209824105154, "phrase": "small_and_large_domains"}, {"score": 0.0021049977753042253, "phrase": "better_policies"}], "paper_keywords": ["Reinforcement learning", " Markov decision processes"], "paper_abstract": "The goal of approximate policy evaluation is to \"best\" represent a target value function according to a specific criterion. Different algorithms offer different choices of the optimization criterion. Two popular least-squares algorithms for performing this task are the Bellman residual method, which minimizes the Bellman residual, and the fixed point method, which minimizes the projection of the Bellman residual. When used within policy iteration, the fixed point algorithm tends to ultimately find better performing policies whereas the Bellman residual algorithm exhibits more stable behavior between rounds of policy iteration. We propose two hybrid least-squares algorithms to try to combine the advantages of these algorithms. We provide an analytical and geometric interpretation of hybrid algorithms and demonstrate their utility on a simple problem. Experimental results on both small and large domains suggest hybrid algorithms may find solutions that lead to better policies when performing policy iteration.", "paper_title": "Hybrid least-squares algorithms for approximate policy evaluation", "paper_id": "WOS:000269013000006"}