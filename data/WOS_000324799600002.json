{"auto_keywords": [{"score": 0.04920822493417558, "phrase": "g-kl"}, {"score": 0.01489001259223546, "phrase": "bayesian"}, {"score": 0.011749186369709182, "phrase": "g-kl_methods"}, {"score": 0.00481495049065317, "phrase": "gaussian_kullback-leibler"}, {"score": 0.004641351004344458, "phrase": "variational_approximate_inference_techniques"}, {"score": 0.004515252649226697, "phrase": "generalised_linear_models"}, {"score": 0.004081385289516914, "phrase": "sufficient_conditions"}, {"score": 0.0039341294012600085, "phrase": "g-kl_objective"}, {"score": 0.0037244947082546984, "phrase": "gaussian"}, {"score": 0.002878140062594849, "phrase": "local_lower_bounding_methods"}, {"score": 0.002825681384008326, "phrase": "complexity_and_model_applicability_issues"}, {"score": 0.0026252106881752067, "phrase": "numerical_results"}, {"score": 0.00241658945942583, "phrase": "robust_gaussian_process_regression_models"}, {"score": 0.0023079225317714815, "phrase": "laplace_likelihoods"}, {"score": 0.00226583335721762, "phrase": "large_scale_bayesian_binary_logistic_regression_models"}, {"score": 0.0021049977753042253, "phrase": "sequential_experimental_design"}], "paper_keywords": ["generalised linear models", " latent linear models", " variational approximate inference", " large scale inference", " sparse learning", " experimental design", " active learning", " Gaussian processes"], "paper_abstract": "We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.", "paper_title": "Gaussian Kullback-Leibler Approximate Inference", "paper_id": "WOS:000324799600002"}