{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multiclass_empirical_risk_minimization_methods"}, {"score": 0.004444921827311577, "phrase": "classification_algorithm"}, {"score": 0.004327981760540394, "phrase": "central_role"}, {"score": 0.004251728624599691, "phrase": "statistical_learning_theory"}, {"score": 0.004139850082354509, "phrase": "consistent_algorithm"}, {"score": 0.0037209035703970705, "phrase": "unknown_distribution"}, {"score": 0.0034963129868796033, "phrase": "erm_scheme"}, {"score": 0.0031422859551159506, "phrase": "multiclass_classification"}, {"score": 0.002952516738708181, "phrase": "mild_conditions"}, {"score": 0.002824005136794786, "phrase": "quantitative_relationship"}, {"score": 0.0027741761880779535, "phrase": "classification_errors"}, {"score": 0.0027252240573281163, "phrase": "convex_risks"}, {"score": 0.00258347644721642, "phrase": "related_previous_work"}, {"score": 0.0021049977753042253, "phrase": "convex_function"}], "paper_keywords": ["multiclass classification", " classifier", " consistency", " empirical risk minimization", " constrained comparison method", " Tsybakov noise condition"], "paper_abstract": "The consistency of classification algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially suffices to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classifiers) in multiclass classification. Our approach is, under some mild conditions, to establish a quantitative relationship between classification errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function.", "paper_title": "Consistency of multiclass empirical risk minimization methods based on convex loss", "paper_id": "WOS:000245390700006"}