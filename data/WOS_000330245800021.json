{"auto_keywords": [{"score": 0.05007764930807702, "phrase": "best_practices"}, {"score": 0.049711581425976384, "phrase": "qoe_crowdtesting"}, {"score": 0.04028758691303017, "phrase": "qoe"}, {"score": 0.0047415081747150065, "phrase": "qoe_assessment_with_crowdsourcing"}, {"score": 0.004580314008293946, "phrase": "multimedia_applications"}, {"score": 0.004475893575036647, "phrase": "end_users'_perception"}, {"score": 0.004390689303780405, "phrase": "subjective_user_studies"}, {"score": 0.004065721092116729, "phrase": "qoe_assessment"}, {"score": 0.003988293689448734, "phrase": "anonymous_test_subjects"}, {"score": 0.003957735532719663, "phrase": "subjective_tests"}, {"score": 0.003735862292632131, "phrase": "reduced_time"}, {"score": 0.0035810720634458933, "phrase": "large_and_diverse_panel"}, {"score": 0.0034725269588525534, "phrase": "realistic_user_settings"}, {"score": 0.0034194902384248006, "phrase": "conceptual_and_technical_challenges"}, {"score": 0.0033543281764294927, "phrase": "remote_test_settings"}, {"score": 0.003328611206488653, "phrase": "key_issues"}, {"score": 0.003215295260684715, "phrase": "user_ratings"}, {"score": 0.0031298260299218684, "phrase": "payment_schemes"}, {"score": 0.0030938932434437178, "phrase": "unknown_environmental_context"}, {"score": 0.0027992118597278087, "phrase": "test_design"}, {"score": 0.0027247727563746694, "phrase": "actual_test_campaign"}, {"score": 0.0026934777251033776, "phrase": "statistical_methods"}, {"score": 0.0026421139329830755, "phrase": "reliable_user_ratings"}, {"score": 0.002601727192945944, "phrase": "high_data_quality"}, {"score": 0.0024181413029825205, "phrase": "large_set"}, {"score": 0.0023995846112991625, "phrase": "conducted_qoe_crowdtesting_studies"}, {"score": 0.0022388390432214415, "phrase": "video_quality_assessment"}, {"score": 0.0021876814398268775, "phrase": "proposed_best_practices"}, {"score": 0.0021294699637639564, "phrase": "qoe_crowdtesting_design"}], "paper_keywords": ["Best practices", " cognition", " human computer interaction", " instrumentation and measurementmultimedia systems", " testing"], "paper_abstract": "Quality of Experience (QoE) in multimedia applications is closely linked to the end users' perception and therefore its assessment requires subjective user studies in order to evaluate the degree of delight or annoyance as experienced by the users. QoE crowdtesting refers to QoE assessment using crowdsourcing, where anonymous test subjects conduct subjective tests remotely in their preferred environment. The advantages of QoE crowdtesting lie not only in the reduced time and costs for the tests, but also in a large and diverse panel of international, geographically distributed users in realistic user settings. However, conceptual and technical challenges emerge due to the remote test settings. Key issues arising from QoE crowdtesting include the reliability of user ratings, the influence of incentives, payment schemes and the unknown environmental context of the tests on the results. In order to counter these issues, strategies and methods need to be developed, included in the test design, and also implemented in the actual test campaign, while statistical methods are required to identify reliable user ratings and to ensure high data quality. This contribution therefore provides a collection of best practices addressing these issues based on our experience gained in a large set of conducted QoE crowdtesting studies. The focus of this article is in particular on the issue of reliability and we use video quality assessment as an example for the proposed best practices, showing that our recommended two-stage QoE crowdtesting design leads to more reliable results.", "paper_title": "Best Practices for QoE Crowdtesting: QoE Assessment With Crowdsourcing", "paper_id": "WOS:000330245800021"}