{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "direct_face-to-face_interaction"}, {"score": 0.004459334186889869, "phrase": "main_challenge"}, {"score": 0.004278807087784626, "phrase": "significant_portion"}, {"score": 0.00420368356873615, "phrase": "user's_face"}, {"score": 0.004129873531628877, "phrase": "effective_facial_capture"}, {"score": 0.004081385289516914, "phrase": "traditional_techniques"}, {"score": 0.003986103192038901, "phrase": "virtual_reality"}, {"score": 0.003916098498242888, "phrase": "next-generation_communication_platform"}, {"score": 0.003802135020791801, "phrase": "novel_hmd"}, {"score": 0.003562912424153324, "phrase": "ultra-thin_flexible_electronic_materials"}, {"score": 0.0034388111429203222, "phrase": "foam_liner"}, {"score": 0.0033190180867511605, "phrase": "surface_strain_signals"}, {"score": 0.003260690513595527, "phrase": "upper_face"}, {"score": 0.0031845067058084613, "phrase": "strain_signals"}, {"score": 0.003091767402370757, "phrase": "head-mounted_rgb-d_camera"}, {"score": 0.002966438503447793, "phrase": "mouth_region"}, {"score": 0.0028800314406625996, "phrase": "inaccurate_hmd_placement"}, {"score": 0.0027961342030212353, "phrase": "input_signals"}, {"score": 0.00266693888510356, "phrase": "single-instance_offline_training_session"}, {"score": 0.002573965014439041, "phrase": "reusable_and_accurate_online_operation"}, {"score": 0.002498961351132696, "phrase": "short_calibration_step"}, {"score": 0.0024405312482831646, "phrase": "gaussian_mixture_distribution"}, {"score": 0.0023139986108451967, "phrase": "resulting_animations"}, {"score": 0.002233301265743633, "phrase": "cutting-edge_depth_sensor-driven_facial_performance_capture_systems"}, {"score": 0.0021049977753042253, "phrase": "virtual_worlds"}], "paper_keywords": ["real-time facial performance capture", " virtual reality", " depth camera", " strain gauge", " head-mounted display", " wearable sensors"], "paper_abstract": "There are currently no solutions for enabling direct face-to-face interaction between virtual reality (VR) users wearing head-mounted displays (HMDs). The main challenge is that the headset obstructs a significant portion of a user's face, preventing effective facial capture with traditional techniques. To advance virtual reality as a next-generation communication platform, we develop a novel HMD that enables 3D facial performance-driven animation in real-time. Our wearable system uses ultra-thin flexible electronic materials that are mounted on the foam liner of the headset to measure surface strain signals corresponding to upper face expressions. These strain signals are combined with a head-mounted RGB-D camera to enhance the tracking in the mouth region and to account for inaccurate HMD placement. To map the input signals to a 3D face model, we perform a single-instance offline training session for each person. For reusable and accurate online operation, we propose a short calibration step to readjust the Gaussian mixture distribution of the mapping before each use. The resulting animations are visually on par with cutting-edge depth sensor-driven facial performance capture systems and hence, are suitable for social interactions in virtual worlds.", "paper_title": "Facial Performance Sensing Head-Mounted Display", "paper_id": "WOS:000358786600013"}