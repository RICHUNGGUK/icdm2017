{"auto_keywords": [{"score": 0.04978573858550203, "phrase": "large-scale_image"}, {"score": 0.04680789077928369, "phrase": "text_query"}, {"score": 0.0368366523805859, "phrase": "discriminative_classifier"}, {"score": 0.00481495049065317, "phrase": "visual_search"}, {"score": 0.004584045720295891, "phrase": "large-scale_video_datasets"}, {"score": 0.004377595128584772, "phrase": "visual_models"}, {"score": 0.004245130986749751, "phrase": "run_time"}, {"score": 0.004180403355909392, "phrase": "image_search_engine"}, {"score": 0.004142039499650824, "phrase": "visual_training_data"}, {"score": 0.0036069265660350915, "phrase": "visual_words"}, {"score": 0.0035409897891004105, "phrase": "object_categories"}, {"score": 0.0034655801487210158, "phrase": "key_frames"}, {"score": 0.003360620035679697, "phrase": "ranking_face_tracks"}, {"score": 0.003199560566006696, "phrase": "fisher"}, {"score": 0.003140727213993457, "phrase": "convolutional_neural_networks"}, {"score": 0.0029806849142611984, "phrase": "real-time_system"}, {"score": 0.0027011613811219323, "phrase": "important_implementation_issues"}, {"score": 0.0026112423766508543, "phrase": "downloaded_images"}, {"score": 0.0025398975698084484, "phrase": "single_descriptor"}, {"score": 0.002516550704970801, "phrase": "face_track"}, {"score": 0.0023956021950024124, "phrase": "quantitative_results"}, {"score": 0.00221130685073215, "phrase": "real-world_applicability"}, {"score": 0.0021376596688185605, "phrase": "unedited_footage"}, {"score": 0.0021245346610409698, "phrase": "bbc_news"}], "paper_keywords": ["Object category retrieval and recognition", " Object instance retrieval", " Face retrieval", " On-the-fly", " Convolutional neural networks"], "paper_abstract": "The objective of this work is to visually search large-scale video datasets for semantic entities specified by a text query. The paradigm we explore is constructing visual models for such semantic entities on-the-fly, i.e. at run time, by using an image search engine to source visual training data for the text query. The approach combines fast and accurate learning and retrieval, and enables videos to be returned within seconds of specifying a query. We describe three classes of queries, each with its associated visual search method: object instances (using a bag of visual words approach for matching); object categories (using a discriminative classifier for ranking key frames); and faces (using a discriminative classifier for ranking face tracks). We discuss the features suitable for each class of query, for example Fisher vectors or features derived from convolutional neural networks (CNNs), and how these choices impact on the trade-off between three important performance measures for a real-time system of this kind, namely: (1) accuracy, (2) memory footprint, and (3) speed. We also discuss and compare a number of important implementation issues, such as how to remove 'outliers' in the downloaded images efficiently, and how to best obtain a single descriptor for a face track. We also sketch the architecture of the real-time on-the-fly system. Quantitative results are given on a number of large-scale image and video benchmarks (e.g. TRECVID INS, MIRFLICKR-1M), and we further demonstrate the performance and real-world applicability of our methods over a dataset sourced from 10,000 h of unedited footage from BBC News, comprising 5M+ key frames.", "paper_title": "On-the-fly learning for visual search of large-scale image and video datasets", "paper_id": "WOS:000363895500002"}