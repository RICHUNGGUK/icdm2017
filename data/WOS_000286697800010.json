{"auto_keywords": [{"score": 0.03852424281765506, "phrase": "lipsvm"}, {"score": 0.008653968539241037, "phrase": "convex_minimization"}, {"score": 0.008565030272379684, "phrase": "proximal_support_vector_machine"}, {"score": 0.00481495049065317, "phrase": "twin_svm"}, {"score": 0.004571630734130793, "phrase": "effective_classification_tool"}, {"score": 0.004540126719762935, "phrase": "supervised_learning"}, {"score": 0.004207527897084009, "phrase": "instable_classification_performance"}, {"score": 0.004149716134716874, "phrase": "matrix_singularity"}, {"score": 0.003953534716060798, "phrase": "gepsvm"}, {"score": 0.0038190873308414333, "phrase": "similar_formulation"}, {"score": 0.0037796484923665855, "phrase": "maximum_margin_criterion"}, {"score": 0.003466382911409225, "phrase": "reduced_algorithm"}, {"score": 0.0033951334049808733, "phrase": "selectively_generated_points"}, {"score": 0.0033138516659108387, "phrase": "major_advantage"}, {"score": 0.003113637673848439, "phrase": "geometric_intuition"}, {"score": 0.002997269132281551, "phrase": "localized_twin_svm"}, {"score": 0.0027773810175217273, "phrase": "superior_characteristics"}, {"score": 0.002720254890261961, "phrase": "lctsvm"}, {"score": 0.0026276369627747896, "phrase": "similar_or_better_classification_capability"}, {"score": 0.00258251424573557, "phrase": "twsvm"}, {"score": 0.002564682406589373, "phrase": "lstsvm"}, {"score": 0.00246877952076635, "phrase": "quadratic_programming_problem"}, {"score": 0.0024096216848276094, "phrase": "special_convex_difference_optimization"}, {"score": 0.0022639147412879487, "phrase": "linear_equations"}, {"score": 0.0022327484973125936, "phrase": "considerably_lesser_computational_cost"}, {"score": 0.0021566930572796935, "phrase": "global_minimum"}, {"score": 0.0021049977753042253, "phrase": "toy_and_real-world_problems"}], "paper_keywords": ["GEPSVM", " Outliers", " Quadratic programming", " Linear equations", " Global minimum"], "paper_abstract": "Multisurface proximal support vector machine via generalized eigenvalues (GEPSVM), being an effective classification tool for supervised learning, tries to seek two nonparallel planes that are determined by solving two generalized eigenvalue problems (GEPs). The GEPs may lead to an instable classification performance, due to matrix singularity. Proximal support vector machine using local information (LIPSVM), as a variant of GEPSVM, attempts to avoid the above shortcoming through adopting a similar formulation to the Maximum Margin Criterion (MMC). The solution to an LIPSVM follows directly from solving two standard eigenvalue problems. Actually, an LIPSVM can be viewed as a reduced algorithm, because it uses the selectively generated points to train the classifier. A major advantage of an LIPSVM is that it is resistant to outliers. In this paper, following the geometric intuition of an LIPSVM, a novel multiplane learning approach called Localized Twin SVM via Convex Minimization (LCTSVIvI) is proposed. This approach determines two nonparallel planes by solving two newly formed SVM-type problems. In addition to keeping the superior characteristics of an LIPSVM, an LCTSVM still has its additional edges: (1) it has similar or better classification capability compared to LIPSVM, TWSVM and LSTSVM: (2) each plane is generated from a quadratic programming problem (QPP) instead of a special convex difference optimization arising from an LIPSVM: (3) the solution can be reduced to solving two systems of linear equations, resulting in considerably lesser computational cost: and (4) it can find the global minimum. Experiments carried out on both toy and real-world problems disclose the effectiveness of an LCTSVM. (c) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Localized twin SVM via convex minimization", "paper_id": "WOS:000286697800010"}