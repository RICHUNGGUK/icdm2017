{"auto_keywords": [{"score": 0.003689053436223376, "phrase": "variable_interpretations"}, {"score": 0.0036218682585355895, "phrase": "multiple_semantics"}, {"score": 0.003097865379563127, "phrase": "active_learning"}, {"score": 0.0030414140732789186, "phrase": "kernel_methods"}, {"score": 0.002723607240517261, "phrase": "active-learning_strategies"}, {"score": 0.0023944552329463035, "phrase": "concept_complexity"}, {"score": 0.002329258721870127, "phrase": "dataset_size"}, {"score": 0.0021049977753042253, "phrase": "future_directions"}], "paper_keywords": ["active learning", " image retrieval", " relevance feedback", " support vector machines"], "paper_abstract": "Query-by-example and query-by-keyword both suffer from the problem of \"aliasing,\" meaning that example-images and keywords potentially have variable interpretations or multiple semantics. For discerning which semantic is appropriate for a given query, we have established that combining active learning with kernel methods is a very effective approach. In this work, we first examine active-learning strategies, and then focus on addressing the challenges of two scalability issues: scalability in concept complexity and in dataset size. We present remedies, explain limitations, and discuss future directions that research might take.", "paper_title": "Active learning in very large databases", "paper_id": "WOS:000243048800002"}