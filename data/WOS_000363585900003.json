{"auto_keywords": [{"score": 0.050078487022614894, "phrase": "deep_dependency"}, {"score": 0.0357982630927138, "phrase": "ddss"}, {"score": 0.029175913066560154, "phrase": "topic-sensitive_mtl_model"}, {"score": 0.028170790074395588, "phrase": "frequent_ddsss"}, {"score": 0.004668957916795845, "phrase": "multidocument_summarization"}, {"score": 0.004412654209742722, "phrase": "multiple_documents"}, {"score": 0.004322967486451877, "phrase": "proper_subset"}, {"score": 0.004085579055228594, "phrase": "textual_units"}, {"score": 0.0038217146888983576, "phrase": "deep_syntactic_and_semantic_information"}, {"score": 0.0037057255600764475, "phrase": "novel_extractive_topic-focused_multidocument_summarization_framework"}, {"score": 0.003048370487544551, "phrase": "deep_dependency_structures"}, {"score": 0.0030017207229983385, "phrase": "head-driven_phrase_structure_grammar"}, {"score": 0.002836679611901583, "phrase": "semantic_normalization"}, {"score": 0.002443671484766342, "phrase": "summary_extraction"}, {"score": 0.0024186620196958867, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "topic-focused_multidocument_summarization"}], "paper_keywords": ["Algorithms", " Experimentation", " Document summarization", " deep dependency sub-structure", " multi-task learning"], "paper_abstract": "Most extractive style topic-focused multidocument summarization systems generate a summary by ranking textual units in multiple documents and extracting a proper subset of sentences biased to the given topic. Usually, the textual units are simply represented as sentences or n-grams, which do not carry deep syntactic and semantic information. This article presents a novel extractive topic-focused multidocument summarization framework. The framework proposes a new kind of more meaningful and informative units named frequent Deep Dependency Sub-Structure (DDSS) and a topic-sensitive Multi-Task Learning (MTL) model for frequent DDSS ranking. Given a document set, first, we parse all the sentences into deep dependency structures with a Head-driven Phrase Structure Grammar (HPSG) parser and mine the frequent DDSSs after semantic normalization. Then we employ a topic-sensitive MTL model to learn the importance of these frequent DDSSs. Finally, we exploit an Integer Linear Programming (ILP) formulation and use the frequent DDSSs as the essentials for summary extraction. Experimental results on two DUC datasets demonstrate that our proposed approach can achieve state-of-the-art performance. Both the DDSS information and the topic-sensitive MTL model are validated to be very helpful for topic-focused multidocument summarization.", "paper_title": "Deep Dependency Substructure-Based Learning for Multidocument Summarization", "paper_id": "WOS:000363585900003"}