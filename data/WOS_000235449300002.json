{"auto_keywords": [{"score": 0.049124089120586564, "phrase": "data_mining"}, {"score": 0.04726673405217005, "phrase": "large_databases"}, {"score": 0.036020302585102346, "phrase": "squared_residuals"}, {"score": 0.00481495049065317, "phrase": "lts_regression"}, {"score": 0.004766683200410046, "phrase": "large_data_sets"}, {"score": 0.004624751696958501, "phrase": "previously_unknown_patterns"}, {"score": 0.004331501320993783, "phrase": "robust_estimation"}, {"score": 0.004288059466938456, "phrase": "outlier_detection"}, {"score": 0.004160319763461951, "phrase": "e.g._rousseeuw"}, {"score": 0.0041185937574534474, "phrase": "leroy"}, {"score": 0.003916098498242888, "phrase": "least_trimmed_squares"}, {"score": 0.0036676174658755683, "phrase": "h_cases"}, {"score": 0.003452218803245602, "phrase": "smallest_sum"}, {"score": 0.003366233863411861, "phrase": "coverage_h"}, {"score": 0.003216804708106142, "phrase": "computation_time"}, {"score": 0.0031845067058084583, "phrase": "existing_lts_algorithms"}, {"score": 0.0030431198971423937, "phrase": "data_set"}, {"score": 0.0028498718992429825, "phrase": "new_algorithm"}, {"score": 0.002821247621250663, "phrase": "fast-lts."}, {"score": 0.0027929100418926725, "phrase": "basic_ideas"}, {"score": 0.002723301730223192, "phrase": "order_statistics"}, {"score": 0.0024125253170522816, "phrase": "intercept_adjustment_technique"}, {"score": 0.0022937210378944457, "phrase": "fast-lts"}, {"score": 0.0022478501955483007, "phrase": "exact_lts"}, {"score": 0.0021263705039915198, "phrase": "existing_algorithms"}, {"score": 0.002105010001444309, "phrase": "lts"}], "paper_keywords": ["breakdown value", " linear model", " outlier detection", " regression", " robust estimation"], "paper_abstract": "Data mining aims to extract previously unknown patterns or substructures from large databases. In statistics, this is what methods of robust estimation and outlier detection were constructed for, see e.g. Rousseeuw and Leroy (1987). Here we will focus on least trimmed squares (LTS) regression, which is based on the subset of h cases (out of n) whose least squares fit possesses the smallest sum of squared residuals. The coverage h may be set between n/2 and n. The computation time of existing LTS algorithms grows too much with the size of the data set, precluding their use for data mining. In this paper we develop a new algorithm called FAST-LTS. The basic ideas are an inequality involving order statistics and sums of squared residuals, and techniques which we call 'selective iteration' and nested extensions'. We also use an intercept adjustment technique to improve the precision. For small data sets FAST-LTS typically finds the exact LTS, whereas for larger data sets it gives more accurate results than existing algorithms for LTS and is faster by orders of magnitude. This allows us to apply FAST-LTS to large databases.", "paper_title": "Computing LTS regression for large data sets", "paper_id": "WOS:000235449300002"}