{"auto_keywords": [{"score": 0.0482650857509263, "phrase": "dialogue_management_policies"}, {"score": 0.03763429310267491, "phrase": "supervised_learning"}, {"score": 0.00481495049065317, "phrase": "dialogue_policies"}, {"score": 0.004777917315737992, "phrase": "fixed_data_sets"}, {"score": 0.004388796812234033, "phrase": "information_state_update"}, {"score": 0.004141742095830515, "phrase": "large_set"}, {"score": 0.00398478333092707, "phrase": "huge_policy_space"}, {"score": 0.0038634933357842302, "phrase": "fixed_data_set"}, {"score": 0.0037749456215035856, "phrase": "small_portions"}, {"score": 0.0037314328720368453, "phrase": "state_and_policy_spaces"}, {"score": 0.0036600192572132676, "phrase": "hybrid_model"}, {"score": 0.003534889186471058, "phrase": "reinforcement_learning"}, {"score": 0.003427246079774314, "phrase": "dialogue_reward"}, {"score": 0.0032972747743006603, "phrase": "learned_policy"}, {"score": 0.0030994630902300133, "phrase": "linear_function_approximation"}, {"score": 0.002993439939273828, "phrase": "fixed_amount"}, {"score": 0.002947486683884831, "phrase": "large_state_spaces"}, {"score": 0.00282470997972535, "phrase": "challenging_task"}, {"score": 0.0027386335451725762, "phrase": "communicator_corpus"}, {"score": 0.002644920521740564, "phrase": "user_actions"}, {"score": 0.002554406040896888, "phrase": "user_simulation"}, {"score": 0.0025151755329660837, "phrase": "different_part"}, {"score": 0.002429090371567499, "phrase": "pure_supervised_learning_model"}, {"score": 0.002401053532270901, "phrase": "pure_reinforcement_learning_model"}, {"score": 0.002345944657572499, "phrase": "hand-crafted_systems"}, {"score": 0.0023188652648490874, "phrase": "communicator_data"}, {"score": 0.002283243905993669, "phrase": "automatic_evaluation_measures"}, {"score": 0.0022394839933089074, "phrase": "average_communicator_system_policy"}, {"score": 0.0021965609224818853, "phrase": "proposed_method"}, {"score": 0.0021378442381693847, "phrase": "automatic_optimization"}, {"score": 0.0021049977753042253, "phrase": "limited_initial_data_sets"}], "paper_keywords": [""], "paper_abstract": "We propose a method for learning dialogue management policies from a fixed data set. The method addresses the challenges posed by Information State Update (ISU)-based dialogue systems, which represent the state of a dialogue as a large set of features, resulting in a very large state space and a huge policy space. To address the problem that any fixed data set will only provide information about small portions of these state and policy spaces, we propose a hybrid model that combines reinforcement learning with supervised learning. The reinforcement learning is used to optimize a measure of dialogue reward, while the supervised learning is used to restrict the learned policy to the portions of these spaces for which we have data. We also use linear function approximation to address the need to generalize from a fixed amount of data to large state spaces. To demonstrate the effectiveness of this method on this challenging task, we trained this model on the COMMUNICATOR corpus, to which we have added annotations for user actions and Information States. When tested with a user simulation trained on a different part of the same data set, our hybrid model outperforms a pure supervised learning model and a pure reinforcement learning model. It also outperforms the hand-crafted systems on the COMMUNICATOR data, according to automatic evaluation measures, improving over the average COMMUNICATOR system policy by 10%. The proposed method will improve techniques for bootstrapping and automatic optimization of dialogue management policies from limited initial data sets.", "paper_title": "Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets", "paper_id": "WOS:000261404600002"}