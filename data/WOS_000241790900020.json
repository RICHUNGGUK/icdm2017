{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "state_similarity"}, {"score": 0.004722254884093624, "phrase": "reinforcement_learning"}, {"score": 0.004454691230204595, "phrase": "novel_approach"}, {"score": 0.004243289764306128, "phrase": "similar_sub-policies"}, {"score": 0.003964007795174772, "phrase": "reinforcement_learning_framework"}, {"score": 0.0038876302202924644, "phrase": "better_learning_performance"}, {"score": 0.003631671435460452, "phrase": "common_action_sequences"}, {"score": 0.003359647377874098, "phrase": "possible_optimal_policies"}, {"score": 0.003200045185608395, "phrase": "tree_structure"}, {"score": 0.0028471655497661528, "phrase": "similarity_function"}, {"score": 0.0025578967205807843, "phrase": "action-value_function"}, {"score": 0.0024126686412257407, "phrase": "similar_states"}, {"score": 0.0021049977753042253, "phrase": "broader_context"}], "paper_keywords": [""], "paper_abstract": "This paper presents a novel approach that locates states with similar sub-policies, and incorporates them into the reinforcement learning framework for better learning performance. This is achieved by identifying common action sequences of states, which are derived from possible optimal policies and reflected into a tree structure. Based on the number of such sequences, we define a similarity function between two states, which helps to reflect updates on the action-value function of a state to all similar states. This way, experience acquired during learning can be applied to a broader context. The effectiveness of the method is demonstrated empirically.", "paper_title": "Effectiveness of considering state similarity for reinforcement learning", "paper_id": "WOS:000241790900020"}