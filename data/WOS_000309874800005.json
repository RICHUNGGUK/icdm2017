{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "feature_selection"}, {"score": 0.004762202897268383, "phrase": "mixture_modeling"}, {"score": 0.0045820747375409435, "phrase": "machine_learning"}, {"score": 0.0045318666714939905, "phrase": "data_mining_applications"}, {"score": 0.004457579615095599, "phrase": "important_challenge"}, {"score": 0.004384504920139083, "phrase": "finite_mixture_models"}, {"score": 0.0040366412779632085, "phrase": "recent_developments"}, {"score": 0.003757480190200645, "phrase": "non-parametric_bayesian_techniques"}, {"score": 0.0036352070472201086, "phrase": "important_crucial_preprocessing_step"}, {"score": 0.0034024277222012597, "phrase": "main_approach"}, {"score": 0.003097865379563127, "phrase": "generalized_dirichlet_mixture_model"}, {"score": 0.003030247482854539, "phrase": "-parametric_bayesian_estimation"}, {"score": 0.002883439044844259, "phrase": "finite_generalized_dirichlet_mixture_models"}, {"score": 0.0026986685585382347, "phrase": "relevant_features"}, {"score": 0.002497976173193543, "phrase": "natural_representation"}, {"score": 0.0024299678474208023, "phrase": "challenging_problem"}, {"score": 0.00240328454980006, "phrase": "model_selection"}, {"score": 0.002337848196418384, "phrase": "markov_chain_monte_carlo_algorithm"}, {"score": 0.002286781334850213, "phrase": "resulted_infinite_mixture"}, {"score": 0.002200077887697072, "phrase": "image_categorization"}, {"score": 0.0021401625519713577, "phrase": "infinite_mixture_models"}], "paper_keywords": ["Non-parametric Bayesian methods", " Dirichlet process", " Clustering", " Feature selection", " Mixture models", " Generalized Dirichlet", " MCMC", " Categorization"], "paper_abstract": "Mixture modeling is one of the most useful tools in machine learning and data mining applications. An important challenge when applying finite mixture models is the selection of the number of clusters which best describes the data. Recent developments have shown that this problem can be handled by the application of non-parametric Bayesian techniques to mixture modeling. Another important crucial preprocessing step to mixture learning is the selection of the most relevant features. The main approach in this paper, to tackle these problems, consists on storing the knowledge in a generalized Dirichlet mixture model by applying non-parametric Bayesian estimation and inference techniques. Specifically, we extend finite generalized Dirichlet mixture models to the infinite case in which the number of components and relevant features do not need to be known a priori. This extension provides a natural representation of uncertainty regarding the challenging problem of model selection. We propose a Markov Chain Monte Carlo algorithm to learn the resulted infinite mixture. Through applications involving text and image categorization, we show that infinite mixture models offer a more powerful and robust performance than classic finite mixtures for both clustering and feature selection.", "paper_title": "A countably infinite mixture model for clustering and feature selection", "paper_id": "WOS:000309874800005"}