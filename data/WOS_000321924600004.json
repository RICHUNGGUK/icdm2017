{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "facial_activities"}, {"score": 0.004670597652149685, "phrase": "great_attention"}, {"score": 0.004630150894006861, "phrase": "computer_vision_field"}, {"score": 0.004394688617163029, "phrase": "bottom_level"}, {"score": 0.004262881313926916, "phrase": "facial_component"}, {"score": 0.004028452405970376, "phrase": "detailed_face_shape_information"}, {"score": 0.003924629168434799, "phrase": "middle_level"}, {"score": 0.0038906171474520756, "phrase": "facial_action_units"}, {"score": 0.003806866174028117, "phrase": "facial_action_coding_system"}, {"score": 0.0036926236934657864, "phrase": "specific_set"}, {"score": 0.003660614912817745, "phrase": "facial_muscles"}, {"score": 0.0034143003168921114, "phrase": "top_level"}, {"score": 0.003384695931421288, "phrase": "six_prototypical_facial_expressions"}, {"score": 0.003340768341189764, "phrase": "global_facial_muscle_movement"}, {"score": 0.0032263862712733934, "phrase": "human_emotion_states"}, {"score": 0.0031431690368229443, "phrase": "mainstream_approaches"}, {"score": 0.002818850283787242, "phrase": "unified_probabilistic_framework"}, {"score": 0.002770150086489194, "phrase": "dynamic_bayesian_network"}, {"score": 0.002686935113435742, "phrase": "facial_evolvement"}, {"score": 0.002663620639539737, "phrase": "different_levels"}, {"score": 0.002594880866651459, "phrase": "advanced_machine_learning_methods"}, {"score": 0.0024842242921857705, "phrase": "training_data"}, {"score": 0.002462664392109308, "phrase": "subjective_prior_knowledge"}, {"score": 0.0023679315987170857, "phrase": "facial_motions"}, {"score": 0.002257070536352418, "phrase": "probabilistic_inference"}, {"score": 0.002237477619351199, "phrase": "extensive_experiments"}, {"score": 0.002132711053003407, "phrase": "proposed_model"}], "paper_keywords": ["Bayesian network", " expression recognition", " facial action unit recognition", " facial feature tracking", " simultaneous tracking and recognition"], "paper_abstract": "The tracking and recognition of facial activities from images or videos have attracted great attention in computer vision field. Facial activities are characterized by three levels. First, in the bottom level, facial feature points around each facial component, i.e., eyebrow, mouth, etc., capture the detailed face shape information. Second, in the middle level, facial action units, defined in the facial action coding system, represent the contraction of a specific set of facial muscles, i.e., lid tightener, eyebrow raiser, etc. Finally, in the top level, six prototypical facial expressions represent the global facial muscle movement and are commonly used to describe the human emotion states. In contrast to the mainstream approaches, which usually only focus on one or two levels of facial activities, and track (or recognize) them separately, this paper introduces a unified probabilistic framework based on the dynamic Bayesian network to simultaneously and coherently represent the facial evolvement in different levels, their interactions and their observations. Advanced machine learning methods are introduced to learn the model based on both training data and subjective prior knowledge. Given the model and the measurements of facial motions, all three levels of facial activities are simultaneously recognized through a probabilistic inference. Extensive experiments are performed to illustrate the feasibility and effectiveness of the proposed model on all three level facial activities.", "paper_title": "Simultaneous Facial Feature Tracking and Facial Expression Recognition", "paper_id": "WOS:000321924600004"}