{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "ensemble_kalman_filter"}, {"score": 0.04820613313024096, "phrase": "online_learning"}, {"score": 0.043453500997964904, "phrase": "computational_complexity"}, {"score": 0.0046797114400479135, "phrase": "main_drawbacks"}, {"score": 0.004574247691703376, "phrase": "recurrent_neural_networks"}, {"score": 0.004420472010542188, "phrase": "high_computational_cost"}, {"score": 0.004104722208522451, "phrase": "online_learning_algorithms"}, {"score": 0.003966667994579941, "phrase": "real_time"}, {"score": 0.003789763907758991, "phrase": "significant_reductions"}, {"score": 0.0037042817107563785, "phrase": "rtrl"}, {"score": 0.003459191419478298, "phrase": "model_performance"}, {"score": 0.0033619033669956317, "phrase": "different_approach"}, {"score": 0.00332375606483146, "phrase": "complexity_reduction"}, {"score": 0.003193606324727067, "phrase": "sequential_bayesian_filtering_framework"}, {"score": 0.0030337085953063125, "phrase": "derivative_free_parameter_estimation"}, {"score": 0.0029822214416722124, "phrase": "enkf"}, {"score": 0.002931569771634119, "phrase": "online_training_solution"}, {"score": 0.002706383680762465, "phrase": "original_rtrl_algorithm"}, {"score": 0.0026452734009203764, "phrase": "modeling_potential"}, {"score": 0.0025127604406208668, "phrase": "observed_data"}, {"score": 0.0024842242921857705, "phrase": "nonlinear_systems"}, {"score": 0.0022802596433757565, "phrase": "real_computational_time"}, {"score": 0.0021784009230363627, "phrase": "better_forecasts"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Recurrent neural networks", " Ensemble Kalman filter"], "paper_abstract": "One of the main drawbacks for online learning of recurrent neural networks (RNNs) is the high computational cost of training. Much effort has been spent to reduce the computational complexity of online learning algorithms, usually focusing on the real time recurrent learning (RTRL) algorithm. Significant reductions in complexity of RTRL have been achieved, but with a tradeoff, degradation of model performance. We take a different approach to complexity reduction in online learning of RNNs through a sequential Bayesian filtering framework and propose the ensemble Kalman filter (EnKF) for derivative free parameter estimation. The EnKF provides an online training solution that under certain assumptions can reduce the computational complexity by two orders of magnitude from the original RTRL algorithm without sacrificing the modeling potential of the network. Through forecasting experiments on observed data from nonlinear systems, it is shown that the EnKF trained RNN outperforms other RNN training algorithms in terms of real computational time and also leads to models that produce better forecasts. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Efficient online recurrent connectionist learning with the ensemble Kalman filter", "paper_id": "WOS:000275643000058"}