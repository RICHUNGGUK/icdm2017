{"auto_keywords": [{"score": 0.04974633013174414, "phrase": "log_files"}, {"score": 0.011882786287243614, "phrase": "industrial_world"}, {"score": 0.00800212456913137, "phrase": "complex_logical_units"}, {"score": 0.00481495049065317, "phrase": "logical_units"}, {"score": 0.004682525167261325, "phrase": "new_technologies"}, {"score": 0.004443921948822114, "phrase": "decision_maker"}, {"score": 0.004367108655796158, "phrase": "probably_best_known_example"}, {"score": 0.004321656690200878, "phrase": "web_log_file_analysis"}, {"score": 0.0042617859335771615, "phrase": "efficient_tools"}, {"score": 0.0038787040860731935, "phrase": "web_site"}, {"score": 0.0037719307016238998, "phrase": "well-formed_structures"}, {"score": 0.003505378950567746, "phrase": "data_blocks"}, {"score": 0.0031239353051539347, "phrase": "main_consequences"}, {"score": 0.002862767868795934, "phrase": "new_tools"}, {"score": 0.0027838831266525773, "phrase": "appropriate_part"}, {"score": 0.0027452574218375593, "phrase": "passage_retrieval_methods"}, {"score": 0.0026602925366283952, "phrase": "relevant_parts"}, {"score": 0.00256895975568914, "phrase": "new_approach"}, {"score": 0.002506894277338543, "phrase": "relevant_segments"}, {"score": 0.0023214036848949876, "phrase": "generalized_vs-grams"}, {"score": 0.0022574026832458344, "phrase": "syntactic_characteristics"}, {"score": 0.002241679436497571, "phrase": "special_structures"}, {"score": 0.002202847728425357, "phrase": "conducted_experiments"}, {"score": 0.002172266188075892, "phrase": "real_datasets"}], "paper_keywords": ["Log files", " text segmentation", " logical units", " generalized vs-grams"], "paper_abstract": "With the development of new technologies more and more information is stored in log files. Analyzing such logs can be very useful for the decision maker. One of the probably best known example is the Web log file analysis where lots of efficient tools have been proposed to extract the top-k accessed pages, the best users or even the patterns describing the behaviors of users on a Web site. These tools take advantages of the well-formed structures of the data. Unfortunately, logs files from the industrial world have very heterogeneous complex structures (e.g., tables, lists, data blocks). For experts, analyzing logs to find messages helping to better understand causes of a failure, if a problem have already occurred in the past or even knowing the main consequences of a failure is a hard, tedious, time-consuming and error-prone task. There is thus a need for new tools helping the experts to easily recognize the appropriate part in logs. Passage retrieval methods have proved to be very useful for extracting relevant parts in documents. In this paper we propose a new approach for automatically split logs files into relevant segments based on their logical units. We characterize the complex logical units found in logs according to their syntactic characteristics. We also introduce the notion of generalized vs-grams which is used to automatically extract the syntactic characteristics of special structures found in log files. Conducted experiments are performed on real datasets from the industrial world to demonstrate the efficiency of our proposal on the recognition of complex logical units.", "paper_title": "Recognition of logical units in log files", "paper_id": "WOS:000353062400012"}