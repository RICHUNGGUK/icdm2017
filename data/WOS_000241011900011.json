{"auto_keywords": [{"score": 0.0453665111651286, "phrase": "hidden_layers"}, {"score": 0.042051296116313676, "phrase": "output_layer"}, {"score": 0.00481495049065317, "phrase": "synaptic_faults"}, {"score": 0.004756805752518494, "phrase": "low-power_analog_hardware"}, {"score": 0.00453113241842654, "phrase": "training_method"}, {"score": 0.004449280529563265, "phrase": "convolutional_neural_network"}, {"score": 0.004395532072377065, "phrase": "threshold_neurons"}, {"score": 0.0041112665381266315, "phrase": "feed-forward_manner"}, {"score": 0.0038923605771735838, "phrase": "supervised_perceptron_rule"}, {"score": 0.0036627245728600073, "phrase": "existing_low-power_analog_hardware_architecture"}, {"score": 0.0034887749053459584, "phrase": "computation_accuracy"}, {"score": 0.0034465894339710864, "phrase": "unspecified_ways"}, {"score": 0.0031845067058084613, "phrase": "possible_errors"}, {"score": 0.0029066973917258655, "phrase": "on-chip_approach"}, {"score": 0.0027685531038981847, "phrase": "present_work"}, {"score": 0.0025735173163957993, "phrase": "iterative_perceptron_rule"}, {"score": 0.002183347623351868, "phrase": "network_layers"}, {"score": 0.0021307975800397816, "phrase": "mnist_database"}, {"score": 0.0021049977753042253, "phrase": "hand-written_digits"}], "paper_keywords": [""], "paper_abstract": "Recently, the authors described a training method for a convolutional neural network of threshold neurons. Hidden layers are trained by by clustering, in a feed-forward manner, while the output layer is trained using the supervised Perceptron rule. The system is designed for implementation on an existing low-power analog hardware architecture, exhibiting inherent error sources affecting the computation accuracy in unspecified ways. One key technique is to train the network on-chip, taking possible errors into account without any need to quantify them. For the hidden layers, an on-chip approach has been applied previously. In the present work, a chip-in-the-loop version of the iterative Perceptron rule is introduced for training the output layer. Influences of various types of errors are thoroughly investigated (noisy, deleted, and clamped weights) for all network layers, using the MNIST database of hand-written digits as a benchmark.", "paper_title": "A convolutional neural network tolerant of synaptic faults for low-power analog hardware", "paper_id": "WOS:000241011900011"}