{"auto_keywords": [{"score": 0.046459681772617736, "phrase": "eigenclassifiers_method"}, {"score": 0.04223218903917887, "phrase": "base_classifier_outputs"}, {"score": 0.00481495049065317, "phrase": "fusion_model_selection"}, {"score": 0.0047152877145889656, "phrase": "base_classifiers"}, {"score": 0.004593591780245872, "phrase": "key_issues"}, {"score": 0.004545793387330536, "phrase": "classifier_combination"}, {"score": 0.004382364826200616, "phrase": "ulas_et_al"}, {"score": 0.004202741104973709, "phrase": "uncorrelated_base_classifier_outputs"}, {"score": 0.004115697805448384, "phrase": "multiclass_classification_problems"}, {"score": 0.003926359151817743, "phrase": "redundant_features"}, {"score": 0.00386519448487341, "phrase": "transformed_classifier_output_space"}, {"score": 0.003785115566529808, "phrase": "higher_estimator_variance"}, {"score": 0.0037456980766939836, "phrase": "lower_prediction_accuracy"}, {"score": 0.0035361001070845677, "phrase": "truly_uncorrelated_base_classifiers"}, {"score": 0.0032012284065895537, "phrase": "class_imbalance_problem"}, {"score": 0.002678944072162297, "phrase": "ten_different_fusion_methods"}, {"score": 0.0025960262223338293, "phrase": "accuracy_diversity_relationship"}, {"score": 0.0025288869172313674, "phrase": "experimental_dataset"}, {"score": 0.0024894376091589244, "phrase": "eigenvalue_distributions"}, {"score": 0.0024634797027967203, "phrase": "divergence_metrics"}, {"score": 0.0024250482323322606, "phrase": "kuncheva"}, {"score": 0.002399761899193403, "phrase": "whitaker"}, {"score": 0.0023133055945761235, "phrase": "basic_rules"}, {"score": 0.002206700502606511, "phrase": "fusion_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Classifier combination", " Classifier fusion", " Dropout", " Eigenclassifiers", " Kernelized Eigenclassifiers"], "paper_abstract": "Diversity among base classifiers is one of the key issues in classifier combination. Although the Eigenclassifiers method proposed by Ulas et al. (2012) aims to create uncorrelated base classifier outputs, however for multiclass classification problems, correlation among base classifier outputs arise due to the redundant features in the transformed classifier output space, which causes higher estimator variance and lower prediction accuracy. In this paper, we extend Eigenclassifiers method to obtain truly uncorrelated base classifiers. We also generalize the distribution on base classifier outputs from unimodal to multimodal, which lets us handle the class imbalance problem. We also aim to answer the question of which classifier fusion method should be used for a given dataset. In order to answer this question, we generate a dataset by calculating the performances of ten different fusion methods on 38 different datasets. We investigate accuracy diversity relationship of ensembles on this experimental dataset by using eigenvalue distributions and divergence metrics defined by Kuncheva and Whitaker (2001). We obtain basic rules which can be used to decide on a fusion method given a dataset. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Extended multimodal Eigenclassifiers and criteria for fusion model selection", "paper_id": "WOS:000349590000004"}