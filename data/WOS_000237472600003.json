{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cognitive_learning"}, {"score": 0.004718897469501381, "phrase": "algorithmic_standpoint"}, {"score": 0.004382737590042589, "phrase": "small_number"}, {"score": 0.00407042677958844, "phrase": "huge_amount"}, {"score": 0.0038833278847237858, "phrase": "novel_algorithmic_analysis"}, {"score": 0.0037549546301384336, "phrase": "robust_concept_learning"}, {"score": 0.0036064757492752703, "phrase": "margin_classifiers"}, {"score": 0.003417565253539315, "phrase": "relatively_small_number"}, {"score": 0.003260377175139591, "phrase": "rich_concept_classes"}, {"score": 0.003195236629209403, "phrase": "new_algorithms"}, {"score": 0.002907992115184249, "phrase": "low_levels"}, {"score": 0.002755567647101251, "phrase": "robust_half-space"}, {"score": 0.002664375650349975, "phrase": "linear_time"}, {"score": 0.0021049977753042253, "phrase": "psychological_studies"}], "paper_keywords": ["learning", " cognition", " random projection", " robust concepts"], "paper_abstract": "We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel algorithmic analysis via a model of robust concept learning (closely related to \"margin classifiers\"), and show that a relatively small number of examples are sufficient to learn rich concept classes. The new algorithms have several advantages-they are faster, conceptually simpler, and resistant to low levels of noise. For example, a robust half-space can be learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that \"more robust concepts are easier to learn\", is supported by a multitude of psychological studies.", "paper_title": "An algorithmic theory of learning: Robust concepts and random projection", "paper_id": "WOS:000237472600003"}