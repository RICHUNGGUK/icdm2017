{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "visual_entities"}, {"score": 0.009245000584773788, "phrase": "browse-to-search_system"}, {"score": 0.0047552046776774565, "phrase": "image_search_technology"}, {"score": 0.004618651617663879, "phrase": "textual_descriptions"}, {"score": 0.0043390411584632695, "phrase": "rich_or_complex_content"}, {"score": 0.0041794326814696145, "phrase": "general_search_interests"}, {"score": 0.004110385316390559, "phrase": "specific_knowledge"}, {"score": 0.004059346366551609, "phrase": "new_form"}, {"score": 0.003983969024275096, "phrase": "exploratory_search"}, {"score": 0.003934493593074662, "phrase": "research_community"}, {"score": 0.0038214180282841272, "phrase": "additional_knowledge"}, {"score": 0.0037976119476908245, "phrase": "accurate_queries"}, {"score": 0.0036654749116543637, "phrase": "integrated_exploratory_search_solutions"}, {"score": 0.003604887777269133, "phrase": "exploring_loop"}, {"score": 0.0035232064948163455, "phrase": "understanding_users'_search_interests"}, {"score": 0.0034290486992368014, "phrase": "novel_system"}, {"score": 0.00340058474139766, "phrase": "effective_and_efficient_way"}, {"score": 0.0032890643902085863, "phrase": "visual-based_exploratory_search_tasks"}, {"score": 0.0032481905782548097, "phrase": "browse-to-search"}, {"score": 0.0031878272443845793, "phrase": "visual_objects"}, {"score": 0.003076841995933412, "phrase": "users'_underlying_intent"}, {"score": 0.0030259549056122184, "phrase": "original_image_content"}, {"score": 0.0029821174948442583, "phrase": "textual-based_browsing_context"}, {"score": 0.002963524404894801, "phrase": "associated_heterogeneous_attributes"}, {"score": 0.0029389132903654093, "phrase": "large-scale_image_search_technology"}, {"score": 0.002914505965747, "phrase": "associated_textual_attributes"}, {"score": 0.0028543682595018587, "phrase": "encapsulated_visual_entities"}, {"score": 0.00283656940227644, "phrase": "search_tasks"}, {"score": 0.0027896449982305533, "phrase": "first_attempts"}, {"score": 0.002737779722707355, "phrase": "visual-based_exploratory_search"}, {"score": 0.002647937953458732, "phrase": "browsing_session"}, {"score": 0.0026369164899054305, "phrase": "search_results"}, {"score": 0.0025397597214124046, "phrase": "text-based_contextual_cues"}, {"score": 0.0024718193736802136, "phrase": "visual_content"}, {"score": 0.0024258487599384494, "phrase": "query_formulation"}, {"score": 0.002312206270838452, "phrase": "natural_user_experience"}, {"score": 0.0022550391854274745, "phrase": "system_performance"}, {"score": 0.0021764583511977333, "phrase": "user's_exploratory_search"}, {"score": 0.0021583694166902386, "phrase": "conventional_image_search_methods"}], "paper_keywords": ["Design", " Algorithms", " Performance", " Multimedia information systems", " interactive visual search", " exploratory search", " user interaction", " multimedia browsing", " gesture"], "paper_abstract": "With the development of image search technology, users are no longer satisfied with searching for images using just metadata and textual descriptions. Instead, more search demands are focused on retrieving images based on similarities in their contents (textures, colors, shapes etc.). Nevertheless, one image may deliver rich or complex content and multiple interests. Sometimes users do not sufficiently define or describe their seeking demands for images even when general search interests appear, owing to a lack of specific knowledge to express their intents. A new form of information seeking activity, referred to as exploratory search, is emerging in the research community, which generally combines browsing and searching content together to help users gain additional knowledge and form accurate queries, thereby assisting the users with their seeking and investigation activities. However, there have been few attempts at addressing integrated exploratory search solutions when image browsing is incorporated into the exploring loop. In this work, we investigate the challenges of understanding users' search interests from the images being browsed and infer their actual search intentions. We develop a novel system to explore an effective and efficient way for allowing users to seamlessly switch between browse and search processes, and naturally complete visual-based exploratory search tasks. The system, called Browse-to-Search enables users to specify their visual search interests by circling any visual objects in the webpages being browsed, and then the system automatically forms the visual entities to represent users' underlying intent. One visual entity is not limited by the original image content, but also encapsulated by the textual-based browsing context and the associated heterogeneous attributes. We use large-scale image search technology to find the associated textual attributes from the repository. Users can then utilize the encapsulated visual entities to complete search tasks. The Browse-to-Search system is one of the first attempts to integrate browse and search activities for a visual-based exploratory search, which is characterized by four unique properties: (1) in session-searching is performed during browsing session and search results naturally accompany with browsing content; (2) in context-the pages being browsed provide text-based contextual cues for searching; (3) in focus-users can focus on the visual content of interest without worrying about the difficulties of query formulation, and visual entities will be automatically formed; and (4) intuitiveness-a touch and visual search-based user interface provides a natural user experience. We deploy the Browse-to-Search system on tablet devices and evaluate the system performance using millions of images. We demonstrate that it is effective and efficient in facilitating the user's exploratory search compared to the conventional image search methods and, more importantly, provides users with more robust results to satisfy their exploring experience.", "paper_title": "Browse-to-Search: Interactive Exploratory Search with Visual Entities", "paper_id": "WOS:000344623800003"}