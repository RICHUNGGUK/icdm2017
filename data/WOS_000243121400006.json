{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "heterogeneous_perceptual_capabilities"}, {"score": 0.004761968711051715, "phrase": "real_world_applications_robots"}, {"score": 0.0045895181082906715, "phrase": "higher_level_cognitive_functions"}, {"score": 0.004374593906165761, "phrase": "incompletely_known_and_unpredictable_environments"}, {"score": 0.004294625285753825, "phrase": "major_tasks"}, {"score": 0.0040783736687590635, "phrase": "information_fusion"}, {"score": 0.003974349782102708, "phrase": "low_level_sensor_signals"}, {"score": 0.0039016690475620185, "phrase": "high_level"}, {"score": 0.003830312347175889, "phrase": "dynamically_changing_erivironment"}, {"score": 0.003816197777388722, "phrase": "even_a_single_agent"}, {"score": 0.003774164192882704, "phrase": "varying_abilities"}, {"score": 0.003664315990489347, "phrase": "particular_conditions"}, {"score": 0.0035445401296650535, "phrase": "different_agents"}, {"score": 0.0035184576341236317, "phrase": "different_perceptual_capabilities"}, {"score": 0.003196292402522254, "phrase": "low_and_high_level"}, {"score": 0.003035177667751495, "phrase": "heterogeneous_and_contextually_limited_perceptual_capabilities"}, {"score": 0.0029576827451501956, "phrase": "agent's_perceptual_capabilities"}, {"score": 0.0028928306994090453, "phrase": "partial_tolerance_spaces"}, {"score": 0.0027368368719283298, "phrase": "lower_and_upper_approximations"}, {"score": 0.0026966744704447275, "phrase": "approximate_relations"}, {"score": 0.0026570998719084153, "phrase": "rough_sets"}, {"score": 0.002598821419664902, "phrase": "sensory_and_other_limitations"}, {"score": 0.0025045102029373854, "phrase": "approximate_databases"}, {"score": 0.0024768883044869023, "phrase": "respective_agent"}, {"score": 0.0024586427394152196, "phrase": "complex_relations"}, {"score": 0.0024136132575631273, "phrase": "primitive_relations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["rough sets", " database and sensor fusion", " approximate reasoning", " intelligent agents", " cognitive robotics", " software agents"], "paper_abstract": "In real world applications robots and software agents often have to be equipped with higher level cognitive functions that enable them to reason, act and perceive in changing, incompletely known and unpredictable environments. One of the major tasks in such circumstances is to fuse information from various data sources. There are many levels of information fusion, ranging from the fusing of low level sensor signals to the fusing of high level, complex knowledge structures. In a dynamically changing erivironment even a single agent may have varying abilities to perceive its environment which are dependent on particular conditions. The situation becomes even more complex when different agents have different perceptual capabilities and need to communicate with each other. In this paper, we propose a framework that provides agents with the ability to fuse both low and high level approximate knowledge in the context of dynamically changing environments while taking account of heterogeneous and contextually limited perceptual capabilities. To model limitations on an agent's perceptual capabilities we introduce the idea of partial tolerance spaces. We assume that each agent has one or more approximate databases where approximate relations are represented using lower and upper approximations on sets. Approximate relations are generalizations of rough sets. It is shown how sensory and other limitations can be taken into account when constructing and querying approximate databases for each respective agent. Complex relations inherit the approximativeness of primitive relations used in their definitions. Agents then query these databases and receive answers through the filters of their perceptual limitations as represented by (partial) tolerance spaces and approximate queries. The techniques used are all tractable. (C) 2005 Elsevier B.V. All rights reserved.", "paper_title": "Communication between agents with heterogeneous perceptual capabilities", "paper_id": "WOS:000243121400006"}