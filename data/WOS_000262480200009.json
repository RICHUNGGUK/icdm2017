{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multiple_occluding"}, {"score": 0.004710306419574744, "phrase": "multiple_scene_planes"}, {"score": 0.004547564208690591, "phrase": "crowded_and_cluttered_scenes"}, {"score": 0.004429191773525288, "phrase": "individual_people"}, {"score": 0.00427612041204006, "phrase": "single_view"}, {"score": 0.004183137595671171, "phrase": "multiview_approach"}, {"score": 0.0038818006513785977, "phrase": "single_camera"}, {"score": 0.003847802030433298, "phrase": "camera_pair"}, {"score": 0.0036179751816682454, "phrase": "synergistic_framework"}, {"score": 0.0033277936960179892, "phrase": "fully_calibrated_views"}, {"score": 0.0030608148631528767, "phrase": "planar_homographic_occupancy_constraint"}, {"score": 0.0030206579994328975, "phrase": "foreground_likelihood_information"}, {"score": 0.002994178996584381, "phrase": "multiple_views"}, {"score": 0.0028778584739756786, "phrase": "reference_scene_plane"}, {"score": 0.00284009509859163, "phrase": "greater_robustness"}, {"score": 0.002753891109273246, "phrase": "multiple_planes"}, {"score": 0.002705808393126695, "phrase": "reference_plane"}, {"score": 0.002623669616798891, "phrase": "plane_homologies"}, {"score": 0.0025217065652145443, "phrase": "schmieder"}, {"score": 0.002499590390751339, "phrase": "weathersby_clutter_measure"}, {"score": 0.002381365139274329, "phrase": "higher_fusion_weight"}, {"score": 0.002339771435910601, "phrase": "lesser_clutter"}, {"score": 0.002248816575548875, "phrase": "graph_cuts"}, {"score": 0.0021901485584955487, "phrase": "space-time_occupancy_likelihood_data"}, {"score": 0.002142426911242168, "phrase": "detailed_qualitative_and_quantitative_analysis"}, {"score": 0.0021049977753042253, "phrase": "challenging_multiview_crowded_scenes"}], "paper_keywords": ["Tracking", " sensor fusion", " graph-theoretic methods"], "paper_abstract": "Occlusion and lack of visibility in crowded and cluttered scenes make it difficult to track individual people correctly and consistently, particularly in a single view. We present a multiview approach to solve this problem. In our approach, we neither detect nor track objects from any single camera or camera pair; rather, evidence is gathered from all of the cameras into a synergistic framework and detection and tracking results are propagated back to each view. Unlike other multiview approaches that require fully calibrated views, our approach is purely image-based and uses only 2D constructs. To this end, we develop a planar homographic occupancy constraint that fuses foreground likelihood information from multiple views to resolve occlusions and localize people on a reference scene plane. For greater robustness, this process is extended to multiple planes parallel to the reference plane in the framework of plane to plane homologies. Our fusion methodology also models scene clutter using the Schmieder and Weathersby clutter measure, which acts as a confidence prior, to assign higher fusion weight to views with lesser clutter. Detection and tracking are performed simultaneously by graph cuts segmentation of tracks in the space-time occupancy likelihood data. Experimental results with detailed qualitative and quantitative analysis are demonstrated in challenging multiview crowded scenes.", "paper_title": "Tracking Multiple Occluding People by Localizing on Multiple Scene Planes", "paper_id": "WOS:000262480200009"}