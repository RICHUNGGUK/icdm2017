{"auto_keywords": [{"score": 0.049226991195811115, "phrase": "prior_distribution"}, {"score": 0.04838919478929019, "phrase": "bayes_network"}, {"score": 0.00481495049065317, "phrase": "kernel_density_estimation"}, {"score": 0.004521566043596533, "phrase": "key_problem"}, {"score": 0.003916098498242888, "phrase": "general_naive_bayes"}, {"score": 0.0038119123317396954, "phrase": "continuous_variables"}, {"score": 0.0035794251590584563, "phrase": "kernel_function"}, {"score": 0.0034841647157189985, "phrase": "orthogonal_polynomials"}, {"score": 0.0032423089921154503, "phrase": "density_function"}, {"score": 0.0028076592725295646, "phrase": "kernel_estimation"}, {"score": 0.002282667669149476, "phrase": "sample_size"}, {"score": 0.0021049977753042253, "phrase": "good_convergence_rates"}], "paper_keywords": [""], "paper_abstract": "The key problem of inductive-learning in Bayes network is the estimator of prior distribution. This paper adopts general naive Bayes to handle continuous variables, and proposes a kind of kernel function constructed by orthogonal polynomials, which is used to estimate the density function of prior distribution in Bayes network. The paper then makes further researches into optimality of the kernel estimation of density and derivatives. When the sample is fixed, the estimators can keep continuity and smoothness, and when the sample size tends to infinity, the estimators can keep good convergence rates.", "paper_title": "Optimality of kernel density estimation of prior distribution in Bayes network", "paper_id": "WOS:000241790100074"}