{"auto_keywords": [{"score": 0.040980305034149664, "phrase": "petrov"}, {"score": 0.00481495049065317, "phrase": "spectral_learning_of_latent-variable_pcfgs"}, {"score": 0.004358676214574977, "phrase": "spectral_learning_algorithm"}, {"score": 0.004272724011826495, "phrase": "latent-variable_pcfgs"}, {"score": 0.003264872488939969, "phrase": "statistically_consistent_parameter_estimates"}, {"score": 0.0028966970007550824, "phrase": "tensor_form"}, {"score": 0.0028113140998735366, "phrase": "inside-outside_algorithm"}, {"score": 0.00256993314099902, "phrase": "required_tensors"}, {"score": 0.002420611343093316, "phrase": "training_examples"}, {"score": 0.0023727863021828547, "phrase": "hidden-variable_values"}, {"score": 0.0021907299375981356, "phrase": "pac-style_convergence"}, {"score": 0.0021049977753042253, "phrase": "estimation_method"}], "paper_keywords": ["latent-variable PCFGs", " spectral learning algorithms"], "paper_abstract": "We introduce a spectral learning algorithm for latent-variable PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides statistically consistent parameter estimates. Our result rests on three theorems: the first gives a tensor form of the inside-outside algorithm for PCFGs; the second shows that the required tensors can be estimated directly from training examples where hidden-variable values are missing; the third gives a PAC-style convergence bound for the estimation method.", "paper_title": "Spectral Learning of Latent-Variable PCFGs: Algorithms and Sample Complexity", "paper_id": "WOS:000344638400002"}