{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "task-pipelined_parallelism"}, {"score": 0.004645057834614705, "phrase": "many-core_gpus"}, {"score": 0.004513450434595191, "phrase": "data-level_parallelism"}, {"score": 0.004449045430179066, "phrase": "graphics_processing_units"}, {"score": 0.003909065355459264, "phrase": "stream_processing_pattern"}, {"score": 0.003637822529522622, "phrase": "data_parallelism"}, {"score": 0.0035858673522839407, "phrase": "current_gpus"}, {"score": 0.003361069456060196, "phrase": "insufficient_usage"}, {"score": 0.003313053707792957, "phrase": "computing_resources"}, {"score": 0.0032423089921154503, "phrase": "excessive_off-chip_memory_traffic"}, {"score": 0.003038983069081333, "phrase": "microarchitectural_enhancements"}, {"score": 0.0029740732099993706, "phrase": "task-pipelined_execution"}, {"score": 0.002931569771634119, "phrase": "data-parallel_kernels"}, {"score": 0.00278752137116301, "phrase": "efficient_adaptive_dynamic_scheduling_mechanism"}, {"score": 0.002669683153809812, "phrase": "minor_hardware_overhead"}, {"score": 0.002538470301402269, "phrase": "data_parallelisms"}, {"score": 0.0024842242921857705, "phrase": "unified_manner"}, {"score": 0.002448704249007665, "phrase": "simulation_results"}, {"score": 0.0023791769068748194, "phrase": "cycle-accurate_simulator"}, {"score": 0.0023451553255837317, "phrase": "real-world_applications"}, {"score": 0.0022785613865764923, "phrase": "proposed_gpu_microarchitecture"}, {"score": 0.0022298571016688335, "phrase": "computing_throughput"}, {"score": 0.002135542806459067, "phrase": "overall_accesses"}, {"score": 0.0021049977753042253, "phrase": "off-chip_gpu_memory"}], "paper_keywords": ["GPU", " task-pipeline", " dynamic scheduling", " load balance", " L2 cache"], "paper_abstract": "By exploiting data-level parallelism, Graphics Processing Units (GPUs) have become a high-throughput, general purpose computing platform. Many real-world applications especially those following a stream processing pattern, however, feature interleaved task-pipelined and data parallelism. Current GPUs are ill equipped for such applications due to the insufficient usage of computing resources and/or the excessive off-chip memory traffic. In this paper, we focus on microarchitectural enhancements to enable task-pipelined execution of data-parallel kernels on GPUs. We propose an efficient adaptive dynamic scheduling mechanism and a moderately modified L2 design. With minor hardware overhead, our techniques orchestrate both task-pipeline and data parallelisms in a unified manner. Simulation results derived by a cycle-accurate simulator on real-world applications prove that the proposed GPU microarchitecture improves the computing throughput by 18% and reduces the overall accesses to off-chip GPU memory by 13%.", "paper_title": "Exploiting the Task-Pipelined Parallelism of Stream Programs on Many-Core GPUs", "paper_id": "WOS:000326667900002"}