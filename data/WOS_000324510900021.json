{"auto_keywords": [{"score": 0.03526300604455722, "phrase": "fusion_process"}, {"score": 0.00481495049065317, "phrase": "automatic_audio-visual_fusion"}, {"score": 0.004713656264685915, "phrase": "multimodal_fusion"}, {"score": 0.004670896353428476, "phrase": "complex_topic"}, {"score": 0.004628522534620679, "phrase": "surveillance_applications"}, {"score": 0.004614483128885678, "phrase": "audio-visual_fusion"}, {"score": 0.00453113241842654, "phrase": "complementary_nature"}, {"score": 0.0044089082776901135, "phrase": "correct_conclusion"}, {"score": 0.004382196270563035, "phrase": "multi-sensor_data"}, {"score": 0.004303023184313407, "phrase": "previous_work"}, {"score": 0.004212453011596424, "phrase": "audiovisual_recordings"}, {"score": 0.004186926251483987, "phrase": "unwanted_behavior"}, {"score": 0.004000322474812552, "phrase": "limited_subset"}, {"score": 0.003964007795174772, "phrase": "recorded_data"}, {"score": 0.0037873013080596137, "phrase": "aggression_scores"}, {"score": 0.0036627245728600073, "phrase": "trivial_fusion_algorithms"}, {"score": 0.003618443379266972, "phrase": "multimodal_labels"}, {"score": 0.0035855828944304506, "phrase": "unimodal_labels"}, {"score": 0.0034676182423067307, "phrase": "unimodal_streams"}, {"score": 0.0034152842718313888, "phrase": "intermediate_step"}, {"score": 0.0029783256772952073, "phrase": "general_case"}, {"score": 0.0029512611306191194, "phrase": "entire_database"}, {"score": 0.0028715299874549245, "phrase": "positive_effect"}, {"score": 0.002685558057339722, "phrase": "automatic_prediction"}, {"score": 0.002661146838358236, "phrase": "intermediate_level_variables"}, {"score": 0.002644995764097807, "phrase": "multimodal_aggression"}, {"score": 0.002526938673434697, "phrase": "multiple_classifiers"}, {"score": 0.0025039656549083497, "phrase": "intermediate_level_features"}, {"score": 0.002481200970201421, "phrase": "low_level_features"}, {"score": 0.002436289099444074, "phrase": "multimodal_label"}, {"score": 0.002414138203828421, "phrase": "intermediate_variables"}, {"score": 0.0023632309822037637, "phrase": "probabilistic_graphical_models"}, {"score": 0.0022923586577645143, "phrase": "conditional_random_fields"}, {"score": 0.0022033855800303623, "phrase": "specific_aggression_classes"}, {"score": 0.0021767087933168717, "phrase": "meta-features_yields"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Audio-visual fusion", " Automatic surveillance", " Meta-information", " Context-based fusion"], "paper_abstract": "Multimodal fusion is a complex topic. For surveillance applications audio-visual fusion is very promising given the complementary nature of the two streams. However, drawing the correct conclusion from multi-sensor data is not straightforward. In previous work we have analysed a database with audiovisual recordings of unwanted behavior in trains (Lefter et al., 2012) and focused on a limited subset of the recorded data. We have collected multi- and unimodal assessments by humans, who have given aggression scores on a 3 point scale. We showed that there are no trivial fusion algorithms to predict the multimodal labels from the unimodal labels since part of the information is lost when using the unimodal streams. We proposed an intermediate step to discover the structure in the fusion process. This step is based upon meta-features and we find a set of five which have an impact on the fusion process. In this paper we extend the findings in (Lefter et al., 2012) for the general case using the entire database. We prove that the meta-features have a positive effect on the fusion process in terms of labels. We then compare three fusion methods that encapsulate the meta-features. They are based on automatic prediction of the intermediate level variables and multimodal aggression from state of the art low level acoustic, linguistic and visual features. The first fusion method is based on applying multiple classifiers to predict intermediate level features from the low level features, and to predict the multimodal label from the intermediate variables. The other two approaches are based on probabilistic graphical models, one using (Dynamic) Bayesian Networks and the other one using Conditional Random Fields. We learn that each approach has its strengths and weaknesses in predicting specific aggression classes and using the meta-features yields significant improvements in all cases. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "A comparative study on automatic audio-visual fusion for aggression detection using meta-information", "paper_id": "WOS:000324510900021"}