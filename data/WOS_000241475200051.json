{"auto_keywords": [{"score": 0.03410735276717894, "phrase": "feature_detectors"}, {"score": 0.00481495049065317, "phrase": "bayesian_integrate"}, {"score": 0.004515967518891287, "phrase": "object_categories"}, {"score": 0.0042354707960660706, "phrase": "bayesian_inference"}, {"score": 0.0035949949979695063, "phrase": "specific_locations"}, {"score": 0.003395640013786862, "phrase": "fixation_point"}, {"score": 0.0029024303666003153, "phrase": "large_amount"}, {"score": 0.002861302930564756, "phrase": "training_data"}, {"score": 0.0025892330566358503, "phrase": "intermediate_representation"}, {"score": 0.0021352776064254195, "phrase": "new_category"}], "paper_keywords": [""], "paper_abstract": "In this work we introduce a Bayesian Integrate And Shift (BIAS) model for learning object categories. The model is biologically inspired and uses Bayesian inference to integrate information within and across fixations. In our model, an object is represented as a collection of features arranged at specific locations with respect to the location of the fixation point. Even though the number of feature detectors that we use is large, we show that learning does not require a large amount of training data due to the fact that between an object and features we introduce an intermediate representation, object views, and thus reduce the dependence among the feature detectors. We tested the system on four object categories and demonstrated that it can learn a new category from only a few training examples.", "paper_title": "Learning by integrating information within and across fixations", "paper_id": "WOS:000241475200051"}