{"auto_keywords": [{"score": 0.048201896565263386, "phrase": "autonomous_mobile_robot"}, {"score": 0.03978021035630368, "phrase": "corner_points"}, {"score": 0.00481495049065317, "phrase": "indoor_environments"}, {"score": 0.00448958076531243, "phrase": "unknown_environment"}, {"score": 0.004327981760540394, "phrase": "vision_sensors"}, {"score": 0.004299228562179176, "phrase": "attractive_equipment"}, {"score": 0.003995244786790969, "phrase": "slam"}, {"score": 0.003942299471749811, "phrase": "general_pin-hole_camera_suffer"}, {"score": 0.003749969582830009, "phrase": "feature_map"}, {"score": 0.003663443192307343, "phrase": "narrow_field"}, {"score": 0.0036028592508983402, "phrase": "pin-hole_camera"}, {"score": 0.0035079946851603186, "phrase": "high_speed_camera_motion"}, {"score": 0.003381607100127276, "phrase": "new_slam_method"}, {"score": 0.0033479323773364716, "phrase": "vertical_lines"}, {"score": 0.0033035520810722886, "phrase": "omni-directional_camera_image"}, {"score": 0.0032380786944838204, "phrase": "range_sensor_data"}, {"score": 0.0031845067058084583, "phrase": "large_field"}, {"score": 0.003131818247415968, "phrase": "omni-directional_camera"}, {"score": 0.0028715299874549245, "phrase": "proposed_slam"}, {"score": 0.0026950674053179404, "phrase": "partial_occlusion"}, {"score": 0.002520990102833368, "phrase": "information_panels"}, {"score": 0.002430076037416881, "phrase": "range_sensor"}, {"score": 0.002366017944545902, "phrase": "horizontal_lines"}, {"score": 0.002242911772136627, "phrase": "camera_calibration"}, {"score": 0.0022279798606617356, "phrase": "experimental_work"}, {"score": 0.0021261972801049573, "phrase": "human's_pace"}, {"score": 0.0021049977753042253, "phrase": "real_indoor_environment"}], "paper_keywords": ["imultaneous localization and map-building", " mobile robotics", " omni-directional vision"], "paper_abstract": "An autonomous mobile robot must have the ability to navigate in an unknown environment. The simultaneous localization and map building (SLAM) problem have relation to this autonomous ability. Vision sensors are attractive equipment for an autonomous mobile robot because they are information-rich and rarely have restrictions on various applications. However, many vision based SLAM methods using a general pin-hole camera suffer from variation in illumination and occlusion, because they mostly extract corner points for the feature map. Moreover, due to the narrow field of view of the pin-hole camera, they are not adequate for a high speed camera motion. To solve these problems, this paper presents a new SLAM method which uses vertical lines extracted from an omni-directional camera image and horizontal lines from the range sensor data. Due to the large field of view of the omni-directional camera, features remain in the image for enough time to estimate the pose of the robot and the features more accurately. Furthermore, since the proposed SLAM does not use corner points but the lines as the features, it reduces the effect of illumination and partial occlusion. Moreover, we use not only the lines at corners of wall but also many other vertical lines at doors, columns and the information panels on the wall which cannot be extracted by a range sensor. Finally, since we use the horizontal lines to estimate the positions of the vertical line features, we do not require any camera calibration. Experimental work based on MORIS, our mobile robot test bed, moving at a human's pace in the real indoor environment verifies the efficacy of this approach.", "paper_title": "SLAM in indoor environments using omni-directional vertical and horizontal line features", "paper_id": "WOS:000251655300002"}