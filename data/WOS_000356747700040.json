{"auto_keywords": [{"score": 0.04489709945690732, "phrase": "input_space"}, {"score": 0.03555325709627262, "phrase": "binary_masks"}, {"score": 0.00481495049065317, "phrase": "selective_regularization"}, {"score": 0.004771196027464407, "phrase": "restricted_boltzmann_machines"}, {"score": 0.0046848707075530256, "phrase": "present_work"}, {"score": 0.004455409162867219, "phrase": "rbm's_learning_capabilities"}, {"score": 0.0035133644847622383, "phrase": "mutual_information"}, {"score": 0.003465521267039616, "phrase": "input_representation_space"}, {"score": 0.0030491497463386924, "phrase": "standard_layer-by-layer_unsupervised_learning"}, {"score": 0.0029128614098881253, "phrase": "selective_application"}, {"score": 0.0026704265616705023, "phrase": "generalization_capabilities"}, {"score": 0.002610043728801011, "phrase": "interesting_capability"}, {"score": 0.0024481198073210567, "phrase": "learning_phase"}, {"score": 0.0023818288346547692, "phrase": "best_set"}, {"score": 0.0023173287402646577, "phrase": "current_weights_configuration"}, {"score": 0.002254571365634257, "phrase": "new_ideas"}, {"score": 0.002163600705898299, "phrase": "different_well-known_corpus"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["Restricted Boltzmann machine", " Deep belief networks", " Regularization"], "paper_abstract": "In the present work, we propose to deal with two important issues regarding to the RBM's learning capabilities. First, the topology of the input space, and second, the sparseness of the RBM obtained. One problem of RBMs is that they do not take advantage of the topology of the input space. In order to alleviate this lack, we propose to use a surrogate of the mutual information of the input representation space to build a set of binary masks. This approach is general and not only applicable to images, thus it can be extended to other layers in the standard layer-by-layer unsupervised learning. On the other hand, we propose a selective application of two different regularization terms, L-1 and L-2, in order to ensure the sparseness of the representation and the generalization capabilities. Additionally, another interesting capability of our approach is the adaptation of the topology of the network during the learning phase by means of selecting the best set of binary masks that fit the current weights configuration. The performance of these new ideas is assessed with a set of experiments on different well-known corpus. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Mask selective regularization for restricted Boltzmann machines", "paper_id": "WOS:000356747700040"}