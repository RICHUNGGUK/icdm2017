{"auto_keywords": [{"score": 0.0474680407483571, "phrase": "approximate_nearest_neighbor_search"}, {"score": 0.04359758181831713, "phrase": "data_points"}, {"score": 0.03775114821059378, "phrase": "data_point"}, {"score": 0.03009738409236426, "phrase": "multiple_sub_codewords"}, {"score": 0.00481495049065317, "phrase": "optimized_cartesian"}, {"score": 0.004703878334423852, "phrase": "product_quantization-based_approaches"}, {"score": 0.004595356579614969, "phrase": "high-dimensional_data_points"}, {"score": 0.004385733482617193, "phrase": "cartesian_product"}, {"score": 0.004344965512058521, "phrase": "low-dimensional_subspaces"}, {"score": 0.004205226397573506, "phrase": "sub_codebook"}, {"score": 0.0040890178236849825, "phrase": "compact_binary_codes"}, {"score": 0.004032118497047498, "phrase": "sub_codebooks"}, {"score": 0.0037592362369401546, "phrase": "precomputed_lookup_tables"}, {"score": 0.0034559594248079807, "phrase": "corresponding_sub_codebook"}, {"score": 0.0033447157263095223, "phrase": "strict_restrictions"}, {"score": 0.00329813918116714, "phrase": "search_accuracy"}, {"score": 0.003162252895465901, "phrase": "novel_approach"}, {"score": 0.0031182092708983184, "phrase": "optimized_cartesian_k-means"}, {"score": 0.0026474076254702525, "phrase": "sub_codeword"}, {"score": 0.0026105159490849364, "phrase": "different_sub_codebooks"}, {"score": 0.002422249273477746, "phrase": "distortion_errors"}, {"score": 0.002388487669097511, "phrase": "high-dimensional_data_point"}, {"score": 0.0021750990136123367, "phrase": "lower_distortion_errors"}, {"score": 0.002154835714714195, "phrase": "traditional_methods"}, {"score": 0.0021347607855562102, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "standard_real-life_data_sets"}], "paper_keywords": ["Clustering", " cartesian product", " nearest neighbor search"], "paper_abstract": "Product quantization-based approaches are effective to encode high-dimensional data points for approximate nearest neighbor search. The space is decomposed into a Cartesian product of low-dimensional subspaces, each of which generates a sub codebook. Data points are encoded as compact binary codes using these sub codebooks, and the distance between two data points can be approximated efficiently from their codes by the precomputed lookup tables. Traditionally, to encode a subvector of a data point in a subspace, only one sub codeword in the corresponding sub codebook is selected, which may impose strict restrictions on the search accuracy. In this paper, we propose a novel approach, named optimized cartesian K-means (ock-means), to better encode the data points for more accurate approximate nearest neighbor search. In ock-means, multiple sub codewords are used to encode the subvector of a data point in a subspace. Each sub codeword stems from different sub codebooks in each subspace, which are optimally generated with regards to the minimization of the distortion errors. The high-dimensional data point is then encoded as the concatenation of the indices of multiple sub codewords from all the subspaces. This can provide more flexibility and lower distortion errors than traditional methods. Experimental results on the standard real-life data sets demonstrate the superiority over state-of-the-art approaches for approximate nearest neighbor search.", "paper_title": "Optimized Cartesian K-Means", "paper_id": "WOS:000346167800015"}