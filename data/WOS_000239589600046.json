{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "one-class_svm"}, {"score": 0.004749106104067268, "phrase": "imbalance_text_classification"}, {"score": 0.004620093797504386, "phrase": "conventional_two-class_learning_schemes"}, {"score": 0.004342430078306461, "phrase": "single_class"}, {"score": 0.004283019592801715, "phrase": "training_purposes"}, {"score": 0.004195418010131832, "phrase": "one-class_classification"}, {"score": 0.004025531820226182, "phrase": "imbalanced_data"}, {"score": 0.0038624981944755813, "phrase": "better_performance"}, {"score": 0.0037834649508375544, "phrase": "two-class_one"}, {"score": 0.0035072146967082083, "phrase": "best_use"}, {"score": 0.0033883824438527316, "phrase": "learning_procedure"}, {"score": 0.00327356326590461, "phrase": "general_framework"}, {"score": 0.0031626225189867354, "phrase": "minority_class"}, {"score": 0.0030554300179552415, "phrase": "one-class_classification_stage"}, {"score": 0.0028914057633212045, "phrase": "majority_class"}, {"score": 0.0028127163745564777, "phrase": "generalization_performance"}, {"score": 0.0027551040319267446, "phrase": "constructed_classifier"}, {"score": 0.0026616868981501006, "phrase": "generalization_performance_measurement"}, {"score": 0.0026252106881752067, "phrase": "parameter_search_algorithm"}, {"score": 0.002571429085596686, "phrase": "best_parameter_settings"}, {"score": 0.002450180286425144, "phrase": "uci"}, {"score": 0.002416592337720292, "phrase": "reuters"}, {"score": 0.0023999697562763433, "phrase": "text_data"}, {"score": 0.0022092084106039834, "phrase": "standard_one-class_svm"}, {"score": 0.0021049977753042253, "phrase": "one-class_naive_bayes"}], "paper_keywords": [""], "paper_abstract": "Compared with conventional two-class learning schemes, one-class classification simply uses a single class for training purposes. Applying one-class classification to the minorities in an imbalanced data has been shown to achieve better performance than the two-class one. In this paper, in order to make the best use of all the available information during the learning procedure, we propose a general framework which first uses the minority class for training in the one-class classification stage; and then uses both minority and majority class for estimating the generalization performance of the constructed classifier. Based upon this generalization performance measurement, parameter search algorithm selects the best parameter settings for this classifier. Experiments on UCI and Reuters text data show that one-class SVM embedded in this framework achieves much better performance than the standard one-class SVM alone and other learning schemes, such as one-class Naive Bayes, one-class nearest neighbour and neural network.", "paper_title": "Parameter estimation of one-class SVM on imbalance text classification", "paper_id": "WOS:000239589600046"}