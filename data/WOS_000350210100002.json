{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dynamic_facial_emotion_recognition"}, {"score": 0.00475382761232911, "phrase": "hci_applications"}, {"score": 0.004633888966463624, "phrase": "multimodal_animated_avatar"}, {"score": 0.0045556080393342165, "phrase": "marcos-pablos_et_al"}, {"score": 0.004328576620172575, "phrase": "human-computer_interaction"}, {"score": 0.003858110309630846, "phrase": "dynamic_recognition"}, {"score": 0.0038253590626087237, "phrase": "displayed_facial_emotions"}, {"score": 0.0037928847801943404, "phrase": "low-resolution_streaming_images"}, {"score": 0.0036501160751879784, "phrase": "action_units"}, {"score": 0.003557924423642353, "phrase": "facial_action_coding_system"}, {"score": 0.003527712410706358, "phrase": "active_shape_models"}, {"score": 0.0034977560416755726, "phrase": "gabor_filters"}, {"score": 0.003468053168607071, "phrase": "normalized_outputs"}, {"score": 0.003423969509052361, "phrase": "au_recognition_step"}, {"score": 0.003309124298009635, "phrase": "neural_network"}, {"score": 0.003239304090565123, "phrase": "habituation_network"}, {"score": 0.0031981188104809994, "phrase": "competitive_network"}, {"score": 0.003104038398386081, "phrase": "habituation_layer_use"}, {"score": 0.0029871209444889716, "phrase": "dynamic_information"}, {"score": 0.0029617414861784525, "phrase": "facial_expressions"}, {"score": 0.0029116257477000617, "phrase": "experimental_results"}, {"score": 0.0028623555849897632, "phrase": "live_video_sequences"}, {"score": 0.0028139168128067343, "phrase": "cohn-kanade_face_database_show"}, {"score": 0.002742787059497186, "phrase": "high_recognition_hit_rates"}, {"score": 0.0026507289852980512, "phrase": "developed_emotional_recognition_system"}, {"score": 0.0026282000779095987, "phrase": "human-computer_interaction_applications"}, {"score": 0.002433843373780471, "phrase": "preliminary_experiment"}, {"score": 0.002362192695471035, "phrase": "promising_results"}, {"score": 0.0022634699331607615, "phrase": "emotional_recognition_system"}, {"score": 0.0022346638243276717, "phrase": "clear_increase"}, {"score": 0.0021049977753042253, "phrase": "emotional_response"}], "paper_keywords": ["graphical user interfaces", " agent-based interaction", " gestural input", " intelligent avatars", " computer vision", " empirical studies in ubiquitous and mobile computing"], "paper_abstract": "As part of a multimodal animated avatar previously presented in Marcos-Pablos et al. ((2010) A realistic, virtual head for human-computer interaction. Interact. Comput., 22, 176-192, ISSN 0953-5438), in this paper we describe a method for dynamic recognition of displayed facial emotions on low-resolution streaming images. First, we address the detection of action units (AUs) of the facial action coding system using active shape models and Gabor filters. Normalized outputs of the AU recognition step are then used as inputs for a neural network that consists of an habituation network plus a competitive network. Both the competitive and the habituation layer use differential equations, thus taking into account the dynamic information of facial expressions through time. Experimental results carried out on live video sequences and on the Cohn-Kanade face database show that the proposed method provides high recognition hit rates. To assess the suitability of the developed emotional recognition system for human-computer interaction applications, it has been successfully integrated in the architecture of an avatar and we have conducted a preliminary experiment on empathy. The experiment showed promising results, as the avatar that made use of the emotional recognition system obtained a clear increase in the positivity of the rating when compared with the same avatar with no emotional response.", "paper_title": "Dynamic Facial Emotion Recognition Oriented to HCI Applications", "paper_id": "WOS:000350210100002"}