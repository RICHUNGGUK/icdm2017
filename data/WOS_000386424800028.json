{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "argonaut"}, {"score": 0.007159494866019699, "phrase": "crowd_workers"}, {"score": 0.004784909348794283, "phrase": "macrotask"}, {"score": 0.004740197030914445, "phrase": "complex_data_processing"}, {"score": 0.004710620078577185, "phrase": "crowdsourced_workflows"}, {"score": 0.004494570360108983, "phrase": "databases_community"}, {"score": 0.004342430078306461, "phrase": "entity_resolution"}, {"score": 0.004169226023596022, "phrase": "multiple_choice_questions"}, {"score": 0.004040688646993605, "phrase": "voting_algorithms"}, {"score": 0.004002902686509964, "phrase": "redundant_responses"}, {"score": 0.003977907771686101, "phrase": "multiple_crowd_workers"}, {"score": 0.003940706575253453, "phrase": "result_quality"}, {"score": 0.003807242767474929, "phrase": "larger_context"}, {"score": 0.0037130155038449554, "phrase": "significant_time_investment"}, {"score": 0.003587235362628753, "phrase": "large-document_structured_data_extraction"}, {"score": 0.0034984340449789745, "phrase": "context-heavy_data"}, {"score": 0.0032756158688245, "phrase": "worker_pools"}, {"score": 0.0032551474344211582, "phrase": "existing_crowdsourcing_platforms"}, {"score": 0.0032145922365231093, "phrase": "macrotask_scheduling"}, {"score": 0.003095923814666088, "phrase": "macrotask-powered_work"}, {"score": 0.0030003766828110277, "phrase": "worker's_output"}, {"score": 0.0029722900280316216, "phrase": "ground_truth"}, {"score": 0.0029260602766930065, "phrase": "redundancy-based_quality_control_schemes"}, {"score": 0.002818013056816282, "phrase": "macrotask_powered_work_quality"}, {"score": 0.0027916287357898544, "phrase": "hierarchical_review"}, {"score": 0.002748201169075936, "phrase": "predictive_model"}, {"score": 0.0027310193758440376, "phrase": "worker_quality"}, {"score": 0.0027054473441439422, "phrase": "trusted_workers"}, {"score": 0.002613709503583786, "phrase": "task_quality"}, {"score": 0.002509284173541882, "phrase": "ideal_trade-off"}, {"score": 0.0024857831949206166, "phrase": "single_phase"}, {"score": 0.0024547897375637308, "phrase": "multiple_phases"}, {"score": 0.00241658945942583, "phrase": "constrained_review_budget"}, {"score": 0.0023789822182914877, "phrase": "overall_output_quality"}, {"score": 0.0023419588513908783, "phrase": "industrial_use"}, {"score": 0.002298288831092204, "phrase": "structured_data_extraction_pipeline"}, {"score": 0.002255431275670053, "phrase": "crowd_worker_input"}, {"score": 0.0021382831203116686, "phrase": "random_spot-check_reviews"}, {"score": 0.0021182494144182805, "phrase": "budget-constrained_environments"}], "paper_keywords": [""], "paper_abstract": "Crowdsourced workflows are used in research and industry to solve a variety of tasks. The databases community has used crowd workers in query operators/optimization and for tasks such as entity resolution. Such research utilizes microtasks where crowd workers are asked to answer simple yes/no or multiple choice questions with little training. Typically, microtasks are used with voting algorithms to combine redundant responses from multiple crowd workers to achieve result quality. Microtasks are powerful, but fail in cases where larger context (e.g., domain knowledge) or significant time investment is needed to solve a problem, for example in large-document structured data extraction. In this paper, we consider context-heavy data processing tasks that may require many hours of work, and refer to such tasks as macrotasks. Leveraging the infrastructure and worker pools of existing crowdsourcing platforms, we automate macrotask scheduling, evaluation, and pay scales. A key challenge in macrotask-powered work, however, is evaluating the quality of a worker's output, since ground truth is seldom available and redundancy-based quality control schemes are impractical. We present Argonaut, a framework that improves macrotask powered work quality using a hierarchical review. Argonaut uses a predictive model of worker quality to select trusted workers to perform review, and a separate predictive model of task quality to decide which tasks to review. Finally, Argonaut can identify the ideal trade-off between a single phase of review and multiple phases of review given a constrained review budget in order to maximize overall output quality. We evaluate an industrial use of Argonaut to power a structured data extraction pipeline that has utilized over half a million hours of crowd worker input to complete millions of macrotasks. We show that Argonaut can capture up to 118% more errors than random spot-check reviews in review budget-constrained environments with up to two review layers.", "paper_title": "Argonaut: Macrotask Crowdsourcing for Complex Data Processing", "paper_id": "WOS:000386424800028"}