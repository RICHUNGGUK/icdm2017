{"auto_keywords": [{"score": 0.04430009168506124, "phrase": "observed_behavior"}, {"score": 0.010695740169556673, "phrase": "agent's_goals"}, {"score": 0.004815059535982149, "phrase": "modular"}, {"score": 0.004716520738331953, "phrase": "visuomotor_behavior"}, {"score": 0.0046392209103157936, "phrase": "large_variety"}, {"score": 0.00446987606516801, "phrase": "expressive_and_accurate_model"}, {"score": 0.004378469469879815, "phrase": "human_behavior"}, {"score": 0.004324521292490181, "phrase": "general_purpose_mathematical_models"}, {"score": 0.004271234971981848, "phrase": "successfully_properties"}, {"score": 0.004132312946154097, "phrase": "root_models"}, {"score": 0.004098291340525772, "phrase": "biological_facts"}, {"score": 0.004031082737449308, "phrase": "ample_empirical_evidence"}, {"score": 0.003997891221012094, "phrase": "reward-based_learning"}, {"score": 0.0038678251731669865, "phrase": "computational_model"}, {"score": 0.003757480190200645, "phrase": "observed_agent"}, {"score": 0.003459191419478298, "phrase": "reinforcement_learning"}, {"score": 0.0033883824438527316, "phrase": "well-established_algorithms"}, {"score": 0.0033327768024847397, "phrase": "visuomotor_task_solutions"}, {"score": 0.0031064226956732497, "phrase": "inverse_reinforcement_learning"}, {"score": 0.0028715299874549245, "phrase": "modular_cognitive_architecture"}, {"score": 0.0028127163745564777, "phrase": "modular_inverse_reinforcement_learning_algorithm"}, {"score": 0.0027665315772006575, "phrase": "relative_reward_contributions"}, {"score": 0.0027323899681456535, "phrase": "component_tasks"}, {"score": 0.0024842242921857705, "phrase": "component_reward_weights"}, {"score": 0.0024637379406427856, "phrase": "individual_tasks"}, {"score": 0.002413256359147736, "phrase": "observed_trajectories"}, {"score": 0.0023540387251475615, "phrase": "behavioral_goals"}, {"score": 0.0022773309395419427, "phrase": "good_estimates"}, {"score": 0.0022214412809055013, "phrase": "modest_amounts"}, {"score": 0.002203117204010397, "phrase": "observation_data"}, {"score": 0.0021049977753042253, "phrase": "novel_configurations"}], "paper_keywords": ["Inverse reinforcement learning", " Visuomotor behavior", " Spatial navigation", " Task priorities"], "paper_abstract": "In a large variety of situations one would like to have an expressive and accurate model of observed animal or human behavior. While general purpose mathematical models may capture successfully properties of observed behavior, it is desirable to root models in biological facts. Because of ample empirical evidence for reward-based learning in visuomotor tasks, we use a computational model based on the assumption that the observed agent is balancing the costs and benefits of its behavior to meet its goals. This leads to using the framework of reinforcement learning, which additionally provides well-established algorithms for learning of visuomotor task solutions. To quantify the agent's goals as rewards implicit in the observed behavior, we propose to use inverse reinforcement learning, which quantifies the agent's goals as rewards implicit in the observed behavior. Based on the assumption of a modular cognitive architecture, we introduce a modular inverse reinforcement learning algorithm that estimates the relative reward contributions of the component tasks in navigation, consisting of following a path while avoiding obstacles and approaching targets. It is shown how to recover the component reward weights for individual tasks and that variability in observed trajectories can be explained succinctly through behavioral goals. It is demonstrated through simulations that good estimates can be obtained already with modest amounts of observation data, which in turn allows the prediction of behavior in novel configurations.", "paper_title": "Modular inverse reinforcement learning for visuomotor behavior", "paper_id": "WOS:000323655700006"}