{"auto_keywords": [{"score": 0.04852843488568066, "phrase": "rl"}, {"score": 0.005252603132983937, "phrase": "dp"}, {"score": 0.00481495049065317, "phrase": "tutorial_survey"}, {"score": 0.004689815048273592, "phrase": "reinforcement_learning"}, {"score": 0.004405440436639404, "phrase": "powerful_tool"}, {"score": 0.00436213603647063, "phrase": "complex_sequential_decision-making_problems"}, {"score": 0.004276794599014972, "phrase": "seminal_research"}, {"score": 0.004165586713723931, "phrase": "artificial_intelligence"}, {"score": 0.003977858136481356, "phrase": "optimization_theorists"}, {"score": 0.00383628868871796, "phrase": "large-scale_and_complex_problems"}, {"score": 0.003736700743267932, "phrase": "markov"}, {"score": 0.0036875896301083884, "phrase": "mdp"}, {"score": 0.0034183895670057717, "phrase": "large-scale_mdps"}, {"score": 0.0032858160838749196, "phrase": "classical_dynamic_programming"}, {"score": 0.0029960701622407924, "phrase": "classical_dp"}, {"score": 0.0028141081648971754, "phrase": "monte_carlo_simulation"}, {"score": 0.002795607293713107, "phrase": "function_approximation"}, {"score": 0.0027680831622262027, "phrase": "ai._topics"}, {"score": 0.0026871217010784143, "phrase": "temporal_differences"}, {"score": 0.002394104138423188, "phrase": "numerous_examples"}, {"score": 0.002293582591899972, "phrase": "mathematical_roots"}, {"score": 0.0022264684349099775, "phrase": "clear_understanding"}, {"score": 0.0022045353683203626, "phrase": "core_concepts"}], "paper_keywords": ["artificial intelligence", " dynamic programming", " simulation", " reinforcement learning"], "paper_abstract": "In the last few years, reinforcement learning (RL), also called adaptive (or approximate) dynamic programming, has emerged as a powerful tool for solving complex sequential decision-making problems in control theory. Although seminal research in this area was performed in the artificial intelligence (AI) community, more recently it has attracted the attention of optimization theorists because of several noteworthy success stories from operations management. It is on large-scale and complex problems of dynamic optimization, in particular the Markov decision problem (MDP) and its variants, that the power of RL becomes more obvious. It has been known for many years that on large-scale MDPs, the curse of dimensionality and the curse of modeling render classical dynamic programming (DP) ineffective. The excitement in RL stems from its direct attack on these curses, which allows it to solve problems that were considered intractable via classical DP in the past. The success of RL is due to its strong mathematical roots in the principles of DP, Monte Carlo simulation, function approximation, and AI. Topics treated in some detail in this survey are temporal differences, Q-learning, semi-MDPs, and stochastic games. Several recent advances in RL, e. g., policy gradients and hierarchical RL, are covered along with references. Pointers to numerous examples of applications are provided. This overview is aimed at uncovering the mathematical roots of this science so that readers gain a clear understanding of the core concepts and are able to use them in their own research. The survey points to more than 100 references from the literature.", "paper_title": "Reinforcement Learning: A Tutorial Survey and Recent Advances", "paper_id": "WOS:000265756900001"}