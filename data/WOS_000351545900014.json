{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "incomplete_information_cournot_games"}, {"score": 0.004734448316824488, "phrase": "state_estimation_approach"}, {"score": 0.00462919213721188, "phrase": "incomplete_information_game"}, {"score": 0.004551781491498328, "phrase": "big_challenge"}, {"score": 0.004425616928975799, "phrase": "best_way"}, {"score": 0.004351595642056769, "phrase": "available_information"}, {"score": 0.0043029342321223825, "phrase": "optimal_decision_making"}, {"score": 0.0036759314172445934, "phrase": "repeated_cournot_competition"}, {"score": 0.0035539019235553897, "phrase": "day-ahead_electricity_market"}, {"score": 0.0034359094594153304, "phrase": "rivals'_offered_quantities"}, {"score": 0.0033974527047473044, "phrase": "sroq"}, {"score": 0.003104849179057156, "phrase": "adaptive_expectation_method"}, {"score": 0.003035711716881469, "phrase": "model-based_approach"}, {"score": 0.00293487292076612, "phrase": "agents'_strategies"}, {"score": 0.0028857130140890787, "phrase": "nash_equilibrium_point"}, {"score": 0.002743105561254975, "phrase": "learning-based_approach"}, {"score": 0.0026971491199975083, "phrase": "optimal_bidding_strategy"}, {"score": 0.002607527117561699, "phrase": "state_estimation"}, {"score": 0.0025638362414466278, "phrase": "reinforcement_learning_method"}, {"score": 0.002506715532901322, "phrase": "estimated_state"}, {"score": 0.0024234059612683032, "phrase": "optimal_decision"}, {"score": 0.0023560952790080943, "phrase": "fuzzy_q-learning_algorithm"}, {"score": 0.0023035922232490106, "phrase": "case_study"}, {"score": 0.0022145052062614514, "phrase": "three-bus_benchmark_cournot_model"}, {"score": 0.0021408862709571615, "phrase": "generators'_bids"}, {"score": 0.0021049977753042253, "phrase": "nash-cournot_equilibrium"}], "paper_keywords": ["Cournot oligopoly", " fuzzy Q-learning (FQL)", " learning", " repeated game", " state estimation"], "paper_abstract": "In an incomplete information game, a big challenge is to find the best way of exploiting available information for optimal decision making of the agents. In this paper, two decision making methods, namely model-based and learning-based bidding strategies, are proposed and compared, for repeated Cournot competition of the generators in a day-ahead electricity market. The sum of the rivals' offered quantities (SROQ) is considered as the state of the agent and its value is estimated using an adaptive expectation method. In the model-based approach, the convergence of the agents' strategies to the Nash equilibrium point is also studied in two different cases. In the learning-based approach, the optimal bidding strategy is learned through combination of state estimation and a reinforcement learning method. Using the estimated state (SROQ), the optimal decision is learned through a fuzzy Q-learning algorithm. Through a case study, which is performed on the three-bus benchmark Cournot model, the convergence of the generators' bids to the Nash-Cournot equilibrium is examined.", "paper_title": "Model-Based and Learning-Based Decision Making in Incomplete Information Cournot Games: A State Estimation Approach", "paper_id": "WOS:000351545900014"}