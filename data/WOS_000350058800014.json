{"auto_keywords": [{"score": 0.04659626041220383, "phrase": "high-dimensional_data"}, {"score": 0.00481495049065317, "phrase": "residuals-based_sparse_pls"}, {"score": 0.004787116526591974, "phrase": "sparse_kernel_pls_regression"}, {"score": 0.004677372885331908, "phrase": "vast_literature"}, {"score": 0.004596711717456095, "phrase": "relating_gene_profiles"}, {"score": 0.00457013354092966, "phrase": "subject_survival"}, {"score": 0.004517435235940661, "phrase": "cancer_recurrence"}, {"score": 0.004491313367627919, "phrase": "biomarker_discovery"}, {"score": 0.004413846562278824, "phrase": "transcriptomic_or_single_nucleotide_polymorphism_profiles"}, {"score": 0.004250535418141079, "phrase": "proportional_hazard_regression_model"}, {"score": 0.004213814889211045, "phrase": "cox"}, {"score": 0.0038513073787989054, "phrase": "survival_data"}, {"score": 0.0037958359268041426, "phrase": "multivariate_regression"}, {"score": 0.0036979855349860134, "phrase": "complete_data"}, {"score": 0.0034291991359158827, "phrase": "over-fitting_and_model_misidentification"}, {"score": 0.0033602211267167435, "phrase": "estimation_accuracy"}, {"score": 0.003292626019313584, "phrase": "relevant_predictors"}, {"score": 0.0032546105164772995, "phrase": "model_interpretability"}, {"score": 0.00323576714040102, "phrase": "parsimonious_representation"}, {"score": 0.0031614748814351823, "phrase": "variable_selection_issues"}, {"score": 0.003115908199991072, "phrase": "least_absolute_shrinkage_and_selection_operator_penalized_cox_proportional_hazards"}, {"score": 0.003035543107511785, "phrase": "tibshirani"}, {"score": 0.0029658232220207254, "phrase": "dimension_reduction"}, {"score": 0.0029145912511665843, "phrase": "partial_least_squares"}, {"score": 0.0027661298116867013, "phrase": "dksplsdr"}, {"score": 0.0027341763737751467, "phrase": "sparse_pls_regression"}, {"score": 0.002686935113435742, "phrase": "deviance_residuals"}, {"score": 0.0025723630851105304, "phrase": "simulated_and_real_reference_benchmark_datasets"}, {"score": 0.0023990982005125763, "phrase": "benchmark_datasets"}, {"score": 0.002343970440509428, "phrase": "pls_regression"}, {"score": 0.0022967713922705, "phrase": "biplots_representation"}, {"score": 0.0022439896633013297, "phrase": "missing_data"}, {"score": 0.0021860555252905876, "phrase": "useful_addition"}, {"score": 0.0021420294823879292, "phrase": "prediction_methods"}, {"score": 0.00212343307513777, "phrase": "widely_used_cox's_model"}, {"score": 0.0021049977753042253, "phrase": "high-dimensional_and_low-sample_size_settings"}], "paper_keywords": [""], "paper_abstract": "Motivation: A vast literature from the past decade is devoted to relating gene profiles and subject survival or time to cancer recurrence. Biomarker discovery from high-dimensional data, such as transcriptomic or single nucleotide polymorphism profiles, is a major challenge in the search for more precise diagnoses. The proportional hazard regression model suggested by Cox (1972), to study the relationship between the time to event and a set of covariates in the presence of censoring is the most commonly used model for the analysis of survival data. However, like multivariate regression, it supposes that more observations than variables, complete data, and not strongly correlated variables are available. In practice, when dealing with high-dimensional data, these constraints are crippling. Collinearity gives rise to issues of over-fitting and model misidentification. Variable selection can improve the estimation accuracy by effectively identifying the subset of relevant predictors and enhance the model interpretability with parsimonious representation. To deal with both collinearity and variable selection issues, many methods based on least absolute shrinkage and selection operator penalized Cox proportional hazards have been proposed since the reference paper of Tibshirani. Regularization could also be performed using dimension reduction as is the case with partial least squares (PLS) regression. We propose two original algorithms named sPLSDR and its non-linear kernel counterpart DKsPLSDR, by using sparse PLS regression (sPLS) based on deviance residuals. We compared their predicting performance with state-of-the-art algorithms on both simulated and real reference benchmark datasets. Results: sPLSDR and DKsPLSDR compare favorably with other methods in their computational time, prediction and selectivity, as indicated by results based on benchmark datasets. Moreover, in the framework of PLS regression, they feature other useful tools, including biplots representation, or the ability to deal with missing data. Therefore, we view them as a useful addition to the toolbox of estimation and prediction methods for the widely used Cox's model in the high-dimensional and low-sample size settings.", "paper_title": "Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data", "paper_id": "WOS:000350058800014"}