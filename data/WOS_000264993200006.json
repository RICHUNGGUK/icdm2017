{"auto_keywords": [{"score": 0.041469740173514034, "phrase": "feature_vectors"}, {"score": 0.03807495921495179, "phrase": "feature_vector"}, {"score": 0.00481495049065317, "phrase": "statistical_language_model"}, {"score": 0.004721162131188487, "phrase": "non-linear_prediction"}, {"score": 0.004363846362238565, "phrase": "state-of-the-art_neural_network_language_model"}, {"score": 0.0035141291835319682, "phrase": "next_word"}, {"score": 0.003445591238183763, "phrase": "significant_improvements"}, {"score": 0.003378385493203297, "phrase": "predictive_accuracy"}, {"score": 0.0031845067058084613, "phrase": "non-linear_subnetwork"}, {"score": 0.0029722900280316216, "phrase": "context_words"}, {"score": 0.0028293966058473476, "phrase": "non-linear_correction_term"}, {"score": 0.002640781900605126, "phrase": "log-bilinear_language_model"}, {"score": 0.0023003498123420237, "phrase": "best_n-gram_model"}, {"score": 0.002233301265743633, "phrase": "fairly_large_dataset"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Statistical language modelling", " Distributed representations", " Neural networks"], "paper_abstract": "We show how to improve a state-of-the-art neural network language model that converts the previous \"context\" words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector. A log-bilinear language model that incorporates both of these improvements achieves a 26% reduction in perplexity over the best n-gram model on a fairly large dataset. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Improving a statistical language model through non-linear prediction", "paper_id": "WOS:000264993200006"}