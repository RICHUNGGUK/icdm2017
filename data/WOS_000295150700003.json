{"auto_keywords": [{"score": 0.007939862327795026, "phrase": "robust_learning_algorithms"}, {"score": 0.007539824007847785, "phrase": "noise_impact"}, {"score": 0.00481495049065317, "phrase": "data_imperfections"}, {"score": 0.004603352344461032, "phrase": "challenging_and_reality_issue"}, {"score": 0.004493275458211613, "phrase": "data_quality_enhancement"}, {"score": 0.004431554974667461, "phrase": "preprocessing_techniques"}, {"score": 0.004325568312096711, "phrase": "overly_complicated_structures"}, {"score": 0.004222105714769577, "phrase": "essential_goal"}, {"score": 0.004050442607427367, "phrase": "noise-corrupted_data"}, {"score": 0.003926273177600597, "phrase": "novel_corrective_classification"}, {"score": 0.0038190873308414333, "phrase": "data_cleansing"}, {"score": 0.0037927494831151553, "phrase": "error_correction"}, {"score": 0.0036891952845575036, "phrase": "effective_learning"}, {"score": 0.003663749926218976, "phrase": "noisy_data_sources"}, {"score": 0.0036008989464331835, "phrase": "existing_classifier_ensembling"}, {"score": 0.0034544053534458093, "phrase": "diverse_base_learners"}, {"score": 0.003336875060853387, "phrase": "bootstrap_sampling_process"}, {"score": 0.0032233306034106127, "phrase": "base_learner"}, {"score": 0.003201088113538101, "phrase": "unifying_error_detection"}, {"score": 0.0030496164547515565, "phrase": "classifier_ensemble"}, {"score": 0.0029052913631676435, "phrase": "experimental_comparisons"}, {"score": 0.0027677775883341556, "phrase": "original_noisy_sources"}, {"score": 0.0024096216848276094, "phrase": "decorate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Noisy data", " Error correction", " Bagging", " Bootstrap sampling", " Classifier ensemble"], "paper_abstract": "Learning from imperfect (noisy) information sources is a challenging and reality issue for many data mining applications. Common practices include data quality enhancement by applying data preprocessing techniques or employing robust learning algorithms to avoid developing overly complicated structures that overfit the noise. The essential goal is to reduce noise impact and eventually enhance the learners built from noise-corrupted data. In this paper, we propose a novel corrective classification (C2) design, which incorporates data cleansing, error correction. Bootstrap sampling and classifier ensembling for effective learning from noisy data sources. C2 differs from existing classifier ensembling or robust learning algorithms in two aspects. On one hand, a set of diverse base learners of C2 constituting the ensemble are constructed via a Bootstrap sampling process; on the other hand, C2 further improves each base learner by unifying error detection, correction and data cleansing to reduce noise impact. Being corrective, the classifier ensemble is built from data preprocessed/corrected by the data cleansing and correcting modules. Experimental comparisons demonstrate that C2 is not only more accurate than the learner built from original noisy sources, but also more reliable than Bagging [4] or aggressive classifier ensemble (ACE) [56], which are two degenerated components/variants of C2. The comparisons also indicate that C2 is more stable than Boosting and DECORATE, which are two state-of-the-art ensembling methods. For real-world imperfect information sources (i.e. noisy training and/or test data), C2 is able to deliver more accurate and reliable prediction models than its other peers can offer. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Corrective classification: Learning from data imperfections with aggressive and diverse classifier ensembling", "paper_id": "WOS:000295150700003"}