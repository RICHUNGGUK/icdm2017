{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "piecewise_polynomial_estimator"}, {"score": 0.049146612112995924, "phrase": "regression_function"}, {"score": 0.02246846748295754, "phrase": "cart_algorithm"}, {"score": 0.004392565073466714, "phrase": "first_part"}, {"score": 0.0041136473765874815, "phrase": "piecewise_polynomial_models"}, {"score": 0.003903269701018361, "phrase": "partition_m"}, {"score": 0.0031429517477457925, "phrase": "penalized_least_squares_criterion"}, {"score": 0.0024325244760735566, "phrase": "second_part"}, {"score": 0.002323142666222693, "phrase": "tree-structured_collections"}, {"score": 0.002146926767291475, "phrase": "first_step"}], "paper_keywords": ["CART", " concentration inequalities", " model selection", " oracle inequalities", " polynomial estimation", " regression"], "paper_abstract": "We deal with the problem of choosing a piecewise polynomial estimator of a regression function mapping [0, 1](p) into. In a first part of this paper, we consider some collection of piecewise polynomial models. Each model is defined by a partition M of [0, 1](p) and a series of degrees (d) under bar = (d(J))(J is an element of M) is an element of N-M. We propose a penalized least squares criterion which selects a model whose associated piecewise polynomial estimator performs approximately as well as the best one, in the sense that its quadratic risk is close to the infimum of the risks. The risk bound we provide is nonasymptotic. In a second part, we apply this result to tree-structured collections of partitions, which look like the one constructed in the first step of the CART algorithm. And we propose an extension of the CART algorithm to build a piecewise polynomial estimator of a regression function.", "paper_title": "Piecewise Polynomial Estimation of a Regression Function", "paper_id": "WOS:000273134100043"}