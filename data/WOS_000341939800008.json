{"auto_keywords": [{"score": 0.02563142992595596, "phrase": "video_stream"}, {"score": 0.00481495049065317, "phrase": "online_estimation"}, {"score": 0.004775918707955445, "phrase": "evolving_human_visual_interest"}, {"score": 0.0046987973264610685, "phrase": "video_streams"}, {"score": 0.004660702714929615, "phrase": "human_interest"}, {"score": 0.004585433265422727, "phrase": "human_understanding"}, {"score": 0.004420472010542188, "phrase": "salient_and_informative_regions"}, {"score": 0.004244102948402086, "phrase": "eye_movements"}, {"score": 0.004192571571622019, "phrase": "challenging_problem"}, {"score": 0.004108066884382925, "phrase": "content-aware_retargeting"}, {"score": 0.004041686156098498, "phrase": "different_aspect_ratios"}, {"score": 0.00399260264462941, "phrase": "informative_regions"}, {"score": 0.003960210592458847, "phrase": "smart_insertion"}, {"score": 0.003740679163860117, "phrase": "predicted_rois"}, {"score": 0.0035332741328861776, "phrase": "model_eye_movements"}, {"score": 0.003490343299449249, "phrase": "visual_saliency"}, {"score": 0.0034620119220744347, "phrase": "yet-unseen_frames"}, {"score": 0.003310236836421397, "phrase": "visual_attention"}, {"score": 0.003217124533918232, "phrase": "important_eye-gaze_characteristics"}, {"score": 0.003126623133221065, "phrase": "sudden_eye_movements"}, {"score": 0.0030635374056374016, "phrase": "behavioral_artifacts"}, {"score": 0.0030262967460085366, "phrase": "novel_statistical-and_algorithm-based_method_gaze"}, {"score": 0.002881793492919646, "phrase": "content-based_features"}, {"score": 0.0027441711627551064, "phrase": "video_aspect_ratios"}, {"score": 0.002677837900473396, "phrase": "content-aware_video_retargeting"}, {"score": 0.002570817338292482, "phrase": "display_sizes"}, {"score": 0.002539550974361945, "phrase": "second_application"}, {"score": 0.002508663916554954, "phrase": "active_speakers"}, {"score": 0.0024781515864648242, "phrase": "dialog_captions"}, {"score": 0.002321563536990284, "phrase": "active_speaker_locations"}, {"score": 0.002265422518417868, "phrase": "salient_content"}, {"score": 0.0021049977753042253, "phrase": "individual_users"}], "paper_keywords": ["Human Factors", " Video retargeting", " video captioning", " gaze", " visual attention"], "paper_abstract": "Regions in video streams attracting human interest contribute significantly to human understanding of the video. Being able to predict salient and informative Regions of Interest (ROIs) through a sequence of eye movements is a challenging problem. Applications such as content-aware retargeting of videos to different aspect ratios while preserving informative regions and smart insertion of dialog (closed-caption text) into the video stream can significantly be improved using the predicted ROIs. We propose an interactive human-in-the-loop framework to model eye movements and predict visual saliency into yet-unseen frames. Eye tracking and video content are used to model visual attention in a manner that accounts for important eye-gaze characteristics such as temporal discontinuities due to sudden eye movements, noise, and behavioral artifacts. A novel statistical-and algorithm-based method gaze buffering is proposed for eye-gaze analysis and its fusion with content-based features. Our robust saliency prediction is instantiated for two challenging and exciting applications. The first application alters video aspect ratios on-the-fly using content-aware video retargeting, thus making them suitable for a variety of display sizes. The second application dynamically localizes active speakers and places dialog captions on-the-fly in the video stream. Our method ensures that dialogs are faithful to active speaker locations and do not interfere with salient content in the video stream. Our framework naturally accommodates personalisation of the application to suit biases and preferences of individual users.", "paper_title": "Online Estimation of Evolving Human Visual Interest", "paper_id": "WOS:000341939800008"}