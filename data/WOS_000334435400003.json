{"auto_keywords": [{"score": 0.02839829974363596, "phrase": "particle_filter"}, {"score": 0.004815340879031441, "phrase": "adaptive"}, {"score": 0.004788851448504016, "phrase": "sequential_monte_carlo"}, {"score": 0.0045108906244892165, "phrase": "proposal_kernel"}, {"score": 0.004462105963334103, "phrase": "particle_filters"}, {"score": 0.004342430078306461, "phrase": "significant_importance"}, {"score": 0.0042489946866108895, "phrase": "bad_choice"}, {"score": 0.004090274535243564, "phrase": "particle_sample"}, {"score": 0.003937459891461576, "phrase": "computational_power"}, {"score": 0.0037697665240643066, "phrase": "novel_algorithm"}, {"score": 0.0036886073959526396, "phrase": "so-called_optimal_proposal_kernel"}, {"score": 0.003589602322126282, "phrase": "integrated_curved_exponential_distributions"}, {"score": 0.0030654278282822027, "phrase": "strongly_skewed_distributions"}, {"score": 0.002934764205541183, "phrase": "online-em_methods"}, {"score": 0.0028715299874549245, "phrase": "optimal_kernel"}, {"score": 0.0027943943921568456, "phrase": "kullback-leibler_divergence"}, {"score": 0.0027491084541806823, "phrase": "auxiliary_target"}, {"score": 0.002719325177422626, "phrase": "instrumental_distributions"}, {"score": 0.0024253827365474734, "phrase": "whole_particle_sample"}, {"score": 0.002235040467016128, "phrase": "simulation_study"}, {"score": 0.002128067018576581, "phrase": "optimal_filtering"}, {"score": 0.0021049977753042253, "phrase": "nonlinear_state-space_models"}], "paper_keywords": ["Optimal proposal kernel", " Adaptive algorithms", " Kullback-Leibler divergence", " Coefficient of variation", " Expectation-maximisation", " Particle filter", " Sequential Monte Carlo", " Shannon entropy"], "paper_abstract": "Appropriately designing the proposal kernel of particle filters is an issue of significant importance, since a bad choice may lead to deterioration of the particle sample and, consequently, waste of computational power. In this paper we introduce a novel algorithm adaptively approximating the so-called optimal proposal kernel by a mixture of integrated curved exponential distributions with logistic weights. This family of distributions, referred to as mixtures of experts, is broad enough to be used in the presence of multi-modality or strongly skewed distributions. The mixtures are fitted, via online-EM methods, to the optimal kernel through minimisation of the Kullback-Leibler divergence between the auxiliary target and instrumental distributions of the particle filter. At each iteration of the particle filter, the algorithm is required to solve only a single optimisation problem for the whole particle sample, yielding an algorithm with only linear complexity. In addition, we illustrate in a simulation study how the method can be successfully applied to optimal filtering in nonlinear state-space models.", "paper_title": "Adaptive sequential Monte Carlo by means of mixture of experts", "paper_id": "WOS:000334435400003"}