{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "recognition_of_on-premise"}, {"score": 0.00475924837336259, "phrase": "weakly_labeled_street_view_images"}, {"score": 0.0047224703862074665, "phrase": "camera-enabled_mobile_devices"}, {"score": 0.004631758267671427, "phrase": "interaction_platforms"}, {"score": 0.004560438999891081, "phrase": "user's_virtual_and_physical_worlds"}, {"score": 0.004525190364108967, "phrase": "numerous_research_and_commercial_applications"}, {"score": 0.004421063493736839, "phrase": "augmented_reality_interface"}, {"score": 0.004386887374401584, "phrase": "mobile_information_retrieval"}, {"score": 0.004252793268613575, "phrase": "key_technique"}, {"score": 0.004219912413995784, "phrase": "daily_life_visual_object_recognition"}, {"score": 0.003996727479807572, "phrase": "commercial_advertising"}, {"score": 0.0038148090091532933, "phrase": "great_visual_diversity"}, {"score": 0.0037125240439927114, "phrase": "arbitrary_size"}, {"score": 0.0036411406244705557, "phrase": "complex_environmental_conditions"}, {"score": 0.0033690254452595865, "phrase": "existing_image_data_sets"}, {"score": 0.0032406665327678616, "phrase": "ops_data_set"}, {"score": 0.003069112841132911, "phrase": "google's_street_view"}, {"score": 0.002963640433319524, "phrase": "real-world_ops_learning"}, {"score": 0.002884111441153185, "phrase": "probabilistic_framework"}, {"score": 0.002839625463110821, "phrase": "distributional_clustering"}, {"score": 0.002742017872865778, "phrase": "distributional_information"}, {"score": 0.0027102313244646934, "phrase": "visual_feature"}, {"score": 0.002617059777997427, "phrase": "reliable_selection_criterion"}, {"score": 0.0025867180355950816, "phrase": "discriminative_ops_models"}, {"score": 0.0024496978447849835, "phrase": "state-of-the-art_probabilistic_latent_semantic_analysis_models"}, {"score": 0.002310915790114425, "phrase": "average_recognition_rate"}, {"score": 0.0021715177915174375, "phrase": "parallel_fashion"}, {"score": 0.0021049977753042253, "phrase": "large-scale_multimedia_applications"}], "paper_keywords": ["Real-world objects", " street view scenes", " learning and recognition", " object image data set"], "paper_abstract": "Camera-enabled mobile devices are commonly used as interaction platforms for linking the user's virtual and physical worlds in numerous research and commercial applications, such as serving an augmented reality interface for mobile information retrieval. The various application scenarios give rise to a key technique of daily life visual object recognition. On-premise signs (OPSs), a popular form of commercial advertising, are widely used in our living life. The OPSs often exhibit great visual diversity (e. g., appearing in arbitrary size), accompanied with complex environmental conditions (e. g., foreground and background clutter). Observing that such real-world characteristics are lacking in most of the existing image data sets, in this paper, we first proposed an OPS data set, namely OPS-62, in which totally 4649 OPS images of 62 different businesses are collected from Google's Street View. Further, for addressing the problem of real-world OPS learning and recognition, we developed a probabilistic framework based on the distributional clustering, in which we proposed to exploit the distributional information of each visual feature (the distribution of its associated OPS labels) as a reliable selection criterion for building discriminative OPS models. Experiments on the OPS-62 data set demonstrated the outperformance of our approach over the state-of-the-art probabilistic latent semantic analysis models for more accurate recognitions and less false alarms, with a significant 151.28% relative improvement in the average recognition rate. Meanwhile, our approach is simple, linear, and can be executed in a parallel fashion, making it practical and scalable for large-scale multimedia applications.", "paper_title": "Learning and Recognition of On-Premise Signs From Weakly Labeled Street View Images", "paper_id": "WOS:000331196900005"}