{"auto_keywords": [{"score": 0.03414229704911109, "phrase": "random_subspace_methods_ensembles"}, {"score": 0.00481495049065317, "phrase": "boosting_and_random_subspace_ensembles"}, {"score": 0.004437613070853689, "phrase": "random_subspace_methods"}, {"score": 0.004347972379250724, "phrase": "well_known_re-sampling_ensemble_methods"}, {"score": 0.002949598537993663, "phrase": "averaging_methodology"}, {"score": 0.0028026735035370206, "phrase": "final_prediction"}, {"score": 0.0026091594819333654, "phrase": "simple_bagging"}, {"score": 0.002261204313587616, "phrase": "standard_benchmark_datasets"}, {"score": 0.0021928902836049384, "phrase": "proposed_technique"}, {"score": 0.0021484955960643167, "phrase": "better_correlation_coefficient"}], "paper_keywords": ["Machine learning", " Data mining", " Regression"], "paper_abstract": "Bagging, boosting and random subspace methods are well known re-sampling ensemble methods that generate and combine a diversity of learners using the same learning algorithm for the base-regressor. In this work, we built an ensemble of bagging, boosting and random subspace methods ensembles with 8 sub-regressors in each one and then an averaging methodology is used for the final prediction. We performed a comparison with simple bagging, boosting and random subspace methods ensembles with 25 sub-regressors, as well as other well known combining methods, on standard benchmark datasets and the proposed technique had better correlation coefficient in most cases.", "paper_title": "COMBINING BAGGING, BOOSTING AND RANDOM SUBSPACE ENSEMBLES FOR REGRESSION PROBLEMS", "paper_id": "WOS:000305161100006"}