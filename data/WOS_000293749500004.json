{"auto_keywords": [{"score": 0.039745155174602975, "phrase": "unlabeled_data"}, {"score": 0.010612387000973441, "phrase": "semisupervised_generalized_discriminant_analysis"}, {"score": 0.010415010219316328, "phrase": "gda"}, {"score": 0.004769702911065878, "phrase": "generalized_discriminant_analysis"}, {"score": 0.0046364836476864325, "phrase": "commonly_used_method"}, {"score": 0.004592905436175987, "phrase": "dimensionality_reduction"}, {"score": 0.004422632150619634, "phrase": "nonlinear_projection"}, {"score": 0.0043194180167921165, "phrase": "between-class_dissimilarity"}, {"score": 0.004139639802595566, "phrase": "class_separability"}, {"score": 0.004081385289516914, "phrase": "real-world_applications"}, {"score": 0.0036958398682001015, "phrase": "large_quantities"}, {"score": 0.0035085965086415474, "phrase": "novel_gda_algorithm"}, {"score": 0.0032529910932231924, "phrase": "optimality_criterion"}, {"score": 0.0031174595932694036, "phrase": "optimization_problem"}, {"score": 0.003030247482854539, "phrase": "constrained_concave-convex_procedure"}, {"score": 0.002890266318361907, "phrase": "class_labels"}, {"score": 0.002782938683493689, "phrase": "novel_confidence_measure"}, {"score": 0.0026922926476971453, "phrase": "unlabeled_data_points"}, {"score": 0.0026169435194789772, "phrase": "high_confidence"}, {"score": 0.002580061425699194, "phrase": "selected_unlabeled_data"}, {"score": 0.002426137929553722, "phrase": "gda."}, {"score": 0.0023470862747075228, "phrase": "ssgda"}, {"score": 0.0022385926290967263, "phrase": "manifold_assumption"}, {"score": 0.002175913488493802, "phrase": "extensive_experiments"}], "paper_keywords": ["Constrained concave-convex procedure", " dimensionality reduction", " generalized discriminant analysis", " semisupervised learning"], "paper_abstract": "Generalized discriminant analysis (GDA) is a commonly used method for dimensionality reduction. In its general form, it seeks a nonlinear projection that simultaneously maximizes the between-class dissimilarity and minimizes the within-class dissimilarity to increase class separability. In real-world applications where labeled data are scarce, GDA may not work very well. However, unlabeled data are often available in large quantities at very low cost. In this paper, we propose a novel GDA algorithm which is abbreviated as semisupervised generalized discriminant analysis (SSGDA). We utilize unlabeled data to maximize an optimality criterion of GDA and formulate the problem as an optimization problem that is solved using the constrained concave-convex procedure. The optimization procedure leads to estimation of the class labels for the unlabeled data. We propose a novel confidence measure and a method for selecting those unlabeled data points whose labels are estimated with high confidence. The selected unlabeled data can then be used to augment the original labeled dataset for performing GDA. We also propose a variant of SSGDA, called M-SSGDA, which adopts the manifold assumption to utilize the unlabeled data. Extensive experiments on many benchmark datasets demonstrate the effectiveness of our proposed methods.", "paper_title": "Semisupervised Generalized Discriminant Analysis", "paper_id": "WOS:000293749500004"}