{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "weak_learners"}, {"score": 0.004747485328395149, "phrase": "adaboost"}, {"score": 0.004392565073466714, "phrase": "adaboost_algorithms"}, {"score": 0.0038407203087210775, "phrase": "new_adaboost_algorithm"}, {"score": 0.0033579708256295847, "phrase": "novel_distance"}, {"score": 0.0029357204796552653, "phrase": "generalization_capability_metric"}, {"score": 0.0027741761880779535, "phrase": "error_rate"}, {"score": 0.0023408288327263294, "phrase": "kernel-based_perceptron"}, {"score": 0.002275477757478827, "phrase": "synthetic_and_real_scene_data_sets"}, {"score": 0.002196342933346204, "phrase": "conventional_adaboost"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Adaboost algorithm", " Distance related criterion", " Kernel-based perceptron"], "paper_abstract": "Despite of its great success, two key problems are still unresolved for AdaBoost algorithms: how to select the most discriminative weak learners and how to optimally combine them. In this paper, a new AdaBoost algorithm is proposed to make improvement in the two aspects. First, we select the most discriminative weak learners by minimizing a novel distance related criterion, i.e., error-degree-weighted training error metric (ETEM) together with generalization capability metric (GCM), rather than training error rate only. Second, after getting the coefficients that are set empirically, we combine the weak learners optimally by tuning the coefficients using kernel-based perceptron. Experiments with synthetic and real scene data sets show our algorithm outperforms conventional AdaBoost. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "On selection and combination of weak learners in AdaBoost", "paper_id": "WOS:000278186200026"}