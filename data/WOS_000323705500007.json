{"auto_keywords": [{"score": 0.04945343670728552, "phrase": "deep_web_sources"}, {"score": 0.04240035589136514, "phrase": "deep_web"}, {"score": 0.029219690623003284, "phrase": "sourcerank"}, {"score": 0.00481782331106142, "phrase": "relevance"}, {"score": 0.00468991337957911, "phrase": "inter-source_agreement"}, {"score": 0.004665296491645837, "phrase": "deep_web_search_engines"}, {"score": 0.004628612093713039, "phrase": "formidable_challenge"}, {"score": 0.004592214823540993, "phrase": "high-quality_results"}, {"score": 0.004556102454638294, "phrase": "vast_collection"}, {"score": 0.0045321847286888435, "phrase": "searchable_databases"}, {"score": 0.004508391992630655, "phrase": "deep_web_search"}, {"score": 0.004472935829694716, "phrase": "two-step_process"}, {"score": 0.004426092397211699, "phrase": "high-quality_sources"}, {"score": 0.004345288603071002, "phrase": "selected_sources"}, {"score": 0.004288472456976113, "phrase": "existing_methods"}, {"score": 0.004111584914709491, "phrase": "query-result_similarity"}, {"score": 0.003789282090309032, "phrase": "query-based_relevance"}, {"score": 0.003594831725408056, "phrase": "open_collections"}, {"score": 0.003226744277130179, "phrase": "source_quality"}, {"score": 0.002989313352047128, "phrase": "possible_collusion"}, {"score": 0.0029424082747871927, "phrase": "adjusted_agreement"}, {"score": 0.0028357972357118118, "phrase": "agreement_graph"}, {"score": 0.002813457986293993, "phrase": "quality_score"}, {"score": 0.002711506442590442, "phrase": "stationary_visit_probability"}, {"score": 0.002690143663703648, "phrase": "random_walk"}, {"score": 0.0026689487424251907, "phrase": "ranking_results"}, {"score": 0.0026339933320302725, "phrase": "second-order_agreement"}, {"score": 0.002498678935673443, "phrase": "query_domains"}, {"score": 0.0024855348966965566, "phrase": "multiple_domain-specific_rankings"}, {"score": 0.002395437787820304, "phrase": "final_ranking"}, {"score": 0.00237029943579966, "phrase": "extensive_evaluations"}, {"score": 0.002333084527546362, "phrase": "google_base_sources"}, {"score": 0.002242577809454225, "phrase": "deep_web_search_engine"}, {"score": 0.0022015486263149897, "phrase": "agreement_analysis"}, {"score": 0.0021899641786881337, "phrase": "source_corruption"}, {"score": 0.0021217237206326914, "phrase": "google_base"}], "paper_keywords": ["Algorithms", " Experimentation", " Deep web search", " web trust", " source rank", " web database search", " deep web integration", " database integration", " agreement analysis"], "paper_abstract": "Deep web search engines face the formidable challenge of retrieving high-quality results from the vast collection of searchable databases. Deep web search is a two-step process of selecting the high-quality sources and ranking the results from the selected sources. Though there are existing methods for both the steps, they assess the relevance of the sources and the results using the query-result similarity. When applied to the deep web these methods have two deficiencies. First is that they are agnostic to the correctness (trustworthiness) of the results. Second, the query-based relevance does not consider the importance of the results and sources. These two considerations are essential for the deep web and open collections in general. Since a number of deep web sources provide answers to any query, we conjuncture that the agreements between these answers are helpful in assessing the importance and the trustworthiness of the sources and the results. For assessing source quality, we compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for the possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source, that we call SourceRank, is calculated as the stationary visit probability of a random walk. For ranking results, we analyze the second-order agreement between the results. Further extending SourceRank to multidomain search, we propose a source ranking sensitive to the query domains. Multiple domain-specific rankings of a source are computed, and these ranks are combined for the final ranking. We perform extensive evaluations on online and hundreds of Google Base sources spanning across domains. The proposed result and source rankings are implemented in the deep web search engine Factal. We demonstrate that the agreement analysis tracks source corruption. Further, our relevance evaluations show that our methods improve precision significantly over Google Base and the other baseline methods. The result ranking and the domain-specific source ranking are evaluated separately.", "paper_title": "Assessing Relevance and Trust of the Deep Web Sources and Results Based on Inter-Source Agreement", "paper_id": "WOS:000323705500007"}