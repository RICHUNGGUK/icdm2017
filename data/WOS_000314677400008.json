{"auto_keywords": [{"score": 0.04580340075872884, "phrase": "gc_process"}, {"score": 0.014523887564457614, "phrase": "free_blocks"}, {"score": 0.004751216062917875, "phrase": "hard_disks"}, {"score": 0.004709193603747991, "phrase": "flash_devices"}, {"score": 0.004667541072056214, "phrase": "out-of-place_updates_operations"}, {"score": 0.004327981760540394, "phrase": "major_cause"}, {"score": 0.004289686492089576, "phrase": "performance_degradation"}, {"score": 0.004158290806404304, "phrase": "internal_bandwidth"}, {"score": 0.004048860957751714, "phrase": "invalid_pages"}, {"score": 0.003838531755488468, "phrase": "low_watermark"}, {"score": 0.0036715973701254823, "phrase": "different_intervals"}, {"score": 0.0034807976670686628, "phrase": "workload_characteristics"}, {"score": 0.0032852337992126564, "phrase": "semipreemptible_gc"}, {"score": 0.0031845067058084583, "phrase": "gc_processing"}, {"score": 0.002952516738708181, "phrase": "flash_performance"}, {"score": 0.0029133627624098064, "phrase": "internal_gc_operations"}, {"score": 0.0027495914857929584, "phrase": "semi-pgc_scheme"}, {"score": 0.0026771333996253783, "phrase": "improved_performance"}, {"score": 0.0026534064006291853, "phrase": "reduced_performance_variability"}, {"score": 0.0026182085765020548, "phrase": "-dominant_workloads"}, {"score": 0.002537880772794665, "phrase": "average_response_time"}, {"score": 0.002482013330276732, "phrase": "response_time"}, {"score": 0.002438203996977464, "phrase": "non-pgc_scheme"}, {"score": 0.00233202607446404, "phrase": "new_nand_flash_device"}, {"score": 0.002200861826643147, "phrase": "fully_pgc"}, {"score": 0.0021049977753042253, "phrase": "f-pgc_enabled_flash_device_show"}], "paper_keywords": ["Flash memory", " garbage collection (GC)", " I/O scheduling", " preemptive I/O", " solid-state drives (SSDs)", " storage systems"], "paper_abstract": "Unlike hard disks, flash devices use out-of-place updates operations and require a garbage collection (GC) process to reclaim invalid pages to create free blocks. This GC process is a major cause of performance degradation when running concurrently with other I/O operations as internal bandwidth is consumed to reclaim these invalid pages. The invocation of the GC process is generally governed by a low watermark on free blocks and other internal device metrics that different workloads meet at different intervals. This results in an I/O performance that is highly dependent on workload characteristics. In this paper, we examine the GC process and propose a semipreemptible GC (PGC) scheme that allows GC processing to be preempted while pending I/O requests in the queue are serviced. Moreover, we further enhance flash performance by pipelining internal GC operations and merge them with pending I/O requests whenever possible. Our experimental evaluation of this semi-PGC scheme with realistic workloads demonstrates both improved performance and reduced performance variability. Write-dominant workloads show up to a 66.56% improvement in average response time with a 83.30% reduced variance in response time compared to the non-PGC scheme. In addition, we explore opportunities of a new NAND flash device that supports suspend/resume commands for read, write, and erase operations for fully PGC (F-PGC). Our experiments with an F-PGC enabled flash device show that request response time can be improved by up to 14.57% compared to semi-PGC.", "paper_title": "Preemptible I/O Scheduling of Garbage Collection for Solid State Drives", "paper_id": "WOS:000314677400008"}