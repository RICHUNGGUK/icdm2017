{"auto_keywords": [{"score": 0.0330444258052794, "phrase": "variational_distance"}, {"score": 0.029684859955805155, "phrase": "upper_bound"}, {"score": 0.004815410932411291, "phrase": "fano"}, {"score": 0.004598933498016774, "phrase": "upper_and_lower_bounds"}, {"score": 0.004273196830970426, "phrase": "absolute_value"}, {"score": 0.0036218682585355895, "phrase": "discrete_random_variables"}, {"score": 0.0030414140732789186, "phrase": "probability_distributions"}, {"score": 0.00241658945942583, "phrase": "special_cases"}, {"score": 0.0021049977753042253, "phrase": "lower_bound"}], "paper_keywords": [""], "paper_abstract": "Some upper and lower bounds are obtained for the maximum of the absolute value of the difference between the mutual information |I(X; Y) - I(X'; Y')| of two pairs of discrete random variables (X, Y) and (X', Y') via the variational distance between the probability distributions of these pairs. In particular, the upper bound obtained here substantially generalizes and improves the upper bound of [1]. In some special cases, our upper and lower bounds coincide or are rather close. It is also proved that the lower bound is asymptotically tight in the case where the variational distance between (X, Y) and (X' Y') tends to zero.", "paper_title": "Mutual Information, Variation, and Fano's Inequality", "paper_id": "WOS:000260150400002"}