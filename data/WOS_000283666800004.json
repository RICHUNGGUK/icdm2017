{"auto_keywords": [{"score": 0.049584128839210984, "phrase": "sensor_management"}, {"score": 0.00481495049065317, "phrase": "bandit_solutions"}, {"score": 0.004667149874736771, "phrase": "information-rich_and_dynamic_environments"}, {"score": 0.004523865550323285, "phrase": "sequential_action_selection_problem"}, {"score": 0.004272456401437226, "phrase": "dynamic_multi-armed_bandit"}, {"score": 0.004055998173160551, "phrase": "multi-armed_bandit"}, {"score": 0.003993216345115755, "phrase": "expected_rewards"}, {"score": 0.0039519004610213595, "phrase": "time-varying_linear_functions"}, {"score": 0.0038106195546165574, "phrase": "learning_goal"}, {"score": 0.0036553070575015344, "phrase": "optimal_action"}, {"score": 0.0034700069041302003, "phrase": "covariate_space"}, {"score": 0.0032430426853131346, "phrase": "precise_nature"}, {"score": 0.0030625949549620475, "phrase": "sensor_manager"}, {"score": 0.003015143226706584, "phrase": "evolving_environment"}, {"score": 0.0028771413440398614, "phrase": "selected_actions"}, {"score": 0.002759769970554556, "phrase": "static_problems"}, {"score": 0.0026888503045081505, "phrase": "exploitation-exploration_dilemma"}, {"score": 0.002579140290642792, "phrase": "different_factors"}, {"score": 0.002499798127196099, "phrase": "interesting_insights"}, {"score": 0.002422890839801611, "phrase": "environment_dynamics"}, {"score": 0.002385326947545194, "phrase": "action_selection_problem"}, {"score": 0.002324007171619245, "phrase": "covariate_dimensionality"}, {"score": 0.0022642601698943687, "phrase": "surprising_result"}, {"score": 0.0021049977753042253, "phrase": "dynamic_environments"}], "paper_keywords": ["multi-armed bandits with covariates", " sensor management", " action-selection", " dynamic environment"], "paper_abstract": "Sensor management in information-rich and dynamic environments can be posed as a sequential action selection problem with side information. To study such problems we employ the dynamic multi-armed bandit with covariates framework. In this generalization of the multi-armed bandit, the expected rewards are time-varying linear functions of the covariate vector. The learning goal is to associate the covariate with the optimal action at each instance, essentially learning to partition the covariate space adaptively. Applications of sensor management are frequently in environments in which the precise nature of the dynamics is unknown. In such settings, the sensor manager tracks the evolving environment by observing only the covariates and the consequences of the selected actions. This creates difficulties not encountered in static problems, and changes the exploitation-exploration dilemma. We study the relationship between the different factors of the problem and provide interesting insights. The impact of the environment dynamics on the action selection problem is influenced by the covariate dimensionality. We present the surprising result that strategies that perform very little or no exploration perform surprisingly well in dynamic environments.", "paper_title": "Prospects for Bandit Solutions in Sensor Management", "paper_id": "WOS:000283666800004"}