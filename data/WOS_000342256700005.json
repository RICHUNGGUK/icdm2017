{"auto_keywords": [{"score": 0.042654484177158294, "phrase": "pain_expression"}, {"score": 0.03867712206987224, "phrase": "mil"}, {"score": 0.03359376477981844, "phrase": "extensive_experiments"}, {"score": 0.00481495049065317, "phrase": "pain_localization"}, {"score": 0.004788592035020304, "phrase": "multiple_segment_representation"}, {"score": 0.004762377182821785, "phrase": "automatic_pain_recognition"}, {"score": 0.004697463242638162, "phrase": "vital_clinical_application"}, {"score": 0.004595427853257205, "phrase": "interesting_challenges"}, {"score": 0.004570265610324106, "phrase": "automatic_facial_expression_recognition"}, {"score": 0.004495598802612427, "phrase": "previous_pain"}, {"score": 0.004458722048888919, "phrase": "-pain_systems"}, {"score": 0.004232062908751438, "phrase": "target_expression"}, {"score": 0.0038973170902076707, "phrase": "novel_framework"}, {"score": 0.0037297085404352347, "phrase": "multiple_segments"}, {"score": 0.003699090930502201, "phrase": "multiple_instance_learning"}, {"score": 0.003608732492631878, "phrase": "weakly_labeled_data"}, {"score": 0.0035594871529132195, "phrase": "sequence_level_ground-truth"}, {"score": 0.0034916666462352893, "phrase": "multiple_clustering"}, {"score": 0.0034251339122991094, "phrase": "multi-scale_temporal_scanning_window"}, {"score": 0.0032508456537917673, "phrase": "'concept_frames"}, {"score": 0.0032241463631279975, "phrase": "'concept_segments"}, {"score": 0.003043272957851679, "phrase": "key_advantages"}, {"score": 0.0029688865799057814, "phrase": "painful_frames"}, {"score": 0.0028963131376095745, "phrase": "temporal_dynamics"}, {"score": 0.002841092355583839, "phrase": "individual_frames"}, {"score": 0.002703869368554536, "phrase": "uncertain_temporal_location"}, {"score": 0.0026377570446517004, "phrase": "unbc-mcmaster_shoulder_pain_dataset"}, {"score": 0.002573257062541549, "phrase": "competitive_results"}, {"score": 0.00253810599185473, "phrase": "pain_classification"}, {"score": 0.0024489385825401536, "phrase": "different_components"}, {"score": 0.0023824806252532352, "phrase": "discriminative_facial_patches"}, {"score": 0.0023564039938288217, "phrase": "pain_detection"}, {"score": 0.002292451418825592, "phrase": "action_units"}, {"score": 0.0021816694697063843, "phrase": "significant_improvement"}, {"score": 0.002163732292308364, "phrase": "spontaneous_facial_expression_dataset"}, {"score": 0.002145942273032774, "phrase": "feedtum"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Emotion classification", " Action classification", " Pain", " Temporal segmentation", " Bag of Words", " Weakly supervised learning", " Boosting", " Bagging"], "paper_abstract": "Automatic pain recognition from videos is a vital clinical application and, owing to its spontaneous nature, poses interesting challenges to automatic facial expression recognition (AFER) research. Previous pain vs no-pain systems have highlighted two major challenges: (1) ground truth is provided for the sequence, but the presence or absence of the target expression for a given frame is unknown, and (2) the time point and the duration of the pain expression event(s) in each video are unknown. To address these issues we propose a novel framework (referred to as MS-MIL) where each sequence is represented as a bag containing multiple segments, and multiple instance learning (MIL) is employed to handle this weakly labeled data in the form of sequence level ground-truth. These segments are generated via multiple clustering of a sequence or running a multi-scale temporal scanning window, and are represented using a state-of-the-art Bag of Words (BoW) representation. This work extends the idea of detecting facial expressions through 'concept frames' to 'concept segments' and argues through extensive experiments that algorithms such as MIL are needed to reap the benefits of such representation. The key advantages of our approach are: (1) joint detection and localization of painful frames using only sequence-level ground-truth, (2) incorporation of temporal dynamics by representing the data not as individual frames but as segments, and (3) extraction of multiple segments, which is well suited to signals with uncertain temporal location and duration in the video. Extensive experiments on UNBC-McMaster Shoulder Pain dataset highlight the effectiveness of the approach by achieving competitive results on both tasks of pain classification and localization in videos. We also empirically evaluate the contributions of different components of MS-MIL The paper also includes the visualization of discriminative facial patches, important for pain detection, as discovered by our algorithm and relates them to Action Units that have been associated with pain expression. We conclude the paper by demonstrating that MS-MIL yields a significant improvement on another spontaneous facial expression dataset, the FEEDTUM dataset. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Classification and weakly, supervised pain localization using multiple segment representation", "paper_id": "WOS:000342256700005"}