{"auto_keywords": [{"score": 0.036384452583045866, "phrase": "symbolic_facts"}, {"score": 0.004633888966463624, "phrase": "incoming_utterances"}, {"score": 0.004273601449537163, "phrase": "recurrent_neural_network_models"}, {"score": 0.0041835168653521, "phrase": "long_short-term_memory"}, {"score": 0.003991932110277654, "phrase": "functional_grasp"}, {"score": 0.003941215575554871, "phrase": "small_language"}, {"score": 0.003858110309630846, "phrase": "question_answering_game"}, {"score": 0.0037449520093281244, "phrase": "english"}, {"score": 0.0036813725040313002, "phrase": "temporal_streams"}, {"score": 0.003634587156159668, "phrase": "sound_patterns"}, {"score": 0.003409399407390298, "phrase": "simulated_visual_environment"}, {"score": 0.0032950412841674026, "phrase": "input_questions"}, {"score": 0.0032531496610470408, "phrase": "question_answer_pairs"}, {"score": 0.0029617414861784525, "phrase": "second_model"}, {"score": 0.0028623555849897632, "phrase": "full_english_sentences"}, {"score": 0.002825949329865926, "phrase": "latter_task"}, {"score": 0.0027078974645805736, "phrase": "human_language_learners"}, {"score": 0.002572709810193776, "phrase": "observable_auditory_input"}, {"score": 0.0025291601927956765, "phrase": "speech_signals"}, {"score": 0.0025076619172878945, "phrase": "intended_environmental_referents"}, {"score": 0.002215663255682209, "phrase": "highly_systematic_learned_representations"}, {"score": 0.002178144355363467, "phrase": "best_properties"}, {"score": 0.002159623311924437, "phrase": "distributed_and_symbolic_representations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Question answering", " Language comprehension", " Speech production", " Recurrent neural network", " Long short term memory"], "paper_abstract": "Two related and integral parts of linguistic competence are the ability to comprehend incoming utterances and the ability to respond to them appropriately. In this context, we present two biologically inspired recurrent neural network models, based on the long short-term memory (LSTM) architecture, each of which develops a functional grasp of a small language by participating in a question answering game. Both models receive questions in English, presented as temporal streams of speech sound patterns. As a further input, the models receive a set of symbolic facts about a simulated visual environment. The models learn to correctly answer input questions by observing question answer pairs produced by other participants. The first of our two models produces its answers as symbolic facts, demonstrating an ability to ground language. The second model learns by observation to produce its answers as full English sentences. This latter task in particular is closely analogous to that faced by human language learners, involving segmentation of morphemes, words, and phrases from observable auditory input, mapping of speech signals onto intended environmental referents, comprehension of questions, and content-addressed search capabilities for discovering the answers to these questions. Analysis of the models shows that their performance depends upon highly systematic learned representations that combine the best properties of distributed and symbolic representations. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Neural architectures for learning to answer questions", "paper_id": "WOS:000209359400004"}