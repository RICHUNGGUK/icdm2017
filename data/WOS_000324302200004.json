{"auto_keywords": [{"score": 0.03684411833285443, "phrase": "benchmark"}, {"score": 0.00481495049065317, "phrase": "oaei_benchmark_test_set"}, {"score": 0.004637478829869904, "phrase": "main_reference"}, {"score": 0.004529881123652277, "phrase": "ontology_matching_systems"}, {"score": 0.004424768776701965, "phrase": "test_set"}, {"score": 0.00422177384286051, "phrase": "relatively_easy_task"}, {"score": 0.003953068310598579, "phrase": "flexible_test_generator"}, {"score": 0.0038794730876377057, "phrase": "extensible_set"}, {"score": 0.003701401653850141, "phrase": "different_test_sets"}, {"score": 0.0036667768092524576, "phrase": "different_seed_ontologies"}, {"score": 0.0036324746826127997, "phrase": "different_alteration_modalities"}, {"score": 0.0034332736827447654, "phrase": "original_seed_ontology"}, {"score": 0.003306559295382181, "phrase": "remarkable_stability"}, {"score": 0.0032449610741097992, "phrase": "different_generations"}, {"score": 0.0031399038722901788, "phrase": "seed_ontologies"}, {"score": 0.003052557891280862, "phrase": "systematic_bias"}, {"score": 0.0030097975788892896, "phrase": "initial_benchmark_test_set"}, {"score": 0.002885066818322192, "phrase": "overall_winning_matcher"}, {"score": 0.0028312983170567948, "phrase": "exactly_the_properties"}, {"score": 0.002613709503583786, "phrase": "new_test_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Ontology matching", " Matching evaluation", " Test generation", " Semantic web"], "paper_abstract": "The OAEI Benchmark test set has been used for many years as a main reference to evaluate and compare ontology matching systems. However, this test set has barely varied since 2004 and has become a relatively easy task for matchers. In this paper, we present the design of a flexible test generator based on an extensible set of alterators which may be used programmatically for generating different test sets from different seed ontologies and different alteration modalities. It has been used for reproducing Benchmark both with the original seed ontology and with other ontologies. This highlights the remarkable stability of results over different generations and the preservation of difficulty across seed ontologies, as well as a systematic bias towards the initial Benchmark test set and the inability of such tests to identify an overall winning matcher. These were exactly the properties for which Benchmark had been designed. Furthermore, the generator has been used for providing new test sets aiming at increasing the difficulty and discriminability of Benchmark. Although difficulty may be easily increased with the generator, attempts to increase discriminability proved unfruitful. However, efforts towards this goal raise questions about the very nature of discriminability. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Ontology matching benchmarks: Generation, stability, and discriminability", "paper_id": "WOS:000324302200004"}