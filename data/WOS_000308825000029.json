{"auto_keywords": [{"score": 0.04490388164168672, "phrase": "feature_selection"}, {"score": 0.00481495049065317, "phrase": "unsupervised_maximum_margin_feature_selection_algorithm"}, {"score": 0.0038332390893263844, "phrase": "coherent_framework"}, {"score": 0.003371508176308189, "phrase": "transformation_matrix"}, {"score": 0.0030949200330108156, "phrase": "data_samples"}, {"score": 0.0027610089418453614, "phrase": "convex_optimization_problem"}, {"score": 0.0026077803487003, "phrase": "iterative_algorithm"}, {"score": 0.0024281181691350085, "phrase": "optimal_solution"}, {"score": 0.002326304140277375, "phrase": "convergence_analysis"}, {"score": 0.0021049977753042253, "phrase": "experimental_results"}], "paper_keywords": ["Feature selection", " K-means clustering", " Maximum margin criterion", " Regularization"], "paper_abstract": "In this article, we present an unsupervised maximum margin feature selection algorithm via sparse constraints. The algorithm combines feature selection and K-means clustering into a coherent framework. L (2,1)-norm regularization is performed to the transformation matrix to enable feature selection across all data samples. Our method is equivalent to solving a convex optimization problem and is an iterative algorithm that converges to an optimal solution. The convergence analysis of our algorithm is also provided. Experimental results demonstrate the efficiency of our algorithm.", "paper_title": "Unsupervised maximum margin feature selection via L (2,1)-norm minimization", "paper_id": "WOS:000308825000029"}