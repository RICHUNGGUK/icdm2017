{"auto_keywords": [{"score": 0.03574736129897079, "phrase": "leverage-based_sampling"}, {"score": 0.015719716506582538, "phrase": "algorithmic_leveraging"}, {"score": 0.010584248600555356, "phrase": "observed_data"}, {"score": 0.004733347359490221, "phrase": "large-scale_data_sets"}, {"score": 0.004600389726895716, "phrase": "empirical_statistical_leverage_scores"}, {"score": 0.004561232179195321, "phrase": "importance_sampling_distribution"}, {"score": 0.0044967055264893184, "phrase": "algorithmic_leveraging_samples"}, {"score": 0.004445739135694758, "phrase": "data_matrices"}, {"score": 0.0043953478557600565, "phrase": "data_size"}, {"score": 0.004211393664694384, "phrase": "computational_efficiency"}, {"score": 0.0041636475197809825, "phrase": "matrix_problems"}, {"score": 0.004128192012388041, "phrase": "least-squares_approximation"}, {"score": 0.004104722208522451, "phrase": "least_absolute_deviations_approximation"}, {"score": 0.004069766428492073, "phrase": "low-rank_matrix_approximation"}, {"score": 0.003989351600022163, "phrase": "algorithmic_issues"}, {"score": 0.0039553744129764016, "phrase": "worst-case_running_times"}, {"score": 0.003877211065759289, "phrase": "high-quality_implementations"}, {"score": 0.003800586434296711, "phrase": "statistical_aspects"}, {"score": 0.0036727235025069828, "phrase": "simple_yet_effective_framework"}, {"score": 0.003631062193606885, "phrase": "statistical_properties"}, {"score": 0.0035189057079817285, "phrase": "linear_regression_model"}, {"score": 0.0034889212590758865, "phrase": "fixed_number"}, {"score": 0.003166384930533443, "phrase": "statistical_perspective"}, {"score": 0.002965229277927781, "phrase": "well-known_result"}, {"score": 0.002923214590192792, "phrase": "algorithmic_perspective"}, {"score": 0.002906575389963227, "phrase": "worst-case_analysis"}, {"score": 0.0028735797685358, "phrase": "uniformly_superior_worst-case_algorithmic_results"}, {"score": 0.002784755056588371, "phrase": "theoretical_results"}, {"score": 0.0026986685585382347, "phrase": "smaller_least-squares_problem"}, {"score": 0.002683304054823427, "phrase": "\"shrinkage\"_leverage_scores"}, {"score": 0.0025343769150179764, "phrase": "existing_leverage-based_methods"}, {"score": 0.0023936956017015696, "phrase": "good_predictor"}, {"score": 0.00238006327644733, "phrase": "practical_performance"}, {"score": 0.002366508404418069, "phrase": "existing_and_new_leverage-based_algorithms"}, {"score": 0.0023196699954340437, "phrase": "improved_performance"}, {"score": 0.0022479284902217006, "phrase": "original_algorithmic_leveraging_approach"}, {"score": 0.002209734300698144, "phrase": "improved_biases"}, {"score": 0.0021049977753042253, "phrase": "improved_unconditional_biases"}], "paper_keywords": ["randomized algorithm", " leverage scores", " subsampling", " least squares", " linear regression"], "paper_abstract": "One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method. In this paper, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with \"shrinkage\" leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). A detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets. The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance. For example, with the same computation reduction as in the original algorithmic leveraging approach, our proposed SLEV typically leads to improved biases and variances both unconditionally and conditionally (on the observed data), and our proposed LEVUNW typically yields improved unconditional biases and variances.", "paper_title": "A Statistical Perspective on Algorithmic Leveraging", "paper_id": "WOS:000369886300007"}