{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mesh_terms"}, {"score": 0.04413109051936228, "phrase": "mesh_indexing"}, {"score": 0.03679218229665449, "phrase": "mesh_main_headings"}, {"score": 0.004760765880314089, "phrase": "biomedical_articles"}, {"score": 0.004654210559456924, "phrase": "high_cost"}, {"score": 0.0046192222897398685, "phrase": "manual_curation"}, {"score": 0.004584495834677719, "phrase": "key_aspects"}, {"score": 0.004532892812505505, "phrase": "scientific_literature"}, {"score": 0.004498812433096501, "phrase": "automated_methods"}, {"score": 0.004267287100135339, "phrase": "novel_approach"}, {"score": 0.004171730509424374, "phrase": "challenging_task"}, {"score": 0.00409372989087314, "phrase": "medline_citations"}, {"score": 0.003971938976270902, "phrase": "previous_methods"}, {"score": 0.003942059405218598, "phrase": "automatic_mesh_term_assignment"}, {"score": 0.0038683363637006902, "phrase": "indexing_task"}, {"score": 0.0038247634489737142, "phrase": "ranking_problem"}, {"score": 0.00378167947598949, "phrase": "relevant_mesh_headings"}, {"score": 0.0036969566358089644, "phrase": "irrelevant_ones"}, {"score": 0.0033765627434550432, "phrase": "list_net_a_learning-to-rank_algorithm"}, {"score": 0.003226899588787952, "phrase": "previously_used_benchmark_set"}, {"score": 0.003154566400499823, "phrase": "larger_dataset"}, {"score": 0.0030606302942851027, "phrase": "benchmark_dataset"}, {"score": 0.0028919522313933525, "phrase": "average_precision"}, {"score": 0.0027017308406240563, "phrase": "statistically_significant_improvements"}, {"score": 0.0026435735748974393, "phrase": "map"}, {"score": 0.002552781867125423, "phrase": "similar_significant_improvements"}, {"score": 0.002495521662819317, "phrase": "larger_document"}, {"score": 0.0024673738215365104, "phrase": "conclusion_experimental_results"}, {"score": 0.0023225152126545067, "phrase": "practical_impact"}, {"score": 0.0022447938263410005, "phrase": "proposed_learning_framework"}, {"score": 0.002129020447793464, "phrase": "biomedical_domain"}, {"score": 0.0021049977753042253, "phrase": "data_sets"}], "paper_keywords": [""], "paper_abstract": "Background Due to the high cost of manual curation of key aspects from the scientific literature, automated methods for assisting this process are greatly desired. Here, we report a novel approach to facilitate MeSH indexing, a challenging task of assigning MeSH terms to MEDLINE citations for their archiving and retrieval. Methods Unlike previous methods for automatic MeSH term assignment, we reformulate the indexing task as a ranking problem such that relevant MeSH headings are ranked higher than those irrelevant ones. Specifically, for each document we retrieve 20 neighbor documents, obtain a list of MeSH main headings from neighbors, and rank the MeSH main headings using List Net a learning-to-rank algorithm. We trained our algorithm on 200 documents and tested on a previously used benchmark set of 200 documents and a larger dataset of 1000 documents. Results Tested on the benchmark dataset, our method achieved a precision of 0.390, recall of 0.712, and mean average precision (MAP) of 0.626. In comparison to the state of the art, we observe statistically significant improvements as large as 39% in MAP (p-value <0.001). Similar significant improvements were also obtained on the larger document set. Conclusion Experimental results show that our approach makes the most accurate MeSH predictions to date, which suggests its great potential in making a practical impact on MeSH indexing. Furthermore, as discussed the proposed learning framework is robust and can be adapted to many other similar tasks beyond MeSH indexing in the biomedical domain. All data sets are available at: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/indexing.", "paper_title": "Recommending MeSH terms for annotating biomedical articles", "paper_id": "WOS:000294572000021"}