{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "concept_drift"}, {"score": 0.004539005582217309, "phrase": "training_and_test_data"}, {"score": 0.0044426140044122545, "phrase": "different_probability_distributions"}, {"score": 0.0043482604833437735, "phrase": "fundamental_stationary_distribution_assumption"}, {"score": 0.004278807087784626, "phrase": "supervised_learning"}, {"score": 0.003969018740062018, "phrase": "classifier_performance"}, {"score": 0.0037213333316218522, "phrase": "classification_problems"}, {"score": 0.00366185681714204, "phrase": "class_priors"}, {"score": 0.0034332736827447654, "phrase": "conditional_data_densities"}, {"score": 0.0031504667809625344, "phrase": "unlabeled_test_data"}, {"score": 0.0030834712590249863, "phrase": "classifier_outputs"}, {"score": 0.003050508020268648, "phrase": "new_operating_conditions"}, {"score": 0.0030017207229983385, "phrase": "re-training_it"}, {"score": 0.002875391177926809, "phrase": "posterior_probability_model"}, {"score": 0.0025273383208209922, "phrase": "prior_subclass_probabilities"}, {"score": 0.0025003054002226965, "phrase": "experimental_results"}, {"score": 0.002460295872115956, "phrase": "neural_network_model"}, {"score": 0.0024339783012484032, "phrase": "synthetic_and_remote_sensing_practical_settings"}, {"score": 0.0023440589015485077, "phrase": "subclass_level"}, {"score": 0.0022941723937662927, "phrase": "better_adjustment"}, {"score": 0.0022574539115388075, "phrase": "new_operational_conditions"}, {"score": 0.002174041708084857, "phrase": "prior_changes"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Changing operational conditions", " Concept drift", " Imprecise class distribution", " Imprecise data distribution", " Supervised classification", " Posterior probability estimation", " Neural networks"], "paper_abstract": "We consider the problem of classification in environments where training and test data may come from different probability distributions. When the fundamental stationary distribution assumption made in supervised learning (and often not satisfied in practice) does not hold, the classifier performance may significantly deteriorate. Several proposals have been made to deal with classification problems where the class priors change after training, but they may fail when the class conditional data densities also change. To cope with this problem, we propose an algorithm that uses unlabeled test data to adapt the classifier outputs to new operating conditions, without re-training it. The algorithm is based on a posterior probability model with two main assumptions: (1) the classes may be decomposed in several (unknown) subclasses, and (2) all changes in data distributions arise from changes in prior subclass probabilities. Experimental results with a neural network model on synthetic and remote sensing practical settings show that the adaptation at the subclass level can get a better adjustment to the new operational conditions than the methods based on class prior changes. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Class and subclass probability re-estimation to adapt a classifier in the presence of concept drift", "paper_id": "WOS:000295106000025"}