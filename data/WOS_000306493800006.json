{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "size_selection"}, {"score": 0.004763895422211728, "phrase": "optimization_methods"}, {"score": 0.004713379149462482, "phrase": "machine_learning"}, {"score": 0.004492573860101133, "phrase": "varying_sample_sizes"}, {"score": 0.004444921827311577, "phrase": "batch-type_optimization_methods"}, {"score": 0.00439777299939725, "phrase": "large-scale_machine_learning_problems"}, {"score": 0.004236639288852276, "phrase": "paper_deals"}, {"score": 0.004169394481182493, "phrase": "delicate_issue"}, {"score": 0.0041251561573501455, "phrase": "dynamic_sample_selection"}, {"score": 0.003767517649628532, "phrase": "sample_size"}, {"score": 0.0037076911119175455, "phrase": "variance_estimates"}, {"score": 0.003571751184289622, "phrase": "batch_gradient"}, {"score": 0.003404244237372396, "phrase": "total_cost"}, {"score": 0.00335016692413066, "phrase": "gradient_method"}, {"score": 0.0032969458013463807, "phrase": "second_part"}, {"score": 0.0031930184711888867, "phrase": "practical_newton_method"}, {"score": 0.0031255543155487234, "phrase": "smaller_sample"}, {"score": 0.0030758904011907533, "phrase": "hessian_vector-products"}, {"score": 0.0028391271282216758, "phrase": "dynamic_sampling_technique"}, {"score": 0.0027495914857929584, "phrase": "paper_shifts"}, {"score": 0.0027058852678238632, "phrase": "third_part"}, {"score": 0.002537880772794665, "phrase": "sparse_solutions"}, {"score": 0.002470987929071079, "phrase": "newton-like_method"}, {"score": 0.0022806798371116698, "phrase": "zero_variables"}, {"score": 0.002185235873435714, "phrase": "subsampled_hessian_newton_iteration"}, {"score": 0.002150481043278758, "phrase": "free_variables"}, {"score": 0.0021276181317652163, "phrase": "numerical_tests"}, {"score": 0.0021049977753042253, "phrase": "speech_recognition_problems"}], "paper_keywords": [""], "paper_abstract": "This paper presents a methodology for using varying sample sizes in batch-type optimization methods for large-scale machine learning problems. The first part of the paper deals with the delicate issue of dynamic sample selection in the evaluation of the function and gradient. We propose a criterion for increasing the sample size based on variance estimates obtained during the computation of a batch gradient. We establish an complexity bound on the total cost of a gradient method. The second part of the paper describes a practical Newton method that uses a smaller sample to compute Hessian vector-products than to evaluate the function and the gradient, and that also employs a dynamic sampling technique. The focus of the paper shifts in the third part of the paper to L (1)-regularized problems designed to produce sparse solutions. We propose a Newton-like method that consists of two phases: a (minimalistic) gradient projection phase that identifies zero variables, and subspace phase that applies a subsampled Hessian Newton iteration in the free variables. Numerical tests on speech recognition problems illustrate the performance of the algorithms.", "paper_title": "Sample size selection in optimization methods for machine learning", "paper_id": "WOS:000306493800006"}