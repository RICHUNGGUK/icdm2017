{"auto_keywords": [{"score": 0.03251200191281709, "phrase": "test_generation_tools"}, {"score": 0.00481495049065317, "phrase": "automated_unit_test_generation"}, {"score": 0.004758987153057194, "phrase": "software_testers"}, {"score": 0.004594951819987416, "phrase": "automated_test_generation"}, {"score": 0.004471270242168067, "phrase": "test_data"}, {"score": 0.0044192837023190445, "phrase": "high_structural_coverage"}, {"score": 0.004071846628708832, "phrase": "test_oracle"}, {"score": 0.004024485331377682, "phrase": "test_input"}, {"score": 0.003885668358145471, "phrase": "generated_tests"}, {"score": 0.0034162643819889054, "phrase": "conclusive_evidence"}, {"score": 0.003298357357267836, "phrase": "limited_adoption"}, {"score": 0.0030986716686217768, "phrase": "practical_value"}, {"score": 0.002756180088265064, "phrase": "automated_unit_test_generation_tool"}, {"score": 0.0026506222660465126, "phrase": "tool_support"}, {"score": 0.0025892330566358503, "phrase": "commonly_applied_quality_metrics"}, {"score": 0.002559072326933071, "phrase": "code_coverage"}, {"score": 0.0023946630984825207, "phrase": "measurable_improvement"}, {"score": 0.0022146817879683204, "phrase": "research_community"}, {"score": 0.002129818297734379, "phrase": "future_work"}, {"score": 0.0021049977753042253, "phrase": "automated_test_generation_tools"}], "paper_keywords": ["Algorithms", " Experimentation", " Reliability", " Theory", " Unit testing", " automated test generation", " branch coverage", " empirical software engineering"], "paper_abstract": "Work on automated test generation has produced several tools capable of generating test data which achieves high structural coverage over a program. In the absence of a specification, developers are expected to manually construct or verify the test oracle for each test input. Nevertheless, it is assumed that these generated tests ease the task of testing for the developer, as testing is reduced to checking the results of tests. While this assumption has persisted for decades, there has been no conclusive evidence to date confirming it. However, the limited adoption in industry indicates this assumption may not be correct, and calls into question the practical value of test generation tools. To investigate this issue, we performed two controlled experiments comparing a total of 97 subjects split between writing tests manually and writing tests with the aid of an automated unit test generation tool, EVOSUITE. We found that, on one hand, tool support leads to clear improvements in commonly applied quality metrics such as code coverage (up to 300% increase). However, on the other hand, there was no measurable improvement in the number of bugs actually found by developers. Our results not only cast some doubt on how the research community evaluates test generation tools, but also point to improvements and future work necessary before automated test generation tools will be widely adopted by practitioners.", "paper_title": "Does Automated Unit Test Generation Really Help Software Testers? A Controlled Empirical Study", "paper_id": "WOS:000361066700003"}