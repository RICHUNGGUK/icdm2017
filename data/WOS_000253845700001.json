{"auto_keywords": [{"score": 0.03827907937304216, "phrase": "computational_complexity"}, {"score": 0.03185896123808046, "phrase": "proposed_method"}, {"score": 0.025500902621675058, "phrase": "fsdd"}, {"score": 0.00481495049065317, "phrase": "based_solution"}, {"score": 0.004712463245074863, "phrase": "feature_selection"}, {"score": 0.004612147352416241, "phrase": "optimal_subset"}, {"score": 0.004268293864953256, "phrase": "exhaustive_search_strategy"}, {"score": 0.004123781267600542, "phrase": "best_subset"}, {"score": 0.003816197777388722, "phrase": "considerably_high_computational_complexity"}, {"score": 0.003767203575060969, "phrase": "alternative_suboptimal_feature_selection_methods"}, {"score": 0.0035162927451540065, "phrase": "finally_selected_feature_subset"}, {"score": 0.0033971539563683174, "phrase": "new_feature_selection_algorithm"}, {"score": 0.0033391018638565715, "phrase": "distance_discriminant"}, {"score": 0.003157174936641705, "phrase": "high_computational_costs"}, {"score": 0.0030501665085457606, "phrase": "suboptimal_methods"}, {"score": 0.002934098412396701, "phrase": "optimal_feature_subset"}, {"score": 0.0029089095588612007, "phrase": "exhaustive_search"}, {"score": 0.0028591768694918, "phrase": "bound_algorithm"}, {"score": 0.0027982016314450717, "phrase": "optimal_feature_selection"}, {"score": 0.0027622407190589326, "phrase": "search_problem"}, {"score": 0.0026916956768363158, "phrase": "feature_ranking_problem"}, {"score": 0.002668582249630492, "phrase": "rigorous_theoretical_proof"}, {"score": 0.0024906610581090223, "phrase": "linear_transformation"}, {"score": 0.002437525632451029, "phrase": "diagonal_transformation_matrix"}, {"score": 0.0023548518827575, "phrase": "relieff"}, {"score": 0.0023346234464901978, "phrase": "mrmrmid"}, {"score": 0.002304606411453748, "phrase": "mutual_information"}, {"score": 0.002255431275670053, "phrase": "experiment_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["optimal feature selection", " distance discriminant", " feature ranking"], "paper_abstract": "The goal of feature selection is to find the optimal subset consisting of m features chosen from the total it features. One critical problem for many feature selection methods is that an exhaustive search strategy has to be applied to seek the best subset among all the possible ((n)(m)) feature subsets, which usually results in a considerably high computational complexity. The alternative suboptimal feature selection methods provide more practical solutions in terms of computational complexity but they cannot promise that the finally selected feature subset is globally optimal. We propose a new feature selection algorithm based on a distance discriminant (FSDD), which not only solves the problem of the high computational costs but also overcomes the drawbacks of the suboptimal methods. The proposed method is able to find the optimal feature subset without exhaustive search or Branch and Bound algorithm. The most difficult problem for optimal feature selection, the search problem, is converted into a feature ranking problem following rigorous theoretical proof such that the computational complexity can be greatly reduced. The proposed method is invariant to the linear transformation of data when a diagonal transformation matrix is applied. FSDD was compared with ReliefF and mrmrMID based on mutual information on 8 data sets. The experiment results show that FSDD outperforms the other two methods and is highly efficient. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Invariant optimal feature selection: A distance discriminant and feature ranking based solution", "paper_id": "WOS:000253845700001"}