{"auto_keywords": [{"score": 0.045923793841804846, "phrase": "ng"}, {"score": 0.00481495049065317, "phrase": "massive_data_sets"}, {"score": 0.004680624753255133, "phrase": "huge_data_sets"}, {"score": 0.004627944735913613, "phrase": "new_problems"}, {"score": 0.004575854901681043, "phrase": "popular_clustering"}, {"score": 0.004524348699657099, "phrase": "visualization_algorithms"}, {"score": 0.004448169552315403, "phrase": "neural_gas"}, {"score": 0.004156012879026324, "phrase": "memory_and_time_constraints"}, {"score": 0.0038610400240820307, "phrase": "data_points"}, {"score": 0.0037959868107895053, "phrase": "main_memory"}, {"score": 0.0035065523764817143, "phrase": "whole_data_set"}, {"score": 0.003370190834214415, "phrase": "feasible_training_time"}, {"score": 0.003239114833599948, "phrase": "single_pass_extensions"}, {"score": 0.0031845067058084583, "phrase": "classical_clustering_algorithms_ng"}, {"score": 0.0030433302702291116, "phrase": "simple_patch_decomposition"}, {"score": 0.0029249294679606656, "phrase": "batch_optimization_schemes"}, {"score": 0.0028756028995504035, "phrase": "underlying_cost_function"}, {"score": 0.002763709443278514, "phrase": "fixed_memory_space"}, {"score": 0.0026411384812121503, "phrase": "original_ones"}, {"score": 0.0026113524778702624, "phrase": "easy_implementation"}, {"score": 0.0025239898026543964, "phrase": "large_flexibility"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural gas", " Clustering streaming data", " Parallelization"], "paper_abstract": "The presence of huge data sets poses new problems to popular clustering and visualization algorithms such as neural gas (NG) and the self-organising-map (SOM) due to memory and time constraints. In such situations, it is no longer possible to store all data points in the main memory at once and only a few, ideally only one run over the whole data set is still affordable to achieve a feasible training time. In this contribution we propose single pass extensions of the classical clustering algorithms NG and SOM which are based on a simple patch decomposition of the data set and fast batch optimization schemes of the underlying cost function. The algorithms only require a fixed memory space. They maintain the benefits of the original ones including easy implementation and interpretation as well as large flexibility and adaptability. We demonstrate that parallelization of the methods becomes easily possible and we show the efficiency of the approach in a variety of experiments. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Patch clustering for massive data sets", "paper_id": "WOS:000264993200010"}