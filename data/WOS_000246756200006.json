{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "mobile_robots"}, {"score": 0.004526503177267299, "phrase": "novelty_detection"}, {"score": 0.00445712058827745, "phrase": "visual_input"}, {"score": 0.0043215158337835706, "phrase": "mobile_robot"}, {"score": 0.0038485929962942776, "phrase": "uncommon_visual_features"}, {"score": 0.0035899674590142653, "phrase": "great_importance"}, {"score": 0.0035076666315744525, "phrase": "robotic_exploration_and_inspection_tasks"}, {"score": 0.0032718761046588835, "phrase": "computational_and_attentional_resources"}, {"score": 0.0029361086955646625, "phrase": "proposed_system"}, {"score": 0.0028687545730861665, "phrase": "image_encoding_mechanism"}, {"score": 0.002802941205762953, "phrase": "local_colour_statistics"}, {"score": 0.002675797331220082, "phrase": "biologically-inspired_model"}, {"score": 0.0023278569001363263, "phrase": "image_transformations"}, {"score": 0.0022222152430170254, "phrase": "engineered_scenario"}], "paper_keywords": ["automated inspection", " mobile robotics", " on-line novelty detection", " real-time computer vision"], "paper_abstract": "We present a framework to perform novelty detection using visual input in which a mobile robot first learns a model of normality in its operating environment and later uses this to highlight uncommon visual features that may appear. This ability is of great importance for both robotic exploration and inspection tasks, because it enables the robot to allocate computational and attentional resources efficiently to those features which are novel. At the heart of the proposed system is the image encoding mechanism which uses local colour statistics from regions selected by a biologically-inspired model of visual attention. Our approach works in real-time with a wide, unrestricted field of view and is robust to image transformations. Experiments conducted in an engineered scenario demonstrate the efficiency and functionality of our method.", "paper_title": "Real-time automated visual inspection using mobile robots", "paper_id": "WOS:000246756200006"}