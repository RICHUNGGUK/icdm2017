{"auto_keywords": [{"score": 0.043612899193540264, "phrase": "classification_accuracy"}, {"score": 0.00481495049065317, "phrase": "nearest_neighbor_rule"}, {"score": 0.004743594110762319, "phrase": "new_rank_methods"}, {"score": 0.004650086856905758, "phrase": "best_prototypes"}, {"score": 0.004581162468419365, "phrase": "training_set"}, {"score": 0.004272724011826495, "phrase": "external_parameter"}, {"score": 0.004105850229681625, "phrase": "traditional_methods"}, {"score": 0.003945467978172845, "phrase": "classification_task"}, {"score": 0.003380791705551809, "phrase": "new_voting_methods"}, {"score": 0.003281186608301359, "phrase": "prototype_probability"}, {"score": 0.0031686717525002935, "phrase": "new_sample"}, {"score": 0.002925728709297618, "phrase": "relevance_factor"}, {"score": 0.0027834145465170292, "phrase": "best_candidates"}, {"score": 0.002337537183727731, "phrase": "different_high_dimensional_databases"}, {"score": 0.0022573076362326135, "phrase": "final_error_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Editing", " Condensing", " Rank methods", " Sorted prototypes selection"], "paper_abstract": "Some new rank methods to select the best prototypes from a training set are proposed in this paper in order to establish its size according to an external parameter, while maintaining the classification accuracy. The traditional methods that filter the training set in a classification task like editing or condensing have some rules that apply to the set in order to remove outliers or keep some prototypes that help in the classification. In our approach, new voting methods are proposed to compute the prototype probability and help to classify correctly a new sample. This probability is the key to sorting the training set out, so a relevance factor from 0 to 1 is used to select the best candidates for each class whose accumulated probabilities are less than that parameter. This approach makes it possible to select the number of prototypes necessary to maintain or even increase the classification accuracy. The results obtained in different high dimensional databases show that these methods maintain the final error rate while reducing the size of the training set. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "New rank methods for reducing the size of the training set using the nearest neighbor rule", "paper_id": "WOS:000301212400020"}