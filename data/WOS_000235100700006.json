{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "neural_networks"}, {"score": 0.00421410521089045, "phrase": "learning_neural_networks"}, {"score": 0.003942299471749811, "phrase": "sample_size"}, {"score": 0.0035908627899097407, "phrase": "almost-optimal_stochastic_approximation"}, {"score": 0.0030595112110658675, "phrase": "accuracy_confidence_function"}, {"score": 0.002824005136794786, "phrase": "least-squares_estimator"}, {"score": 0.0022806798371116698, "phrase": "smale's_network_problem"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["learning theory", " neural networks", " entropy", " stochastic approximation"], "paper_abstract": "We consider the problem of Learning Neural Networks from samples. The sample size which is sufficient for obtaining the almost-optimal stochastic approximation of function classes is obtained. In the terms of the accuracy confidence function, we show that the least-squares estimator is almost-optimal for the problem. These results can be used to solve Smale's network problem. (C) 2005 Elsevier Inc. All rights reserved.", "paper_title": "Approximation by neural networks and learning theory", "paper_id": "WOS:000235100700006"}