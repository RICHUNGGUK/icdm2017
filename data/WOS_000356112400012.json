{"auto_keywords": [{"score": 0.046448599314105624, "phrase": "neural_networks"}, {"score": 0.04515299348767124, "phrase": "six_component-trees"}, {"score": 0.00481495049065317, "phrase": "text_detection"}, {"score": 0.004767826806287385, "phrase": "natural_scene_images"}, {"score": 0.00462919213721188, "phrase": "robust_text_detection_approach"}, {"score": 0.004561386908745206, "phrase": "color-enhanced_contrasting_extremal_region"}, {"score": 0.004516745654465524, "phrase": "cer"}, {"score": 0.004363846362238565, "phrase": "color_natural_scene_image"}, {"score": 0.004154332348800334, "phrase": "saturation_channel_images"}, {"score": 0.004093453936415085, "phrase": "perception-based_illumination_invariant_color_space"}, {"score": 0.003858697684681798, "phrase": "color-enhanced_cers"}, {"score": 0.0037834649508375544, "phrase": "character_candidates"}, {"score": 0.0036914757355166966, "phrase": "\"divide-and-conquer\"_strategy"}, {"score": 0.003445591238183763, "phrase": "five_types"}, {"score": 0.0033786371988271547, "phrase": "long"}, {"score": 0.0030314419455512013, "phrase": "corresponding_neural_network"}, {"score": 0.002928682556953258, "phrase": "ambiguity-free_learning_strategy"}, {"score": 0.0028715299874549245, "phrase": "unambiguous_non-text_components"}, {"score": 0.0027200305495826797, "phrase": "remaining_components"}, {"score": 0.0026538282523433684, "phrase": "candidate_text-lines"}, {"score": 0.002416589459425828, "phrase": "post-processing_step"}, {"score": 0.002357755095722877, "phrase": "lost_characters"}, {"score": 0.0023117182131967523, "phrase": "superior_performance"}, {"score": 0.002233301265743633, "phrase": "reading_text"}, {"score": 0.0022113879114507577, "phrase": "scene_images"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Text detection", " Natural scene images", " Color-enhanced contrasting extremal region", " Neural networks"], "paper_abstract": "This paper presents a robust text detection approach based on color-enhanced contrasting extremal region (CER) and neural networks. Given a color natural scene image, six component-trees are built from its grayscale image, hue and saturation channel images in a perception-based illumination invariant color space, and their inverted images respectively. From each component-tree, color-enhanced CERs are extracted as character candidates. By using a \"divide-and-conquer\" strategy, each candidate image patch is labeled reliably by rules as one of five types, namely, Long, Thin, Fill, Square-large and Square-small, and classified as text or non-text by a corresponding neural network, which is trained by an ambiguity-free learning strategy. After pruning unambiguous non-text components, repeating components in each component-tree are pruned further. Remaining components are then grouped into candidate text-lines and verified by another set of neural networks. Finally, results from six component-trees are combined, and a post-processing step is used to recover lost characters. Our proposed method achieves superior performance on both ICDAR-2011 and ICDAR-2013 \"Reading Text in Scene Images\" \"test sets. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "A robust approach for text detection from natural scene images", "paper_id": "WOS:000356112400012"}