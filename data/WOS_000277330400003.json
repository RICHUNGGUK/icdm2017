{"auto_keywords": [{"score": 0.03761646802023329, "phrase": "bayesian"}, {"score": 0.005695077215822097, "phrase": "bc"}, {"score": 0.00481495049065317, "phrase": "emotion_recognition"}, {"score": 0.0047747348769918, "phrase": "optimum-path_forest_classification"}, {"score": 0.004656082803151484, "phrase": "new_method"}, {"score": 0.004559451202070545, "phrase": "spoken_emotions"}, {"score": 0.004408976280483648, "phrase": "glottal_airflow_signal"}, {"score": 0.00431049145004989, "phrase": "ope"}, {"score": 0.004281372589330329, "phrase": "new_optimum_path"}, {"score": 0.004122699822340296, "phrase": "six_other_previously_established_classification_methods"}, {"score": 0.004054071599735255, "phrase": "gaussian_mixture_model"}, {"score": 0.00396988421137599, "phrase": "support_vector_machine"}, {"score": 0.003887438245620965, "phrase": "artificial_neural_networks"}, {"score": 0.0038549394272661356, "phrase": "layer_perceptron"}, {"score": 0.003398714712611042, "phrase": "speech_database"}, {"score": 0.0032726467814278345, "phrase": "anechoic_environment"}, {"score": 0.003245270848972794, "phrase": "ten_speakers"}, {"score": 0.0031117775337824853, "phrase": "ten_sentences"}, {"score": 0.0029340342068081247, "phrase": "glottal_waveform"}, {"score": 0.002885135695263876, "phrase": "fluent_speech"}, {"score": 0.002860992017190221, "phrase": "inverse_filtering"}, {"score": 0.0028251537271620996, "phrase": "investigated_features"}, {"score": 0.002789763110395331, "phrase": "glottal_symmetry"}, {"score": 0.002766415324343602, "phrase": "mfcc_vectors"}, {"score": 0.0026737815780001305, "phrase": "gmm"}, {"score": 0.0026637384761565605, "phrase": "corresponding_speech_signal"}, {"score": 0.0026414426081951734, "phrase": "experimental_results"}, {"score": 0.0026083473154448326, "phrase": "best_performance"}, {"score": 0.002438699976481979, "phrase": "highest_recognition_rates"}, {"score": 0.0023481578019624843, "phrase": "glottal_and_speech_features_performance"}, {"score": 0.0022514836269627186, "phrase": "multi_speaker"}, {"score": 0.0022232633360792222, "phrase": "top_performing_classifiers"}, {"score": 0.002204646157481869, "phrase": "perfect_recognition_rates"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Emotion recognition", " Glottal analysis", " Speech analysis", " Optimum-path forest"], "paper_abstract": "A new method for the recognition of spoken emotions is presented based on features of the glottal airflow signal. Its effectiveness is tested on the new optimum path classifier (OPF) as well as on six other previously established classification methods that included the Gaussian mixture model (GMM), support vector machine (SVM), artificial neural networks multi layer perceptron (ANN-MLP), k-nearest neighbor rule (k-NN), Bayesian classifier (BC) and the C4.5 decision tree. The speech database used in this work was collected in an anechoic environment with ten speakers (5 M and 5 F) each speaking ten sentences in four different emotions: Happy, Angry, Sad, and Neutral. The glottal waveform was extracted from fluent speech via inverse filtering. The investigated features included the glottal symmetry and MFCC vectors of various lengths both for the glottal and the corresponding speech signal. Experimental results indicate that best performance is obtained for the glottal-only features with SVM and OPE generally providing the highest recognition rates, while for GMM or the combination of glottal and speech features performance was relatively inferior. For this text dependent, multi speaker task the top performing classifiers achieved perfect recognition rates for the case of 6th order glottal MFCCs. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Spoken emotion recognition through optimum-path forest classification using glottal features", "paper_id": "WOS:000277330400003"}