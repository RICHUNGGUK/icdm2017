{"auto_keywords": [{"score": 0.04619449822463329, "phrase": "llc"}, {"score": 0.03435629624166484, "phrase": "helm"}, {"score": 0.007642407492231216, "phrase": "heterogeneous_multicore_processors"}, {"score": 0.007431185109935679, "phrase": "gpu"}, {"score": 0.005860413168884213, "phrase": "memory_access_latency"}, {"score": 0.00481495049065317, "phrase": "shared_cache_management"}, {"score": 0.004778145748896516, "phrase": "heterogeneous_multicore_processor"}, {"score": 0.0047174256688408064, "phrase": "cpu_cores"}, {"score": 0.0046813628851623676, "phrase": "-parallel_accelerators"}, {"score": 0.004645574499882206, "phrase": "graphic_processing_unit"}, {"score": 0.004482129830695465, "phrase": "-chip_resources"}, {"score": 0.004447857748696647, "phrase": "shared_last-level_cache"}, {"score": 0.004247635679434271, "phrase": "shared_llc"}, {"score": 0.004119166170145527, "phrase": "significantly_higher_number"}, {"score": 0.004098133997961539, "phrase": "concurrent_threads"}, {"score": 0.0040253600867528, "phrase": "current_cache_management_policies"}, {"score": 0.0039945666268426175, "phrase": "cpu_applications'_share"}, {"score": 0.0038737214010638745, "phrase": "competing_gpu_applications"}, {"score": 0.003814671449620158, "phrase": "reduced_share"}, {"score": 0.003746912275953477, "phrase": "significant_performance_degradation"}, {"score": 0.0036897882079331887, "phrase": "gpu_applications"}, {"score": 0.0036057249778792388, "phrase": "sufficient_thread-level_parallelism"}, {"score": 0.0035677359347230293, "phrase": "off-chip_bandwidth_utilization"}, {"score": 0.0035235701594414624, "phrase": "performance_challenge"}, {"score": 0.003487658196780032, "phrase": "diverse_cores"}, {"score": 0.0034344729968020476, "phrase": "energy_consumption_profile"}, {"score": 0.0033648146343317562, "phrase": "energy_efficiency"}, {"score": 0.0032713278333553713, "phrase": "heterogeneous_llc_management"}, {"score": 0.0031399495795313504, "phrase": "gpu's_tolerance"}, {"score": 0.0030683745626994712, "phrase": "gpu_llc_accesses"}, {"score": 0.003044879531463441, "phrase": "llc_space"}, {"score": 0.0030293159059213755, "phrase": "cache-sensitive_cpu_applications"}, {"score": 0.0029678505123458578, "phrase": "gpu_accesses"}, {"score": 0.0029001870731706113, "phrase": "memory_access"}, {"score": 0.0028486252355914084, "phrase": "latency_tolerance"}, {"score": 0.0028268080886618345, "phrase": "gpu_application"}, {"score": 0.0027765484480496536, "phrase": "tlp"}, {"score": 0.0027132333865796687, "phrase": "average_number"}, {"score": 0.002637801970766725, "phrase": "baseline_configuration"}, {"score": 0.002584259877640948, "phrase": "existing_heterogeneous_processor_designs"}, {"score": 0.0024995563056109224, "phrase": "competing_policies"}, {"score": 0.0024238286657286864, "phrase": "varying_core_mix"}, {"score": 0.0023868275250400347, "phrase": "performance_benefit"}, {"score": 0.0023564240664681116, "phrase": "total_accesses"}, {"score": 0.00229088962675884, "phrase": "energy_consumption"}, {"score": 0.00227333434557518, "phrase": "llc_module"}, {"score": 0.002250135919199481, "phrase": "llc_bypassing"}, {"score": 0.002204445277617949, "phrase": "dram_energy_consumption"}, {"score": 0.0021652260317080244, "phrase": "better_energy_efficiency"}, {"score": 0.0021212577136009627, "phrase": "lru"}], "paper_keywords": ["Architecture", " Experimentation", " Performance", " Heterogeneous multicore", " cache management policy", " last-level cache", " bypassing"], "paper_abstract": "Heterogeneous multicore processors that integrate CPU cores and data-parallel accelerators such as graphic processing unit (GPU) cores onto the same die raise several new issues for sharing various on-chip resources. The shared last-level cache (LLC) is one of the most important shared resources due to its impact on performance. Accesses to the shared LLC in heterogeneous multicore processors can be dominated by the GPU due to the significantly higher number of concurrent threads supported by the architecture. Under current cache management policies, the CPU applications' share of the LLC can be significantly reduced in the presence of competing GPU applications. For many CPU applications, a reduced share of the LLC could lead to significant performance degradation. On the contrary, GPU applications can tolerate increase in memory access latency when there is sufficient thread-level parallelism (TLP). In addition to the performance challenge, introduction of diverse cores onto the same die changes the energy consumption profile and, in turn, affects the energy efficiency of the processor. In this work, we propose heterogeneous LLC management (HeLM), a novel shared LLC management policy that takes advantage of the GPU's tolerance for memory access latency. HeLM is able to throttle GPU LLC accesses and yield LLC space to cache-sensitive CPU applications. This throttling is achieved by allowing GPU accesses to bypass the LLC when an increase in memory access latency can be tolerated. The latency tolerance of a GPU application is determined by the availability of TLP, which is measured at runtime as the average number of threads that are available for issuing. For a baseline configuration with two CPU cores and four GPU cores, modeled after existing heterogeneous processor designs, HeLM outperforms least recently used (LRU) policy by 10.4%. Additionally, HeLM also outperforms competing policies. Our evaluations show that HeLM is able to sustain performance with varying core mix. In addition to the performance benefit, bypassing also reduces total accesses to the LLC, leading to a reduction in the energy consumption of the LLC module. However, LLC bypassing has the potential to increase off-chip bandwidth utilization and DRAM energy consumption. Our experiments show that HeLM exhibits better energy efficiency by reducing the ED2 by 18% over LRU while impacting only a 7% increase in off-chip bandwidth utilization.", "paper_title": "Performance-Energy Considerations for Shared Cache Management in a Heterogeneous Multicore Processor", "paper_id": "WOS:000353749300003"}