{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "mixture_models"}, {"score": 0.004724365740062236, "phrase": "simple_mcmc"}, {"score": 0.004548253531664032, "phrase": "mixture_model_likelihood_function"}, {"score": 0.0036207206487324506, "phrase": "classification_applications"}, {"score": 0.003419943984098415, "phrase": "likelihood_function"}, {"score": 0.003355515315708125, "phrase": "valid_inequality_constraints"}, {"score": 0.0027479040199262393, "phrase": "prediction_applications"}, {"score": 0.0024747842618794255, "phrase": "monte_carlo"}, {"score": 0.0022287497627555895, "phrase": "entire_posterior_distribution"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Bayesian", " classification", " labeling"], "paper_abstract": "The mixture model likelihood function is invariant with respect to permutation of the components of the mixture. If functions of interest are permutation sensitive, as in classification applications, then interpretation of the likelihood function requires valid inequality constraints and a very large sample may be required to resolve ambiguities. If functions of interest are permutation invariant, as in prediction applications, then there are no such problems of interpretation. Contrary to assessments in some recent publications, simple and widely used Markov chain Monte Carlo (MCMC) algorithms with data augmentation reliably recover the entire posterior distribution. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Interpretation and inference in mixture models: Simple MCMC works", "paper_id": "WOS:000245497700018"}