{"auto_keywords": [{"score": 0.03918716279113589, "phrase": "expected_amount"}, {"score": 0.00832454239345265, "phrase": "work_replication"}, {"score": 0.00481495049065317, "phrase": "unrecoverable_interruptions"}, {"score": 0.004739630859016419, "phrase": "large_workload"}, {"score": 0.0044922081848067135, "phrase": "remote_worker_computers"}, {"score": 0.00406083086983316, "phrase": "known_likelihood"}, {"score": 0.0034352452951903305, "phrase": "vulnerable_work"}, {"score": 0.003095257144856771, "phrase": "effective_remote_workforce"}, {"score": 0.0030660649234990284, "phrase": "current_study"}, {"score": 0.002788823648033457, "phrase": "asymptotic_sense"}, {"score": 0.002601535991493226, "phrase": "exact_maximization"}, {"score": 0.002520604609793834, "phrase": "multiple_workers"}, {"score": 0.002434477906829832, "phrase": "work-killing_interruptions"}, {"score": 0.0023661988912166234, "phrase": "efficient_heuristics"}, {"score": 0.0022637650216685906, "phrase": "work-_killing_interruptions"}, {"score": 0.0021453112313233554, "phrase": "exhaustive_simulations"}, {"score": 0.0021183508914633478, "phrase": "idealized_models"}, {"score": 0.0021049977753042253, "phrase": "actual_trace_data"}], "paper_keywords": ["Fault-aware scheduling", " Unrecoverable interruptions", " Divisible load", " Probabilistic scheduling"], "paper_abstract": "One has a large workload that is \"divisible\"- its constituent work's granularity can be adjusted arbitrarily-and one has access to p remote worker computers that can assist in computing the workload. How can one best utilize the workers? Complicating this question is the fact that each worker is subject to interruptions (of known likelihood) that kill all work in progress on it. One wishes to orchestrate sharing the workload with the workers in a way that maximizes the expected amount of work completed. Strategies are presented for achieving this goal, by balancing the desire to checkpoint often-thereby decreasing the amount of vulnerable work at any point-vs. the desire to avoid the context-switching required to checkpoint. Schedules must also temper the desire to replicate work, because such replication diminishes the effective remote workforce. The current study demonstrates the accessibility of strategies that provably maximize the expected amount of work when there is only one worker (the case p = 1) and, at least in an asymptotic sense, when there are two workers (the case p = 2); but the study strongly suggests the intractability of exact maximization for p >= 2 computers, as work replication on multiple workers joins checkpointing as a vehicle for decreasing the impact of work-killing interruptions. We respond to that challenge by developing efficient heuristics that employ both checkpointing and work replication as mechanisms for decreasing the impact of work- killing interruptions. The quality of these heuristics, in expected amount of work completed, is assessed through exhaustive simulations that use both idealized models and actual trace data.", "paper_title": "Static Strategies for Worksharing with Unrecoverable Interruptions", "paper_id": "WOS:000322744700002"}