{"auto_keywords": [{"score": 0.03630257633958766, "phrase": "landmark_images"}, {"score": 0.00481495049065317, "phrase": "multimodal_hypergraph_learning"}, {"score": 0.004757087682427765, "phrase": "content-based_landmark_image_search"}, {"score": 0.004442032546224076, "phrase": "challenging_problem"}, {"score": 0.0043358695338488445, "phrase": "high_diverse_visual_content"}, {"score": 0.004081385289516914, "phrase": "visual_appearances"}, {"score": 0.003999907278478564, "phrase": "different_sources"}, {"score": 0.0039677715255034595, "phrase": "different_landmarks"}, {"score": 0.0035726811019151984, "phrase": "single_type"}, {"score": 0.0035439655241057207, "phrase": "visual_feature"}, {"score": 0.003308986613384597, "phrase": "effective_modeling_scheme"}, {"score": 0.0032038465090068646, "phrase": "open_question"}, {"score": 0.0030895394261534776, "phrase": "multimodal_hypergraph"}, {"score": 0.003003452468800793, "phrase": "complex_associations"}, {"score": 0.0029434304167580256, "phrase": "mmhg"}, {"score": 0.002872979681637667, "phrase": "independent_vertices"}, {"score": 0.0027929100418926725, "phrase": "particular_views"}, {"score": 0.0027704444816096505, "phrase": "multiple_hypergraphs"}, {"score": 0.0026932247685847246, "phrase": "different_visual_modalities"}, {"score": 0.0026500668745484957, "phrase": "hidden_high-order_relations"}, {"score": 0.0025145109881926143, "phrase": "discriminative_information"}, {"score": 0.002494279207996077, "phrase": "heterogeneous_sources"}, {"score": 0.0024345529783611703, "phrase": "novel_content-based_visual_landmark_search_system"}, {"score": 0.002319346861878611, "phrase": "existing_approaches"}, {"score": 0.002272964710550345, "phrase": "unified_computational_module"}, {"score": 0.002174155795026218, "phrase": "extensive_experiment_study"}, {"score": 0.0021479597698614373, "phrase": "large-scale_test_collection"}], "paper_keywords": ["Content-based visual landmark search", " high-order relations", " multimodal hypergraph (MMHG)", " visual diversity"], "paper_abstract": "While content-based landmark image search has recently received a lot of attention and became a very active domain, it still remains a challenging problem. Among the various reasons, high diverse visual content is the most significant one. It is common that for the same landmark, images with a wide range of visual appearances can be found from different sources and different landmarks may share very similar sets of images. As a consequence, it is very hard to accurately estimate the similarities between the landmarks purely based on single type of visual feature. Moreover, the relationships between landmark images can be very complex and how to develop an effective modeling scheme to characterize the associations still remains an open question. Motivated by these concerns, we propose multimodal hypergraph (MMHG) to characterize the complex associations between landmark images. In MMHG, images are modeled as independent vertices and hyperedges contain several vertices corresponding to particular views. Multiple hypergraphs are firstly constructed independently based on different visual modalities to describe the hidden high-order relations from different aspects. Then, they are integrated together to involve discriminative information from heterogeneous sources. We also propose a novel content-based visual landmark search system based on MMHG to facilitate effective search. Distinguished from the existing approaches, we design a unified computational module to support query-specific combination weight learning. An extensive experiment study on a large-scale test collection demonstrates the effectiveness of our scheme over state-of-the-art approaches.", "paper_title": "Content-Based Visual Landmark Search via Multimodal Hypergraph Learning", "paper_id": "WOS:000365320300011"}