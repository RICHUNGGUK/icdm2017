{"auto_keywords": [{"score": 0.027140291257134692, "phrase": "bayesian"}, {"score": 0.00481495049065317, "phrase": "variational_bayesian_clustering"}, {"score": 0.004556901872357751, "phrase": "evaluation_function"}, {"score": 0.004119049028319832, "phrase": "log_likelihood"}, {"score": 0.0039016577370853387, "phrase": "vb"}, {"score": 0.0036553070575015344, "phrase": "model_parameters"}, {"score": 0.0035233696341819437, "phrase": "novel_formalism"}, {"score": 0.0033651019362620866, "phrase": "evaluation_functions"}, {"score": 0.0031845067058084583, "phrase": "information_criteria"}, {"score": 0.003126481516783286, "phrase": "model_selection"}, {"score": 0.0030414140732789186, "phrase": "kl_divergence"}, {"score": 0.0029586543333524904, "phrase": "heavy_penalty"}, {"score": 0.0026494723416072316, "phrase": "update_process"}, {"score": 0.0024842242921857705, "phrase": "finite_mixture_student's_t-distribution"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Unsupervised learning", " Bayesian estimation", " Variational approximation", " Model selection", " Robust variational Bayes"], "paper_abstract": "We prove that the evaluation function of variational Bayesian (VB) clustering algorithms can be described as the log likelihood of given data minus the Kullback-Leibler (KL) divergence between the prior and the posterior of model parameters. In this novel formalism of VB, the evaluation functions can be explicitly interpreted as information criteria for model selection and the KL divergence imposes a heavy penalty on the posterior far from the prior. We derive the update process of the variational Bayesian clustering with finite mixture Student's t-distribution, taking the penalty term for the degree of freedoms into account. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "A novel view of the variational Bayesian clustering", "paper_id": "WOS:000268733700066"}