{"auto_keywords": [{"score": 0.04782591545346595, "phrase": "classification_learning"}, {"score": 0.010612387000973441, "phrase": "multiobjective_simultaneous_learning_framework"}, {"score": 0.00929374569773796, "phrase": "learning_methods"}, {"score": 0.006916664416992632, "phrase": "mscc"}, {"score": 0.0047090997998449095, "phrase": "traditional_pattern_recognition"}, {"score": 0.004554650954322244, "phrase": "clustering_result"}, {"score": 0.004487635647709636, "phrase": "generalization_ability"}, {"score": 0.0043889488321824785, "phrase": "class_information"}, {"score": 0.004292422863834094, "phrase": "clustering_learning"}, {"score": 0.003897989308486044, "phrase": "sequential_fusing_way"}, {"score": 0.0038263829059336564, "phrase": "clustering_criterion"}, {"score": 0.0037421829222825964, "phrase": "obtained_clustering_results"}, {"score": 0.003579280847524597, "phrase": "simultaneous_optimality"}, {"score": 0.003423445743298995, "phrase": "classification_performance"}, {"score": 0.0031086137705026483, "phrase": "multiple_objective_functions"}, {"score": 0.0030401601744013793, "phrase": "classification_problems"}, {"score": 0.0029185415814209055, "phrase": "bayesian_theory"}, {"score": 0.002553431170229379, "phrase": "clustering_centers"}, {"score": 0.0024603551979225195, "phrase": "promising_classification_performance"}, {"score": 0.002370663905320869, "phrase": "multiple_pareto-optimality_solutions"}, {"score": 0.0022927343007881846, "phrase": "interesting_observation"}, {"score": 0.002242206465278159, "phrase": "great_extent"}, {"score": 0.002176560171729638, "phrase": "empirical_results"}, {"score": 0.0021049977753042253, "phrase": "mscc."}], "paper_keywords": ["Bayesian theory", " classification learning", " clustering learning", " multiobjective optimization", " pattern recognition"], "paper_abstract": "Traditional pattern recognition involves two tasks: clustering learning and classification learning. Clustering result can enhance the generalization ability of classification learning, while the class information can improve the accuracy of clustering learning. Hence, both learning methods can complement each other. To fuse the advantages of both learning methods together, many existing algorithms have been developed in a sequential fusing way by first optimizing the clustering criterion and then the classification criterion associated with the obtained clustering results. However, such kind of algorithms naturally fails to achieve the simultaneous optimality for two criteria, and thus has to sacrifice either the clustering performance or the classification performance. To overcome that problem, in this paper, we present a multiobjective simultaneous learning framework (MSCC) for both clustering and classification learning. MSCC utilizes multiple objective functions to formulate the clustering and classification problems, respectively, and more importantly, it employs the Bayesian theory to make these functions all only dependent on a set of the same parameters, i.e., clustering centers which play a role of the bridge connecting the clustering and classification learning. By simultaneously optimizing the clustering centers embedded in these functions, not only the effective clustering performance but also the promising classification performance can be simultaneously attained. Furthermore, from the multiple Pareto-optimality solutions obtained in MSCC, we can get an interesting observation that there is complementarity to great extent between clustering and classification learning processes. Empirical results on both synthetic and real data sets demonstrate the effectiveness and potential of MSCC.", "paper_title": "A Multiobjective Simultaneous Learning Framework for Clustering and Classification", "paper_id": "WOS:000274382400001"}