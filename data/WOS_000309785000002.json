{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "robust_face_recognition"}, {"score": 0.004694700982410345, "phrase": "weak_photometric_model"}, {"score": 0.004635701867952543, "phrase": "learnt_generic_face_invariant"}, {"score": 0.004500891637057374, "phrase": "intense_research"}, {"score": 0.004425616928975799, "phrase": "pose_invariance"}, {"score": 0.004388450908826472, "phrase": "prohibitively_challenging_aspects"}, {"score": 0.004351595642056769, "phrase": "face_recognition"}, {"score": 0.004102096151528319, "phrase": "video_sequences"}, {"score": 0.004016485454498076, "phrase": "recognition_input"}, {"score": 0.0038343546245999285, "phrase": "user_motion_pattern"}, {"score": 0.0037861265016041813, "phrase": "wide_variability"}, {"score": 0.0037543106253910313, "phrase": "face_images"}, {"score": 0.0037070855410818986, "phrase": "low_resolution"}, {"score": 0.003660452313567116, "phrase": "central_contribution"}, {"score": 0.0036144035798673967, "phrase": "illumination_invariant"}, {"score": 0.003421437601439154, "phrase": "loosely_constrained_head_motion"}, {"score": 0.0032115093493071366, "phrase": "photometric_model"}, {"score": 0.0031845067058084613, "phrase": "image_formation"}, {"score": 0.003104849179057156, "phrase": "statistical_model"}, {"score": 0.003078740573398959, "phrase": "generic_face_appearance_variation"}, {"score": 0.003027178161479457, "phrase": "proposed_invariant"}, {"score": 0.002939007076062123, "phrase": "extreme_illumination_changes"}, {"score": 0.00284137141260141, "phrase": "video_sequence"}, {"score": 0.002735392281320413, "phrase": "fine_alignment"}, {"score": 0.0025892330566358503, "phrase": "geodesically_local_appearance_manifold_structure"}, {"score": 0.002556625752785268, "phrase": "robust_same-identity_likelihood"}, {"score": 0.002503187934017557, "phrase": "unseen_head_poses"}, {"score": 0.002450864311898833, "phrase": "fully_automatic_recognition_system"}, {"score": 0.0024097920845706795, "phrase": "proposed_method"}, {"score": 0.0023794391656001466, "phrase": "extensive_evaluation"}, {"score": 0.00231987280116332, "phrase": "extreme_illumination"}, {"score": 0.002271372252250557, "phrase": "motion_variation"}, {"score": 0.0022145052062614514, "phrase": "nearly_perfect_recognition_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Manifold", " Illumination", " Pose", " Motion", " Invariance", " Generic"], "paper_abstract": "In spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. The central contribution is an illumination invariant, which we show to be suitable for recognition from video of loosely constrained head motion. In particular there are three contributions: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation to exploit the proposed invariant and generalize in the presence of extreme illumination changes; (ii) we introduce a video sequence \"re-illumination\" algorithm to achieve fine alignment of two video sequences; and (iii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve robustness to unseen head poses. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 323 individuals and 1474 video sequences with extreme illumination, pose and head motion variation. Our system consistently achieved a nearly perfect recognition rate (over 99.7% on all four databases). (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Achieving robust face recognition from video by combining a weak photometric model and a learnt generic face invariant", "paper_id": "WOS:000309785000002"}