{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "sequence_multi-labeling"}, {"score": 0.043473048044295366, "phrase": "spatial_and_temporal_context"}, {"score": 0.03843856552198325, "phrase": "multi-label_sequence"}, {"score": 0.03217160122346606, "phrase": "joint_kernel"}, {"score": 0.004757643419056999, "phrase": "unified_video_annotation_scheme_with_spatial"}, {"score": 0.004719816332271661, "phrase": "temporal_context"}, {"score": 0.004682288586254123, "phrase": "automatic_video_annotation"}, {"score": 0.004626553103375699, "phrase": "challenging_yet_important_problem"}, {"score": 0.004589763483242731, "phrase": "content-based_video_indexing"}, {"score": 0.004375061788023814, "phrase": "multi-labeling_problem"}, {"score": 0.003975200317233532, "phrase": "video_annotation"}, {"score": 0.0038655854071187115, "phrase": "shot_sequence"}, {"score": 0.003758981686762617, "phrase": "individual_shots"}, {"score": 0.0037290700021435394, "phrase": "sml"}, {"score": 0.0035687137873965684, "phrase": "global_optimization_manner"}, {"score": 0.0034841647157189985, "phrase": "unified_learning_framework"}, {"score": 0.0034426417353349567, "phrase": "novel_discriminative_method"}, {"score": 0.002945669860541221, "phrase": "concept-level_context_relationships"}, {"score": 0.0028302049070944944, "phrase": "low-level_features"}, {"score": 0.002581477142020187, "phrase": "kernel_weights"}, {"score": 0.0025001755089445092, "phrase": "sml_score_function"}, {"score": 0.0024408799645640323, "phrase": "desirable_multi-label_sequence"}, {"score": 0.0024117603727471654, "phrase": "large_output_space"}, {"score": 0.002363995836370621, "phrase": "test_phases"}, {"score": 0.0023171750718791713, "phrase": "approximate_method"}, {"score": 0.0022531762659857507, "phrase": "binary_markov_random_field"}, {"score": 0.0022085455097790537, "phrase": "extensive_experiments"}, {"score": 0.0021049977753042253, "phrase": "superior_performance"}], "paper_keywords": ["Sequence multi-labeling", " spatial correlation", " temporal correlation", " video annotation"], "paper_abstract": "Automatic video annotation is a challenging yet important problem for content-based video indexing and retrieval. In most existing works, annotation is formulated as a multi-labeling problem over individual shots. However, video is by nature informative in spatial and temporal context of semantic concepts. In this paper, we formulate video annotation as a sequence multi-labeling (SML) problem over a shot sequence. Different from many video annotation paradigms working on individual shots, SML aims to predict a multi-label sequence for consecutive shots in a global optimization manner by incorporating spatial and temporal context into a unified learning framework. A novel discriminative method, called sequence multi-label support vector machine (SVMSML), is accordingly proposed to infer the multi-label sequence for a given shot sequence. In (SVMSML), a joint kernel is employed to model the feature-level and concept-level context relationships (i.e., the dependencies of concepts on the low-level features, spatial and temporal correlations of concepts). A multiple-kernel learning (MKL) algorithm is developed to optimize the kernel weights of the joint kernel as well as the SML score function. To efficiently search the desirable multi-label sequence over the large output space in both training and test phases, we adopt an approximate method to maximize the energy of a binary Markov random field (BMRF). Extensive experiments on TRECVID'05 and TRECVID'07 datasets have shown that our proposed (SVMSML) gains superior performance over the state-of-the-art.", "paper_title": "Sequence Multi-Labeling: A Unified Video Annotation Scheme With Spatial and Temporal Context", "paper_id": "WOS:000284365100004"}