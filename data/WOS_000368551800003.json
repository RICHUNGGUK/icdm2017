{"auto_keywords": [{"score": 0.04947447224326201, "phrase": "random_forests"}, {"score": 0.00481495049065317, "phrase": "variable_selection"}, {"score": 0.004576253356889219, "phrase": "r_package"}, {"score": 0.0038789122930433305, "phrase": "important_variables"}, {"score": 0.003571018150056717, "phrase": "second_one"}, {"score": 0.003503497128638099, "phrase": "smaller_subset"}, {"score": 0.003204841443605308, "phrase": "prediction_objective"}, {"score": 0.003144222265269426, "phrase": "two-stage_strategy"}, {"score": 0.0030457198309616694, "phrase": "preliminary_ranking"}, {"score": 0.002988101395789399, "phrase": "explanatory_variables"}, {"score": 0.002664513608074113, "phrase": "variable_introduction"}, {"score": 0.0025160740422447837, "phrase": "data-driven_default_values"}, {"score": 0.002421726275597666, "phrase": "interesting_results"}, {"score": 0.0021593396036472777, "phrase": "simulated_example"}, {"score": 0.0021049977753042253, "phrase": "real_datasets"}], "paper_keywords": [""], "paper_abstract": "This paper describes the R package VSURF. Based on random forests, and for both regression and classification problems, it returns two subsets of variables. The first is a subset of important variables including some redundancy which can be relevant for interpretation, and the second one is a smaller subset corresponding to a model trying to avoid redundancy focusing more closely on the prediction objective. The two-stage strategy is based on a preliminary ranking of the explanatory variables using the random forests permutation-based score of importance and proceeds using a stepwise forward strategy for variable introduction. The two proposals can be obtained automatically using data-driven default values, good enough to provide interesting results, but strategy can also be tuned by the user. The algorithm is illustrated on a simulated example and its applications to real datasets are presented.", "paper_title": "VSURF: An R Package for Variable Selection Using Random Forests", "paper_id": "WOS:000368551800003"}