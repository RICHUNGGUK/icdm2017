{"auto_keywords": [{"score": 0.035923713457127915, "phrase": "bga"}, {"score": 0.008713766275018558, "phrase": "text_classification"}, {"score": 0.008091898700149524, "phrase": "text_documents"}, {"score": 0.004548939310612501, "phrase": "training_examples"}, {"score": 0.004316396683332752, "phrase": "text_document_repositories"}, {"score": 0.004223032986655664, "phrase": "storage_requirement"}, {"score": 0.004186253035932442, "phrase": "computational_cost"}, {"score": 0.004149792080414689, "phrase": "model_learning"}, {"score": 0.00407781620412982, "phrase": "instance_selection"}, {"score": 0.0038692638652494697, "phrase": "data_size"}, {"score": 0.003802135020791801, "phrase": "noisy_data"}, {"score": 0.003607630811220324, "phrase": "novel_algorithm"}, {"score": 0.0033197446406648626, "phrase": "\"biological_evolution"}, {"score": 0.0032621183241997777, "phrase": "evolutionary_process"}, {"score": 0.0031360786664165093, "phrase": "reasonable_rules"}, {"score": 0.0030547611342112693, "phrase": "long-term_evolution"}, {"score": 0.0028108690791085536, "phrase": "natural_evolution"}, {"score": 0.0026091594819333654, "phrase": "experimental_results"}, {"score": 0.0024864020775253767, "phrase": "five_state-of-the-art_algorithms"}, {"score": 0.002328238264850759, "phrase": "largest_dataset_reduction_rate"}, {"score": 0.0022678201262721323, "phrase": "least_computational_time"}, {"score": 0.0021896891011033105, "phrase": "k-nn_and_svm_classifiers"}, {"score": 0.0021705796895687864, "phrase": "similar_or_slightly_better_classification_accuracy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Instance selection", " Text classification", " Genetic algorithms"], "paper_abstract": "Text classification is usually based on constructing a model through learning from training examples to automatically classify text documents. However, as the size of text document repositories grows rapidly, the storage requirement and computational cost of model learning become higher. Instance selection is one solution to solve these limitations whose aim is to reduce the data size by filtering out noisy data from a given training dataset. In this paper, we introduce a novel algorithm for these tasks, namely a biological-based genetic algorithm (BGA). BGA fits a \"biological evolution\" into the evolutionary process, where the most streamlined process also complies with the reasonable rules. In other words, after long-term evolution, organisms find the most efficient way to allocate resources and evolve. Consequently, we can closely simulate the natural evolution of an algorithm, such that the algorithm will be both efficient and effective. The experimental results based on the TechTC-100 and Reuters-21578 datasets show the outperformance of BGA over five state-of-the-art algorithms. In particular, using BGA to select text documents not only results in the largest dataset reduction rate, but also requires the least computational time. Moreover, BGA can make the k-NN and SVM classifiers provide similar or slightly better classification accuracy than GA. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Evolutionary instance selection for text classification", "paper_id": "WOS:000333490400009"}