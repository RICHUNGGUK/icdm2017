{"auto_keywords": [{"score": 0.044776654657324805, "phrase": "execution_uncertainty"}, {"score": 0.00481495049065317, "phrase": "dynamic_execution_uncertainty"}, {"score": 0.004703671193348433, "phrase": "attacker-defender_stackelberg_security_games"}, {"score": 0.004559270778306828, "phrase": "important_research_area"}, {"score": 0.004523865550323285, "phrase": "multi-agent_systems"}, {"score": 0.004453874101119545, "phrase": "existing_ssgs_models"}, {"score": 0.0043509032299218955, "phrase": "dynamic_domains"}, {"score": 0.004119762981892093, "phrase": "unanticipated_disruptions"}, {"score": 0.004040211152784802, "phrase": "concrete_example"}, {"score": 0.0038554737609608255, "phrase": "defender's_schedule"}, {"score": 0.003781005930058317, "phrase": "fare_evaders"}, {"score": 0.00373701482510911, "phrase": "static_schedules"}, {"score": 0.003483573659272443, "phrase": "novel_general_bayesian_stackelberg_game_model"}, {"score": 0.0034029588672379926, "phrase": "dynamic_uncertain_domains"}, {"score": 0.0033502512757488433, "phrase": "new_model"}, {"score": 0.0032472646337850073, "phrase": "markov_decision_process"}, {"score": 0.003222109290677066, "phrase": "mdp"}, {"score": 0.0031721009574803127, "phrase": "defender_policies"}, {"score": 0.0030387767523076528, "phrase": "stackelberg_equilibrium"}, {"score": 0.002968424524565369, "phrase": "problem_structure"}, {"score": 0.0028996963188373144, "phrase": "polynomial-sized_optimization_problem"}, {"score": 0.002702887319160534, "phrase": "previous_ssg_algorithms"}, {"score": 0.0025892330566358503, "phrase": "complete_system"}, {"score": 0.0025292620341476283, "phrase": "schedule_interruptions"}, {"score": 0.00243237365704391, "phrase": "first_controlled_experiments"}, {"score": 0.0022760856906838814, "phrase": "real-world_experiment"}, {"score": 0.002258372771321775, "phrase": "metro"}, {"score": 0.0022320551224679526, "phrase": "los_angeles"}, {"score": 0.0021049977753042253, "phrase": "security_resource_allocation"}], "paper_keywords": [""], "paper_abstract": "Attacker-Defender Stackelberg security games (SSGs) have emerged as an important research area in multi-agent systems. However, existing SSGs models yield fixed, static, schedules which fail in dynamic domains where defenders face execution uncertainty, i.e., in domains where defenders may face unanticipated disruptions of their schedules. A concrete example is an application involving checking fares on trains, where a defender's schedule is frequently interrupted by fare evaders, making static schedules useless. To address this shortcoming, this paper provides four main contributions. First, we present a novel general Bayesian Stackelberg game model for security resource allocation in dynamic uncertain domains. In this new model, execution uncertainty is handled by using a Markov decision process (MDP) for generating defender policies. Second, we study the problem of computing a Stackelberg equilibrium for this game and exploit problem structure to reduce it to a polynomial-sized optimization problem. Shifting to evaluation, our third contribution shows, in simulation, that our MDP-based policies overcome the failures of previous SSG algorithms. In so doing, we can now build a complete system, that enables handling of schedule interruptions and, consequently, to conduct some of the first controlled experiments on SSGs in the field. Hence, as our final contribution, we present results from a real-world experiment on Metro trains in Los Angeles validating our MDP-based model, and most importantly, concretely measuring the benefits of SSGs for security resource allocation.", "paper_title": "Game-Theoretic Security Patrolling with Dynamic Execution Uncertainty and a Case Study on a Real Transit System", "paper_id": "WOS:000339311500001"}