{"auto_keywords": [{"score": 0.03693094095368546, "phrase": "action_priors"}, {"score": 0.00481495049065317, "phrase": "learning_domain_invariances"}, {"score": 0.004613175917207636, "phrase": "different_decision"}, {"score": 0.0045348429403481464, "phrase": "similar_environments"}, {"score": 0.004382127278148236, "phrase": "longer_timescale"}, {"score": 0.004326188427877999, "phrase": "individual_task"}, {"score": 0.004216434794603139, "phrase": "different_tasks"}, {"score": 0.004127094600862448, "phrase": "behavioral_invariances"}, {"score": 0.003920295222165263, "phrase": "local_contexts"}, {"score": 0.0036137713873827374, "phrase": "new_problems"}, {"score": 0.0034033029171804106, "phrase": "action_space"}, {"score": 0.0033454448643854525, "phrase": "environment_state"}, {"score": 0.0031640918630415566, "phrase": "value_functions"}, {"score": 0.003044313656896057, "phrase": "reinforcement_learning"}, {"score": 0.0030054007508185858, "phrase": "bias_action_selection"}, {"score": 0.002954287204671029, "phrase": "aggressive_use"}, {"score": 0.0028916124463906983, "phrase": "based_pruning"}, {"score": 0.002854645802851, "phrase": "available_actions"}, {"score": 0.002665305036900055, "phrase": "observation_features"}, {"score": 0.0025099367332615794, "phrase": "additional_benefit"}, {"score": 0.002477837342313456, "phrase": "feature_selection"}, {"score": 0.0023839754995935184, "phrase": "simulated_factory_environment"}, {"score": 0.002353483216544667, "phrase": "large_random_graph_domain"}, {"score": 0.0023134312557290043, "phrase": "significant_speed_ups"}, {"score": 0.002283839204519042, "phrase": "new_tasks"}, {"score": 0.0021049977753042253, "phrase": "cognitive_psychology"}], "paper_keywords": ["Action ordering", " action selection", " reinforcement learning", " search pruning", " transfer learning"], "paper_abstract": "An agent tasked with solving a number of different decision making problems in similar environments has an opportunity to learn over a longer timescale than each individual task. Through examining solutions to different tasks, it can uncover behavioral invariances in the domain, by identifying actions to be prioritized in local contexts, invariant to task details. This information has the effect of greatly increasing the speed of solving new problems. We formalise this notion as action priors, defined as distributions over the action space, conditioned on environment state, and show how these can be learnt from a set of value functions. We apply action priors in the setting of reinforcement learning, to bias action selection during exploration. Aggressive use of action priors performs context based pruning of the available actions, thus reducing the complexity of lookahead during search. We additionally define action priors over observation features, rather than states, which provides further flexibility and generalizability, with the additional benefit of enabling feature selection. Action priors are demonstrated in experiments in a simulated factory environment and a large random graph domain, and show significant speed ups in learning new tasks. Furthermore, we argue that this mechanism is cognitively plausible, and is compatible with findings from cognitive psychology.", "paper_title": "Action Priors for Learning Domain Invariances", "paper_id": "WOS:000356164500004"}