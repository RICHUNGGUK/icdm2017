{"auto_keywords": [{"score": 0.04234004787461167, "phrase": "multi-label_datasets"}, {"score": 0.040368844119497864, "phrase": "label_dependency"}, {"score": 0.00481495049065317, "phrase": "multiple_labels"}, {"score": 0.0047784273577245505, "phrase": "crowdsourced_annotations"}, {"score": 0.004742179945772899, "phrase": "artificial_intelligence_techniques"}, {"score": 0.00463507149599759, "phrase": "human_comprehension"}, {"score": 0.004547656324173302, "phrase": "multi-label_classification"}, {"score": 0.004461882369408972, "phrase": "enormous_amount"}, {"score": 0.004428025469002531, "phrase": "high-quality_multi-label_data"}, {"score": 0.004327981760540394, "phrase": "multi-label_classifier"}, {"score": 0.004087609837102023, "phrase": "lower_cost_way"}, {"score": 0.003919830848124645, "phrase": "noisy_crowdsourced_annotations"}, {"score": 0.003802135020791801, "phrase": "label-generation_process"}, {"score": 0.0037446140218109895, "phrase": "multiple_true_labels"}, {"score": 0.003673930454794753, "phrase": "crowdsourced_multi-label_annotations"}, {"score": 0.003563600033416698, "phrase": "dawid"}, {"score": 0.0035365272242181937, "phrase": "skene"}, {"score": 0.0033399639721674954, "phrase": "label_pairwise_ds"}, {"score": 0.0031542917620807093, "phrase": "uncorrelated_labels"}, {"score": 0.0029789104204803137, "phrase": "data_sparsity_problem"}, {"score": 0.002687366778212309, "phrase": "nd-ds_model"}, {"score": 0.002636587060621144, "phrase": "multi-label_estimation_problem"}, {"score": 0.002499438189264537, "phrase": "p-ds_model"}, {"score": 0.0024428607154967806, "phrase": "pairwise_comparison_relationships"}, {"score": 0.0022894012453842064, "phrase": "promising_way"}, {"score": 0.0022375675189464715, "phrase": "data_collection"}, {"score": 0.002220551141101661, "phrase": "future_applications"}, {"score": 0.002203663884543687, "phrase": "minimal_degradation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multi-label estimation", " Crowdsourced annotation", " Label dependency", " Quality control", " Human computation"], "paper_abstract": "Artificial intelligence techniques aimed at more naturally simulating human comprehension fit the paradigm of multi-label classification. Generally, an enormous amount of high-quality multi-label data is needed to form a multi-label classifier. The creation of such datasets is usually expensive and time-consuming. A lower cost way to obtain multi-label datasets for use with such comprehension simulation techniques is to use noisy crowdsourced annotations. We propose incorporating label dependency into the label-generation process to estimate the multiple true labels for each instance given crowdsourced multi-label annotations. Three statistical quality control models based on the work of Dawid and Skene are proposed. The label-dependent DS (D-DS) model simply incorporates dependency relationships among all labels. The label pairwise DS (P-DS) model groups labels into pairs to prevent interference from uncorrelated labels. The Bayesian network label-dependent DS (ND-DS) model compactly represents label dependency using conditional independence properties to overcome the data sparsity problem. Results of two experiments, \"affect annotation for lines in story\" and \"intention annotation for tweets\", show that (I) the ND-DS model most effectively handles the multi-label estimation problem with annotations provided by only about five workers per instance and that (2) the P-DS model is best if there are pairwise comparison relationships among the labels. To sum up, flexibly using label dependency to obtain multi-label datasets is a promising way to reduce the cost of data collection for future applications with minimal degradation in the quality of the results. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Separate or joint? Estimation of multiple labels from crowdsourced annotations", "paper_id": "WOS:000336872300007"}