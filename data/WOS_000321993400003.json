{"auto_keywords": [{"score": 0.03757432921561726, "phrase": "grasped_objects"}, {"score": 0.00481495049065317, "phrase": "object_context"}, {"score": 0.004696491408936132, "phrase": "recent_work"}, {"score": 0.004657653216262924, "phrase": "contextual_recognition"}, {"score": 0.004412881870378388, "phrase": "human_hands"}, {"score": 0.0037527984559873745, "phrase": "free_hand"}, {"score": 0.0036756196381448015, "phrase": "surrounding_environment"}, {"score": 0.0034678302347682694, "phrase": "severe_challenge"}, {"score": 0.0033964919992320024, "phrase": "hand_pose"}, {"score": 0.003340475943517995, "phrase": "presented_method"}, {"score": 0.0033128141572497704, "phrase": "object_occlusion"}, {"score": 0.0031647054743284947, "phrase": "pose_estimation"}, {"score": 0.0031254704616199614, "phrase": "contextual_fashion"}, {"score": 0.0030611534228319717, "phrase": "explicit_model"}, {"score": 0.0030357976028714557, "phrase": "object_shape"}, {"score": 0.002936451109357539, "phrase": "nearest_neighbor_search"}, {"score": 0.002900037689072025, "phrase": "large_database"}, {"score": 0.002690820958068933, "phrase": "real_time"}, {"score": 0.0026354246446771324, "phrase": "self_occlusions"}, {"score": 0.0026135859312619875, "phrase": "object_occlusions"}, {"score": 0.002591927716261631, "phrase": "segmentation_errors"}, {"score": 0.002549146857303602, "phrase": "full_hand_pose_reconstruction"}, {"score": 0.002496659907186784, "phrase": "temporal_consistency"}, {"score": 0.002335838283092986, "phrase": "high-dim_pose_space"}, {"score": 0.0022877332844249065, "phrase": "non-parametric_method"}, {"score": 0.002231310272815607, "phrase": "art_regression_methods"}, {"score": 0.002176275802465835, "phrase": "significantly_lower_computational_cost"}, {"score": 0.002158233639156017, "phrase": "comparable_model-based_hand_tracking_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Articulated hand pose", " Approximate nearest neighbor", " Context"], "paper_abstract": "In the spirit of recent work on contextual recognition and estimation, we present a method for estimating the pose of human hands, employing information about the shape of the object in the hand. Despite the fact that most applications of human hand tracking involve grasping and manipulation of objects, the majority of methods in the literature assume a free hand, isolated from the surrounding environment. Occlusion of the hand from grasped objects does in fact often pose a severe challenge to the estimation of hand pose. In the presented method, object occlusion is not only compensated for, it contributes to the pose estimation in a contextual fashion; this without an explicit model of object shape. Our hand tracking method is non-parametric, performing a nearest neighbor search in a large database (.. entries) of hand poses with and without grasped objects. The system that operates in real time, is robust to self occlusions, object occlusions and segmentation errors, and provides full hand pose reconstruction from monocular video. Temporal consistency in hand pose is taken into account, without explicitly tracking the hand in the high-dim pose space. Experiments show the non-parametric method to outperform other state of the art regression methods, while operating at a significantly lower computational cost than comparable model-based hand tracking methods. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Non-parametric hand pose estimation with object context", "paper_id": "WOS:000321993400003"}