{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "high_quality_crowdwork"}, {"score": 0.004742379087286591, "phrase": "causal_effects"}, {"score": 0.004713656264685915, "phrase": "financial_incentives"}, {"score": 0.0045726187330434025, "phrase": "performance-based_payments"}, {"score": 0.0044089082776901135, "phrase": "high_quality_work"}, {"score": 0.004329254684453746, "phrase": "randomized_behavioral_experiments"}, {"score": 0.004289966839994858, "phrase": "popular_crowdsourcing_platform"}, {"score": 0.004276949941406725, "phrase": "amazon_mechanical_turk"}, {"score": 0.004012500850311087, "phrase": "payment_structure"}, {"score": 0.0029602552900111407, "phrase": "higher_quality_work"}, {"score": 0.0028802818629647614, "phrase": "simple_method"}, {"score": 0.0027019561451518768, "phrase": "mechanical_turk"}, {"score": 0.0024887661666249876, "phrase": "full_version"}, {"score": 0.0024288830311280573, "phrase": "new_model"}, {"score": 0.002414138203828421, "phrase": "worker_behavior"}, {"score": 0.0023849158959156026, "phrase": "standard_principal-agent_model"}, {"score": 0.0023417427763424916, "phrase": "worker's_subjective_beliefs"}, {"score": 0.0021307975800397816, "phrase": "theoretical_studies"}, {"score": 0.0021049977753042253, "phrase": "crowdsourcing_markets"}], "paper_keywords": ["Crowdsourcing", " Performance-Based Payments", " Incentives"], "paper_abstract": "We study the causal effects of financial incentives on the quality of crowdwork. We focus on performance-based payments (PBPs), bonus payments awarded to workers for producing high quality work. We design and run randomized behavioral experiments on the popular crowdsourcing platform Amazon Mechanical Turk with the goal of understanding when, where, and why PBPs help, identifying properties of the payment, payment structure, and the task itself that make them most effective. We provide examples of tasks for which PBPs do improve quality. For such tasks, the effectiveness of PBPs is not too sensitive to the threshold for quality required to receive the bonus, while the magnitude of the bonus must be large enough to make the reward salient. We also present examples of tasks for which PBPs do not improve quality. Our results suggest that for PBPs to improve quality, the task must be effort-responsive: the task must allow workers to produce higher quality work by exerting more effort. We also give a simple method to determine if a task is effort-responsive a priori. Furthermore, our experiments suggest that all payments on Mechanical Turk are, to some degree, implicitly performance-based in that workers believe their work may be rejected if their performance is sufficiently poor. In the full version of this paper, we propose a new model of worker behavior that extends the standard principal-agent model from economics to include a worker's subjective beliefs about his likelihood of being paid, and show that the predictions of this model are in line with our experimental findings. This model may be useful as a foundation for theoretical studies of incentives in crowdsourcing markets.", "paper_title": "Incentivizing High Quality Crowdwork", "paper_id": "WOS:000372616400004"}