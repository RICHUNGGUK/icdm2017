{"auto_keywords": [{"score": 0.0422808838186638, "phrase": "bayesian_regularisation"}, {"score": 0.00481495049065317, "phrase": "structured_additive_regression"}, {"score": 0.0044079880067019765, "phrase": "paper_surveys"}, {"score": 0.004325339932975582, "phrase": "smoothing_and_selection_priors"}, {"score": 0.004244234872662471, "phrase": "unifying_perspective"}, {"score": 0.003959594975607075, "phrase": "general_class"}, {"score": 0.003909923690310641, "phrase": "structured_additive_regression_models"}, {"score": 0.0038124354878360032, "phrase": "common_feature"}, {"score": 0.0037409118365761894, "phrase": "regularisation_priors"}, {"score": 0.003673455768763824, "phrase": "gaussian"}, {"score": 0.003579179502431823, "phrase": "regularising_model_complexity"}, {"score": 0.003095257144856771, "phrase": "bayesian_analogues"}, {"score": 0.0030194211903653853, "phrase": "inference"}, {"score": 0.002905752244360772, "phrase": "unified_and_computationally_efficient_mcmc_schemes"}, {"score": 0.002780025943393764, "phrase": "basis_function_coefficients"}, {"score": 0.0027278178983579085, "phrase": "complexity_parameters"}, {"score": 0.00264296841319955, "phrase": "corresponding_marginal_posteriors"}, {"score": 0.0025933275734892508, "phrase": "variable_and_function_selection"}, {"score": 0.0023144166368024603, "phrase": "conditionally_gaussian_priors"}, {"score": 0.0021453112313233554, "phrase": "hazard_regression_model"}, {"score": 0.0021049977753042253, "phrase": "high-dimensional_geoadditive_regression_model"}], "paper_keywords": ["Conditionally Gaussian priors", " lasso", " MCMC", " P-splines", " Spike and slab prior", " Structured additive regression"], "paper_abstract": "This paper surveys various shrinkage, smoothing and selection priors from a unifying perspective and shows how to combine them for Bayesian regularisation in the general class of structured additive regression models. As a common feature, all regularisation priors are conditionally Gaussian, given further parameters regularising model complexity. Hyperpriors for these parameters encourage shrinkage, smoothness or selection. It is shown that these regularisation (log-) priors can be interpreted as Bayesian analogues of several well-known frequentist penalty terms. Inference can be carried out with unified and computationally efficient MCMC schemes, estimating regularised regression coefficients and basis function coefficients simultaneously with complexity parameters and measuring uncertainty via corresponding marginal posteriors. For variable and function selection we discuss several variants of spike and slab priors which can also be cast into the framework of conditionally Gaussian priors. The performance of the Bayesian regularisation approaches is demonstrated in a hazard regression model and a high-dimensional geoadditive regression model.", "paper_title": "Bayesian regularisation in structured additive regression: a unifying perspective on shrinkage, smoothing and predictor selection", "paper_id": "WOS:000276075700008"}