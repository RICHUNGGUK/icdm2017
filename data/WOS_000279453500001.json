{"auto_keywords": [{"score": 0.047296735825449, "phrase": "training_phase"}, {"score": 0.04656392144364241, "phrase": "attention_phase"}, {"score": 0.00481495049065317, "phrase": "biologically_inspired_object-based_visual_attention_model"}, {"score": 0.004295459433506106, "phrase": "training_targets"}, {"score": 0.004180234323211199, "phrase": "target_class"}, {"score": 0.004112582094962078, "phrase": "training_backgrounds"}, {"score": 0.004002242925856335, "phrase": "background_class"}, {"score": 0.003958937342925219, "phrase": "weight_vector"}, {"score": 0.0037903327482565097, "phrase": "mean_target_class_saliency"}, {"score": 0.003728966994353039, "phrase": "mean_background_class_saliency"}, {"score": 0.003512308265830794, "phrase": "attended_scene"}, {"score": 0.0034554281349858836, "phrase": "feature_maps"}, {"score": 0.003362660648932247, "phrase": "top-down_salience_map"}, {"score": 0.003236941658990869, "phrase": "hierarchy_method"}, {"score": 0.0031329184724341592, "phrase": "bottom-up_salience_map"}, {"score": 0.003048782877550259, "phrase": "global_salience_map"}, {"score": 0.0029830992938695007, "phrase": "visual_attention"}, {"score": 0.0028404241985421096, "phrase": "salient_region"}, {"score": 0.0025892330566358503, "phrase": "class_target_object"}, {"score": 0.0025059724686541263, "phrase": "corresponding_background_class"}, {"score": 0.0024788167298469455, "phrase": "experimental_results"}, {"score": 0.0023990982005125763, "phrase": "attended_target_object"}, {"score": 0.002235040467016128, "phrase": "training_images"}, {"score": 0.0021631447601689444, "phrase": "navalpakkam's_model"}, {"score": 0.002128067018576581, "phrase": "top-down_approach"}, {"score": 0.0021049977753042253, "phrase": "vocus."}], "paper_keywords": ["Visual attention", " Object-based", " Salience map", " Attentional selection"], "paper_abstract": "A biologically inspired object-based visual attention model is proposed in this paper. This model includes a training phase and an attention phase. In the training phase, all training targets are fused into a target class and all training backgrounds are fused into a background class. Weight vector is computed as the ratio of the mean target class saliency and the mean background class saliency for each feature. In the attention phase, for an attended scene, all feature maps are combined into a top-down salience map with the weight vector by a hierarchy method. Then, top-down and bottom-up salience map are fused into a global salience map which guides the visual attention. At last, the size of each salient region is obtained by maximizing entropy. The merit of our model is that it can attend a class target object which can appear in the corresponding background class. Experimental results indicate that: when the attended target object doesn't always appear in the background corresponding to that in the training images, our proposed model is excellent to Navalpakkam's model and the top-down approach of VOCUS.", "paper_title": "A biologically inspired object-based visual attention model", "paper_id": "WOS:000279453500001"}