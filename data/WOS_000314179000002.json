{"auto_keywords": [{"score": 0.041770649640999316, "phrase": "hidden_markov_models"}, {"score": 0.00481495049065317, "phrase": "particle-based_approximation"}, {"score": 0.004724708238414131, "phrase": "block_online_expectation_maximization_algorithm"}, {"score": 0.004549242970806199, "phrase": "expectation_maximization"}, {"score": 0.004244234872662471, "phrase": "parameter_inference"}, {"score": 0.004191007935893067, "phrase": "large_data_sets"}, {"score": 0.0041384457437123635, "phrase": "data_streams"}, {"score": 0.00406083086983316, "phrase": "independent_latent_models"}, {"score": 0.0038608730929994696, "phrase": "convergence_properties"}, {"score": 0.003717368939083015, "phrase": "open_problem"}, {"score": 0.0036018503805251424, "phrase": "hidden_markov_case"}, {"score": 0.0034461093496505127, "phrase": "new_online_em_algorithm"}, {"score": 0.003297080148944292, "phrase": "deterministic_times"}, {"score": 0.003235193484261443, "phrase": "convergence_results"}, {"score": 0.0031148723927633955, "phrase": "general_latent_models"}, {"score": 0.0028692628457408025, "phrase": "intermediate_quantities"}, {"score": 0.0027976491158911514, "phrase": "closed_form"}, {"score": 0.0026935568995690947, "phrase": "monte_carlo_methods"}, {"score": 0.00264296841319955, "phrase": "monte_carlo_error"}, {"score": 0.0023587312574818208, "phrase": "sequential_monte_carlo_methods"}, {"score": 0.0022142194702079866, "phrase": "averaged_version"}, {"score": 0.0021049977753042253, "phrase": "monte_carlo_experiments"}], "paper_keywords": ["Hidden Markov models", " Expectation maximization", " online estimation", " sequential Monte Carlo methods"], "paper_abstract": "Online variants of the Expectation Maximization (EM) algorithm have recently been proposed to perform parameter inference with large data sets or data streams, in independent latent models and in hidden Markov models. Nevertheless, the convergence properties of these algorithms remain an open problem at least in the hidden Markov case. This contribution deals with a new online EM algorithm that updates the parameter at some deterministic times. Some convergence results have been derived even in general latent models such as hidden Markov models. These properties rely on the assumption that some intermediate quantities are available in closed form or can be approximated by Monte Carlo methods when the Monte Carlo error vanishes rapidly enough. In this article, we propose an algorithm that approximates these quantities using Sequential Monte Carlo methods. The convergence of this algorithm and of an averaged version is established and their performance is illustrated through Monte Carlo experiments.", "paper_title": "Convergence of a Particle-Based Approximation of the Block Online Expectation Maximization Algorithm", "paper_id": "WOS:000314179000002"}