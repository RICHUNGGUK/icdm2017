{"auto_keywords": [{"score": 0.042506405968858046, "phrase": "speech_signal"}, {"score": 0.03654738501829699, "phrase": "consonant_closure"}, {"score": 0.00481495049065317, "phrase": "neural-network_prediction"}, {"score": 0.004762665113390216, "phrase": "vocal_tract_resonances"}, {"score": 0.004728122501822142, "phrase": "fluent_speech"}, {"score": 0.0046938292389998824, "phrase": "mel-cepstral_coefficients"}, {"score": 0.00454254773622547, "phrase": "state-space_formulation"}, {"score": 0.004493207015584002, "phrase": "neural-network-based_hidden_dynamic_model"}, {"score": 0.004348363291608004, "phrase": "approximate_em_algorithm"}, {"score": 0.00430112267051817, "phrase": "efficient_and_effective_training"}, {"score": 0.004177631185971939, "phrase": "off-the-shelf_formant_tracker"}, {"score": 0.003955523458325766, "phrase": "mel-cepstral_observations"}, {"score": 0.0038841330270679097, "phrase": "complex_sufficient_statistics"}, {"score": 0.003786337629480068, "phrase": "exact_em_algorithm"}, {"score": 0.003745179888113379, "phrase": "trained_model"}, {"score": 0.0036775717816768133, "phrase": "state_equation"}, {"score": 0.0036375920152538783, "phrase": "target-directed_vocal_tract_resonance"}, {"score": 0.0033942533792009687, "phrase": "observation_equation"}, {"score": 0.003308751289007707, "phrase": "mel-cepstral_acoustic_measurement"}, {"score": 0.003213660268958121, "phrase": "unobserved_vtr"}, {"score": 0.0031671412657333364, "phrase": "extended_kalman_filter"}, {"score": 0.0031099353574309606, "phrase": "accurate_estimation"}, {"score": 0.0030315735229977958, "phrase": "rapid_consonant-vowel"}, {"score": 0.003009548189374684, "phrase": "vowel-consonant_transitions"}, {"score": 0.0029444248503861167, "phrase": "acoustic_measurement"}, {"score": 0.002838993542880416, "phrase": "vtr_values"}, {"score": 0.002808104467715375, "phrase": "practical_significance"}, {"score": 0.002737327060030586, "phrase": "consonantal_closure"}, {"score": 0.0026683288015968243, "phrase": "target_frequency_values"}, {"score": 0.0026392916936302355, "phrase": "vtr"}, {"score": 0.002591595184168144, "phrase": "adjacent_sounds"}, {"score": 0.002535492891830764, "phrase": "vtr_transitions"}, {"score": 0.0023657047082774286, "phrase": "previous_formant_tracking_techniques"}, {"score": 0.002306052064028863, "phrase": "new_technique"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v._all_rights"}], "paper_keywords": ["vocal tract resonance", " tracking", " cepstra", " neural network", " multi-layer perceptron", " EM algorithm", " hidden dynamics", " state-space model"], "paper_abstract": "In this paper, we present a state-space formulation of a neural-network-based hidden dynamic model of speech whose parameters are trained using an approximate EM algorithm. This efficient and effective training makes use of the output of an off-the-shelf formant tracker (for the vowel segments of the speech signal), in addition to the Mel-cepstral observations, to simplify the complex sufficient statistics that would be required in the exact EM algorithm. The trained model, consisting of the state equation for the target-directed vocal tract resonance (VTR) dynamics on all classes of speech sounds (including consonant closure and constriction) and the observation equation for mapping from the VTR to Mel-cepstral acoustic measurement, is then used to recover the unobserved VTR based on the extended Kalman filter. The results demonstrate accurate estimation of the VTR, especially during rapid consonant-vowel or vowel-consonant transitions and during consonant closure when the acoustic measurement alone provides weak or no information to infer the VTR values. The practical significance of correctly identifying the VTRs during consonantal closure or constriction is that they provide target frequency values for the VTR or formant transitions from adjacent sounds. Without such target values, the VTR transitions from vowel to consonant or from consonant to vowel are often very difficult to extract accurately by the previous formant tracking techniques. With the use of the new technique reported in this paper, the consonantal VTRs and the related transitions become more reliably identified from the speech signal. (C) 2006 Elsevier B.V. All rights reserved.", "paper_title": "A state-space model with neural-network prediction for recovering vocal tract resonances in fluent speech from Mel-cepstral coefficients", "paper_id": "WOS:000239446300008"}