{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "irrelevant_data"}, {"score": 0.021650248413100728, "phrase": "labeled_data"}, {"score": 0.010030887612918175, "phrase": "unlabeled_data"}, {"score": 0.004786315041142917, "phrase": "semi-supervised_learning"}, {"score": 0.004757858500534204, "phrase": "ssl"}, {"score": 0.0047014216428514465, "phrase": "typical_learning_paradigms_training"}, {"score": 0.004631823134072564, "phrase": "labeled_and_unlabeled_data"}, {"score": 0.004590557276496284, "phrase": "traditional_ssl_models"}, {"score": 0.004363536354756615, "phrase": "targeted_labeled_data"}, {"score": 0.003942489281950188, "phrase": "maximum_margin_model"}, {"score": 0.0037586021907673427, "phrase": "available_training_data"}, {"score": 0.0036589469980321046, "phrase": "targeted_data"}, {"score": 0.0034778368609967754, "phrase": "prior_knowledge"}, {"score": 0.00345712586265106, "phrase": "explicit_assumption"}, {"score": 0.0034262895977021854, "phrase": "data_relatedness"}, {"score": 0.003305661550005516, "phrase": "irrelevant_unlabeled_data"}, {"score": 0.0032664003336950243, "phrase": "logistic_principle"}, {"score": 0.003246944366436126, "phrase": "maximum_entropy_principle"}, {"score": 0.003170268825120196, "phrase": "ideal_classifier"}, {"score": 0.0030495020877548324, "phrase": "relevant_data"}, {"score": 0.0029953212912307114, "phrase": "decision_hyperplane"}, {"score": 0.0028640315078411784, "phrase": "theoretical_analysis"}, {"score": 0.0026737800197678884, "phrase": "generalized_model"}, {"score": 0.00231637194556478, "phrase": "original_mixed_integer_programming"}, {"score": 0.0022888335017747768, "phrase": "semi-definite_programming_relaxation"}, {"score": 0.002234732712120406, "phrase": "quadratic_programming_subproblems"}], "paper_keywords": ["Maximum margin classifier", " Irrelevant data", " Semi-supervised learning", " Concave convex procedure"], "paper_abstract": "Semi-supervised learning (SSL) is a typical learning paradigms training a model from both labeled and unlabeled data. The traditional SSL models usually assume unlabeled data are relevant to the labeled data, i.e., following the same distributions of the targeted labeled data. In this paper, we address a different, yet formidable scenario in semi-supervised classification, where the unlabeled data may contain irrelevant data to the labeled data. To tackle this problem, we develop a maximum margin model, named tri-class support vector machine (3C-SVM), to utilize the available training data, while seeking a hyperplane for separating the targeted data well. Our 3C-SVM exhibits several characteristics and advantages. First, it does not need any prior knowledge and explicit assumption on the data relatedness. On the contrary, it can relieve the effect of irrelevant unlabeled data based on the logistic principle and maximum entropy principle. That is, 3C-SVM approaches an ideal classifier. This classifier relies heavily on labeled data and is confident on the relevant data lying far away from the decision hyperplane, while maximally ignoring the irrelevant data, which are hardly distinguished. Second, theoretical analysis is provided to prove that in what condition, the irrelevant data can help to seek the hyperplane. Third, 3C-SVM is a generalized model that unifies several popular maximum margin models, including standard SVMs, Semi-supervised SVMs ((SVMS)-V-3), and SVMs learned from the universum (u-SVM5) as its special cases. More importantly, we deploy a concave convex produce to solve the proposed 3C-SVM, transforming the original mixed integer programming, to a semi-definite programming relaxation, and finally to a sequence of quadratic programming subproblems, which yields the same worst case time complexity as that of (SVMs)-V-3. Finally, we demonstrate the effectiveness and efficiency of our proposed 3C-SVM through systematical experimental comparisons. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Maximum margin semi-supervised learning with irrelevant data", "paper_id": "WOS:000361582200010"}