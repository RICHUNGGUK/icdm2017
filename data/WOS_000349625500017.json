{"auto_keywords": [{"score": 0.0482937679895443, "phrase": "gp"}, {"score": 0.00481495049065317, "phrase": "multidimensional_inference"}, {"score": 0.004769444094319619, "phrase": "structured_gaussian_processes"}, {"score": 0.004462663711734448, "phrase": "data_size_n"}, {"score": 0.004337276328199398, "phrase": "large_n._many_algorithms"}, {"score": 0.004155742403177802, "phrase": "lower_rank_matrices"}, {"score": 0.004058180509184539, "phrase": "structure_inherent_in_particular_covariance_functions"}, {"score": 0.003962899903266524, "phrase": "implied_markov_structure"}, {"score": 0.0035525230119401153, "phrase": "gp_advances"}, {"score": 0.003419943984098415, "phrase": "multidimensional_input"}, {"score": 0.003307988966112615, "phrase": "multidimensional_applications"}, {"score": 0.0031693980719538317, "phrase": "structured_gps"}, {"score": 0.0031393948330209255, "phrase": "multidimensional_inputs"}, {"score": 0.003036595866130997, "phrase": "multiplicative_kernels"}, {"score": 0.0029511579080222137, "phrase": "new_method"}, {"score": 0.002895535087931938, "phrase": "additive_gps"}, {"score": 0.0028409576481842457, "phrase": "novel_connection"}, {"score": 0.0028006988338192375, "phrase": "classic_backfitting_method"}, {"score": 0.0027610089418453614, "phrase": "bayesian_framework"}, {"score": 0.0026077803487003, "phrase": "projection_pursuit_regression"}, {"score": 0.002558940269534974, "phrase": "laplace"}, {"score": 0.002522344964242888, "phrase": "non-gaussian_observations"}, {"score": 0.0024630345285372958, "phrase": "multiplicative_kernel_structure"}, {"score": 0.0024051153560799335, "phrase": "novel_method"}, {"score": 0.002326304140277375, "phrase": "multidimensional_grid"}, {"score": 0.0021049977753042253, "phrase": "naive_gp"}], "paper_keywords": ["Gaussian processes", " backfitting", " projection-pursuit regression", " Kronecker matrices"], "paper_abstract": "Exact Gaussian process (GP) regression has O(N-3) runtime for data size N, making it intractable for large N. Many algorithms for improving GP scaling approximate the covariance with lower rank matrices. Other work has exploited structure inherent in particular covariance functions, including GPs with implied Markov structure, and inputs on a lattice (both enable O(N) or O(N log N) runtime). However, these GP advances have not been well extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests three novel extensions of structured GPs to multidimensional inputs, for models with additive and multiplicative kernels. First we present a new method for inference in additive GPs, showing a novel connection between the classic backfitting method and the Bayesian framework. We extend this model using two advances: a variant of projection pursuit regression, and a Laplace approximation for non-Gaussian observations. Lastly, for multiplicative kernel structure, we present a novel method for GPs with inputs on a multidimensional grid. We illustrate the power of these three advances on several data sets, achieving performance equal to or very close to the naive GP at orders of magnitude less cost.", "paper_title": "Scaling Multidimensional Inference for Structured Gaussian Processes", "paper_id": "WOS:000349625500017"}