{"auto_keywords": [{"score": 0.04325124255431721, "phrase": "algorithm_designer"}, {"score": 0.04261955549208519, "phrase": "arbitrary_loss_functions"}, {"score": 0.00481495049065317, "phrase": "kernel-matching_pursuits_with_arbitrary_loss"}, {"score": 0.004260134699483774, "phrase": "computational_efficiency"}, {"score": 0.004216880725292, "phrase": "generalization_ability"}, {"score": 0.003966388754366831, "phrase": "give_problem_domain"}, {"score": 0.0035269696779744266, "phrase": "margin_maximization"}, {"score": 0.0034556603183020407, "phrase": "standard_error_minimization"}, {"score": 0.0033685408148574846, "phrase": "excellent_performance"}, {"score": 0.003334309053314909, "phrase": "computational_savings"}, {"score": 0.002676692093594358, "phrase": "example_loss_functions"}, {"score": 0.002622528664187849, "phrase": "algorithm_performance"}, {"score": 0.0025433263646920364, "phrase": "experimental_results"}, {"score": 0.0025046245057922557, "phrase": "first_group"}, {"score": 0.002453934333239467, "phrase": "proposed_algorithms"}, {"score": 0.002126635713519053, "phrase": "data_characteristics"}, {"score": 0.0021049977753042253, "phrase": "problem_domain"}], "paper_keywords": ["Boosting", " imbalanced data", " iteratively reweighted least squares", " kernel machines", " kernel-matching pursuits (KMPs)", " margin maximization", " robust classification", " robust statistics", " unbalanced data"], "paper_abstract": "The purpose of this research is to develop a classifier capable of state-of-the-art performance in both computational efficiency and generalization ability while allowing the algorithm designer to choose arbitrary loss functions as appropriate for a give problem domain. This is critical in applications involving heavily imbalanced, noisy, or non-Gaussian distributed data. To achieve this goal, a kernel-matching pursuit (KMP) framework is formulated where the objective is margin maximization rather than the standard error minimization. This approach enables excellent performance and computational savings in the presence of large, imbalanced training data sets and facilitates the development of two general algorithms. These algorithms support the use of arbitrary loss functions allowing the algorithm designer to control the degree to which outliers are penalized and the manner in which non-Gaussian distributed data is handled. Example loss functions are provided and algorithm performance is illustrated in two groups of experimental results. The first group demonstrates that the proposed algorithms perform equivalent to several state-of-the-art machine learning algorithms on well-published, balanced data. The second group of results illustrates superior performance by the proposed algorithms on imbalanced, non-Gaussian data achieved by employing loss functions appropriate for the data characteristics and problem domain.", "paper_title": "Kernel-Matching Pursuits With Arbitrary Loss Functions", "paper_id": "WOS:000263831800003"}