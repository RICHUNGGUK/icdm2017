{"auto_keywords": [{"score": 0.036615118920189677, "phrase": "greedy_policy"}, {"score": 0.00950882915044125, "phrase": "net_information_gain"}, {"score": 0.00481495049065317, "phrase": "greedy_adaptive_linear_compression"}, {"score": 0.004757227624616643, "phrase": "signal-plus-noise_models"}, {"score": 0.004533142516832776, "phrase": "adaptive_compression_policies"}, {"score": 0.004372002674739612, "phrase": "vector-valued_measurements"}, {"score": 0.004165987300206134, "phrase": "compressed_variables"}, {"score": 0.004017846065981189, "phrase": "optimization_criterion"}, {"score": 0.003969640975997097, "phrase": "information_gain"}, {"score": 0.0038284549148537373, "phrase": "sequential_scalar_compressions"}, {"score": 0.0037597491055035895, "phrase": "unit-norm_compression_vectors"}, {"score": 0.0036700484729006136, "phrase": "per-stage_information_gain"}, {"score": 0.003312009953245191, "phrase": "posterior_covariance_matrix"}, {"score": 0.003174903873774882, "phrase": "previous_compressions"}, {"score": 0.0030434561939032597, "phrase": "water-filling_solution"}, {"score": 0.002952900365829518, "phrase": "optimum_compression_policy"}, {"score": 0.002779769492357829, "phrase": "average_norm"}, {"score": 0.0027463775689957255, "phrase": "compression_vectors"}, {"score": 0.002680789041160613, "phrase": "sufficient_conditions"}, {"score": 0.0025697460140513932, "phrase": "stepwise_information_gain"}, {"score": 0.0023328599525181707, "phrase": "scalar_compressions"}, {"score": 0.0022771244331421586, "phrase": "simulation_results"}, {"score": 0.0021049977753042253, "phrase": "noise_sequences"}], "paper_keywords": ["Entropy", " information gain", " compressive sensing", " compressed sensing", " greedy policy", " optimal policy"], "paper_abstract": "In this paper, we examine adaptive compression policies, when the sequence of vector-valued measurements to be compressed is noisy and the compressed variables are themselves noisy. The optimization criterion is information gain. In the case of sequential scalar compressions, the unit-norm compression vectors that greedily maximize per-stage information gain are eigenvectors of an a priori error covariance matrix, and the greedy policy selects them according to eigenvalues of a posterior covariance matrix. These eigenvalues depend on all previous compressions and are computed recursively. A water-filling solution is given for the optimum compression policy that maximizes net information gain, under a constraint on the average norm of compression vectors. We provide sufficient conditions under which the greedy policy for maximizing stepwise information gain actually is optimal in the sense of maximizing the net information gain. In the case of scalar compressions, our examples and simulation results illustrate that the greedy policy can be quite close to optimal when the noise sequences are white.", "paper_title": "Greedy Adaptive Linear Compression in Signal-Plus-Noise Models", "paper_id": "WOS:000333099400019"}