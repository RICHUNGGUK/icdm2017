{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "adaptive_optimal_control"}, {"score": 0.004767261858417823, "phrase": "partially_unknown_nonlinear_systems"}, {"score": 0.004558414405811025, "phrase": "continuous-time_framework"}, {"score": 0.004446349752828045, "phrase": "direct_adaptive_optimal_control"}, {"score": 0.004402295697276237, "phrase": "infinite_horizon_cost"}, {"score": 0.004358676214574977, "phrase": "nonlinear_systems"}, {"score": 0.004188459638572375, "phrase": "optimal_control_solution"}, {"score": 0.0040854526743211396, "phrase": "internal_system_dynamics"}, {"score": 0.003829290275367182, "phrase": "oil_a_reinforcement_learning_scheme"}, {"score": 0.0037729164114510957, "phrase": "policy"}, {"score": 0.003643185395780667, "phrase": "neural_networks"}, {"score": 0.0034660938095219846, "phrase": "control_policy"}, {"score": 0.0033639840192975835, "phrase": "control_system"}, {"score": 0.0032164136388931805, "phrase": "optimal_controller"}, {"score": 0.0031845067058084583, "phrase": "optimal_cost_function"}, {"score": 0.003121636289442593, "phrase": "infinite_horizon_control_performance"}, {"score": 0.0029698211157405618, "phrase": "realistic_assumption"}, {"score": 0.00286795254228461, "phrase": "perfect_representations"}, {"score": 0.0028253682745674608, "phrase": "nonlinear_control"}, {"score": 0.002701361707441567, "phrase": "hybrid_control_structure"}, {"score": 0.0026480044868438875, "phrase": "continuous-time_controller"}, {"score": 0.0026086775334428617, "phrase": "supervisory_adaptation_structure"}, {"score": 0.002444883254621769, "phrase": "continuous-time_performance_dynamics"}, {"score": 0.002349228457383314, "phrase": "standard_form"}, {"score": 0.0022460727898764216, "phrase": "simulation_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Direct adaptive optimal control", " Policy iteration", " Neural networks", " Online control"], "paper_abstract": "In this paper we present in a continuous-time framework an online approach to direct adaptive optimal control with infinite horizon cost for nonlinear systems. The algorithm converges online to the optimal control solution without knowledge of the internal system dynamics. Closed-loop dynamic stability is guaranteed throughout. The algorithm is based oil a reinforcement learning scheme, namely Policy iterations, and makes use of neural networks, in an Actor/Critic structure, to parametrically represent the control policy and the performance of the control system. The two neural networks are trained to express the optimal controller and optimal cost function which describes the infinite horizon control performance. Convergence of the algorithm is proven under the realistic assumption that the two neural networks do not provide perfect representations for the nonlinear control and cost functions. The result is a hybrid control structure which involves a continuous-time controller and a Supervisory adaptation structure which operates based on data sampled from the plant and from the continuous-time performance dynamics. Such control structure is unlike any standard form of controllers previously seen in the literature. Simulation results, obtained considering two second-order nonlinear systems, are provided. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Neural network approach to continuous-time direct adaptive optimal control for partially unknown nonlinear systems", "paper_id": "WOS:000266998900006"}