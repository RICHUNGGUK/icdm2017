{"auto_keywords": [{"score": 0.02807165003858346, "phrase": "prototype_classifier"}, {"score": 0.008903419666789408, "phrase": "support_vector_machine"}, {"score": 0.00481495049065317, "phrase": "machine_learning"}, {"score": 0.0044896599221451216, "phrase": "decoding_problem"}, {"score": 0.004431149592844886, "phrase": "generalized_prototype_framework"}, {"score": 0.004373398434399966, "phrase": "discrimination_process"}, {"score": 0.003920384090110374, "phrase": "threshold_stage"}, {"score": 0.003818807671883534, "phrase": "projected_patterns"}, {"score": 0.003607630811220324, "phrase": "popular_mean-of-class_prototype_classification"}, {"score": 0.003438058620195309, "phrase": "invariance_properties"}, {"score": 0.0033636292024305406, "phrase": "simple_yet_general_approach"}, {"score": 0.0033197446406648626, "phrase": "different_types"}, {"score": 0.0032908057622663732, "phrase": "linear_classification_algorithms"}, {"score": 0.003247868216333148, "phrase": "identical_and_easy-to-visualize_formal_framework"}, {"score": 0.0030816306874747426, "phrase": "normal_vector"}, {"score": 0.002962543869356265, "phrase": "non-margin_classifiers"}, {"score": 0.0029110999689379497, "phrase": "classical_prototype_classifier"}, {"score": 0.0028731024405707277, "phrase": "fisher_classifier"}, {"score": 0.002823207272381655, "phrase": "relevance_vector_machine"}, {"score": 0.0027620516806198354, "phrase": "hard_and_soft_margin_classifiers"}, {"score": 0.0026786469998321084, "phrase": "boosted_version"}, {"score": 0.0025750928944698673, "phrase": "mean-of-class_prototype_classification"}, {"score": 0.0024113009621042677, "phrase": "soft_margin_classifier"}, {"score": 0.002287783663138828, "phrase": "novel_insights"}, {"score": 0.0022089663761854657, "phrase": "common_and_unified_formalism"}, {"score": 0.0021516366862117707, "phrase": "efficient_visualization"}, {"score": 0.002123531043641047, "phrase": "principled_comparison"}, {"score": 0.0021049977753042253, "phrase": "machine_learning_classification"}], "paper_keywords": [""], "paper_abstract": "We shed light on the discrimination between patterns belonging to two different classes by casting this decoding problem into a generalized prototype framework. The discrimination process is then separated into two stages: a projection stage that reduces the dimensionality of the data by projecting it on a line and a threshold stage where the distributions of the projected patterns of both classes are separated. For this, we extend the popular mean-of-class prototype classification using algorithms from machine learning that satisfy a set of invariance properties. We report a simple yet general approach to express different types of linear classification algorithms in an identical and easy-to-visualize formal framework using generalized prototypes where these prototypes are used to express the normal vector and offset of the hyperplane. We investigate non-margin classifiers such as the classical prototype classifier, the Fisher classifier, and the relevance vector machine. We then study hard and soft margin classifiers such as the support vector machine and a boosted version of the prototype classifier. Subsequently, we relate mean-of-class prototype classification to other classification algorithms by showing that the prototype classifier is a limit of any soft margin classifier and that boosting a prototype classifier yields the support vector machine. While giving novel insights into classification per se by presenting a common and unified formalism, our generalized prototype framework also provides an efficient visualization and a principled comparison of machine learning classification.", "paper_title": "Prototype Classification: Insights from Machine Learning", "paper_id": "WOS:000262110600010"}