{"auto_keywords": [{"score": 0.04599283119366167, "phrase": "kernel-based_halfspaces"}, {"score": 0.00481495049065317, "phrase": "learning_kernel-based"}, {"score": 0.004518442937844318, "phrase": "new_algorithm"}, {"score": 0.0041074234667687875, "phrase": "previous_formulations"}, {"score": 0.003978866233891427, "phrase": "surrogate_convex_loss_functions"}, {"score": 0.003757480190200645, "phrase": "support_vector_machines"}, {"score": 0.003548368289666932, "phrase": "logistic_regression"}, {"score": 0.003144222265269426, "phrase": "worst-case_time"}, {"score": 0.002548331088496964, "phrase": "learned_classifier"}, {"score": 0.002468450551616404, "phrase": "optimal_halfspace"}, {"score": 0.0023458049872325214, "phrase": "hardness_result"}, {"score": 0.0021051167107362717, "phrase": "l."}], "paper_keywords": ["learning halfspaces", " kernel methods", " learning theory"], "paper_abstract": "We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces with respect to the 0-1 loss function. Unlike most of the previous formulations, which rely on surrogate convex loss functions (e. g., hinge-loss in support vector machines (SVMs) and log-loss in logistic regression), we provide finite time/sample guarantees with respect to the more natural 0-1 loss function. The proposed algorithm can learn kernel-based halfspaces in worst-case time poly(exp(L log(L/epsilon))), for any distribution, where L is a Lipschitz constant (which can be thought of as the reciprocal of the margin), and the learned classifier is worse than the optimal halfspace by at most epsilon. We also prove a hardness result, showing that under a certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time polynomial in L.", "paper_title": "LEARNING KERNEL-BASED HALFSPACES WITH THE 0-1 LOSS", "paper_id": "WOS:000298377900006"}