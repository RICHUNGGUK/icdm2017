{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "training_data_complexity"}, {"score": 0.004718897469501381, "phrase": "nearest_neighbor_classifiers"}, {"score": 0.0038833278847237858, "phrase": "great_variety"}, {"score": 0.0038314621564116192, "phrase": "real-world_domains"}, {"score": 0.0036064757492752703, "phrase": "practical_use"}, {"score": 0.0033045378860812403, "phrase": "computational_costs"}, {"score": 0.0026111116934081284, "phrase": "k-nn_rule"}, {"score": 0.002441118140830714, "phrase": "present_analysis"}, {"score": 0.0023287359930308864, "phrase": "data_complexity_measures"}, {"score": 0.0022365262201954643, "phrase": "feature_space_dimensionality"}, {"score": 0.002206606452557159, "phrase": "class_density"}, {"score": 0.0021049977753042253, "phrase": "practical_accuracy"}], "paper_keywords": [""], "paper_abstract": "The k-nearest neighbors (k-NN) classifier is one of the most popular supervised classification methods. It is very simple, intuitive and accurate in a great variety of real-world domains. Nonetheless, despite its simplicity and effectiveness, practical use of this rule has been historically limited due to its high storage requirements and the computational costs involved. On the other hand, the performance of this classifier appears strongly sensitive to training data complexity. In this context, by means of several problem difficulty measures, we try to characterize the behavior of the k-NN rule when working under certain situations. More specifically, the present analysis focuses on the use of some data complexity measures to describe class overlapping, feature space dimensionality and class density, and discover their relation with the practical accuracy of this classifier.", "paper_title": "An analysis of how training data complexity affects the nearest neighbor classifiers", "paper_id": "WOS:000248383000003"}