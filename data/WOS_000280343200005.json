{"auto_keywords": [{"score": 0.03902727831247399, "phrase": "answer_candidates"}, {"score": 0.010541469995859979, "phrase": "answer_ranking"}, {"score": 0.007384212097686412, "phrase": "multiple_evidence"}, {"score": 0.00481495049065317, "phrase": "probabilistic_framework"}, {"score": 0.004546666837896004, "phrase": "exact_answers"}, {"score": 0.004500891637057374, "phrase": "user's_question"}, {"score": 0.004455575231721042, "phrase": "large_collection"}, {"score": 0.0043663006236597975, "phrase": "information_retrieval"}, {"score": 0.004336939968686303, "phrase": "extraction_techniques"}, {"score": 0.0042500322609733574, "phrase": "likely_candidates"}, {"score": 0.004178935413340453, "phrase": "ranking_strategy"}, {"score": 0.004122911675081472, "phrase": "final_answers"}, {"score": 0.004081385289516914, "phrase": "ranking_process"}, {"score": 0.003945949295597681, "phrase": "relevant_answers"}, {"score": 0.0038278890186559276, "phrase": "multi-strategy_qa"}, {"score": 0.0037765536386719775, "phrase": "multiple_answering_agents"}, {"score": 0.0036388906120614012, "phrase": "different_agents"}, {"score": 0.0036144035798673967, "phrase": "different_score_distributions"}, {"score": 0.003518086415996508, "phrase": "important_role"}, {"score": 0.003378385493203297, "phrase": "unified_probabilistic_framework"}, {"score": 0.003255189076644247, "phrase": "answer_merging"}, {"score": 0.003042555825368091, "phrase": "answer_relevance"}, {"score": 0.0028630535573013686, "phrase": "multiple_extraction_techniques"}, {"score": 0.0027773024878187656, "phrase": "list_questions"}, {"score": 0.0027400176498756823, "phrase": "factoid_questions"}, {"score": 0.0026222551419772867, "phrase": "different_qa_system"}, {"score": 0.002509541181962613, "phrase": "qa_system"}, {"score": 0.0024842242921857705, "phrase": "extensive_set"}, {"score": 0.0022984065518052476, "phrase": "preliminary_research"}, {"score": 0.0022829193139224636, "phrase": "ko_et_al"}, {"score": 0.0022070310530794097, "phrase": "answer_selection"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Answer ranking", " Answer merging", " Question answering"], "paper_abstract": "Question answering (QA) aims at finding exact answers to a user's question from a large collection of documents. Most QA systems combine information retrieval with extraction techniques to identify a set of likely candidates and then utilize some ranking strategy to generate the final answers. This ranking process can be challenging, as it entails identifying the relevant answers amongst many irrelevant ones. This is more challenging in multi-strategy QA, in which multiple answering agents are used to extract answer candidates. As answer candidates come from different agents with different score distributions, how to merge answer candidates plays an important role in answer ranking. In this paper, we propose a unified probabilistic framework which combines multiple evidence to address challenges ill answer ranking and answer merging. The hypotheses of the paper are that: (1) the framework effectively combines multiple evidence for identifying answer relevance and their correlation in answer ranking, (2) the framework supports answer merging on answer candidates returned by multiple extraction techniques, (3) the framework can support list questions as well as factoid questions, (4) the framework can be easily applied to a different QA system, and (5) the framework significantly improves performance of a QA system. An extensive set of experiments was done to support our hypotheses and demonstrate the effectiveness of the framework. All of the work substantially extends the preliminary research in Ko et al. (2007a). A probabilistic framework for answer selection in question answering. In: Proceedings of NAACL/HLT. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Combining evidence with a probabilistic framework for answer ranking and answer merging in question answering", "paper_id": "WOS:000280343200005"}