{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "multiscale_bagging"}, {"score": 0.04432396220512617, "phrase": "bootstrap_samples"}, {"score": 0.012974410587451649, "phrase": "sample_size"}, {"score": 0.011432197033582893, "phrase": "bootstrap_probability"}, {"score": 0.00882623781510355, "phrase": "first_usage"}, {"score": 0.00813711467419928, "phrase": "second_usage"}, {"score": 0.004483120978156604, "phrase": "bagging_procedure"}, {"score": 0.0044150318724093226, "phrase": "ordinary_bagging"}, {"score": 0.004347972379250724, "phrase": "bootstrap_resampling"}, {"score": 0.004089725258435817, "phrase": "multiscale_bootstrap_algorithm"}, {"score": 0.0034556603183020407, "phrase": "class_label"}, {"score": 0.003334309053314909, "phrase": "specified_class_label"}, {"score": 0.002860546817196148, "phrase": "geometrical_theory"}, {"score": 0.0025826247025542213, "phrase": "class_labels"}, {"score": 0.0025174594088331853, "phrase": "single_label"}, {"score": 0.0023798115577749225, "phrase": "decision_boundaries"}, {"score": 0.002272794656458419, "phrase": "active_learning"}, {"score": 0.0021595094738887767, "phrase": "appropriate_choice"}], "paper_keywords": ["bagging", " active learning", " confidence level", " classification"], "paper_abstract": "We propose multiscale bagging as a modification of the bagging procedure. In ordinary bagging, the bootstrap resampling is used for generating bootstrap samples. We replace it with the multiscale bootstrap algorithm. In multiscale bagging, the sample size in of bootstrap samples may be altered from the sample size n of learning dataset. For assessing the output of a classifier, we compute bootstrap probability of class label; the frequency of observing a specified class label in the outputs of classifiers learned from bootstrap samples. A scaling-law of bootstrap probability with respect to sigma(2) = n/m has been developed in connection with the geometrical theory. We consider two different ways for using multiscale bagging of classifiers. The first usage is to construct a confidence set of class labels, instead of a single label. The second usage is to find. inputs close to decision boundaries in the context of query by bagging for active learning. It turned out, interestingly, that an appropriate choice of m is m = -n, i.e., sigma(2) = -1, for the first usage, and m = infinity, i.e., sigma(2) = 0, for the second usage.", "paper_title": "Multiscale Bagging and Its Applications", "paper_id": "WOS:000295601200011"}