{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "self-training_approach"}, {"score": 0.04795274928516732, "phrase": "uncertainty_sampling"}, {"score": 0.041712156789843864, "phrase": "loss-reduction_methods"}, {"score": 0.03754947136229219, "phrase": "total_misclassification_costs"}, {"score": 0.004685575787741084, "phrase": "sensitive_uncertainty_sampling"}, {"score": 0.004477595431209663, "phrase": "effective_method"}, {"score": 0.004357245559948974, "phrase": "active_learning"}, {"score": 0.0034403745992576808, "phrase": "different_costs"}, {"score": 0.003170060683860338, "phrase": "cost-sensitive_uncertainty_sampling"}, {"score": 0.002816556934231128, "phrase": "misclassification_costs"}, {"score": 0.0026188490199688013, "phrase": "faster_reduction"}, {"score": 0.002305589818994465, "phrase": "posterior_probability_estimates"}, {"score": 0.0022231606165229235, "phrase": "standard_uncertainty_sampling"}], "paper_keywords": ["Active learning", " Cost-sensitive learning", " Self-training"], "paper_abstract": "Uncertainty sampling is an effective method for performing active learning that is computationally efficient compared to other active learning methods such as loss-reduction methods. However, unlike loss-reduction methods, uncertainty sampling cannot minimize total misclassification costs when errors incur different costs. This paper introduces a method for performing cost-sensitive uncertainty sampling that makes use of self-training. We show that, even when misclassification costs are equal, this self-training approach results in faster reduction of loss as a function of number of points labeled and more reliable posterior probability estimates as compared to standard uncertainty sampling. We also show why other more naive methods of modifying uncertainty sampling to minimize total misclassification costs will not always work well.", "paper_title": "A self-training approach to cost sensitive uncertainty sampling", "paper_id": "WOS:000269013000007"}