{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "timely_staging"}, {"score": 0.004770211811866158, "phrase": "hpc_job_input_data"}, {"score": 0.004725886853744007, "phrase": "innovative_scientific_applications"}, {"score": 0.004681971826924847, "phrase": "emerging_dense_data_sources"}, {"score": 0.004595356579614969, "phrase": "data_deluge"}, {"score": 0.004552648949390462, "phrase": "high-end_supercomputing_systems"}, {"score": 0.004304574865291508, "phrase": "distributed_user_base"}, {"score": 0.004264558080356621, "phrase": "input_and_output_data_sets"}, {"score": 0.003976007772514762, "phrase": "supercomputer's_specialized_high-speed_storage"}, {"score": 0.003570889780957037, "phrase": "unnecessary_delays"}, {"score": 0.0034238045290210534, "phrase": "current_practice"}, {"score": 0.003206916619690171, "phrase": "storage_failures"}, {"score": 0.0030891871245908665, "phrase": "job_throughput"}, {"score": 0.0029618842673339173, "phrase": "timely_staging_framework"}, {"score": 0.0028799352389969443, "phrase": "job_start-up_time_predictions"}, {"score": 0.002787181295054319, "phrase": "cloud-based_intermediate_storage_nodes"}, {"score": 0.002684819249720834, "phrase": "input_data"}, {"score": 0.002422249273477746, "phrase": "jaguar_supercomputer"}, {"score": 0.002311525062244175, "phrase": "staging_times"}, {"score": 0.0022793032191981404, "phrase": "direct_transfers"}, {"score": 0.002226592967032469, "phrase": "wait_time"}, {"score": 0.0021049977753042253, "phrase": "earlier_version"}], "paper_keywords": ["High performance data management", " data-staging", " HPC center serviceability", " end-user data delivery"], "paper_abstract": "Innovative scientific applications and emerging dense data sources are creating a data deluge for high-end supercomputing systems. Modern applications are often collaborative in nature, with a distributed user base for input and output data sets. Processing such large input data typically involves copying (or staging) the data onto the supercomputer's specialized high-speed storage, scratch space, for sustained high I/O throughput. This copying is crucial as remotely accessing the data while an application executes results in unnecessary delays and consequently performance degradation. However, the current practice of conservatively staging data as early as possible makes the data vulnerable to storage failures, which may entail restaging and reduced job throughput. To address this, we present a timely staging framework that uses a combination of job start-up time predictions, user-specified volunteer or cloud-based intermediate storage nodes, and decentralized data delivery to coincide input data staging with job start-up. Evaluation of our approach using both PlanetLab and Azure cloud services, as well as simulations based on three years of Jaguar supercomputer (No. 3 in Top500) job logs show as much as 91.0 percent reduction in staging times compared to direct transfers, 75.2 percent reduction in wait time on scratch, and 2.4 percent reduction in usage/hour. (An earlier version of this paper appears in [30].)", "paper_title": "On Timely Staging of HPC Job Input Data", "paper_id": "WOS:000322516200013"}