{"auto_keywords": [{"score": 0.036593342712279084, "phrase": "visual_features"}, {"score": 0.034421077210550224, "phrase": "audio_and_visual_features"}, {"score": 0.00481495049065317, "phrase": "spatio-temporal_and_audio_features"}, {"score": 0.0046164478775379105, "phrase": "human_emotion_recognition_systems"}, {"score": 0.004544127927346964, "phrase": "audio_and_spatio-temporal_visual_features"}, {"score": 0.00433386573276633, "phrase": "audio_visual_emotion_data"}, {"score": 0.004265953661191515, "phrase": "different_subjects"}, {"score": 0.004133292222875435, "phrase": "mel-frequency_cepstral_coefficient"}, {"score": 0.004090012417681691, "phrase": "mfcc"}, {"score": 0.004025885154716127, "phrase": "prosodic_features"}, {"score": 0.0038597866217852353, "phrase": "emotional_speech"}, {"score": 0.003799275066001196, "phrase": "facial_expressions"}, {"score": 0.0037793152987497286, "phrase": "spatio-temporal_features"}, {"score": 0.0037005154613244363, "phrase": "visual_streams"}, {"score": 0.0036617315529835497, "phrase": "principal_component_analysis"}, {"score": 0.0036237501771035894, "phrase": "pca"}, {"score": 0.003529149828147388, "phrase": "dimensionality_reduction"}, {"score": 0.0033480568016044, "phrase": "codebook"}, {"score": 0.0032275689111938787, "phrase": "euclidean"}, {"score": 0.0029972030224205506, "phrase": "state-of-the-art_svm_classifier"}, {"score": 0.002711506442590442, "phrase": "bayes_sum_rule"}, {"score": 0.0026270572353632297, "phrase": "final_decision_step"}, {"score": 0.0025858215477025117, "phrase": "proposed_system"}, {"score": 0.0025318431634446426, "phrase": "public_data"}, {"score": 0.0024659480471085405, "phrase": "human_emotions"}, {"score": 0.002440071551819324, "phrase": "experimental_results"}, {"score": 0.002266364679369248, "phrase": "audio_features"}, {"score": 0.0022307779508260205, "phrase": "recognition_average_accuracy"}, {"score": 0.0021049977753042253, "phrase": "overall_system_accuracy"}], "paper_keywords": ["Human computer interface (HCI)", " Multimodal system", " Human emotions", " Support vector machines (SVM)", " Spatio-temporal features"], "paper_abstract": "In this paper, we present human emotion recognition systems based on audio and spatio-temporal visual features. The proposed system has been tested on audio visual emotion data set with different subjects for both genders. The mel-frequency cepstral coefficient (MFCC) and prosodic features are first identified and then extracted from emotional speech. For facial expressions spatio-temporal features are extracted from visual streams. Principal component analysis (PCA) is applied for dimensionality reduction of the visual features and capturing 97 % of variances. Codebook is constructed for both audio and visual features using Euclidean space. Then occurrences of the histograms are employed as input to the state-of-the-art SVM classifier to realize the judgment of each classifier. Moreover, the judgments from each classifier are combined using Bayes sum rule (BSR) as a final decision step. The proposed system is tested on public data set to recognize the human emotions. Experimental results and simulations proved that using visual features only yields on average 74.15 % accuracy, while using audio features only gives recognition average accuracy of 67.39 %. Whereas by combining both audio and visual features, the overall system accuracy has been significantly improved up to 80.27 %.", "paper_title": "Human emotion recognition from videos using spatio-temporal and audio features", "paper_id": "WOS:000326929200004"}