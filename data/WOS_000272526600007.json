{"auto_keywords": [{"score": 0.04954397446159369, "phrase": "data_mining_techniques"}, {"score": 0.015719716506582538, "phrase": "reinforcement_learning"}, {"score": 0.014623299892989822, "phrase": "automatic_discovering"}, {"score": 0.004425616928975799, "phrase": "useful_temporal_abstraction"}, {"score": 0.00413684709497912, "phrase": "data_mining_algorithms"}, {"score": 0.003888659470009222, "phrase": "large_data_sets"}, {"score": 0.003823584649521733, "phrase": "state_transitions"}, {"score": 0.00378080526397273, "phrase": "action_trajectories"}, {"score": 0.003717528595128948, "phrase": "learning_agent"}, {"score": 0.0036144035798673967, "phrase": "data_sets"}, {"score": 0.0033405704229694656, "phrase": "different_regions"}, {"score": 0.003247868216333148, "phrase": "different_parts"}, {"score": 0.0028373742057129126, "phrase": "main_idea"}, {"score": 0.0027898428590137515, "phrase": "proposed_action_sequence_mining"}, {"score": 0.0026222551419772867, "phrase": "agent's_accumulated_experience"}, {"score": 0.00257831812581194, "phrase": "mined_action_sequences"}, {"score": 0.0023694065215394593, "phrase": "different_data_sets"}, {"score": 0.0023296961971785357, "phrase": "significant_speedup"}, {"score": 0.0022906498741695094, "phrase": "q-learning_algorithm"}, {"score": 0.0022020622730208514, "phrase": "state_clustering"}, {"score": 0.0021773853011112882, "phrase": "action_sequence_mining_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Reinforcement learning", " Abstraction", " Data mining", " Clustering", " Sequence mining"], "paper_abstract": "In this paper, we used data mining techniques for the automatic discovering Of useful temporal abstraction in reinforcement learning. This idea was motivated by the ability of data mining algorithms in automatic discovering of structures and patterns, when applied to large data sets. The state transitions and action trajectories of the learning agent are stored as the data sets for data mining techniques. The proposed state clustering algorithms partition the state space to different regions. Policies for reaching different parts of the space are separately learned and added to the model in a form of options (macro-actions). The main idea of the proposed action sequence mining is to search for patterns that occur frequently within an agent's accumulated experience. The mined action sequences are also added to the model in a form of options. Our experiments with different data sets indicate a significant speedup of the Q-learning algorithm using the options discovered by the state Clustering and action sequence mining algorithms. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Automatic abstraction in reinforcement learning using data mining techniques", "paper_id": "WOS:000272526600007"}