{"auto_keywords": [{"score": 0.038906160240873275, "phrase": "convolutional_network"}, {"score": 0.00481495049065317, "phrase": "circular_pairwise_convolutional_networks"}, {"score": 0.004743164872278112, "phrase": "invariant_object_recognition"}, {"score": 0.004672446555914495, "phrase": "ior"}, {"score": 0.00396050388346298, "phrase": "best_performance"}, {"score": 0.0038721890530859578, "phrase": "possible_domains"}, {"score": 0.003459191419478298, "phrase": "good_candidate"}, {"score": 0.003306559295382181, "phrase": "large_numbers"}, {"score": 0.0032571885279065126, "phrase": "training_samples"}, {"score": 0.0027811441123794427, "phrase": "invariant_features"}, {"score": 0.0025795066118838067, "phrase": "lengthy_training_time"}, {"score": 0.002447101794397243, "phrase": "circular_pairwise_classification_technique"}, {"score": 0.0023745087648471613, "phrase": "training_time"}, {"score": 0.002286781334850213, "phrase": "recognition_accuracy"}, {"score": 0.002252602622785975, "phrase": "training_time_complexity"}, {"score": 0.0021049977753042253, "phrase": "monolithic_convolutional_network"}], "paper_keywords": [""], "paper_abstract": "Invariant object recognition (IOR) has been one of the most active research areas in computer vision. However, there is no technique able to achieve the best performance in all possible domains. Out of many techniques, convolutional network (CN) is proven to be a good candidate in this area. Given large numbers of training samples of objects under various variation aspects such as lighting, pose, background, etc., convolutional network can learn to extract invariant features by itself. This comes with the price of lengthy training time. Hence, we propose a circular pairwise classification technique to shorten the training time. We compared the recognition accuracy and training time complexity between our approach and a benchmark generic object recognizer LeNet7 which is a monolithic convolutional network.", "paper_title": "Invariant object recognition using circular pairwise convolutional networks", "paper_id": "WOS:000240091500167"}