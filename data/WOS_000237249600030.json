{"auto_keywords": [{"score": 0.038554955926630094, "phrase": "negative_training_documents"}, {"score": 0.009594161745372207, "phrase": "svm_classifier"}, {"score": 0.00481495049065317, "phrase": "imbalanced_text_classification"}, {"score": 0.004725385523804256, "phrase": "support_vector_machines"}, {"score": 0.004342430078306461, "phrase": "text_classification_tasks"}, {"score": 0.004104456271230213, "phrase": "imbalanced_training"}, {"score": 0.003531474887552889, "phrase": "positive_ones"}, {"score": 0.0034332736827447654, "phrase": "generic_algorithm"}, {"score": 0.003306559295382181, "phrase": "instance_selection_algorithm"}, {"score": 0.002818013056816282, "phrase": "smaller_carefully_selected_training"}, {"score": 0.0025171669791790438, "phrase": "comparable_or_better_classification_accuracy"}, {"score": 0.002145003108466034, "phrase": "fisa"}], "paper_keywords": [""], "paper_abstract": "Support Vector Machines (SVM) classifiers are widely used in text classification tasks and these tasks often involve imbalanced training. In this paper, we specifically address the cases where negative training documents significantly outnumber the positive ones. A generic algorithm known as FISA (Feature-based Instance Selection Algorithm), is proposed to select only a subset of negative training documents for training a SVM classifier. With a smaller carefully selected training set, a SVM classifier can be more efficiently trained while delivering comparable or better classification accuracy. In our experiments on the 20-Newsgroups dataset, using only 35% negative training examples and 60% learning time, methods based on FISA delivered much better classification accuracy than those methods using all negative training documents.", "paper_title": "FISA: Feature-based instance selection for imbalanced text classification", "paper_id": "WOS:000237249600030"}