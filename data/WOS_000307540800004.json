{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "jaynes'_maximum_entropy_principle"}, {"score": 0.04787636758356028, "phrase": "latent_maximum_entropy"}, {"score": 0.0339822353069861, "phrase": "latent_maximum_entropy_principle"}, {"score": 0.004224786847720058, "phrase": "better_estimates"}, {"score": 0.004115697805448384, "phrase": "hidden_variables"}, {"score": 0.004072851768196397, "phrase": "limited_training_data"}, {"score": 0.0038854765143440965, "phrase": "latent_maximum_entropy_model"}, {"score": 0.0038249462699142733, "phrase": "hard_nonlinear_constrained_optimization_problem"}, {"score": 0.003629882559085245, "phrase": "feasible_solutions"}, {"score": 0.003444732262403694, "phrase": "special_case"}, {"score": 0.0034088473318056537, "phrase": "log-linear_models"}, {"score": 0.0032689949867583633, "phrase": "efficient_approximation"}, {"score": 0.0030538300202383106, "phrase": "iterative_scaling"}, {"score": 0.0030062165552187086, "phrase": "feasible_log-linear_solutions"}, {"score": 0.002867771524071872, "phrase": "alternating_minimization_algorithm"}, {"score": 0.002823050755743821, "phrase": "information_divergence"}, {"score": 0.002750056254309311, "phrase": "intimate_connection"}, {"score": 0.002678944072162297, "phrase": "maximum_likelihood_principles"}, {"score": 0.0026096659285441384, "phrase": "final_model"}, {"score": 0.0025156683465194967, "phrase": "feasible_candidates"}, {"score": 0.0023254633298356894, "phrase": "highest_entropy"}, {"score": 0.002183684303511462, "phrase": "better_results"}, {"score": 0.0021609076467365247, "phrase": "maximum_likelihood"}, {"score": 0.002127186380459509, "phrase": "latent_variable_models"}, {"score": 0.0021049977753042253, "phrase": "small_observed_data_samples"}], "paper_keywords": ["Maximum entropy", " iterative scaling", " expectation maximization", " latent variable models", " information geometry"], "paper_abstract": "We present an extension to Jaynes' maximum entropy principle that incorporates latent variables. The principle of latent maximum entropy we propose is different from both Jaynes' maximum entropy principle and maximum likelihood estimation, but can yield better estimates in the presence of hidden variables and limited training data. We first show that solving for a latent maximum entropy model poses a hard nonlinear constrained optimization problem in general. However, we then show that feasible solutions to this problem can be obtained efficiently for the special case of log-linear models-which forms the basis for an efficient approximation to the latent maximum entropy principle. We derive an algorithm that combines expectation-maximization with iterative scaling to produce feasible log-linear solutions. This algorithm can be interpreted as an alternating minimization algorithm in the information divergence, and reveals an intimate connection between the latent maximum entropy and maximum likelihood principles. To select a final model, we generate a series of feasible candidates, calculate the entropy of each, and choose the model that attains the highest entropy. Our experimental results show that estimation based on the latent maximum entropy principle generally gives better results than maximum likelihood when estimating latent variable models on small observed data samples.", "paper_title": "The Latent Maximum Entropy Principle", "paper_id": "WOS:000307540800004"}