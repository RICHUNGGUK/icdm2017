{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "random_tree_ensembles"}, {"score": 0.04647778853118898, "phrase": "confidence_levels"}, {"score": 0.03548517282140461, "phrase": "non-conformity_measure"}, {"score": 0.03022084635258387, "phrase": "random_forests"}, {"score": 0.004007083686875716, "phrase": "varied_application_contexts"}, {"score": 0.003970439654470374, "phrase": "model_outputs"}, {"score": 0.0038802832668554457, "phrase": "potentially_costly_decisions"}, {"score": 0.0037747835341722636, "phrase": "risk_sensitive_applications"}, {"score": 0.0037231102027070724, "phrase": "conformal_prediction_framework"}, {"score": 0.0036721416244816455, "phrase": "novel_approach"}, {"score": 0.0036218682585355895, "phrase": "valid_confidence_measures"}, {"score": 0.0035395987202335223, "phrase": "machine_learning_algorithms"}, {"score": 0.003427540517249369, "phrase": "underlying_algorithm"}, {"score": 0.002945081009084692, "phrase": "inductive_conformal_prediction"}, {"score": 0.002673957615079041, "phrase": "classification_tasks"}, {"score": 0.0026252106881752067, "phrase": "realistic_data_contexts"}, {"score": 0.0026011706232569316, "phrase": "class_imbalance"}, {"score": 0.0025537472239953807, "phrase": "non-conformity_measures"}, {"score": 0.0024842242921857705, "phrase": "predicted_class_labels"}, {"score": 0.0023507916951658455, "phrase": "multiple_data_sets"}, {"score": 0.0021539955106204354, "phrase": "conformal_prediction_random_forests"}, {"score": 0.0021049977753042253, "phrase": "associated_confidence"}], "paper_keywords": ["Prediction confidence", " Random forests", " Conformal prediction", " Classification", " Data mining"], "paper_abstract": "Obtaining an indication of confidence of predictions is desirable for many data mining applications. Predictions complemented with confidence levels can inform on the certainty or extent of reliability that may be associated with the prediction. This can be useful in varied application contexts where model outputs form the basis for potentially costly decisions, and in general across risk sensitive applications. The conformal prediction framework presents a novel approach for obtaining valid confidence measures associated with predictions from machine learning algorithms. Confidence levels are obtained from the underlying algorithm, using a non-conformity measure which indicates how 'atypical' a given example set is. The non-conformity measure is a key to determining the usefulness and efficiency of the approach. This paper considers inductive conformal prediction in the context of random tree ensembles like random forests, which have been noted to perform favorably across problems. Focusing on classification tasks, and considering realistic data contexts including class imbalance, we develop non-conformity measures for assessing the confidence of predicted class labels from random forests. We examine the performance of these measures on multiple data sets. Results demonstrate the usefulness and validity of the measures, their relative differences, and highlight the effectiveness of conformal prediction random forests for obtaining predictions with associated confidence.", "paper_title": "Confidence in predictions from random tree ensembles", "paper_id": "WOS:000317350800006"}