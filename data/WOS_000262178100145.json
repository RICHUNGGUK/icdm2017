{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaussian_kernel"}, {"score": 0.04960202571834177, "phrase": "sparse_bayesian_learning_framework"}, {"score": 0.0046491050101721545, "phrase": "kernel_based_machine_learning_techniques"}, {"score": 0.004444203710097833, "phrase": "function_approximation"}, {"score": 0.004399905285945236, "phrase": "regression_estimation"}, {"score": 0.0043560464825349275, "phrase": "relevance_vector_machine"}, {"score": 0.004184921610672245, "phrase": "art_performance"}, {"score": 0.004143196873246147, "phrase": "sparse_regression"}, {"score": 0.004060986196931943, "phrase": "popular_and_competent_kernel_function"}, {"score": 0.00396050388346298, "phrase": "conventional_gaussian_kernel"}, {"score": 0.00392100783796804, "phrase": "unified_kernel_width"}, {"score": 0.0038431888954930083, "phrase": "basis_functions"}, {"score": 0.003036333273800443, "phrase": "nonlinear_regression"}, {"score": 0.002976019715894587, "phrase": "stagewise_optimization_algorithm"}, {"score": 0.002931569771634119, "phrase": "bayesian_evidence"}, {"score": 0.0028446460307503343, "phrase": "model_selection"}, {"score": 0.0025730425700096365, "phrase": "regression_problem"}, {"score": 0.0025473475682739784, "phrase": "higher_levels"}, {"score": 0.002496722870659199, "phrase": "better_performance"}, {"score": 0.0024717881264735477, "phrase": "classical_rvm."}, {"score": 0.002447101794397243, "phrase": "attractive_ability"}, {"score": 0.0023390219342052623, "phrase": "right_kernel_widths"}, {"score": 0.002327310963701377, "phrase": "locally_fitting_rvs"}, {"score": 0.0022925278792429553, "phrase": "training_dataset"}, {"score": 0.00223570488212345, "phrase": "right_level"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Relevance vector machine (RVM)", " Gaussian kernel function", " Regression", " Gradient descent algorithm", " Bayesian evidence"], "paper_abstract": "Kernel based machine learning techniques have been widely used to tackle problems of function approximation and regression estimation. Relevance vector machine (RVM) has state of the art performance in sparse regression. As a popular and competent kernel function in machine learning. conventional Gaussian kernel has unified kernel width with each of basis functions, which make impliedly a basic assumption: the response is represented below certain frequency and the noise is represented above such certain frequency. However, in many case, this assumption does not hold. To overcome this limitation, a novel adaptive spherical Gaussian kernel is utilized for nonlinear regression, and the stagewise optimization algorithm for maximizing Bayesian evidence in sparse Bayesian learning framework is proposed for model selection. Extensive empirical study, on two artificial datasets and two real-world benchmark datasets. shows its effectiveness and flexibility of model on representing regression problem with higher levels of sparsity and better performance than classical RVM. The attractive ability of this approach is to automatically choose the right kernel widths locally fitting RVs from the training dataset, which could keep right level smoothing at each scale of signal. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Adaptive spherical Gaussian kernel in sparse Bayesian learning framework for nonlinear regression", "paper_id": "WOS:000262178100145"}