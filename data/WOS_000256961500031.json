{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "non-negative_matrix_factorization"}, {"score": 0.004640079189342692, "phrase": "popular_technique"}, {"score": 0.004583205112737672, "phrase": "pattern_recognition"}, {"score": 0.004527024978801684, "phrase": "data_analysis"}, {"score": 0.004444037594955185, "phrase": "dimensionality_reduction"}, {"score": 0.004204054047427536, "phrase": "non-negative_data_matrix_x"}, {"score": 0.004076352180903171, "phrase": "basis_matrix_a"}, {"score": 0.004026359831748917, "phrase": "encoding_variable_matrix_s"}, {"score": 0.003625347557965931, "phrase": "amari's_alpha-divergence"}, {"score": 0.0034294219963679857, "phrase": "multiplicative_updating_algorithm"}, {"score": 0.0030309961956273028, "phrase": "monotonic_convergence"}, {"score": 0.002938821840519756, "phrase": "auxiliary_functions"}, {"score": 0.0025972447844047515, "phrase": "projected_gradient"}, {"score": 0.0025027144548626975, "phrase": "image_denoising"}, {"score": 0.002471973283582411, "phrase": "eeg_classification"}, {"score": 0.0024116163689723333, "phrase": "interesting_and_useful_behavior"}, {"score": 0.00230950802983053, "phrase": "different_values"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["alpha-divergence", " multiplicative updates", " non-negative matrix factorization", " projected gradient"], "paper_abstract": "Non-negative matrix factorization (NMF) is a popular technique for pattern recognition, data analysis, and dimensionality reduction, the goal of which is to decompose non-negative data matrix X into a product of basis matrix A and encoding variable matrix S with both A and S allowed to have only non-negative elements. In this paper, we consider Amari's alpha-divergence as a discrepancy measure and rigorously derive a multiplicative updating algorithm (proposed in our recent work) which iteratively minimizes the alpha-divergence between X and AS. We analyze and prove the monotonic convergence of the algorithm using auxiliary functions. In addition, we show that the same algorithm can be also derived using Karush-Kuhn-Tucker (KKT) conditions as well as the projected gradient. We provide two empirical study for image denoising and EEG classification, showing the interesting and useful behavior of the algorithm in cases where different values of alpha (alpha = 0.5,1,2) are used. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Non-negative matrix factorization with alpha-divergence", "paper_id": "WOS:000256961500031"}