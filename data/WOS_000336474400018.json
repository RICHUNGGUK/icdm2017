{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "triple_level_environment"}, {"score": 0.007941627654030961, "phrase": "proposed_method"}, {"score": 0.00462773520532301, "phrase": "learning_automata"}, {"score": 0.004567245761518841, "phrase": "la"}, {"score": 0.004477270137801732, "phrase": "great_efforts"}, {"score": 0.004303127794432684, "phrase": "p-model_automaton"}, {"score": 0.0041357305373417455, "phrase": "stochastic_environment"}, {"score": 0.0037699306119313154, "phrase": "stochastic_environments"}, {"score": 0.0035285548582081627, "phrase": "general_approach"}, {"score": 0.003459191419478298, "phrase": "p-model_fixed_structure_stochastic_automata"}, {"score": 0.0033025823350563087, "phrase": "optimal_action"}, {"score": 0.003010250522930773, "phrase": "small_scale_penalty"}, {"score": 0.0029706504420983896, "phrase": "large_scale_penalty"}, {"score": 0.002893001738785978, "phrase": "existing_p-model_fssa_algorithms"}, {"score": 0.0027256122357680393, "phrase": "existing_systems"}, {"score": 0.002671990515821213, "phrase": "performance_evaluation"}, {"score": 0.002602127996667857, "phrase": "compatible_stochastic_environment"}, {"score": 0.0025678829712650437, "phrase": "incompatible_environment"}, {"score": 0.0025340874787959195, "phrase": "boundary_environment"}, {"score": 0.002387415084371473, "phrase": "simulation_results"}, {"score": 0.0022049428043454966, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["P-model", " Q-model", " Learning Automata", " epsilon-Optimal", " Triple level environment", " Compatible environment"], "paper_abstract": "Within the context of Learning Automata (LA), great efforts have been made to deliver P-model automaton which accepts two values from the stochastic environment, i.e., reward or penalty. However, in many real life problems, the stochastic environments have more than two feedback choices. This paper aims at proposing a general approach for extending P-model Fixed Structure Stochastic Automata (FSSA) to learn the optimal action in a triple level environment which provides three choices of feedback, i.e., reward, small scale penalty and large scale penalty while keeping the existing P-model FSSA algorithms intact in order not to impose much impact on existing systems. For performance evaluation, we introduce compatible stochastic environment, incompatible environment and boundary environment to assess the performance of the proposed method. Simulation results demonstrate that the proposed method is expedient in dealing with triple level environment. Crown Copyright (C) 2014 Published by Elsevier B.V. All rights reserved.", "paper_title": "A general method for P-model FSSA learning in triple level environment", "paper_id": "WOS:000336474400018"}