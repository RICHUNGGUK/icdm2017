{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "analytical_usability_evaluation_methods"}, {"score": 0.00462919213721188, "phrase": "empirical_evaluation"}, {"score": 0.003928969293303814, "phrase": "detailed_comparisons"}, {"score": 0.00389048276567065, "phrase": "single_interface"}, {"score": 0.0035489064326306263, "phrase": "uem"}, {"score": 0.0034118234596993836, "phrase": "detailed_comparison"}, {"score": 0.0033894950343858967, "phrase": "eight_analytical_uems"}, {"score": 0.003356275163203055, "phrase": "eight_methods"}, {"score": 0.0033016282588376397, "phrase": "robotic_arm_interface"}, {"score": 0.0032054891061528896, "phrase": "video_data"}, {"score": 0.0031636612155559267, "phrase": "ill_use"}, {"score": 0.0031326477464204768, "phrase": "usability_issues"}, {"score": 0.003051418904573872, "phrase": "five_categories"}, {"score": 0.00290473242917264, "phrase": "physical_issues"}, {"score": 0.0028762499230524812, "phrase": "contextual_ones"}, {"score": 0.0028293966058473476, "phrase": "user_experience"}, {"score": 0.0027741761880779535, "phrase": "particular_study"}, {"score": 0.0027200305495826797, "phrase": "heuristic_evaluation"}, {"score": 0.0026321198895843173, "phrase": "analytical_method"}, {"score": 0.0024891269874339553, "phrase": "multimodal_usability"}, {"score": 0.0024485643104830814, "phrase": "analysis_of_surface_and_structural_misfits"}, {"score": 0.0024007585788965655, "phrase": "particular_niches"}, {"score": 0.0021968983291791784, "phrase": "particular_method"}, {"score": 0.0021049977753042253, "phrase": "problem_count"}], "paper_keywords": [""], "paper_abstract": "Analytical usability evaluation methods (UEMs) can complement empirical evaluation of systems: for example, they can often be used earlier in design and can provide accounts of why users might experience difficulties, as well as what those difficulties are. However, their properties and value are only partially understood. One way to improve our understanding is by detailed comparisons using a single interface or system as a target for evaluation, but we need to look deeper than simple problem counts: we need to consider what kinds of accounts each UEM offers, and why. Here, we report on a detailed comparison of eight analytical UEMs. These eight methods were applied to it robotic arm interface, and the findings were systematically compared against video data of the arm ill use. The usability issues that were identified could be grouped into five categories: system design, user misconceptions, conceptual fit between user and system, physical issues, and contextual ones. Other possible categories such as User experience did not emerge in this particular study. With the exception of Heuristic Evaluation, which supported a range of insights, each analytical method was found to focus attention on just one or two categories of issues. Two of the three \"home-grown\" methods (Evaluating Multimodal Usability and Concept-based Analysis of Surface and Structural Misfits) were found to occupy particular niches in the space, whereas the third (Programmable User Modeling) did not. This approach has identified commonalities and contrasts between methods and provided accounts of why a particular method yielded the insights it did. Rather than considering measures such as problem count or thoroughness, this approach has yielded insights into the scope of each method.", "paper_title": "Scoping analytical usability evaluation methods: A case study", "paper_id": "WOS:000259373500003"}