{"auto_keywords": [{"score": 0.04734457699310803, "phrase": "human_actions"}, {"score": 0.04577290335074107, "phrase": "low-level_features"}, {"score": 0.028827055276017907, "phrase": "temporal_context"}, {"score": 0.00481495049065317, "phrase": "discriminative_mid-level_feature"}, {"score": 0.004417487764326481, "phrase": "existing_approaches"}, {"score": 0.004149103940568346, "phrase": "action_video"}, {"score": 0.003958529104387973, "phrase": "complex_environments"}, {"score": 0.0039122637074546895, "phrase": "cluttered_background"}, {"score": 0.0038817197332605647, "phrase": "camera_movement"}, {"score": 0.003732526540522333, "phrase": "novel_random_forest_learning_framework"}, {"score": 0.003674461410107989, "phrase": "discriminative_and_informative_mid-level_feature"}, {"score": 0.0034646205853140558, "phrase": "corresponding_random_forests"}, {"score": 0.003424107910734022, "phrase": "novel_fusion_scheme"}, {"score": 0.0033708244679086265, "phrase": "cuboid's_posterior_probabilities"}, {"score": 0.003056029284235528, "phrase": "single_low-level_feature"}, {"score": 0.002926973459215349, "phrase": "multiple_complementary_low-level_features"}, {"score": 0.0026954938318408464, "phrase": "local_cuboids"}, {"score": 0.002622465142275264, "phrase": "low-level_feature"}, {"score": 0.0024150092189446424, "phrase": "proposed_learning_framework"}, {"score": 0.0023680783509308525, "phrase": "mid-level_feature"}, {"score": 0.0023220573674955776, "phrase": "random_forest_classifier"}, {"score": 0.0023038997972591193, "phrase": "robust_action_recognition"}, {"score": 0.0022591245742576093, "phrase": "weizmann"}, {"score": 0.0022414566728770745, "phrase": "ucf_sports"}, {"score": 0.0021383149182448522, "phrase": "multiple_low-level_features"}, {"score": 0.0021049977753042253, "phrase": "superior_performance"}], "paper_keywords": ["action recognition", " mid-level feature", " feature fusion", " temporal context"], "paper_abstract": "In this paper, we address the problem of recognizing human actions from videos. Most of the existing approaches employ low-level features (e. g., local features and global features) to represent an action video. However, algorithms based on low-level features are not robust to complex environments such as cluttered background, camera movement and illumination change. Therefore, we propose a novel random forest learning framework to construct a discriminative and informative mid-level feature from low-level features of densely sampled 3D cuboids. Each cuboid is classified by the corresponding random forests with a novel fusion scheme, and the cuboid's posterior probabilities of all categories are normalized to generate a histogram. After that, we obtain our mid-level feature by concatenating histograms of all the cuboids. Since a single low-level feature is not enough to capture the variations of human actions, multiple complementary low-level features (i.e., optical flow and histogram of gradient 3D features) are employed to describe 3D cuboids. Moreover, temporal context between local cuboids is exploited as another type of low-level feature. The above three low-level features (i.e., optical flow, histogram of gradient 3D features and temporal context) are effectively fused in the proposed learning framework. Finally, the mid-level feature is employed by a random forest classifier for robust action recognition. Experiments on the Weizmann, UCF sports, Ballet, and multi-view IXMAS datasets demonstrate that out mid-level feature learned from multiple low-level features can achieve a superior performance over state-of-the-art methods.", "paper_title": "Learning a discriminative mid-level feature for action recognition", "paper_id": "WOS:000334860600017"}