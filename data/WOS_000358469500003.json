{"auto_keywords": [{"score": 0.04092662713864431, "phrase": "specialized_low-level_network_layers"}, {"score": 0.00481495049065317, "phrase": "new_dynamic_credit-based_end-to-end_flow_control_mechanism"}, {"score": 0.004765854932591942, "phrase": "hpc"}, {"score": 0.004481287195546646, "phrase": "mpi"}, {"score": 0.004367582951338992, "phrase": "openshmem"}, {"score": 0.004085579055228594, "phrase": "large-scale_clusters"}, {"score": 0.003724810609448597, "phrase": "hardware_interconnects"}, {"score": 0.0036678537886417895, "phrase": "infiniband"}, {"score": 0.003611751858608048, "phrase": "gb_ethernet"}, {"score": 0.0035201528214608914, "phrase": "extoll"}, {"score": 0.0034663083890589235, "phrase": "emerging_network"}, {"score": 0.003413284742632989, "phrase": "high_performance_clusters"}, {"score": 0.003292685392413911, "phrase": "flow_control"}, {"score": 0.003209152260950748, "phrase": "buffer_overflows"}, {"score": 0.003160049822855747, "phrase": "receiver_side"}, {"score": 0.0030327408069772293, "phrase": "new_end-to-end_flow_control_mechanism"}, {"score": 0.002895620643297197, "phrase": "execution_time"}, {"score": 0.002851301751761804, "phrase": "buffer_resources"}, {"score": 0.0027363971333961967, "phrase": "communication_pattern"}, {"score": 0.0026945086500597304, "phrase": "parallel_application"}, {"score": 0.0026532596852754525, "phrase": "varying_activity"}, {"score": 0.0026261108615242557, "phrase": "communicating_peers"}, {"score": 0.0024062532032402533, "phrase": "extraordinarily_high_buffer_efficiency"}, {"score": 0.0023694065215394593, "phrase": "overall_buffer_resources"}, {"score": 0.002193447487477984, "phrase": "static_flow_control_protocol"}, {"score": 0.0021709933368474223, "phrase": "similar_low_overhead_levels"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["EXTOLL", " VELO", " RMA", " MPI", " Dynamic flow control", " Static flow control"], "paper_abstract": "High Performance Computing usually leverages messaging libraries such as MPI, GASNet, or OpenSHMEM, among others, in order to exchange data among processes in large-scale clusters. Furthermore, these libraries make use of specialized low-level network layers in order to achieve as much performance as possible from hardware interconnects such as InfiniBand or 40 Gb Ethernet, for example. EXTOLL is an emerging network targeted at high performance clusters. Specialized low-level network layers require some kind of flow control in order to prevent buffer overflows at the receiver side. In this paper we present a new end-to-end flow control mechanism that is able to dynamically adapt, at execution time, the buffer resources used by a process according to the communication pattern of the parallel application and the varying activity among communicating peers. The tests carried out on a 64-node 1024-core EXTOLL cluster show that our new dynamic flow control mechanism presents very low overhead with an extraordinarily high buffer efficiency, as overall buffer resources are reduced by 4x with respect to the amount of buffers required by a static flow control protocol achieving similar low overhead levels. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "On the design of a new dynamic credit-based end-to-end flow control mechanism for HPC clusters", "paper_id": "WOS:000358469500003"}