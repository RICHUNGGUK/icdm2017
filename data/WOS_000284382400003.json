{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "k-median_problem"}, {"score": 0.011434897902675514, "phrase": "i-median_problem"}, {"score": 0.004712988631183401, "phrase": "arbitrary_dissimilarity_measure"}, {"score": 0.0046928556063004214, "phrase": "d._given"}, {"score": 0.004419819325202464, "phrase": "set_c"}, {"score": 0.0038702288460744274, "phrase": "main_result"}, {"score": 0.0033026956021014204, "phrase": "random_sample"}, {"score": 0.003274499008699115, "phrase": "constant_size"}, {"score": 0.0026312238938540787, "phrase": "arbitrary_metric_space"}, {"score": 0.002608744950946475, "phrase": "bounded_doubling_dimension"}, {"score": 0.002564360070426064, "phrase": "kullback-leibler_divergence"}, {"score": 0.002477837342313456, "phrase": "itakura-saito_divergence"}, {"score": 0.0024461474613359994, "phrase": "mahalanobis_distances"}, {"score": 0.0023942269247089277, "phrase": "special_cases"}, {"score": 0.002373767863833987, "phrase": "bregman_divergences"}, {"score": 0.0023134312557290043, "phrase": "previously_known_results"}, {"score": 0.002283839204519042, "phrase": "median_problem"}, {"score": 0.002216251512062482, "phrase": "simplified_manner"}, {"score": 0.002150659690967412, "phrase": "new_analysis"}, {"score": 0.0021049977753042253, "phrase": "kumar_et_al"}], "paper_keywords": ["Approximation algorithm", " Bregman divergences", " Itakura-Saito divergence", " k-means clustering", " k-median clustering", " Kullback-Leibler divergence", " Mahalanobis distance", " random sampling"], "paper_abstract": "We study a generalization of the k-median problem with respect to an arbitrary dissimilarity measure D. Given a finite set P of size n, our goal is to find a set C of size k such that the sum of errors D(P, C) = Sigma(p is an element of P) min(c is an element of C) {D(p, c)} is minimized. The main result in this article can be stated as follows: There exists a (1 + epsilon)-approximation algorithm for the k-median problem with respect to D, if the I-median problem can be approximated within a factor of (1 + epsilon) by taking a random sample of constant size and solving the I-median problem on the sample exactly. This algorithm requires time n2(O(mklog(mk/epsilon))), where m is a constant that depends only on epsilon and D. Using this characterization, we obtain the first linear time (1 + epsilon)-approximation algorithms for the k-median problem in an arbitrary metric space with bounded doubling dimension, for the Kullback-Leibler divergence (relative entropy), for the Itakura-Saito divergence, for Mahalanobis distances, and for some special cases of Bregman divergences. Moreover, we obtain previously known results for the Euclidean k-median problem and the Euclidean k-means problem in a simplified manner. Our results are based on a new analysis of an algorithm of Kumar et al. [2004].", "paper_title": "Clustering for Metric and Nonmetric Distance Measures", "paper_id": "WOS:000284382400003"}