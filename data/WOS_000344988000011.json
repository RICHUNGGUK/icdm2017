{"auto_keywords": [{"score": 0.023153759706908136, "phrase": "adl"}, {"score": 0.00481495049065317, "phrase": "learning_human_actions"}, {"score": 0.004741845603430361, "phrase": "global_dynamics"}, {"score": 0.0044150318724093226, "phrase": "human_action_recognition"}, {"score": 0.004347972379250724, "phrase": "global_temporal_dynamics"}, {"score": 0.004303830426772906, "phrase": "local_visual_spatio-temporal_appearance"}, {"score": 0.00411064950774695, "phrase": "global_temporal_dimension"}, {"score": 0.004060159024003153, "phrase": "weizmann"}, {"score": 0.003966388754366831, "phrase": "motion_dynamics"}, {"score": 0.003926105480202798, "phrase": "robust_linear_dynamical_systems"}, {"score": 0.003788296551609616, "phrase": "model_parameters"}, {"score": 0.0037498152007206815, "phrase": "motion_descriptors"}, {"score": 0.0036181717017041387, "phrase": "non-euclidean_space"}, {"score": 0.003509006038574667, "phrase": "non-vector_form"}, {"score": 0.00342054627009158, "phrase": "shift_invariant_subspace_angles"}, {"score": 0.0032172054825137866, "phrase": "local_visual_dimension"}, {"score": 0.0031521392193570846, "phrase": "curved_spatio-temporal_cuboids"}, {"score": 0.0030726480920918097, "phrase": "densely_sampled_feature_points"}, {"score": 0.002964706925144123, "phrase": "oriented_gradients"}, {"score": 0.0028574670439467684, "phrase": "virat"}, {"score": 0.0028459677232555176, "phrase": "motion_sequences"}, {"score": 0.0027741761880779535, "phrase": "chi-squared_histogram_distance"}, {"score": 0.002731971103291716, "phrase": "bag-of-words_framework"}, {"score": 0.002622528664187849, "phrase": "maximum_margin_distance_learning_method"}, {"score": 0.0025694584194998356, "phrase": "global_dynamic_distances"}, {"score": 0.002530359917257626, "phrase": "local_visual_distances"}, {"score": 0.002453934333239467, "phrase": "action_recognition"}, {"score": 0.0024289743270061157, "phrase": "five_short_clips_data_sets"}, {"score": 0.0021049977753042253, "phrase": "competitive_results"}], "paper_keywords": ["Action recognition", " linear dynamical system", " local spatio-temporal feature", " non-vector descriptor", " distance learning"], "paper_abstract": "In this paper, we address the problem of human action recognition through combining global temporal dynamics and local visual spatio-temporal appearance features. For this purpose, in the global temporal dimension, we propose to model the motion dynamics with robust linear dynamical systems (LDSs) and use the model parameters as motion descriptors. Since LDSs live in a non-euclidean space and the descriptors are in non-vector form, we propose a shift invariant subspace angles based distance to measure the similarity between LDSs. In the local visual dimension, we construct curved spatio-temporal cuboids along the trajectories of densely sampled feature points and describe them using histograms of oriented gradients (HOG). The distance between motion sequences is computed with the Chi-Squared histogram distance in the bag-of-words framework. Finally we perform classification using the maximum margin distance learning method by combining the global dynamic distances and the local visual distances. We evaluate our approach for action recognition on five short clips data sets, namely Weizmann, KTH, UCF sports, Hollywood2 and UCF50, as well as three long continuous data sets, namely VIRAT, ADL and CRIM13. We show competitive results as compared with current state-of-the-art methods.", "paper_title": "Learning Human Actions by Combining Global Dynamics and Local Appearance", "paper_id": "WOS:000344988000011"}