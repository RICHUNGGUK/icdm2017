{"auto_keywords": [{"score": 0.04707681335767127, "phrase": "linpack_benchmark"}, {"score": 0.041579187248500515, "phrase": "performance_improvement"}, {"score": 0.04072472046898861, "phrase": "cpu-only_case"}, {"score": 0.03170047508375886, "phrase": "main_computation_task"}, {"score": 0.0277504128817395, "phrase": "computation_tasks"}, {"score": 0.00481495049065317, "phrase": "computation-communication_overlap_of_linpack"}, {"score": 0.00476334344613564, "phrase": "gpu-accelerated_pc_cluster"}, {"score": 0.004562362497952548, "phrase": "enhanced_performance"}, {"score": 0.00446506039438222, "phrase": "cpu-accelerated_pc_cluster"}, {"score": 0.00441718685609235, "phrase": "relatively_slow_inter-node_connections"}, {"score": 0.0041703614123283165, "phrase": "cpu-gpu_parallel_double-precision"}, {"score": 0.0034716556902775037, "phrase": "computation-communication_overlap_scheme"}, {"score": 0.00326572165784246, "phrase": "cpu-accelerated_high-performance_unpack_benchmark"}, {"score": 0.0031164983132321417, "phrase": "main_inter-node_communication"}, {"score": 0.0030499378654797143, "phrase": "gpu_device_memory"}, {"score": 0.002963390095037466, "phrase": "cpu_cores"}, {"score": 0.002910545717767559, "phrase": "multi-core_processors"}, {"score": 0.00284837132359445, "phrase": "today's_high-performance_computers"}, {"score": 0.0027378045447760705, "phrase": "cpu_core"}, {"score": 0.0027181663098561066, "phrase": "communication_tasks"}, {"score": 0.002538470301402269, "phrase": "inter-node_communication"}, {"score": 0.0024223970976460173, "phrase": "smaller_tasks"}, {"score": 0.0023199580436336163, "phrase": "cpu_computation_power"}, {"score": 0.0022058958906038466, "phrase": "optimal_computation_ratio"}, {"score": 0.0021202154023229123, "phrase": "parallel_dgemm_operation"}], "paper_keywords": ["parallel processing", " multi-core processor", " GPU", " computation-communication overlap"], "paper_abstract": "In this paper, we propose an approach to obtaining enhanced performance of the Linpack benchmark on a CPU-accelerated PC cluster connected via relatively slow inter-node connections. For one node with a quad-core Intel Xeon W3520 processor and a NVIDIA Testa C1060 CPU card, we implement a CPU-GPU parallel double-precision general matrix-matrix multiplication (dgemm) operation, and achieve a performance improvement of 34% compared with the CPU-only case and 64% compared with the CPU-only case. For an entire 16-node cluster, each node of which is the same as the above and is connected with two gigabit Ethernet links, we use a computation-communication overlap scheme with CPU acceleration for the Linpack benchmark, and achieve a performance improvement of 28% compared with the CPU-accelerated high-performance Unpack benchmark (HPL) without overlapping. Our overlap CPU acceleration solution uses overlaps in which the main inter-node communication and data transfer to the GPU device memory are overlapped with the main computation task on the CPU cores. These overlaps use multi-core processors, which almost all of today's high-performance computers use. In particular, as well as using a CPU core for communication tasks, we also simultaneously use other CPU cores and the CPU for computation tasks. In order to enable overlap between inter-node communication and computation tasks, we eliminate their close dependence by breaking the main computation task into smaller tasks and rescheduling. Based on a scheme in which part of the CPU computation power is simultaneously used for tasks other than computation tasks, we experimentally find the optimal computation ratio for CPUs; this ratio differs from the case of parallel dgemm operation of one node.", "paper_title": "Computation-Communication Overlap of Linpack on a GPU-Accelerated PC Cluster", "paper_id": "WOS:000298304900004"}