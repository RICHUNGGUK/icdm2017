{"auto_keywords": [{"score": 0.04810170554931846, "phrase": "divergence_criterion"}, {"score": 0.045458312844898874, "phrase": "feature_subset"}, {"score": 0.02998999927405984, "phrase": "novovicova_et_al"}, {"score": 0.00481495049065317, "phrase": "-independent_feature_selection"}, {"score": 0.004539005582217309, "phrase": "feature_selection"}, {"score": 0.004102096151528319, "phrase": "original_feature"}, {"score": 0.003965975628742746, "phrase": "practical_cases"}, {"score": 0.0032938942910948096, "phrase": "underlying_information"}, {"score": 0.0026895647154863405, "phrase": "classifier-independent_feature_selection_method"}, {"score": 0.0022145052062614514, "phrase": "feature_subset_size"}], "paper_keywords": ["classifier-independent feature selection", " Bayes classifier", " Gaussian mixture", " garbage feature", " J-divergence", " two-stage feature selection"], "paper_abstract": "Feature selection aims to choose a feature subset that has the most discriminative information from the original feature set. In practical cases, it is preferable to select a feature subset that is universally effective for any kind of classifier because there is no underlying information about a given dataset. Such a trial is called classifier-independent feature selection. We took notice of Novovicova et al.'s study as a classifier-independent feature selection method. However, the number of features have to be selected beforehand in their method. It is more desirable to determine a feature subset size automatically so as to remove only garbage features. In this study, we propose a divergence criterion on the basis of Novovicova et al.'s method.", "paper_title": "Classifier-independent feature selection on the basis of divergence criterion", "paper_id": "WOS:000240830900003"}