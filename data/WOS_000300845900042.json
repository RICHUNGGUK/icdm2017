{"auto_keywords": [{"score": 0.0475341023543958, "phrase": "tsp"}, {"score": 0.00481495049065317, "phrase": "mutual_information_estimation"}, {"score": 0.004703671193348433, "phrase": "new_histogram-based_mutual_information_estimator"}, {"score": 0.004630910803911567, "phrase": "data-driven_tree-structured_partitions"}, {"score": 0.004250302819248558, "phrase": "derived_tsp"}, {"score": 0.004024485331377682, "phrase": "regularized_empirical_information_maximization"}, {"score": 0.003781005930058317, "phrase": "good_tradeoff"}, {"score": 0.003693533648584145, "phrase": "known_estimation"}, {"score": 0.0035245918797629804, "phrase": "distribution-free_concentration_inequality"}, {"score": 0.003443031148055198, "phrase": "tree-structured_learning_problem"}, {"score": 0.0033372020049592726, "phrase": "finite_sample_performance_bounds"}, {"score": 0.0032599633778910516, "phrase": "proposed_histogram-based_solution"}, {"score": 0.0027240799841720957, "phrase": "arbitrary_high_probability"}, {"score": 0.002579140290642792, "phrase": "mentioned_estimation"}, {"score": 0.0024040357029838774, "phrase": "emblematic_scenario"}, {"score": 0.002188874450946842, "phrase": "tsp_estimate"}], "paper_keywords": ["Complexity regularization", " data-dependent partitions", " histogram-based estimates", " minimum cost tree pruning", " mutual information (MI)", " strong consistency", " tree-structured partitions (TSPs)", " Vapnik and Chervonenkis inequality"], "paper_abstract": "A new histogram-based mutual information estimator using data-driven tree-structured partitions (TSP) is presented in this paper. The derived TSP is a solution to a complexity regularized empirical information maximization, with the objective of finding a good tradeoff between the known estimation and approximation errors. A distribution-free concentration inequality for this tree-structured learning problem as well as finite sample performance bounds for the proposed histogram-based solution is derived. It is shown that this solution is density-free strongly consistent and that it provides, with an arbitrary high probability, an optimal balance between the mentioned estimation and approximation errors. Finally, for the emblematic scenario of independence, I(X;Y), it is shown that the TSP estimate converges to zero with O(e(-n1/3+log log n)).", "paper_title": "Complexity-Regularized Tree-Structured Partition for Mutual Information Estimation", "paper_id": "WOS:000300845900042"}