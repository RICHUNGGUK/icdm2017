{"auto_keywords": [{"score": 0.049306105869104874, "phrase": "musical_emotions"}, {"score": 0.007778369738610245, "phrase": "similar_sequence_patterns"}, {"score": 0.006639328588272245, "phrase": "music_retrieval"}, {"score": 0.00481495049065317, "phrase": "dynamic_time_warping_for_music_retrieval"}, {"score": 0.004775918707955445, "phrase": "time_series_modeling"}, {"score": 0.0046987973264610685, "phrase": "musical_signals"}, {"score": 0.004660702714929615, "phrase": "rich_temporal_information"}, {"score": 0.004566805735863274, "phrase": "physical_level"}, {"score": 0.004493045859210086, "phrase": "emotion_level"}, {"score": 0.00434906528856802, "phrase": "music_excerpts"}, {"score": 0.004244374148143989, "phrase": "emotion_dynamics"}, {"score": 0.004041686156098498, "phrase": "emotion-based_music_retrieval_concentrate"}, {"score": 0.0038962096720793443, "phrase": "dynamic_analysis"}, {"score": 0.003710323357333052, "phrase": "novel_approach"}, {"score": 0.0036207206487324506, "phrase": "time-varying_musical_emotion_dynamics"}, {"score": 0.003217124533918232, "phrase": "musical_emotion_dynamics"}, {"score": 0.0031780228058373235, "phrase": "time_series"}, {"score": 0.0029773436815290215, "phrase": "expectation_maximization"}, {"score": 0.00289359158300027, "phrase": "kalman"}, {"score": 0.0028006988338192375, "phrase": "model_parameters"}, {"score": 0.0024984517279573906, "phrase": "optimal_solution"}, {"score": 0.002388817995161952, "phrase": "subsequence_dynamic_time"}, {"score": 0.0022933216359142736, "phrase": "experimental_results"}, {"score": 0.0022470134186921573, "phrase": "mdt"}, {"score": 0.0022196745041124888, "phrase": "time-varying_musical_emotions"}, {"score": 0.0021309254505358253, "phrase": "retrieval_methods"}, {"score": 0.0021049977753042253, "phrase": "acoustic_features"}], "paper_keywords": ["Musical emotion", " multiple dynamic textures", " EM algorithm", " Kalman filter and smoother", " dynamic time warping"], "paper_abstract": "Musical signals have rich temporal information not only at the physical level but at the emotion level. The listeners may wish to find music excerpts that have similar sequence patterns of musical emotions with given excerpts. Most state-of-the-art systems for emotion-based music retrieval concentrate on static analysis of musical emotions, and ignore dynamic analysis and modeling of musical emotions over time. This paper presents a novel approach to perform music retrieval based on time-varying musical emotion dynamics. A three-dimensional musical emotion model-Resonance-Arousal-Valence (RAV)-is used, and emotions of a piece of music are represented by musical emotion dynamics in a time series. A multiple dynamic textures (MDT) model is proposed to model music and emotion dynamics over time, and expectation maximization (EM) algorithm along with Kalman filtering and smoothing is used to estimate model parameters. Two smoothing methods-Rauch-Tung-Striebel (RTS) and minimum-variance smoothing (MVS)-to robust model are investigated and compared to find an optimal solution to enhance prediction. To find similar sequence patterns of musical emotions, subsequence dynamic time warping (DTW) for emotion dynamics matching is presented. Experimental results demonstrate the benefits of MDT to predict time-varying musical emotions, and our proposed method for music retrieval based on emotion dynamics outperforms retrieval methods based on acoustic features.", "paper_title": "Dynamic Time Warping for Music Retrieval Using Time Series Modeling of Musical Emotions", "paper_id": "WOS:000356172600007"}