{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "community_atmosphere_model"}, {"score": 0.004752809102427463, "phrase": "century-long_global_climate_simulations"}, {"score": 0.004691465925553365, "phrase": "large_amounts"}, {"score": 0.004630910803911567, "phrase": "parallel_architecture"}, {"score": 0.004453874101119545, "phrase": "ncar_community_climate_system_model"}, {"score": 0.004339609346924681, "phrase": "serious_bottleneck"}, {"score": 0.004130487085777539, "phrase": "novel_remapping"}, {"score": 0.0040771431404398855, "phrase": "parallel_netcdf_library"}, {"score": 0.004014035333627087, "phrase": "cam_history_variables"}, {"score": 0.003972504904621217, "phrase": "disk_file"}, {"score": 0.00394163819159128, "phrase": "different_index_order"}, {"score": 0.0038907236419588255, "phrase": "cpu_resident_memory"}, {"score": 0.003860489913620456, "phrase": "parallel_decomposition"}, {"score": 0.0038304902197108643, "phrase": "index_reshuffle"}, {"score": 0.0036458121607783004, "phrase": "distributed_architecture"}, {"score": 0.0034881076466982226, "phrase": "last_array_dimension"}, {"score": 0.0034609916645637288, "phrase": "data_transfer"}, {"score": 0.0034251635157421033, "phrase": "maximum_block_sizes"}, {"score": 0.0033198817440839795, "phrase": "recently_developed_parallel_netcdf_library"}, {"score": 0.003234615195382533, "phrase": "long-standing_issue"}, {"score": 0.0032178256466508406, "phrase": "netcdf_data_format"}, {"score": 0.0031762308449055305, "phrase": "climate_system_models"}, {"score": 0.0031597433845085092, "phrase": "benchmark_tests"}, {"score": 0.0031107911390188055, "phrase": "different_resolutions"}, {"score": 0.002759769970554556, "phrase": "fv-c"}, {"score": 0.0026471739851621143, "phrase": "standard_single_history_output"}, {"score": 0.0025724335893708627, "phrase": "total_size"}, {"score": 0.0025591351793770255, "phrase": "gb"}, {"score": 0.002366763442497185, "phrase": "estimated_time"}, {"score": 0.00234834406550549, "phrase": "fv_d-resolution"}, {"score": 0.0023361829580250803, "phrase": "ibm"}, {"score": 0.0021049977753042253, "phrase": "memory_usage_limitation"}], "paper_keywords": ["CAM", " climate modeling", " index reshuffle", " parallel I/O", " parallel netCDF"], "paper_abstract": "Century-long global climate simulations at high resolutions generate large amounts of data in a parallel architecture. Currently, the community atmosphere model (CAM), the atmospheric component of the NCAR community climate system model (CCSM), uses sequential I/O which causes a serious bottleneck for these simulations. We describe the parallel I/O development of CAM in this paper. The parallel I/O combines a novel remapping of 3-D arrays with the parallel netCDF library as the I/O interface. Because CAM history variables are stored in disk file in a different index order than the one in CPU resident memory because of parallel decomposition, an index reshuffle is done on the fly. Our strategy is first to remap 3-D arrays from its native decomposition to z-decomposition on a distributed architecture, and from there write data out to disk. Because z-decomposition is consistent with the last array dimension, the data transfer can occur at maximum block sizes and, therefore, achieve maximum I/O bandwidth. We also incorporate the recently developed parallel netCDF library at Argonne/Northwestern as the collective I/O interface, which resolves a long-standing issue because netCDF data format is extensively used in climate system models. Benchmark tests are performed on several platforms using different resolutions. We test the performance of our new parallel I/O on five platforms (SP3, SP4, SP5, Cray X1E, BlueGene/L) up to 1024 processors. More than four realistic model resolutions are examined, e. g. EUL T85 (similar to 1.4 degrees), FV-B (2 degrees x 2.5 degrees), FV-C (1 degrees x 1.25 degrees), and FV-D (0.5 degrees x 0.625 degrees) resolutions. For a standard single history output of CAM 3.1 FV-D resolution run (multiple 2-D and 3-D arrays with total size 4.1 GB), our parallel I/O speeds up by a factor of 14 on IBM SP3, compared with the existing I/O; on IBM SP5, we achieve a factor of 9 speedup. The estimated time for a typical century-long simulation of FV D-resolution on IBM SP5 shows that the I/O time can be reduced from more than 8 days (wall clock) to less than 1 day for daily output. This parallel I/O is also implemented on IBM BlueGene/L and the results are shown, whereas the existing sequential I/O fails due to memory usage limitation.", "paper_title": "Efficient parallel I/O in community atmosphere model (CAM)", "paper_id": "WOS:000257940100006"}