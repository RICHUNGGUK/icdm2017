{"auto_keywords": [{"score": 0.031361087211432506, "phrase": "merit_function_values"}, {"score": 0.00481495049065317, "phrase": "robust_descent"}, {"score": 0.004690305164360866, "phrase": "differentiable_optimization"}, {"score": 0.004568871736067739, "phrase": "automatic_finite_differences"}, {"score": 0.0041136473765874815, "phrase": "merit_function"}, {"score": 0.003852371777780562, "phrase": "global_convergence"}, {"score": 0.003752550907120244, "phrase": "generated_iterates"}, {"score": 0.003334309053314909, "phrase": "second_order"}, {"score": 0.0032908057622663732, "phrase": "necessary_optimality_conditions"}, {"score": 0.0025638362414466278, "phrase": "square_root"}, {"score": 0.002464709648714996, "phrase": "machine_precision"}, {"score": 0.0023079225317714815, "phrase": "clever_way"}, {"score": 0.0021049977753042253, "phrase": "full_machine_precision"}], "paper_keywords": ["unconstrained optimization", " numerical methods", " automatic differentiation", " high-precision computing", " line search procedures", " trust region methods"], "paper_abstract": "Descent algorithms use comparisons of a merit function, to ensure the global convergence of generated iterates to at least a stationary point, even to solutions satisfying the second order necessary optimality conditions. Unfortunately, the comparison of the merit function values may fail to be precise enough for estimates reaching the square root of the machine precision. We present a clever way to compare merit function values, ensuring that full machine precision may be reached.", "paper_title": "Robust descent in differentiable optimization using automatic finite differences", "paper_id": "WOS:000238565300007"}