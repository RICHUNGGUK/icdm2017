{"auto_keywords": [{"score": 0.04261556039727903, "phrase": "computational_cost"}, {"score": 0.00481495049065317, "phrase": "hierarchical_clustering"}, {"score": 0.0047395356621991935, "phrase": "great_importance"}, {"score": 0.00468991337957911, "phrase": "data_analytics"}, {"score": 0.004568108391981922, "phrase": "exponential_growth"}, {"score": 0.004520272777451894, "phrase": "real-world_data"}, {"score": 0.004133292222875435, "phrase": "huge_data_collections"}, {"score": 0.0036810726176482278, "phrase": "agglomerative_hierarchical_clustering"}, {"score": 0.0035853745478488254, "phrase": "cluster_hierarchies"}, {"score": 0.003529149828147388, "phrase": "raw_data_points"}, {"score": 0.0031929098985796814, "phrase": "adjacent_points"}, {"score": 0.0031428202743657057, "phrase": "data_space"}, {"score": 0.0030449789178940787, "phrase": "feature_extraction"}, {"score": 0.0030130446986008277, "phrase": "dimensionality_reduction"}, {"score": 0.002813457986293993, "phrase": "comprehensive_experimental_study"}, {"score": 0.002711506442590442, "phrase": "different_clustering_methods"}, {"score": 0.0026549137719259984, "phrase": "upgma"}, {"score": 0.0026270572353632297, "phrase": "slink"}, {"score": 0.0023646607651138987, "phrase": "euclidean"}, {"score": 0.0023392515066728583, "phrase": "canberra"}, {"score": 0.0022904032758108775, "phrase": "experimental_results"}, {"score": 0.002219040041886641, "phrase": "centroid_based_approach"}, {"score": 0.0021049977753042253, "phrase": "clustering_performance"}], "paper_keywords": ["Clustering analysis", " Hybrid clustering", " Data mining", " Data distribution", " Coefficient of correlation"], "paper_abstract": "Hierarchical clustering is of great importance in data analytics especially because of the exponential growth of real-world data. Often these data are unlabelled and there is little prior domain knowledge available. One challenge in handling these huge data collections is the computational cost. In this paper, we aim to improve the efficiency by introducing a set of methods of agglomerative hierarchical clustering. Instead of building cluster hierarchies based on raw data points, our approach builds a hierarchy based on a group of centroids. These centroids represent a group of adjacent points in the data space. By this approach, feature extraction or dimensionality reduction is not required. To evaluate our approach, we have conducted a comprehensive experimental study. We tested the approach with different clustering methods (i.e., UPGMA and SLINK), data distributions, (i.e., normal and uniform), and distance measures (i.e., Euclidean and Canberra). The experimental results indicate that, using the centroid based approach, computational cost can be significantly reduced without compromising the clustering performance. The performance of this approach is relatively consistent regardless the variation of the settings, i.e., clustering methods, data distributions, and distance measures. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Efficient agglomerative hierarchical clustering", "paper_id": "WOS:000348619900044"}