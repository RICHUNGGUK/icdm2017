{"auto_keywords": [{"score": 0.0437107789799431, "phrase": "consistent_subset"}, {"score": 0.004815315858763141, "phrase": "distributed"}, {"score": 0.0040186038781533946, "phrase": "nearest_neighbor_classification_rule"}, {"score": 0.0036710872046569532, "phrase": "distributed_environments"}, {"score": 0.003577414339646767, "phrase": "memory_requirements"}, {"score": 0.003531474887552889, "phrase": "different_variants"}, {"score": 0.0034636656250083744, "phrase": "basic_pfcnn_method"}, {"score": 0.003310447652300423, "phrase": "spatial_cost"}, {"score": 0.003267925215811654, "phrase": "cpu_cost"}, {"score": 0.0032051601987130207, "phrase": "communication_overhead"}, {"score": 0.0030832123624449028, "phrase": "experimental_results"}, {"score": 0.0027982016314450717, "phrase": "enormous_collections"}, {"score": 0.002555950807368337, "phrase": "memory_consumption"}, {"score": 0.0024906610581090223, "phrase": "theoretical_analysis"}, {"score": 0.002427035025924384, "phrase": "noticeable_data_reduction"}, {"score": 0.0023958326775913165, "phrase": "good_classification_accuracy"}, {"score": 0.0022312377180023282, "phrase": "first_distributed_algorithm"}, {"score": 0.0021049977753042253, "phrase": "nearest_neighbor_rule"}], "paper_keywords": ["classification", " parallel and distributed algorithms", " nearest neighbor rule", " data condensation"], "paper_abstract": "In this work, the Parallel Fast Condensed Nearest Neighbor (PFCNN) rule, a distributed method for computing a consistent subset of a very large data set for the nearest neighbor classification rule is presented. In order to cope with the communication overhead typical of distributed environments and to reduce memory requirements, different variants of the basic PFCNN method are introduced. An analysis of spatial cost, CPU cost, and communication overhead is accomplished for all the algorithms. Experimental results, performed on both synthetic and real very large data sets, revealed that these methods can be profitably applied to enormous collections of data. Indeed, they scale up well and are efficient in memory consumption, confirming the theoretical analysis, and achieve noticeable data reduction and good classification accuracy. To the best of our knowledge, this is the first distributed algorithm for computing a training set consistent subset for the nearest neighbor rule.", "paper_title": "Distributed nearest neighbor-based condensation of very large data sets", "paper_id": "WOS:000250216200001"}