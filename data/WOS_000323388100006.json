{"auto_keywords": [{"score": 0.04218262960910763, "phrase": "exploratory_search"}, {"score": 0.030119639676744, "phrase": "written_summaries"}, {"score": 0.00481495049065317, "phrase": "participant-generated_summaries"}, {"score": 0.004525629293824217, "phrase": "search_task"}, {"score": 0.004297795905522757, "phrase": "search_process"}, {"score": 0.004180992206290323, "phrase": "exploratory_behaviors"}, {"score": 0.0039431758381711125, "phrase": "effective_measures"}, {"score": 0.0038758293658651237, "phrase": "better_support"}, {"score": 0.0036553070575015344, "phrase": "fixed_source"}, {"score": 0.003483120583663528, "phrase": "open-ended_learning"}, {"score": 0.0034118234596993836, "phrase": "handwritten_summaries"}, {"score": 0.003151736708029235, "phrase": "topic_coverage"}, {"score": 0.0030031998605136788, "phrase": "simple_variables"}, {"score": 0.0029722900280316216, "phrase": "summary_length"}, {"score": 0.0029114187110123105, "phrase": "new_technique"}, {"score": 0.0028127163745564777, "phrase": "bloom's_taxonomy"}, {"score": 0.0026986685585382347, "phrase": "grounded_theory"}, {"score": 0.0024842242921857705, "phrase": "large_collection"}, {"score": 0.0024333239726872604, "phrase": "task-based_study"}, {"score": 0.002318566141177149, "phrase": "fact-to-statement_ratio"}, {"score": 0.0021564750534280863, "phrase": "clear_areas"}, {"score": 0.0021416404045850224, "phrase": "future_work"}, {"score": 0.0021049977753042253, "phrase": "continued_research"}], "paper_keywords": ["summarization", " learning", " content analysis"], "paper_abstract": "While it is easy to identify whether someone has found a piece of information during a search task, it is much harder to measure how much someone has learned during the search process. Searchers who are learning often exhibit exploratory behaviors, and so current research is often focused on improving support for exploratory search. Consequently, we need effective measures of learning to demonstrate better support for exploratory search. Some approaches, such as quizzes, measure recall when learning from a fixed source of information. This research, however, focuses on techniques for measuring open-ended learning, which often involve analyzing handwritten summaries produced by participants after a task. There are two common techniques for analyzing such summaries: (a) counting facts and statements and (b) judging topic coverage. Both of these techniques, however, can be easily confounded by simple variables such as summary length. This article presents a new technique that measures depth of learning within written summaries based on Bloom's taxonomy (B.S. Bloom & M.D. Engelhart, 1956). This technique was generated using grounded theory and is designed to be less susceptible to such confounding variables. Together, these three categories of measure were compared by applying them to a large collection of written summaries produced in a task-based study, and our results provide insights into each of their strengths and weaknesses. Both fact-to-statement ratio and our own measure of depth of learning were effective while being less affected by confounding variables. Recommendations and clear areas of future work are provided to help continued research into supporting sensemaking and learning.", "paper_title": "A comparison of techniques for measuring sensemaking and learning within participant-generated summaries", "paper_id": "WOS:000323388100006"}