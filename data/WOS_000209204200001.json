{"auto_keywords": [{"score": 0.04320256940116906, "phrase": "ge"}, {"score": 0.0397243768176007, "phrase": "knn_algorithm"}, {"score": 0.03680246559828074, "phrase": "non-asymptotic_behavior"}, {"score": 0.00481495049065317, "phrase": "nearest_neighbor_classifier"}, {"score": 0.004758188582347737, "phrase": "k-nearest_neighbor_classification_algorithm"}, {"score": 0.00450198136261303, "phrase": "major_applications"}, {"score": 0.004466519142879166, "phrase": "text_categorization"}, {"score": 0.00443133501650381, "phrase": "outlier_detection"}, {"score": 0.004396426822203258, "phrase": "handwritten_character_recognition"}, {"score": 0.004361792415184563, "phrase": "fraud_detection"}, {"score": 0.00425951071471662, "phrase": "sound_theoretical_results"}, {"score": 0.004159617445102161, "phrase": "generalization_error"}, {"score": 0.0040300463421312225, "phrase": "bayes_error"}, {"score": 0.003753022329975749, "phrase": "real_world_scenarios"}, {"score": 0.0036217089991592275, "phrase": "categorical_attributes"}, {"score": 0.0035507457285983268, "phrase": "principled_way"}, {"score": 0.003359334465113303, "phrase": "exact_closed_form_expressions"}, {"score": 0.00303072749758085, "phrase": "joint_probability_distribution"}, {"score": 0.0029830992938695007, "phrase": "input-output_space"}, {"score": 0.0028110458014332187, "phrase": "statistical_behavior"}, {"score": 0.0027341763737751467, "phrase": "interest_viz"}, {"score": 0.002576442680766742, "phrase": "monte_carlo_approximations"}, {"score": 0.0024960649759529275, "phrase": "dhurandhar"}, {"score": 0.0024763629987097587, "phrase": "dobra"}, {"score": 0.002456816881853114, "phrase": "j_mach"}, {"score": 0.0023896108055582615, "phrase": "acm_trans"}, {"score": 0.0022876877275935757, "phrase": "superior_alternative"}, {"score": 0.002155656133617155, "phrase": "monte_carlo"}, {"score": 0.0021049977753042253, "phrase": "semi-analytical_methodology"}], "paper_keywords": ["kNN", " Moments"], "paper_abstract": "The k-nearest neighbor classification algorithm (kNN) is one of the most simple yet effective classification algorithms in use. It finds major applications in text categorization, outlier detection, handwritten character recognition, fraud detection and in other related areas. Though sound theoretical results exist regarding convergence of the generalization error (GE) of this algorithm to Bayes error, these results are asymptotic in nature. The understanding of the behavior of the kNN algorithm in real world scenarios is limited. In this paper, assuming categorical attributes, we provide a principled way of studying the non-asymptotic behavior of the kNN algorithm. In particular, we derive exact closed form expressions for the moments of the GE for this algorithm. The expressions are functions of the sample, and hence can be computed given any joint probability distribution defined over the input-output space. These expressions can be used as a tool that aids in unveiling the statistical behavior of the algorithm in settings of interest viz. an acceptable value of k for a given sample size and distribution. Moreover, Monte Carlo approximations of such closed form expressions have been shown in Dhurandhar and Dobra (J Mach Learn Res 9, 2008; ACM Trans Knowl Discov Data 3, 2009) to be a superior alternative in terms of speed and accuracy when compared with computing the moments directly using Monte Carlo. This work employs the semi-analytical methodology that was proposed recently to better understand the non-asymptotic behavior of learning algorithms.", "paper_title": "Probabilistic characterization of nearest neighbor classifier", "paper_id": "WOS:000209204200001"}