{"auto_keywords": [{"score": 0.0395729582202271, "phrase": "rre_algorithm"}, {"score": 0.02574377082412744, "phrase": "initial_ensemble"}, {"score": 0.00481495049065317, "phrase": "greedy_algorithms"}, {"score": 0.0047839735747931605, "phrase": "high_efficiency"}, {"score": 0.004707396263523012, "phrase": "suboptimal_solutions"}, {"score": 0.004662036750452837, "phrase": "ensemble_pruning_problem"}, {"score": 0.00455788259881503, "phrase": "large_extent"}, {"score": 0.004499409294175065, "phrase": "marked_defect"}, {"score": 0.00447045328907701, "phrase": "almost_all_the_currently_existing_ensemble"}, {"score": 0.004398872331144531, "phrase": "greedy_ones"}, {"score": 0.00415050117148714, "phrase": "ensemble_selection"}, {"score": 0.00409723267214392, "phrase": "considerable_waste"}, {"score": 0.0040708542958968605, "phrase": "useful_resources"}, {"score": 0.003694884775158077, "phrase": "defeated_candidate_networks"}, {"score": 0.003612259264584205, "phrase": "worst_single_model"}, {"score": 0.0033971539563683174, "phrase": "selected_components"}, {"score": 0.0033643769947857066, "phrase": "pruned_ensemble"}, {"score": 0.003246877287434945, "phrase": "wsm"}, {"score": 0.003163985877966426, "phrase": "test_samples"}, {"score": 0.003093195541269291, "phrase": "classical_re"}, {"score": 0.00306334226516593, "phrase": "near-optimal_solution"}, {"score": 0.003004494701922268, "phrase": "pruned_error"}, {"score": 0.002937262271695137, "phrase": "backfitting_step"}, {"score": 0.0028715299874549245, "phrase": "selection_step"}, {"score": 0.0026830048370777315, "phrase": "soft_voting_approach"}, {"score": 0.0025642311494861797, "phrase": "rre_algorithms"}, {"score": 0.002466608618694819, "phrase": "best_single_model"}, {"score": 0.002357391911165024, "phrase": "member_networks"}, {"score": 0.0022749744354271816, "phrase": "seven_benchmark_classification_tasks"}, {"score": 0.002260301305523054, "phrase": "different_initial_ensemble_setups"}, {"score": 0.0022168460543913787, "phrase": "empirical_investigation"}, {"score": 0.002181276018774315, "phrase": "rre"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural network ensemble", " Machine learning", " Pattern recognition", " Classifier", " Reduce-Error (RE) pruning", " Reverse Reduce-Error (RRE) pruning"], "paper_abstract": "Although greedy algorithms possess high efficiency, they often receive suboptimal solutions of the ensemble pruning problem, since their exploration areas are limited in large extent. And another marked defect of almost all the currently existing ensemble pruning algorithms, including greedy ones, consists in: they simply abandon all of the classifiers which fail in the competition of ensemble selection, causing a considerable waste of useful resources and information. Inspired by these observations, an interesting greedy Reverse Reduce-Error (RRE) pruning algorithm incorporated with the operation of subtraction is proposed in this work. The RRE algorithm makes the best of the defeated candidate networks in a way that, the Worst Single Model (WSM) is chosen, and then, its votes are subtracted from the votes made by those selected components within the pruned ensemble. The reason is because, for most cases, the WSM might make mistakes in its estimation for the test samples. And, different from the classical RE, the near-optimal solution is produced based on the pruned error of all the available sequential subensembles. Besides, the backfitting step of RE algorithm is replaced with the selection step of a WSM in RRE. Moreover, the problem of ties might be solved more naturally with RRE. Finally, soft voting approach is employed in the testing to RRE algorithm. The performances of RE and RRE algorithms, and two baseline methods, i.e., the method which selects the Best Single Model (BSM) in the initial ensemble, and the method which retains all member networks of the initial ensemble (ALL), are evaluated on seven benchmark classification tasks under different initial ensemble setups. The results of the empirical investigation show the superiority of RRE over the other three ensemble pruning algorithms. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A new reverse reduce-error ensemble pruning algorithm", "paper_id": "WOS:000348452500025"}