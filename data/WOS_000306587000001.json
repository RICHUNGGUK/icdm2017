{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "variational_bayesian_inference"}, {"score": 0.004278807087784626, "phrase": "mean-field_variational_bayesian_approximation"}, {"score": 0.0036553070575015344, "phrase": "modern_machine_learning_terminology"}, {"score": 0.003445591238183763, "phrase": "statistical_physics_concepts"}, {"score": 0.0028857130140890787, "phrase": "approximate_mean-field_distribution"}, {"score": 0.00266693888510356, "phrase": "target_joint"}, {"score": 0.0025137856138000014, "phrase": "kl-divergence_sense"}, {"score": 0.0022777800735119405, "phrase": "local_node_updates"}, {"score": 0.0021049977753042253, "phrase": "recent_variational_message_passing_framework"}], "paper_keywords": ["Variational Bayes", " Mean-field", " Tutorial"], "paper_abstract": "This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.", "paper_title": "A tutorial on variational Bayesian inference", "paper_id": "WOS:000306587000001"}