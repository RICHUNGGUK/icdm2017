{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "automatic_general-purpose_sanitization_of_textual_documents"}, {"score": 0.0046993598473345395, "phrase": "new_information_sharing_technologies"}, {"score": 0.004476399615305783, "phrase": "textual_documents"}, {"score": 0.004284755353695709, "phrase": "confidential_information"}, {"score": 0.0040222701316738295, "phrase": "sensitive_data"}, {"score": 0.0038876302202924644, "phrase": "precisely_the_goal"}, {"score": 0.0038499929374478125, "phrase": "document_sanitization"}, {"score": 0.0037030390595465673, "phrase": "sanitization_process"}, {"score": 0.003459191419478298, "phrase": "specific_types"}, {"score": 0.00342568771304873, "phrase": "sensitive_entities"}, {"score": 0.003392507399258166, "phrase": "concrete_domains"}, {"score": 0.003278878522165815, "phrase": "user_supervision"}, {"score": 0.0031845067058084583, "phrase": "sensitive_terms"}, {"score": 0.0029458777184930896, "phrase": "sanitized_document"}, {"score": 0.002861062731782219, "phrase": "general-purpose_sanitization_method"}, {"score": 0.0027922467043592597, "phrase": "information_theory"}, {"score": 0.002751751956416832, "phrase": "knowledge_bases"}, {"score": 0.002685558057339722, "phrase": "sensitive_textual_information"}, {"score": 0.002570385604065795, "phrase": "automatic_and_unsupervised_way"}, {"score": 0.0024842242921857705, "phrase": "heterogeneous_documents"}, {"score": 0.002366110899738696, "phrase": "massive_and_heterogeneous_information-sharing_needs"}, {"score": 0.002242647404307089, "phrase": "trained_classifiers"}, {"score": 0.0022101055924375725, "phrase": "detection_recall"}, {"score": 0.002135995282374723, "phrase": "document's_utility"}, {"score": 0.0021049977753042253, "phrase": "term-suppression_methods"}], "paper_keywords": ["Data publishing", " document sanitization", " information theory", " privacy"], "paper_abstract": "The advent of new information sharing technologies has led society to a scenario where thousands of textual documents are publicly published every day. The existence of confidential information in many of these documents motivates the use of measures to hide sensitive data before being published, which is precisely the goal of document sanitization. Even though methods to assist the sanitization process have been proposed, most of them are focused on the detection of specific types of sensitive entities for concrete domains, lacking generality and and requiring user supervision. Moreover, to hide sensitive terms, most approaches opt to remove them, a measure that hampers the utility of the sanitized document. This paper presents a general-purpose sanitization method that, based on information theory and exploiting knowledge bases, detects and hides sensitive textual information while preserving its meaning. Our proposal works in an automatic and unsupervised way and it can be applied to heterogeneous documents, which make it specially suitable for environments with massive and heterogeneous information-sharing needs. Evaluation results show that our method outperforms strategies based on trained classifiers regarding the detection recall, whereas it better retains the document's utility compared to term-suppression methods.", "paper_title": "Automatic General-Purpose Sanitization of Textual Documents", "paper_id": "WOS:000319724100003"}