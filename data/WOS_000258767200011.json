{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "human_emotional_state"}, {"score": 0.007891142631860939, "phrase": "proposed_system"}, {"score": 0.004764220976404241, "phrase": "audiovisual_signals"}, {"score": 0.004714023408114857, "phrase": "machine_recognition"}, {"score": 0.004590820632326585, "phrase": "important_component"}, {"score": 0.004542441734438654, "phrase": "efficient_human-computer_interaction"}, {"score": 0.0044237033788643715, "phrase": "existing_works"}, {"score": 0.004285289574869884, "phrase": "audio_signals"}, {"score": 0.003957833102043277, "phrase": "systematic_approach"}, {"score": 0.00385431710959668, "phrase": "front_audiovisual_signals"}, {"score": 0.0037935066882208235, "phrase": "audio_characteristics"}, {"score": 0.0037534983455974947, "phrase": "emotional_speech"}, {"score": 0.0036553070575015344, "phrase": "extracted_prosodic"}, {"score": 0.003616751756961252, "phrase": "mel-frequency"}, {"score": 0.003597625122515975, "phrase": "cepstral_coefficient"}, {"score": 0.003559684533187643, "phrase": "mfcc"}, {"score": 0.0034849681064606543, "phrase": "formant_frequency_features"}, {"score": 0.0034299649219748513, "phrase": "face_detection_scheme"}, {"score": 0.003375826910838081, "phrase": "hsv_color_model"}, {"score": 0.0031676593724896075, "phrase": "visual_information"}, {"score": 0.003101153870424188, "phrase": "gabor_wavelet_features"}, {"score": 0.003036040409062437, "phrase": "feature_selection"}, {"score": 0.0029722900280316216, "phrase": "stepwise_method"}, {"score": 0.0029253546782349875, "phrase": "mahalanobis_distance"}, {"score": 0.002879158342988017, "phrase": "selected_audiovisual_features"}, {"score": 0.002687235334499192, "phrase": "comparative_study"}, {"score": 0.002658863179011018, "phrase": "different_classification_algorithms"}, {"score": 0.0026307897899760383, "phrase": "specific_characteristics"}, {"score": 0.002603012037901889, "phrase": "individual_emotion"}, {"score": 0.002561892963059385, "phrase": "novel_multiclassifier_scheme"}, {"score": 0.002481588370380518, "phrase": "recognition_performance"}, {"score": 0.0023038418282549274, "phrase": "human_subjects_front_different_languages"}, {"score": 0.0022434880661725493, "phrase": "experimental_results"}, {"score": 0.0021387992835532367, "phrase": "multiclassifier_scheme"}, {"score": 0.0021049977753042253, "phrase": "best_overall_recognition_rate"}], "paper_keywords": ["audiovisual information", " emotion recognition", " multiclassifier"], "paper_abstract": "Machine recognition of human emotional state is an important component for efficient human-computer interaction. The majority of existing works address this problem by utilizing audio signals alone, or visual information only. In this paper, we explore a systematic approach for recognition of human emotional state front audiovisual signals. The audio characteristics of emotional speech are represented by the extracted prosodic, Mel-frequency Cepstral Coefficient (MFCC), and formant frequency features. A face detection scheme based on HSV color model is used to detect the face from the background. The visual information is represented by Gabor wavelet features. We perform feature selection by using a stepwise method based on Mahalanobis distance. The selected audiovisual features are used to classify the data into their corresponding emotions. Based on a comparative study of different classification algorithms and specific characteristics of individual emotion, a novel multiclassifier scheme is proposed to boost the recognition performance. The feasibility of the proposed system is tested over a database that incorporates human subjects front different languages and cultural backgrounds. Experimental results demonstrate the effectiveness of the proposed system. The multiclassifier scheme achieves the best overall recognition rate of 82.14%.", "paper_title": "Recognizing human emotional state from audiovisual signals", "paper_id": "WOS:000258767200011"}