{"auto_keywords": [{"score": 0.04782950330883694, "phrase": "svm"}, {"score": 0.040944186666327306, "phrase": "huge_data_compression"}, {"score": 0.033563576934562606, "phrase": "available_features"}, {"score": 0.00481495049065317, "phrase": "feature_import_vector_machine"}, {"score": 0.004690305164360866, "phrase": "flexible_feature_selection"}, {"score": 0.00462919213721188, "phrase": "support_vector_machine"}, {"score": 0.004316396683332752, "phrase": "generalization_capability"}, {"score": 0.004278807087784626, "phrase": "general_theme"}, {"score": 0.0041136473765874815, "phrase": "training_data"}, {"score": 0.00406001734835979, "phrase": "high_dimensional_space"}, {"score": 0.00398959232769596, "phrase": "available_dimensions"}, {"score": 0.003703611021999795, "phrase": "classifier_function"}, {"score": 0.0031360786664165093, "phrase": "equal_importance"}, {"score": 0.0030951536886513567, "phrase": "classification_context"}, {"score": 0.003068166587650604, "phrase": "possible_selection"}, {"score": 0.0030281251520610604, "phrase": "useful_fraction"}, {"score": 0.002823207272381655, "phrase": "algorithmic_approach"}, {"score": 0.0026091594819333654, "phrase": "traditional_sequential_observation_selection_strategy"}, {"score": 0.0025414699652861667, "phrase": "sequential_feature_selection"}, {"score": 0.0024113504993554043, "phrase": "zhu"}, {"score": 0.0023902653800359206, "phrase": "hastie"}, {"score": 0.0023384629381463054, "phrase": "import_vector_machine"}, {"score": 0.0022579036318895753, "phrase": "optimal_sub-dimensional_model"}, {"score": 0.002218668420806254, "phrase": "final_classifier"}, {"score": 0.0021993066642488237, "phrase": "sufficient_accuracy"}, {"score": 0.0021422271362195734, "phrase": "wiley_periodicals"}, {"score": 0.002123531043641047, "phrase": "inc._statistical_analysis"}, {"score": 0.0021049977753042253, "phrase": "data_mining"}], "paper_keywords": ["classification", " import vector machine", " radial basis function", " regularization", " reproducing kernel Hilbert space", " support vector machine"], "paper_abstract": "The support vector machine (SVM) and other reproducing kernel Hilbert space (RKHS) based classifier systems are drawing much attention recently owing to its robustness and generalization capability. General theme here is to construct classifiers based on the training data in a high dimensional space by using all available dimensions. The SVM achieves huge data compression by selecting only few observations that lie close to the boundary of the classifier function. However when the number of observations is not very large (small n) but the number of dimensions/features is large (large p), then it is not necessary that all available features are of equal importance in the classification context. Possible selection of a useful fraction of the available features may result in huge data compression. In this paper, we propose an algorithmic approach by means of which such an optimal set of features could be selected. In short, we reverse the traditional sequential observation selection strategy of SVM to that of sequential feature selection. To achieve this we have modified the solution proposed by Zhu and Hastie in the context of import vector machine (IVM), to select an optimal sub-dimensional model to build the final classifier with sufficient accuracy. (C) 2015 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 8: 49-63, 2015", "paper_title": "Feature Import Vector Machine: A General Classifier with Flexible Feature Selection", "paper_id": "WOS:000359795400004"}