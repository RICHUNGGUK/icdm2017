{"auto_keywords": [{"score": 0.049378038061471535, "phrase": "polynomial_kernels"}, {"score": 0.014542150195867836, "phrase": "dimension_reduction"}, {"score": 0.010641500172970966, "phrase": "data_distributions"}, {"score": 0.00923394875080441, "phrase": "polynomial_kernel_operator"}, {"score": 0.00481495049065317, "phrase": "eigen-analysis_of"}, {"score": 0.004791330993892003, "phrase": "nonlinear_pca"}, {"score": 0.00462919213721188, "phrase": "growing_interest"}, {"score": 0.004583878086896926, "phrase": "kernel_methods"}, {"score": 0.004342430078306461, "phrase": "kernel_fisher_discriminant_analysis"}, {"score": 0.004216112320418982, "phrase": "principal_component_analysis"}, {"score": 0.0041136473765874815, "phrase": "statistical_learning"}, {"score": 0.004073359218158693, "phrase": "data_mining_applications"}, {"score": 0.004013662619667083, "phrase": "empirical_success"}, {"score": 0.003954837422946628, "phrase": "kernel_method"}, {"score": 0.003858697684681798, "phrase": "nonlinear_feature_mapping"}, {"score": 0.0030614565460256897, "phrase": "nonlinear_embedding"}, {"score": 0.0030314419455512013, "phrase": "kernel_principal_component_analysis"}, {"score": 0.003001885485226858, "phrase": "pca"}, {"score": 0.0025137856138000014, "phrase": "nonlinear_data"}, {"score": 0.0024768883044869023, "phrase": "practical_guidelines"}, {"score": 0.002428530921948425, "phrase": "appropriate_degree"}, {"score": 0.0022777800735119405, "phrase": "centering_kernels"}, {"score": 0.0022443390559699974, "phrase": "spectral_property"}, {"score": 0.002146926767291475, "phrase": "wiley_periodicals"}, {"score": 0.002125859121671525, "phrase": "inc._statistical_analysis"}, {"score": 0.0021049977753042253, "phrase": "data_mining"}], "paper_keywords": ["Gaussian kernel", " kernel methods", " kernel PCA", " nonlinear embedding", " polynomial kernel"], "paper_abstract": "There has been growing interest in kernel methods for classification, clustering and dimension reduction. For example, kernel Fisher discriminant analysis, spectral clustering and kernel principal component analysis are widely used in statistical learning and data mining applications. The empirical success of the kernel method is generally attributed to nonlinear feature mapping induced by the kernel, which in turn determines a low dimensional data embedding. It is important to understand the effect of a kernel and its associated kernel parameter(s) on the embedding in relation to data distributions. In this paper, we examine the geometry of the nonlinear embedding for kernel principal component analysis (PCA) when polynomial kernels are used. We carry out eigen-analysis of the polynomial kernel operator associated with data distributions and investigate the effect of the degree of polynomial. The results provide both insights into the geometry of nonlinear data embedding and practical guidelines for choosing an appropriate degree for dimension reduction with polynomial kernels. We further comment on the effect of centering kernels on the spectral property of the polynomial kernel operator. (C) 2013 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 6: 529-544, 2013", "paper_title": "Eigen-Analysis of Nonlinear PCA with Polynomial Kernels", "paper_id": "WOS:000209525600008"}