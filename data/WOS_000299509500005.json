{"auto_keywords": [{"score": 0.040063879386947346, "phrase": "distributed_environment"}, {"score": 0.00973094168660802, "phrase": "computing_node"}, {"score": 0.00481495049065317, "phrase": "networked-distributed_unstructured_environment"}, {"score": 0.004725831427731115, "phrase": "path_planning_technique"}, {"score": 0.004647993721295559, "phrase": "unstructured_networked_distributed_environment"}, {"score": 0.004440458022366789, "phrase": "distributed_memory"}, {"score": 0.004277537795378613, "phrase": "distributed_network"}, {"score": 0.004129138311177554, "phrase": "single_separate_distributed_computing_node"}, {"score": 0.004052661132349559, "phrase": "autonomous_agent_motion_planning"}, {"score": 0.003977594755967813, "phrase": "potential_field_model"}, {"score": 0.0039446780026489905, "phrase": "reinforcement_learning"}, {"score": 0.0039120325834029355, "phrase": "boundary_detection_algorithms"}, {"score": 0.003895810929926511, "phrase": "potential_fields"}, {"score": 0.003863568356785813, "phrase": "fast_convergence"}, {"score": 0.003799878496075633, "phrase": "reenforcement_learning"}, {"score": 0.003721735062704032, "phrase": "consistent_convergence"}, {"score": 0.0036527758615121715, "phrase": "agent_decision"}, {"score": 0.00352597432462803, "phrase": "path_retracing"}, {"score": 0.0035040573145725877, "phrase": "challenging_problem"}, {"score": 0.003424850089214916, "phrase": "complete_knowledge"}, {"score": 0.003368367923339975, "phrase": "backtracking_technique"}, {"score": 0.003340475943517995, "phrase": "distributed_agent"}, {"score": 0.003285380677401785, "phrase": "step_count"}, {"score": 0.0031712922100918706, "phrase": "entire_global_path"}, {"score": 0.0030931452940698404, "phrase": "separate_node"}, {"score": 0.0030421168600695625, "phrase": "partial_path"}, {"score": 0.002799293099907125, "phrase": "initial_knowledge"}, {"score": 0.0027588314610845705, "phrase": "proposed_distributed_technique"}, {"score": 0.002696424060787447, "phrase": "shortest_global_path"}, {"score": 0.00264091270098001, "phrase": "different_node"}, {"score": 0.0024098944158211758, "phrase": "internode_communication"}, {"score": 0.0023849523850778807, "phrase": "experimental_results"}, {"score": 0.0023701109535893584, "phrase": "proposed_method"}, {"score": 0.0021492686054379755, "phrase": "optimal_shortest_path"}, {"score": 0.002113778537619233, "phrase": "single-agent_case"}, {"score": 0.0021049977753042253, "phrase": "noninformation_sharing_multiagent_case"}], "paper_keywords": ["Agent navigation", " Obstacle avoidance", " Message-passing interface", " Distributed artificial intelligence", " Reenforcement learning"], "paper_abstract": "This paper proposes a path planning technique for autonomous agent(s) located in an unstructured networked distributed environment, where each agent has limited and not complete knowledge of the environment. Each agent has only the knowledge available in the distributed memory of the computing node the agent is running on and the agents share some information learned over a distributed network. In particular, the environment is divided into several sectors with each sector located on a single separate distributed computing node. We consider hybrid reactive-cognitive agent(s) where we use autonomous agent motion planning that is based on the use of a potential field model accompanied by a reinforcement learning as well as boundary detection algorithms. Potential fields are used for fast convergence toward a path in a distributed environment while reenforcement learning is used to guarantee a variety of behavior and consistent convergence in a distributed environment. We show how the agent decision making process is enhanced by the combination of the two techniques in a distributed environment. Furthermore, path retracing is a challenging problem in a distributed environment, since the agent does not have complete knowledge of the environment. We propose a backtracking technique to keep the distributed agent informed all the time of its path information and step count including when migrating from one node to another. Note that no node has knowledge of the entire global path from a source to a goal when such a goal resides on a separate node. Each agent has only knowledge of a partial path (internal to a node) and related number of steps corresponding to the portion of the path that agent traversed when running on the node. In particular, we show how each of the agents(s), starting in one of the many sectors with no initial knowledge of the environment, using the proposed distributed technique, develops its intelligence based on its experience and seamlessly discovers the shortest global path to the target, which is located in a different node, while avoiding any obstacle(s) it encounters in its way, including when transitioning and migrating from one distributed computing node to another. The agent(s) use (s) multiple-token-ring message passing interface (MPI) to perform internode communication. Finally, the experimental results of the proposed method show that single and multiagents sharing the same goal and running on the same or different nodes successfully coordinate the sharing of their respective environment states/information to collaboratively perform their respective tasks. The results also show that distributed multiagent sharing information increases by an order of magnitude the speed of convergence to the optimal shortest path to the goal in comparison with the single-agent case or noninformation sharing multiagent case.", "paper_title": "A hybrid cognitive/reactive intelligent agent autonomous path planning technique in a networked-distributed unstructured environment for reinforcement learning", "paper_id": "WOS:000299509500005"}