{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "statistical_analysis"}, {"score": 0.004730338746443662, "phrase": "search_engines"}, {"score": 0.004630755876583033, "phrase": "index_documents"}, {"score": 0.004517210371632577, "phrase": "rank_documents"}, {"score": 0.0044535760439663235, "phrase": "information_retrieval"}, {"score": 0.004423997779701372, "phrase": "ir"}, {"score": 0.004163290958633733, "phrase": "benchmark_collections"}, {"score": 0.004003925104720979, "phrase": "best_system_configuration"}, {"score": 0.0035995085881892464, "phrase": "new_query"}, {"score": 0.003437154518139677, "phrase": "deep_analysis"}, {"score": 0.0033408932140621948, "phrase": "corresponding_performance"}, {"score": 0.0030679359570782595, "phrase": "query_types"}, {"score": 0.0029819840849398234, "phrase": "query_clustering"}, {"score": 0.0029087480993530796, "phrase": "average_precision"}, {"score": 0.00286770704394124, "phrase": "second_approach"}, {"score": 0.0027972697690729453, "phrase": "query_features"}, {"score": 0.002767613012044169, "phrase": "query_difficulty_predictors"}, {"score": 0.002496615320278004, "phrase": "major_components"}, {"score": 0.0024789326485851666, "phrase": "ir_process"}, {"score": 0.0023754296478100865, "phrase": "post-evaluation_approach"}, {"score": 0.0022843440226146005, "phrase": "easy_queries"}, {"score": 0.002268161377269513, "phrase": "trecquerytags_process"}, {"score": 0.0022441016229967025, "phrase": "hard_queries"}, {"score": 0.0021967433763351884, "phrase": "current_query_features"}, {"score": 0.0021049977753042253, "phrase": "influential_parameters"}], "paper_keywords": ["Information Retrieval", " query difficulty", " query clustering", " IR system parameters", " Random Forest"], "paper_abstract": "Search engines are based on models to index documents, match queries and documents and rank documents. Research in Information Retrieval (IR) aims at defining these models and their parameters in order to optimize the results. Using benchmark collections, it has been shown that there is not a best system configuration that works for any query, but rather that performance varies from one query to another. It would be interesting if a meta-system could decide which system configuration should process a new query by learning from the context of previousqueries. This paper reports a deep analysis considering more than 80,000 search engine configurations applied to 100 queries and the corresponding performance. The goal of the analysis is to identify which configuration responds best to a certain type of query. We considered two approaches to define query types: one is post-evaluation, based on query clustering according to the performance measured with Average Precision, while the second approach is pre-evaluation, using query features (including query difficulty predictors) to cluster queries. Globally, we identified two parameters that should be optimized: retrieving model and TrecQueryTags_process. One could expect such results as these two parameters are major components of IR process. However our work results in two main conclusions: 1/ based on post-evaluation approach, we found that retrieving model is the most influential parameter for easy queries while TrecQueryTags_process is for hard queries; 2/ for pre-evaluation, current query features do not allow to cluster queries to identify differences in the influential parameters.", "paper_title": "Statistical Analysis to Establish the Importance of Information Retrieval Parameters", "paper_id": "WOS:000376441500005"}