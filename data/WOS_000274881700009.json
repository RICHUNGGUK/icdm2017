{"auto_keywords": [{"score": 0.04954397446159371, "phrase": "rnn"}, {"score": 0.04124461891167507, "phrase": "long-term_dependencies"}, {"score": 0.03354629653887737, "phrase": "serial_recall_task"}, {"score": 0.00481495049065317, "phrase": "recurrent_neural_network"}, {"score": 0.004655285779819366, "phrase": "powerful_connectionist_model"}, {"score": 0.003378385493203297, "phrase": "training_rnns"}, {"score": 0.003266201217790818, "phrase": "significant_long-term_dependencies"}, {"score": 0.002951444412262927, "phrase": "large_number"}, {"score": 0.0027898428590137515, "phrase": "temporal-kernel_recurrent_neural_network"}, {"score": 0.002464709648714996, "phrase": "standard_rnn"}, {"score": 0.0023827927890399357, "phrase": "tkrnn"}, {"score": 0.0023560952790080943, "phrase": "short-term_memory"}, {"score": 0.0022396019251440724, "phrase": "input_string"}, {"score": 0.0022020622730208514, "phrase": "stable_state"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Recurrent Neural Networks", " Fixed points", " Long-term dependencies", " Backpropagation through time", " Supervised learning"], "paper_abstract": "A Recurrent Neural Network (RNN) is a powerful connectionist model that can be applied to many challenging sequential problems, including problems that naturally arise in language and speech. However, RNNs are extremely hard to train on problems that have long-term dependencies, where it is necessary to remember events for many timesteps before using them to make a prediction. In this paper we consider the problem of training RNNs to predict sequences that exhibit significant long-term dependencies, focusing on a serial recall task where the RNN needs to remember a sequence of characters for a large number of steps before reconstructing it. We introduce the Temporal-Kernel Recurrent Neural Network (TKRNN), which is a variant of the RNN that can cope with long-term dependencies much more easily than a standard RNN, and show that the TKRNN develops short-term memory that successfully solves the serial recall task by representing the input string with a stable state of its hidden units. (C) 2009 Elsevier Ltd. All rights reserved.", "paper_title": "Temporal-Kernel Recurrent Neural Networks", "paper_id": "WOS:000274881700009"}