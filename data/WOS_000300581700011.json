{"auto_keywords": [{"score": 0.04057701431158676, "phrase": "latent_space"}, {"score": 0.00481495049065317, "phrase": "shared_kernel_information_embedding"}, {"score": 0.004766090327062509, "phrase": "discriminative_inference"}, {"score": 0.004717723626157272, "phrase": "latent_variable_models"}, {"score": 0.0045989362308239525, "phrase": "gplvm"}, {"score": 0.004552255360140208, "phrase": "related_methods"}, {"score": 0.00437021209960886, "phrase": "small_or_moderately_sized_training_sets"}, {"score": 0.003966388754366831, "phrase": "explicit_mappings"}, {"score": 0.003636692152882991, "phrase": "well-defined_density"}, {"score": 0.003438058620195309, "phrase": "kernel_information_embedding"}, {"score": 0.0033173234105492895, "phrase": "coherent_joint_density"}, {"score": 0.0032172054825137866, "phrase": "learned_latent_space"}, {"score": 0.0030414140732789186, "phrase": "small_data_sets"}, {"score": 0.0027459678326877744, "phrase": "multiple_input_spaces"}, {"score": 0.002718045523483502, "phrase": "e._g."}, {"score": 0.002569459184476759, "phrase": "kie"}, {"score": 0.0025174594088331853, "phrase": "missing_data"}, {"score": 0.0023435920614377306, "phrase": "data_sets"}, {"score": 0.002272794656458419, "phrase": "coherent_global_model"}, {"score": 0.0021705796895687864, "phrase": "local_online_models"}, {"score": 0.0021049977753042253, "phrase": "human_pose_inference"}], "paper_keywords": ["Latent variable models", " kernel information embedding", " inference", " nonparametric", " mutual information"], "paper_abstract": "Latent variable models, such as the GPLVM and related methods, help mitigate overfitting when learning from small or moderately sized training sets. Nevertheless, existing methods suffer from several problems: 1) complexity, 2) the lack of explicit mappings to and from the latent space, 3) an inability to cope with multimodality, and 4) the lack of a well-defined density over the latent space. We propose an LVM called the Kernel Information Embedding (KIE) that defines a coherent joint density over the input and a learned latent space. Learning is quadratic, and it works well on small data sets. We also introduce a generalization, the shared KIE (sKIE), that allows us to model multiple input spaces (e. g., image features and poses) using a single, shared latent representation. KIE and sKIE permit missing data during inference and partially labeled data during learning. We show that with data sets too large to learn a coherent global model, one can use the sKIE to learn local online models. We use sKIE for human pose inference.", "paper_title": "Shared Kernel Information Embedding for Discriminative Inference", "paper_id": "WOS:000300581700011"}