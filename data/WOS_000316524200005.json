{"auto_keywords": [{"score": 0.047369575881600845, "phrase": "ntd"}, {"score": 0.008713764663609583, "phrase": "non-negative_tucker_decomposition"}, {"score": 0.004677221529739889, "phrase": "markov"}, {"score": 0.004412494693940597, "phrase": "unsupervised_training"}, {"score": 0.004361428012479218, "phrase": "discrete_density_hmms"}, {"score": 0.004236321075918639, "phrase": "sequential_patterns"}, {"score": 0.004090901264701185, "phrase": "sequential_data"}, {"score": 0.003904713104101532, "phrase": "discovered_patterns"}, {"score": 0.003859500298805362, "phrase": "unseen_data"}, {"score": 0.0038148090091532933, "phrase": "structure_constraints"}, {"score": 0.0035366216368204182, "phrase": "hmm._two_training_schemes"}, {"score": 0.003221861835185307, "phrase": "hmm"}, {"score": 0.003039445153076014, "phrase": "bw_output"}, {"score": 0.0030042210975126616, "phrase": "vice_versa"}, {"score": 0.0029009722548088306, "phrase": "unsupervised_spoken_pattern_discovery"}, {"score": 0.002850682396853918, "phrase": "tidigits_database"}, {"score": 0.00280126189041041, "phrase": "training_schemes"}, {"score": 0.0027049694095434905, "phrase": "bw_training"}, {"score": 0.002642615537454746, "phrase": "pattern_purity"}, {"score": 0.002566685388128598, "phrase": "segmentation_boundaries"}, {"score": 0.002507511169156787, "phrase": "speech_recognition"}, {"score": 0.0023792975669318615, "phrase": "alternative_training"}, {"score": 0.0023244400300253534, "phrase": "bw"}, {"score": 0.0022841153001549193, "phrase": "ntd_regularized_bw"}, {"score": 0.0021799789348982516, "phrase": "simulated_annealing"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Non-negative Tucker decomposition", " Hidden Markov models", " Unsupervised training", " Regularization", " Speech recognition", " Sequential pattern discovery", " Vocabulary acquisition"], "paper_abstract": "Non-negative Tucker decomposition (NTD) is applied to unsupervised training of discrete density HMMs for the discovery of sequential patterns in data, for segmenting sequential data into patterns and for recognition of the discovered patterns in unseen data. Structure constraints are imposed on the NTD such that it shares its parameters with the HMM. Two training schemes are proposed: one uses NTD as a regularizer for the Baum-Welch (BW) training of the HMM, the other alternates between initializing the NTD with the BW output and vice versa. On the task of unsupervised spoken pattern discovery from the TIDIGITS database, both training schemes are observed to improve over BW training in terms of pattern purity, accuracy of the segmentation boundaries and accuracy for speech recognition. Furthermore, we experimentally observe that the alternative training of NTD and BW outperforms the NTD regularized BW, BW training and BW training with simulated annealing. (c) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "paper_id": "WOS:000316524200005"}