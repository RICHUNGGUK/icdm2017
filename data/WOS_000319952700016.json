{"auto_keywords": [{"score": 0.044673980465410684, "phrase": "test_sample"}, {"score": 0.02096289081339778, "phrase": "training_samples"}, {"score": 0.009795585816296672, "phrase": "conventional_transformation_methods"}, {"score": 0.006327400881414668, "phrase": "proposed_method"}, {"score": 0.005755057771353823, "phrase": "new_space"}, {"score": 0.00481495049065317, "phrase": "\"sparse_representation"}, {"score": 0.004757849078050144, "phrase": "representation-based_transformation_method"}, {"score": 0.004755419170703849, "phrase": "minimum_error"}, {"score": 0.004687419167634463, "phrase": "transformation_methods"}, {"score": 0.004563250218021092, "phrase": "face_recognition"}, {"score": 0.004536104856119294, "phrase": "gait_recognition"}, {"score": 0.004509120242098446, "phrase": "palmprint_recognition"}, {"score": 0.0040740829470571425, "phrase": "transform_axes"}, {"score": 0.003989844368560806, "phrase": "transformation_method"}, {"score": 0.003966096429996537, "phrase": "linear_discriminant_analysis"}, {"score": 0.00382653658519243, "phrase": "corresponding_transformation"}, {"score": 0.0037250869218569244, "phrase": "-class_distance"}, {"score": 0.003488238679515793, "phrase": "between-class_distance"}, {"score": 0.003395727443433411, "phrase": "test_samples"}, {"score": 0.003305661550005516, "phrase": "principal_component_analysis"}, {"score": 0.003286032601729616, "phrase": "pca"}, {"score": 0.002915844417747579, "phrase": "training_phase"}, {"score": 0.002779708854857718, "phrase": "\"optimal\"_representation"}, {"score": 0.0026420037427645725, "phrase": "conventional_transformation_method"}, {"score": 0.0025795767081439277, "phrase": "representation-based_classification"}, {"score": 0.0024961348944169616, "phrase": "conventional_distance-based_classification"}, {"score": 0.0024081753091661396, "phrase": "\"closest\"_training_samples"}, {"score": 0.002302561649365666, "phrase": "weighted_distance"}, {"score": 0.0022214080683353476, "phrase": "representation_coefficient"}, {"score": 0.0022015695920570167, "phrase": "linear_combination"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Biometrics", " Face recognition", " Feature extraction", " Sparse representation"], "paper_abstract": "Transformation methods have been widely used in biometrics such as face recognition, gait recognition and palmprint recognition. It seems that conventional transformation methods seem to be \"optimal\" for training samples but not for every test sample to be classified. The reason is that conventional transformation methods use only the information of training samples to obtain transform axes. For example, if the transformation method is linear discriminant analysis (LDA), then in the new space obtained using the corresponding transformation, the training samples must have the maximum between-class distance and the minimum within-class distance. However, it is hard to guarantee that the transformation also maximizes the between-class distance and minimizes the within-class distance of the test samples in the new space. Another example is that principal component analysis (PCA) can best represent the training samples with the minimum error; however, it is not guaranteed that every test sample can be also represented with the minimum error. In this paper, we propose to improve conventional transformation methods by relating the training phase with the test sample. The proposed method simultaneously uses both the training samples and test sample to obtain an \"optimal\" representation of the test sample. In other words, the proposed method not only is an improvement to the conventional transformation method but also has the merits of the representation-based classification, which has shown very good performance in various problems. Differing from conventional distance-based classification, the proposed method evaluates only the distances between the test sample and the \"closest\" training samples and depends on only them to perform classification. Moreover, the proposed method uses the weighted distance to classify the test sample. The weight is set to the representation coefficient of a linear combination of the training samples that can well represent the test sample. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "From the idea of \"sparse representation\" to a representation-based transformation method for feature extraction", "paper_id": "WOS:000319952700016"}