{"auto_keywords": [{"score": 0.04834807320597276, "phrase": "artificial_agents"}, {"score": 0.00481495049065317, "phrase": "motivated_agents"}, {"score": 0.004775726266706856, "phrase": "reinforcement_learning"}, {"score": 0.0045284072672385975, "phrase": "behavioral_responses"}, {"score": 0.0041555469216463855, "phrase": "shortest_path"}, {"score": 0.003988901168289797, "phrase": "reward_functions"}, {"score": 0.0039402177950179345, "phrase": "fixed_value"}, {"score": 0.0036453427552753533, "phrase": "intermediate_step"}, {"score": 0.00357146811369898, "phrase": "fixed_strategy"}, {"score": 0.00342816445816304, "phrase": "dynamic_environments"}, {"score": 0.0031975786158658158, "phrase": "significant_evidence"}, {"score": 0.0031327496009959464, "phrase": "reward_value"}, {"score": 0.002770389097966904, "phrase": "simplified_formalization"}, {"score": 0.0025211567863488962, "phrase": "motivated_actor-critic"}, {"score": 0.00241989408309933, "phrase": "physiological_stability"}, {"score": 0.002370794661629904, "phrase": "different_resource_distribution"}, {"score": 0.002202123699556382, "phrase": "agent's_motivational_state"}, {"score": 0.0021398115287338693, "phrase": "behavioral_cycles"}, {"score": 0.0021049977753042253, "phrase": "agent's_physiological_stability"}], "paper_keywords": ["Hedonic value", " motivation", " reinforcement learning", " actor-critic", " grounding"], "paper_abstract": "Reinforcement learning (RL) in the context of artificial agents is typically used to produce behavioral responses as a function of the reward obtained by interaction with the environment. When the problem consists of learning the shortest path to a goal, it is common to use reward functions yielding a fixed value after each decision, for example a positive value if the target location has been attained and a negative value at each intermediate step. However, this fixed strategy may be overly simplistic for agents to adapt to dynamic environments, in which resources may vary from time to time. By contrast, there is significant evidence that most living beings internally modulate reward value as a function of their context to expand their range of adaptivity. Inspired by the potential of this operation, we present a review of its underlying processes and we introduce a simplified formalization for artificial agents. The performance of this formalism is tested by monitoring the adaptation of an agent endowed with a model of motivated actor-critic, embedded with our formalization of value and constrained by physiological stability, to environments with different resource distribution. Our main result shows that the manner in which reward is internally processed as a function of the agent's motivational state, strongly influences adaptivity of the behavioral cycles generated and the agent's physiological stability.", "paper_title": "Hedonic value: enhancing adaptation for motivated agents", "paper_id": "WOS:000329666400004"}