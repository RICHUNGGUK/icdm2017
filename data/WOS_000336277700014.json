{"auto_keywords": [{"score": 0.048992899974696226, "phrase": "self-adaptive_differential_evolution"}, {"score": 0.04758008554756529, "phrase": "naive_bayes"}, {"score": 0.046206033143893974, "phrase": "training_samples"}, {"score": 0.04309248770037891, "phrase": "zero-frequency_problem"}, {"score": 0.030424249019695215, "phrase": "estimation_model"}, {"score": 0.004815218631132883, "phrase": "bayes"}, {"score": 0.004211504818267892, "phrase": "probability_estimation_method"}, {"score": 0.004007083686875716, "phrase": "laplace-estimate"}, {"score": 0.003531474887552889, "phrase": "direct_impact"}, {"score": 0.0034911335714333507, "phrase": "underlying_experimental_results"}, {"score": 0.003372844338742438, "phrase": "existing_probability_estimation_methods"}, {"score": 0.003308862957060602, "phrase": "parameter_cross-test"}, {"score": 0.0031967297437935772, "phrase": "different_settings"}, {"score": 0.00307657476125683, "phrase": "experimental_result"}, {"score": 0.0030297830963114483, "phrase": "optimal_parameter_values"}, {"score": 0.002983700956478823, "phrase": "different_data_sets"}, {"score": 0.0029270797740550973, "phrase": "analysis_results"}, {"score": 0.002731971103291716, "phrase": "optimal_m_and_p_value"}, {"score": 0.0025596276616952516, "phrase": "classification_accuracy"}, {"score": 0.0025110337774260773, "phrase": "repository_data_sets"}, {"score": 0.0022815262950738814, "phrase": "possible_optimal_settings"}, {"score": 0.0022382002476510573, "phrase": "experimental_results"}, {"score": 0.0021375377706211686, "phrase": "traditional_probability_estimation"}, {"score": 0.0021049977753042253, "phrase": "large_data_sets"}], "paper_keywords": ["Naive Bayes", " Probability estimation", " M-estimate", " Laplace-estimate", " Differential evolution", " Self-adaptive", " Classification"], "paper_abstract": "In the process of learning the naive Bayes, estimating probabilities from a given set of training samples is crucial. However, when the training samples are not adequate, probability estimation method will inevitably suffer from the zero-frequency problem. To avoid this problem, Laplace-estimate and M-estimate are the two main methods used to estimate probabilities. The estimation of two important parameters m (integer variable) and p (probability variable) in these methods has a direct impact on the underlying experimental results. In this paper, we study the existing probability estimation methods and carry out a parameter Cross-test by experimentally analyzing the performance of M-estimate with different settings for the two parameters m and p. This part of experimental result shows that the optimal parameter values vary corresponding to different data sets. Motivated by these analysis results, we propose an estimation model based on self-adaptive differential evolution. Then we propose an approach to calculate the optimal m and p value for each conditional probability to avoid the zero-frequency problem. We experimentally test our approach in terms of classification accuracy using the 36 benchmark machine learning repository data sets, and compare it to a naive Bayes with Laplace-estimate and M-estimate with a variety of setting of parameters from literature and those possible optimal settings via our experimental analysis. The experimental results show that the estimation model is efficient and our proposed approach significantly outperforms the traditional probability estimation approaches especially for large data sets (large number of instances and attributes).", "paper_title": "A naive Bayes probability estimation model based on self-adaptive differential evolution", "paper_id": "WOS:000336277700014"}