{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sparse_ms-svr"}, {"score": 0.004692227830462433, "phrase": "expectation-maximization_algorithm"}, {"score": 0.004494570360108983, "phrase": "multi-scale_support_vector_regression"}, {"score": 0.004417848254181138, "phrase": "ms-svr"}, {"score": 0.004268293864953256, "phrase": "quadratic_loss_function"}, {"score": 0.0040186038781533946, "phrase": "time-consuming_quadratic_programming"}, {"score": 0.0027741761880779535, "phrase": "qp_algorithm"}, {"score": 0.002726740692380339, "phrase": "large_data_sets"}, {"score": 0.0025892330566358503, "phrase": "far_sparser_solution"}, {"score": 0.002437525632451029, "phrase": "good_performance"}, {"score": 0.0022749744354271816, "phrase": "multi-scale_kernels"}, {"score": 0.0022168460543913787, "phrase": "regularization_terms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["multi-scale support vector regression (MS-SVR)", " hierarchical-Bayes model", " maximum a posteriori (MAP) estimation", " expectation-maximization (EM) algorithm"], "paper_abstract": "The solution of multi-scale support vector regression (MS-SVR) with the quadratic loss function can be obtained by solving a time-consuming quadratic programming (QP) problem and a post-processing. This paper adapts an expectation-maximization (EM) algorithm based on two 2-level hierarchical-Bayes models, which implement the l(1)-norm and the l(0)-norm regularization term asymptotically, to fast train MS-SVR. Experimental results illuminate that the EM algorithm is faster than the QP algorithm for large data sets, the l(0)-norm regularization term promotes a far sparser solution than the l(1)-norm, and the good performance of MS-SVR should be attributed to the multi-scale kernels and the regularization terms. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Training sparse MS-SVR with an expectation-maximization algorithm", "paper_id": "WOS:000239015000026"}