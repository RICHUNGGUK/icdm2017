{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "human_behavioral_coding_system"}, {"score": 0.004777744121315302, "phrase": "married_couples'_interactions"}, {"score": 0.004740823885631586, "phrase": "speech_acoustic_features"}, {"score": 0.004704187604192605, "phrase": "observational_methods"}, {"score": 0.004578165674080246, "phrase": "human_behavior"}, {"score": 0.004525190364108967, "phrase": "behavioral_sciences"}, {"score": 0.004336115760817672, "phrase": "intimate_relationships"}, {"score": 0.004302593474473502, "phrase": "psychologists'_hypotheses"}, {"score": 0.0042035670399069485, "phrase": "video_recording_interactions"}, {"score": 0.004106810349956121, "phrase": "relevant_behaviors"}, {"score": 0.004075053556360943, "phrase": "standardized_coding_systems"}, {"score": 0.004027876348834879, "phrase": "coding_process"}, {"score": 0.003889583851551197, "phrase": "resulting_coded_data"}, {"score": 0.0038296485293910026, "phrase": "high_degree"}, {"score": 0.0034753508746887957, "phrase": "engineering_methods"}, {"score": 0.0033952989364738353, "phrase": "human_behavioral_data"}, {"score": 0.0032786537599187125, "phrase": "large_corpus"}, {"score": 0.003253279945540651, "phrase": "married_couples'_problem-solving_interactions"}, {"score": 0.0031537270875516108, "phrase": "multiple_session-level_behavioral_observations"}, {"score": 0.0029867619862229853, "phrase": "acoustic_speech_features"}, {"score": 0.002940697344380201, "phrase": "extreme_instances"}, {"score": 0.002917931350071647, "phrase": "six_selected_codes"}, {"score": 0.002828611294209297, "phrase": "\"high\"_blame"}, {"score": 0.0026477564788515984, "phrase": "global_acoustic_properties"}, {"score": 0.0024880904032564583, "phrase": "best_overall_automatic_system"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Behavioral signal processing (BSP)", " Couple therapy", " Dyadic interaction", " Human behavior analysis", " Prosody", " Emotion recognition"], "paper_abstract": "Observational methods are fundamental to the study of human behavior in the behavioral sciences. For example, in the context of research on intimate relationships, psychologists' hypotheses are often empirically tested by video recording interactions of couples and manually coding relevant behaviors using standardized coding systems. This coding process can be time-consuming, and the resulting coded data may have a high degree of variability because of a number of factors (e.g., inter-evaluator differences). These challenges provide an opportunity to employ engineering methods to aid in automatically coding human behavioral data. In this work, we analyzed a large corpus of married couples' problem-solving interactions. Each spouse was manually coded with multiple session-level behavioral observations (e.g., level of blame toward other spouse), and we used acoustic speech features to automatically classify extreme instances for six selected codes (e.g., \"low\" vs. \"high\" blame). Specifically, we extracted prosodic, spectral, and voice quality features to capture global acoustic properties for each spouse and trained gender-specific and gender-independent classifiers. The best overall automatic system correctly classified 74.1% of the instances, an improvement of 3.95% absolute (5.63% relative) over our previously reported best results. We compare performance for the various factors: across codes, gender, classifier type, and feature type. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Toward automating a human behavioral coding system for married couples' interactions using speech acoustic features", "paper_id": "WOS:000312422900001"}