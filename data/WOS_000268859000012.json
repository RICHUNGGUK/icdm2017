{"auto_keywords": [{"score": 0.0344648815496662, "phrase": "noisy_parities"}, {"score": 0.008716837604460835, "phrase": "unknown_function"}, {"score": 0.004786849427167849, "phrase": "monomials"}, {"score": 0.004620773128779254, "phrase": "agnostic_learning_framework"}, {"score": 0.00452673046862969, "phrase": "comput"}, {"score": 0.004344026959123821, "phrase": "m._kearns"}, {"score": 0.0043185468826678185, "phrase": "r._schapire"}, {"score": 0.004280605517385785, "phrase": "l._sellie"}, {"score": 0.004255495852160294, "phrase": "machine_learning"}, {"score": 0.004024150214304478, "phrase": "uniform_distribution"}, {"score": 0.003988784961917646, "phrase": "learning_parities"}, {"score": 0.00391898046407615, "phrase": "random_classification_noise"}, {"score": 0.0038390783879368585, "phrase": "noisy_parity_problem"}, {"score": 0.0036732826889280487, "phrase": "h._wasserman"}, {"score": 0.003651722462180419, "phrase": "j._acm"}, {"score": 0.00347347805815395, "phrase": "first_nontrivial_algorithm"}, {"score": 0.003453086514392362, "phrase": "agnostic_learning"}, {"score": 0.00339262490894945, "phrase": "similar_techniques"}, {"score": 0.0031798212866637485, "phrase": "disjunctive_normal_form"}, {"score": 0.0029715605634810793, "phrase": "k_variables"}, {"score": 0.0029367553751816237, "phrase": "essentially_optimal_hardness_results"}, {"score": 0.002744370297766629, "phrase": "constant_epsilon"}, {"score": 0.0026031494994699414, "phrase": "np"}, {"score": 0.0024106618562440563, "phrase": "open_question"}, {"score": 0.0023894483778281834, "phrase": "blum"}, {"score": 0.0023337634986506234, "phrase": "previous_hardness_results"}, {"score": 0.0021049977753042253, "phrase": "stronger_complexity_assumptions"}], "paper_keywords": ["agnostic learning", " random noise", " parities", " monomials", " halfspaces"], "paper_abstract": "We study the learnability of several fundamental concept classes in the agnostic learning framework of [D. Haussler, Inform. and Comput., 100 (1992), pp. 78-150] and [M. Kearns, R. Schapire, and L. Sellie, Machine Learning, 17 (1994), pp. 115-141]. We show that under the uniform distribution, agnostically learning parities reduce to learning parities with random classification noise, commonly referred to as the noisy parity problem. Together with the parity learning algorithm of [A. Blum, A. Kalai, and H. Wasserman, J. ACM, 50 (2003), pp. 506-519], this gives the first nontrivial algorithm for agnostic learning of parities. We use similar techniques to reduce learning of two other fundamental concept classes under the uniform distribution to learning of noisy parities. Namely, we show that learning of disjunctive normal form (DNF) expressions reduces to learning noisy parities of just logarithmic number of variables, and learning of k-juntas reduces to learning noisy parities of k variables. We give essentially optimal hardness results for agnostic learning of monomials over {0, 1}(n) and halfspaces over Q(n). We show that for any constant epsilon finding a monomial (halfspace) that agrees with an unknown function on 1/2 + epsilon fraction of the examples is NP-hard even when there exists a monomial (halfspace) that agrees with the unknown function on 1 - epsilon fraction of the examples. This resolves an open question due to Blum and significantly improves on a number of previous hardness results for these problems. We extend these results to epsilon = 2(-log1-lambda n) (epsilon = 2(-root log n) in the case of halfspaces) for any constant lambda > 0 under stronger complexity assumptions.", "paper_title": "ON AGNOSTIC LEARNING OF PARITIES, MONOMIALS, AND HALFSPACES", "paper_id": "WOS:000268859000012"}