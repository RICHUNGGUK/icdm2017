{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "location-invariant_orthographic_processing"}, {"score": 0.004653120760786245, "phrase": "backpropagation_neural_networks"}, {"score": 0.0043953478557600565, "phrase": "different_positions"}, {"score": 0.0043208252177835815, "phrase": "horizontal_letter_array"}, {"score": 0.004247560712327667, "phrase": "first_topology"}, {"score": 0.004058180509184539, "phrase": "hidden_layer"}, {"score": 0.0038114397487574838, "phrase": "single_hidden_layer"}, {"score": 0.0037042817107563785, "phrase": "last_topology"}, {"score": 0.0033427754823819157, "phrase": "explicit_word-centred_letters"}, {"score": 0.0031215289677725693, "phrase": "skilled_human_reading"}, {"score": 0.0028653893877383188, "phrase": "two-deck_topology"}, {"score": 0.0026302120935562568, "phrase": "fewest_connection_weights"}, {"score": 0.0025561788724870974, "phrase": "internal_representations"}, {"score": 0.0024984517279573906, "phrase": "zero-deck_networks"}, {"score": 0.0024560114164181765, "phrase": "letter-based_scheme"}, {"score": 0.002414290278501595, "phrase": "position_bias"}, {"score": 0.0023463203637929466, "phrase": "one-deck_networks"}, {"score": 0.0023064582783867645, "phrase": "holographic_overlap_coding"}, {"score": 0.002153653728048424, "phrase": "linear_combinations"}, {"score": 0.0021049977753042253, "phrase": "two-deck_networks"}], "paper_keywords": ["reading", " orthographic processing", " supervised learning", " artificial neural networks", " network coding analysis"], "paper_abstract": "We trained three topologies of backpropagation neural networks to discriminate 2000 words (lexical representations) presented at different positions of a horizontal letter array. The first topology (zero-deck) contains no hidden layer, the second (one-deck) has a single hidden layer, and for the last topology (two-deck), the task is divided in two subtasks implemented as two stacked neural networks, with explicit word-centred letters as intermediate representations. All topologies successfully simulated two key benchmark phenomena observed in skilled human reading: transposed-letter priming and relative-position priming. However, the two-deck topology most accurately simulated the ability to discriminate words from nonwords, while containing the fewest connection weights. We analysed the internal representations after training. Zero-deck networks implement a letter-based scheme with a position bias to differentiate anagrams. One-deck networks implement a holographic overlap coding in which representations are essentially letter-based and words are linear combinations of letters. Two-deck networks also implement holographic-coding.", "paper_title": "Computational models of location-invariant orthographic processing", "paper_id": "WOS:000323302500001"}