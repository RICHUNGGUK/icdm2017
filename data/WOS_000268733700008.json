{"auto_keywords": [{"score": 0.05007846572397809, "phrase": "incremental_learning"}, {"score": 0.04633740170112977, "phrase": "ncl"}, {"score": 0.02760440099357579, "phrase": "sncl"}, {"score": 0.009880690292891311, "phrase": "selection_process"}, {"score": 0.004612376827433774, "phrase": "successful_approach"}, {"score": 0.004546755482549647, "phrase": "neural_network_ensembles"}, {"score": 0.004482063543727601, "phrase": "batch_learning_mode"}, {"score": 0.004172094622123562, "phrase": "potentially_powerful_approach"}, {"score": 0.0037377733149169573, "phrase": "selective_ncl"}, {"score": 0.0035125292519354724, "phrase": "new_training_data"}, {"score": 0.003413183361838603, "phrase": "previously_trained_neural_network_ensemble"}, {"score": 0.0033166379485150507, "phrase": "cloned_ensemble"}, {"score": 0.0032382660477699695, "phrase": "new_data"}, {"score": 0.003131637033429892, "phrase": "new_ensemble"}, {"score": 0.00305762327908599, "phrase": "previous_ensemble"}, {"score": 0.002928765962664163, "phrase": "whole_ensemble"}, {"score": 0.002887028262617443, "phrase": "fixed_size"}, {"score": 0.0028053237950173508, "phrase": "extended_version"}, {"score": 0.002699960312990493, "phrase": "previous_work"}, {"score": 0.002623536040408411, "phrase": "deeper_investigation"}, {"score": 0.0025614999929672, "phrase": "different_objective_functions"}, {"score": 0.002197692190587106, "phrase": "artmap"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Negative correlation learning", " Neural network ensemble", " Incremental learning", " Selective ensemble"], "paper_abstract": "Negative correlation learning (NCL) is a successful approach to constructing neural network ensembles. In batch learning mode, NCL outperforms many other ensemble learning approaches. Recently, NCL has also shown to be a potentially powerful approach to incremental learning, while the advantages of NCL have not yet been fully exploited. In this paper, we propose a selective NCL (SNCL) algorithm for incremental learning. Concretely, every time a new training data set is presented, the previously trained neural network ensemble is cloned. Then the cloned ensemble is trained on the new data set. After that, the new ensemble is combined with the previous ensemble and a selection process is applied to prune the whole ensemble to a fixed size. This paper is an extended version of our preliminary paper on SNCL. Compared to the previous work, this paper presents a deeper investigation into SNCL, considering different objective functions for the selection process and comparing SNCL to other NCL-based incremental learning algorithms on two more real world bioinformatics data sets. Experimental results demonstrate the advantage of SNCL Further, comparisons between SNCL and other existing incremental learning algorithms, such Learn + + and ARTMAP are also presented. (C) 2009 Elsevier B.V. All rights reserved.", "paper_title": "Selective negative correlation learning approach to incremental learning", "paper_id": "WOS:000268733700008"}