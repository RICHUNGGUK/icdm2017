{"auto_keywords": [{"score": 0.024701264010756754, "phrase": "krylov"}, {"score": 0.00481495049065317, "phrase": "high_dimensional_gaussian_distributions"}, {"score": 0.004371660146981191, "phrase": "high_dimensional_gaussian_models"}, {"score": 0.003342280102112269, "phrase": "cholesky_factorisations"}, {"score": 0.0030017207229983385, "phrase": "massive_memory_requirements"}, {"score": 0.0028446460307503343, "phrase": "novel_approach"}, {"score": 0.0025546627686955656, "phrase": "matrix-vector_products"}, {"score": 0.0023694065215394593, "phrase": "matrix_functions"}, {"score": 0.0021049977753042253, "phrase": "iterative_numerical_method"}], "paper_keywords": ["Gaussian distribution", " Krylov methods", " Matrix functions", " Numerical linear algebra", " Estimation"], "paper_abstract": "In order to compute the log-likelihood for high dimensional Gaussian models, it is necessary to compute the determinant of the large, sparse, symmetric positive definite precision matrix. Traditional methods for evaluating the log-likelihood, which are typically based on Cholesky factorisations, are not feasible for very large models due to the massive memory requirements. We present a novel approach for evaluating such likelihoods that only requires the computation of matrix-vector products. In this approach we utilise matrix functions, Krylov subspaces, and probing vectors to construct an iterative numerical method for computing the log-likelihood.", "paper_title": "Parameter estimation in high dimensional Gaussian distributions", "paper_id": "WOS:000331643200009"}