{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "statistical_benchmarking"}, {"score": 0.004576253356889219, "phrase": "machine_learning"}, {"score": 0.004518442937844318, "phrase": "algorithm_performance_evaluation"}, {"score": 0.004349337717774093, "phrase": "machine_learning_community"}, {"score": 0.003639835714755229, "phrase": "serious_limitations"}, {"score": 0.003503497128638099, "phrase": "great_faith"}, {"score": 0.00337224813954299, "phrase": "ritualistic_manner"}, {"score": 0.0032666255024083983, "phrase": "fixed_set"}, {"score": 0.0021049977753042253, "phrase": "good_first_step"}], "paper_keywords": ["machine learning", " algorithm evaluation", " benchmarking", " null hypothesis tests"], "paper_abstract": "Algorithm performance evaluation is so entrenched in the machine learning community that one could call it an addiction. Like most addictions, it is harmful and very difficult to give up. It is harmful because it has serious limitations. Yet, we have great faith in practicing it in a ritualistic manner: we follow a fixed set of rules telling us the measure, the data sets and the statistical test to use. When we read a paper, even as reviewers, we are not sufficiently critical of results that follow these rules. Here, we will debate what are the limitations and how to best address them. This article may not cure the addiction but hopefully it will be a good first step along that road.", "paper_title": "Warning: statistical benchmarking is addictive. Kicking the habit in machine learning", "paper_id": "WOS:000274673700004"}