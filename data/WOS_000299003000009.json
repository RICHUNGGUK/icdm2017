{"auto_keywords": [{"score": 0.04635653728486136, "phrase": "unlabeled_instances"}, {"score": 0.015719716506582538, "phrase": "instance_weighted_naive_bayes"}, {"score": 0.010532534169865241, "phrase": "iwnb"}, {"score": 0.010275499361849851, "phrase": "naive_bayes"}, {"score": 0.0047555023078686386, "phrase": "labeled_and_unlabeled_data"}, {"score": 0.0046676969718111765, "phrase": "real-world_data_mining_applications"}, {"score": 0.004305481798183612, "phrase": "available_labeled_instances"}, {"score": 0.004122179528285764, "phrase": "semi-supervised_learning"}, {"score": 0.003946650265907885, "phrase": "large_amount"}, {"score": 0.0038978806337961565, "phrase": "unlabeled_data"}, {"score": 0.003825849505589868, "phrase": "labeled_data"}, {"score": 0.003077372515276563, "phrase": "labeled_instances"}, {"score": 0.0029830992938695007, "phrase": "trained_naive_bayes"}, {"score": 0.0028737647829311587, "phrase": "class_membership_probabilities"}, {"score": 0.0027341763737751467, "phrase": "estimated_class_membership_probabilities"}, {"score": 0.0022825131641434964, "phrase": "large_number"}, {"score": 0.0022542611024997474, "phrase": "uci_data_sets"}, {"score": 0.0021580985506699105, "phrase": "classification_accuracy"}, {"score": 0.002131383155056741, "phrase": "original_naive_bayes"}, {"score": 0.0021049977753042253, "phrase": "available_labeled_data"}], "paper_keywords": ["Semi-supervised learning", " Unlabeled data", " Naive Bayes", " Instance Weighted Naive Bayes (IWNB)", " Class membership probability", " Classification"], "paper_abstract": "In real-world data mining applications, it is often the case that unlabeled instances are abundant, while available labeled instances are very limited. Thus, semi-supervised learning, which attempts to benefit from large amount of unlabeled data together with labeled data, has attracted much attention from researchers. In this paper, we propose a very fast and yet highly effective semi-supervised learning algorithm. We call our proposed algorithm Instance Weighted Naive Bayes (simply IWNB). IWNB firstly trains a naive Bayes using the labeled instances only. And the trained naive Bayes is used to estimate the class membership probabilities of the unlabeled instances. Then, the estimated class membership probabilities are used to label and weight unlabeled instances. At last, a naive Bayes is trained again using both the originally labeled data and the (newly labeled and weighted) unlabeled data. Our experimental results based on a large number of UCI data sets show that IWNB often improves the classification accuracy of original naive Bayes when available labeled data are very limited.", "paper_title": "Learning Instance Weighted Naive Bayes from labeled and unlabeled data", "paper_id": "WOS:000299003000009"}