{"auto_keywords": [{"score": 0.0328973830323921, "phrase": "kernel_methods"}, {"score": 0.00481495049065317, "phrase": "large-scale_kernel_methods"}, {"score": 0.004443276312972159, "phrase": "support_vector_machine"}, {"score": 0.0041001740903338834, "phrase": "modern_machine_learning"}, {"score": 0.0038713805573219297, "phrase": "linear_algorithms"}, {"score": 0.003697538418750941, "phrase": "non-linear_scenarios"}, {"score": 0.0035722806911457545, "phrase": "straightforward_way"}, {"score": 0.003334309053314909, "phrase": "kernel_trick"}, {"score": 0.0031845067058084583, "phrase": "naive_use"}, {"score": 0.00290473242917264, "phrase": "computational_complexity"}, {"score": 0.0025892330566358503, "phrase": "training_samples"}, {"score": 0.002361632539339945, "phrase": "recent_advances"}, {"score": 0.0021049977753042253, "phrase": "massive_problems"}], "paper_keywords": ["kernel methods", " support vector machines", " kernel trick", " low-rank approximation", " optimilzation", " structured data"], "paper_abstract": "Kernel methods such as the support vector machine are one of the most successful algorithms in modern machine learning. Their advantage is that linear algorithms are extended to non-linear scenarios in a straightforward way by the use of the kernel trick. However, naive use of kernel methods is computationally expensive since the computational complexity typically scales cubically with respect to the number of training samples. In this article. we review recent advances in the kernel methods. with emphasis on scalability for massive problems.", "paper_title": "Recent Advances and Trends in Large-Scale Kernel Methods", "paper_id": "WOS:000268506200002"}