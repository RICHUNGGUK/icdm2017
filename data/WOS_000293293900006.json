{"auto_keywords": [{"score": 0.04669713469702, "phrase": "original_data"}, {"score": 0.01543806385940299, "phrase": "function_approximation_problems"}, {"score": 0.011019990999542697, "phrase": "data_set"}, {"score": 0.00481495049065317, "phrase": "generating_balanced_learning_and"}, {"score": 0.004267643450023782, "phrase": "building_models"}, {"score": 0.004159763048814389, "phrase": "genuine_out-of-sample_evaluation"}, {"score": 0.004039792863335641, "phrase": "test_sets"}, {"score": 0.0037823114064881357, "phrase": "non-balanced_and_unrepresentative_learning"}, {"score": 0.003659768899837118, "phrase": "wrong_conclusions"}, {"score": 0.0035802796531956413, "phrase": "learning_algorithm"}, {"score": 0.0034516024325468653, "phrase": "key_issue"}, {"score": 0.0032433414042726356, "phrase": "pessimistic_effects"}, {"score": 0.0030032915123993837, "phrase": "deterministic_data_mining_approach"}, {"score": 0.002853183476661506, "phrase": "balanced_sets"}, {"score": 0.002832359116458113, "phrase": "roughly_equal_size"}, {"score": 0.0026516301310428756, "phrase": "learning's_accuracy"}, {"score": 0.002613056713968863, "phrase": "reproducible_machine_learning_experiments"}, {"score": 0.002575042977852052, "phrase": "random_distributions"}, {"score": 0.002473325513637559, "phrase": "clustering_procedure"}, {"score": 0.002265095354846931, "phrase": "nearest-neighbor_approach"}, {"score": 0.0022321322970228308, "phrase": "experiments_section"}, {"score": 0.0021835829960683666, "phrase": "proposed_methodology"}, {"score": 0.0021049977753042253, "phrase": "anova-based_statistical_study"}], "paper_keywords": ["Clustering", " data distribution", " nearest-neighbor", " function approximation", " least-squares support vector machines"], "paper_abstract": "In function approximation problems, one of the most common ways to evaluate a learning algorithm consists in partitioning the original data set (input/output data) into two sets: learning, used for building models, and test, applied for genuine out-of-sample evaluation. When the partition into learning and test sets does not take into account the variability and geometry of the original data, it might lead to non-balanced and unrepresentative learning and test sets and, thus, to wrong conclusions in the accuracy of the learning algorithm. How the partitioning is made is therefore a key issue and becomes more important when the data set is small due to the need of reducing the pessimistic effects caused by the removal of instances from the original data set. Thus, in this work, we propose a deterministic data mining approach for a distribution of a data set (input/output data) into two representative and balanced sets of roughly equal size taking the variability of the data set into consideration with the purpose of allowing both a fair evaluation of learning's accuracy and to make reproducible machine learning experiments usually based on random distributions. The sets are generated using a combination of a clustering procedure, especially suited for function approximation problems, and a distribution algorithm which distributes the data set into two sets within each cluster based on a nearest-neighbor approach. In the experiments section, the performance of the proposed methodology is reported in a variety of situations through an ANOVA-based statistical study of the results.(a)", "paper_title": "GENERATING BALANCED LEARNING AND TEST SETS FOR FUNCTION APPROXIMATION PROBLEMS", "paper_id": "WOS:000293293900006"}