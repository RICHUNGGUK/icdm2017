{"auto_keywords": [{"score": 0.03180308481916928, "phrase": "similarity_function"}, {"score": 0.00481495049065317, "phrase": "state_similarity"}, {"score": 0.004289686492089576, "phrase": "novel_approach"}, {"score": 0.004103212598803194, "phrase": "similar_subpolicies"}, {"score": 0.003754140294151593, "phrase": "reinforcement_learning_framework"}, {"score": 0.0036553070575015344, "phrase": "learning_performance"}, {"score": 0.003465350958971198, "phrase": "specialized_tree_structure"}, {"score": 0.003374094774912924, "phrase": "common_action_sequences"}, {"score": 0.0031422859551159506, "phrase": "possible_optimal_policies"}, {"score": 0.00258347644721642, "phrase": "action-value_function"}, {"score": 0.002405853977978678, "phrase": "similar_states"}, {"score": 0.0021049977753042253, "phrase": "broader_context"}], "paper_keywords": ["action-value function", " learning performance", " optimal policies", " reinforcement learning (RL)", " similarity function", " state similarity"], "paper_abstract": "In this paper, we propose a novel approach to identify states with similar subpolicies and show how they can be integrated into the reinforcement learning framework to improve learning performance. The method utilizes a specialized tree structure to identify common action sequences of states, which are derived from possible optimal policies, and defines a similarity function between two states based on the number of such sequences. Using this similarity function, updates on the action-value function of a state are reflected onto all similar states. This allows experience that is acquired during learning to be applied to a broader context. The effectiveness of the method is demonstrated empirically.", "paper_title": "Positive impact of state similarity on reinforcement learning performance", "paper_id": "WOS:000249594500017"}