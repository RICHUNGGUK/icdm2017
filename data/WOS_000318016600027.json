{"auto_keywords": [{"score": 0.03718034097391595, "phrase": "objective-specific_measures"}, {"score": 0.02954804361689818, "phrase": "loss_functions"}, {"score": 0.00481495049065317, "phrase": "automatic_image_annotation"}, {"score": 0.004688321299229397, "phrase": "multi-label_classification_problem"}, {"score": 0.004595529592539951, "phrase": "major_tools"}, {"score": 0.0045196012587080114, "phrase": "semantic_understanding"}, {"score": 0.00448958076531243, "phrase": "web_images"}, {"score": 0.004242291530697285, "phrase": "image_annotation"}, {"score": 0.0041721750101390825, "phrase": "insufficient_performance"}, {"score": 0.004144452614362445, "phrase": "image_annotation_methods"}, {"score": 0.003981928669805484, "phrase": "specific_measures"}, {"score": 0.0038257535874187953, "phrase": "specific_objective"}, {"score": 0.0035669891404642015, "phrase": "suboptimal_performance"}, {"score": 0.003381607100127276, "phrase": "objective-guided_performance_measures"}, {"score": 0.0032815823097599316, "phrase": "macro-averaging_measures"}, {"score": 0.0032272929113113203, "phrase": "infrequent_keywords"}, {"score": 0.003195149953590668, "phrase": "hamming_measure"}, {"score": 0.0031422859551159506, "phrase": "skewed_distributions"}, {"score": 0.0030799988391729464, "phrase": "unified_multi-label_learning_framework"}, {"score": 0.0029789104204803137, "phrase": "multi-label_learning_tasks"}, {"score": 0.002900426826591769, "phrase": "multilayer_hierarchical_structure"}, {"score": 0.00285242509099472, "phrase": "multi-label_problems"}, {"score": 0.0027495914857929584, "phrase": "objective-guided_measures"}, {"score": 0.002641621667026613, "phrase": "relaxed_surrogate_functions"}, {"score": 0.0025978919678702793, "phrase": "structural_svms"}, {"score": 0.0025125868638809284, "phrase": "high_time_complexity"}, {"score": 0.0024958641143547856, "phrase": "optimizing_micro-averaging_measures"}, {"score": 0.002421975107313603, "phrase": "example-based_measures"}, {"score": 0.002366017944545902, "phrase": "image_annotation_tasks"}, {"score": 0.0022579435307530944, "phrase": "formal_analysis"}, {"score": 0.00216923754570397, "phrase": "state-of-the-art_baseline_methods"}], "paper_keywords": ["Image annotation", " multi-label learning", " performance measures", " structural support vector machine (SVM)"], "paper_abstract": "Automatic image annotation, which is usually formulated as a multi-label classification problem, is one of the major tools used to enhance the semantic understanding of web images. Many multimedia applications (e. g., tag-based image retrieval) can greatly benefit from image annotation. However, the insufficient performance of image annotation methods prevents these applications from being practical. On the other hand, specific measures are usually designed to evaluate how well one annotation method performs for a specific objective or application, but most image annotation methods do not consider optimization of these measures, so that they are inevitably trapped into suboptimal performance of these objective-specific measures. To address this issue, we first summarize a variety of objective-guided performance measures under a unified representation. Our analysis reveals that macro-averaging measures are very sensitive to infrequent keywords, and hamming measure is easily affected by skewed distributions. We then propose a unified multi-label learning framework, which directly optimizes a variety of objective-specific measures of multi-label learning tasks. Specifically, we first present a multilayer hierarchical structure of learning hypotheses for multi-label problems based on which a variety of loss functions with respect to objective-guided measures are defined. And then, we formulate these loss functions as relaxed surrogate functions and optimize them by structural SVMs. According to the analysis of various measures and the high time complexity of optimizing micro-averaging measures, in this paper, we focus on example-based measures that are tailor-made for image annotation tasks but are seldom explored in the literature. Experiments show consistency with the formal analysis on two widely used multi-label datasets, and demonstrate the superior performance of our proposed method over state-of-the-art baseline methods in terms of example-based measures on four image annotation datasets.", "paper_title": "Objective-Guided Image Annotation", "paper_id": "WOS:000318016600027"}