{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "semi-supervised_learning"}, {"score": 0.0046361494357125355, "phrase": "multiple_semi-supervised_assumptions"}, {"score": 0.004217537792708984, "phrase": "labeled_and_unlabeled_data"}, {"score": 0.0031148723927633955, "phrase": "novel_cost_functional_consisting"}, {"score": 0.0030563953060027175, "phrase": "margin_cost"}, {"score": 0.00301801991152303, "phrase": "labeled_data"}, {"score": 0.0029613556844064713, "phrase": "regularization_penalty"}, {"score": 0.0029241700853970013, "phrase": "unlabeled_data"}, {"score": 0.002676587673012276, "phrase": "greedy_yet_stagewise_functional_optimization_procedure"}, {"score": 0.002609770322855842, "phrase": "generic_boosting_framework"}, {"score": 0.002560751416355291, "phrase": "-supervised_learning"}, {"score": 0.0025285834281312705, "phrase": "extensive_experiments"}, {"score": 0.002449915958738505, "phrase": "favorite_results"}, {"score": 0.002419136901298873, "phrase": "benchmark_and_real-world_classification_tasks"}, {"score": 0.0023587312574818208, "phrase": "state-of-the-art_semi-supervised_learning_algorithms"}, {"score": 0.002186395051086251, "phrase": "relevant_issues"}, {"score": 0.0021049977753042253, "phrase": "previous_work"}], "paper_keywords": ["Semi-supervised learning", " boosting framework", " smoothness assumption", " cluster assumption", " manifold assumption", " regularization"], "paper_abstract": "Semi-supervised learning concerns the problem of learning in the presence of labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes all three semi-supervised assumptions, i.e., smoothness, cluster, and manifold assumptions, together into account during boosting learning. In this paper, we propose a novel cost functional consisting of the margin cost on labeled data and the regularization penalty on unlabeled data based on three fundamental semi-supervised assumptions. Thus, minimizing our proposed cost functional with a greedy yet stagewise functional optimization procedure leads to a generic boosting framework for semi-supervised learning. Extensive experiments demonstrate that our algorithm yields favorite results for benchmark and real-world classification tasks in comparison to state-of-the-art semi-supervised learning algorithms, including newly developed boosting algorithms. Finally, we discuss relevant issues and relate our algorithm to the previous work.", "paper_title": "Semi-Supervised Learning via Regularized Boosting Working on Multiple Semi-Supervised Assumptions", "paper_id": "WOS:000284277600010"}