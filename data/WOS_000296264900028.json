{"auto_keywords": [{"score": 0.04928275842421276, "phrase": "multiple_gpus"}, {"score": 0.007740489563401459, "phrase": "single_compute_device_image"}, {"score": 0.0047221610124516524, "phrase": "opencl_runtime"}, {"score": 0.004559451202070545, "phrase": "opencl_framework"}, {"score": 0.00444612502361583, "phrase": "single_compute_device"}, {"score": 0.0043721365634105565, "phrase": "single_gpu_image"}, {"score": 0.004317450843594688, "phrase": "opencl_application"}, {"score": 0.0042455945330942746, "phrase": "single_gpu_portable"}, {"score": 0.004192484908247302, "phrase": "gpgpu_systems"}, {"score": 0.004003348169928476, "phrase": "full_computing_power"}, {"score": 0.003854939427266132, "phrase": "gpu_memories"}, {"score": 0.003696460568091852, "phrase": "run_time"}, {"score": 0.003665552439863732, "phrase": "opencl_kernel"}, {"score": 0.003604506544515226, "phrase": "single_gpu"}, {"score": 0.003574364535717202, "phrase": "multiple_cuda_kernels"}, {"score": 0.003427380591229386, "phrase": "run-time_memory_access_range_analysis"}, {"score": 0.0033280910751441496, "phrase": "sampling_run"}, {"score": 0.0032726467814278345, "phrase": "optimal_workload_distribution"}, {"score": 0.0030857431184193765, "phrase": "virtual_device_memory"}, {"score": 0.0030089355635315005, "phrase": "main_memory"}, {"score": 0.002971249577042111, "phrase": "gpgpu_system"}, {"score": 0.002789763110395331, "phrase": "single_gpo_device"}, {"score": 0.002575665611013556, "phrase": "c_translator"}, {"score": 0.002543392353000551, "phrase": "sampling_code"}, {"score": 0.002511522462767081, "phrase": "opencl_kernel_code"}, {"score": 0.002459297981752542, "phrase": "cuda"}, {"score": 0.0022139352210982398, "phrase": "source_translators"}, {"score": 0.002149722786728988, "phrase": "gpgu_system"}, {"score": 0.002122775261369663, "phrase": "eight_gpus"}, {"score": 0.0021049977753042253, "phrase": "eleven_opencl_benchmark_applications"}], "paper_keywords": ["Algorithm", " Design", " Experimentation", " Languages", " Measurement", " Performance", " OpenCL", " Compilers", " Runtime", " Access range analysis", " Workload distribution", " Virtual device memory"], "paper_abstract": "In this paper, we propose an OpenCL framework that treats multiple GPUs as a single compute device. Providing the single GPU image makes an OpenCL application written for a single GPU portable to the GPGPU systems with multiple GPUs. It also makes the application exploit the full computing power of the multiple GPUs and the entire amount of GPU memories available in the system. Our OpenCL framework automatically distributes at run time an OpenCL kernel written for a single GPU into multiple CUDA kernels that execute on the multiple GPUs. It applies a run-time memory access range analysis to the kernel by performing a sampling run and identifies an optimal workload distribution for the kernel. To achieve a single compute device image, the runtime maintains a virtual device memory that is allocated in the main memory of the GPGPU system. The OpenCL runtime treats the memory as if it were the memory of a single GPO device and keeps it consistent to the memories of the multiple GPUs. Our OpenCL-C-to-C translator generates the sampling code from the OpenCL kernel code and our OpenCL-C-to-CUDA-C translator generates the CUDA kernel code for the distributed OpenCL kernel. We show the effectiveness of our OpenCL framework by implementing the OpenCL runtime and the two source-to-source translators. We evaluate its performance with a GPGU system that contains eight GPUs using eleven OpenCL benchmark applications.", "paper_title": "Achieving a Single Compute Device Image in OpenCL for Multiple GPUs", "paper_id": "WOS:000296264900028"}