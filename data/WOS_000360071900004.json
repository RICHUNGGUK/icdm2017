{"auto_keywords": [{"score": 0.04775829835465016, "phrase": "synthesis_model"}, {"score": 0.00481495049065317, "phrase": "structured_overcomplete_sparsifying_transform_learning"}, {"score": 0.004780926543714981, "phrase": "convergence_guarantees"}, {"score": 0.004663716126870148, "phrase": "sparse_signal_modeling"}, {"score": 0.004377068234090498, "phrase": "np"}, {"score": 0.004207840198934805, "phrase": "sparsifying_transform_model"}, {"score": 0.0041485458333515, "phrase": "sparse_coding"}, {"score": 0.004061160863655662, "phrase": "natural_images"}, {"score": 0.004018158280364063, "phrase": "diverse_textures"}, {"score": 0.0038918527536981897, "phrase": "single_transform"}, {"score": 0.0034987163424726546, "phrase": "proposed_model"}, {"score": 0.003424972304865554, "phrase": "structured_overcomplete"}, {"score": 0.003400736815046841, "phrase": "transform_model"}, {"score": 0.0033766722372180003, "phrase": "block_cosparsity"}, {"score": 0.0033408932140621948, "phrase": "octobos."}, {"score": 0.0032358074601765555, "phrase": "simple_closed-form_solutions"}, {"score": 0.0031563574387360465, "phrase": "convergence_guarantee"}, {"score": 0.0029819840849398234, "phrase": "partial_minimizers"}, {"score": 0.002950374774004939, "phrase": "non-convex_learning_problem"}, {"score": 0.0027873491386063913, "phrase": "stationary_points"}, {"score": 0.0027577972695911825, "phrase": "overall_objective"}, {"score": 0.0026333078412549807, "phrase": "well-conditioned_square_transforms"}, {"score": 0.0025233755103976317, "phrase": "resulting_sparse_representations"}, {"score": 0.0024180213826477093, "phrase": "single_learned_transform"}, {"score": 0.0023838876943453515, "phrase": "analytical_transforms"}, {"score": 0.002341895788806396, "phrase": "promising_performance"}, {"score": 0.002317055705600351, "phrase": "proposed_approach"}, {"score": 0.0023006418600072325, "phrase": "image_denoising"}, {"score": 0.002220296517089146, "phrase": "single_learned_square_transform"}, {"score": 0.0021967433763351884, "phrase": "overcomplete_synthesis_dictionary"}, {"score": 0.0021734395455430167, "phrase": "gaussian_mixture_models"}, {"score": 0.0021503823972534096, "phrase": "proposed_denoising_method"}, {"score": 0.0021049977753042253, "phrase": "synthesis_dictionary"}], "paper_keywords": ["Sparsifying transform learning", " Dictionary learning", " Convergence guarantees", " Overcomplete representation", " Clustering", " Image representation", " Sparse representation", " Image denoising", " Machine learning"], "paper_abstract": "In recent years, sparse signal modeling, especially using the synthesis model has been popular. Sparse coding in the synthesis model is however, NP-hard. Recently, interest has turned to the sparsifying transform model, for which sparse coding is cheap. However, natural images typically contain diverse textures that cannot be sparsified well by a single transform. Hence, in this work, we propose a union of sparsifying transforms model. Sparse coding in this model reduces to a form of clustering. The proposed model is also equivalent to a structured overcomplete sparsifying transform model with block cosparsity, dubbed OCTOBOS. The alternating algorithm introduced for learning such transforms involves simple closed-form solutions. A theoretical analysis provides a convergence guarantee for this algorithm. It is shown to be globally convergent to the set of partial minimizers of the non-convex learning problem. We also show that under certain conditions, the algorithm converges to the set of stationary points of the overall objective. When applied to images, the algorithm learns a collection of well-conditioned square transforms, and a good clustering of patches or textures. The resulting sparse representations for the images are much better than those obtained with a single learned transform, or with analytical transforms. We show the promising performance of the proposed approach in image denoising, which compares quite favorably with approaches involving a single learned square transform or an overcomplete synthesis dictionary, or gaussian mixture models. The proposed denoising method is also faster than the synthesis dictionary based approach.", "paper_title": "Structured Overcomplete Sparsifying Transform Learning with Convergence Guarantees and Applications", "paper_id": "WOS:000360071900004"}