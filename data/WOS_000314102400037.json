{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "elastic-net_regularization"}, {"score": 0.0043786774510943625, "phrase": "statistical_learning_theory"}, {"score": 0.004215397034321004, "phrase": "elastic-net_regularization_problem"}, {"score": 0.004019797503157808, "phrase": "capacity_assumption"}, {"score": 0.003944112858235306, "phrase": "hypothesis_space"}, {"score": 0.0038332390893263844, "phrase": "infinite_features"}, {"score": 0.0037610536973798113, "phrase": "significant_contributions"}, {"score": 0.003485605409687907, "phrase": "concentration_estimates"}, {"score": 0.003419943984098415, "phrase": "sample_error"}, {"score": 0.0031393948330209224, "phrase": "iteration_process"}, {"score": 0.0027479040199262393, "phrase": "elastic-net_learning"}, {"score": 0.0024747842618794255, "phrase": "tuning_parameter"}, {"score": 0.0021049977753042253, "phrase": "existing_results"}], "paper_keywords": ["Learning theory", " Elastic-net regularization", " l(2)-empirical covering number", " Learning rate"], "paper_abstract": "In this paper, within the framework of statistical learning theory we address the elastic-net regularization problem. Based on the capacity assumption of hypothesis space composed by infinite features, significant contributions are made in several aspects. First, concentration estimates for sample error are presented by introducing l(2)-empirical covering number and utilizing an iteration process. Second, a constructive approximation approach for estimating approximation error is presented. Third, the elastic-net learning with infinite features is studied and the role that the tuning parameter. plays is also discussed. Finally, our learning rate is shown to be faster compared with existing results. (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Learning performance of elastic-net regularization", "paper_id": "WOS:000314102400037"}