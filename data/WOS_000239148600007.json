{"auto_keywords": [{"score": 0.04086616588311292, "phrase": "low_dimensional_space"}, {"score": 0.03888852734951892, "phrase": "pca"}, {"score": 0.00481495049065317, "phrase": "greedy_kernel_principal_component_analysis"}, {"score": 0.004644014990871423, "phrase": "statistical_learning"}, {"score": 0.004364919932563366, "phrase": "cognitive_systems"}, {"score": 0.004320055598195391, "phrase": "cognitive_vision"}, {"score": 0.004145143433523008, "phrase": "huge_training_set"}, {"score": 0.0038558497470953306, "phrase": "possibly_minimal_representation_error"}, {"score": 0.0037381083634085424, "phrase": "principal_component_analysis"}, {"score": 0.0033883824438527316, "phrase": "data_representation"}, {"score": 0.003267925215811654, "phrase": "training_set"}, {"score": 0.003151736708029235, "phrase": "basis_vectors"}, {"score": 0.0030554300179552415, "phrase": "simple_algorithm"}, {"score": 0.0030083825666563898, "phrase": "low_computational_requirements"}, {"score": 0.0029620573986902416, "phrase": "on-line_processing"}, {"score": 0.002931569771634119, "phrase": "huge_data_sets"}, {"score": 0.0028273061946184645, "phrase": "proposed_algorithm"}, {"score": 0.002726740692380339, "phrase": "dot_product"}, {"score": 0.0026986685585382347, "phrase": "kernel_methods"}, {"score": 0.0026026666736474404, "phrase": "non-linear_problems"}, {"score": 0.0025625729433234623, "phrase": "proposed_method"}, {"score": 0.0024971144603069006, "phrase": "training_sets"}, {"score": 0.0022749744354271816, "phrase": "proposed_approximation"}, {"score": 0.0021826828321952615, "phrase": "found_classifiers"}], "paper_keywords": [""], "paper_abstract": "This contribution discusses one aspect of statistical learning and generalization. The theory of learning is very relevant to cognitive systems including cognitive vision. A technique allowing to approximate a huge training set is proposed. The approach aims to represent data in a low dimensional space with possibly minimal representation error which is similar to the Principal Component Analysis (PCA). In contrast to the PCA, the basis vectors of the low dimensional space used for data representation are properly selected vectors from the training set and not as their linear combinations. The basis vectors can be selected by a simple algorithm which has low computational requirements and allows on-line processing of huge data sets. As the computations in the proposed algorithm appear in the form of a dot product, kernel methods can be used to cope with non-linear problems. The proposed method was tested to approximate training sets of the Support Vector Machines and the Kernel Fisher Linear Discriminant which are known methods for learning classifiers. The experiments show that the proposed approximation can significantly reduce the complexity of the found classifiers while retaining their accuracy. On the other hand, the method is not very suitable for denoising.", "paper_title": "Greedy Kernel Principal Component Analysis", "paper_id": "WOS:000239148600007"}