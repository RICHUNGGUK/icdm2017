{"auto_keywords": [{"score": 0.03531738520573261, "phrase": "omvd"}, {"score": 0.00481495049065317, "phrase": "mvd."}, {"score": 0.004422827355213055, "phrase": "stephen_d._bay"}, {"score": 0.0039085400187964196, "phrase": "data_mining"}, {"score": 0.0035076666315744525, "phrase": "limited_amount"}, {"score": 0.0032718795115917374, "phrase": "mvd"}, {"score": 0.003051887221639784, "phrase": "genetic_algorithm"}, {"score": 0.002675797331220082, "phrase": "difference_threshold"}, {"score": 0.0025347150832077175, "phrase": "basic_intervals"}, {"score": 0.0023278569001363263, "phrase": "multiple_attributes"}, {"score": 0.0022394839933089074, "phrase": "synthetic_and_real_datasets"}], "paper_keywords": [""], "paper_abstract": "Most discretization algorithms are univariate and consider only one attribute at a time. Stephen D. Bay presented a multivariate discretization(MVD) method that considers the affects of all the attributes in the procedure of data mining. But as the author mentioned, any test of differences has a limited amount of power. We present OMVD by improving MVD on the power of testing differences with a genetic algorithm. OMVD is more powerful than MVD because the former does not suffer from setting the difference threshold and from seriously depending on the basic intervals. In addition, the former simultaneously searches partitions for multiple attributes. Our experiments with some synthetic and real datasets suggest that OMVD could obtain more interesting discretizations than could MVD.", "paper_title": "OMVD: An optimization of MVD", "paper_id": "WOS:000240088200052"}