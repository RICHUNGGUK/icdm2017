{"auto_keywords": [{"score": 0.04848034369451246, "phrase": "autonomous_development"}, {"score": 0.004820730463759954, "phrase": "scales"}, {"score": 0.0047787596516662, "phrase": "cluttered_natural_video"}, {"score": 0.0046192222897398685, "phrase": "brain-inspired_circuits"}, {"score": 0.004251210982670317, "phrase": "inborn_reflexes"}, {"score": 0.004171730509424374, "phrase": "caretaker's_supervision"}, {"score": 0.004047628461126961, "phrase": "real_world"}, {"score": 0.0035734052458933547, "phrase": "task_nonspecific"}, {"score": 0.003325921766364862, "phrase": "attended_object"}, {"score": 0.0032270699806071098, "phrase": "dp"}, {"score": 0.0030606302942851027, "phrase": "neuronal_firing_patterns"}, {"score": 0.0030375852871252934, "phrase": "object_type"}, {"score": 0.0028919522313933525, "phrase": "unsegmented_cluttered_scenes"}, {"score": 0.0027119634978446895, "phrase": "first_one"}, {"score": 0.002671263324230699, "phrase": "multiple_\"brain\"_areas"}, {"score": 0.0026212437316947197, "phrase": "different_range"}, {"score": 0.0026014984513015368, "phrase": "object_scales"}, {"score": 0.002552781867125423, "phrase": "large_natural_video_experiments"}, {"score": 0.0024673738215365104, "phrase": "automatic_attention"}, {"score": 0.0024487847651989128, "phrase": "unknown_cluttered_backgrounds"}, {"score": 0.0024211628058125067, "phrase": "last_experimental_group"}, {"score": 0.002402921062033032, "phrase": "disjoint_tests"}, {"score": 0.0023579143175099324, "phrase": "large_within-class_variations"}, {"score": 0.0021944267528581094, "phrase": "large_similar_and_different_unknown_backgrounds"}, {"score": 0.0021451873499061633, "phrase": "global_classification_tests"}, {"score": 0.002121007284116587, "phrase": "imagenet"}, {"score": 0.0021049977753042253, "phrase": "atari_games"}], "paper_keywords": ["Attention", " brain informed", " cluttered scene", " feature development", " invariance", " multimodal", " neural networks", " object detection", " object recognition"], "paper_abstract": "We model the autonomous development of brain-inspired circuits through two modalities-video stream and action stream that are synchronized in time. We assume that such multimodal streams are available to a baby through inborn reflexes, self-supervision, and caretaker's supervision, when the baby interacts with the real world. By autonomous development, we mean that not only that the internal (inside the \"skull\") self-organization is fully autonomous, but the developmental program (DP) that regulates the computation of the network is also task nonspecific. In this work, the task-nonspecificity is reflected by the fact that the actions associated with an attended object in a cluttered, natural, and dynamic scene is taught after the DP is finished and the \"life\" has begun. The actions correspond to neuronal firing patterns representing object type, object location and object scale, but learning is directly from unsegmented cluttered scenes. Along the line of where-what networks (WWN), this is the first one that explicitly models multiple \"brain\" areas-each for a different range of object scales. Among experiments, large natural video experiments were conducted. To show the power of automatic attention in unknown cluttered backgrounds, the last experimental group demonstrated disjoint tests in the presence of large within-class variations (object 3-D-rotations in very different unknown backgrounds), but small between-class variations (small object patches in large similar and different unknown backgrounds), in contrast with global classification tests such as ImageNet and Atari Games.", "paper_title": "Types, Locations, and Scales from Cluttered Natural Video and Actions", "paper_id": "WOS:000366614800002"}