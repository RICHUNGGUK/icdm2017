{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "pairwise_distances"}, {"score": 0.015585785997602667, "phrase": "missing_data"}, {"score": 0.0047323590206695305, "phrase": "maximum_cuts"}, {"score": 0.00469159356595623, "phrase": "normalized_cuts"}, {"score": 0.0045124316982387315, "phrase": "pairwise_similarities"}, {"score": 0.004228839109569368, "phrase": "particularly_popular_class"}, {"score": 0.004192392802008709, "phrase": "spectral_clustering_algorithms"}, {"score": 0.00403221710768334, "phrase": "pairwise_distance_matrix"}, {"score": 0.003665874747419775, "phrase": "computing_similarities"}, {"score": 0.003261295948815592, "phrase": "clustering_algorithm"}, {"score": 0.003205268682789165, "phrase": "sdp_relaxation"}, {"score": 0.0030035939666559963, "phrase": "frieze"}, {"score": 0.002977665462048617, "phrase": "jerrum"}, {"score": 0.0028887473487202065, "phrase": "yu"}, {"score": 0.0028514043881000674, "phrase": "shi"}, {"score": 0.0027902541641601324, "phrase": "spectral_relaxation"}, {"score": 0.0026718978608934077, "phrase": "simple_heuristic"}, {"score": 0.0023460397258997525, "phrase": "natural_language_terms"}, {"score": 0.002315719347244451, "phrase": "google_distance"}, {"score": 0.002236763286593885, "phrase": "cilibrasi"}, {"score": 0.0022174479745556094, "phrase": "vitanyi"}, {"score": 0.002188784900536406, "phrase": "relative_frequency_counts"}, {"score": 0.002169882514603539, "phrase": "www_queries"}, {"score": 0.0021049977753042253, "phrase": "kolmogorov_complexity"}], "paper_keywords": [""], "paper_abstract": "Clustering algorithms based on a matrix of pairwise similarities (kernel matrix) for the data are widely known and used, a particularly popular class being spectral clustering algorithms. In contrast, algorithms working with the pairwise distance matrix have been studied rarely for clustering. This is surprising, as in many applications, distances are directly given, and computing similarities involves another step that is error-prone, since the kernel has to be chosen appropriately, albeit computationally cheap. This paper proposes a clustering algorithm based on the SDP relaxation of the max-k-cut of the graph of pairwise distances, based on the work of Frieze and Jerrum. We compare the algorithm with Yu and. Shi's algorithm based on spectral relaxation of a norm-k-cut. Moreover, we propose a simple heuristic for dealing with missing data, i.e., the case where some of the pairwise distances or similarities are not known. We evaluate the algorithms on the task of clustering natural language terms with the Google distance, a semantic distance recently introduced by Cilibrasi and Vitanyi, using relative frequency counts from WWW queries and based on the theory of Kolmogorov complexity.", "paper_title": "Clustering pairwise distances with missing data: Maximum cuts versus normalized cuts", "paper_id": "WOS:000241631200021"}