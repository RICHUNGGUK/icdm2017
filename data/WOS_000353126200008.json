{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "regularized_auto-encoders"}, {"score": 0.004731492618963217, "phrase": "data-generating_distribution"}, {"score": 0.004568871736067739, "phrase": "underlying_data-generating_distribution"}, {"score": 0.004431149592844886, "phrase": "auto-encoder_variants"}, {"score": 0.004373398434399966, "phrase": "good_job"}, {"score": 0.004297560964892607, "phrase": "local_manifold_structure"}, {"score": 0.0041136473765874815, "phrase": "previous_observations"}, {"score": 0.004007083686875716, "phrase": "particular_form"}, {"score": 0.00397217701571219, "phrase": "regularized_reconstruction_error"}, {"score": 0.003920384090110374, "phrase": "reconstruction_function"}, {"score": 0.0037855348847339655, "phrase": "data-generating_density"}, {"score": 0.003408091901859624, "phrase": "previous_interpretations"}, {"score": 0.003378385493203297, "phrase": "reconstruction_error"}, {"score": 0.003334309053314909, "phrase": "energy_function"}, {"score": 0.0032908057622663732, "phrase": "previous_results"}, {"score": 0.0027741761880779535, "phrase": "contractive_training_criterion"}, {"score": 0.0026786469998321084, "phrase": "denoising_auto-encoder_training_criterion"}, {"score": 0.002541469965286169, "phrase": "whole_reconstruction_function"}, {"score": 0.0023902621773166963, "phrase": "proposed_training_criterion"}, {"score": 0.002359046870732839, "phrase": "convenient_alternative"}, {"score": 0.0023384629381463054, "phrase": "maximum_likelihood"}, {"score": 0.0022678201262721323, "phrase": "partition_function"}, {"score": 0.0021896891011033105, "phrase": "approximate_metropolis-hastings_mcmc"}, {"score": 0.0021049977753042253, "phrase": "estimated_distribution"}], "paper_keywords": ["auto-encoders", " denoising auto-encoders", " score matching", " unsupervised representation learning", " manifold learning", " Markov chains", " generative models"], "paper_abstract": "What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.", "paper_title": "What Regularized Auto-Encoders Learn from the Data-Generating Distribution", "paper_id": "WOS:000353126200008"}