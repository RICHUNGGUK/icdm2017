{"auto_keywords": [{"score": 0.04010882425349306, "phrase": "underlying_graph"}, {"score": 0.015719716506582538, "phrase": "nonchordal_graphs"}, {"score": 0.01379149283548322, "phrase": "covariance_selection"}, {"score": 0.010539402957566718, "phrase": "hessian"}, {"score": 0.0047517225918491226, "phrase": "chordal_embedding"}, {"score": 0.004566954456805558, "phrase": "maximum_likelihood_estimation"}, {"score": 0.004506968388996092, "phrase": "gaussian_graphical_models"}, {"score": 0.004447766706144458, "phrase": "conditional_independence_constraints"}, {"score": 0.0039748192550806815, "phrase": "unconstrained_convex_optimization_problem"}, {"score": 0.003896717568943166, "phrase": "closed-form_solution"}, {"score": 0.0035519826240477444, "phrase": "iterative_algorithms"}, {"score": 0.0033465923623290034, "phrase": "efficient_methods"}, {"score": 0.003153041030690117, "phrase": "log-likelihood_function"}, {"score": 0.002912222095383313, "phrase": "simple_recursions"}, {"score": 0.0028549396580672417, "phrase": "clique_tree"}, {"score": 0.0023249757987087055, "phrase": "efficient_implementations"}, {"score": 0.002294369814584028, "phrase": "newton's_method"}, {"score": 0.0022492129111567824, "phrase": "conjugate_gradient_method"}, {"score": 0.0022196020509093694, "phrase": "large_nonchordal_graphs"}, {"score": 0.0021049977753042253, "phrase": "chordal_graph"}], "paper_keywords": ["convex optimization", " sparse matrices", " chordal graphs"], "paper_abstract": "We describe algorithms for maximum likelihood estimation of Gaussian graphical models with conditional independence constraints. This problem is also known as covariance selection, and it can be expressed as an unconstrained convex optimization problem with a closed-form solution if the underlying graph is chordal. The focus of the paper is on iterative algorithms for covariance selection with nonchordal graphs. We first derive efficient methods for evaluating the gradient and Hessian of the log-likelihood function when the underlying graph is chordal. The algorithms are formulated as simple recursions on a clique tree associated with the graph. We also show that the gradient and Hessian mappings are easily inverted when the underlying graph is chordal. We then exploit these results to obtain efficient implementations of Newton's method and the conjugate gradient method for large nonchordal graphs, by embedding the graph in a chordal graph.", "paper_title": "Covariance selection for nonchordal graphs via chordal embedding", "paper_id": "WOS:000257545100004"}