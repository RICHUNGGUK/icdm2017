{"auto_keywords": [{"score": 0.040657919400976524, "phrase": "problem_domain"}, {"score": 0.013225114781308655, "phrase": "\"good_proximal_setup"}, {"score": 0.007319438778441233, "phrase": "primal_problem"}, {"score": 0.00481495049065317, "phrase": "large-scale_nonsmooth_learning_problems"}, {"score": 0.004646496622257368, "phrase": "convex_optimization"}, {"score": 0.004580769304731227, "phrase": "mirror_descent_algorithm"}, {"score": 0.004548253531664032, "phrase": "nesterov's_optimal_algorithm"}, {"score": 0.004515967518891287, "phrase": "smooth_convex_optimization"}, {"score": 0.004296264970612285, "phrase": "problem_dimension"}, {"score": 0.0038060092376105414, "phrase": "\"favorable_geometry"}, {"score": 0.0035949949979695063, "phrase": "moderate_cost_proximal_transformation"}, {"score": 0.003419943984098415, "phrase": "optimization_problems"}, {"score": 0.003383552654319383, "phrase": "computational_learning"}, {"score": 0.00332375606483146, "phrase": "proximal_type_fo_methods"}, {"score": 0.003105979498529766, "phrase": "multi-task_learning"}, {"score": 0.002965229277927781, "phrase": "favorable_geometry"}, {"score": 0.0029024303666003153, "phrase": "completion_problems"}, {"score": 0.002881793492919646, "phrase": "nuclear_norm_constraint"}, {"score": 0.0028409576481842457, "phrase": "numerical_cost"}, {"score": 0.0028107098861552124, "phrase": "proximal_transformation"}, {"score": 0.0027708785175203556, "phrase": "large-scale_problems"}, {"score": 0.0027218799769229596, "phrase": "novel_approach"}, {"score": 0.0026928966146175636, "phrase": "nonsmooth_optimization_problems"}, {"score": 0.002635850034826311, "phrase": "fenchel-type_representation"}, {"score": 0.0026077803487003, "phrase": "objective_function"}, {"score": 0.002516350372379076, "phrase": "fo_algorithms"}, {"score": 0.0024895500851796603, "phrase": "dual_problem"}, {"score": 0.002454258761940649, "phrase": "accuracy_certificates"}, {"score": 0.002385166298265828, "phrase": "primal_solution"}, {"score": 0.002326304140277375, "phrase": "accuracy_guaranties"}, {"score": 0.0023015232625177755, "phrase": "proposed_approach"}, {"score": 0.0021659920594094407, "phrase": "linear_optimization_oracle"}, {"score": 0.0021049977753042253, "phrase": "linear_form"}], "paper_keywords": [""], "paper_abstract": "Classical First Order (FO) algorithms of convex optimization, such as Mirror Descent algorithm or Nesterov's optimal algorithm of smooth convex optimization, are well known to have optimal (theoretical) complexity estimates which do not depend on the problem dimension. However, to attain the optimality, the domain of the problem should admit a \"good proximal setup\". The latter essentially means that (1) the problem domain should satisfy certain geometric conditions of \"favorable geometry\", and (2) the practical use of these methods is conditioned by our ability to compute at a moderate cost proximal transformation at each iteration. More often than not these two conditions are satisfied in optimization problems arising in computational learning, what explains why proximal type FO methods recently became methods of choice when solving various learning problems. Yet, they meet their limits in several important problems such as multi-task learning with large number of tasks, where the problem domain does not exhibit favorable geometry, and learning and matrix completion problems with nuclear norm constraint, when the numerical cost of computing proximal transformation becomes prohibitive in large-scale problems. We propose a novel approach to solving nonsmooth optimization problems arising in learning applications where Fenchel-type representation of the objective function is available. The approach is based on applying FO algorithms to the dual problem and using the accuracy certificates supplied by the method to recover the primal solution. While suboptimal in terms of accuracy guaranties, the proposed approach does not rely upon \"good proximal setup\" for the primal problem but requires the problem domain to admit a Linear Optimization oracle-the ability to efficiently maximize a linear form on the domain of the primal problem.", "paper_title": "Dual subgradient algorithms for large-scale nonsmooth learning problems", "paper_id": "WOS:000344809700006"}