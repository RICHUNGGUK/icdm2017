{"auto_keywords": [{"score": 0.04274518365190484, "phrase": "input_video"}, {"score": 0.00481495049065317, "phrase": "facial_expression"}, {"score": 0.004614643339546038, "phrase": "data-driven_approach"}, {"score": 0.004360412703519539, "phrase": "face_video"}, {"score": 0.004299064805926241, "phrase": "target_subject"}, {"score": 0.003948613642680196, "phrase": "pre-existing_facial_expression_database"}, {"score": 0.003820144626025562, "phrase": "realistic_synthesis"}, {"score": 0.0036095259108028105, "phrase": "new_facial_expression_similarity"}, {"score": 0.0034920505103165403, "phrase": "expression_database"}, {"score": 0.0034428776029411974, "phrase": "target_person"}, {"score": 0.0033943947596098583, "phrase": "multiple_candidate_images"}, {"score": 0.0032376465741070274, "phrase": "similarity_metric"}, {"score": 0.0031619994642255846, "phrase": "metric_learning_approach"}, {"score": 0.003102752392819577, "phrase": "appearance_difference"}, {"score": 0.003073545175760353, "phrase": "different_subjects"}, {"score": 0.00297346171320082, "phrase": "optimization_approach"}, {"score": 0.002917736974519881, "phrase": "best_candidate_image"}, {"score": 0.0028093921138602606, "phrase": "retrieved_sequence"}, {"score": 0.0026922926476971453, "phrase": "spatio-temporal_expression_mapping_method"}, {"score": 0.0026045914281028473, "phrase": "synthesized_sequence"}, {"score": 0.002580061425699194, "phrase": "experimental_results"}, {"score": 0.0024842242921857705, "phrase": "high_quality_facial_expression_videos"}, {"score": 0.0024146842687658467, "phrase": "input_sequences"}, {"score": 0.0023030731312573246, "phrase": "big_identity_difference"}, {"score": 0.0022492129111567824, "phrase": "extensive_evaluations"}, {"score": 0.0022175019341824603, "phrase": "high_accuracy"}], "paper_keywords": ["Facial expression", " expression retargeting", " expression synthesis", " expression similarity metric", " data-driven"], "paper_abstract": "This paper presents a data-driven approach for facial expression retargeting in video, i.e., synthesizing a face video of a target subject that mimics the expressions of a source subject in the input video. Our approach takes advantage of a pre-existing facial expression database of the target subject to achieve realistic synthesis. First, for each frame of the input video, a new facial expression similarity metric is proposed for querying the expression database of the target person to select multiple candidate images that are most similar to the input. The similarity metric is developed using a metric learning approach to reliably handle appearance difference between different subjects. Secondly, we employ an optimization approach to choose the best candidate image for each frame, resulting in a retrieved sequence that is temporally coherent. Finally, a spatio-temporal expression mapping method is employed to further improve the synthesized sequence. Experimental results show that our system is capable of generating high quality facial expression videos that match well with the input sequences, even when the source and target subjects have big identity difference. In addition, extensive evaluations demonstrate the high accuracy of the learned expression similarity metric and the effectiveness of our retrieval strategy.", "paper_title": "A Data-Driven Approach for Facial Expression Retargeting in Video", "paper_id": "WOS:000330245800002"}