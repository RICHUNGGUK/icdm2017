{"auto_keywords": [{"score": 0.04351549802598363, "phrase": "boltzmann"}, {"score": 0.013465336960602614, "phrase": "gpu"}, {"score": 0.00481495049065317, "phrase": "soft_matter_physics"}, {"score": 0.004721711709176679, "phrase": "graphics_processing_units"}, {"score": 0.004562847483469705, "phrase": "multi-graphics_processing_unit"}, {"score": 0.004409304640026443, "phrase": "ludwig_application"}, {"score": 0.004219427915728972, "phrase": "complex_fluids"}, {"score": 0.00413767314434795, "phrase": "fluid_dynamics"}, {"score": 0.004077394032903726, "phrase": "additional_physics"}, {"score": 0.004037694641919865, "phrase": "complex_fluid_constituents"}, {"score": 0.0038827100184187805, "phrase": "original_central_processing_unit"}, {"score": 0.003770373393393271, "phrase": "gpu_functionality"}, {"score": 0.003715425273229361, "phrase": "maintainable_fashion"}, {"score": 0.0035379628447303703, "phrase": "gpu_architecture"}, {"score": 0.0034524248924699985, "phrase": "gpu_memory_hierarchy"}, {"score": 0.003192312493850036, "phrase": "major_diversion"}, {"score": 0.003115105266246263, "phrase": "gpu_codebases"}, {"score": 0.0030696773722514105, "phrase": "minimising_data_transfer"}, {"score": 0.002795750446832946, "phrase": "amd"}, {"score": 0.0024028135317872, "phrase": "gpu_version"}, {"score": 0.0022434880661725493, "phrase": "cpu_version"}, {"score": 0.0021049977753042253, "phrase": "equal_numbers"}], "paper_keywords": ["Lattice Boltzmann", " fluid dynamics", " molecular dynamics", " Compute Unified Device Architecture", " MPI", " parallel scaling"], "paper_abstract": "We describe a multi-graphics processing unit (GPU) implementation of the Ludwig application, which specialises in simulating a variety of complex fluids via lattice Boltzmann fluid dynamics coupled to additional physics describing complex fluid constituents. We describe our methodology in augmenting the original central processing unit (CPU) version with GPU functionality in a maintainable fashion. We present several optimisations that maximise performance on the GPU architecture through tuning for the GPU memory hierarchy. We describe how we implement particles within the fluid in such a way to avoid a major diversion of the CPU and GPU codebases, whilst minimising data transfer at each time step. We detail our halo-exchange communication phase for the code, which exploits overlapping to allow efficient parallel scaling to many GPUs. We present results showing that the application demonstrates excellent scaling to at least 8192 GPUs in parallel, the largest system tested at the time of writing. The GPU version (on NVIDIA K20X GPUs) is around 3.5-5 times faster that the CPU version (on fully utilised AMD Opteron 6274 16-core CPUs), comparing equal numbers of CPUs and GPUs.", "paper_title": "Scaling soft matter physics to thousands of graphics processing units in parallel", "paper_id": "WOS:000358414200004"}