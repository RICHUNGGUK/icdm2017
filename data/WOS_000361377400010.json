{"auto_keywords": [{"score": 0.03528453069935319, "phrase": "game_annotations"}, {"score": 0.013100131086842716, "phrase": "human_annotators"}, {"score": 0.01266263934707297, "phrase": "game_states"}, {"score": 0.00481495049065317, "phrase": "learning_from_game_annotations"}, {"score": 0.004613175917207636, "phrase": "evaluation_function_learning"}, {"score": 0.004363401436068803, "phrase": "expert_feedback"}, {"score": 0.004216434794603139, "phrase": "annotated_games"}, {"score": 0.004039639718055365, "phrase": "qualitative_information"}, {"score": 0.0039035351575463103, "phrase": "precise_utility_values"}, {"score": 0.003598317040926701, "phrase": "qualitative_feedback"}, {"score": 0.003537155405923517, "phrase": "evaluation_function"}, {"score": 0.003274499008699115, "phrase": "preference_statements"}, {"score": 0.0030705342386533083, "phrase": "utility_function"}, {"score": 0.0030183163057929687, "phrase": "preference_constraints"}, {"score": 0.002954287204671032, "phrase": "resulting_function"}, {"score": 0.0029165216647792924, "phrase": "multiple_heuristics"}, {"score": 0.002879237498994052, "phrase": "different_sized_subsets"}, {"score": 0.002842428606954139, "phrase": "training_data"}, {"score": 0.0027702126237754625, "phrase": "tournament_scenario"}, {"score": 0.0023942269247089277, "phrase": "chess_program"}, {"score": 0.0022257833099336858, "phrase": "\"interesting\"_positions"}, {"score": 0.0021414489155042885, "phrase": "basic_information"}, {"score": 0.0021049977753042253, "phrase": "material_advantage"}], "paper_keywords": ["Computer Chess", " machine learning in games", " preference learning"], "paper_abstract": "Most of the research in the area of evaluation function learning is focused on self-play. However in many domains, like Chess, expert feedback is amply available in the form of annotated games. This feedback usually comes in the form of qualitative information because human annotators find it hard to determine precise utility values for game states. The goal of this work is to investigate inasmuch it is possible to leverage this qualitative feedback for learning an evaluation function for the game. To this end, we show how the game annotations can be translated into preference statements over moves and game states, which in turn can be used for learning a utility function that respects these preference constraints. We evaluate the resulting function by creating multiple heuristics based upon different sized subsets of the training data and compare them in a tournament scenario. The results showed that learning from game annotations is possible, but, on the other hand, our learned functions did not quite reach the performance of the original, manually tuned function of the Chess program. The reason for this failure seems to lie in the fact that human annotators only annotate \"interesting\" positions, so that it is hard to learn basic information, such as material advantage from game annotations alone.", "paper_title": "On Learning From Game Annotations", "paper_id": "WOS:000361377400010"}