{"auto_keywords": [{"score": 0.04876479047261597, "phrase": "linear_model_selection"}, {"score": 0.04523707438879961, "phrase": "sos"}, {"score": 0.040130255517357764, "phrase": "ls"}, {"score": 0.0045765783794477505, "phrase": "computationally_effective_algorithm"}, {"score": 0.004134508282135594, "phrase": "thresholded_lasso"}, {"score": 0.003947884118204803, "phrase": "screened_predictors"}, {"score": 0.0038577428353448596, "phrase": "least_squares"}, {"score": 0.0035335154419206634, "phrase": "greedy_generalized_information_criterion"}, {"score": 0.0033428820402140683, "phrase": "nested_family"}, {"score": 0.0032215231177304513, "phrase": "non-asymptotic_upper_bounds"}, {"score": 0.00309023627455429, "phrase": "sos_algorithm"}, {"score": 0.002950608985750058, "phrase": "selection_consistency"}, {"score": 0.002652878591019016, "phrase": "numerical_experiments"}, {"score": 0.0025447065763362984, "phrase": "multi-stage_convex_relaxation"}, {"score": 0.0024409345168575833, "phrase": "traditional_setting"}, {"score": 0.0023522423291484212, "phrase": "sanov-type_bounds"}, {"score": 0.0023198180380964305, "phrase": "error_probabilities"}, {"score": 0.0022878396720586044, "phrase": "ordering-selection_algorithm"}, {"score": 0.0022458849724980904, "phrase": "surprising_consequence"}, {"score": 0.0021945194903838132, "phrase": "selection_error"}, {"score": 0.002174303033371867, "phrase": "greedy_gic"}, {"score": 0.0021049977753042253, "phrase": "exhaustive_gic."}], "paper_keywords": ["linear model selection", " penalized least squares", " Lasso", " generalized information criterion", " greedy search", " multi-stage convex relaxation"], "paper_abstract": "We introduce a computationally effective algorithm for a linear model selection consisting of three steps: screening-ordering-selection (SOS). Screening of predictors is based on the thresholded Lasso that is l(1) penalized least squares. The screened predictors are then fitted using least squares (LS) and ordered with respect to their vertical bar t vertical bar statistics. Finally, a model is selected using greedy generalized information criterion (GIC) that is l(0) penalized LS in a nested family induced by the ordering. We give non-asymptotic upper bounds on error probability of each step of the SOS algorithm in terms of both penalties. Then we obtain selection consistency for different (n, p) scenarios under conditions which are needed for screening consistency of the Lasso. Our error bounds and numerical experiments show that SOS is worth considering alternative for multi-stage convex relaxation, the latest quasiconvex penalized LS. For the traditional setting (n > p) we give Sanov-type bounds on the error probabilities of the ordering-selection algorithm. It is surprising consequence of our bounds that the selection error of greedy GIC is asymptotically not larger than of exhaustive GIC.", "paper_title": "Combined l(1) and Greedy l(0) Penalized Least Squares for Linear Model Selection", "paper_id": "WOS:000369886400001"}