{"auto_keywords": [{"score": 0.04862079694002719, "phrase": "delayed_evaluative_feedback"}, {"score": 0.027896593916530085, "phrase": "td-falcon_systems"}, {"score": 0.00481495049065317, "phrase": "temporal_difference_methods"}, {"score": 0.004772593750252861, "phrase": "self-organizing_neural_networks"}, {"score": 0.004546174820239776, "phrase": "neural_architecture"}, {"score": 0.0044863018460183784, "phrase": "category_nodes"}, {"score": 0.004407690586271672, "phrase": "multimodal_patterns"}, {"score": 0.00436890075128947, "phrase": "sensory_inputs"}, {"score": 0.004198510172614725, "phrase": "adaptive_resonance_theory"}, {"score": 0.004106725110985659, "phrase": "temporal_difference"}, {"score": 0.0035965032173588753, "phrase": "autonomous_agent"}, {"score": 0.0034868462688262864, "phrase": "dynamic_environment"}, {"score": 0.0032342101452939977, "phrase": "value_functions"}, {"score": 0.003191560305761507, "phrase": "state-action_space"}, {"score": 0.0030942116765996426, "phrase": "off-policy_td_learning_methods"}, {"score": 0.0029212079665414466, "phrase": "sarsa"}, {"score": 0.0028195727965553367, "phrase": "learned_value_functions"}, {"score": 0.002733539375462443, "phrase": "optimal_actions"}, {"score": 0.002685558057339722, "phrase": "action_selection_policy"}, {"score": 0.0025241429061867633, "phrase": "task_completion"}, {"score": 0.0024148064819862337, "phrase": "space_efficiency"}, {"score": 0.002351485871160907, "phrase": "minefield_navigation_task"}, {"score": 0.0022297712188622293, "phrase": "immediate_and_delayed_reinforcement"}, {"score": 0.0021906130276146325, "phrase": "stable_performance"}, {"score": 0.0021049977753042253, "phrase": "standard_gradient-descent-based_reinforcement_learning_systems"}], "paper_keywords": ["reinforcement learning", " self-organizing neural networks (NNs)", " temporal difference (TD) methods"], "paper_abstract": "This paper presents a neural architecture for learning category nodes encoding mappings across multimodal patterns involving sensory inputs, actions, and rewards. By integrating adaptive resonance theory (ART) and temporal difference (TD) methods, the proposed neural model, called TD fusion architecture for learning, cognition, and navigation (TD-FALCON), enables an autonomous agent to adapt and function in a dynamic environment with immediate as well as delayed evaluative feedback (reinforcement) signals. TD-FALCON learns the value functions of the state-action space estimated through on-policy and off-policy TD learning methods, specifically state-action-reward-state-action (SARSA) and Q-learning. The learned value functions are then used to determine the optimal actions based on an action selection policy. We have developed TD-FALCON systems using various TD learning strategies and compared their performance in terms of task completion, learning speed, as well as time and space efficiency. Experiments based on a minefield navigation task have shown that TD-FALCON systems are able to learn effectively with both immediate and delayed reinforcement and achieve a stable performance in a pace much faster than those of standard gradient-descent-based reinforcement learning systems.", "paper_title": "Integrating temporal difference methods and self-organizing neural networks for reinforcement learning with delayed evaluative feedback", "paper_id": "WOS:000253272100003"}