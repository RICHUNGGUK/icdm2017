{"auto_keywords": [{"score": 0.04131289278612189, "phrase": "reduced_set"}, {"score": 0.00481495049065317, "phrase": "globally_optimal_prototype_subset"}, {"score": 0.004758457292840376, "phrase": "nearest-neighbor_classification"}, {"score": 0.004674952036988355, "phrase": "nearest-neighbor_classifier"}, {"score": 0.004485735417293941, "phrase": "powerful_tool"}, {"score": 0.00443308765153716, "phrase": "multiclass_classification"}, {"score": 0.004304144138805271, "phrase": "theoretical_properties"}, {"score": 0.004253618550301311, "phrase": "empirical_behavior"}, {"score": 0.004178935413340453, "phrase": "variant_method"}, {"score": 0.00405735420159564, "phrase": "nearest-neighbor_rule"}, {"score": 0.003584025547693836, "phrase": "empirical_misclassification_cost"}, {"score": 0.0033584796908285894, "phrase": "nearest-neighbor_method"}, {"score": 0.0030207495700347075, "phrase": "np"}, {"score": 0.002914288956035066, "phrase": "mixed_integer_programming"}, {"score": 0.0028800571553271774, "phrase": "mip"}, {"score": 0.0026827569390113822, "phrase": "standard_mip_solver"}, {"score": 0.002651213849567608, "phrase": "small_problem_instances"}, {"score": 0.0024405312482831646, "phrase": "benchmark_procedures"}, {"score": 0.0023834640793589435, "phrase": "large_problem_instances"}, {"score": 0.0023003498123420237, "phrase": "good_classification_rules"}, {"score": 0.002273292707863821, "phrase": "reasonable_time"}, {"score": 0.002246553137021003, "phrase": "additional_experiments"}, {"score": 0.0022070310530794097, "phrase": "prototype-based_nearest-neighbor_classifiers"}, {"score": 0.0021049977753042253, "phrase": "missing_values"}], "paper_keywords": ["classification", " optimal prototype subset", " nearest neighbor", " dissimilarities", " integer programming", " variable neighborhood search", " missing values"], "paper_abstract": "The nearest-neighbor classifier has been shown to be a powerful tool for multiclass classification. We explore both theoretical properties and empirical behavior of a variant method, in which the nearest-neighbor rule is applied to a reduced set of prototypes. This set is selected a priori by fixing its cardinality and minimizing the empirical misclassification cost. In this way we alleviate the two serious drawbacks of the nearest-neighbor method: high storage requirements and time-consuming queries. Finding this reduced set is shown to be NP-hard. We provide mixed integer programming (MIP) formulations, which are theoretically compared and solved by a standard MIP solver for small problem instances. We show that the classifiers derived from these formulations are comparable to benchmark procedures. We solve large problem instances by a metaheuristic that yields good classification rules in reasonable time. Additional experiments indicate that prototype-based nearest-neighbor classifiers remain quite stable in the presence of missing values.", "paper_title": "On the selection of the globally optimal prototype subset for nearest-neighbor classification", "paper_id": "WOS:000248990800014"}