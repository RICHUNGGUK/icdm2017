{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "general_single_images"}, {"score": 0.004023120753104693, "phrase": "depth_information"}, {"score": 0.0034426417353349567, "phrase": "small-scale_defocus_blur"}, {"score": 0.0031277316566463978, "phrase": "estimation_problem"}, {"score": 0.002981216610222911, "phrase": "non-parametric_matching_scheme"}, {"score": 0.0025202582748972122, "phrase": "non-local_scheme"}, {"score": 0.00240213106042981, "phrase": "semantic_depth_order_cues"}, {"score": 0.0023451553255837317, "phrase": "physically_based_inference"}, {"score": 0.0021821915946356168, "phrase": "natural_images"}, {"score": 0.0021049977753042253, "phrase": "geometry_based_rendering"}], "paper_keywords": ["small-blur estimation", " depth from defocus", " single-image", " depth", " out-of-focus"], "paper_abstract": "Photos compress 3D visual data to 2D. However, it is still possible to infer depth information even without sophisticated object learning. We propose a solution based on small-scale defocus blur inherent in optical lens and tackle the estimation problem by proposing a non-parametric matching scheme for natural images. It incorporates a matching prior with our newly constructed edgelet dataset using a non-local scheme, and includes semantic depth order cues for physically based inference. Several applications are enabled on natural images, including geometry based rendering and editing.", "paper_title": "Break Ames Room Illusion: Depth from General Single Images", "paper_id": "WOS:000363671200062"}