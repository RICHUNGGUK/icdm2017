{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayesian_citation-knn"}, {"score": 0.02899905057183092, "phrase": "bcknn"}, {"score": 0.004761850836420121, "phrase": "distance_weighting"}, {"score": 0.0046062280623555395, "phrase": "mi"}, {"score": 0.0044800522651664695, "phrase": "growing_attention"}, {"score": 0.004406121974246201, "phrase": "machine_learning_research_field"}, {"score": 0.004309434349688514, "phrase": "learning_examples"}, {"score": 0.004054299982508618, "phrase": "single_instance"}, {"score": 0.003965347351665673, "phrase": "knn"}, {"score": 0.0038782499819752423, "phrase": "simple_and_effective_classification_model"}, {"score": 0.003814212645832246, "phrase": "traditional_supervised_learning"}, {"score": 0.00370981613076245, "phrase": "bayesian-knn"}, {"score": 0.003668859600471096, "phrase": "bknn"}, {"score": 0.0033384259516301223, "phrase": "multi-instance_classification_problems"}, {"score": 0.003265090934183249, "phrase": "cknn"}, {"score": 0.0031933617014256676, "phrase": "simplest_majority_vote_approach"}, {"score": 0.0030545815929619306, "phrase": "unseen_bags"}, {"score": 0.002905629026254221, "phrase": "improved_algorithm"}, {"score": 0.002763919794139171, "phrase": "unseen_bag"}, {"score": 0.0026584843838039166, "phrase": "q_citers"}, {"score": 0.0025713100093065645, "phrase": "bayesian_approach"}, {"score": 0.002473203873496859, "phrase": "distance_weighted_majority_vote_approach"}, {"score": 0.0023788319661581696, "phrase": "experimental_results"}, {"score": 0.0022502133699402018, "phrase": "previous_bknn"}, {"score": 0.0022253345930383257, "phrase": "cknn._besides"}, {"score": 0.0021285340816566906, "phrase": "computational_overhead"}, {"score": 0.0021049977753042253, "phrase": "cknn."}], "paper_keywords": ["Multi-instance learning", " KNN", " Bayesian-KNN", " Citation-KNN", " Bayesian Citation-KNN", " Distance weighting"], "paper_abstract": "Multi-instance (MI) learning is receiving growing attention in the machine learning research field, in which learning examples are represented by a bag of instances instead of a single instance. K-nearest-neighbor (KNN) is a simple and effective classification model in the traditional supervised learning. As its two variants, Bayesian-KNN (BKNN) and Citation-KNN (CKNN) are proposed and are widely used for solving multi-instance classification problems. However, CKNN still applies the simplest majority vote approach among the references and citers to classify unseen bags. In this paper, we propose an improved algorithm called Bayesian Citation-KNN (BCKNN). For each unseen bag, BCKNN firstly finds its k references and q citers respectively, and then a Bayesian approach is applied to its k references and a distance weighted majority vote approach is applied to its q citers. The experimental results on several benchmark datasets show that our BCKNN is generally better than previous BKNN and CKNN. Besides, BCKNN almost maintains the same order of computational overhead as CKNN.", "paper_title": "Bayesian Citation-KNN with distance weighting", "paper_id": "WOS:000348039800002"}