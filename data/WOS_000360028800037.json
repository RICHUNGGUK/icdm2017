{"auto_keywords": [{"score": 0.037493396833055075, "phrase": "drelm"}, {"score": 0.03558266368816025, "phrase": "elm"}, {"score": 0.015719576230497776, "phrase": "deep_representations"}, {"score": 0.015481293532995096, "phrase": "extreme_learning_machine"}, {"score": 0.004777917315737992, "phrase": "extreme_learning_machines"}, {"score": 0.004632597963417253, "phrase": "emerging_technology"}, {"score": 0.004579244735256393, "phrase": "exceptional_performance"}, {"score": 0.004544016163767266, "phrase": "large-scale_settings"}, {"score": 0.004422827355213055, "phrase": "binary_and_multi-class_classification"}, {"score": 0.004338239182984759, "phrase": "regression_tasks"}, {"score": 0.004271729935895193, "phrase": "existing_elm"}, {"score": 0.004190019394361629, "phrase": "single_hidden_layer_feedforward_networks"}, {"score": 0.004125773078637226, "phrase": "popular_and_potentially_powerful_stacked_generalization"}, {"score": 0.004046843083499649, "phrase": "predictive_deep_representations"}, {"score": 0.004015693806064219, "phrase": "input_data"}, {"score": 0.00398478333092707, "phrase": "deep_architectures"}, {"score": 0.00393886140366453, "phrase": "higher-level_representations"}, {"score": 0.0038485929962942776, "phrase": "relevant_higher-level_abstractions"}, {"score": 0.0037749456215035856, "phrase": "current_deep_learning_methods"}, {"score": 0.0037170399498025215, "phrase": "difficult_and_non-convex_optimization_problem"}, {"score": 0.0035899674590142653, "phrase": "stacked_model"}, {"score": 0.0034405207528624983, "phrase": "stacked_generalization_philosophy"}, {"score": 0.0034008496666026585, "phrase": "proposed_model"}, {"score": 0.0033357416981328577, "phrase": "base_building_block"}, {"score": 0.0032972747743006603, "phrase": "random_shift"}, {"score": 0.0030994630902300133, "phrase": "random_projection"}, {"score": 0.002993439939273828, "phrase": "original_feature"}, {"score": 0.0029361086955646625, "phrase": "kernel_functions"}, {"score": 0.002891032998395276, "phrase": "resultant_feature"}, {"score": 0.0028138046006772567, "phrase": "regression_performance"}, {"score": 0.0026043035241045394, "phrase": "kernel_elms"}, {"score": 0.0025054621733192283, "phrase": "predictive_features"}, {"score": 0.0024574537875026634, "phrase": "prediction_tasks"}, {"score": 0.002401053532270901, "phrase": "deep_models"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Extreme learning machine", " Deep learning", " Representation learning", " Stacked ELMs", " Stacked generalization", " DrELM"], "paper_abstract": "Extreme learning machine (ELM) as an emerging technology has achieved exceptional performance in large-scale settings, and is well suited to binary and multi-class classification, as well as regression tasks. However, existing ELM and its variants predominantly employ single hidden layer feedforward networks, leaving the popular and potentially powerful stacked generalization principle unexploited for seeking predictive deep representations of input data. Deep architectures can find higher-level representations, thus can potentially capture relevant higher-level abstractions. But most of current deep learning methods require solving a difficult and non-convex optimization problem. In this paper, we propose a stacked model, DrELM, to learn deep representations via extreme learning machine according to stacked generalization philosophy. The proposed model utilizes ELM as a base building block and incorporates random shift and kernelization as stacking elements. Specifically, in each layer. DrELM integrates a random projection of the predictions obtained by ELM into the original feature, and then applies kernel functions to generate the resultant feature. To verify the classification and regression performance of DrELM, we conduct the experiments on both synthetic and real-world data sets. The experimental results show that DrELM outperforms ELM and kernel ELMs, which appear to demonstrate that DrELM could yield predictive features that are suitable for prediction tasks. The performances of the deep models (i.e. Stacked Auto-encoder) are comparable. However, due to the utilization of ELM, DrELM is easier to learn and faster in testing. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Learning deep representations via extreme learning machines", "paper_id": "WOS:000360028800037"}