{"auto_keywords": [{"score": 0.04314658848168205, "phrase": "hidden_markov_models"}, {"score": 0.0400336914832026, "phrase": "conditional_relative_entropy"}, {"score": 0.00481495049065317, "phrase": "finite-alphabet_hidden_markov_models"}, {"score": 0.0037392451233558234, "phrase": "conditional_probability_measures"}, {"score": 0.0031383410838018984, "phrase": "realized_observation_sequence"}, {"score": 0.0029602552900111407, "phrase": "measure_change_technique"}, {"score": 0.0025085435314540837, "phrase": "conditional_expectations"}, {"score": 0.0021049977753042253, "phrase": "information_state_approach"}], "paper_keywords": ["a posteriori probability distances", " conditional relative entropy", " finite-alphabet hidden Markov models", " information state", " measure change"], "paper_abstract": "In this correspondence, we consider a probability distance problem for a class of hidden Markov models (HMMs). The notion of conditional relative entropy between conditional probability measures is introduced as an a posteriori probability distance which can be used to measure the discrepancy between hidden Markov models when a realized observation sequence is observed. Using a measure change technique, we derive a representation for conditional relative entropy in terms of the parameters of the HMMs and conditional expectations given measurements. With this representation, we show that this distance can be calculated using an information state approach.", "paper_title": "A posteriori probability distances between finite-alphabet hidden Markov models", "paper_id": "WOS:000243953400023"}