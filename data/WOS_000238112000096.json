{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "elm"}, {"score": 0.004667149874736771, "phrase": "multi-variable_functions"}, {"score": 0.004559270778306828, "phrase": "new_off-line_learning_algorithm"}, {"score": 0.004488734023485018, "phrase": "single_layer_feed-forward_neural_networks"}, {"score": 0.004317109072311102, "phrase": "extreme_learning_machine"}, {"score": 0.004087756756529841, "phrase": "huang_et_al"}, {"score": 0.0036080776793146843, "phrase": "traditional_bp_method"}, {"score": 0.0034700069041302003, "phrase": "good_generalization_performance"}, {"score": 0.003389704997952315, "phrase": "extremely_fast_learning_speed"}, {"score": 0.003234615195382533, "phrase": "hidden_neuron_parameters"}, {"score": 0.002991710982939703, "phrase": "rbf"}, {"score": 0.0026609940110902666, "phrase": "non-optimized_parameters"}, {"score": 0.002519402443228795, "phrase": "global_minimum"}, {"score": 0.0022760856906838814, "phrase": "optimized_value"}, {"score": 0.0022407927161620855, "phrase": "input_weights"}, {"score": 0.002206045784326466, "phrase": "gradient-based_algorithm"}, {"score": 0.0021049977753042253, "phrase": "activation_function"}], "paper_keywords": [""], "paper_abstract": "A new off-line learning algorithm for single layer feed-forward neural networks (SLFNs) called Extreme Learning Machine (ELM) was introduced by Huang et al. [1, 2 3:4]. ELM is not as the same as traditional BP method as it can achieve good generalization performance at an extremely fast learning speed. In ELM, the hidden neuron parameters (the input weights and hidden biases or the RBF centers and impact factors) were pre-determined randomly so a set of non-optimized parameters might avoid ELM to achieve the global minimum in some applications. This paper tries to find a set of optimized value of input weights using gradient-based algorithm in training SLFN where the activation function is differentiable.", "paper_title": "A gradient-based ELM algorithm in regressing multi-variable functions", "paper_id": "WOS:000238112000096"}