{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "universum_data"}, {"score": 0.01050200727147549, "phrase": "adaboost"}, {"score": 0.008210071736837, "phrase": "uadaboost"}, {"score": 0.00471465954562773, "phrase": "gradient_descent"}, {"score": 0.004379737380744606, "phrase": "training_data"}, {"score": 0.0042212686864468805, "phrase": "better_classifiers"}, {"score": 0.003839510177926892, "phrase": "classification_performance"}, {"score": 0.0035477930970463432, "phrase": "labeled_data"}, {"score": 0.0034555479218891638, "phrase": "cost_function"}, {"score": 0.0032437952431945724, "phrase": "training_stage"}, {"score": 0.0030935140058168885, "phrase": "standard_adaboost_weights"}, {"score": 0.003028969851366049, "phrase": "training_iterations"}, {"score": 0.0029501745152877732, "phrase": "explicit_weighting_scheme"}, {"score": 0.00291923164591297, "phrase": "universum_samples"}, {"score": 0.0027547408718334603, "phrase": "practical_conditions"}, {"score": 0.0026830601040919166, "phrase": "universum_learning"}, {"score": 0.0025052769325080255, "phrase": "ensemble_predictions"}, {"score": 0.0024789887640568093, "phrase": "training_samples"}, {"score": 0.0024272350606639147, "phrase": "handwritten_digits_classification"}, {"score": 0.0024017638230939514, "phrase": "gender_classification_problems"}, {"score": 0.0022783523466965187, "phrase": "proposed_method"}, {"score": 0.002242577809454225, "phrase": "superior_performances"}, {"score": 0.002207363758847308, "phrase": "standard_adaboost"}, {"score": 0.002172701452347454, "phrase": "proper_universum_data"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Ada Boost", " Gradient boost", " uAdaBoost", " Universum", " u-SVM"], "paper_abstract": "Recently, Universum data that does not belong to any class of the training data, has been applied for training better classifiers. In this paper, we address a novel boosting algorithm called AdaBoost that can improve the classification performance of AdaBoost with Universum data. uAdaBoost chooses a function by minimizing the loss for labeled data and Universum data. The cost function is minimized by a greedy, stagewise, functional gradient procedure. Each training stage of uAdaBoost is fast and efficient. The standard AdaBoost weights labeled samples during training iterations while uAdaBoost gives an explicit weighting scheme for Universum samples as well. In addition, this paper describes the practical conditions for the effectiveness of Universum learning. These conditions are based on the analysis of the distribution of ensemble predictions over training samples. Experiments on handwritten digits classification and gender classification problems are presented. As exhibited by our experimental results, the proposed method can obtain superior performances over the standard AdaBoost by selecting proper Universum data. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Exploiting Universum data in AdaBoost using gradient descent", "paper_id": "WOS:000339039600009"}