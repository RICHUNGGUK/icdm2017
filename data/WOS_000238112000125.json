{"auto_keywords": [{"score": 0.041269222832803924, "phrase": "activation_function"}, {"score": 0.00481495049065317, "phrase": "different_mlp_activation_functions"}, {"score": 0.004633429997452688, "phrase": "multilayer_perceptrons"}, {"score": 0.004545301285682967, "phrase": "mlp"}, {"score": 0.003714368305523811, "phrase": "mlp_power"}, {"score": 0.003643607569483598, "phrase": "careful_selection"}, {"score": 0.0034392837410790293, "phrase": "huge_impact"}, {"score": 0.00334144503360907, "phrase": "network_performance"}, {"score": 0.003154012038613403, "phrase": "quantitative_comparison"}, {"score": 0.002948547333717581, "phrase": "gaussian_rbf"}, {"score": 0.002837188425640173, "phrase": "ten_real_different_datasets"}, {"score": 0.002677964894127131, "phrase": "sigmoid_activation_function"}, {"score": 0.0021049977753042253, "phrase": "rbf_networks"}], "paper_keywords": [""], "paper_abstract": "Multilayer perceptrons (MLP) has been proven to be very successful in many applications including classification. The activation function is the source of the MLP power. Careful selection of the activation function has a huge impact on the network performance. This paper gives a quantitative comparison of the four most commonly used activation functions, including the Gaussian RBF network, over ten real different datasets. Results show that the sigmoid activation function substantially outperforms the other activation functions. Also, using only the needed number of hidden units in the MLP, we improved its conversion time to be competitive with the RBF networks most of the time.", "paper_title": "A quantitative comparison of different MLP activation functions in classification", "paper_id": "WOS:000238112000125"}