{"auto_keywords": [{"score": 0.03850253843948161, "phrase": "enrbf"}, {"score": 0.03363896917055258, "phrase": "enrbf_model"}, {"score": 0.00481495049065317, "phrase": "extended_nrbf_regression_model"}, {"score": 0.004692227830462433, "phrase": "classification_problem"}, {"score": 0.004525629293824217, "phrase": "traditional_normalized_radial_basis_function"}, {"score": 0.004479120557712926, "phrase": "nrbf"}, {"score": 0.004102528738941734, "phrase": "bayesian_ying-yang"}, {"score": 0.0036996627006622975, "phrase": "supplementary_study"}, {"score": 0.003336225354214798, "phrase": "learning_accuracies"}, {"score": 0.0030872017736888113, "phrase": "regression_and_function_approximation_problems"}, {"score": 0.0028127163745564777, "phrase": "classification_problems"}, {"score": 0.002740885774408238, "phrase": "new_proposed_enrbf_classifier"}, {"score": 0.0026161698049580804, "phrase": "special_cases"}, {"score": 0.0025100713447593773, "phrase": "xu_et_al"}, {"score": 0.0023346234464901978, "phrase": "neural_information_processing_systems"}, {"score": 0.002310578855290687, "phrase": "mit_press"}, {"score": 0.0022868112984241328, "phrase": "cambridge"}, {"score": 0.0022053994402287925, "phrase": "experimental_results"}, {"score": 0.0021490449563443025, "phrase": "enrbfc"}], "paper_keywords": ["Radial basis function", " Expectation maximization", " Gaussian mixture model", " Regression", " Classification"], "paper_abstract": "As an extension of the traditional normalized radial basis function (NRBF) model, the extended normalized RBF (ENRBF) model was proposed by Xu [RBF nets, mixture experts, and Bayesian Ying-Yang learning, Neurocomputing 19 (1998) 223-257]. In this paper, we perform a supplementary study on ENRBF with several properly designed experiments and some further theoretical discussions. It is shown that ENRBF is able to efficiently improve the learning accuracies under some circumstances. Moreover, since the ENRBF model is initially proposed for the regression and function approximation problems, a further step is taken in this work to modify the ENRBF model to deal with the classification problems. Both the original ENRBF model and the new proposed ENRBF classifier (ENRBFC) can be viewed as the special cases of the mixture-of-experts (ME) model that is discussed in Xu et al. [An alternative model for mixtures of experts, in: Advances in Neural Information Processing Systems, MIT Press, Cambridge, MA, 1995]. Experimental results show the potentials of ENRBFC compared to some other related classifiers. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "An experimental study of the extended NRBF regression model and its enhancement for classification problem", "paper_id": "WOS:000261643700051"}