{"auto_keywords": [{"score": 0.03525836607351927, "phrase": "ensemble_pruning_techniques"}, {"score": 0.027415105999821002, "phrase": "static_and_dynamic_pruning_techniques"}, {"score": 0.00481495049065317, "phrase": "double_pruning_scheme_for_boosting_ensembles"}, {"score": 0.004431041346291019, "phrase": "single_unified_decision"}, {"score": 0.004349955692802962, "phrase": "complementary_classifiers"}, {"score": 0.004309968592440261, "phrase": "accurate_and_robust_predictions"}, {"score": 0.004096493540086106, "phrase": "individual_classifiers"}, {"score": 0.0037006243674469657, "phrase": "final_ensemble_prediction"}, {"score": 0.003517223847512266, "phrase": "potential_queries"}, {"score": 0.0034528025626422154, "phrase": "larger_storage_requirements"}, {"score": 0.0034210342328868017, "phrase": "slower_predictions"}, {"score": 0.0033739270918502285, "phrase": "single_classifier"}, {"score": 0.0032215231177304513, "phrase": "static_pruning_techniques"}, {"score": 0.003177154457204778, "phrase": "ensemble_size"}, {"score": 0.0030476702431093687, "phrase": "original_ensemble"}, {"score": 0.0030056887638095883, "phrase": "dynamic_pruning"}, {"score": 0.0029642838574678526, "phrase": "querying_process"}, {"score": 0.002896535621940078, "phrase": "partial_ensemble_prediction"}, {"score": 0.0028172726995962173, "phrase": "stable_final_decision"}, {"score": 0.0027784560880013886, "phrase": "reasonable_amount"}, {"score": 0.0026042472337367015, "phrase": "comprehensive_analysis"}, {"score": 0.002544736501754668, "phrase": "adaboost"}, {"score": 0.0024409345168575833, "phrase": "wide_range"}, {"score": 0.002418453525649304, "phrase": "classification_problems"}, {"score": 0.0022149235007885826, "phrase": "memory_requirements"}, {"score": 0.002154272413276506, "phrase": "classification_time"}, {"score": 0.0021245712108461227, "phrase": "significant_loss"}, {"score": 0.0021049977753042253, "phrase": "prediction_accuracy"}], "paper_keywords": ["Adaboost", " double pruning", " ensemble pruning", " instance-based pruning", " semi-definite programming"], "paper_abstract": "Ensemble learning consists of generating a collection of classifiers whose predictions are then combined to yield a single unified decision. Ensembles of complementary classifiers provide accurate and robust predictions, which are often better than the predictions of the individual classifiers in the ensemble. Nevertheless, ensembles also have some drawbacks: typically, all classifiers are queried to compute the final ensemble prediction. Therefore, all the classifiers need to be accessible to address potential queries. This entails larger storage requirements and slower predictions than a single classifier. Ensemble pruning techniques are useful to alleviate these drawbacks. Static pruning techniques reduce the ensemble size by selecting a sub-ensemble of classifiers from the original ensemble. In dynamic pruning, the querying process is halted when the partial ensemble prediction is sufficient to reach a stable final decision with a reasonable amount of confidence. In this paper, we present the results of a comprehensive analysis of static and dynamic pruning techniques applied to Adaboost ensembles. These ensemble pruning techniques are evaluated on a wide range of classification problems. From this analysis, one concludes that the combination of static and dynamic pruning techniques provides a notable reduction in the memory requirements and an improvement in the classification time without a significant loss of prediction accuracy.", "paper_title": "A Double Pruning Scheme for Boosting Ensembles", "paper_id": "WOS:000345629000036"}