{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "latent_common_space"}, {"score": 0.007663121140016606, "phrase": "relatively-paired_observations"}, {"score": 0.007251112930560787, "phrase": "cross-modality_pattern_recognition"}, {"score": 0.004624751696958501, "phrase": "different_modalities"}, {"score": 0.004555371402128666, "phrase": "important_role"}, {"score": 0.0043753813457279404, "phrase": "absolutely-paired_observations"}, {"score": 0.004118587636081584, "phrase": "cross-modality_observations"}, {"score": 0.0038379074449947067, "phrase": "general_framework"}, {"score": 0.003693058821567136, "phrase": "proposed_framework"}, {"score": 0.0032989853590079153, "phrase": "relative_proximities"}, {"score": 0.0031366648074858555, "phrase": "discriminative_model"}, {"score": 0.003073988343893906, "phrase": "distance_margin"}, {"score": 0.002628747284714371, "phrase": "learning_procedure"}, {"score": 0.0026023381988430666, "phrase": "large_scale_training_data"}, {"score": 0.002486734224234698, "phrase": "structural_model"}, {"score": 0.002388283252319309, "phrase": "cutting_plane_algorithm"}, {"score": 0.0021479597698614373, "phrase": "face_recognition"}, {"score": 0.0021263705039915198, "phrase": "text-image_retrieval"}, {"score": 0.0021049977753042253, "phrase": "attribute-image_retrieval"}], "paper_keywords": ["Multi-modality analysis", " Multi-viewanalysis", " Absolutely-paired observations", " Relatively-paired observations", " Structural learning", " Cutting plane algorithm"], "paper_abstract": "Discovering a latent common space between different modalities plays an important role in cross-modality pattern recognition. Existing techniques often require absolutely-paired observations as training data, and are incapable of capturing more general semantic relationships between cross-modality observations. This greatly limits their applications. In this paper, we propose a general framework for learning a latent common space from relatively-paired observations (i.e., two observations from different modalities are more-likely-paired than another two). Relative-pairing information is encoded using relative proximities of observations in the latent common space. By building a discriminative model and maximizing a distance margin, a projection function that maps observations into the latent common space is learned for each modality. Cross-modality pattern recognition can then be carried out in the latent common space. To speed up the learning procedure for large scale training data, the problem is reformulated into learning a structural model, which is efficiently solved by the cutting plane algorithm. To evaluate the performance of the proposed framework, it has been applied to feature fusion, cross-pose face recognition, text-image retrieval and attribute-image retrieval. Experimental results demonstrate that the proposed framework outperforms other state-of-the-art approaches.", "paper_title": "Relatively-Paired Space Analysis: Learning a Latent Common Space From Relatively-Paired Observations", "paper_id": "WOS:000356091900003"}