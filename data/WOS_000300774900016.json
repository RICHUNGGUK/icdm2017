{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "cuda-zero"}, {"score": 0.004677711903652145, "phrase": "memory_gpu_applications"}, {"score": 0.004525629293824217, "phrase": "general_purpose_computations"}, {"score": 0.004488725449273653, "phrase": "gpu"}, {"score": 0.00443308765153716, "phrase": "memory_programming_models"}, {"score": 0.00428892403807229, "phrase": "gpu_programming"}, {"score": 0.00418387341007444, "phrase": "demanding_needs"}, {"score": 0.004031082737449308, "phrase": "gpu_programs"}, {"score": 0.0036653985479652854, "phrase": "mixed_programming_models"}, {"score": 0.0036202721696130236, "phrase": "cuda"}, {"score": 0.0034165310204389682, "phrase": "workload_distribution"}, {"score": 0.0033465923623290034, "phrase": "scheduling_mechanisms"}, {"score": 0.002968192753809507, "phrase": "gpu_program"}, {"score": 0.002931569771634119, "phrase": "shared_memory_model"}, {"score": 0.0028715299874549245, "phrase": "access_patterns"}, {"score": 0.0028243822059448266, "phrase": "kernel_functions"}, {"score": 0.002778006389296707, "phrase": "data_partition_schemes"}, {"score": 0.0027211030647461324, "phrase": "access_pattern"}, {"score": 0.0023834640793589435, "phrase": "zero_efforts"}, {"score": 0.0023249757987087055, "phrase": "existing_application"}, {"score": 0.002305799666781492, "phrase": "distributed_memory_environment"}, {"score": 0.002122507495446441, "phrase": "efficient_parallelization"}, {"score": 0.0021049977753042253, "phrase": "multi-gpu_environment"}], "paper_keywords": ["CUDA", " parallelization", " data access pattern", " multi-GPU"], "paper_abstract": "As the prevalence of general purpose computations on GPU, shared memory programming models were proposed to ease the pain of GPU programming. However, with the demanding needs of more intensive workloads, it's desirable to port GPU programs to more scalable distributed memory environment, such as multi-GPUs. To achieve this, programs need to be re-written with mixed programming models (e.g. CUDA and message passing). Programmers not only need to work carefully on workload distribution, but also on scheduling mechanisms to ensure the efficiency of the execution. In this paper, we studied the possibilities of automating the process of parallelization to multi-GPUs. Starting from a GPU program written in shared memory model, our framework analyzes the access patterns of arrays in kernel functions to derive the data partition schemes. To acquire the access pattern, we proposed a 3-tiers approach: static analysis, profile based analysis and user annotation. Experiments show that most access patterns can be derived correctly by the first two tiers, which means that zero efforts are needed to port an existing application to distributed memory environment. We use our framework to parallelize several applications, and show that for certain kinds of applications, CUDA-Zero can achieve efficient parallelization in multi-GPU environment.", "paper_title": "CUDA-Zero: a framework for porting shared memory GPU applications to multi-GPUs", "paper_id": "WOS:000300774900016"}