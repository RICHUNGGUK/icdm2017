{"auto_keywords": [{"score": 0.031315543324394984, "phrase": "vms"}, {"score": 0.00481495049065317, "phrase": "large-scale_datasets"}, {"score": 0.0045843245695654875, "phrase": "data_distribution"}, {"score": 0.004546970518808189, "phrase": "public_clouds"}, {"score": 0.0044005599565517875, "phrase": "virtually_any_institution"}, {"score": 0.0043114466990006334, "phrase": "virtual_machines"}, {"score": 0.0034990853199511982, "phrase": "elapsed_time"}, {"score": 0.003107186371289819, "phrase": "big_data_provisioning_service"}, {"score": 0.003069230909006614, "phrase": "hierarchical_and_peer-to-peer_data_distribution_techniques"}, {"score": 0.002921975748248106, "phrase": "data_processing"}, {"score": 0.0027477745788313163, "phrase": "speed-up_data_loading"}, {"score": 0.002541910967437926, "phrase": "current_state"}, {"score": 0.002510843167476253, "phrase": "art_techniques"}, {"score": 0.002480154140902474, "phrase": "dynamic_topology_mechanism"}, {"score": 0.0024298350266697905, "phrase": "classic_declarative_machine_configuration_techniques"}, {"score": 0.002370794661629904, "phrase": "single_high-level_declarative_configuration_file"}, {"score": 0.0023131855385489764, "phrase": "data_loading"}, {"score": 0.0022111720236287547, "phrase": "big_data"}, {"score": 0.0021662983565317283, "phrase": "end_users"}, {"score": 0.0021049977753042253, "phrase": "infrastructure_management"}], "paper_keywords": ["Large-scale data transfer", " flash crowd", " big data", " BitTorrent", " p2p overlay", " provisioning", " big data distribution"], "paper_abstract": "Public clouds have democratised the access to analytics for virtually any institution in the world. Virtual machines (VMs) can be provisioned on demand to crunch data after uploading into the VMs. While this task is trivial for a few tens of VMs, it becomes increasingly complex and time consuming when the scale grows to hundreds or thousands of VMs crunching tens or hundreds of TB. Moreover, the elapsed time comes at a price: the cost of provisioning VMs in the cloud and keeping them waiting to load the data. In this paper we present a big data provisioning service that incorporates hierarchical and peer-to-peer data distribution techniques to speed-up data loading into the VMs used for data processing. The system dynamically mutates the sources of the data for the VMs to speed-up data loading. We tested this solution with 1000 VMs and 100 TB of data, reducing time by at least 30 percent over current state of the art techniques. This dynamic topology mechanism is tightly coupled with classic declarative machine configuration techniques (the system takes a single high-level declarative configuration file and configures both software and data loading). Together, these two techniques simplify the deployment of big data in the cloud for end users who may not be experts in infrastructure management.", "paper_title": "Deploying Large-Scale Datasets on-Demand in the Cloud: Treats and Tricks on Data Distribution", "paper_id": "WOS:000366167500005"}