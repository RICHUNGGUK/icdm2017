{"auto_keywords": [{"score": 0.004246947739596565, "phrase": "linear_programming_problem"}, {"score": 0.0037066895459425824, "phrase": "hinge_loss_function"}, {"score": 0.003234934827553253, "phrase": "exact_support_vector"}, {"score": 0.0030378759123133644, "phrase": "gram_matrix_rank_bounds"}, {"score": 0.0029438815270548175, "phrase": "esv"}, {"score": 0.0027356847952125433, "phrase": "sparser_representation_model"}, {"score": 0.0026233771102489394, "phrase": "kernel_gram_matrix_rank"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Support vector novelty detection", " 1-norm regularization", " Linear programming"], "paper_abstract": "This paper proposes a 1-norm support vector novelty detection (SVND) method and discusses its sparseness. 1-norm SVND is formulated as a linear programming problem and uses two techniques for inducing sparseness, or the 1-norm regularization and the hinge loss function. We also find two upper bounds on the sparseness of 1-norm SVND, or exact support vector (ESV) and kernel Gram matrix rank bounds. The ESV bound indicates that 1-norm SVND has a sparser representation model than SVND. The kernel Gram matrix rank bound can loosely estimate the sparseness of 1-norm SVND. Experimental results show that 1-norm SVND is feasible and effective. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "1-norm support vector novelty detection and its sparseness", "paper_id": "WOS:000328923800013"}