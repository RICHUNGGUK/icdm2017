{"auto_keywords": [{"score": 0.04512022970606561, "phrase": "cl"}, {"score": 0.014449792422175214, "phrase": "aoe"}, {"score": 0.00481495049065317, "phrase": "cooperative_q-learning"}, {"score": 0.0046432244185370605, "phrase": "multiagent_system"}, {"score": 0.004437115547184535, "phrase": "extra_resources"}, {"score": 0.00438370856272802, "phrase": "higher_efficiency"}, {"score": 0.004240116703465252, "phrase": "individual_learning"}, {"score": 0.004151188559520802, "phrase": "real_world"}, {"score": 0.0040273609512482, "phrase": "straightforward_task"}, {"score": 0.003954837422946628, "phrase": "possible_differences"}, {"score": 0.003802135020791801, "phrase": "reinforcement-learning_homogenous_agents"}, {"score": 0.0037223583464229097, "phrase": "multiple_goals"}, {"score": 0.0035894592199604246, "phrase": "different_domains"}, {"score": 0.0035677736570659813, "phrase": "different_amounts"}, {"score": 0.0034823290869920804, "phrase": "one-step_q-learning_algorithm"}, {"score": 0.002947611960613497, "phrase": "state_transitions"}, {"score": 0.0029209233709517634, "phrase": "gold_standard"}, {"score": 0.0028944757265206332, "phrase": "behavioral_point"}, {"score": 0.0027490536097875685, "phrase": "agents'_expertness"}, {"score": 0.002724157916806019, "phrase": "state_level"}, {"score": 0.002603012037901889, "phrase": "overall_performance"}, {"score": 0.0025560719383290356, "phrase": "developed_methods"}, {"score": 0.0024722390667554967, "phrase": "ss"}, {"score": 0.0023983322584501332, "phrase": "obtained_results"}, {"score": 0.002376605162340897, "phrase": "superior_performance"}, {"score": 0.0023622296334998324, "phrase": "aoe-based_methods"}, {"score": 0.0023196214633527264, "phrase": "existing_cl_methods"}, {"score": 0.0021049977753042253, "phrase": "general_cl_methods"}], "paper_keywords": ["area of expertise (AOE)", " cooperative Q-learning agents", " cooperative Q-learning using AOE", " extraction of AOE", " multiagent systems (MASs)"], "paper_abstract": "Cooperation in learning (CL) can be realized in a multiagent system, if agents are capable of learning from both their own experiments and other agents' knowledge and expertise. Extra resources are exploited into higher efficiency and faster learning in CL as compared to that of individual learning (IL). In the real world, however, implementation of CL is not a straightforward task, in part due to possible differences in area of expertise (AOE). In this paper, reinforcement-learning homogenous agents are considered in an environment with multiple goals or tasks. As a result, they become expert in different domains with different amounts of expertness. Each agent uses a one-step Q-learning algorithm and is capable of exchanging its Q-table with those of its teammates. Two crucial questions are addressed in this paper: \"How the AOE of an agent can be extracted?\" and \"How agents can improve their performance in CL by knowing their AOEs?\" An algorithm is developed to extract the AOE based on state transitions as a gold standard from a behavioral point of view. Moreover, it is discussed that the AOE can be implicitly obtained through agents' expertness in the state level. Three new methods for CL through the combination of Q-tables are developed and examined for overall performance after CL. The performances of developed methods are compared with that of IL, strategy sharing (SS), and weighted SS (WSS). Obtained results show the superior performance of AOE-based methods as compared to that of existing CL methods, which do not use the notion of AOE. These results are very encouraging in support of the idea that \"cooperation based on the AOE\" performs better than the general CL methods.", "paper_title": "A study on expertise of agents and its effects on cooperative Q-learning", "paper_id": "WOS:000245109300014"}