{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "local_learning-based_clustering"}, {"score": 0.004398094237005956, "phrase": "input_space"}, {"score": 0.004324031677173549, "phrase": "hilbert_space"}, {"score": 0.004275347843870176, "phrase": "kernel_methods"}, {"score": 0.004086010164600727, "phrase": "appropriate_data_representation"}, {"score": 0.004039995317682942, "phrase": "feature_selection"}, {"score": 0.00399449659126893, "phrase": "kernel_learning"}, {"score": 0.003546513036085932, "phrase": "high-dimensional_data"}, {"score": 0.003148612096891635, "phrase": "built-in_regularization"}, {"score": 0.0030955250132409964, "phrase": "llc_algorithm"}, {"score": 0.0026864540069792275, "phrase": "clustering_process"}, {"score": 0.0025965853546625352, "phrase": "resulting_weighted_regularization"}, {"score": 0.002552781867125423, "phrase": "additional_constraint"}, {"score": 0.0024395426929046415, "phrase": "known_sparse-promoting_penalty"}, {"score": 0.0023313149689465386, "phrase": "irrelevant_features"}, {"score": 0.002215274527909304, "phrase": "extensive_experiments"}, {"score": 0.002141134176057931, "phrase": "proposed_methods"}, {"score": 0.0021049977753042253, "phrase": "benchmark_data_sets"}], "paper_keywords": ["High-dimensional data", " local learning-based clustering", " feature selection", " kernel learning", " sparse weighting"], "paper_abstract": "The performance of the most clustering algorithms highly relies on the representation of data in the input space or the Hilbert space of kernel methods. This paper is to obtain an appropriate data representation through feature selection or kernel learning within the framework of the Local Learning-Based Clustering (LLC) (Wu and Scholkopf 2006) method, which can outperform the global learning-based ones when dealing with the high-dimensional data lying on manifold. Specifically, we associate a weight to each feature or kernel and incorporate it into the built-in regularization of the LLC algorithm to take into account the relevance of each feature or kernel for the clustering. Accordingly, the weights are estimated iteratively in the clustering process. We show that the resulting weighted regularization with an additional constraint on the weights is equivalent to a known sparse-promoting penalty. Hence, the weights of those irrelevant features or kernels can be shrunk toward zero. Extensive experiments show the efficacy of the proposed methods on the benchmark data sets.", "paper_title": "Feature Selection and Kernel Learning for Local Learning-Based Clustering", "paper_id": "WOS:000291807200004"}