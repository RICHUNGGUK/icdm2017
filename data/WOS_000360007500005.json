{"auto_keywords": [{"score": 0.04407253113473033, "phrase": "online_advertising_systems"}, {"score": 0.00481495049065317, "phrase": "industrial_applications"}, {"score": 0.004765788394931875, "phrase": "latent_dirichlet_allocation"}, {"score": 0.004717612728398697, "phrase": "lda"}, {"score": 0.004621279527317074, "phrase": "popular_topic"}, {"score": 0.004322967486451877, "phrase": "large-scale_applications"}, {"score": 0.0041490029730869345, "phrase": "main_underlying_reason"}, {"score": 0.004064653055162238, "phrase": "topic_models"}, {"score": 0.0037057255600764475, "phrase": "largest_lda_models"}, {"score": 0.0034308688638878286, "phrase": "long-tail_semantic_word_sets"}, {"score": 0.003176333519010176, "phrase": "key_factor"}, {"score": 0.003048370487544551, "phrase": "topic-modeling_systems"}, {"score": 0.002910545717767559, "phrase": "\"big\"_lda_model"}, {"score": 0.0027363971333961967, "phrase": "significant_improvement"}, {"score": 0.002708399936381754, "phrase": "industrial_search_engine"}, {"score": 0.00250732953434891, "phrase": "novel_distributed_system"}, {"score": 0.002443671484766342, "phrase": "big_lda_models"}, {"score": 0.0024186620196958867, "phrase": "big_data"}, {"score": 0.0023816257768353344, "phrase": "main_features"}, {"score": 0.0023331227492950422, "phrase": "hierarchical_distributed_architecture"}, {"score": 0.002309242063033298, "phrase": "real-time_prediction"}, {"score": 0.0021821915946356168, "phrase": "peacock_system"}, {"score": 0.0021267708018655493, "phrase": "significant_benefits"}, {"score": 0.0021049977753042253, "phrase": "highly_scalable_lda_topic_models"}], "paper_keywords": ["Algorithms", " Experimentation", " Performance", " Latent Dirichlet allocation", " big topic models", " big data", " long-tail topic features", " search engine", " online advertising systems"], "paper_abstract": "Latent Dirichlet allocation (LDA) is a popular topic modeling technique in academia but less so in industry, especially in large-scale applications involving search engine and online advertising systems. A main underlying reason is that the topic models used have been too small in scale to be useful; for example, some of the largest LDA models reported in literature have up to 10(3) topics, which difficultly cover the long-tail semantic word sets. In this article, we show that the number of topics is a key factor that can significantly boost the utility of topic-modeling systems. In particular, we show that a \"big\" LDA model with at least 10(5) topics inferred from 10(9) search queries can achieve a significant improvement on industrial search engine and online advertising systems, both of which serve hundreds of millions of users. We develop a novel distributed system called Peacock to learn big LDA models from big data. The main features of Peacock include hierarchical distributed architecture, real-time prediction, and topic de-duplication. We empirically demonstrate that the Peacock system is capable of providing significant benefits via highly scalable LDA topic models for several industrial applications.", "paper_title": "Peacock: Learning Long-Tail Topic Features for Industrial Applications", "paper_id": "WOS:000360007500005"}