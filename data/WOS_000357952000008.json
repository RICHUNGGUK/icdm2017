{"auto_keywords": [{"score": 0.040454428776653985, "phrase": "multiple_sms"}, {"score": 0.03186922105859791, "phrase": "buddy_sm."}, {"score": 0.030660592326805094, "phrase": "buddy_cluster"}, {"score": 0.00481495049065317, "phrase": "buddy_sm"}, {"score": 0.004757917303420435, "phrase": "pipeline_front-end"}, {"score": 0.004720269202737973, "phrase": "improved_energy_efficiency"}, {"score": 0.004627441072541506, "phrase": "modern_general-purpose_graphics_processing_unit"}, {"score": 0.004590841774186121, "phrase": "gpgpu"}, {"score": 0.00450052670755585, "phrase": "multiple_streaming_multiprocessors"}, {"score": 0.004206548500039536, "phrase": "common_instruction_flow"}, {"score": 0.004156764641404464, "phrase": "sms"}, {"score": 0.0037336520904376687, "phrase": "lock-step_manner"}, {"score": 0.003573859492987647, "phrase": "front-end_units"}, {"score": 0.0033937776228549557, "phrase": "schedule_components"}, {"score": 0.003313741359811683, "phrase": "architectural_challenges"}, {"score": 0.0032614317990293695, "phrase": "performance_degradation"}, {"score": 0.0025180781216911246, "phrase": "front-end_logics"}, {"score": 0.002429451930463481, "phrase": "efficient_flow_control"}, {"score": 0.0024101837218440834, "phrase": "program_correctness"}, {"score": 0.002381566880282456, "phrase": "proposed_architecture"}, {"score": 0.0023532890136269986, "phrase": "unfavorable_conditions"}, {"score": 0.002208035721125305, "phrase": "energy_efficiency"}, {"score": 0.002181813794658713, "phrase": "detailed_experiments"}, {"score": 0.0021049977753042253, "phrase": "total_gpu_energy_reduction"}], "paper_keywords": ["Design", " Experimentation", " Performance", " GPU", " front-end", " energy efficiency"], "paper_abstract": "A modern general-purpose graphics processing unit (GPGPU) usually consists of multiple streaming multiprocessors (SMs), each having a pipeline that incorporates a group of threads executing a common instruction flow. Although SMs are designed to work independently, we observe that they tend to exhibit very similar behavior for many workloads. If multiple SMs can be grouped and work in the lock-step manner, it is possible to save energy by sharing the front-end units among multiple SMs, including the instruction fetch, decode, and schedule components. However, such sharing brings architectural challenges and sometime causes performance degradation. In this article, we show our design, implementation, and evaluation for such an architecture, which we call Buddy SM. Specifically, multiple SMs can be opportunistically grouped into a buddy cluster. One SM becomes the master, and the rest become the slaves. The front-end unit of the master works actively for itself as well as for the slaves, whereas the front-end logics of the slaves are power gated. For efficient flow control and program correctness, the proposed architecture can identify unfavorable conditions and ungroup the buddy cluster when necessary. We analyze various techniques to improve the performance and energy efficiency of Buddy SM. Detailed experiments manifest that 37.2% front-end and 7.5% total GPU energy reduction can be achieved.", "paper_title": "Buddy SM: Sharing Pipeline Front-End for Improved Energy Efficiency in GPGPUs", "paper_id": "WOS:000357952000008"}