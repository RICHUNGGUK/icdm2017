{"auto_keywords": [{"score": 0.04017990702934365, "phrase": "unlabeled_data"}, {"score": 0.00481495049065317, "phrase": "human_motions"}, {"score": 0.0046937237797352515, "phrase": "inertial_data"}, {"score": 0.004529093447865156, "phrase": "human_motion"}, {"score": 0.004392565073466714, "phrase": "automation_and_human_computer_interaction"}, {"score": 0.00411064950774695, "phrase": "arbitrary_human_actions"}, {"score": 0.004068907083697369, "phrase": "body-worn_sensors"}, {"score": 0.003946195919158241, "phrase": "temporal_scaling"}, {"score": 0.0037498152007206815, "phrase": "appropriate_model_complexity"}, {"score": 0.0035997452281985465, "phrase": "severe_case"}, {"score": 0.00342054627009158, "phrase": "dynamic_time_alignment"}, {"score": 0.0033857878144822906, "phrase": "gaussian_mixture_model_clusters"}, {"score": 0.0032836104153151973, "phrase": "unsupervised_temporal_segmentation"}, {"score": 0.0031521392193570846, "phrase": "extensive_corpus"}, {"score": 0.003120099687922751, "phrase": "continuous_motion_sequences"}, {"score": 0.0030726480920918097, "phrase": "everyday_tasks"}, {"score": 0.003010496595090984, "phrase": "analysis_scenarios"}, {"score": 0.0029196116767956273, "phrase": "average_accuracy"}, {"score": 0.0027883885425383534, "phrase": "different_participants"}, {"score": 0.0027459678326877744, "phrase": "labeled_data"}, {"score": 0.002718045523483502, "phrase": "recognition_models"}, {"score": 0.002676692093594358, "phrase": "particular_classes"}, {"score": 0.0024414225109100672, "phrase": "modeling_process"}, {"score": 0.00235560370536971, "phrase": "benchmark_methods"}, {"score": 0.0022961534630650347, "phrase": "systematic_validation"}, {"score": 0.0022154296909169826, "phrase": "improved_performance"}, {"score": 0.0021928902836049384, "phrase": "mixture_model_prediction"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Human motion", " Classification", " Recognition", " Segmentation", " Inertial sensors", " Gaussian mixture model", " Minimum message length", " Dynamic time warping"], "paper_abstract": "Systems that recognize patterns in human motion are central to improvements in automation and human computer interaction. This work addresses challenges which arise in the context of recognizing arbitrary human actions from body-worn sensors. Chiefly the invariance to temporal scaling of events, coping with unlabeled data and estimating an appropriate model complexity. In order to deal with the severe case of unlabeled data, a method is proposed based on dynamic time alignment of Gaussian mixture model clusters for matching actions in an unsupervised temporal segmentation. In facilitation of this, an extensive corpus of continuous motion sequences composed of everyday tasks was recorded as analysis scenarios. The technique achieved an average accuracy of 72% for correctly merging actions performed by different participants. With labeled data and recognition models designed for particular classes, an accuracy of 89% was achieved in classifying the motion of participants left out of the modeling process. These results are contrasted with benchmark methods for recognition in a systematic validation revealing, in particular, an improved performance for mixture model prediction utilizing segments. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Recognizing human motions through mixture modeling of inertial data", "paper_id": "WOS:000354582700005"}