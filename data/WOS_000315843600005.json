{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "lstm-modeling"}, {"score": 0.004770461993586338, "phrase": "continuous_emotions"}, {"score": 0.004704495156558224, "phrase": "audiovisual_affect_recognition_framework"}, {"score": 0.004617949594931096, "phrase": "human_emotions"}, {"score": 0.004575272950516775, "phrase": "spontaneous_and_non-prototypical_real-life_data"}, {"score": 0.004307286039184164, "phrase": "affective_computing"}, {"score": 0.00415019999222481, "phrase": "dimensional_representations"}, {"score": 0.00383508177668219, "phrase": "audiovisual_human-computer_interaction_scenario"}, {"score": 0.003678023470672862, "phrase": "long-range_context_modeling"}, {"score": 0.00357689487273538, "phrase": "emotion_recognition"}, {"score": 0.0034947408949397127, "phrase": "fully_automatic_audiovisual_recognition_approach"}, {"score": 0.003446353878654566, "phrase": "long_short-term_memory"}, {"score": 0.0033515736929172644, "phrase": "word-level_audio_and_video_features"}, {"score": 0.003068242064611088, "phrase": "inferred_emotion_estimates"}, {"score": 0.0029699874949715367, "phrase": "optimal_amount"}, {"score": 0.0029152570900178956, "phrase": "extensive_evaluations"}, {"score": 0.002744234034162797, "phrase": "different_affective_dimensions"}, {"score": 0.0026811531172346676, "phrase": "semaine_database"}, {"score": 0.0025592969422491476, "phrase": "challenge_baseline_system"}, {"score": 0.0025355968118803956, "phrase": "visual_features"}, {"score": 0.0024772998651215964, "phrase": "novel_facial_movement_feature_extractor"}, {"score": 0.002397923568994474, "phrase": "recognition_scores"}, {"score": 0.0023646866829474798, "phrase": "audiovisual_sub-challenge_participants"}, {"score": 0.0022995854132747233, "phrase": "proposed_lstm-based_technique"}, {"score": 0.0022571810910697013, "phrase": "best_average_recognition_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Emotion recognition", " Long Short-Term Memory", " Facial movement features", " Context modeling"], "paper_abstract": "Automatically recognizing human emotions from spontaneous and non-prototypical real-life data is currently one of the most challenging tasks in the field of affective computing. This article presents our recent advances in assessing dimensional representations of emotion, such as arousal, expectation, power, and valence, in an audiovisual human-computer interaction scenario. Building on previous studies which demonstrate that long-range context modeling tends to increase accuracies of emotion recognition, we propose a fully automatic audiovisual recognition approach based on Long Short-Term Memory (LSTM) modeling of word-level audio and video features. LSTM networks are able to incorporate knowledge about how emotions typically evolve over time so that the inferred emotion estimates are produced under consideration of an optimal amount of context. Extensive evaluations on the Audiovisual Sub-Challenge of the 2011 Audio/Visual Emotion Challenge show how acoustic, linguistic, and visual features contribute to the recognition of different affective dimensions as annotated in the SEMAINE database. We apply the same acoustic features as used in the challenge baseline system whereas visual features are computed via a novel facial movement feature extractor. Comparing our results with the recognition scores of all Audiovisual Sub-Challenge participants, we find that the proposed LSTM-based technique leads to the best average recognition performance that has been reported for this task so far. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework", "paper_id": "WOS:000315843600005"}