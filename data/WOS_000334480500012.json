{"auto_keywords": [{"score": 0.03620728805533268, "phrase": "blstm"}, {"score": 0.00481495049065317, "phrase": "context-sensitive_bottleneck_neural_networks"}, {"score": 0.0046161999598320486, "phrase": "novel_context-sensitive_feature_extraction_approach"}, {"score": 0.004171891198117439, "phrase": "improved_phoneme_recognition_accuracies"}, {"score": 0.004067635935640407, "phrase": "long-range_contextual_information"}, {"score": 0.003802135020791801, "phrase": "blstm_principle"}, {"score": 0.0037070855410818986, "phrase": "tandem_front-end"}, {"score": 0.0036450381529042103, "phrase": "probabilistic_feature_extraction"}, {"score": 0.0035240305920962766, "phrase": "previously_proposed_approaches"}, {"score": 0.0032938942910948066, "phrase": "discrete_phoneme_prediction_feature"}, {"score": 0.003078740573398959, "phrase": "probabilistic_blstm"}, {"score": 0.0030017207229983385, "phrase": "low-level_features"}, {"score": 0.0029020077365708966, "phrase": "blstm_modeling"}, {"score": 0.002735392281320413, "phrase": "novel_front-end"}, {"score": 0.002600194073758015, "phrase": "context-sensitive_probabilistic_feature_vectors"}, {"score": 0.0024302416436559845, "phrase": "network_training_targets"}, {"score": 0.002195866991284673, "phrase": "recently_published_architectures"}, {"score": 0.00215905882697518, "phrase": "feature-level_context_modeling"}], "paper_keywords": ["Probabilistic feature extraction", " Bottleneck networks", " Long Short-Term Memory", " Bidirectional speech processing"], "paper_abstract": "We introduce a novel context-sensitive feature extraction approach for spontaneous speech recognition. As bidirectional Long Short-Term Memory (BLSTM) networks are known to enable improved phoneme recognition accuracies by incorporating long-range contextual information into speech decoding, we integrate the BLSTM principle into a Tandem front-end for probabilistic feature extraction. Unlike the previously proposed approaches which exploit BLSTM modeling by generating a discrete phoneme prediction feature, our feature extractor merges continuous high-level probabilistic BLSTM features with low-level features. By combining BLSTM modeling and Bottleneck (BN) feature generation, we propose a novel front-end that allows us to produce context-sensitive probabilistic feature vectors of arbitrary size, independent of the network training targets. Evaluations on challenging spontaneous, conversational speech recognition tasks show that this concept prevails over recently published architectures for feature-level context modeling. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Probabilistic speech feature extraction with context-sensitive Bottleneck neural networks", "paper_id": "WOS:000334480500012"}