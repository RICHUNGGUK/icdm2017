{"auto_keywords": [{"score": 0.03961155731026176, "phrase": "efficient_implementation"}, {"score": 0.03799635259867219, "phrase": "gaussian_parameter_matrix"}, {"score": 0.034111453133946666, "phrase": "recognition_accuracy"}, {"score": 0.03203453078231437, "phrase": "overall_speedup"}, {"score": 0.025238752463782085, "phrase": "matrix_multiplication"}, {"score": 0.00481495049065317, "phrase": "speech_recognition"}, {"score": 0.00478526758361257, "phrase": "matrices"}, {"score": 0.004746263721902201, "phrase": "acoustic_modeling"}, {"score": 0.00467855218490191, "phrase": "multivariate_gaussians"}, {"score": 0.004628400269774981, "phrase": "prevalent_approach"}, {"score": 0.004497262692458022, "phrase": "large_set"}, {"score": 0.004465237506197132, "phrase": "gaussians"}, {"score": 0.004276610897002087, "phrase": "computationally_dominant_phase"}, {"score": 0.004052148727852149, "phrase": "augmented_feature_vectors"}, {"score": 0.004023120753104693, "phrase": "gaussian_parameters"}, {"score": 0.00397996666534913, "phrase": "computational_gain"}, {"score": 0.003909065355459264, "phrase": "traditional_methods"}, {"score": 0.0036509280916606937, "phrase": "direct_low-rank_approximation"}, {"score": 0.0035858673522839407, "phrase": "indirect_derivation"}, {"score": 0.0035601676907962626, "phrase": "low-rank_factors"}, {"score": 0.0034967186553667715, "phrase": "optimum_approximation"}, {"score": 0.003459191419478298, "phrase": "likelihood_matrix"}, {"score": 0.00333697550984782, "phrase": "similar_speedups"}, {"score": 0.00326572165784246, "phrase": "far_lesser_impact"}, {"score": 0.0030830390264125923, "phrase": "typical_case"}, {"score": 0.0030609320300175953, "phrase": "matrix_multiplication_based_approach"}, {"score": 0.002942138511829811, "phrase": "timit_task"}, {"score": 0.002797572252783356, "phrase": "computational_performance"}, {"score": 0.0027775064989253575, "phrase": "overall_speedups"}, {"score": 0.0026889764541370106, "phrase": "timit"}, {"score": 0.0026410082462069596, "phrase": "word_error_rate"}, {"score": 0.0024842242921857705, "phrase": "timit."}, {"score": 0.002448704249007665, "phrase": "pairwise_euclidean_distance_computation_phase"}, {"score": 0.0024311346712789553, "phrase": "dynamic_time_warping"}, {"score": 0.0023116191163488824, "phrase": "computational_operations"}, {"score": 0.0021587414711262904, "phrase": "pairwise_euclidean_distances"}, {"score": 0.002104998402294158, "phrase": "dtw."}], "paper_keywords": ["Speech recognition", " Acoustic likelihood computations", " Low-rank matrix approximation", " Euclidean distance matrix computation", " Dynamic time warping"], "paper_abstract": "Acoustic modeling using mixtures of multivariate Gaussians is the prevalent approach for many speech processing problems. Computing likelihoods against a large set of Gaussians is required as a part of many speech processing systems and it is the computationally dominant phase for Large Vocabulary Continuous Speech Recognition (LVCSR) systems. We express the likelihood computation as a multiplication of matrices representing augmented feature vectors and Gaussian parameters. The computational gain of this approach over traditional methods is by exploiting the structure of these matrices and efficient implementation of their multiplication. In particular, we explore direct low-rank approximation of the Gaussian parameter matrix and indirect derivation of low-rank factors of the Gaussian parameter matrix by optimum approximation of the likelihood matrix. We show that both the methods lead to similar speedups but the latter leads to far lesser impact on the recognition accuracy. Experiments on 1,138 work vocabulary RM1 task and 6,224 word vocabulary TIMIT task using Sphinx 3.7 system show that, for a typical case the matrix multiplication based approach leads to overall speedup of 46 % on RM1 task and 115 % for TIMIT task. Our low-rank approximation methods provide a way for trading off recognition accuracy for a further increase in computational performance extending overall speedups up to 61 % for RM1 and 119 % for TIMIT for an increase of word error rate (WER) from 3.2 to 3.5 % for RM1 and for no increase in WER for TIMIT. We also express pairwise Euclidean distance computation phase in Dynamic Time Warping (DTW) in terms of matrix multiplication leading to saving of approximately of computational operations. In our experiments using efficient implementation of matrix multiplication, this leads to a speedup of 5.6 in computing the pairwise Euclidean distances and overall speedup up to 3.25 for DTW.", "paper_title": "Fast Likelihood Computation in Speech Recognition using Matrices", "paper_id": "WOS:000314068300010"}