{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "structured_environments"}, {"score": 0.00464811139983244, "phrase": "intrinsically_motivated_developmental_learning"}, {"score": 0.004601508848655033, "phrase": "abstract_skill_hierarchies"}, {"score": 0.004419703921314433, "phrase": "long-term_learning"}, {"score": 0.004245051445971095, "phrase": "agent's_efficiency"}, {"score": 0.004139401327448807, "phrase": "related_tasks"}, {"score": 0.004077272409331917, "phrase": "complex_domain"}, {"score": 0.004016072233242585, "phrase": "structured_domains"}, {"score": 0.003876806615011266, "phrase": "causal_relationships"}, {"score": 0.00376127170364341, "phrase": "different_features"}, {"score": 0.00363080961099728, "phrase": "skill_learning"}, {"score": 0.003576286811853861, "phrase": "bayesian_network_structure"}, {"score": 0.003332440548684892, "phrase": "reinforcement_learning_agents"}, {"score": 0.0028498718992429825, "phrase": "intrinsic_motivation"}, {"score": 0.0026688629249775925, "phrase": "new_structure"}, {"score": 0.0026023381988430666, "phrase": "agent's_current_set"}, {"score": 0.0023170054862555896, "phrase": "bootstrapping_property"}, {"score": 0.0022706700510423954, "phrase": "developmental_learning_process"}, {"score": 0.002202894673025544, "phrase": "domain_knowledge"}, {"score": 0.0021807544600219216, "phrase": "behavioral_complexity"}], "paper_keywords": ["Active learning", " intrinsic motivation", " options", " planning", " reinforcement learning", " structure learning"], "paper_abstract": "We present a framework for intrinsically motivated developmental learning of abstract skill hierarchies by reinforcement learning agents in structured environments. Long-term learning of skill hierarchies can drastically improve an agent's efficiency in solving ensembles of related tasks in a complex domain. In structured domains composed of many features, understanding the causal relationships between actions and their effects on different features of the environment can greatly facilitate skill learning. Using Bayesian network structure (learning techniques and structured dynamic programming algorithms), we show that reinforcement learning agents can learn incrementally and autonomously both the causal structure of their environment and a hierarchy of skills that exploit this structure. Furthermore, we present a novel active learning scheme that employs intrinsic motivation to maximize the efficiency with which this structure is learned. As new structure is acquired using an agent's current set of skills, more complex skills are learned, which in turn allow the agent to discover more structure, and so on. This bootstrapping property makes our approach a developmental learning process that results in steadily increasing domain knowledge and behavioral complexity as an agent continues to explore its environment.", "paper_title": "Intrinsically Motivated Hierarchical Skill Learning in Structured Environments", "paper_id": "WOS:000294902000007"}