{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "computational_linguistics"}, {"score": 0.004081385289516914, "phrase": "corpus_annotators"}, {"score": 0.0037336520904376687, "phrase": "underlying_assumptions"}, {"score": 0.003459191419478298, "phrase": "krippendorff's_alpha"}, {"score": 0.003287482882222731, "phrase": "scott's_pi"}, {"score": 0.003204841443605308, "phrase": "cohen's_kappa"}, {"score": 0.0025160740422447837, "phrase": "kappa-like_measures"}], "paper_keywords": [""], "paper_abstract": "This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks-but that their use makes the interpretation of the value of the coefficient even harder.", "paper_title": "Inter-Coder Agreement for Computational Linguistics", "paper_id": "WOS:000261404600004"}