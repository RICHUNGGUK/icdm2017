{"auto_keywords": [{"score": 0.03315464707767562, "phrase": "multivariate_data"}, {"score": 0.030683466124390144, "phrase": "ncm"}, {"score": 0.00481495049065317, "phrase": "neighborhood_counting"}, {"score": 0.004767261858417823, "phrase": "nearest_neighbors"}, {"score": 0.004720043307726269, "phrase": "general_idea"}, {"score": 0.0046269981365987915, "phrase": "machine_learning"}, {"score": 0.004596390584582711, "phrase": "data_mining"}, {"score": 0.004565984572487962, "phrase": "natural_language_understanding"}, {"score": 0.004520750567829338, "phrase": "information_retrieval"}, {"score": 0.004387707794516453, "phrase": "k-_nearest_neighbors_algorithm"}, {"score": 0.004065156038847781, "phrase": "general_methodology"}, {"score": 0.00397175830521626, "phrase": "similarity_functions"}, {"score": 0.0037913267098534887, "phrase": "data_space"}, {"score": 0.0037537380967196123, "phrase": "data_point"}, {"score": 0.0035358770665604657, "phrase": "data_points"}, {"score": 0.0032975819773787985, "phrase": "different_types"}, {"score": 0.003254041350487688, "phrase": "different_ways"}, {"score": 0.003004572722189157, "phrase": "ncm._ncm"}, {"score": 0.002925728709297618, "phrase": "knn._experiments"}, {"score": 0.0028489528443764608, "phrase": "vdm"}, {"score": 0.0026304533650262545, "phrase": "relatively_large_k_values"}, {"score": 0.0025699349262643754, "phrase": "heom"}, {"score": 0.002527772077437764, "phrase": "euclidean"}, {"score": 0.002510801832283111, "phrase": "hamming_distances"}, {"score": 0.0024776238724770524, "phrase": "\"_standard"}, {"score": 0.002364907617070299, "phrase": "computational_complexity"}, {"score": 0.002318180580715058, "phrase": "standard_euclidean_distance_function"}, {"score": 0.002249811525923913, "phrase": "numerical_and_categorical_data"}, {"score": 0.0022274718912558343, "phrase": "conceptually_uniform_way"}, {"score": 0.0022053535889915914, "phrase": "neighborhood_counting_methodology"}], "paper_keywords": ["pattern recognition", " machine learning", " nearest neighbors", " distance", " similarity", " neighborhood counting measure"], "paper_abstract": "Finding nearest neighbors is a general idea that underlies many artificial intelligence tasks, including machine learning, data mining, natural language understanding, and information retrieval. This idea is explicitly used in the k- nearest neighbors algorithm ( kNN), a popular classification method. In this paper, this idea is adopted in the development of a general methodology, neighborhood counting, for devising similarity functions. We turn our focus from neighbors to neighborhoods, a region in the data space covering the data point in question. To measure the similarity between two data points, we consider all neighborhoods that cover both data points. We propose to use the number of such neighborhoods as a measure of similarity. Neighborhood can be defined for different types of data in different ways. Here, we consider one definition of neighborhood for multivariate data and derive a formula for such similarity, called neighborhood counting measure or NCM. NCM was tested experimentally in the framework of kNN. Experiments show that NCM is generally comparable to VDM and its variants, the state- of- the- art distance functions for multivariate data, and, at the same time, is consistently better for relatively large k values. Additionally, NCM consistently outperforms HEOM ( a mixture of Euclidean and Hamming distances), the \" standard\" and most widely used distance function for multivariate data. NCM has a computational complexity in the same order as the standard Euclidean distance function and NCM is task independent and works for numerical and categorical data in a conceptually uniform way. The neighborhood counting methodology is proven sound for multivariate data experimentally. We hope it will work for other types of data.", "paper_title": "Nearest neighbors by neighborhood counting", "paper_id": "WOS:000236734400008"}