{"auto_keywords": [{"score": 0.035092275355708184, "phrase": "nmf"}, {"score": 0.0157194792572575, "phrase": "topic_modeling"}, {"score": 0.011217819771206013, "phrase": "lda"}, {"score": 0.004638354165775803, "phrase": "established_method"}, {"score": 0.004543046350523362, "phrase": "text_corpora"}, {"score": 0.004486800020569973, "phrase": "probabilistic_techniques"}, {"score": 0.004431246962597134, "phrase": "latent_dirichlet_allocation"}, {"score": 0.004129138311177554, "phrase": "adequate_attention"}, {"score": 0.003977594755967813, "phrase": "topic_coherence"}, {"score": 0.003928321515373316, "phrase": "semantic_interpretability"}, {"score": 0.0038796562792487, "phrase": "top_terms"}, {"score": 0.003799878496075633, "phrase": "discovered_topics"}, {"score": 0.0033964919992320024, "phrase": "will-matrix_decomposition_techniques"}, {"score": 0.003354393040689453, "phrase": "non-negative_matrix_factorization"}, {"score": 0.003177892611195514, "phrase": "current_work"}, {"score": 0.003048449231088915, "phrase": "popular_variants"}, {"score": 0.002960979624379786, "phrase": "multiple_corpora"}, {"score": 0.00288800021194675, "phrase": "associated_generality"}, {"score": 0.002816814445002924, "phrase": "existing_and_new_measures"}, {"score": 0.0027473784878300133, "phrase": "distributional_semantics"}, {"score": 0.002624482632439123, "phrase": "higher_levels"}, {"score": 0.0025385621498128243, "phrase": "lda_topic_descriptors"}, {"score": 0.0024350966556966757, "phrase": "associated_term_weighting_strategy"}, {"score": 0.002404885278896803, "phrase": "major_role"}, {"score": 0.002158233639156015, "phrase": "non-mainstream_domains"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Topic modeling", " Topic coherence", " LDA", " NMF"], "paper_abstract": "In recent years, topic modeling has become an established method in the analysis of text corpora, with probabilistic techniques such as latent Dirichlet allocation (LDA) commonly employed for this purpose. However, it might be argued that adequate attention is often not paid to the issue of topic coherence, the semantic interpretability of the top terms usually used to describe discovered topics. Nevertheless, a number of studies have proposed measures for analyzing such coherence, where these have been largely focused on topics found by LDA, will-matrix decomposition techniques such as Non-negative Matrix Factorization (NMF) being somewhat overlooked in comparison. This motivates the current work, where we compare and analyze topics found by popular variants of both NMF and LDA in multiple corpora in terms of both their coherence and associated generality, using a combination of existing and new measures, including one based on distributional semantics. Two out of three coherence measures find NMF to regularly produce more coherent topics, with higher levels of generality and redundancy observed with the LDA topic descriptors. In all cases, we observe that the associated term weighting strategy plays a major role. The results observed with NMF suggest that this may be a more suitable topic modeling method when analyzing certain corpora, such as those associated with niche or non-mainstream domains. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "An analysis of the coherence of descriptors in topic modeling", "paper_id": "WOS:000355036900021"}