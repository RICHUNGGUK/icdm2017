{"auto_keywords": [{"score": 0.04011703650429169, "phrase": "elm"}, {"score": 0.03966407606150657, "phrase": "earlier_works"}, {"score": 0.03705282032995245, "phrase": "neural_networks"}, {"score": 0.027425823214030497, "phrase": "multi-hidden-layer_feedforward_networks"}, {"score": 0.024356511288627498, "phrase": "svm"}, {"score": 0.00481495049065317, "phrase": "extreme_learning_machines"}, {"score": 0.004703149944841539, "phrase": "frank_rosenblatt's_dream_and"}, {"score": 0.00468476935399643, "phrase": "john_von_neumann's"}, {"score": 0.004611959213131437, "phrase": "emergent_machine_learning_technique-extreme_learning_machines"}, {"score": 0.0043148760739240575, "phrase": "growing_research_activities"}, {"score": 0.004281202319048493, "phrase": "significant_contributions"}, {"score": 0.00423118169200579, "phrase": "numerous_researchers"}, {"score": 0.003974071684351861, "phrase": "misplaced_notions"}, {"score": 0.003589046900483573, "phrase": "open_problem"}, {"score": 0.0031782684007251403, "phrase": "common_knowledge"}, {"score": 0.0031534367609709233, "phrase": "conventional_neural_network"}, {"score": 0.0029966788947444535, "phrase": "wide_types"}, {"score": 0.002792375587031108, "phrase": "elm_theories"}, {"score": 0.0027167292419065514, "phrase": "theoretical_foundations"}, {"score": 0.0026954938318408464, "phrase": "feedforward_neural_networks"}, {"score": 0.002674423963978094, "phrase": "random_hidden_nodes"}, {"score": 0.0026019646284747024, "phrase": "generalized_single-hidden-layer_feedforward_network"}, {"score": 0.0022152149658434916, "phrase": "suboptimal_solutions"}, {"score": 0.0021215910859085146, "phrase": "feature_representations"}, {"score": 0.0021049977753042253, "phrase": "hidden_layers"}], "paper_keywords": ["Extreme learning machine", " Random vector functional link", " QuickNet", " Radial basis function network", " Feedforward neural network", " Randomness"], "paper_abstract": "The emergent machine learning technique-extreme learning machines (ELMs)-has become a hot area of research over the past years, which is attributed to the growing research activities and significant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that (1) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes/neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes/neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc.). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; (2) ELM is proposed for both generalized single-hidden-layer feedforward network and multi-hidden-layer feedforward networks (including biological neural networks); (3) homogeneous architecture-based ELM is proposed for feature learning, clustering, regression and (binary/multi-class) classification. (4) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi-hidden-layer feedforward networks either.", "paper_title": "What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt's Dream and John von Neumann's Puzzle", "paper_id": "WOS:000354884100001"}