{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "current_activity"}, {"score": 0.004477270137801732, "phrase": "stress_level"}, {"score": 0.004389339243427964, "phrase": "non-invasive_way"}, {"score": 0.004303127794432684, "phrase": "maximum_extent"}, {"score": 0.004190796368475121, "phrase": "behavioral_and_contextual_data"}, {"score": 0.003845500924733243, "phrase": "real-life_situations"}, {"score": 0.0035285548582081627, "phrase": "light_condition"}, {"score": 0.003482160257566501, "phrase": "screen_mode"}, {"score": 0.003368816076401138, "phrase": "current_stress_level_self-assessment"}, {"score": 0.0032807941194429235, "phrase": "current_activity_type"}, {"score": 0.003030247482854539, "phrase": "current_activities"}, {"score": 0.0027803070408286158, "phrase": "high-stress_conditions"}, {"score": 0.0026543514413663893, "phrase": "separate_model"}, {"score": 0.0022196020509093694, "phrase": "android_application"}, {"score": 0.0021049977753042253, "phrase": "current_activity-type_identification"}], "paper_keywords": [""], "paper_abstract": "In this study, we aim to determine the stress level in a non-invasive way to the maximum extent possible by analyzing behavioral and contextual data received from the only source being a smartphone containing the data gathered in real-life situations. The information collected includes audio, gyroscope and accelerometer features, light condition, screen mode (on/off), current stress level self-assessment, and the current activity type. Three stress analysis models have been built: two with the consideration of current activities of a participant and one without those. Classification of low- and high-stress conditions, which was executed for a separate model for a certain kind of activity only, enabled us to achieve 3.9 % higher accuracy than that under the conditions when those activities were neglected. Also, the Android application was developed as a means for the current activity-type identification.", "paper_title": "Noninvasive stress recognition considering the current activity", "paper_id": "WOS:000361653300007"}