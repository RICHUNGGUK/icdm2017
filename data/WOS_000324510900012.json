{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "human_action"}, {"score": 0.04960202571834177, "phrase": "video_classification"}, {"score": 0.03806837886274612, "phrase": "visual_words"}, {"score": 0.03505719359718502, "phrase": "high_level_representation"}, {"score": 0.004625881588444304, "phrase": "novel_unsupervised_contextual_spectral"}, {"score": 0.004579820727958527, "phrase": "cse"}, {"score": 0.0043560464825349275, "phrase": "textual_words"}, {"score": 0.004291073117901643, "phrase": "visual_word"}, {"score": 0.00392100783796804, "phrase": "synonymous_words"}, {"score": 0.0036553070575015344, "phrase": "semantic_gap"}, {"score": 0.0031925019616678217, "phrase": "experimental_results"}, {"score": 0.003129096013910626, "phrase": "proposed_approach"}, {"score": 0.003021141590548526, "phrase": "subsequent_classification_performance"}, {"score": 0.0029169007179202164, "phrase": "approximate-semantic_descriptor_learning"}, {"score": 0.0028446460307503343, "phrase": "spectral_clustering_problem"}, {"score": 0.0027881294891497115, "phrase": "semantically_associated_visual_words"}, {"score": 0.002719055901632759, "phrase": "low-dimensional_semantic_space"}, {"score": 0.002560162916190598, "phrase": "human_action_videos"}, {"score": 0.0024842242921857705, "phrase": "inter-video_context"}, {"score": 0.002459414063407306, "phrase": "mid-level_semantics"}, {"score": 0.002422661411769552, "phrase": "non-parametric_correlation_measure"}, {"score": 0.0023040642263522505, "phrase": "significantly_improved_results"}, {"score": 0.0021693692831601745, "phrase": "unconstrained_environments"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Contextual spectral embedding", " Pearson product moment correlation", " Visual vocabulary"], "paper_abstract": "The paper presents a novel unsupervised contextual spectral (CSE) framework for human action and video classification. Similar to textual words, the visual word (a mid-level semantic) representation of an image or video contains a combination of synonymous words which give rise to the ambiguity of the representation. To narrow the semantic gap between visual words (mid-level semantic representation) and high-level semantics, we propose a high level representation called approximate-semantic descriptor. The experimental results show that the proposed approach for visual words disambiguation could improve the subsequent classification performance. In the paper, the approximate-semantic descriptor learning is formulated as a spectral clustering problem, such that semantically associated visual words are placed closely in low-dimensional semantic space and then clustered into one approximate-semantic descriptor. Specifically, the high level representation of human action videos is learnt by capturing the inter-video context of mid-level semantics via a non-parametric correlation measure. Experiments on four standard datasets demonstrate that our approach can achieve significantly improved results with respect to the state of the art, particularly for unconstrained environments. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "Unsupervised approximate-semantic vocabulary learning for human action and video classification", "paper_id": "WOS:000324510900012"}