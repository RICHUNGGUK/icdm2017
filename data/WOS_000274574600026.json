{"auto_keywords": [{"score": 0.03517083259120442, "phrase": "ggd"}, {"score": 0.011358287246688551, "phrase": "sir"}, {"score": 0.01010065376139055, "phrase": "continuous_variable"}, {"score": 0.00481495049065317, "phrase": "generalized_gradient_direction"}, {"score": 0.004752221133109136, "phrase": "sufficient_dimension_reduction_methods"}, {"score": 0.004598933498016774, "phrase": "sliced_inverse_regression"}, {"score": 0.00450933377534525, "phrase": "sliced_average_variance_estimate"}, {"score": 0.003903269701018361, "phrase": "new_effective_method"}, {"score": 0.0035605741849764187, "phrase": "sufficient_dimension_reduction_problems"}, {"score": 0.003021502415143067, "phrase": "numerical_discrete_variable"}, {"score": 0.002962543869356265, "phrase": "existing_methods"}, {"score": 0.0021753413850499467, "phrase": "ggd."}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": [""], "paper_abstract": "Sufficient dimension reduction methods, such as the sliced inverse regression one(SIR)and the sliced average variance estimate one (SAVE), usually put restrictions on the regressor: X being elliptical or normal. We propose a new effective method, called the generalized gradient direction method (GGD), for solving sufficient dimension reduction problems. Compared with SIR, SAVE etc., GGD makes very weak assumptions on X and performs well with X being a continuous variable or a numerical discrete variable, while existing methods are all developed with X being a continuous variable. The computation for GGD is very simple, just like for SIR, SAVE etc. Moreover, GGD proves robust compared with many standard techniques. Simulation results in comparison with results from other methods support the advantages of GGD. (C) 2009 Elsevier B.V. All rights reserved,", "paper_title": "Dimension reduction using the generalized gradient direction", "paper_id": "WOS:000274574600026"}