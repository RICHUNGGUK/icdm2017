{"auto_keywords": [{"score": 0.047155778350020494, "phrase": "mobile_devices"}, {"score": 0.015719716506582538, "phrase": "scalable_mobile_visual_classification"}, {"score": 0.0047747348769918, "phrase": "kernel_preserving_projection_over_high-dimensional_features"}, {"score": 0.004559451202070545, "phrase": "large_semantic_space"}, {"score": 0.0043721365634105565, "phrase": "emerging_problem"}, {"score": 0.004299374036140852, "phrase": "paradigm_shift"}, {"score": 0.00426344619171077, "phrase": "mobile_platforms"}, {"score": 0.004210114198411514, "phrase": "explosive_growth"}, {"score": 0.004174929128880472, "phrase": "visual_data"}, {"score": 0.00377487157059352, "phrase": "severe_resource_constraints"}, {"score": 0.003574364535717202, "phrase": "local_contexts"}, {"score": 0.0035149108423181952, "phrase": "google"}, {"score": 0.0034418037573506837, "phrase": "user_satisfaction"}, {"score": 0.00330025288273863, "phrase": "ignored_challenges"}, {"score": 0.0032181231788427655, "phrase": "feasible_solution"}, {"score": 0.0031248767378106663, "phrase": "mobile_visual_classification"}, {"score": 0.003059925849144792, "phrase": "unsupervised_linear_dimension_reduction_algorithm"}, {"score": 0.002909482525367989, "phrase": "kernel_matrix"}, {"score": 0.002885135695263876, "phrase": "high_dimensional_features"}, {"score": 0.0028609920171902186, "phrase": "low_dimensional_linear_embedding"}, {"score": 0.002766415324343602, "phrase": "projection_matrix"}, {"score": 0.0027088948471991454, "phrase": "mobile_computing"}, {"score": 0.002543392353000551, "phrase": "linear_dimension_reduction"}, {"score": 0.0025221012517852907, "phrase": "low-rank_linear_distance_metric"}, {"score": 0.002501104256106796, "phrase": "taylor"}, {"score": 0.00246964807581858, "phrase": "rbf_kernel"}, {"score": 0.0023879771786116228, "phrase": "proposed_kpp_method"}, {"score": 0.002367983991444061, "phrase": "high-dimensional_features"}, {"score": 0.0022800613046444563, "phrase": "proposed_method"}, {"score": 0.0022609695885550058, "phrase": "existing_dimension_reduction_methods"}, {"score": 0.002149722786728988, "phrase": "storage_consumption"}, {"score": 0.0021049977753042253, "phrase": "classification_results"}], "paper_keywords": ["Dimension reduction", " distance metric learning", " manifold learning", " mobile image classification"], "paper_abstract": "Scalable mobile visual classification-classifying images/videos in a large semantic space on mobile devices in real-time-is an emerging problem as observing the paradigm shift towards mobile platforms and the explosive growth of visual data. Though seeing the advances in detecting thousands of concepts in the servers, the scalability is handicapped in mobile devices due to the severe resource constraints within. However, certain emerging applications require such scalable visual classification with prompt response for detecting local contexts (e.g., Google Glass) or ensuring user satisfaction. In this work, we point out the ignored challenges for scalable mobile visual classification and provide a feasible solution. To overcome the limitations of mobile visual classification, we propose an unsupervised linear dimension reduction algorithm, kernel preserving projection (KPP), which approximates the kernel matrix of high dimensional features with low dimensional linear embedding. We further introduce sparsity to the projection matrix to ensure its compliance with mobile computing (with merely 12% non-zero entries). By inspecting the similarity of linear dimension reduction with low-rank linear distance metric and Taylor expansion of RBF kernel, we justified the feasibility for the proposed KPP method over high-dimensional features. Experimental results on three public datasets confirm that the proposed method outperforms existing dimension reduction methods. What is even more, we can greatly reduce the storage consumption and efficiently compute the classification results on the mobile devices.", "paper_title": "Scalable Mobile Visual Classification by Kernel Preserving Projection Over High-Dimensional Features", "paper_id": "WOS:000344720200013"}