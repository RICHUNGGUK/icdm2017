{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "users'_minds"}, {"score": 0.00467207889925365, "phrase": "implicit_image_annotation"}, {"score": 0.004552981066668676, "phrase": "possible_solutions"}, {"score": 0.004379977556546879, "phrase": "user_attention"}, {"score": 0.004213519983188574, "phrase": "gaze_trajectory"}, {"score": 0.004053362743156545, "phrase": "implicit_information"}, {"score": 0.004001335931796133, "phrase": "target_template"}, {"score": 0.003949974260471287, "phrase": "visual_attention"}, {"score": 0.003816197777388722, "phrase": "fuzzy_logic"}, {"score": 0.0037188360359702182, "phrase": "gaze-movement_features"}, {"score": 0.0036553070575015344, "phrase": "user_interest_level"}, {"score": 0.003623949764016325, "phrase": "uil"}, {"score": 0.003324743991757282, "phrase": "gaze_features"}, {"score": 0.0031845067058084613, "phrase": "new_processing_system"}, {"score": 0.0031435968684569112, "phrase": "new_user"}, {"score": 0.0031032109441552287, "phrase": "higher_accuracy"}, {"score": 0.00306334226516593, "phrase": "generated_uils"}, {"score": 0.0029980260560551982, "phrase": "image_annotation_purposes"}, {"score": 0.0027150087516999047, "phrase": "developed_framework"}, {"score": 0.0026916956768363158, "phrase": "promising_and_reliable_uils"}, {"score": 0.0026456667695073043, "phrase": "target_images"}, {"score": 0.0023145690608400425, "phrase": "existing_information"}, {"score": 0.002294686545883412, "phrase": "gaze_patterns"}, {"score": 0.002226430168622392, "phrase": "machine's_judgement"}, {"score": 0.0022073031056287903, "phrase": "image_content"}, {"score": 0.0021695394558562927, "phrase": "human_interest"}, {"score": 0.0021049977753042253, "phrase": "virtual_environments"}], "paper_keywords": ["Gaze tracking", " human computer interaction", " image annotation", " image databases", " image retrieval", " implicit annotation", " visual attention"], "paper_abstract": "This paper explores the possible solutions for image annotation and retrieval by implicitly monitoring user attention via eye-tracking. Features are extracted from the gaze trajectory of users examining sets of images to provide implicit information on the target template that guides visual attention. Our Gaze Inference System (GIS) is a fuzzy logic based framework that analyzes the gaze-movement features to assign a user interest level (UIL) from 0 to 1 to every image that appeared on the screen. Because some properties of the gaze features are unique for every user, our user adaptive framework builds a new processing system for every new user to achieve higher accuracy. The generated UILs can be used for image annotation purposes; however, the output of our system is not limited as it can be used also for retrieval or other scenarios. The developed framework produces promising and reliable UILs where approximately 53% of target images in the users' minds can be identified by the machine with an error of less than 20% and the top 10% of them with no error. We show in this paper that the existing information in gaze patterns can be employed to improve the machine's judgement of image content by assessment of human interest and attention to the objects inside virtual environments.", "paper_title": "Reading Users' Minds From Their Eyes: A Method for Implicit Image Annotation", "paper_id": "WOS:000304166700013"}