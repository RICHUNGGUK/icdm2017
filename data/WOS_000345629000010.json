{"auto_keywords": [{"score": 0.04838011372739916, "phrase": "human-machine_interaction"}, {"score": 0.014766723391064846, "phrase": "cognitive_load_measurement"}, {"score": 0.00481495049065317, "phrase": "robust_pupil"}, {"score": 0.00468627326602559, "phrase": "near-field_video_sequences"}, {"score": 0.004602395776895036, "phrase": "monitoring_pupil_and_blink_dynamics"}, {"score": 0.004204904449336037, "phrase": "real-time_applications"}, {"score": 0.004092463367307375, "phrase": "eye_images"}, {"score": 0.003965061486705487, "phrase": "manual_intervention"}, {"score": 0.003929391999210588, "phrase": "fine_tuning"}, {"score": 0.003688497722149817, "phrase": "infrared-illuminated_eye_images"}, {"score": 0.0034623203126093833, "phrase": "background_images"}, {"score": 0.003400274964240732, "phrase": "low_cost_webcam"}, {"score": 0.003294350220187021, "phrase": "convex_hull"}, {"score": 0.003249966782844541, "phrase": "dual-ellipse_fitting_method"}, {"score": 0.0031629800646754505, "phrase": "pupil_boundary_points"}, {"score": 0.0030922667860233603, "phrase": "eyelid_occlusion_state"}, {"score": 0.0030094882299407256, "phrase": "realistic_video_dataset_show"}, {"score": 0.0029689311901364797, "phrase": "measurement_accuracy"}, {"score": 0.0029289191071848403, "phrase": "proposed_methods"}, {"score": 0.0028505007899807446, "phrase": "widely_used_manually_tuned_parameter_methods"}, {"score": 0.0026634937939104177, "phrase": "accurate_and_fast_estimate"}, {"score": 0.0025341792091114184, "phrase": "different_users"}, {"score": 0.0025113494092413604, "phrase": "task_types"}, {"score": 0.0023571835517819124, "phrase": "computationally_efficient_implementation"}, {"score": 0.0023148964315569866, "phrase": "threshold_calibration"}, {"score": 0.00222249726328354, "phrase": "mini_ir_camera"}, {"score": 0.0021826210858934933, "phrase": "lightweight_glasses_frame"}, {"score": 0.0021531831652035482, "phrase": "google_glass"}, {"score": 0.0021241414408077895, "phrase": "convenient_applications"}, {"score": 0.0021049977753042253, "phrase": "real-time_adaptive_aiding"}], "paper_keywords": ["Biometrics", " cognitive informatics", " human factors", " image segmentation", " pervasive computing"], "paper_abstract": "Monitoring pupil and blink dynamics has applications in cognitive load measurement during human-machine interaction. However, accurate, efficient, and robust pupil size and blink estimation pose significant challenges to the efficacy of real-time applications due to the variability of eye images, hence to date, require manual intervention for fine tuning of parameters. In this paper, a novel self-tuning threshold method, which is applicable to any infrared-illuminated eye images without a tuning parameter, is proposed for segmenting the pupil from the background images recorded by a low cost webcam placed near the eye. A convex hull and a dual-ellipse fitting method are also proposed to select pupil boundary points and to detect the eyelid occlusion state. Experimental results on a realistic video dataset show that the measurement accuracy using the proposed methods is higher than that of widely used manually tuned parameter methods or fixed parameter methods. Importantly, it demonstrates convenience and robustness for an accurate and fast estimate of eye activity in the presence of variations due to different users, task types, load, and environments. Cognitive load measurement in human-machine interaction can benefit from this computationally efficient implementation without requiring a threshold calibration beforehand. Thus, one can envisage a mini IR camera embedded in a lightweight glasses frame, like Google Glass, for convenient applications of real-time adaptive aiding and task management in the future.", "paper_title": "Efficient and Robust Pupil Size and Blink Estimation from Near-Field Video Sequences for Human-Machine Interaction", "paper_id": "WOS:000345629000010"}