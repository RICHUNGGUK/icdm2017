{"auto_keywords": [{"score": 0.03479253313308492, "phrase": "faster_learning"}, {"score": 0.00481495049065317, "phrase": "actor-critic_control"}, {"score": 0.004388796812234033, "phrase": "local_linear_regression"}, {"score": 0.0039694170761766226, "phrase": "crucial_feature"}, {"score": 0.003674192201248236, "phrase": "process_model"}, {"score": 0.0034538759435451274, "phrase": "llr"}, {"score": 0.0033486631403337555, "phrase": "efficient_policy_update"}, {"score": 0.0032216622984027558, "phrase": "first_algorithm"}, {"score": 0.0031477782772867655, "phrase": "novel_model-based_update_rule"}, {"score": 0.002981885091155911, "phrase": "second_algorithm"}, {"score": 0.0028687545730861665, "phrase": "explicit_actor"}, {"score": 0.0027813397260607487, "phrase": "reference_model"}, {"score": 0.0026965813247045427, "phrase": "desired_behavior"}, {"score": 0.002476546030310661, "phrase": "learned_process_model"}, {"score": 0.002364172626241202, "phrase": "standard_actor-critic_algorithm"}, {"score": 0.0022744242010653997, "phrase": "pendulum_swing-up_problem"}, {"score": 0.00218807532188779, "phrase": "novel_methods"}, {"score": 0.0021049977753042253, "phrase": "standard_algorithm"}], "paper_keywords": ["Actor-critic", " inverse model", " local linear regression (LLR)", " machine learning algorithms", " reinforcement learning (RL)"], "paper_abstract": "We propose two new actor-critic algorithms for reinforcement learning. Both algorithms use local linear regression (LLR) to learn approximations of the functions involved. A crucial feature of the algorithms is that they also learn a process model, and this, in combination with LLR, provides an efficient policy update for faster learning. The first algorithm uses a novel model-based update rule for the actor parameters. The second algorithm does not use an explicit actor but learns a reference model which represents a desired behavior, from which desired control actions can be calculated using the inverse of the learned process model. The two novel methods and a standard actor-critic algorithm are applied to the pendulum swing-up problem, in which the novel methods achieve faster learning than the standard algorithm.", "paper_title": "Efficient Model Learning Methods for Actor-Critic Control", "paper_id": "WOS:000304163200002"}