{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "margin_distribution"}, {"score": 0.006308617130668757, "phrase": "diverse_base_classifiers"}, {"score": 0.0047517225918491226, "phrase": "ensemble_learning"}, {"score": 0.004625018407864454, "phrase": "drmf"}, {"score": 0.004536862734384202, "phrase": "important_factor"}, {"score": 0.004418456827500267, "phrase": "generalization_performance"}, {"score": 0.0041084684278314305, "phrase": "novel_ensemble_learning_algorithm"}, {"score": 0.004054479957160662, "phrase": "double_rotation_margin_forest"}, {"score": 0.0037203741725514126, "phrase": "combined_system"}, {"score": 0.0036472537997966938, "phrase": "training_set"}, {"score": 0.0035519826240477444, "phrase": "random_rotation"}, {"score": 0.0031950646670695546, "phrase": "optimal_ensemble"}, {"score": 0.0029903851294825023, "phrase": "large-margin_ensembles"}, {"score": 0.0028173769301594745, "phrase": "good_generalization_performance"}, {"score": 0.002689746491842271, "phrase": "extensive_set"}, {"score": 0.0026543514413663893, "phrase": "benchmark_classification_tasks"}, {"score": 0.002602127996667857, "phrase": "experimental_results"}, {"score": 0.0024678232457521856, "phrase": "bagging"}, {"score": 0.00240328454980006, "phrase": "rotation_forest"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Ensemble learning", " Margin distribution", " Diversity", " Fusion strategy", " Rotation"], "paper_abstract": "Margin distribution is acknowledged as an important factor for improving the generalization performance of classifiers. In this paper, we propose a novel ensemble learning algorithm named Double Rotation Margin Forest (DRMF), that aims to improve the margin distribution of the combined system over the training set. We utilise random rotation to produce diverse base classifiers, and optimize the margin distribution to exploit the diversity for producing an optimal ensemble. We demonstrate that diverse base classifiers are beneficial in deriving large-margin ensembles, and that therefore our proposed technique will lead to good generalization performance. We examine our method on an extensive set of benchmark classification tasks. The experimental results confirm that DRMF outperforms other classical ensemble algorithms such as Bagging, AdaBoostM1 and Rotation Forest. The success of DRMF is explained from the viewpoints of margin distribution and diversity. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Exploiting diversity for optimizing margin distribution in ensemble learning", "paper_id": "WOS:000340221600008"}