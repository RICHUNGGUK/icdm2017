{"auto_keywords": [{"score": 0.03499287706783538, "phrase": "trajectory_generative_model"}, {"score": 0.00481495049065317, "phrase": "bi-directional_nonlinear_learning"}, {"score": 0.004573842508130535, "phrase": "behavior_recognition"}, {"score": 0.00443878620915958, "phrase": "numerous_fields"}, {"score": 0.004307700591183651, "phrase": "difficult_problem"}, {"score": 0.004216434794603139, "phrase": "spatio-temporal_variations"}, {"score": 0.003937126962709418, "phrase": "joint_trajectory_tracking_and_recognition_algorithm"}, {"score": 0.0035523480068927614, "phrase": "bayesian_estimation_framework"}, {"score": 0.00350696416714684, "phrase": "\"autoencoder\"_network"}, {"score": 0.003477029725200912, "phrase": "high-dimensional_trajectories"}, {"score": 0.003432604816716035, "phrase": "two-dimensional_plane"}, {"score": 0.003374250301861101, "phrase": "peculiar_training_rule"}, {"score": 0.003218823621926739, "phrase": "plausible_trajectories"}, {"score": 0.0030837288356760973, "phrase": "tracking_process"}, {"score": 0.0030054007508185858, "phrase": "plausible_trajectory_set"}, {"score": 0.0029416448256236057, "phrase": "mixed_likelihood"}, {"score": 0.002854645802851, "phrase": "target_state_estimation"}, {"score": 0.002746549549029177, "phrase": "particle_filtering"}, {"score": 0.0027114323601885666, "phrase": "trajectory_identity"}, {"score": 0.0026425357050743003, "phrase": "improved_hausdorff_distance"}, {"score": 0.002608744950946475, "phrase": "estimated_trajectory"}, {"score": 0.0025424508991051483, "phrase": "truncated_reference_trajectories"}, {"score": 0.0024884913795612707, "phrase": "trajectory_recognition_results"}, {"score": 0.002373767863833987, "phrase": "next_time"}, {"score": 0.0022936610196951962, "phrase": "handwritten_digits"}, {"score": 0.002254624822004988, "phrase": "proposed_approach"}, {"score": 0.002216251512062482, "phrase": "robust_tracking"}, {"score": 0.0021973099304606076, "phrase": "exact_recognition"}, {"score": 0.002178529882942255, "phrase": "background_clutter"}, {"score": 0.0021599099979586946, "phrase": "partial_occlusion"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Visual tracking", " Trajectory generative model", " Autoencoder network", " Nonlinear dimensionality reduction", " Particle filter", " Improved Hausdorff distance"], "paper_abstract": "Motion trajectory is one of the most important cues for tracking and behavior recognition and can be widely applied to numerous fields. However, it is a difficult problem to directly model the spatio-temporal variations of trajectories due to their high dimensionality and nonlinearity. In this paper, we propose a joint trajectory tracking and recognition algorithm by combining a generative model derived from a bi-directional deep. neural network (called \"autoencoder\") into a Bayesian estimation framework. The \"autoencoder\" network embeds high-dimensional trajectories into a two-dimensional plane based on a peculiar training rule and learns a trajectory generative model by its inverse mapping. A set of plausible trajectories can be generated by the trajectory generative model. In the tracking process, the samples from the plausible trajectory set are weighted by a mixed likelihood and are resampled to obtain the target state estimation at each time step in spirit of the particle filtering. The trajectory identity is inferred by evaluating the improved Hausdorff distance between the estimated trajectory up to now and the truncated reference trajectories. Moreover, the trajectory recognition results are also used to guide the trajectory tracking for the next time. The experiments on tracking and recognizing handwritten digits show that the proposed approach can achieve both robust tracking and exact recognition in background clutter and partial occlusion. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Joint trajectory tracking and recognition based on bi-directional nonlinear learning", "paper_id": "WOS:000267723200007"}