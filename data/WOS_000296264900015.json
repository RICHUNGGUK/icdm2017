{"auto_keywords": [{"score": 0.03517980202387243, "phrase": "grace"}, {"score": 0.023700064103413042, "phrase": "data_races"}, {"score": 0.008672754646049406, "phrase": "gpu_programs"}, {"score": 0.005874007092665102, "phrase": "static_analysis"}, {"score": 0.005072682126953179, "phrase": "false_positives"}, {"score": 0.00481495049065317, "phrase": "detecting_data_races"}, {"score": 0.0047925606638348035, "phrase": "grace-addr"}, {"score": 0.004680093492185585, "phrase": "extremely_cost-effective_means"}, {"score": 0.004548996299741169, "phrase": "prior_parallel_programming_experience"}, {"score": 0.004400683733002952, "phrase": "cuda"}, {"score": 0.004379877645055758, "phrase": "opencl"}, {"score": 0.004348945041353169, "phrase": "gpu"}, {"score": 0.004318073739627171, "phrase": "non-graphical_applications"}, {"score": 0.004267240950397144, "phrase": "explicitly_parallel_languages"}, {"score": 0.004237027741481277, "phrase": "parallel_programmers"}, {"score": 0.0040505630529518305, "phrase": "multithreaded_environment"}, {"score": 0.003965112819778777, "phrase": "program_reliability"}, {"score": 0.003927713554718667, "phrase": "tool_support"}, {"score": 0.003899894875397329, "phrase": "race_conditions"}, {"score": 0.003863108365451395, "phrase": "gpu_application_developers"}, {"score": 0.0038448449361635152, "phrase": "existing_approaches"}, {"score": 0.0036322790330476813, "phrase": "non-lock_synchronization_primitives"}, {"score": 0.003530428755107654, "phrase": "state_explosion_problem"}, {"score": 0.0034641146456183976, "phrase": "simplified_modeling"}, {"score": 0.00341519498059637, "phrase": "prohibitive_runtime"}, {"score": 0.0033990419039540776, "phrase": "space_overhead"}, {"score": 0.0032186654570267857, "phrase": "carefully_designed_dynamic_checker"}, {"score": 0.0031358032682090595, "phrase": "gpus_memory_hierarchy"}, {"score": 0.0031135758917600754, "phrase": "runtime_data_accesses"}, {"score": 0.0028997611989210327, "phrase": "thread_scheduling"}, {"score": 0.0028792021166715895, "phrase": "execution_model"}, {"score": 0.002858788378807115, "phrase": "underlying_gpus"}, {"score": 0.0026247955256974726, "phrase": "nvidia_gpus"}, {"score": 0.002461926406033419, "phrase": "existing_approach"}, {"score": 0.0023533964075338016, "phrase": "evaluated_cases"}, {"score": 0.00232012560670907, "phrase": "b-tool_reports"}, {"score": 0.002249639986821758, "phrase": "low_runtime_overhead"}, {"score": 0.002170961332674992, "phrase": "evaluated_kernels"}, {"score": 0.0021402641482129356, "phrase": "grace-stmt"}, {"score": 0.0021049977753042253, "phrase": "larger_overhead"}], "paper_keywords": ["CUDA", " Concurrency", " Data Race", " GPU", " Multithreading", " Algorithm", " Design", " Reliability"], "paper_abstract": "In recent years, GPUs have emerged as an extremely cost-effective means for achieving high performance. Many application developers, including those with no prior parallel programming experience, are now trying to scale their applications using GPUs. While languages like CUDA and OpenCL have eased GPU programming for non-graphical applications, they are still explicitly parallel languages. All parallel programmers, particularly the novices, need tools that can help ensuring the correctness of their programs. Like any multithreaded environment, data races on GPUs can severely affect the program reliability. Thus, tool support for detecting race conditions can significantly benefit GPU application developers. Existing approaches for detecting data races on CPUs or CPUs have one or more of the following limitations: 1) being ill-suited for handling non-lock synchronization primitives on GPUs; 2) lacking of scalability due to the state explosion problem; 3) reporting many false positives because of simplified modeling; and/or 4) incurring prohibitive runtime and space overhead. In this paper, we propose GRace, a new mechanism for detecting races in GPU programs that combines static analysis with a carefully designed dynamic checker for logging and analyzing information at runtime. Our design utilizes GPUs memory hierarchy to log runtime data accesses efficiently. To improve the performance, GRace leverages static analysis to reduce the number of statements that need to be instrumented. Additionally, by exploiting the knowledge of thread scheduling and the execution model in the underlying GPUs, GRace can accurately detect data races with no false positives reported. Based on the above idea, we have built a prototype of GRace with two schemes, i.e., GRace-stmt and GRace-addr, for NVIDIA GPUs. Both schemes are integrated with the same static analysis. We have evaluated GRace-stmt and GRace-addr with three data race bugs in three GPU kernel functions and also have compared them with the existing approach, referred to as B-tool. Our experimental results show that both schemes of GRace are effective in detecting all evaluated cases with no false positives, whereas B-tool reports many false positives for one evaluated case. On the one hand, GRace-addr incurs low runtime overhead, i.e., 22-116%, and low space overhead, i.e., 9-18 MB, for the evaluated kernels. On the other hand, GRace-stmt offers more help in diagnosing data races with larger overhead.", "paper_title": "GRace: A Low-Overhead Mechanism for Detecting Data Races in GPU Programs", "paper_id": "WOS:000296264900015"}