{"auto_keywords": [{"score": 0.04941715257189374, "phrase": "parallel_applications"}, {"score": 0.040667183270113566, "phrase": "high-level_directives"}, {"score": 0.038853936934138286, "phrase": "trellis"}, {"score": 0.00481495049065317, "phrase": "high-level_framework"}, {"score": 0.004774937143916685, "phrase": "increasing_computational_needs"}, {"score": 0.0046828549666981635, "phrase": "parallel_architectures"}, {"score": 0.004618166136069052, "phrase": "heterogeneous_processing_resources"}, {"score": 0.004404805543906722, "phrase": "common_parallel_programming_paradigm"}, {"score": 0.004260134699483774, "phrase": "low-level_frameworks"}, {"score": 0.004236487143672267, "phrase": "architecture-specific_optimizations"}, {"score": 0.004154745345684458, "phrase": "code_base"}, {"score": 0.00392975069429078, "phrase": "performance_portability"}, {"score": 0.003897064755246921, "phrase": "common_set"}, {"score": 0.003447707406399446, "phrase": "gpu."}, {"score": 0.0033905067962602515, "phrase": "single_set"}, {"score": 0.0031360786664165093, "phrase": "openacc_compiler"}, {"score": 0.0030840850263807204, "phrase": "gpu_kernel"}, {"score": 0.0030414140732789186, "phrase": "thread_synchronization_directive"}, {"score": 0.0029993317330442104, "phrase": "transformation_techniques"}, {"score": 0.002941389980674657, "phrase": "gpu_code"}, {"score": 0.0029169007179202164, "phrase": "desired_parallelization"}, {"score": 0.002860546817196148, "phrase": "common_high-level_programming_framework"}, {"score": 0.002820961214013094, "phrase": "gpo"}, {"score": 0.002728166176898541, "phrase": "best-case_gpu_performance"}, {"score": 0.0027130000539445395, "phrase": "openacc"}, {"score": 0.002565879248732055, "phrase": "braided_parallelism"}, {"score": 0.002530359917257626, "phrase": "conditional_statements"}, {"score": 0.0025162898934995895, "phrase": "serial_sections"}, {"score": 0.002474546341064081, "phrase": "prior_knowledge"}, {"score": 0.0024607859021142887, "phrase": "compiler_behavior"}, {"score": 0.002447101794397243, "phrase": "optimal_performance"}, {"score": 0.0022696277881969896, "phrase": "correct_parallelization"}, {"score": 0.0022507188208913394, "phrase": "original_codes"}, {"score": 0.002188844565664576, "phrase": "openmp"}, {"score": 0.0021766826770280474, "phrase": "cuda"}, {"score": 0.0021405207258001118, "phrase": "overall_code_length"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Parallel computation", " Parallel frameworks", " Parallel architectures", " Loop mapping"], "paper_abstract": "The increasing computational needs of parallel applications inevitably require portability across parallel architectures, which now include heterogeneous processing resources, such as CPUs and GPUs, and multiple SIMD/SIMT widths. However, the lack of a common parallel programming paradigm that provides predictable, near-optimal performance on each resource leads to the use of low-level frameworks with architecture-specific optimizations, which in turn cause the code base to diverge and makes porting difficult. Our experiences with parallel applications and frameworks lead us to the conclusion that achieving performance portability requires a common set of high-level directives and efficient mapping onto each architecture. In order to demonstrate this concept, we develop Trellis, a prototype programming framework that allows the programmer to maintain only a single generic and structured codebase that executes efficiently on both the CPU and the GPU. Our approach annotates such code with a single set of high-level directives, derived from both OpenMP and OpenACC, that is made compatible for both architectures. Most importantly, motivated by the limitations of the OpenACC compiler in transforming such code into a GPU kernel, we introduce a thread synchronization directive and a set of transformation techniques that allow us to obtain the GPU code with the desired parallelization that yields more optimal performance. While a common high-level programming framework for both CPU and GPO is not yet available, our analysis shows that even obtaining the best-case GPU performance with OpenACC, state-of-the-art solution, requires modifications to the structure of codes to properly exploit braided parallelism, and cope with conditional statements or serial sections. While this already requires prior knowledge of compiler behavior the optimal performance is still unattainable due to the lack of synchronization. We describe the contributions of Trellis in addressing these problems by showing how it can achieve correct parallelization of the original codes for three parallel applications, with performance competitive to that of OpenMP and CUDA, improved programmability and reduced overall code length. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Trellis: Portability across architectures with a high-level framework", "paper_id": "WOS:000323404700006"}