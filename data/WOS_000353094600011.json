{"auto_keywords": [{"score": 0.04887120582438803, "phrase": "rprop"}, {"score": 0.01065867131090141, "phrase": "new_algorithm"}, {"score": 0.004310173770196077, "phrase": "so-called_globally_convergent_rprop_algorithm"}, {"score": 0.0042373381286051354, "phrase": "grprop"}, {"score": 0.0038253590626087237, "phrase": "pathological_behaviour"}, {"score": 0.0035731269727067496, "phrase": "neuralnet_software_package"}, {"score": 0.0034828730516958807, "phrase": "new_robust_convergent_back-propagation_algorithm"}, {"score": 0.003423969509052361, "phrase": "arcprop"}, {"score": 0.002838033152426417, "phrase": "sufficient_reduction"}, {"score": 0.002790004829699627, "phrase": "global_error"}, {"score": 0.0024969812322112174, "phrase": "similar_levels"}, {"score": 0.002332133125576037, "phrase": "training_speed"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Supervised learning", " First-order training algorithms", " Global convergence property", " Rprop", " GRprop", " Neuralnet"], "paper_abstract": "This paper examines conditions under which the Resilient Propagation algorithm, Rprop, fails to converge, identifies limitations of the so-called Globally Convergent Rprop algorithm, GRprop, which was previously thought to guarantee convergence, and considers pathological behaviour of the implementation of GRprop in the neuralnet software package. A new robust convergent back-propagation algorithm, ARCprop, is presented. The new algorithm builds on Rprop, but guarantees convergence by shortening steps as necessary to achieve a sufficient reduction in global error. Simulation results on four benchmark problems from the PROBEN1 collection show that the new algorithm achieves similar levels of performance to Rprop in terms of training speed, training accuracy, and generalization. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Convergence of Rprop and variants", "paper_id": "WOS:000353094600011"}