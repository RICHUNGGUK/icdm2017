{"auto_keywords": [{"score": 0.04159733181382348, "phrase": "hash_functions"}, {"score": 0.007624651972586902, "phrase": "different_hash_functions"}, {"score": 0.00481495049065317, "phrase": "large-scale_unsupervised"}, {"score": 0.004761494044422926, "phrase": "shared_structure_learning"}, {"score": 0.004726184966910756, "phrase": "hashing_methods"}, {"score": 0.0046390482426405324, "phrase": "compact_binary_signatures"}, {"score": 0.0044695432851061525, "phrase": "important_open_issue"}, {"score": 0.004306205029641728, "phrase": "compact_hash_codes"}, {"score": 0.0041488110506566825, "phrase": "prior_studies"}, {"score": 0.004042056914551798, "phrase": "time-consuming_sequential_learning_algorithms"}, {"score": 0.0038798015183611275, "phrase": "deliberately-designed_constraints"}, {"score": 0.003587842947046177, "phrase": "new_solution"}, {"score": 0.0034566143156030426, "phrase": "feature_space"}, {"score": 0.0032686865485199806, "phrase": "shared_subspace"}, {"score": 0.0032083351151314405, "phrase": "common_structure"}, {"score": 0.0031140747752837826, "phrase": "hashing_task"}, {"score": 0.0030113280682863234, "phrase": "irrelevant_information"}, {"score": 0.00295571428544335, "phrase": "hash_function_generation"}, {"score": 0.002847540262784593, "phrase": "complementary_subspace"}, {"score": 0.002815864555488112, "phrase": "useful_information"}, {"score": 0.0027949428646076627, "phrase": "specific_hash_functions"}, {"score": 0.002763850622950247, "phrase": "final_form"}, {"score": 0.002574788013427524, "phrase": "local_neighborhood_structure"}, {"score": 0.0025272158996086378, "phrase": "global_cluster_distribution"}, {"score": 0.002499094513009327, "phrase": "whole_data"}, {"score": 0.00247128526955136, "phrase": "objective_function"}, {"score": 0.0024529174827030787, "phrase": "spectral_embedding_loss"}, {"score": 0.0024346858809238766, "phrase": "binary_quantization_loss"}, {"score": 0.0024075916292945715, "phrase": "shared_subspace_contribution"}, {"score": 0.002354302193867031, "phrase": "hash_function"}, {"score": 0.0023021895404322767, "phrase": "efficient_alternating_optimization_method"}, {"score": 0.0022261705957258506, "phrase": "experimental_results"}, {"score": 0.0021850257158040664, "phrase": "nus-wide"}, {"score": 0.0021049977753042253, "phrase": "state-of-the-art_hashing_methods"}], "paper_keywords": ["Locality sensitive hashing (LSH)", " nearest neighbor search", " shared structure learning", " unsupervised hashing"], "paper_abstract": "Hashing methods are effective in generating compact binary signatures for images and videos. This paper addresses an important open issue in the literature, i.e., how to learn compact hash codes by enhancing the complementarity among different hash functions. Most of prior studies solve this problem either by adopting time-consuming sequential learning algorithms or by generating the hash functions which are subject to some deliberately-designed constraints (e.g., enforcing hash functions orthogonal to one another). We analyze the drawbacks of past works and propose a new solution to this problem. Our idea is to decompose the feature space into a subspace shared by all hash functions and its complementary subspace. On one hand, the shared subspace, corresponding to the common structure across different hash functions, conveys most relevant information for the hashing task. Similar to data de-noising, irrelevant information is explicitly suppressed during hash function generation. On the other hand, in case that the complementary subspace also contains useful information for specific hash functions, the final form of our proposed hashing scheme is a compromise between these two kinds of subspaces. To make hash functions not only preserve the local neighborhood structure but also capture the global cluster distribution of the whole data, an objective function incorporating spectral embedding loss, binary quantization loss, and shared subspace contribution is introduced to guide the hash function learning. We propose an efficient alternating optimization method to simultaneously learn both the shared structure and the hash functions. Experimental results on three well-known benchmarks CIFAR-10, NUS-WIDE, and a-TRECVID demonstrate that our approach significantly outperforms state-of-the-art hashing methods.", "paper_title": "Large-Scale Unsupervised Hashing with Shared Structure Learning", "paper_id": "WOS:000360019000009"}