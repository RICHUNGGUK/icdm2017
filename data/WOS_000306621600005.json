{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "hilbert_space"}, {"score": 0.004363846362238565, "phrase": "separable_hilbert_space"}, {"score": 0.0040334640350288, "phrase": "regularized_regression"}, {"score": 0.0038397509389856625, "phrase": "rademacher_averages_techniques"}, {"score": 0.003584025547693836, "phrase": "regression_learning_algorithm"}, {"score": 0.0034118234596993836, "phrase": "learning_rates"}, {"score": 0.003312486229143855, "phrase": "regularized_regression_learning_algorithm"}, {"score": 0.003091767402370757, "phrase": "theoretical_analysis"}, {"score": 0.0027741761880779535, "phrase": "projected_domain"}, {"score": 0.002538687900036229, "phrase": "computational_complexity"}, {"score": 0.0024405312482831646, "phrase": "regularized_least_square_regression_algorithm"}, {"score": 0.002233301265743633, "phrase": "finite_dimensional_space"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Regularized least square regression", " Random projection", " Reproducing kernel Hilbert spaces", " Learning rate"], "paper_abstract": "Based on m randomly drawn vectors in a separable Hilbert space, we investigate the consistency of the regularized regression learning algorithm by using Rademacher averages techniques. Furthermore, random projection technique for speeding up the regression learning algorithm is used. The learning rates of the regularized regression learning algorithm with random projection are established. Theoretical analysis shows that it is possible to learn directly in the projected domain. Our results reflect a tradeoff between accuracy and computational complexity when one uses regularized least square regression algorithm after random projection of the data to a finite dimensional space. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "On the performance of regularized regression learning in Hilbert space", "paper_id": "WOS:000306621600005"}