{"auto_keywords": [{"score": 0.04920822493417558, "phrase": "batch_training_algorithm"}, {"score": 0.00481495049065317, "phrase": "multi-layer_perceptron"}, {"score": 0.00458488025289647, "phrase": "fully_connected_multi-layer_perceptron"}, {"score": 0.004473982381245469, "phrase": "single_hidden_layer"}, {"score": 0.004208282550386197, "phrase": "first_stage"}, {"score": 0.004157058883925715, "phrase": "newton's_method"}, {"score": 0.003958299299658418, "phrase": "optimal_learning_factors"}, {"score": 0.003769006950727621, "phrase": "hidden_unit"}, {"score": 0.0035887342563437935, "phrase": "input_weights"}, {"score": 0.003545024951402818, "phrase": "linear_equations"}, {"score": 0.003459191419478298, "phrase": "output_weights"}, {"score": 0.0033961782274326948, "phrase": "second_stage"}, {"score": 0.003293688299319582, "phrase": "new_method's_hessian_matrix"}, {"score": 0.0031362652675927575, "phrase": "hessian"}, {"score": 0.0030789331635764122, "phrase": "whole_network"}, {"score": 0.002985988377995789, "phrase": "linearly_dependent_inputs"}, {"score": 0.002949598537993663, "phrase": "hidden_units"}, {"score": 0.002825681384008326, "phrase": "improved_version"}, {"score": 0.002641360377825228, "phrase": "improved_method"}, {"score": 0.00257735013358606, "phrase": "first_order_training_methods"}, {"score": 0.002499508839669243, "phrase": "conjugate_gradient"}, {"score": 0.002453934333239467, "phrase": "minimal_computational_overhead"}, {"score": 0.0023507979953912065, "phrase": "levenberg-marquardt"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multilayer perceptron", " Newton's method", " Hessian", " Orthogonal least squares", " Multiple optimal learning factor", " Whitening transform"], "paper_abstract": "A batch training algorithm is developed for a fully connected multi-layer perceptron, with a single hidden layer, which uses two-stages per iteration. In the first stage, Newton's method is used to find a vector of optimal learning factors (OLFs), one for each hidden unit, which is used to update the input weights. Linear equations are solved for output weights in the second stage. Elements of the new method's Hessian matrix are shown to be weighted sums of elements from the Hessian of the whole network. The effects of linearly dependent inputs and hidden units on training are analyzed and an improved version of the batch training algorithm is developed. In several examples, the improved method performs better than first order training methods like backpropagation and scaled conjugate gradient, with minimal computational overhead and performs almost as well as Levenberg-Marquardt, a second order training method, with several orders of magnitude fewer multiplications. (c) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Multiple optimal learning factors for the multi-layer perceptron", "paper_id": "WOS:000356105100038"}