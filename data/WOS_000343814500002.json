{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dense_symmetric_matrix"}, {"score": 0.004779085953614184, "phrase": "multiple_gpus"}, {"score": 0.00453537012358554, "phrase": "computing_power"}, {"score": 0.004501578643090429, "phrase": "emerging_heterogeneous_computers"}, {"score": 0.00440170067770959, "phrase": "required_computational_kernels"}, {"score": 0.004320156581220516, "phrase": "specific_hardware_architectures"}, {"score": 0.004256005669252138, "phrase": "effective_scheduling_scheme"}, {"score": 0.004161553532799662, "phrase": "available_heterogeneous_computational_units"}, {"score": 0.00399378014373321, "phrase": "case_study"}, {"score": 0.003919763262805793, "phrase": "static_scheduling_scheme"}, {"score": 0.0038327445210488355, "phrase": "symmetric_dense_matrix"}, {"score": 0.0038041680562514064, "phrase": "multicore_cpus"}, {"score": 0.003775803846431323, "phrase": "multiple_graphics_processing_units"}, {"score": 0.003691969158027291, "phrase": "single_compute_node"}, {"score": 0.003412887952564431, "phrase": "good_scalability"}, {"score": 0.0033747739383101095, "phrase": "multi-gpu_blas_kernels"}, {"score": 0.003287482882222731, "phrase": "twelve_intel_xeon_processors"}, {"score": 0.0031195948946937378, "phrase": "software_stacks"}, {"score": 0.0027985291920293127, "phrase": "higher-level_kernels"}, {"score": 0.0027363334666886912, "phrase": "solution_time"}, {"score": 0.002665511985052972, "phrase": "larger-scale_problems"}, {"score": 0.00252930673503721, "phrase": "new_scientific_discoveries"}, {"score": 0.0022688546528208133, "phrase": "useful_testbeds"}, {"score": 0.0022101055924375725, "phrase": "emerging_computers"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["dense linear algebra", " GPU accelerators", " symmetric tridiagonal reduction", " symmetric matrix-vector multiplication", " parallel eigensolver"], "paper_abstract": "For software to fully exploit the computing power of emerging heterogeneous computers, not only must the required computational kernels be optimized for the specific hardware architectures but also an effective scheduling scheme is needed to utilize the available heterogeneous computational units and to hide the communication between them. As a case study, we develop a static scheduling scheme for the tridiagonalization of a symmetric dense matrix on multicore CPUs with multiple graphics processing units (GPUs) on a single compute node. We then parallelize and optimize the Basic Linear Algebra Subroutines (BLAS)-2 symmetric matrix-vector multiplication, and the BLAS-3 low rank symmetric matrix updates on the GPUs. We demonstrate the good scalability of these multi-GPU BLAS kernels and the effectiveness of our scheduling scheme on twelve Intel Xeon processors and three NVIDIA GPUs. We then integrate our hybrid CPU-GPU kernel into computational kernels at higher-levels of software stacks, that is, a shared-memory dense eigensolver and a distributed-memory sparse eigensolver. Our experimental results show that our kernels greatly improve the performance of these higher-level kernels, not only reducing the solution time but also enabling the solution of larger-scale problems. Because such symmetric eigenvalue problems arise in many scientific and engineering simulations, our kernels could potentially lead to new scientific discoveries. Furthermore, these dense linear algebra algorithms present algorithmic characteristics that can be found in other algorithms. Hence, they are not only important computational kernels on their own but also useful testbeds to study the performance of the emerging computers and the effects of the various optimization techniques. Copyright (c) 2013 John Wiley & Sons, Ltd.", "paper_title": "Tridiagonalization of a dense symmetric matrix on multiple GPUs and its application to symmetric eigenvalue problems", "paper_id": "WOS:000343814500002"}