{"auto_keywords": [{"score": 0.04969896787799215, "phrase": "computational_fluid_dynamics"}, {"score": 0.004815316573636744, "phrase": "gpu"}, {"score": 0.004751419645257496, "phrase": "directive-based_programming"}, {"score": 0.0047262415242538105, "phrase": "graphics_processing_units"}, {"score": 0.004626850025956576, "phrase": "a.viable_alter"}, {"score": 0.004565789684788205, "phrase": "specialized_low-level_languages"}, {"score": 0.004529539189761968, "phrase": "cuda_c"}, {"score": 0.004505564975737921, "phrase": "opencl"}, {"score": 0.004481650497729742, "phrase": "general-purpose_gpu_programming"}, {"score": 0.004364122496690351, "phrase": "\"pragma\"_statements"}, {"score": 0.004329466192376432, "phrase": "source_codes"}, {"score": 0.004295083912947681, "phrase": "traditional_high-level_languages"}, {"score": 0.004215912212133127, "phrase": "unified_code_base"}, {"score": 0.004182427928965346, "phrase": "multiple_computational_platforms"}, {"score": 0.004094425626777551, "phrase": "popular_openacc_programming_standard"}, {"score": 0.004029636266402555, "phrase": "pgi_compiler_suite"}, {"score": 0.003944836112147246, "phrase": "performance_potential"}, {"score": 0.003903139206687395, "phrase": "cfd"}, {"score": 0.003790597992352057, "phrase": "openacc_fortran_api"}, {"score": 0.003760478950217445, "phrase": "test_cfd_code"}, {"score": 0.0036813220391317273, "phrase": "full-scale_research_code"}, {"score": 0.0036520680903953623, "phrase": "virginia_tech"}, {"score": 0.00362304576537247, "phrase": "test_code"}, {"score": 0.00350923650240526, "phrase": "common_gpu_platforms"}, {"score": 0.003371972121469904, "phrase": "original_source_code"}, {"score": 0.003300966061142746, "phrase": "gpu._performance"}, {"score": 0.0032487173832255454, "phrase": "nvidia"}, {"score": 0.0032315051778489205, "phrase": "amd"}, {"score": 0.0031972416916554542, "phrase": "double_and_single_precision_arithmetic"}, {"score": 0.003163393994245967, "phrase": "accelerator_code"}, {"score": 0.0031050177923315587, "phrase": "multithreaded_cpu_version"}, {"score": 0.00305587522701115, "phrase": "openmp"}, {"score": 0.0030234813328317207, "phrase": "single_nvidia_kepler_cpu_card"}, {"score": 0.002951925359532765, "phrase": "single_cpu_core"}, {"score": 0.0028591372344443154, "phrase": "optimization_techniques"}, {"score": 0.00280635987704498, "phrase": "manual_intervention"}, {"score": 0.00276189608309668, "phrase": "accelerator_performance"}, {"score": 0.002725380070974483, "phrase": "default_compiler_heuristics"}, {"score": 0.0026608603558321816, "phrase": "specific_platforms"}, {"score": 0.002611733442571296, "phrase": "multiple_accelerators"}, {"score": 0.002597864852891147, "phrase": "openacc"}, {"score": 0.0025566952106314484, "phrase": "experimental_high-level_interface"}, {"score": 0.0025431173776207885, "phrase": "multi-gpu_programming"}, {"score": 0.0025228853687229978, "phrase": "scheduling_tasks"}, {"score": 0.002509486637787865, "phrase": "multiple_devices"}, {"score": 0.002482901744545283, "phrase": "overall_performance"}, {"score": 0.0024631476037838937, "phrase": "openacc_code"}, {"score": 0.0023920477114211104, "phrase": "significant_limitations"}, {"score": 0.002354132757230988, "phrase": "openacc_api"}, {"score": 0.0023291898023664517, "phrase": "modern_fortran"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Graphics processing unit (GPU)", " Directive-based programming", " OpenACC", " Fortran", " Finite-difference method"], "paper_abstract": "Directive-based programming of graphics processing units (CPUs) has recently appeared as a.viable alter. native to using specialized low-level languages such as CUDA C and OpenCL for general-purpose GPU programming. This technique, which uses \"directive\" or \"pragma\" statements to annotate source codes written in traditional high-level languages, is designed to permit a unified code base to serve multiple computational platforms. In this work we analyze the popular OpenACC programming standard, as implemented by the PGI compiler suite, in order to evaluate its utility and performance potential in computational fluid dynamics (CFD) applications. We examine the process of applying the OpenACC Fortran API to a test CFD code that serves as a proxy for a full-scale research code developed at Virginia Tech; this test code is used to asses the performance improvements attainable for our CFD algorithm on common GPU platforms, as well as to determine the modifications that must be made to the original source code in order to run efficiently on the GPU. Performance is measured on several recent GPU architectures from NVIDIA and AMD (using both double and single precision arithmetic) and the accelerator code is bench-marked against a multithreaded CPU version constructed from the same Fortran source code using OpenMP directives. A single NVIDIA Kepler CPU card is found to perform approximately 20x faster than a single CPU core and more than 2x faster than a 16-core Xeon server. An analysis of optimization techniques for OpenACC reveals cases in which manual intervention by the programmer can improve accelerator performance by up to 30% over the default compiler heuristics, although these optimizations are relevant only for specific platforms. Additionally, the use of multiple accelerators with OpenACC is investigated, including an experimental high-level interface for multi-GPU programming that automates scheduling tasks across multiple devices. While the overall performance of the OpenACC code is found to be satisfactory, we also observe some significant limitations and restrictions imposed by the OpenACC API regarding certain useful features of modern Fortran (2003/8); these are sufficient for us to conclude that it would not be practical to apply OpenACC to our full research code at this time due to the amount of refactoring required. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Directive-based GPU programming for computational fluid dynamics", "paper_id": "WOS:000354141400021"}