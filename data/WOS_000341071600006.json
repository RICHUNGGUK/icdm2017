{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "parallel_workloads_archive"}, {"score": 0.004661116930794176, "phrase": "scientific_study"}, {"score": 0.0046276016230633495, "phrase": "complex_computer_systems"}, {"score": 0.003948043391567917, "phrase": "performance_evaluation"}, {"score": 0.003891430578662587, "phrase": "new_systems"}, {"score": 0.0038356264368681107, "phrase": "workload_data"}, {"score": 0.0037806195039533355, "phrase": "quality_issues"}, {"score": 0.003712964709503484, "phrase": "study_results"}, {"score": 0.0036597104126905437, "phrase": "scientific_observations"}, {"score": 0.003581252556182843, "phrase": "measurement_errors"}, {"score": 0.0035426543993952184, "phrase": "cumulative_experience"}, {"score": 0.0034417406427881075, "phrase": "job-level_usage_data"}, {"score": 0.003416963042033614, "phrase": "large-scale_parallel_supercomputers"}, {"score": 0.0026918477311176376, "phrase": "missing_data"}, {"score": 0.002672454243212711, "phrase": "inconsistent_data"}, {"score": 0.0026532001044943117, "phrase": "erroneous_data"}, {"score": 0.002634084319025334, "phrase": "system_configuration_changes"}, {"score": 0.00260566789186855, "phrase": "logging_period"}, {"score": 0.0024503217593801587, "phrase": "problematic_data_items"}, {"score": 0.0022547515816852266, "phrase": "specific_domain"}, {"score": 0.002238500096284007, "phrase": "parallel_jobs"}, {"score": 0.00216680270402079, "phrase": "similar_situations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Workload log", " Data quality", " Parallel job scheduling"], "paper_abstract": "Science is based upon observation. The scientific study of complex computer systems should therefore be based on observation of how they are used in practice, as opposed to how they are assumed to be used or how they were designed to be used. In particular, detailed workload logs from real computer systems are invaluable for research on performance evaluation and for designing new systems. Regrettably, workload data may suffer from quality issues that might distort the study results, just as scientific observations in other fields may suffer from measurement errors. The cumulative experience with the Parallel Workloads Archive, a repository of job-level usage data from large-scale parallel supercomputers, clusters, and grids, has exposed many such issues. Importantly, these issues were not anticipated when the data was collected, and uncovering them was not trivial. As the data in this archive is used in hundreds of studies, it is necessary to describe and debate procedures that may be used to improve its data quality. Specifically, we consider issues like missing data, inconsistent data, erroneous data, system configuration changes during the logging period, and unrepresentative user behavior. Some of these may be countered by filtering out the problematic data items. In other cases, being cognizant of the problems may affect the decision of which datasets to use. While grounded in the specific domain of parallel jobs, our findings and suggested procedures can also inform similar situations in other domains. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Experience with using the Parallel Workloads Archive", "paper_id": "WOS:000341071600006"}