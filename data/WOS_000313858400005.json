{"auto_keywords": [{"score": 0.04706664181384447, "phrase": "face_recognition"}, {"score": 0.00481495049065317, "phrase": "original_and_'symmetrical_face'_training_samples"}, {"score": 0.0047323590206695305, "phrase": "based_two-step_face_recognition"}, {"score": 0.004611108249179507, "phrase": "available_training_samples"}, {"score": 0.004454237726525018, "phrase": "real-world_applications"}, {"score": 0.004396790930873782, "phrase": "face_image"}, {"score": 0.004284100953217494, "phrase": "varying_illumination"}, {"score": 0.004247180402119585, "phrase": "facial_expression"}, {"score": 0.004138309011478741, "phrase": "non-sufficient_training_samples"}, {"score": 0.00403221710768334, "phrase": "possible_changes"}, {"score": 0.003571850980647625, "phrase": "new_samples"}, {"score": 0.003510507340837656, "phrase": "representation_based_method"}, {"score": 0.0034204548167213545, "phrase": "new_training_samples"}, {"score": 0.003361702510958392, "phrase": "possible_appearance"}, {"score": 0.00327545462323687, "phrase": "devised_representation"}, {"score": 0.00319141242685261, "phrase": "original_and_new_training_samples"}, {"score": 0.0031365819432206004, "phrase": "two-step_classification"}, {"score": 0.0030560920414081645, "phrase": "small_number"}, {"score": 0.0029264925857346497, "phrase": "test_sample"}, {"score": 0.0028145455182562807, "phrase": "similar_advantage"}, {"score": 0.0027781870000242004, "phrase": "sparse_representation_method"}, {"score": 0.0026718978608934077, "phrase": "score_level_fusion"}, {"score": 0.0025036862991477437, "phrase": "decision_level"}, {"score": 0.0024820711838337713, "phrase": "feature_level_fusion"}, {"score": 0.002449997042686432, "phrase": "experimental_results"}, {"score": 0.0024078737745407614, "phrase": "proposed_method"}, {"score": 0.002387083859537412, "phrase": "state-of-the-art_face_recognition_methods"}, {"score": 0.0022857899334246946, "phrase": "linear_regression_classification"}, {"score": 0.002236762786982225, "phrase": "collaborative_representation"}, {"score": 0.002188784900536406, "phrase": "two-phase_test_sample_sparse_representation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Pattern recognition", " Face recognition", " Sparse representation method"], "paper_abstract": "A limited number of available training samples have become one bottleneck of face recognition. In real-world applications, the face image might have various changes owing to varying illumination, facial expression and poses. However, non-sufficient training samples cannot comprehensively convey these possible changes, so it is hard to improve the accuracy of face recognition. In this paper, we propose to exploit the symmetry of the face to generate new samples and devise a representation based method to perform face recognition. The new training samples really reflect some possible appearance of the face. The devised representation based method simultaneously uses the original and new training samples to perform a two-step classification, which ultimately uses a small number of classes that are 'near' to the test sample to represent and classify it and has a similar advantage as the sparse representation method. This method also takes advantages of the score level fusion, which has proven to be very competent and usually performs better than the decision level and feature level fusion. The experimental results show that the proposed method outperforms state-of-the-art face recognition methods including the sparse representation classification (SRC), linear regression classification (LRC), collaborative representation (CR) and two-phase test sample sparse representation (TPTSSR). (C) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Using the original and 'symmetrical face' training samples to perform representation based two-step face recognition", "paper_id": "WOS:000313858400005"}