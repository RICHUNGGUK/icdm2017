{"auto_keywords": [{"score": 0.03867712206987224, "phrase": "instability_index"}, {"score": 0.00481495049065317, "phrase": "regression_trees"}, {"score": 0.004458722048888919, "phrase": "cart_regression_trees"}, {"score": 0.0042495320879288615, "phrase": "boosting_predictors_sequence"}, {"score": 0.0034063576796640603, "phrase": "training_sample"}, {"score": 0.0031845067058084583, "phrase": "bagging_algorithm"}, {"score": 0.0028923335491735564, "phrase": "additional_instability"}, {"score": 0.0027830917623582903, "phrase": "boosting_process"}, {"score": 0.0026523079380037706, "phrase": "bagging_one"}, {"score": 0.0022956075241520064, "phrase": "hard_observations"}, {"score": 0.002166711569953301, "phrase": "non-standard_regression_context"}], "paper_keywords": ["bagging", " boosting", " CART", " instability", " prediction", " regression"], "paper_abstract": "The AdaBoost like algorithm for boosting CART regression trees is considered. The boosting predictors sequence is analysed on various data sets and the behaviour of the algorithm is investigated. An instability index of a given estimation method with respect to some training sample is defined. Based on the bagging algorithm, this instability index is then extended to quantify the additional instability provided by the boosting process with respect to the bagging one. Finally, the ability of boosting to track outliers and to concentrate on hard observations is used to explore a non-standard regression context. (c) 2004 Elsevier B.V. All rights reserved.", "paper_title": "Boosting and instability for regression trees", "paper_id": "WOS:000232246200016"}