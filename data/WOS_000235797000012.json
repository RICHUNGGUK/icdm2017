{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "efficient_extrapolation"}, {"score": 0.004692227830462433, "phrase": "reduced_rank_approximations"}, {"score": 0.0045726187330434025, "phrase": "graph_kernels"}, {"score": 0.0040186038781533946, "phrase": "unseen_data"}, {"score": 0.0037188360359702182, "phrase": "principled_method"}, {"score": 0.0029089095588612007, "phrase": "matrix_approximation"}, {"score": 0.0026570998719084153, "phrase": "representer_theorem"}, {"score": 0.002555950807368337, "phrase": "multiple_joint_regularization_constraints"}, {"score": 0.002427035025924384, "phrase": "protein_classification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["kernel methods", " regularization", " graph kernels", " protein classification"], "paper_abstract": "We present a framework for efficient extrapolation of reduced rank approximations, graph kernels, and locally linear embeddings (LLE) to unseen data. We also present a principled method to combine many of these kernels and then extrapolate them. Central to our method is a theorem for matrix approximation, and an extension of the representer theorem to handle multiple joint regularization constraints. Experiments in protein classification demonstrate the feasibility of our approach. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Kernel extrapolation", "paper_id": "WOS:000235797000012"}