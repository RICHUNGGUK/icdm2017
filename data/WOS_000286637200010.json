{"auto_keywords": [{"score": 0.03981238870202312, "phrase": "widely_applicable_information_criterion"}, {"score": 0.010235439780035408, "phrase": "singular_learning_theory"}, {"score": 0.00750339665872906, "phrase": "bayes_cross-validation_loss"}, {"score": 0.00481495049065317, "phrase": "bayes_cross_validation"}, {"score": 0.004571133708117498, "phrase": "regular_statistical_models"}, {"score": 0.004500414250942443, "phrase": "leave-one-out_cross-validation"}, {"score": 0.004362226376603478, "phrase": "akaike_information_criterion"}, {"score": 0.004206338218873218, "phrase": "singular_statistical_models"}, {"score": 0.004141238989360548, "phrase": "asymptotic_behavior"}, {"score": 0.003972504904621217, "phrase": "previous_studies"}, {"score": 0.0037321585095070483, "phrase": "expectation_value"}, {"score": 0.003580032420161922, "phrase": "average_bayes_generalization_loss"}, {"score": 0.0035063024767122684, "phrase": "present_paper"}, {"score": 0.0030625949549620475, "phrase": "random_variable"}, {"score": 0.0029994894380575604, "phrase": "model_selection"}, {"score": 0.0027454383581457555, "phrase": "bayes_generalization_error"}, {"score": 0.002702887319160532, "phrase": "bayes_cross-validation_error"}, {"score": 0.0025524176761139413, "phrase": "real_log_canonical_threshold"}, {"score": 0.002461044825190643, "phrase": "training_samples"}, {"score": 0.002372935208038141, "phrase": "cross-validation_error"}, {"score": 0.0023361440016375972, "phrase": "generalization_error"}, {"score": 0.0022760856906838814, "phrase": "algebraic_geometrical_structure"}, {"score": 0.0022407927161620855, "phrase": "learning_machine"}, {"score": 0.0021605514226546497, "phrase": "deviance_information_criteria"}, {"score": 0.0021049977753042253, "phrase": "bayes_cross-validation"}], "paper_keywords": ["cross-validation", " information criterion", " singular learning machine", " birational invariant"], "paper_abstract": "In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2 lambda/n, where lambda is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.", "paper_title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory", "paper_id": "WOS:000286637200010"}