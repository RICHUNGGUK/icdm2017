{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "regularization_schemes"}, {"score": 0.0045108906244892165, "phrase": "kernel_hilbert_spaces"}, {"score": 0.004135010810625326, "phrase": "general_convex_loss_function"}, {"score": 0.0035507457285983268, "phrase": "error_analysis"}, {"score": 0.002918826667294298, "phrase": "elaborate_capacity"}, {"score": 0.002855934865505357, "phrase": "dependent_error_bounds"}, {"score": 0.002675252546913683, "phrase": "concentration_techniques"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Learning theory", " Regularized classification", " beta-mixing sequence", " Reproducing kernel Hilbert spaces", " l(2)-empirical covering number", " Capacity dependent error bounds"], "paper_abstract": "We study learning algorithms for classification generated by regularization schemes in reproducing kernel Hilbert spaces associated with a general convex loss function in a non-i.i.d. process. Error analysis is studied and our main purpose is to provide an elaborate capacity dependent error bounds by applying concentration techniques involving the l(2)-empirical covering numbers. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Classification with non-i.i.d. sampling", "paper_id": "WOS:000291243300012"}