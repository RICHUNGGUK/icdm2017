{"auto_keywords": [{"score": 0.027755210285727, "phrase": "different_feature_maps"}, {"score": 0.027426072593923996, "phrase": "statistical_model"}, {"score": 0.004815350886961311, "phrase": "functional"}, {"score": 0.004604136423327813, "phrase": "relative_contributions"}, {"score": 0.004566805735863274, "phrase": "low-level_visual_guiding_factors"}, {"score": 0.004313793774439379, "phrase": "consecutive_interesting_regions"}, {"score": 0.003976373766630598, "phrase": "visual_system"}, {"score": 0.0039280803011236395, "phrase": "visual_attention_mechanisms_control_eye_movements"}, {"score": 0.0036652492107503956, "phrase": "bottom-up_factors"}, {"score": 0.0036355032885145894, "phrase": "different_visual_features"}, {"score": 0.003378385493203297, "phrase": "quantitatively_the_relative_contribution"}, {"score": 0.00335095982979018, "phrase": "basic_low-level_features"}, {"score": 0.0032700071110068323, "phrase": "visual_attention"}, {"score": 0.003217124533918232, "phrase": "eye_movements"}, {"score": 0.003126623133221065, "phrase": "visual_features"}, {"score": 0.003051073364818384, "phrase": "bottom-up_saliency_model"}, {"score": 0.00278930116312699, "phrase": "free_viewing"}, {"score": 0.0027666443986705453, "phrase": "natural_scenes"}, {"score": 0.0027330029055210926, "phrase": "functional_saliency_model"}, {"score": 0.002677837900473396, "phrase": "primate_visual_system"}, {"score": 0.002634506575564286, "phrase": "visual_scene"}, {"score": 0.002508663916554954, "phrase": "recorded_eye_movements"}, {"score": 0.0024580159789774516, "phrase": "essential_role"}, {"score": 0.002438043578514396, "phrase": "high_frequency_luminance"}, {"score": 0.0024083881142747954, "phrase": "important_contribution"}, {"score": 0.002388817995161952, "phrase": "central_fixation_bias"}, {"score": 0.002359759876402679, "phrase": "relative_contribution"}, {"score": 0.0021926693625590006, "phrase": "saliency_map"}, {"score": 0.0021222476760114914, "phrase": "saliency_model"}, {"score": 0.0021049977753042253, "phrase": "experimental_data"}], "paper_keywords": ["Visual features", " Bottom-up saliency map", " Central fixation bias", " \"Expectation-Maximization\" algorithm"], "paper_abstract": "When looking at a scene, we frequently move our eyes to place consecutive interesting regions on the fovea, the retina centre. At each fixation, only this specific foveal region is analysed in detail by the visual system. The visual attention mechanisms control eye movements and depend on two types of factor: bottom-up and top-down factors. Bottom-up factors include different visual features such as colour, luminance, edges, and orientations. In this paper, we evaluate quantitatively the relative contribution of basic low-level features as candidate guiding factors to visual attention and hence to eye movements. We also study how these visual features can be combined in a bottom-up saliency model. Our work consists of three interactive parts: a functional saliency model, a statistical model and eye movement data recorded during free viewing of natural scenes. The functional saliency model, inspired by the primate visual system, decomposes a visual scene into different feature maps. The statistical model indicates which features best explain the recorded eye movements. We show an essential role of high frequency luminance and an important contribution of central fixation bias. The relative contribution of features, calculated by the statistical model, is then used to combine the different feature maps into a saliency map. Finally, the comparison between the saliency model and experimental data confirmed the influence of these contributions.", "paper_title": "A Functional and Statistical Bottom-Up Saliency Model to Reveal the Relative Contributions of Low-Level Visual Guiding Factors", "paper_id": "WOS:000292777400013"}