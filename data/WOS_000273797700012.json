{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "support_vector_regression"}, {"score": 0.04313137598710988, "phrase": "support_vectors"}, {"score": 0.03142156249606681, "phrase": "svr_machine"}, {"score": 0.004594951819987416, "phrase": "support_vector_machine"}, {"score": 0.0043509032299218955, "phrase": "unnecessarily_liberal_use"}, {"score": 0.004283576270012874, "phrase": "basis_functions"}, {"score": 0.0035245918797629804, "phrase": "simple_post-processing_method"}, {"score": 0.0032094634756436595, "phrase": "main_idea"}, {"score": 0.0028996963188373144, "phrase": "full_training_set"}, {"score": 0.002619748321217153, "phrase": "full_training"}, {"score": 0.002559072326933071, "phrase": "modified_target_values"}, {"score": 0.0022939396079753463, "phrase": "proposed_method"}, {"score": 0.0021381565332182773, "phrase": "good_generalization_capacity"}, {"score": 0.0021049980887993046, "phrase": "svr."}], "paper_keywords": ["Support vector regression (SVR)", " Sparseness", " RVM", " Adaptive sparse supervised learning (ASSL)", " KLASSO"], "paper_abstract": "Although the solution of support vector machine is relatively sparse, it makes unnecessarily liberal use of basis functions since the number of support vectors required typically grows linearly with the size of the training set. In this paper, we present a simple post-processing method to sparsify the solution of support vector regression (SVR). The main idea is as follows: first, we train a SVR machine on the full training set; then another SVR machine is trained only on a subset of the full training set with modified target values. This process is done several times iteratively. Experiments indicate that the proposed method can reduce the support vectors greatly while maintaining the good generalization capacity of SVR.", "paper_title": "A method to sparsify the solution of support vector regression", "paper_id": "WOS:000273797700012"}