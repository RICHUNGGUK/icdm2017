{"auto_keywords": [{"score": 0.036109640613054016, "phrase": "multiple_layers"}, {"score": 0.035888041567770625, "phrase": "collective_algorithms"}, {"score": 0.03426730308455406, "phrase": "intra-_and_inter-node_communications"}, {"score": 0.00481733616971445, "phrase": "kernel"}, {"score": 0.004620093797504386, "phrase": "multicore_clusters"}, {"score": 0.004504104287114781, "phrase": "high_performance_computing"}, {"score": 0.004363185834249464, "phrase": "mpi_applications"}, {"score": 0.00433553336181385, "phrase": "non-uniform_memory_accesses"}, {"score": 0.004253618550301311, "phrase": "recent_advances"}, {"score": 0.004226803247043444, "phrase": "mpi"}, {"score": 0.004213240903095234, "phrase": "collective_communications"}, {"score": 0.004159997145421619, "phrase": "performance_issue"}, {"score": 0.0041205045256557694, "phrase": "deep_memory_hierarchies"}, {"score": 0.004017006809411963, "phrase": "collective_topology"}, {"score": 0.003978866233891427, "phrase": "hardware_topologies"}, {"score": 0.0038789122930433305, "phrase": "single-copy_kernel"}, {"score": 0.0037935066882208235, "phrase": "distributed_environments"}, {"score": 0.003757480190200645, "phrase": "single_level_approach"}, {"score": 0.00369819191449264, "phrase": "extreme_variations"}, {"score": 0.00365143309008563, "phrase": "bandwidth_and_latency_capabilities"}, {"score": 0.003548368289666932, "phrase": "duplex_communications"}, {"score": 0.0035146616294377386, "phrase": "multiple_concurrent_copies"}, {"score": 0.0034482025128821548, "phrase": "collaborative_approach"}, {"score": 0.0033402088717816416, "phrase": "maximum_degree"}, {"score": 0.003287482882222731, "phrase": "collective_algorithm"}, {"score": 0.0031643005824589917, "phrase": "hierknem"}, {"score": 0.002885275573483437, "phrase": "resulting_scheme"}, {"score": 0.002564613964408878, "phrase": "underlying_process-core_binding"}, {"score": 0.002508073621696602, "phrase": "state-of-art_mpi_libraries"}, {"score": 0.002391067964886602, "phrase": "synthetic_benchmarks"}, {"score": 0.0023234950796782376, "phrase": "parallel_graph_application"}, {"score": 0.002236352450500035, "phrase": "linear_speedup"}, {"score": 0.0021049977753042253, "phrase": "future_many-core_hardware"}], "paper_keywords": ["MPI", " Multicore", " Cluster", " HPC", " Collective communication", " Hierarchical"], "paper_abstract": "Multicore Clusters, which have become the most prominent form of High Performance Computing (HPC) systems, challenge the performance of MPI applications with non-uniform memory accesses and shared cache hierarchies. Recent advances in MPI collective communications have alleviated the performance issue exposed by deep memory hierarchies by carefully considering the mapping between the collective topology and the hardware topologies, as well as the use of single-copy kernel assisted mechanisms. However, on distributed environments, a single level approach cannot encompass the extreme variations not only in bandwidth and latency capabilities, but also in the capability to support duplex communications or operate multiple concurrent copies. This calls for a collaborative approach between multiple layers of collective algorithms, dedicated to extracting the maximum degree of parallelism from the collective algorithm by consolidating the intra- and inter-node communications. In this work, we present HierKNEM, a kernel-assisted topology-aware collective framework, and the mechanisms deployed by this framework to orchestrate the collaboration between multiple layers of collective algorithms. The resulting scheme maximizes the overlap of intra- and inter-node communications. We demonstrate experimentally, by considering three of the most used collective operations (Broadcast, Allgather and Reduction), that (1) this approach is immune to modifications of the underlying process-core binding; (2) it outperforms state-of-art MPI libraries (Open MPI, MPICH2 and MVAPICH2) demonstrating up to a 30x speedup for synthetic benchmarks, and up to a 3x acceleration for a parallel graph application (ASP); (3) it furthermore demonstrates a linear speedup with the increase of the number of cores per compute node, a paramount requirement for scalability on future many-core hardware. (c) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Kernel-assisted and topology-aware MPI collective communications on multicore/many-core platforms", "paper_id": "WOS:000320679500011"}