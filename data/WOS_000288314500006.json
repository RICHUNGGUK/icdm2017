{"auto_keywords": [{"score": 0.03992659287798474, "phrase": "real_users"}, {"score": 0.012993173021451086, "phrase": "data-driven_reward_function"}, {"score": 0.011658022326841566, "phrase": "rl"}, {"score": 0.009883429398608352, "phrase": "simulation-based_rl"}, {"score": 0.00481495049065317, "phrase": "learning_and_evaluation_of_dialogue_strategies"}, {"score": 0.004773040022691547, "phrase": "new_applications"}, {"score": 0.00462919213721188, "phrase": "small_data_sets"}, {"score": 0.004529093447865156, "phrase": "new_data-driven_methodology"}, {"score": 0.0044896599221451216, "phrase": "simulation-based_dialogue_strategy_learning"}, {"score": 0.004260134699483774, "phrase": "automatic_optimization"}, {"score": 0.004223032986655664, "phrase": "dialogue_strategies"}, {"score": 0.004167982855788402, "phrase": "effective_dialogue_strategies"}, {"score": 0.0041136473765874815, "phrase": "initial_data"}, {"score": 0.003671338229632309, "phrase": "simulated_and_real_interactions"}, {"score": 0.003498782065781398, "phrase": "multimodal_dialogue_strategies"}, {"score": 0.003423042581201533, "phrase": "simulated_environment"}, {"score": 0.0033197446406648626, "phrase": "small_amounts"}, {"score": 0.003219553875143356, "phrase": "woz_data"}, {"score": 0.003191485583025302, "phrase": "data-driven_development"}, {"score": 0.00308163068747474, "phrase": "working_prototype"}, {"score": 0.002949598537993663, "phrase": "optimal_policies"}, {"score": 0.002823207272381655, "phrase": "original_data"}, {"score": 0.0025977542995083624, "phrase": "supervised_learning"}, {"score": 0.002218668420806254, "phrase": "rl-based_policy"}, {"score": 0.0021049977753042253, "phrase": "simulated_interaction"}], "paper_keywords": [""], "paper_abstract": "We present a new data-driven methodology for simulation-based dialogue strategy learning, which allows us to address several problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and determining a data-driven reward function. In addition, we evaluate the result with real users, and explore how results transfer between simulated and real interactions. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is \"bootstrapped\" from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows data-driven development of optimal strategies for domains where no working prototype is available. Using simulation-based RL allows us to find optimal policies which are not (necessarily) present in the original data. Our results show that simulation-based RL significantly outperforms the average (human wizard) strategy as learned from the data by using Supervised Learning. The bootstrapped RL-based policy gains on average 50 times more reward when tested in simulation, and almost 18 times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher. We also show that results from simulated interaction do transfer to interaction with real users, and we explicitly evaluate the stability of the data-driven reward function.", "paper_title": "Learning and Evaluation of Dialogue Strategies for New Applications: Empirical Methods for Optimization from Small Data Sets", "paper_id": "WOS:000288314500006"}