{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "human-human_interactions"}, {"score": 0.049362803242714734, "phrase": "stressful_situations"}, {"score": 0.0046578649614541995, "phrase": "human_operated_service_desks"}, {"score": 0.004574342831225094, "phrase": "human-computer_interfaces"}, {"score": 0.004533142516832776, "phrase": "public_domain"}, {"score": 0.004505881071218599, "phrase": "automatic_surveillance"}, {"score": 0.004438439373358016, "phrase": "extra_assistance"}, {"score": 0.004385210330180571, "phrase": "human_communication"}, {"score": 0.004358834624798868, "phrase": "inherently_multimodal_e._g._speech"}, {"score": 0.004306556130853961, "phrase": "facial_expressions"}, {"score": 0.004229306699561554, "phrase": "automatic_surveillance_systems"}, {"score": 0.004165987300206134, "phrase": "multimodal_information"}, {"score": 0.004116012194714212, "phrase": "automatic_fusion"}, {"score": 0.004017846065981189, "phrase": "unsolved_problem"}, {"score": 0.003863275399791591, "phrase": "audio-visual_recordings"}, {"score": 0.0038054151244558123, "phrase": "service_desk"}, {"score": 0.0037371212381934853, "phrase": "high_degree"}, {"score": 0.003593311381057356, "phrase": "short_scenarios"}, {"score": 0.0034135407979996673, "phrase": "general_stressful_human-human_interaction"}, {"score": 0.0032427347282370225, "phrase": "surveillance_operators"}, {"score": 0.0031653299069848463, "phrase": "hand_gestures"}, {"score": 0.0028563889983942215, "phrase": "good_fusion_strategies"}, {"score": 0.0028050788526778957, "phrase": "basic_modality"}, {"score": 0.0025388705701873075, "phrase": "emotional_function"}, {"score": 0.0024932498388507084, "phrase": "non-emotional_information"}, {"score": 0.002419025278429845, "phrase": "semantic_function"}, {"score": 0.0021049977753042253, "phrase": "clear_indications"}], "paper_keywords": ["Stressful situations dataset", " Speech", " Gestures", " Multimodal fusion", " Multimodal communication"], "paper_abstract": "Stressful situations are likely to occur at human operated service desks, as well as at human-computer interfaces used in public domain. Automatic surveillance can help notifying when extra assistance is needed. Human communication is inherently multimodal e. g. speech, gestures, facial expressions. It is expected that automatic surveillance systems can benefit from exploiting multimodal information. This requires automatic fusion of modalities, which is still an unsolved problem. To support the development of such systems, we present and analyze audio-visual recordings of human-human interactions at a service desk. The corpus has a high degree of realism: all interactions are freely improvised by actors based on short scenarios where only the sources of conflict were provided. The recordings can be considered as a prototype for general stressful human-human interaction. The recordings were annotated on a 5 point scale on degree of stress from the perspective of surveillance operators. The recordings are very rich in hand gestures. We find that themore stressful the situation, the higher the proportion of speech that is accompanied by gestures. Understanding the function of gestures and their relation to speech is essential for good fusion strategies. Taking speech as the basic modality, one of our research questions was, what is the role of gestures in addition to speech. Both speech and gestures can express emotion, so we say that they have an emotional function. They can also express non-emotional information, in which case we say that they have a semantic function. We learn that when speech and gestures have the same function, they are usually congruent, but intensities and clarity can vary. Most gestures in this dataset convey emotion. We identify classes of gestures in our recordings, and argue that some classes are clear indications of stressful situations", "paper_title": "An audio-visual dataset of human-human interactions in stressful situations", "paper_id": "WOS:000337753000004"}