{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "computer_vision"}, {"score": 0.004652016085325825, "phrase": "best_practices"}, {"score": 0.004372442291178139, "phrase": "computer_vision_algorithms"}, {"score": 0.004025531820226182, "phrase": "new_problems"}, {"score": 0.0032510690712452147, "phrase": "greater_attention"}, {"score": 0.0030344301563232944, "phrase": "detailed_empirical_analysis"}, {"score": 0.0027551040319267446, "phrase": "current_best_practices"}, {"score": 0.0023670716500937667, "phrase": "historical_emphasis"}, {"score": 0.0023346234464901978, "phrase": "algorithmic_novelty"}, {"score": 0.002286781334850213, "phrase": "increasing_importance"}, {"score": 0.002224510044064184, "phrase": "particular_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["performance assessment", " performance evaluation", " vision system design"], "paper_abstract": "It is frequently remarked that designers of computer vision algorithms and systems cannot reliably predict how algorithms will respond to new problems. A variety of reasons have been given for this situation and a variety of remedies prescribed in literature. Most of these involve, in some way, paying greater attention to the domain of the problem and to performing detailed empirical analysis. The goal of this paper is to review what we see as current best practices in these areas and also suggest refinements that may benefit the field of computer vision. A distinction is made between the historical emphasis on algorithmic novelty and the increasing importance of validation on particular data sets and problems. (c) 2007 Elsevier Inc. All rights reserved.", "paper_title": "Performance characterization in computer vision: A guide to best practices", "paper_id": "WOS:000253449800005"}