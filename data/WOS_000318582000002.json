{"auto_keywords": [{"score": 0.03461955407645363, "phrase": "action_recognition"}, {"score": 0.008806064798598256, "phrase": "object_affordances"}, {"score": 0.00481495049065317, "phrase": "object_action_space"}, {"score": 0.004694295735569168, "phrase": "joint_problem"}, {"score": 0.004615535808765734, "phrase": "human_action"}, {"score": 0.004538091274791439, "phrase": "kitchen_scenario"}, {"score": 0.004337838898735768, "phrase": "manufacturing_task"}, {"score": 0.0038858235424573517, "phrase": "human_actions"}, {"score": 0.0037352737838811587, "phrase": "scene_parameters"}, {"score": 0.003412626340030075, "phrase": "pointing_direction"}, {"score": 0.0033934090791564856, "phrase": "important_information"}, {"score": 0.0032435035192147645, "phrase": "captured_tracking_data"}, {"score": 0.00302238310602767, "phrase": "intertwined_problem"}, {"score": 0.002840265821464678, "phrase": "human_body_tracking"}, {"score": 0.0028004259937533167, "phrase": "object-driven_perspective"}, {"score": 0.0027455847214706572, "phrase": "human_body"}, {"score": 0.0026316476955518175, "phrase": "possible_actions"}, {"score": 0.0025439032008217704, "phrase": "body_tracking"}, {"score": 0.00239054815963484, "phrase": "high-dimensional_joint-space"}, {"score": 0.0023703634459582546, "phrase": "low-dimensional_action_space"}, {"score": 0.0023173649298618005, "phrase": "parametric_hidden_markov_models"}, {"score": 0.0022977967191969515, "phrase": "parametric_movements"}, {"score": 0.0022848429198019885, "phrase": "particle_filtering"}, {"score": 0.0022274466028861925, "phrase": "action_parameters"}, {"score": 0.002171488970077653, "phrase": "real_image_sequences"}, {"score": 0.0021592456879408937, "phrase": "human-upper_body_single_arm_actions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Action recognition", " Parametric gestures", " Tracking", " Pose estimation"], "paper_abstract": "In this paper we focus on the joint problem of tracking humans and recognizing human action in scenarios such as a kitchen scenario or a scenario where a robot cooperates with a human, e.g., for a manufacturing task. In these scenarios, the human directly interacts with objects physically by using/manipulating them or by, e.g., pointing at them such as in \"Give me that ... \". To recognize these types of human actions is difficult because (a) they ought to be recognized independent of scene parameters such as viewing direction and (b) the actions are parametric, where the parameters are either object-dependent or as, e.g., in the case of a pointing direction convey important information. One common way to achieve recognition is by using 3D human body tracking followed by action recognition based on the captured tracking data. For the kind of scenarios considered here we would like to argue that 3D body tracking and action recognition should be seen as an intertwined problem that is primed by the objects on which the actions are applied. In this paper, we are looking at human body tracking and action recognition from a object-driven perspective. Instead of the space of human body poses we consider the space of the object affordances, i.e., the space of possible actions that are applied on a given object. This way, 3D body tracking reduces to action tracking in the object (and context) primed parameter space of the object affordances. This reduces the high-dimensional joint-space to a low-dimensional action space. In our approach, we use parametric hidden Markov models to represent parametric movements; particle filtering is used to track in the space of action parameters. We demonstrate its effectiveness on synthetic and on real image sequences using human-upper body single arm actions that involve objects. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Tracking in object action space", "paper_id": "WOS:000318582000002"}