{"auto_keywords": [{"score": 0.0320237018545879, "phrase": "different_collections"}, {"score": 0.00481495049065317, "phrase": "different_document_collections"}, {"score": 0.00458940888106061, "phrase": "different_and_unique_aspects"}, {"score": 0.004081385289516914, "phrase": "differential_topic_model"}, {"score": 0.003931798194574009, "phrase": "topic_differences"}, {"score": 0.003767517649628532, "phrase": "hierarchical_bayesian_nonparametric_models"}, {"score": 0.0035338315037521627, "phrase": "power-law_phenomena"}, {"score": 0.0034963129868796033, "phrase": "topic-word_distributions"}, {"score": 0.0033861224940737846, "phrase": "full_pitman-yor_process"}, {"score": 0.003210110095236981, "phrase": "transformed_pitman-yor_process"}, {"score": 0.003108911488613549, "phrase": "prior_knowledge"}, {"score": 0.0030595112110658675, "phrase": "vocabulary_variations"}, {"score": 0.002884979280317279, "phrase": "non-conjugate_issue"}, {"score": 0.0028391271282216758, "phrase": "prior_and_likelihood"}, {"score": 0.002794001681064862, "phrase": "tpyp"}, {"score": 0.0027058852678238632, "phrase": "efficient_sampling_algorithm"}, {"score": 0.002662871928330334, "phrase": "data_augmentation_technique"}, {"score": 0.00260657976055413, "phrase": "multinomial_theorem"}, {"score": 0.002578880352208335, "phrase": "experimental_results"}, {"score": 0.0025109095718850376, "phrase": "interesting_aspects"}, {"score": 0.002405853977978678, "phrase": "proposed_mcmc_based_algorithm"}, {"score": 0.0023675986784123656, "phrase": "dramatically_reduced_test_perplexity"}, {"score": 0.002317533977770783, "phrase": "existing_topic_models"}, {"score": 0.0021049977753042253, "phrase": "text_collections"}], "paper_keywords": ["Differential topic model", " transformed Pitman-Yor process", " MCMC", " data augmentation"], "paper_abstract": "In applications we may want to compare different document collections: they could have shared content but also different and unique aspects in particular collections. This task has been called comparative text mining or cross-collection modeling. We present a differential topic model for this application that models both topic differences and similarities. For this we use hierarchical Bayesian nonparametric models. Moreover, we found it was important to properly model power-law phenomena in topic-word distributions and thus we used the full Pitman-Yor process rather than just a Dirichlet process. Furthermore, we propose the transformed Pitman-Yor process (TPYP) to incorporate prior knowledge such as vocabulary variations in different collections into the model. To deal with the non-conjugate issue between model prior and likelihood in the TPYP, we thus propose an efficient sampling algorithm using a data augmentation technique based on the multinomial theorem. Experimental results show the model discovers interesting aspects of different collections. We also show the proposed MCMC based algorithm achieves a dramatically reduced test perplexity compared to some existing topic models. Finally, we show our model outperforms the state-of-the-art for document classification/ideology prediction on a number of text collections.", "paper_title": "Differential Topic Models", "paper_id": "WOS:000349625500003"}