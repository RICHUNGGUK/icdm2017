{"auto_keywords": [{"score": 0.047176563797730116, "phrase": "vision_systems"}, {"score": 0.03709405079463304, "phrase": "lrf"}, {"score": 0.00481495049065317, "phrase": "laser_range"}, {"score": 0.004803400946534904, "phrase": "finder_and_machine_learning_techniques"}, {"score": 0.004746065479362881, "phrase": "computer_vision"}, {"score": 0.0046781613377273774, "phrase": "industrial_environments"}, {"score": 0.004611224230543916, "phrase": "important_tasks"}, {"score": 0.004589124394375718, "phrase": "quality_control"}, {"score": 0.004259755581609651, "phrase": "robotic_cell"}, {"score": 0.004079423982698244, "phrase": "particular_work_area"}, {"score": 0.00402101925240207, "phrase": "robotic_arm"}, {"score": 0.003963447369437198, "phrase": "hand_vision_system"}, {"score": 0.0038973170902076707, "phrase": "valid_approach"}, {"score": 0.0037864980290391354, "phrase": "industrial_environment_lighting"}, {"score": 0.003705448880626317, "phrase": "new_approach"}, {"score": 0.0036348574445433856, "phrase": "hand_systems"}, {"score": 0.0034392837410790293, "phrase": "robotic_manipulator"}, {"score": 0.0033981753672128615, "phrase": "pre-defined_path"}, {"score": 0.0033737457695128233, "phrase": "grayscale_images"}, {"score": 0.0033015022122332197, "phrase": "environment_lighting_interference"}, {"score": 0.0031163027401070473, "phrase": "different_objects"}, {"score": 0.003101345168761592, "phrase": "inherent_features"}, {"score": 0.0030642636185693054, "phrase": "invariant_moments"}, {"score": 0.0030496362731435075, "phrase": "hu"}, {"score": 0.0029062860108533374, "phrase": "support_vector_machines"}, {"score": 0.002837188425640173, "phrase": "good_performance"}, {"score": 0.0028167803833646695, "phrase": "classification_model"}, {"score": 0.0027965187255008647, "phrase": "wrapper_method"}, {"score": 0.002710384439697906, "phrase": "assessment_model_technique"}, {"score": 0.0026973731274307074, "phrase": "k-fold"}, {"score": 0.002533774961646797, "phrase": "knn"}, {"score": 0.0024497809033380735, "phrase": "high_performances"}, {"score": 0.00242047093248845, "phrase": "feature_selection_algorithm"}, {"score": 0.0023972750359328235, "phrase": "simulated_annealing_heuristic"}, {"score": 0.0022626835039082746, "phrase": "recognition_process"}, {"score": 0.002214178772426202, "phrase": "best_parameters"}, {"score": 0.002198242191529977, "phrase": "machine_learning_models"}, {"score": 0.0021771713131746636, "phrase": "classification_ratio"}, {"score": 0.0021356322417874106, "phrase": "robot's_environment"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Eye-in-hand", " Laser ranger finder", " Machine learning", " Manufacturing robotic manipulator", " Object recognition"], "paper_abstract": "In recent years, computer vision has been widely used on industrial environments, allowing robots to perform important tasks like quality control, inspection and recognition. Vision systems are typically used to determine the position and orientation of objects in the workstation, enabling them to be transported and assembled by a robotic cell (e.g. industrial manipulator). These systems commonly resort to CCD (Charge-Coupled Device) Cameras fixed and located in a particular work area or attached directly to the robotic arm (eye-in-hand vision system). Although it is a valid approach, the performance of these vision systems is directly influenced by the industrial environment lighting. Taking all these into consideration, a new approach is proposed for eye-on-hand systems, where the use of cameras will be replaced by the 2D Laser Range Finder (LRF). The LRF will be attached to a robotic manipulator, which executes a pre-defined path to produce grayscale images of the workstation. With this technique the environment lighting interference is minimized resulting in a more reliable and robust computer vision system. After the grayscale image is created, this work focuses on the recognition and classification of different objects using inherent features (based on the invariant moments of Hu) with the most well-known machine learning models: k-Nearest Neighbor (kNN), Neural Networks (NNs) and Support Vector Machines (SVMs). In order to achieve a good performance for each classification model, a wrapper method is used to select one good subset of features, as well as an assessment model technique called K-fold cross-validation to adjust the parameters of the classifiers. The performance of the models is also compared, achieving performances of 83.5% for kNN, 95.5% for the NN and 98.9% for the SVM (generalized accuracy). These high performances are related with the feature selection algorithm based on the simulated annealing heuristic, and the model assessment (k-fold cross-validation). It makes possible to identify the most important features in the recognition process, as well as the adjustment of the best parameters for the machine learning models, increasing the classification ratio of the work objects present in the robot's environment. (c) 2012 Elsevier Ltd. All rights reserved.", "paper_title": "Object recognition using laser range finder and machine learning techniques", "paper_id": "WOS:000310116400002"}