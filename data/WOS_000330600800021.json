{"auto_keywords": [{"score": 0.04877232614398384, "phrase": "forgery_detection"}, {"score": 0.04320256940116906, "phrase": "gaussian_mixture"}, {"score": 0.039279164506438924, "phrase": "positive_data"}, {"score": 0.03883196830268189, "phrase": "inverted_dirichlet"}, {"score": 0.00481495049065317, "phrase": "finite_generalized_inverted_dirichlet_mixtures"}, {"score": 0.00459186790405058, "phrase": "mixture_models"}, {"score": 0.004484215355348556, "phrase": "flexible_models"}, {"score": 0.004327429667024575, "phrase": "common_assumption"}, {"score": 0.004078157608867258, "phrase": "inverted_dirichlet_mixture"}, {"score": 0.003966776222128288, "phrase": "better_alternative"}, {"score": 0.0038431888954930083, "phrase": "significant_value"}, {"score": 0.0034949740315415043, "phrase": "positive_correlation"}, {"score": 0.0033860360186761533, "phrase": "bayesian_alternative"}, {"score": 0.003319675107116333, "phrase": "inverted_dirichlet_mixtures"}, {"score": 0.003128270147867462, "phrase": "generalized_inverted_dirichlet_distribution"}, {"score": 0.0030913302090179967, "phrase": "high_flexibility"}, {"score": 0.0028110458014332187, "phrase": "proposed_mixture_model"}, {"score": 0.0027559226203857316, "phrase": "fully_bayesian_analysis"}, {"score": 0.002702109840638027, "phrase": "gibbs"}, {"score": 0.002596937660779953, "phrase": "posterior_distribution"}, {"score": 0.0025259082947443343, "phrase": "bayesian_information_criterion"}, {"score": 0.0025060314090796015, "phrase": "bic"}, {"score": 0.00246657044320953, "phrase": "model_selection"}, {"score": 0.002408623172021922, "phrase": "purely_bayesian_learning_choice"}, {"score": 0.0023427323356117365, "phrase": "bayesian_inference"}, {"score": 0.0022696277881969896, "phrase": "unified_and_consistent_manner"}, {"score": 0.002172811149894415, "phrase": "object_classification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Finite mixtures", " Generalized inverted Dirichlet", " Bayesian inference", " BIC", " Model selection", " Gibbs sampling", " Object classification", " Forgery detection"], "paper_abstract": "The advent of mixture models has opened the possibility of flexible models which are practical to work with. A common assumption is that practitioners typically expect that data are generated from a Gaussian mixture. The inverted Dirichlet mixture has been shown to be a better alternative to the Gaussian mixture and to be of significant value in a variety of applications involving positive data. The inverted Dirichlet is, however, usually undesirable, since it forces an assumption of positive correlation. Our focus here is to develop a Bayesian alternative to both the Gaussian and the inverted Dirichlet mixtures when dealing with positive data. The alternative that we propose is based on the generalized inverted Dirichlet distribution which offers high flexibility and ease of use, as we show in this paper. Moreover, it has a more general covariance structure than the inverted Dirichlet. The proposed mixture model is subjected to a fully Bayesian analysis based on Markov Chain Monte Carlo (MCMC) simulation methods namely Gibbs sampling and Metropolis-Hastings used to compute the posterior distribution of the parameters, and on Bayesian information criterion (BIC) used for model selection. The adoption of this purely Bayesian learning choice is motivated by the fact that Bayesian inference allows to deal with uncertainty in a unified and consistent manner. We evaluate our approach on the basis of two challenging applications concerning object classification and forgery detection. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Bayesian learning of finite generalized inverted Dirichlet mixtures: Application to object classification and forgery detection", "paper_id": "WOS:000330600800021"}