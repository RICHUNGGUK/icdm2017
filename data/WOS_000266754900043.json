{"auto_keywords": [{"score": 0.038827309579548566, "phrase": "feasible_space"}, {"score": 0.03165084894746998, "phrase": "som"}, {"score": 0.00481495049065317, "phrase": "kohonen's_neural_map"}, {"score": 0.004706453355638515, "phrase": "population_based_global_search_methods"}, {"score": 0.0046398818698302835, "phrase": "local_optima_traps"}, {"score": 0.004587300172928083, "phrase": "global_optima_regions"}, {"score": 0.0043703658695579085, "phrase": "search_strategy"}, {"score": 0.004259684990957109, "phrase": "random_search"}, {"score": 0.004223415145617665, "phrase": "optimum_sets"}, {"score": 0.0041636475197809825, "phrase": "small_probability"}, {"score": 0.004128192012388041, "phrase": "current_solution"}, {"score": 0.004035107125458193, "phrase": "genetic_drift"}, {"score": 0.004000741805312148, "phrase": "gas_search_process"}, {"score": 0.003966667994579941, "phrase": "black_box_process"}, {"score": 0.0037682108720193956, "phrase": "gas_search"}, {"score": 0.003569451476205552, "phrase": "existing_information"}, {"score": 0.0035390378585554547, "phrase": "optimality_regions"}, {"score": 0.0034493376812017983, "phrase": "new_method"}, {"score": 0.0032860401904845522, "phrase": "genetic_diversity"}, {"score": 0.003258033491409002, "phrase": "sbmoga"}, {"score": 0.0031393948330209255, "phrase": "self-organizing_map"}, {"score": 0.00308610073274614, "phrase": "variable_neighborhood_search"}, {"score": 0.0029652292779277782, "phrase": "neural_network"}, {"score": 0.0028653893877383188, "phrase": "data_processing_algorithms"}, {"score": 0.0028409576481842457, "phrase": "vns_algorithm"}, {"score": 0.0027927155990963063, "phrase": "local_search_efficiency"}, {"score": 0.0027689017918347755, "phrase": "evolutionary_algorithms"}, {"score": 0.0026986685585382347, "phrase": "multi-objective_learning_rule"}, {"score": 0.0026680267928092167, "phrase": "pareto_dominance"}, {"score": 0.002592931959322531, "phrase": "better_fitness_areas"}, {"score": 0.002519945420045647, "phrase": "optimum_front"}, {"score": 0.002442025065383455, "phrase": "final_state"}, {"score": 0.002386869729952315, "phrase": "new_solutions"}, {"score": 0.002339629270626007, "phrase": "probability_density_distribution_function"}, {"score": 0.0023196699954340437, "phrase": "high_fitness_areas"}, {"score": 0.002299880599424647, "phrase": "multi-objective_space"}, {"score": 0.0022802596433757565, "phrase": "new_set"}, {"score": 0.002235124462642752, "phrase": "gas_overall_efficiency"}, {"score": 0.002209734300698144, "phrase": "last_section"}, {"score": 0.002153653728048424, "phrase": "proposed_algorithm"}, {"score": 0.002123113864239043, "phrase": "optimal_policies"}, {"score": 0.0021049977753042253, "phrase": "real_world_multi-objective_multi-reservoir_system"}], "paper_keywords": ["Multi-objective genetic local search", " Self organizing maps", " Variable Neighborhood Search (VNS)", " Multi-objective evolutionary algorithm", " Learning", " Multi-reservoir operation management"], "paper_abstract": "Genetic Algorithms (GAs) are population based global search methods that can escape from local optima traps and find the global optima regions. However, near the optimum set their intensification process is often inaccurate. This is because the search strategy of GAs is completely probabilistic. With a random search near the optimum sets, there is a small probability to improve current solution. Another drawback of the GAS is genetic drift. The GAs search process is a black box process and no one knows that which region is being searched by the algorithm and it is possible that GAs search only a small region in the feasible space. On the other hand, GAs usually do not use the existing information about the optimality regions in past iterations. In this paper, a new method called SOM-Based Multi-Objective GA (SBMOGA) is proposed to improve the genetic diversity. In SBMOGA, a grid of neurons use the concept of learning rule of Self-Organizing Map (SOM) supporting by Variable Neighborhood Search (VNS) learn from genetic algorithm improving both local and global search. SOM is a neural network which is capable of learning and can improve the efficiency of data processing algorithms. The VNS algorithm is developed to enhance the local search efficiency in the Evolutionary Algorithms (EAs). The SOM uses a multi-objective learning rule based-on Pareto dominance to train its neurons. The neurons gradually move toward better fitness areas in some trajectories in feasible space. The knowledge of optimum front in past generations is saved in form of trajectories. The final state of the neurons determines a set of new solutions that can be regarded as the probability density distribution function of the high fitness areas in the multi-objective space. The new set of solutions potentially can improve the GAs overall efficiency. In the last section of this paper, the applicability of the proposed algorithm is examined in developing optimal policies for a real world multi-objective multi-reservoir system which is a non-linear, non-convex, multi-objective optimization problem. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "Multi-objective genetic local search algorithm using Kohonen's neural map", "paper_id": "WOS:000266754900043"}