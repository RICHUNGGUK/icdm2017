{"auto_keywords": [{"score": 0.04424296209239414, "phrase": "spoken_document"}, {"score": 0.0363392912425693, "phrase": "ncuts"}, {"score": 0.00481495049065317, "phrase": "spoken_documents"}, {"score": 0.004767261858417823, "phrase": "self-validated_acoustic_cuts"}, {"score": 0.004604023526673226, "phrase": "necessary_prerequisite"}, {"score": 0.004272724011826495, "phrase": "topic_segmentation"}, {"score": 0.0036980501827538455, "phrase": "real-world_applications"}, {"score": 0.0036070601923916196, "phrase": "multimedia_data"}, {"score": 0.003500812175550438, "phrase": "previous_lexical-based_spoken_document_segmentation_approaches"}, {"score": 0.003380791705551809, "phrase": "text_transcripts"}, {"score": 0.0033140589981208693, "phrase": "large_vocabulary_continuous_speech_recognizer"}, {"score": 0.0031061129850110994, "phrase": "large_amount"}, {"score": 0.003075296862372287, "phrase": "transcribed_speech_data"}, {"score": 0.003044785536429499, "phrase": "language-specific_knowledges"}, {"score": 0.002984665318717504, "phrase": "inevitable_speech_recognition_errors"}, {"score": 0.0027420820738468577, "phrase": "self-validated_acoustic_normalized_cuts_approach"}, {"score": 0.002701361707441567, "phrase": "sacuts"}, {"score": 0.0025317627242523104, "phrase": "topic_number"}, {"score": 0.002457110101089137, "phrase": "extra_computation_load"}, {"score": 0.0023727863021828547, "phrase": "lexical_approaches"}, {"score": 0.0023143284925318916, "phrase": "high-resource_speech"}, {"score": 0.0022460727898764216, "phrase": "comparable_and_even_better_segmentation_performance"}, {"score": 0.002168975681697056, "phrase": "broadcast_news_topic_segmentation_task"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["Topic segmentation", " Story segmentation", " Topic boundary detection", " Spoken document retrieval", " Normalized cuts"], "paper_abstract": "Topic segmentation serves as a necessary prerequisite for multimedia content analysis and management. The normalized cuts (NCuts) approach has shown superior performance in topic segmentation of spoken document. However, in this method, the number of topics in a document has to be known prior to segmentation. This is impractical for real-world applications with exponential growth of multimedia data. On the other hand, previous lexical-based spoken document segmentation approaches, including NCuts, work on text transcripts generated by a large vocabulary continuous speech recognizer (LVCSR). As we know, training such a recognizer requires a large amount of transcribed speech data and language-specific knowledges. Moreover, inevitable speech recognition errors and the out-of-vocabulary (OOV) problem apparently affect the segmentation performance. This paper addresses these problems by a self-validated acoustic normalized cuts approach, namely SACuts. First, as compared with NCuts, our approach can determine the topic number in a spoken document automatically without extra computation load. Second, as compared with lexical approaches that rely on a high-resource speech recognizer, our approach can achieve comparable and even better segmentation performance using only acoustic-level information. Evaluation on a broadcast news topic segmentation task shows the superiority of the proposed approach.", "paper_title": "Topic segmentation on spoken documents using self-validated acoustic cuts", "paper_id": "WOS:000347407500006"}