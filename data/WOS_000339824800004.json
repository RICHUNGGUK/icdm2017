{"auto_keywords": [{"score": 0.04625336728938103, "phrase": "query_terms"}, {"score": 0.029864978370869735, "phrase": "proximity_model"}, {"score": 0.02791408517466667, "phrase": "existing_models"}, {"score": 0.00481495049065317, "phrase": "ranking_documents"}, {"score": 0.0047268728636719725, "phrase": "information_retrieval_process"}, {"score": 0.004211594861013822, "phrase": "joint_presence"}, {"score": 0.003911578552479064, "phrase": "independent_terms"}, {"score": 0.0037006243674469657, "phrase": "co-occurring_terms"}, {"score": 0.0034528025626422154, "phrase": "proximity_information"}, {"score": 0.0031625006830124512, "phrase": "co-occurring_query_terms"}, {"score": 0.0028566300478906916, "phrase": "unigram_ranking_function"}, {"score": 0.0027275289970185015, "phrase": "occurring_term_combinations"}, {"score": 0.0025329623864528317, "phrase": "co-occurrence_statistics"}, {"score": 0.0024409345168575833, "phrase": "additional_parameters"}, {"score": 0.0023851187337905412, "phrase": "retrieval_speed"}, {"score": 0.0023522423291484212, "phrase": "competing_models"}, {"score": 0.0021945194903838132, "phrase": "web_and_newswire_corpora"}, {"score": 0.002154272413276506, "phrase": "average_performs"}, {"score": 0.0021049977753042253, "phrase": "existing_proximity_models"}], "paper_keywords": ["Term dependency", " Term proximity", " Query expansion"], "paper_abstract": "In the information retrieval process, functions that rank documents according to their estimated relevance to a query typically regard query terms as being independent. However, it is often the joint presence of query terms that is of interest to the user, which is overlooked when matching independent terms. One feature that can be used to express the relatedness of co-occurring terms is their proximity in text. In past research, models that are trained on the proximity information in a collection have performed better than models that are not estimated on data. We analyzed how co-occurring query terms can be used to estimate the relevance of documents based on their distance in text, which is used to extend a unigram ranking function with a proximity model that accumulates the scores of all occurring term combinations. This proximity model is more practical than existing models, since it does not require any co-occurrence statistics, it obviates the need to tune additional parameters, and has a retrieval speed close to competing models. We show that this approach is more robust than existing models, on both Web and newswire corpora, and on average performs equal or better than existing proximity models across collections.", "paper_title": "Distance matters! Cumulative proximity expansions for ranking documents", "paper_id": "WOS:000339824800004"}