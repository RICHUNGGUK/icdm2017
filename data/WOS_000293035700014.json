{"auto_keywords": [{"score": 0.03010368972447254, "phrase": "proposed_algorithm"}, {"score": 0.00481495049065317, "phrase": "architecture_design"}, {"score": 0.004610058444728656, "phrase": "novel_algorithm"}, {"score": 0.0045108906244892165, "phrase": "adaptive_resonance_method"}, {"score": 0.004203030435498069, "phrase": "self-organized_design"}, {"score": 0.0038737214010638745, "phrase": "sensitivity_regions"}, {"score": 0.003668591097652231, "phrase": "redundant_neurons"}, {"score": 0.0034180189629740426, "phrase": "additional_categories"}, {"score": 0.003344407169522133, "phrase": "dynamic_programming-based_reinforcement_learning_method"}, {"score": 0.0031845067058084583, "phrase": "learned_action-value_function"}, {"score": 0.003032228032253871, "phrase": "i.e._the_optimal_action-value_function"}, {"score": 0.0028096544129939277, "phrase": "arm_functions"}, {"score": 0.002719325177422626, "phrase": "input_vectors"}, {"score": 0.002646267286675984, "phrase": "clustered_results"}, {"score": 0.0025611773887867255, "phrase": "q-learning_design"}, {"score": 0.0024386325347607674, "phrase": "optimum_actions"}, {"score": 0.0023730978395564116, "phrase": "simulation_results"}, {"score": 0.0023346234464901978, "phrase": "well-known_control_algorithm"}, {"score": 0.002284290601837099, "phrase": "inverted_pendulum"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Reinforcement learning", " Classification", " Q-Learning", " Self-organizing"], "paper_abstract": "This work describes a novel algorithm that integrates an adaptive resonance method (ARM), i.e. an ART-based algorithm with a self-organized design, and a Q-learning algorithm. By dynamically adjusting the size of sensitivity regions of each neuron and adaptively eliminating one of the redundant neurons, ARM can preserve resources, i.e. available neurons, to accommodate additional categories. As a dynamic programming-based reinforcement learning method, Q-learning involves use of the learned action-value function, Q which directly approximates Q*, i.e. the optimal action-value function, which is independent of the policy followed. In the proposed algorithm, ARM functions as a cluster to classify input vectors from the outside world. Clustered results are then sent to the Q-learning design in order to learn how to implement the optimum actions to the outside world. Simulation results of the well-known control algorithm of balancing an inverted pendulum on a cart demonstrates the effectiveness of the proposed algorithm. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "Self-organizing state aggregation for architecture design of Q-learning", "paper_id": "WOS:000293035700014"}