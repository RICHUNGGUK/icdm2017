{"auto_keywords": [{"score": 0.04961597900198373, "phrase": "spatiotemporal_planning"}, {"score": 0.015420339390084655, "phrase": "forest_ecosystem_management"}, {"score": 0.00481495049065317, "phrase": "equilibrium_policy_gradients"}, {"score": 0.004564291332315995, "phrase": "multiple_locations"}, {"score": 0.004454691230204595, "phrase": "planning_horizon"}, {"score": 0.003721098220825489, "phrase": "utility_models"}, {"score": 0.003392507399258166, "phrase": "large_contiguous_areas"}, {"score": 0.003278878522165815, "phrase": "forestry_researchers"}, {"score": 0.0032471155306148126, "phrase": "detailed_dynamics"}, {"score": 0.0031845067058084583, "phrase": "inaccesible_black_boxes"}, {"score": 0.0030778231027679464, "phrase": "factored_markov_decision_process"}, {"score": 0.003018468582368033, "phrase": "policy_gradient_planning_algorithm"}, {"score": 0.0029602552900111407, "phrase": "stochastic_spatial_policy"}, {"score": 0.002931569771634119, "phrase": "simulated_dynamics"}, {"score": 0.0028471655497661528, "phrase": "environmental_and_resource_planning"}, {"score": 0.002778682854371595, "phrase": "different_locations"}, {"score": 0.0025829353063908256, "phrase": "global_spatial_policy"}, {"score": 0.0025207920578878894, "phrase": "local_policies"}, {"score": 0.0023776656246300063, "phrase": "nearby_locations"}, {"score": 0.0023546121947302877, "phrase": "markov_chain_monte_carlo_simulation"}, {"score": 0.002297949552105756, "phrase": "landscape_policies"}, {"score": 0.0021780349459687622, "phrase": "forestry_planning_problem"}, {"score": 0.0021049977753042253, "phrase": "value_models"}], "paper_keywords": ["Markov decision processes", " reinforcement learning", " machine learning", " policy gradient planning", " computational sustainability", " ecosystem management", " forestry planning", " optimization"], "paper_abstract": "Spatiotemporal planning involves making choices at multiple locations in space over some planning horizon to maximize utility and satisfy various constraints. In Forest Ecosystem Management, the problem is to choose actions for thousands of locations each year including harvesting, treating trees for fire or pests, or doing nothing. The utility models could place value on sale of lumber, ecosystem sustainability or employment levels and incorporate legal and logistical constraints on actions such as avoiding large contiguous areas of clearcutting. Simulators developed by forestry researchers provide detailed dynamics but are generally inaccesible black boxes. We model spatiotemporal planning as a factored Markov decision process and present a policy gradient planning algorithm to optimize a stochastic spatial policy using simulated dynamics. It is common in environmental and resource planning to have actions at different locations be spatially interelated; this makes representation and planning challenging. We define a global spatial policy in terms of interacting local policies defining distributions over actions at each location conditioned on actions at nearby locations. Markov chain Monte Carlo simulation is used to sample landscape policies and estimate their gradients. Evaluation is carried out on a forestry planning problem with 1,880 locations using a variety of value models and constraints.", "paper_title": "Using Equilibrium Policy Gradients for Spatiotemporal Planning in Forest Ecosystem Management", "paper_id": "WOS:000328643100013"}