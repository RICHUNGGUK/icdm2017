{"auto_keywords": [{"score": 0.03533273623232516, "phrase": "sm_algorithms"}, {"score": 0.00481495049065317, "phrase": "surrogate_maximization"}, {"score": 0.0041251561573501455, "phrase": "sm_algorithm"}, {"score": 0.004016594765266821, "phrase": "otherwise_intractable_maximization_problem"}, {"score": 0.003952828685048176, "phrase": "tractable_one"}, {"score": 0.003767517649628532, "phrase": "tractable_surrogate_function"}, {"score": 0.003687960013746783, "phrase": "original_objective_function"}, {"score": 0.0035338315037521627, "phrase": "surrogate_function"}, {"score": 0.003496849402231901, "phrase": "convexity"}, {"score": 0.0034407781868385423, "phrase": "central_role"}, {"score": 0.003261934055562238, "phrase": "em_algorithms"}, {"score": 0.0031930184711888867, "phrase": "mainly_three_approaches"}, {"score": 0.003108911488613549, "phrase": "surrogate_functions"}, {"score": 0.003010893527930816, "phrase": "jensen's_inequality"}, {"score": 0.0029789104204803137, "phrase": "first-order_taylor_approximation"}, {"score": 0.0029159568444629053, "phrase": "low_quadratic_bound_principle"}, {"score": 0.0027058852678238632, "phrase": "logistic_regression_models"}, {"score": 0.0026771648781861194, "phrase": "adaboost"}, {"score": 0.002470987929071079, "phrase": "different_surrogate_function_construction_methods"}, {"score": 0.0023675986784123656, "phrase": "standard_sm"}, {"score": 0.002256435842862802, "phrase": "quadratic_sm_algorithms"}, {"score": 0.002185235873435714, "phrase": "conditional_surrogate_maximization"}, {"score": 0.0021276181317652163, "phrase": "surrogate_conditional_maximization"}, {"score": 0.0021050209733277088, "phrase": "scm"}], "paper_keywords": ["surrogate function", " convexity", " logistic regression", " Adaboost", " log-linear model"], "paper_abstract": "Surrogate maximization (or minimization) (SM) algorithms are a family of algorithms that can be regarded as a generalization of expectation-maximization (EM) algorithms. An SM algorithm aims at turning an otherwise intractable maximization problem into a tractable one by iterating two steps. The S-step computes a tractable surrogate function to substitute the original objective function and the M-step seeks to maximize this surrogate function. Convexity plays a central role in the S-step. SM algorithms enjoy the same convergence properties as EM algorithms. There are mainly three approaches to the construction of surrogate functions, namely, by using Jensen's inequality, first-order Taylor approximation, and the low quadratic bound principle. In this paper, we demonstrate the usefulness of SM algorithms by taking logistic regression models, AdaBoost and the log-linear model as examples. More specifically, by using different surrogate function construction methods, we devise several SM algorithms, including the standard SM, generalized SM, gradient SM, and quadratic SM algorithms, and their two variants called the conditional surrogate maximization (CSM) and surrogate conditional maximization (SCM) algorithms.", "paper_title": "Surrogate maximization/minimization algorithms and extensions", "paper_id": "WOS:000249951700001"}