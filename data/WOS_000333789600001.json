{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "cloud"}, {"score": 0.0047587234712024775, "phrase": "cooperative_tracking"}, {"score": 0.004666460260366703, "phrase": "simultaneous_localization"}, {"score": 0.004575977644865379, "phrase": "autonomous_mobile_robot"}, {"score": 0.004452228677941085, "phrase": "computationally_demanding_process"}, {"score": 0.004417487764326481, "phrase": "medium_and_large-scale_scenarios"}, {"score": 0.00424779023663186, "phrase": "algorithmic_and_hardware_sides"}, {"score": 0.004100620365805411, "phrase": "slam_capabilities"}, {"score": 0.003989675045520656, "phrase": "latest_computers"}, {"score": 0.003943047070755799, "phrase": "power_consumption"}, {"score": 0.003806395038279074, "phrase": "visual_slam_system"}, {"score": 0.0037471850931318942, "phrase": "distributed_framework"}, {"score": 0.003703380599999214, "phrase": "expensive_map"}, {"score": 0.003491894048789576, "phrase": "light_camera"}, {"score": 0.0034782306784111494, "phrase": "tracking_client"}, {"score": 0.003424107910734022, "phrase": "local_computer"}, {"score": 0.0033708244679086265, "phrase": "onboard_computers"}, {"score": 0.003203294948113915, "phrase": "internet_connection"}, {"score": 0.0031658283119347396, "phrase": "data_flow"}, {"score": 0.0029966788947444535, "phrase": "standard_wireless_connection"}, {"score": 0.0029616217177058602, "phrase": "experimental_section"}, {"score": 0.0029040994533437903, "phrase": "real-time_performance"}, {"score": 0.002792375587031108, "phrase": "rgbd_camera"}, {"score": 0.0026954938318408464, "phrase": "map_database"}, {"score": 0.0025215486544004134, "phrase": "stored_maps"}, {"score": 0.002232675154416128, "phrase": "individual_maps"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["SLAM", " Visual SLAM", " Cloud SLAM", " Cloud Robotics", " Cloud Computing"], "paper_abstract": "The Simultaneous Localization And Mapping by an autonomous mobile robot known by its acronym SLAM is a computationally demanding process for medium and large-scale scenarios, in spite of the progress both in the algorithmic and hardware sides. As a consequence, a robot with SLAM capabilities has to be equipped with the latest computers whose weight and power consumption might limit its autonomy. This paper describes a visual SLAM system based on a distributed framework where the expensive map optimization and storage is allocated as a service in the Cloud, while a light camera tracking client runs on a local computer. The robot onboard computers are freed from most of the computation, the only extra requirement being an internet connection. The data flow from and to the Cloud is low enough to be supported by a standard wireless connection. The experimental section is focused on showing real-time performance for single-robot and cooperative SLAM using an RGBD camera. The system provides the interface to a map database where: (1) a map can be built and stored, (2) stored maps can be reused by other robots, (3) a robot can fuse its map online with a map already in the database, and (4) several robots can estimate individual maps and fuse them together if an overlap is detected. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "C(2)TAM: A Cloud framework for cooperative tracking and mapping", "paper_id": "WOS:000333789600001"}