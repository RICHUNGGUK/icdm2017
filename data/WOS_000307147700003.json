{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "decision_tree"}, {"score": 0.014180577875108508, "phrase": "proposed_method"}, {"score": 0.010111896808799974, "phrase": "rl"}, {"score": 0.008356301495992759, "phrase": "state_space"}, {"score": 0.007693560748866818, "phrase": "long-term_evaluations"}, {"score": 0.004692647062494403, "phrase": "tree-construction_method"}, {"score": 0.0046128342233041405, "phrase": "reinforcement_learning"}, {"score": 0.004356424011261929, "phrase": "optimal_policy"}, {"score": 0.004270074571192451, "phrase": "maximal_accumulated_information_gain"}, {"score": 0.004233591368742499, "phrase": "proposed_approach"}, {"score": 0.004149666468555799, "phrase": "first_stage"}, {"score": 0.0040096320322921715, "phrase": "training_patterns"}, {"score": 0.003863235056582662, "phrase": "emulation_data"}, {"score": 0.0037221632527328183, "phrase": "state_aggregation"}, {"score": 0.003535267442543166, "phrase": "split_estimation_phase"}, {"score": 0.003445356410458517, "phrase": "visited_nodes"}, {"score": 0.0034061309156482407, "phrase": "second_phase"}, {"score": 0.0033194932199928267, "phrase": "predicted_long-term_evaluations"}, {"score": 0.0032536298009154052, "phrase": "neural_network_model"}, {"score": 0.0032165800743224645, "phrase": "second_stage"}, {"score": 0.003189069023374276, "phrase": "learned_behavior"}, {"score": 0.003125785288138378, "phrase": "rl_scheme"}, {"score": 0.003099048380523229, "phrase": "discretized_state_space"}, {"score": 0.003028859354852274, "phrase": "previous_stage"}, {"score": 0.0029943614447192775, "phrase": "conventional_greedy_procedure"}, {"score": 0.0028932006069319386, "phrase": "sequential_process"}, {"score": 0.0028766749330297616, "phrase": "tree_induction"}, {"score": 0.002860243381361729, "phrase": "policy_iterations"}, {"score": 0.0028195727965553367, "phrase": "node_split"}, {"score": 0.0027556959972300245, "phrase": "optimal_or_near-optimal_policy"}, {"score": 0.0027165079504297726, "phrase": "splitting_criterion"}, {"score": 0.002685558057339722, "phrase": "action_policy"}, {"score": 0.0026097132486330075, "phrase": "immediate_evaluations"}, {"score": 0.0025579011628578062, "phrase": "cart"}, {"score": 0.0023404848520465543, "phrase": "so-called_tree-based_reinforcement_learning_method"}, {"score": 0.0022874367871565083, "phrase": "discrete_state_space"}, {"score": 0.0022101055924375725, "phrase": "high_performance"}, {"score": 0.0021911838824228676, "phrase": "proposed_system"}, {"score": 0.002172423816988844, "phrase": "state_partition"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Decision trees", " Reinforcement learning", " Critic-actor model", " CART", " State space partition", " Q-learning"], "paper_abstract": "This paper develops a tree-construction method based on the framework of reinforcement learning (RL). The induction of a decision tree is regarded as a problem of RL, where the optimal policy should be found to obtain the maximal accumulated information gain. The proposed approach consists of two stages. At the first stage, the emulation/demonstration stage, sensory-action data in a mechatronic system or samples of training patterns are generated by an operator or stimulator. The records of these emulation data are aggregated into components of the state space represented by a decision tree. State aggregation for decartelization of a state space consists of two phases: split estimation and tree growing. In the split estimation phase, an inducer estimates long-term evaluations of splits at visited nodes. In the second phase, the inducer grows the tree by the predicted long-term evaluations, which is approximated by a neural network model. At the second stage, the learned behavior or classifier is shaped by the RL scheme with a discretized state space constructed by the decision tree derived from the previous stage. Unlike the conventional greedy procedure for constructing and pruning a the tree, the proposed method casts the sequential process of tree induction to policy iterations, where policies for node split are evaluated and improved repeatedly until an optimal or near-optimal policy is obtained. The splitting criterion regarded as an action policy is based on long-term evaluations of payoff instead of using immediate evaluations on impurity. A comparison with CART (classification and regression tree) and C4.5 on several benchmark datasets is presented. Furthermore, to show its applications for learning control, the proposed method is applied further to construct a so-called tree-based reinforcement learning method, where the mechanism works with a discrete state space derived from the proposed method. The results show the feasibility and high performance of the proposed system as a state partition by comparison with the renowned Adaptive Heuristic Critic (AHC) model. (C) 2012 Elsevier Inc. All rights reserved.", "paper_title": "Induced states in a decision tree constructed by Q-learning", "paper_id": "WOS:000307147700003"}