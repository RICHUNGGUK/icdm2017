{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "controlled_experiments"}, {"score": 0.022832022612627116, "phrase": "human_participants"}, {"score": 0.008067795309504043, "phrase": "tool_evaluation"}, {"score": 0.004773040022691547, "phrase": "software_engineering_tools"}, {"score": 0.004690305164360866, "phrase": "empirical_studies"}, {"score": 0.0044118153390803405, "phrase": "software_engineering_research"}, {"score": 0.0042415434489566995, "phrase": "new_software_engineering_tools"}, {"score": 0.00407781620412982, "phrase": "new_tools"}, {"score": 0.003852371777780562, "phrase": "serious_validity_concerns"}, {"score": 0.003818807671883534, "phrase": "recent_research"}, {"score": 0.003408091901859624, "phrase": "inconclusive_or_negative_results"}, {"score": 0.0030148941188459987, "phrase": "practical_methodological_guidance"}, {"score": 0.002835599469639296, "phrase": "empirical_literature"}, {"score": 0.0027620516806198354, "phrase": "practical_perspective"}, {"score": 0.0026321198895843173, "phrase": "informed_consent"}, {"score": 0.0026091594819333654, "phrase": "experimental_procedures"}, {"score": 0.0025863988422430797, "phrase": "demographic_measurements"}, {"score": 0.0025638362414466278, "phrase": "group_assignment"}, {"score": 0.0024113009621042677, "phrase": "common_outcome_variables"}, {"score": 0.0023079225317714815, "phrase": "study_debriefing"}, {"score": 0.0021801135036476136, "phrase": "new_systematic_review"}, {"score": 0.0021516366862117707, "phrase": "tool_evaluations"}], "paper_keywords": ["Research methodology", " Tools", " Human participants", " Human subjects", " Experiments"], "paper_abstract": "Empirical studies, often in the form of controlled experiments, have been widely adopted in software engineering research as a way to evaluate the merits of new software engineering tools. However, controlled experiments involving human participants actually using new tools are still rare, and when they are conducted, some have serious validity concerns. Recent research has also shown that many software engineering researchers view this form of tool evaluation as too risky and too difficult to conduct, as they might ultimately lead to inconclusive or negative results. In this paper, we aim both to help researchers minimize the risks of this form of tool evaluation, and to increase their quality, by offering practical methodological guidance on designing and running controlled experiments with developers. Our guidance fills gaps in the empirical literature by explaining, from a practical perspective, options in the recruitment and selection of human participants, informed consent, experimental procedures, demographic measurements, group assignment, training, the selecting and design of tasks, the measurement of common outcome variables such as success and time on task, and study debriefing. Throughout, we situate this guidance in the results of a new systematic review of the tool evaluations that were published in over 1,700 software engineering papers published from 2001 to 2011.", "paper_title": "A practical guide to controlled experiments of software engineering tools with human participants", "paper_id": "WOS:000348381800004"}