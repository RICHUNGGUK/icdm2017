{"auto_keywords": [{"score": 0.049226991195811136, "phrase": "sparse_representation"}, {"score": 0.0363179781431973, "phrase": "multitask_learning"}, {"score": 0.032942068144253, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "recent_researches"}, {"score": 0.004595263305771512, "phrase": "art_super-resolution_image_reconstruction"}, {"score": 0.004110831924517065, "phrase": "sparser_presentation"}, {"score": 0.0040086845241492424, "phrase": "prototype_patches"}, {"score": 0.003798231095121328, "phrase": "hr_images"}, {"score": 0.0036247638406450735, "phrase": "redundant_dictionary_learning"}, {"score": 0.0035858673522839407, "phrase": "single-image_super-resolution_reconstruction"}, {"score": 0.0034220655476652683, "phrase": "compact_redundant_dictionaries"}, {"score": 0.0033370544263639987, "phrase": "k-means"}, {"score": 0.0032075033252058835, "phrase": "image_reconstruction"}, {"score": 0.003150319576043518, "phrase": "available_srir_methods"}, {"score": 0.003006353490112716, "phrase": "example_patches"}, {"score": 0.0029210388852074208, "phrase": "based_srir"}, {"score": 0.00284837132359445, "phrase": "intensive_computation_complexity"}, {"score": 0.0028177825595998023, "phrase": "enormous_dictionary"}, {"score": 0.002708399936381754, "phrase": "hr_image_examples"}, {"score": 0.00267931029131157, "phrase": "similar_hr_images"}, {"score": 0.0026505322540093783, "phrase": "better_reconstruction_result"}, {"score": 0.002584576723303367, "phrase": "offline_dictionaries"}, {"score": 0.0025112011945912327, "phrase": "rapid_reconstruction"}, {"score": 0.002396371892825691, "phrase": "natural_images"}, {"score": 0.00232832698253493, "phrase": "small_set"}, {"score": 0.0023116191163488824, "phrase": "randomly_chosen_raw_patches"}, {"score": 0.0022703709180757576, "phrase": "small_number"}, {"score": 0.0022298571016688335, "phrase": "good_reconstruction_result"}, {"score": 0.002190064650401852, "phrase": "numerical_guidelines"}, {"score": 0.0021509807762081145, "phrase": "start-of-art_srir_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Super-resolution", " Sparse representation", " Dictionary learning", " Multitask learning"], "paper_abstract": "Recent researches have shown that the sparse representation based technology can lead to state of art super-resolution image reconstruction (SRIR) result. It relies on the idea that the low-resolution (LR) image patches can be regarded as down sampled version of high-resolution (HR) images, whose patches are assumed to have a sparser presentation with respect to a dictionary of prototype patches. In order to avoid a large training patches database and obtain more accurate recovery of HR images, in this paper we introduce the concept of examples-aided redundant dictionary learning into the single-image super-resolution reconstruction, and propose a multiple dictionaries learning scheme inspired by multitask learning. Compact redundant dictionaries are learned from samples classified by K-means clustering in order to provide each sample a more appropriate dictionary for image reconstruction. Compared with the available SRIR methods, the proposed method has the following characteristics: (1) introducing the example patches-aided dictionary learning in the sparse representation based SRIR, in order to reduce the intensive computation complexity brought by enormous dictionary, (2) using the multitask learning and prior from HR image examples to reconstruct similar HR images to obtain better reconstruction result and (3) adopting the offline dictionaries learning and online reconstruction, making a rapid reconstruction possible. Some experiments are taken on testing the proposed method on some natural images, and the results show that a small set of randomly chosen raw patches from training images and small number of atoms can produce good reconstruction result. Both the visual result and the numerical guidelines prove its superiority to some start-of-art SRIR methods. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Multitask dictionary learning and sparse representation based single-image super-resolution reconstruction", "paper_id": "WOS:000296212400050"}