{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multimodal_aspects"}, {"score": 0.004768378525561471, "phrase": "interactional_language_use"}, {"score": 0.004722254884093624, "phrase": "high-quality_multimodal_resources"}, {"score": 0.004586531354697466, "phrase": "vast_amount"}, {"score": 0.004542158659382197, "phrase": "available_written_language_corpora"}, {"score": 0.004454691230204595, "phrase": "transcribed_spoken_language"}, {"score": 0.004326624385772596, "phrase": "visual_as_well_as_auditory_data"}, {"score": 0.0040027552011993005, "phrase": "high-quality_and_multiple-angle_video_recordings"}, {"score": 0.003757480190200645, "phrase": "new_multimodal_corpus_design"}, {"score": 0.0036493838225192883, "phrase": "existing_resources"}, {"score": 0.003359647377874098, "phrase": "full_view"}, {"score": 0.003310951184365485, "phrase": "dialogue_partners'_gestural_behaviour"}, {"score": 0.003215659239270654, "phrase": "facial_expressions"}, {"score": 0.0031845067058084613, "phrase": "body_posture"}, {"score": 0.0030778231027679434, "phrase": "participant_perspective"}, {"score": 0.0029747028229638625, "phrase": "head-mounted_scene_cameras"}, {"score": 0.0027651847105654363, "phrase": "detailed_production_information"}, {"score": 0.0026337482179592422, "phrase": "cognitive_processing"}, {"score": 0.002570385604065795, "phrase": "gaze_analysis"}, {"score": 0.0024481856932959227, "phrase": "resulting_insight_interaction_corpus"}, {"score": 0.0021464286727045623, "phrase": "linguistic_and_gestural_features"}, {"score": 0.0021049977753042253, "phrase": "elan_multimodal_annotation_tool"}], "paper_keywords": ["Multimodal interaction", " Video corpus", " Head-mounted eye-tracking", " Multifocal approach"], "paper_abstract": "Research on the multimodal aspects of interactional language use requires high-quality multimodal resources. In contrast to the vast amount of available written language corpora and collections of transcribed spoken language, truly multimodal corpora including visual as well as auditory data are scarce. In this paper, we first discuss a few notable exceptions that do provide high-quality and multiple-angle video recordings of face-to-face conversations. We then present a new multimodal corpus design that adds two dimensions to the existing resources. First, the recording set-up was designed in such a way as to have a full view of the dialogue partners' gestural behaviour, including hand gestures, facial expressions and body posture. Second, by recording the participant perspective and behaviour during conversation, using head-mounted scene cameras and eye-trackers, we obtained a 3D landscape of the conversation, with detailed production information (scene camera and sound) and indices of cognitive processing (eye movements for gaze analysis) for both participants. In its current form, the resulting InSight Interaction Corpus consists of 15 recorded face-to-face interactions of 20 min each, of which five have been transcribed and annotated for a range of linguistic and gestural features, using the ELAN multimodal annotation tool.", "paper_title": "InSight Interaction: a multimodal and multifocal dialogue corpus", "paper_id": "WOS:000349889400007"}