{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "contextual_information"}, {"score": 0.0071088346963086935, "phrase": "object_detection"}, {"score": 0.006177085981020705, "phrase": "context_model"}, {"score": 0.004759305937310656, "phrase": "context"}, {"score": 0.00458488025289647, "phrase": "context_modeling"}, {"score": 0.004419538941706716, "phrase": "different_degrees"}, {"score": 0.004273196830970426, "phrase": "different_images"}, {"score": 0.00399483173553482, "phrase": "target_object"}, {"score": 0.003850686614685676, "phrase": "robust_context_model"}, {"score": 0.0038154673253945003, "phrase": "larger_training_set"}, {"score": 0.003769006950727621, "phrase": "target_object_appearance_model"}, {"score": 0.0035997452281985465, "phrase": "novel_context_modeling_framework"}, {"score": 0.003512591457930393, "phrase": "prior_scene_segmentation"}, {"score": 0.0034911335714333507, "phrase": "context_annotation"}, {"score": 0.003438058620195309, "phrase": "polar_geometric_context_descriptor"}, {"score": 0.0034066004178183117, "phrase": "multiple_types"}, {"score": 0.00327356326590461, "phrase": "new_maximum_margin_context"}, {"score": 0.003097865379563127, "phrase": "discriminant_context_inference_method"}, {"score": 0.0030135741897058844, "phrase": "context_learning"}, {"score": 0.0029951555610569225, "phrase": "limited_data"}, {"score": 0.002931569771634119, "phrase": "transfer_learning"}, {"score": 0.0026986685585382347, "phrase": "target_objects"}, {"score": 0.00268216942002329, "phrase": "nontarget_objects"}, {"score": 0.00259320616168125, "phrase": "training_samples"}, {"score": 0.00257735013358606, "phrase": "source_object_classes"}, {"score": 0.002499508839669243, "phrase": "target_object_class"}, {"score": 0.002469032978674379, "phrase": "joint_maximum_margin_learning_framework"}, {"score": 0.002315012841166354, "phrase": "i-lids_data"}, {"score": 0.0022382002476510573, "phrase": "outdoor_surveillance_footage"}, {"score": 0.002183938718432764, "phrase": "proposed_models"}, {"score": 0.0021049977753042253, "phrase": "related_alternative_context_models"}], "paper_keywords": ["Context modeling", " object detection", " transfer learning"], "paper_abstract": "Context is critical for reducing the uncertainty in object detection. However, context modeling is challenging because there are often many different types of contextual information coexisting with different degrees of relevance to the detection of target object(s) in different images. It is therefore crucial to devise a context model to automatically quantify and select the most effective contextual information for assisting in detecting the target object. Nevertheless, the diversity of contextual information means that learning a robust context model requires a larger training set than learning the target object appearance model, which may not be available in practice. In this work, a novel context modeling framework is proposed without the need for any prior scene segmentation or context annotation. We formulate a polar geometric context descriptor for representing multiple types of contextual information. In order to quantify context, we propose a new maximum margin context (MMC) model to evaluate and measure the usefulness of contextual information directly and explicitly through a discriminant context inference method. Furthermore, to address the problem of context learning with limited data, we exploit the idea of transfer learning based on the observation that although two categories of objects can have very different visual appearance, there can be similarity in their context and/or the way contextual information helps to distinguish target objects from nontarget objects. To that end, two novel context transfer learning models are proposed which utilize training samples from source object classes to improve the learning of the context model for a target object class based on a joint maximum margin learning framework. Experiments are carried out on PASCAL VOC2005 and VOC2007 data sets, a luggage detection data set extracted from the i-LIDS data set, and a vehicle detection data set extracted from outdoor surveillance footage. Our results validate the effectiveness of the proposed models for quantifying and transferring contextual information, and demonstrate that they outperform related alternative context models.", "paper_title": "Quantifying and Transferring Contextual Information in Object Detection", "paper_id": "WOS:000300581700010"}