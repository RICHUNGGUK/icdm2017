{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "contextual_information"}, {"score": 0.004767261858417823, "phrase": "internet_cross-media_retrieval"}, {"score": 0.004673290248999786, "phrase": "fast_explosive_rate"}, {"score": 0.004558414405811025, "phrase": "image_data"}, {"score": 0.004294052530714228, "phrase": "cross-media_scenario"}, {"score": 0.004230382915464492, "phrase": "urgent_problem"}, {"score": 0.0040854526743211396, "phrase": "contextual_textual_information"}, {"score": 0.0038869467872381957, "phrase": "internet_content"}, {"score": 0.003753738096719609, "phrase": "visual_information"}, {"score": 0.003643185395780667, "phrase": "enhanced_content"}, {"score": 0.0035891316580885665, "phrase": "textual_document"}, {"score": 0.0035183010916001664, "phrase": "image-to-image_similarity"}, {"score": 0.0034317185678527672, "phrase": "document-to-document_similarity"}, {"score": 0.003281186608301359, "phrase": "image_similarities"}, {"score": 0.0031686717525002935, "phrase": "accompanied_textual_documents"}, {"score": 0.0028394925080958205, "phrase": "ranking_structure"}, {"score": 0.0027557911038816256, "phrase": "relative_relations"}, {"score": 0.0027284410545226306, "phrase": "textual_information"}, {"score": 0.002687922713487696, "phrase": "learning_results"}, {"score": 0.0026086775334428617, "phrase": "human's_recognition"}, {"score": 0.00256993314099902, "phrase": "proposed_method"}, {"score": 0.0023846534541336326, "phrase": "image_retrieval"}, {"score": 0.0023259039577313294, "phrase": "cross-modality_multimedia"}, {"score": 0.0022799458141165587, "phrase": "query_expansion_framework"}, {"score": 0.0022016885701010088, "phrase": "extensive_experimental_evaluations"}, {"score": 0.0021798257311714665, "phrase": "large_scale"}, {"score": 0.002168975681697056, "phrase": "internet_dataset"}, {"score": 0.0021049977753042253, "phrase": "proposed_methods"}], "paper_keywords": ["Image similarity learning", " Structural SVM", " Cross-media retrieval", " Query expansion"], "paper_abstract": "With the fast explosive rate of the amount of image data on the Internet, how to efficiently utilize them in the cross-media scenario becomes an urgent problem. Images are usually accompanied with contextual textual information. These two heterogeneous modalities are mutually reinforcing to make the Internet content more informative. In most cases, visual information can be regarded as an enhanced content of the textual document. To make image-to-image similarity being more consistent with document-to-document similarity, this paper proposes a method to learn image similarities according to the relations of the accompanied textual documents. More specifically, instead of using the static quantitative relations, rank-based learning procedure by employing structural SVM is adopted in this paper, and the ranking structure is established by comparing the relative relations of textual information. The learning results are in more accordance with the human's recognition. The proposed method in this paper can be used not only for the image-to-image retrieval, but also for cross-modality multimedia, where a query expansion framework is proposed to get more satisfactory results. Extensive experimental evaluations on large scale Internet dataset validate the performance of the proposed methods.", "paper_title": "Relative image similarity learning with contextual information for Internet cross-media retrieval", "paper_id": "WOS:000344065400003"}