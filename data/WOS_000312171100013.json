{"auto_keywords": [{"score": 0.043400370094307784, "phrase": "proposed_method"}, {"score": 0.012700164882683, "phrase": "locality_constraint"}, {"score": 0.00481495049065317, "phrase": "constrained_representation"}, {"score": 0.0047310525657970615, "phrase": "spatial_pyramid_patches"}, {"score": 0.004547564208690591, "phrase": "linear_representation"}, {"score": 0.004371160978838441, "phrase": "locality_information"}, {"score": 0.00431388720289049, "phrase": "spatial_features"}, {"score": 0.00427612041204006, "phrase": "training_samples"}, {"score": 0.004201571673071299, "phrase": "holistic_face_images"}, {"score": 0.004056334733843591, "phrase": "spatial_pyramid_local_patches"}, {"score": 0.0039333604469462356, "phrase": "bayesian_based_fusion_method"}, {"score": 0.0038309141193444015, "phrase": "representation_coefficients"}, {"score": 0.00376409764683147, "phrase": "approximately_sparse_representation"}, {"score": 0.0036660436272785476, "phrase": "discriminative_nature"}, {"score": 0.003633927813203731, "phrase": "spatial_local_features"}, {"score": 0.00355485940716082, "phrase": "sparse_representation"}, {"score": 0.0034928649850766726, "phrase": "src"}, {"score": 0.003342471131048328, "phrase": "proposed_locality"}, {"score": 0.0033131804981786747, "phrase": "representation_based_classification"}, {"score": 0.003284145698220425, "phrase": "lcrc"}, {"score": 0.00307431840409833, "phrase": "face_recognition"}, {"score": 0.002994178996584381, "phrase": "training_data"}, {"score": 0.0029548937818509656, "phrase": "simple_locality_based_concentration_index"}, {"score": 0.002928990880188932, "phrase": "lci"}, {"score": 0.002802825862847951, "phrase": "local_patch"}, {"score": 0.0025892330566358503, "phrase": "local_patches"}, {"score": 0.0022290933642438353, "phrase": "yale"}, {"score": 0.0021709339252206825, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Face recognition", " Linear representation", " Locality constraint"], "paper_abstract": "In this work, we propose a linear representation based face recognition (FR) method incorporating locality information from both spatial features and training samples. Instead of holistic face images, the proposed method is conducted on the spatial pyramid local patches, which are aggregated by a Bayesian based fusion method. The locality constraint on the representation coefficients leads to an approximately sparse representation, which effectively explores the discriminative nature of spatial local features. Different from the sparse representation based classification (SRC) exposing an l(1)-norm constraint on the coefficients, the proposed locality constrained representation based classification (LCRC) is formulated with a computationally efficient l(2)-norm. The proposed method is robust to two crucial problems in face recognition: occlusion and lack of training data. A simple locality based concentration index (LCI) is defined to measure the reliability of each local patch, by which not only the heavily corrupted patches but also the less discriminant ones are rejected. Due to the use of both local patches and the locality constraint, less training data are required by the proposed method. Based on the locality constrained representation, we present three algorithms which outperform the state-of-the-art on the AR and Extended Yale B datasets for both the occlusion and single sample per person (SSPP) problems. Crown Copyright (C) 2012 Published by Elsevier B.V. All rights reserved.", "paper_title": "Locality constrained representation based classification with spatial pyramid patches", "paper_id": "WOS:000312171100013"}