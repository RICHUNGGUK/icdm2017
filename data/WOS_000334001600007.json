{"auto_keywords": [{"score": 0.048835745230446134, "phrase": "dct_coefficients"}, {"score": 0.047383019958100334, "phrase": "blocking_artifacts"}, {"score": 0.03424577384447476, "phrase": "processed_image"}, {"score": 0.00481495049065317, "phrase": "jpeg_compression"}, {"score": 0.0047395356621991935, "phrase": "comblike_histogram"}, {"score": 0.004496542293425754, "phrase": "adjacent_blocks"}, {"score": 0.0042212686864468805, "phrase": "jpeg._stamm"}, {"score": 0.004177242237944206, "phrase": "liu"}, {"score": 0.004111584914709491, "phrase": "anti-forensics_method"}, {"score": 0.0035853745478488254, "phrase": "processed_images"}, {"score": 0.003492155652400838, "phrase": "noise_distributions"}, {"score": 0.0034013521529056715, "phrase": "resulting_images"}, {"score": 0.0031428202743657057, "phrase": "original_image"}, {"score": 0.0029657683623403085, "phrase": "improved_anti-forensics_method"}, {"score": 0.0029346820693548128, "phrase": "jpeg"}, {"score": 0.0028432828877281388, "phrase": "noise_distribution"}, {"score": 0.002769304577201566, "phrase": "denoising_algorithm"}, {"score": 0.002711506442590442, "phrase": "grainy_noise"}, {"score": 0.0026689487424251907, "phrase": "image_dithering"}, {"score": 0.0025586903829811296, "phrase": "fan_and_queiroz's_forensics"}, {"score": 0.0024529757621396717, "phrase": "proposed_anti-forensics_method"}, {"score": 0.0023891283748647416, "phrase": "comblike_histograms"}, {"score": 0.0023025177990160487, "phrase": "noise_distribution_abnormality"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Image forensics", " Anti-forensics", " Blocking artifact"], "paper_abstract": "The comblike histogram of DCT coefficients on each subband and the blocking artifacts among adjacent blocks are the two main fingerprints of the image that was once compressed by JPEG. Stamm and Liu proposed an anti-forensics method for removing these fingerprints by dithering the DCT coefficients and adding noise into the pixels. However, some defects emerge inside the anti-forensically processed images. First, the noise distributions are abnormal in the resulting images; and second, the quality of the processed image is poor compared with the original image. To fill these gaps, this paper proposes an improved anti-forensics method for JPEG compression. After analyzing the noise distribution, we propose a denoising algorithm to remove the grainy noise caused by image dithering, and a deblocking algorithm to combat Fan and Queiroz's forensics method against blocking artifacts. With the proposed anti-forensics method, fingerprints of the comblike histograms and the blocking artifacts are removed, noise distribution abnormality is avoided, and the quality of the processed image is improved. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Improved anti-forensics of JPEG compression", "paper_id": "WOS:000334001600007"}