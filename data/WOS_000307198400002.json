{"auto_keywords": [{"score": 0.04181790450128157, "phrase": "learning_strategy"}, {"score": 0.030785178003086507, "phrase": "dnlpso"}, {"score": 0.00481495049065317, "phrase": "based_particle_swarm_optimizer"}, {"score": 0.004787595063253874, "phrase": "global_numerical_optimization"}, {"score": 0.004719881293102484, "phrase": "particle_swarms"}, {"score": 0.004626680470601208, "phrase": "social_behavior"}, {"score": 0.004574247691703376, "phrase": "animal_kingdom"}, {"score": 0.004458426563932486, "phrase": "recent_past"}, {"score": 0.0041636475197809825, "phrase": "huge_number"}, {"score": 0.004104722208522451, "phrase": "basic_algorithm"}, {"score": 0.0040351772969919085, "phrase": "pso"}, {"score": 0.003789763907758991, "phrase": "global_best_experience"}, {"score": 0.003757480190200645, "phrase": "whole_swarm"}, {"score": 0.003736110068915636, "phrase": "linear_summation"}, {"score": 0.003631062193606885, "phrase": "powerful_variant"}, {"score": 0.003459191419478298, "phrase": "different_particles"}, {"score": 0.0034395119150793787, "phrase": "different_dimensions"}, {"score": 0.003371508176308189, "phrase": "best_particle"}, {"score": 0.00332375606483146, "phrase": "highest_fitness"}, {"score": 0.003258033491409002, "phrase": "better_value"}, {"score": 0.0031573626285069157, "phrase": "single-objective_pso"}, {"score": 0.0031393948330209255, "phrase": "dynamic_neighborhood_learning_particle_swarm_optimizer"}, {"score": 0.002999274091205852, "phrase": "particle's_velocity"}, {"score": 0.002973704188310602, "phrase": "clpso."}, {"score": 0.002931570207868049, "phrase": "clpso"}, {"score": 0.002881793492919646, "phrase": "exemplar_particle"}, {"score": 0.0027927155990963063, "phrase": "learner_particle"}, {"score": 0.002753138528582461, "phrase": "historical_information"}, {"score": 0.0024490082808807396, "phrase": "premature_convergence"}, {"score": 0.002326304140277375, "phrase": "five_constrained_benchmarks"}, {"score": 0.0022933216359142736, "phrase": "practical_engineering_optimization_problem"}, {"score": 0.0022737565087825058, "phrase": "spread-spectrum_radar_polyphase_code_design"}, {"score": 0.002184631929806752, "phrase": "complicated_and_multimodal_fitness_landscapes"}, {"score": 0.0021598141021966894, "phrase": "five_other_recent_variants"}, {"score": 0.002147515684307197, "phrase": "pso."}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Particle swarm", " Neighborhood", " Exemplar", " Learning strategy"], "paper_abstract": "The concept of particle swarms originated from the simulation of the social behavior commonly observed in animal kingdom and evolved into a very simple but efficient technique for optimization in recent past. Since its advent in 1995, the Particle Swarm Optimization (PSO) algorithm has attracted the attention of a lot of researchers all over the world resulting into a huge number of variants of the basic algorithm as well as many parameter selection/control strategies. PSO relies on the learning strategy of the individuals to guide its search direction. Traditionally, each particle utilizes its historical best experience as well as the global best experience of the whole swarm through linear summation. The Comprehensive Learning PSO (CLPSO) was proposed as a powerful variant of PSO that enhances the diversity of the population by encouraging each particle to learn from different particles on different dimensions, in the metaphor that the best particle, despite having the highest fitness, does not always offer a better value in every dimension. This paper presents a variant of single-objective PSO called Dynamic Neighborhood Learning Particle Swarm Optimizer (DNLPSO), which uses learning strategy whereby all other particles' historical best information is used to update a particle's velocity as in CLPSO. But in contrast to CLPSO, in DNLPSO, the exemplar particle is selected from a neighborhood. This strategy enables the learner particle to learn from the historical information of its neighborhood or sometimes from that of its own. Moreover, the neighborhoods are made dynamic in nature i.e. they are reformed after certain intervals. This helps the diversity of the swarm to be preserved in order to discourage premature convergence. Experiments were conducted on 16 numerical benchmarks in 10, 30 and 50 dimensions, a set of five constrained benchmarks and also on a practical engineering optimization problem concerning the spread-spectrum radar polyphase code design. The results demonstrate very competitive performance of DNLPSO while locating the global optimum on complicated and multimodal fitness landscapes when compared with five other recent variants of PSO. (C) 2012 Published by Elsevier Inc.", "paper_title": "A dynamic neighborhood learning based particle swarm optimizer for global numerical optimization", "paper_id": "WOS:000307198400002"}