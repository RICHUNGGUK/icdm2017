{"auto_keywords": [{"score": 0.04107868567004354, "phrase": "motion_data"}, {"score": 0.00481495049065317, "phrase": "cyclic_dance_sequences"}, {"score": 0.0047839735747931605, "phrase": "multi_factor_tensor_analysis"}, {"score": 0.004647013794908825, "phrase": "novel_approach"}, {"score": 0.004602233424629559, "phrase": "motion_styles"}, {"score": 0.004499450739291382, "phrase": "mft"}, {"score": 0.004398872331144531, "phrase": "musical_information_analysis_method"}, {"score": 0.004342430078306461, "phrase": "motion_sequence"}, {"score": 0.00424538577579635, "phrase": "musical_rhythm"}, {"score": 0.0041773934798743405, "phrase": "task_model"}, {"score": 0.004123781267600542, "phrase": "repeated_motion_segments"}, {"score": 0.004005645977640982, "phrase": "person-invariant_factor_task"}, {"score": 0.003967021436047945, "phrase": "person-dependent_factor_style"}, {"score": 0.0038409323908446297, "phrase": "mft_model"}, {"score": 0.003767203575060969, "phrase": "different_modes"}, {"score": 0.003486123304074186, "phrase": "different_people"}, {"score": 0.0033861930947608207, "phrase": "musical_analysis_approach"}, {"score": 0.003310447652300423, "phrase": "vectorization_method"}, {"score": 0.0032155369606336206, "phrase": "japanese_traditional_dance_sequences"}, {"score": 0.0031132586744640573, "phrase": "unknown_motion_segment"}, {"score": 0.0029947969419015868, "phrase": "different_time"}, {"score": 0.0029658905122290536, "phrase": "time_space"}, {"score": 0.0029089095588612007, "phrase": "motion_segment"}, {"score": 0.002555950807368337, "phrase": "tensor_subdomain"}, {"score": 0.002514947446797848, "phrase": "next_experiment"}, {"score": 0.00247460024299964, "phrase": "function_value"}, {"score": 0.002450702522633428, "phrase": "tensorial_subdomain"}, {"score": 0.0023650305205893353, "phrase": "first_experiment"}, {"score": 0.002297162511308007, "phrase": "human_identities"}, {"score": 0.002181270821530302, "phrase": "recognition_ability"}, {"score": 0.0021186648667919044, "phrase": "high_accuracy"}], "paper_keywords": ["task", " motion style", " recognition", " MFT analysis"], "paper_abstract": "In this paper, we present a novel approach to recognize motion styles and identify people using the Multi Factor Tensor (MFT) model. We apply a musical information analysis method in segmenting the motion sequence relevant to the keyposes and the musical rhythm. We define a task model by considering the repeated motion segments, where the motion is decomposed into a person-invariant factor task and a person-dependent factor style. Given the motion data set, we formulate the MFT model, factorize it efficiently in different modes, and use it in recognizing the tasks and the identities of the persons performing the tasks. We capture the motion data of different people for a few cycles, segment it using the musical analysis approach, normalize the segments using a vectorization method, and realize our MFT model. In our experiments, Japanese traditional dance sequences performed by several people are used. Provided with an unknown motion segment which is to be probed and which was performed at a different time in the time space, we first normalize the motion segment and flatten our MFT model appropriately, then recognize the task and the identity of the person. We follow two approaches in conducting our experiments. In one experiment, we recognize the tasks and the styles by maximizing a function in the tensor subdomain, and in the next experiment, we use a function value in the tensorial subdomain with a threshold for recognition. Interestingly, unlike the first experiment, we are capable of recognizing tasks and human identities that were not known beforehand. We conducted various experiments to evaluate the potential of the recognition ability of our proposed approaches, and the results demonstrate the high accuracy of our model.", "paper_title": "Task recognition and person identification in cyclic dance sequences with Multi Factor Tensor analysis", "paper_id": "WOS:000256860900036"}