{"auto_keywords": [{"score": 0.03829477508328845, "phrase": "dmed"}, {"score": 0.03382949678475337, "phrase": "finite-time_regret"}, {"score": 0.00481495049065317, "phrase": "new_bandit_algorithm"}, {"score": 0.00475924837336259, "phrase": "semi-bounded_reward"}, {"score": 0.004542780650208743, "phrase": "stochastic_multiarmed_bandit_problem"}, {"score": 0.004236321075918639, "phrase": "asymptotic_theoretical_bound"}, {"score": 0.004090901264701185, "phrase": "reward_distribution"}, {"score": 0.003973523379376352, "phrase": "known_bounded_interval"}, {"score": 0.0035366216368204182, "phrase": "asymptotic_form"}, {"score": 0.003435092524339692, "phrase": "finite_time"}, {"score": 0.003129317057177867, "phrase": "new_policy"}, {"score": 0.003093054709625219, "phrase": "indexed_minimum_empirical_divergence"}, {"score": 0.0029694040368299624, "phrase": "large_deviation_probabilities"}, {"score": 0.002917931350071647, "phrase": "simple_nonasymptotic_form"}, {"score": 0.0028341130433482565, "phrase": "refined_analysis"}, {"score": 0.0023792975669318615, "phrase": "minimum_reward"}, {"score": 0.0022974765563692776, "phrase": "maximum_loss"}, {"score": 0.002154693685426657, "phrase": "simulation_results"}, {"score": 0.002104998088799187, "phrase": "imed"}], "paper_keywords": ["stochastic bandit", " finite-time regret", " large deviation principle"], "paper_abstract": "In this paper we consider a stochastic multiarmed bandit problem. It is known in this problem that Deterministic Minimum Empirical Divergence (DMED) policy achieves the asymptotic theoretical bound for the model where each reward distribution is supported in a known bounded interval, say [0; 1]. However, the regret bound of DMED is described in an asymptotic form and the performance in finite time has been unknown. We modify this policy and derive a finite-time regret bound for the new policy, Indexed Minimum Empirical Divergence (IMED), by refining large deviation probabilities to a simple nonasymptotic form. Further, the refined analysis reveals that the finite-time regret bound is valid even in the case that the reward is not bounded from below. Therefore, our finitetime result applies to the case that the minimum reward (that is, the maximum loss) is unknown or unbounded. We also present some simulation results which shows that IMED much improves DMED and performs competitively to other state-of-the-art policies.", "paper_title": "Non-Asymptotic Analysis of a New Bandit Algorithm for Semi-Bounded Reward", "paper_id": "WOS:000369888000042"}