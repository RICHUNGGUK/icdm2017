{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "still_images"}, {"score": 0.01049310776231292, "phrase": "spatial_interest_regions"}, {"score": 0.009441908458976852, "phrase": "key_points"}, {"score": 0.008336235433368016, "phrase": "action_recognition"}, {"score": 0.004671156704912419, "phrase": "common_approach"}, {"score": 0.004635881163634672, "phrase": "human_action"}, {"score": 0.004514491994233911, "phrase": "local_descriptors"}, {"score": 0.004168986485488861, "phrase": "key_point_detector"}, {"score": 0.004121826934975274, "phrase": "dense_sampling"}, {"score": 0.00393844255111604, "phrase": "human_activities"}, {"score": 0.0035820833812158035, "phrase": "person-object_interactions"}, {"score": 0.0033329032920236994, "phrase": "action-specific_points"}, {"score": 0.0031845067058084583, "phrase": "video_data"}, {"score": 0.0030892533800837463, "phrase": "novel_method"}, {"score": 0.0030082398041055003, "phrase": "non-negative_matrix_factorization"}, {"score": 0.002896167001212006, "phrase": "resulting_basis_flows"}, {"score": 0.002841701708055512, "phrase": "image_regions"}, {"score": 0.0027254466299651936, "phrase": "informed_sampling"}, {"score": 0.0026843584448921565, "phrase": "feature_extraction"}, {"score": 0.002623881637290192, "phrase": "generative_model"}, {"score": 0.0025357050267224715, "phrase": "joint_distributions"}, {"score": 0.0024785689583828796, "phrase": "local_image_features"}, {"score": 0.002413531395475291, "phrase": "human_actions"}, {"score": 0.0023952639120005193, "phrase": "experimental_evaluation"}, {"score": 0.0023235629819760018, "phrase": "interest_regions"}, {"score": 0.0022625835448873495, "phrase": "body_parts"}, {"score": 0.0022284577808744316, "phrase": "different_actions"}, {"score": 0.002169968849836984, "phrase": "high_accuracy"}, {"score": 0.002153540857619362, "phrase": "action_classification"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Action recognition", " Non-negative matrix factorization", " Pose estimation", " Optical flow"], "paper_abstract": "A common approach to human action recognition from still images consists in computing local descriptors for classification. Typically, these descriptors are computed in the vicinity of key points which either result from running a key point detector or from dense sampling of pixel coordinates. Such key points are not a priorly related to human activities and thus might not be very informative with regard to action recognition. Several recent approaches, On the other hand, are based On learning person-object interactions and saliency maps in images. In this article, we investigate the possibility and applicability of identifying action-specific points or regions of interest in still images based on information extracted from video data, in particular, we propose a novel method for extracting spatial interest regions where we apply non-negative matrix factorization to optical flow fields extracted from videos. The resulting basis flows are found to indicate image regions that are specific to certain actions and therefore allow for an informed sampling of key points for feature extraction. We thus present a generative model for action recognition in still images that allows for characterizing joint distributions of regions of interest, local image features (visual words), and human actions. Experimental evaluation shows that (a) our approach is able to extract interest regions that are highly correlated to those body parts most relevant for different actions and (b) our generative model achieves high accuracy in action classification. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Action recognition in still images by learning spatial interest regions from videos", "paper_id": "WOS:000345687500002"}