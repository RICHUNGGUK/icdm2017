{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multilingual_settings"}, {"score": 0.012828104455485628, "phrase": "current_research"}, {"score": 0.010628254117815735, "phrase": "multilingual_probabilistic_topic_model"}, {"score": 0.004702270260893693, "phrase": "probabilistic_topic_models"}, {"score": 0.004677588817740052, "phrase": "unsupervised_generative_models"}, {"score": 0.004653036317778496, "phrase": "model_document"}, {"score": 0.004604315481634605, "phrase": "two-step_generation_process"}, {"score": 0.004472935829694716, "phrase": "latent_concepts"}, {"score": 0.00439128062277584, "phrase": "probability_distributions"}, {"score": 0.0043682243489937935, "phrase": "vocabulary_words"}, {"score": 0.004311109608629196, "phrase": "significant_research_effort"}, {"score": 0.004232396043643437, "phrase": "probabilistic_topic"}, {"score": 0.004144188637321245, "phrase": "novel_topic_models"}, {"score": 0.004068510313497539, "phrase": "parallel_and_comparable_texts"}, {"score": 0.004025885154716127, "phrase": "multilingual_probabilistic_topic"}, {"score": 0.003952358633642862, "phrase": "first_full_overview"}, {"score": 0.003839510177926892, "phrase": "muptm."}, {"score": 0.003809294294086141, "phrase": "representative_example"}, {"score": 0.0037594599958631404, "phrase": "natural_extension"}, {"score": 0.003729871726054667, "phrase": "omnipresent_lda_model"}, {"score": 0.0036043137580229873, "phrase": "thorough_overview"}, {"score": 0.00357594216066491, "phrase": "representative_multilingual_model"}, {"score": 0.003437387719239532, "phrase": "data_representation"}, {"score": 0.0034013521529056715, "phrase": "output_sets"}, {"score": 0.003356836689343337, "phrase": "per-topic_word_distributions"}, {"score": 0.0031677663986980666, "phrase": "external_language_pair"}, {"score": 0.0031594291695784286, "phrase": "dependent_translation_resource"}, {"score": 0.002769304577201566, "phrase": "relevant_literature"}, {"score": 0.0026063580561924023, "phrase": "multilingual_probabilistic_topic_modeling"}, {"score": 0.0025586903829811296, "phrase": "potential_applications"}, {"score": 0.0024081065242654754, "phrase": "clear_directions"}, {"score": 0.002395437787820304, "phrase": "future_research"}, {"score": 0.0023516185395256505, "phrase": "systematic_overview"}, {"score": 0.0023085990127201784, "phrase": "aspect_knowledge"}, {"score": 0.0022783523466965187, "phrase": "different_languages"}, {"score": 0.0022603944655015095, "phrase": "shared_space"}, {"score": 0.0022485010715463244, "phrase": "latent_cross-lingual_topics"}, {"score": 0.0021957487795963666, "phrase": "learned_per-topic_word_distributions"}, {"score": 0.0021669774367188673, "phrase": "topic_distributions"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multilingual probabilistic topic models", " Cross-lingual text mining", " Cross-lingual knowledge transfer", " Cross-lingual information retrieval", " Language-independent data representation", " Non-parallel data"], "paper_abstract": "Probabilistic topic models are unsupervised generative models which model document content as a two-step generation process, that is, documents are observed as mixtures of latent concepts or topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable texts. We define multilingual probabilistic topic modeling (MuPTM) and present the first full overview of the current research, methodology, advantages and limitations in MuPTM. As a representative example, we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA (BiLDA). We provide a thorough overview of this representative multilingual model from its high-level modeling assumptions down to its mathematical foundations. We demonstrate how to use the data representation by means of output sets of (i) per-topic word distributions and (ii) per-document topic distributions coming from a multilingual probabilistic topic model in various real-life cross-lingual tasks involving different languages, without any external language pair dependent translation resource: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. We also briefly review several other applications present in the relevant literature, and introduce and illustrate two related modeling concepts: topic smoothing and topic pruning. In summary, this article encompasses the current research in multilingual probabilistic topic modeling. By presenting a series of potential applications, we reveal the importance of the language-independent and language pair independent data representations by means of MuPTM. We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross-lingual topics, that is, how to effectively employ learned per-topic word distributions and per-document topic distributions of any multilingual probabilistic topic model in various cross-lingual applications. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications", "paper_id": "WOS:000345491900007"}