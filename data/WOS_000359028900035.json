{"auto_keywords": [{"score": 0.03663103907138758, "phrase": "group_bridge_regression"}, {"score": 0.004567596860086148, "phrase": "compressed_sensing"}, {"score": 0.004527619017250708, "phrase": "high-dimensional_statistics"}, {"score": 0.00427612041204006, "phrase": "sparse_solutions"}, {"score": 0.00376409764683147, "phrase": "better_trade-off"}, {"score": 0.003698442219929622, "phrase": "algorithmic_stability"}, {"score": 0.0036020923276404403, "phrase": "simplest_case"}, {"score": 0.0035705347414368696, "phrase": "squared_loss"}, {"score": 0.0034168317227203206, "phrase": "theoretical_side"}, {"score": 0.0031845067058084583, "phrase": "important_property"}, {"score": 0.003142732162296465, "phrase": "learning_method"}, {"score": 0.00308788133472785, "phrase": "computational_side"}, {"score": 0.0029161225023394363, "phrase": "computationally_efficient_optimization_algorithms"}, {"score": 0.002705808393126695, "phrase": "iterative_update"}, {"score": 0.0025892330566358503, "phrase": "large-scale_settings"}, {"score": 0.0025328377393370642, "phrase": "clear_advantage"}, {"score": 0.0024776677004255104, "phrase": "proposed_algorithms"}, {"score": 0.0021901485584955487, "phrase": "future_applications"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["l(p) Regularization", " Convex optimization algorithms", " ADMM", " FISTA", " Algorithmic stability", " Lasso", " Group Lasso", " Bridge regression", " Group bridge regression", " Splice detection"], "paper_abstract": "Following advances in compressed sensing and high-dimensional statistics, many pattern recognition methods have been developed with l(1) regularization, which promotes sparse solutions. In this work, we instead advocate the use of l(p) (2 >= p > 1) regularization in a group setting which provides a better trade-off between sparsity and algorithmic stability. We focus on the simplest case with squared loss, which is known as group bridge regression. On the theoretical side, we prove that group bridge regression is uniformly stable and thus generalizes, which is an important property of a learning method. On the computational side, we make group bridge regression more practically attractive by deriving provably convergent and computationally efficient optimization algorithms. We show that there are at least several values of p over (1,2) at which the iterative update is analytical, thus it is even suitable for large-scale settings. We demonstrate the clear advantage of group bridge regression with the proposed algorithms over other competitive alternatives on several datasets. As l(p)-regularization allows one to achieve flexibility in sparseness/denseness of the solution, we hope that the algorithms will be useful for future applications of this regularization. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "On group-wise l(p) regularization: Theory and efficient algorithms", "paper_id": "WOS:000359028900035"}