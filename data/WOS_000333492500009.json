{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "neural_network_ensembles"}, {"score": 0.004766683200410046, "phrase": "random_weights"}, {"score": 0.0044870272550540415, "phrase": "sound_generalization_capability"}, {"score": 0.0043533862892678864, "phrase": "base_learners'_outputs"}, {"score": 0.00418134346679498, "phrase": "feed-forward_neural_networks"}, {"score": 0.004139401327448807, "phrase": "error_back-propagation_algorithms"}, {"score": 0.003935892934907777, "phrase": "slow_convergence"}, {"score": 0.0038964032225585117, "phrase": "local_minima_problem"}, {"score": 0.0038573081851680656, "phrase": "model_uncertainties"}, {"score": 0.003780286517897356, "phrase": "initial_weights"}, {"score": 0.003576286811853861, "phrase": "better_solution"}, {"score": 0.003469676666252421, "phrase": "random_vector_functional_link"}, {"score": 0.003366233863411861, "phrase": "base_components"}, {"score": 0.0032658649336454923, "phrase": "ncl_strategy"}, {"score": 0.0031684791285366315, "phrase": "basis_functions"}, {"score": 0.0031208772639433145, "phrase": "base_models"}, {"score": 0.002982307044758515, "phrase": "rvfl_networks"}, {"score": 0.002878785759716458, "phrase": "linear_equation_system"}, {"score": 0.0028355237433457313, "phrase": "analytical_solution"}, {"score": 0.002709589081384597, "phrase": "cost_function"}, {"score": 0.002668865308451223, "phrase": "ncl"}, {"score": 0.002628747284714371, "phrase": "well-known_least_squares"}, {"score": 0.0024617483174454113, "phrase": "comparative_study"}, {"score": 0.002400373755068185, "phrase": "nine_benchmark_datasets"}, {"score": 0.002282166507956957, "phrase": "testing_datasets"}, {"score": 0.0021807544600219216, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Negative correlation learning", " Neural network ensembles", " Random vector function link networks", " Data regression"], "paper_abstract": "Negative correlation learning (NCL) aims to produce ensembles with sound generalization capability through controlling the disagreement among base learners' outputs. Such a learning scheme is usually implemented by using feed-forward neural networks with error back-propagation algorithms (BPNNs). However, it suffers from slow convergence, local minima problem and model uncertainties caused by the initial weights and the setting of learning parameters. To achieve a better solution, this paper employs the random vector functional link (RVFL) networks as base components, and incorporates with the NCL strategy for building neural network ensembles. The basis functions of the base models are generated randomly and the parameters of the RVFL networks can be determined by solving a linear equation system. An analytical solution is derived for these parameters, where a cost function defined for NCL and the well-known least squares method are used. To examine the merits of our proposed algorithm, a comparative study is carried out with nine benchmark datasets. Results indicate that our approach outperforms other ensembling techniques on the testing datasets in terms of both effectiveness and efficiency. Crown Copyright (C) 2013 Published by Elsevier Inc. All rights reserved.", "paper_title": "Fast decorrelated neural network ensembles with random weights", "paper_id": "WOS:000333492500009"}