{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "image_spatial_entropy"}, {"score": 0.004594964789564398, "phrase": "ise"}, {"score": 0.0043509032299218955, "phrase": "image_processing"}, {"score": 0.0037224647319912293, "phrase": "estimation_methods"}, {"score": 0.003580032420161922, "phrase": "fourth_one"}, {"score": 0.00331125524068286, "phrase": "computational_complexity"}, {"score": 0.003234615195382533, "phrase": "original_formulation"}, {"score": 0.002968424524565369, "phrase": "markovianity_constraint"}, {"score": 0.0028771413440398614, "phrase": "joint_histograms"}, {"score": 0.0028325548627901004, "phrase": "neighboring_pixels"}, {"score": 0.0021049977753042253, "phrase": "classical_monkey_model_entropy"}], "paper_keywords": ["Image spatial entropy", " Efficient computation of image spatial entropy", " Joint histogram", " Quadrilateral Markov random field"], "paper_abstract": "Computation of image spatial entropy (ISE) is prohibitive in many applications of image processing due to its high computational complexity. Four fast or computationally efficient methods for estimation of ISE are thus introduced in this paper. Three of these estimation methods are parametric and the fourth one is non-parametric. The reduction in the computational complexity from the original formulation of ISE is made possible by making use of the Markovianity constraint which causes the joint histograms of neighboring pixels to become dense around their main diagonal. It is shown that by tolerating merely 1% estimation error, the order of complexity is significantly reduced and for applications that can tolerate 6% estimation error, the complexity is reduced to that of the classical monkey model entropy.", "paper_title": "Fast computation methods for estimation of image spatial entropy", "paper_id": "WOS:000290543600007"}