{"auto_keywords": [{"score": 0.047369575881600866, "phrase": "quasi-additive"}, {"score": 0.00481495049065317, "phrase": "quasi-additive_learning_algorithms"}, {"score": 0.004187284711626886, "phrase": "natural_generalization"}, {"score": 0.004043541331944951, "phrase": "perceptron_learning"}, {"score": 0.003726967166630923, "phrase": "on-line_learning"}, {"score": 0.0033559647822437298, "phrase": "input_vectors"}, {"score": 0.003129317057177867, "phrase": "weight_vector"}, {"score": 0.002817639725689939, "phrase": "nonlinear_function"}, {"score": 0.002536926192232391, "phrase": "dually-flat_structure"}, {"score": 0.0024496978447849835, "phrase": "information-geometric_point"}, {"score": 0.0021049977753042253, "phrase": "convergence_properties"}], "paper_keywords": ["Quasi-Additive algorithms", " perceptron", " information geometry"], "paper_abstract": "The family of Quasi-Additive (QA) algorithms is a natural generalization of the perceptron learning, which is a kind of on-line learning having two parameter vectors: One is an accumulation of input vectors and the other is a weight vector for prediction associated with the former by a nonlinear function. We show that the vectors have a dually-flat structure from the information-geometric point of view, and this representation makes it easier to discuss the convergence properties.", "paper_title": "Geometric properties of Quasi-Additive learning algorithms", "paper_id": "WOS:000241305800048"}