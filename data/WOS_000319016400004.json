{"auto_keywords": [{"score": 0.03785284576512495, "phrase": "network_output"}, {"score": 0.00481495049065317, "phrase": "parallel_chaos_search_based_incremental_extreme_learning_machine"}, {"score": 0.00468912237497774, "phrase": "simple_and_efficient_learning_steps"}, {"score": 0.0044237033788643715, "phrase": "huang_et_al"}, {"score": 0.004195418010131832, "phrase": "conventional_methods"}, {"score": 0.004129248271757936, "phrase": "training_time"}, {"score": 0.004085713541752114, "phrase": "neural_networks"}, {"score": 0.0038339398365449507, "phrase": "recent_study"}, {"score": 0.0037336520904376687, "phrase": "random_hidden_nodes"}, {"score": 0.0034849681064606543, "phrase": "network_complexity"}, {"score": 0.003375826910838081, "phrase": "parallel_chaos_search"}, {"score": 0.0031676593724896075, "phrase": "additional_steps"}, {"score": 0.003036040409062437, "phrase": "learning_step"}, {"score": 0.0030039966201398966, "phrase": "optimal_parameters"}, {"score": 0.0029722900280316216, "phrase": "hidden_node"}, {"score": 0.0028944757265206332, "phrase": "parallel_chaos_optimization_algorithm"}, {"score": 0.0027159094186967247, "phrase": "residual_error"}, {"score": 0.002687235334499192, "phrase": "target_function"}, {"score": 0.0026168641536413978, "phrase": "optimization_method"}, {"score": 0.002575526825728026, "phrase": "parallel_chaos_optimization_method"}, {"score": 0.0024423826953133844, "phrase": "increased_network_architecture"}, {"score": 0.002416589459425828, "phrase": "fixed_network_architecture"}, {"score": 0.0021731423856364003, "phrase": "pc-elm._simulation_results"}, {"score": 0.0021274723597955567, "phrase": "proposed_method"}, {"score": 0.0021049977753042253, "phrase": "better_generalization_performance"}], "paper_keywords": ["Extreme learning machine", " Convergence rate", " Chaos optimization algorithm", " Random hidden nodes"], "paper_abstract": "Recently, a simple and efficient learning steps referred to as extreme learning machine (ELM), was proposed by Huang et al. , which has shown that compared to some conventional methods, the training time of neural networks can be reduced even by thousands of times. However, recent study showed that some of random hidden nodes may paly a very minion role in the network output and thus eventually increase the network complexity. This paper proposes a parallel chaos search based incremental extreme learning machine (PC-ELM) with additional steps to obtain a more compact network architecture. At each learning step, optimal parameters of hidden node that are selected by parallel chaos optimization algorithm will be added to exist network in order to minimize the residual error between target function and network output. The optimization method is proposed parallel chaos optimization method. We prove the convergence of PC-ELM both in increased network architecture and fixed network architecture. Then we apply this approach to several regression and classification problems. Experiment of 19 benchmark testing data sets are used to test the performance of PC-ELM. Simulation results demonstrate that the proposed method provides better generalization performance and more compact network architecture.", "paper_title": "Parallel Chaos Search Based Incremental Extreme Learning Machine", "paper_id": "WOS:000319016400004"}