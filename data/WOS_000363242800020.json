{"auto_keywords": [{"score": 0.04501851799576053, "phrase": "sparse_representation"}, {"score": 0.00481495049065317, "phrase": "multitask_multiview"}, {"score": 0.004593467306126913, "phrase": "tracking_problems"}, {"score": 0.00445783412174435, "phrase": "least_squares"}, {"score": 0.004216434794603139, "phrase": "traditional_ls-based_methods"}, {"score": 0.004039639718055365, "phrase": "heavy-tailed_noise"}, {"score": 0.0038868464660354626, "phrase": "tracking_approach"}, {"score": 0.003837205620832367, "phrase": "approximate_least_absolute_deviation"}, {"score": 0.0036448998970284264, "phrase": "lad"}, {"score": 0.0035676056286816915, "phrase": "multiple_types"}, {"score": 0.003537155405923517, "phrase": "visual_features"}, {"score": 0.003359816816853762, "phrase": "proposed_method"}, {"score": 0.003288567185099276, "phrase": "particle_filter_framework"}, {"score": 0.003137074867443131, "phrase": "single_particle"}, {"score": 0.0030705342386533083, "phrase": "individual_task"}, {"score": 0.003031287195944274, "phrase": "underlying_relationship"}, {"score": 0.0029797347088361056, "phrase": "different_views"}, {"score": 0.002954287204671032, "phrase": "different_particles"}, {"score": 0.002879237498994052, "phrase": "unified_robust_multitask_formulation"}, {"score": 0.0027583557819362034, "phrase": "frequently_emerging_outlier_tasks"}, {"score": 0.0026998264237910884, "phrase": "representation_matrix"}, {"score": 0.002564360070426064, "phrase": "proposed_formulation"}, {"score": 0.00249919111154604, "phrase": "nesterov's_smoothing_method"}, {"score": 0.0024356742694508662, "phrase": "accelerated_proximal_gradient_method"}, {"score": 0.002404522326122611, "phrase": "presented_tracker"}, {"score": 0.0022936610196951962, "phrase": "numerous_synthetic_sequences"}, {"score": 0.002274059352027924, "phrase": "real-world_video_sequences"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["L1 minimization", " least absolute deviation (LAD)", " multitask", " multiview", " sparse representation", " tracking"], "paper_abstract": "Various sparse-representation-based methods have been proposed to solve tracking problems, and most of them employ least squares (LSs) criteria to learn the sparse representation. In many tracking scenarios, traditional LS-based methods may not perform well owing to the presence of heavy-tailed noise. In this paper, we present a tracking approach using an approximate least absolute deviation (LAD)-based multitask multiview sparse learning method to enjoy robustness of LAD and take advantage of multiple types of visual features, such as intensity, color, and texture. The proposed method is integrated in a particle filter framework, where learning the sparse representation for each view of the single particle is regarded as an individual task. The underlying relationship between tasks across different views and different particles is jointly exploited in a unified robust multitask formulation based on LAD. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components that enable a more robust and accurate approximation. We show that the proposed formulation can be effectively approximated by Nesterov's smoothing method and efficiently solved using the accelerated proximal gradient method. The presented tracker is implemented using four types of features and is tested on numerous synthetic sequences and real-world video sequences, including the CVPR2013 tracking benchmark and ALOV++ data set. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared with several state-of-the-art trackers.", "paper_title": "Robust Multitask Multiview Tracking in Videos", "paper_id": "WOS:000363242800020"}