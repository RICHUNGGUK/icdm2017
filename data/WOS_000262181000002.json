{"auto_keywords": [{"score": 0.04137518569891578, "phrase": "wpl"}, {"score": 0.020520186996722572, "phrase": "underlying_game"}, {"score": 0.014837271625224724, "phrase": "previous_marl_algorithms"}, {"score": 0.0084500193014937, "phrase": "nash_equilibrium"}, {"score": 0.005832309923411558, "phrase": "marl_algorithm"}, {"score": 0.00481495049065317, "phrase": "non-linear_dynamics"}, {"score": 0.004704408856455752, "phrase": "marl"}, {"score": 0.004654779348666505, "phrase": "wpl's_dynamics"}, {"score": 0.004581162468419365, "phrase": "agents'_decisions"}, {"score": 0.0043731680194078046, "phrase": "previously_developed_marl_algorithms"}, {"score": 0.004188459638572375, "phrase": "nash_equilibria"}, {"score": 0.003998223112665124, "phrase": "new_marl_algorithm"}, {"score": 0.003804126509184437, "phrase": "ne"}, {"score": 0.0037412912454999046, "phrase": "minimum_knowledge"}, {"score": 0.0031898025329389376, "phrase": "benchmark_two-player-two-action_games"}, {"score": 0.003075296862372287, "phrase": "challenging_shapley's_game"}, {"score": 0.0029550541857176283, "phrase": "ne."}, {"score": 0.002701361707441567, "phrase": "important_aspect"}, {"score": 0.002527556628744957, "phrase": "multiple_learning_agents"}, {"score": 0.0022127018991294047, "phrase": "two-player-two-action_games"}, {"score": 0.002168975681697056, "phrase": "wpl's_convergence"}, {"score": 0.0021190503591688834, "phrase": "non-linear_nature"}], "paper_keywords": [""], "paper_abstract": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents' decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapley's game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL's convergence is difficult, because of the non-linear nature of WPL's dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPL's dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "paper_title": "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", "paper_id": "WOS:000262181000002"}