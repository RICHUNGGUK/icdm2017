{"auto_keywords": [{"score": 0.03879588922460013, "phrase": "coupled_dictionary_learning"}, {"score": 0.03852759913182569, "phrase": "sparse_coefficient_vectors"}, {"score": 0.00481495049065317, "phrase": "image_transformation"}, {"score": 0.00476334344613564, "phrase": "learning_dictionaries"}, {"score": 0.0047292449487169345, "phrase": "image_spaces"}, {"score": 0.004481132747216135, "phrase": "source_image_space"}, {"score": 0.00443308765153716, "phrase": "target_image_space"}, {"score": 0.004354149483556514, "phrase": "coupled_dictionaries"}, {"score": 0.004307459965946991, "phrase": "training_set"}, {"score": 0.004276610897002087, "phrase": "paired_images"}, {"score": 0.004037608759610068, "phrase": "image_intrinsic_components"}, {"score": 0.0038671299350540554, "phrase": "local_parametric_regression_approach"}, {"score": 0.0038256426589560774, "phrase": "sparse_feature_representations"}, {"score": 0.003798231095121328, "phrase": "learned_coupled_dictionaries"}, {"score": 0.0037305551963210523, "phrase": "target_image_spaces"}, {"score": 0.003637822529522622, "phrase": "training_image_patch_pairs"}, {"score": 0.0035858673522839407, "phrase": "easily_retrievable_local_clusters"}, {"score": 0.0035346515636097533, "phrase": "test_image_patch"}, {"score": 0.003397535839387366, "phrase": "local_parametric_regression"}, {"score": 0.003361069456060196, "phrase": "learned_sparse_feature_spaces"}, {"score": 0.0033249931673591457, "phrase": "obtained_sparse_representation"}, {"score": 0.00326572165784246, "phrase": "learned_target_space_dictionary"}, {"score": 0.003230665474330377, "phrase": "multiple_constraints"}, {"score": 0.003161674440750158, "phrase": "target_image"}, {"score": 0.0030941521409613417, "phrase": "final_target_image"}, {"score": 0.0028279422436488116, "phrase": "coupled_sparse_coding"}, {"score": 0.0027378045447760705, "phrase": "corresponding_source"}, {"score": 0.0027181663098561066, "phrase": "target_image_patches"}, {"score": 0.0025112011945912327, "phrase": "high-dimensional_but_sparse_feature_space"}, {"score": 0.0024399036842105205, "phrase": "extremely_fast_retrieval"}, {"score": 0.0024223970976460173, "phrase": "closest_local_clusters"}, {"score": 0.0024050158199950024, "phrase": "query_patches"}, {"score": 0.0023536149458842992, "phrase": "sparse_feature-based_image_transformation"}, {"score": 0.002286781334850213, "phrase": "corrupted_input_data"}, {"score": 0.0022218413146898887, "phrase": "simultaneous_image_restoration"}, {"score": 0.0022058958906038466, "phrase": "transformation_process"}, {"score": 0.002174346780014853, "phrase": "intrinsic_image_estimation"}], "paper_keywords": ["Image transformation", " image mapping", " sparse coding", " intrinsic images", " super-resolution"], "paper_abstract": "In this paper, we propose a framework of transforming images from a source image space to a target image space, based on learning coupled dictionaries from a training set of paired images. The framework can be used for applications such as image super-resolution and estimation of image intrinsic components (shading and albedo). It is based on a local parametric regression approach, using sparse feature representations over learned coupled dictionaries across the source and target image spaces. After coupled dictionary learning, sparse coefficient vectors of training image patch pairs are partitioned into easily retrievable local clusters. For any test image patch, we can fast index into its closest local cluster and perform a local parametric regression between the learned sparse feature spaces. The obtained sparse representation (together with the learned target space dictionary) provides multiple constraints for each pixel of the target image to be estimated. The final target image is reconstructed based on these constraints. The contributions of our proposed framework are three-fold. 1) We propose a concept of coupled dictionary learning based on coupled sparse coding which requires the sparse coefficient vectors of a pair of corresponding source and target image patches to have the same support, i.e., the same indices of nonzero elements. 2) We devise a space partitioning scheme to divide the high-dimensional but sparse feature space into local clusters. The partitioning facilitates extremely fast retrieval of closest local clusters for query patches. 3) Benefiting from sparse feature-based image transformation, our method is more robust to corrupted input data, and can be considered as a simultaneous image restoration and transformation process. Experiments on intrinsic image estimation and super-resolution demonstrate the effectiveness and efficiency of our proposed method.", "paper_title": "Image Transformation Based on Learning Dictionaries across Image Spaces", "paper_id": "WOS:000312560600010"}