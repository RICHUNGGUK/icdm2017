{"auto_keywords": [{"score": 0.050078515420922086, "phrase": "sparse_coding"}, {"score": 0.03485368426248375, "phrase": "scnn"}, {"score": 0.027360982670739428, "phrase": "learning_phase"}, {"score": 0.004762435022350756, "phrase": "two-layer_neural_network"}, {"score": 0.004659108142668152, "phrase": "classification_problems"}, {"score": 0.004558012799538011, "phrase": "feature_transformation"}, {"score": 0.004362326712297089, "phrase": "sparse_coding_methods"}, {"score": 0.0043305354943056875, "phrase": "deep_neural_networks"}, {"score": 0.004175006647851034, "phrase": "repeated_application"}, {"score": 0.0041294419262414995, "phrase": "learning_process"}, {"score": 0.004054598617367474, "phrase": "unseen_data_input_vectors"}, {"score": 0.003937649447496154, "phrase": "large_numbers"}, {"score": 0.0037273513742154237, "phrase": "running_time"}, {"score": 0.0035541673947328163, "phrase": "new_approach"}, {"score": 0.003401431460601017, "phrase": "proposed_approach"}, {"score": 0.0033397385936467204, "phrase": "linear_auto-associative_network"}, {"score": 0.003161271802988128, "phrase": "specific_error_function"}, {"score": 0.0030587884803931964, "phrase": "linear_encoder"}, {"score": 0.0030253688063143157, "phrase": "sparse_code"}, {"score": 0.0028014064098604093, "phrase": "neural_network"}, {"score": 0.0026711292765356064, "phrase": "error_function"}, {"score": 0.0025468951021098717, "phrase": "proposed_architecture"}, {"score": 0.002401875938287542, "phrase": "recently_proposed_non-linear_auto-associative_neural_networks"}, {"score": 0.0022734117274137468, "phrase": "sparse_data_representations"}, {"score": 0.0022321322970228308, "phrase": "machine_learning_problems"}, {"score": 0.002191600750693037, "phrase": "appropriate_error_function"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Neural networks", " Sparse coding", " Linear approach", " Encoder-decoder"], "paper_abstract": "Many approaches to transform classification problems from non-linear to linear by feature transformation have been recently presented in the literature. These notably include sparse coding methods and deep neural networks. However, many of these approaches require the repeated application of a learning process upon the presentation of unseen data input vectors, or else involve the use of large numbers of parameters and hyper-parameters, which must be chosen through cross-validation, thus increasing running time dramatically. In this paper, we propose and experimentally investigate a new approach for the purpose of overcoming limitations of both kinds. The proposed approach makes use of a linear auto-associative network (called SCNN) with just one hidden layer. The combination of this architecture with a specific error function to be minimized enables one to learn a linear encoder computing a sparse code which turns out to be as similar as possible to the sparse coding that one obtains by re-training the neural network. Importantly, the linearity of SCNN and the choice of the error function allow one to achieve reduced running time in the learning phase. The proposed architecture is evaluated on the basis of two standard machine learning tasks. Its performances are compared with those of recently proposed non-linear auto-associative neural networks. The overall results suggest that linear encoders can be profitably used to obtain sparse data representations in the context of machine learning problems, provided that an appropriate error function is used during the learning phase. (c) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A linear approach for sparse coding by a two-layer neural network", "paper_id": "WOS:000356105100019"}