{"auto_keywords": [{"score": 0.04841586458101805, "phrase": "data_mining"}, {"score": 0.04355159126538614, "phrase": "correlated_features"}, {"score": 0.00481495049065317, "phrase": "global_redundancy_minimization"}, {"score": 0.004772593750252861, "phrase": "feature_selection"}, {"score": 0.004688989566904871, "phrase": "important_research_topic"}, {"score": 0.004566308708871429, "phrase": "real_data_sets"}, {"score": 0.004506171772318298, "phrase": "high-dimensional_features"}, {"score": 0.00436890075128947, "phrase": "text_mining_applications"}, {"score": 0.004106725110985659, "phrase": "similar_rankings"}, {"score": 0.003929107176349023, "phrase": "large_mutual_information"}, {"score": 0.003742554019950264, "phrase": "limited_number"}, {"score": 0.0035965032173588753, "phrase": "top_non-redundant_features"}, {"score": 0.00353342812142574, "phrase": "useful_mutual_information"}, {"score": 0.00344087639309216, "phrase": "previous_research"}, {"score": 0.003410565849262162, "phrase": "ding_et_al"}, {"score": 0.003350740737336966, "phrase": "important_issue"}, {"score": 0.003066945490483524, "phrase": "greedy_search"}, {"score": 0.003013129637451091, "phrase": "global_feature_redundancy"}, {"score": 0.00278237555651743, "phrase": "new_feature_selection_framework"}, {"score": 0.002721464488351771, "phrase": "feature_redundancy"}, {"score": 0.0024148064819862337, "phrase": "practical_data_mining_application"}, {"score": 0.002361923085067681, "phrase": "benchmark_data_sets"}, {"score": 0.002320449502337321, "phrase": "proposed_method"}, {"score": 0.002279702500235741, "phrase": "feature_selection_results"}, {"score": 0.0022396694076443446, "phrase": "original_methods"}, {"score": 0.0021712920081029194, "phrase": "new_unsupervised_global_and_local_discriminative_feature_selection_method"}, {"score": 0.0021049977753042253, "phrase": "global_feature_redundancy_minimization_framework"}], "paper_keywords": ["Feature selection", " feature ranking", " redundancy minimization"], "paper_abstract": "Feature selection has been an important research topic in data mining, because the real data sets often have high-dimensional features, such as the bioinformatics and text mining applications. Many existing filter feature selection methods rank features by optimizing certain feature ranking criterions, such that correlated features often have similar rankings. These correlated features are redundant and don't provide large mutual information to help data mining. Thus, when we select a limited number of features, we hope to select the top non-redundant features such that the useful mutual information can be maximized. In previous research, Ding et al. recognized this important issue and proposed the minimum Redundancy Maximum Relevance Feature Selection (mRMR) model to minimize the redundancy between sequentially selected features. However, this method used the greedy search, thus the global feature redundancy wasn't considered and the results are not optimal. In this paper, we propose a new feature selection framework to globally minimize the feature redundancy with maximizing the given feature ranking scores, which can come from any supervised or unsupervised methods. Our new model has no parameter so that it is especially suitable for practical data mining application. Experimental results on benchmark data sets show that the proposed method consistently improves the feature selection results compared to the original methods. Meanwhile, we introduce a new unsupervised global and local discriminative feature selection method which can be unified with the global feature redundancy minimization framework and shows superior performance.", "paper_title": "Feature Selection via Global Redundancy Minimization", "paper_id": "WOS:000361245300012"}