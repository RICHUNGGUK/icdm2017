{"auto_keywords": [{"score": 0.050078487022614894, "phrase": "policy_evaluation"}, {"score": 0.027025168654773054, "phrase": "lspe"}, {"score": 0.004649983494539642, "phrase": "comparison"}, {"score": 0.004521803560561489, "phrase": "essential_step"}, {"score": 0.0043713857434347254, "phrase": "value_function"}, {"score": 0.004026505114417176, "phrase": "policy_improvement_step"}, {"score": 0.003911367220315185, "phrase": "research_area"}, {"score": 0.003637669525737665, "phrase": "core_issues"}, {"score": 0.0035852640843794252, "phrase": "stability_guarantees"}, {"score": 0.0035336109352157763, "phrase": "off-policy_scenario"}, {"score": 0.0034826993555190765, "phrase": "sample_efficiency"}, {"score": 0.0034491650585057754, "phrase": "probabilistic_treatment"}, {"score": 0.0031614748814351823, "phrase": "large_number"}, {"score": 0.003131023880006021, "phrase": "new_approaches"}, {"score": 0.0030121159909273897, "phrase": "new_developments"}, {"score": 0.002954361295811154, "phrase": "concise_overview"}, {"score": 0.002883718207958817, "phrase": "underlying_cost_functions"}, {"score": 0.002707829887747594, "phrase": "high_dimensional_feature"}, {"score": 0.0025673860436759874, "phrase": "lsid"}, {"score": 0.0025181367444581993, "phrase": "fpkf"}, {"score": 0.0024818194799172263, "phrase": "residual-gradient_algorithm"}, {"score": 0.002457927249351103, "phrase": "bellman"}, {"score": 0.0024107492572509274, "phrase": "gtd"}, {"score": 0.0023645056426979593, "phrase": "tdc"}, {"score": 0.0021670777282518424, "phrase": "alternative_versions"}, {"score": 0.0021461859545882397, "phrase": "lstd"}, {"score": 0.0021049977753042253, "phrase": "drastically_improved_off-policy_performance"}], "paper_keywords": ["temporal differences", " policy evaluation", " value function estimation", " reinforcement learning"], "paper_abstract": "Policy evaluation is an essential step in most reinforcement learning approaches. It yields a value function, the quality assessment of states for a given policy, which can be used in a policy improvement step. Since the late 1980s, this research area has been dominated by temporal-difference (TD) methods due to their data-efficiency. However, core issues such as stability guarantees in the off-policy scenario, improved sample efficiency and probabilistic treatment of the uncertainty in the estimates have only been tackled recently, which has led to a large number of new approaches. This paper aims at making these new developments accessible in a concise overview, with foci on underlying cost functions, the off-policy scenario as well as on regularization in high dimensional feature, spaces. By presenting the first extensive, systematic comparative evaluations comparing TD, LSID, LSPE, FPKF, the residual-gradient algorithm, Bellman residual minimization, GTD, GTD2 and TDC, we shed light on the strengths and weaknesses of the methods. Moreover, we present alternative versions of LSTD and LSPE with drastically improved off-policy performance.", "paper_title": "Policy Evaluation with Temporal Differences: A Survey and Comparison", "paper_id": "WOS:000335458100001"}