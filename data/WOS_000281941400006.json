{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "least_squares"}, {"score": 0.004649302313891199, "phrase": "linear_regression"}, {"score": 0.00436530211426514, "phrase": "gaussian_noise"}, {"score": 0.004127394241838696, "phrase": "long_time"}, {"score": 0.003821215658570578, "phrase": "robust_regression_techniques"}, {"score": 0.003715579364728729, "phrase": "least_absolute_deviation_method"}, {"score": 0.0034640450599342488, "phrase": "intermediate_cost_function"}, {"score": 0.0033213460816510685, "phrase": "threshold_h"}, {"score": 0.002948066398723126, "phrase": "huber_penalization"}, {"score": 0.002767696345005216, "phrase": "closed-form_solution"}, {"score": 0.002691105581191125, "phrase": "cost_function"}, {"score": 0.0021049977753042253, "phrase": "least_absolute_deviation_estimate"}], "paper_keywords": ["linear regression", " least squares", " least deviations", " robust estimation"], "paper_abstract": "Linear regression is mostly dominated by least squares which corresponds to Gaussian noise. But it is known for a long time that if outliers may be present in the measurements, robust regression techniques such as the least absolute deviation method, are preferable. One can also consider an intermediate cost function where residues larger than a threshold h are weighted by the l(1)-norm and the others by the l(2)-norm. This leads to the Huber penalization that is optimal for a certain contaminated Gaussian distribution. No closed-form solution exist for these cost function and we propose an algorithm which, initialized by the least squares estimate that is optimal for h infinite, builds the sequence of estimates associated with decreasing It, a zero h corresponding the least absolute deviation estimate.", "paper_title": "From least squares to least deviations", "paper_id": "WOS:000281941400006"}