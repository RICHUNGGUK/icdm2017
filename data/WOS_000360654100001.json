{"auto_keywords": [{"score": 0.049324540543845156, "phrase": "polynomial_learnability"}, {"score": 0.013169554714657286, "phrase": "gaussian_mixtures"}, {"score": 0.010518158425545633, "phrase": "low_dimension"}, {"score": 0.008804281583571267, "phrase": "polynomial_number"}, {"score": 0.008529977676233882, "phrase": "polynomial_families"}, {"score": 0.008006537057129825, "phrase": "high_dimensions"}, {"score": 0.00481495049065317, "phrase": "distribution_families."}, {"score": 0.004701556483912018, "phrase": "probability_distributions"}, {"score": 0.004647586226713605, "phrase": "gaussian"}, {"score": 0.004518442937844318, "phrase": "significant_attention"}, {"score": 0.0044826811976193485, "phrase": "theoretical_computer_science"}, {"score": 0.004447201233380302, "phrase": "machine_learning"}, {"score": 0.00435971969507033, "phrase": "major_progress"}, {"score": 0.0043080553835735825, "phrase": "general_question"}, {"score": 0.004240116703465252, "phrase": "gaussian_mixture_distributions"}, {"score": 0.004140203942496321, "phrase": "current_work"}, {"score": 0.003994714109598034, "phrase": "high_dimension"}, {"score": 0.003947358102140903, "phrase": "arbitrary_fixed_number"}, {"score": 0.0037634608487353183, "phrase": "gaussian_distribution"}, {"score": 0.0037188360359702182, "phrase": "fixed_number"}, {"score": 0.003036040409062437, "phrase": "almost_all_common_probability_distributions"}, {"score": 0.0029060170624404242, "phrase": "real_algebraic_geometry"}, {"score": 0.0027267406923803367, "phrase": "polynomial_time"}, {"score": 0.002662393295121998, "phrase": "sample_points"}, {"score": 0.00252812242767189, "phrase": "independent_interest"}, {"score": 0.0024586427394152196, "phrase": "gaussian_mixture_distribution"}, {"score": 0.002391067964886602, "phrase": "deterministic_algorithm"}, {"score": 0.002372103459168231, "phrase": "dimensionality_reduction"}, {"score": 0.0022886031992739126, "phrase": "high-dimensional_mixture"}, {"score": 0.0022434880661725493, "phrase": "parameter_estimations"}, {"score": 0.0021049977753042253, "phrase": "arbitrary_gaussian_mixtures"}], "paper_keywords": ["Gaussian mixture models", " statistical inference", " learning theory", " semialgebraic geometry"], "paper_abstract": "The question of polynomial learnability of probability distributions, particularly Gaussian mixture distributions, has recently received significant attention in theoretical computer science and machine learning. However, despite major progress, the general question of polynomial learnability of Gaussian mixture distributions still remained open. The current work resolves the question of polynomial learnability for Gaussian mixtures in high dimension with an arbitrary fixed number of components. Specifically, we show that parameters of a Gaussian distribution with a fixed number of components can be learned using a sample whose size is polynomial in dimension and all other parameters. The result on learning Gaussian mixtures relies on an analysis of distributions belonging to what we call polynomial families in low dimension. These families are characterized by their moments being polynomial in parameters and include almost all common probability distributions as well as their mixtures and products. Using tools from real algebraic geometry, we show that parameters of any distribution belonging to such a family can be learned in polynomial time and using a polynomial number of sample points. The result on learning polynomial families is quite general and is of independent interest. To estimate parameters of a Gaussian mixture distribution in high dimensions, we provide a deterministic algorithm for dimensionality reduction. This allows us to reduce learning a high-dimensional mixture to a polynomial number of parameter estimations in low dimension. Combining this reduction with the results on polynomial families yields our result on learning arbitrary Gaussian mixtures in high dimensions.", "paper_title": "POLYNOMIAL LEARNING OF DISTRIBUTION FAMILIES", "paper_id": "WOS:000360654100001"}