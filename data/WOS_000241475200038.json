{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "optimal_size"}, {"score": 0.01293308352636005, "phrase": "singular_machines"}, {"score": 0.010047093064961923, "phrase": "singic"}, {"score": 0.00475413908468027, "phrase": "learning_machines"}, {"score": 0.004664352260912174, "phrase": "central_issue"}, {"score": 0.004576253356889219, "phrase": "statistical_learning_theory"}, {"score": 0.004377077848734575, "phrase": "theoretical_criteria"}, {"score": 0.0042672132998646545, "phrase": "bic"}, {"score": 0.0037104924063050848, "phrase": "markov"}, {"score": 0.0036167509502276294, "phrase": "bayesian_networks"}, {"score": 0.0033937776228549557, "phrase": "singular_information_criterion"}, {"score": 0.002912963877144772, "phrase": "learning_coefficient"}, {"score": 0.0021870332034407817, "phrase": "gaussian_mixtures"}, {"score": 0.0021049977753042253, "phrase": "effective_strategy"}], "paper_keywords": [""], "paper_abstract": "To decide the optimal size of learning machines is a central issue in the statistical learning theory, and that is why some theoretical criteria such as the BIC are developed. However, they cannot be applied to singular machines, and it is known that many practical learning machines e.g. mixture models, hidden Markov models, and Bayesian networks, are singular. Recently, we proposed the Singular Information Criterion (SingIC), which allows us to select the optimal size of singular machines. The SingIC is based on the analysis of the learning coefficient. So, the machines, to which the SingIC can be applied, are still limited. In this paper, we propose an extension of this criterion, which enables us to apply it to many singular machines, and evaluate the efficiency in Gaussian mixtures. The results offer an effective strategy to select the optimal size.", "paper_title": "A model selection method based on bound of learning coefficient", "paper_id": "WOS:000241475200038"}