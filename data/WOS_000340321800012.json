{"auto_keywords": [{"score": 0.04505758975136344, "phrase": "mas"}, {"score": 0.0073246487923670145, "phrase": "rl"}, {"score": 0.00481495049065317, "phrase": "multi-agent_territory_division"}, {"score": 0.004548253531664032, "phrase": "disaster_rescuing"}, {"score": 0.004483909657713567, "phrase": "robotic_surveillance"}, {"score": 0.004452078350485375, "phrase": "warehouse_systems"}, {"score": 0.004342430078306461, "phrase": "multi-agent_systems"}, {"score": 0.004101797841848363, "phrase": "better_performance"}, {"score": 0.004015025120733852, "phrase": "duplicate_work"}, {"score": 0.0036727235025069828, "phrase": "territory_division"}, {"score": 0.003633652174971415, "phrase": "children's_game"}, {"score": 0.003607834976869384, "phrase": "hide-"}, {"score": 0.0034077704063304208, "phrase": "hierarchical_learning_scheme"}, {"score": 0.003383552654319383, "phrase": "reinforcement_learning"}, {"score": 0.0031845067058084583, "phrase": "composite_states"}, {"score": 0.003072918736387408, "phrase": "multiple_agent_learning"}, {"score": 0.0030078460113945136, "phrase": "revised_version"}, {"score": 0.002975826686621829, "phrase": "standard_updating_rule"}, {"score": 0.002881793492919646, "phrase": "multiple_seekers"}, {"score": 0.002780783274864117, "phrase": "different_maps"}, {"score": 0.0027025233739857374, "phrase": "optimal_solutions"}, {"score": 0.002664221051021009, "phrase": "complexity_analysis"}, {"score": 0.002570817338292482, "phrase": "state_aggregation"}, {"score": 0.0025073851194697397, "phrase": "state_space_explosion"}, {"score": 0.002359759876402679, "phrase": "learning_model"}, {"score": 0.002309754152767196, "phrase": "aggregation_technique"}, {"score": 0.0022851491693940272, "phrase": "enhanced_model"}, {"score": 0.002212892265364764, "phrase": "promising_performance"}, {"score": 0.0021971473452127126, "phrase": "higher_convergence_rate"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Multi-agent systems", " Reinforcement Learning", " Hierarchical learning", " Q-learning", " State aggregation", " Hide-and-Seek"], "paper_abstract": "In many applications in Robotics such as disaster rescuing, mine detection, robotic surveillance and warehouse systems, it is crucial to build multi-agent systems (MAS) in which agents cooperate to complete a sequence of tasks. For better performance in such systems, e.g. minimizing duplicate work, agents need to agree on how to divide and plan that sequence of tasks among themselves. This paper targets the problem of territory division in the children's game of Hide-and-Seek as a test-bed for our proposed approach. The problem is solved in a hierarchical learning scheme using Reinforcement Learning (RL). Based on Q-learning, our learning model is presented in detail; definition of composite states, actions, and reward function to deal with multiple agent learning. In addition, a revised version of the standard updating rule of the Q-learning is proposed to cope with multiple seekers. The model is examined on a set of different maps, on which it converges to the optimal solutions. After the complexity analysis of the algorithm, we enhanced it by using state aggregation (SA) to alleviate the state space explosion. Two levels of aggregation are devised: topological and hiding aggregation. After elaborating how the learning model is modified to handle the aggregation technique, the enhanced model is examined by some experiments. Results indicate promising performance with higher convergence rate and up to 10 x space reduction. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Aggregate Reinforcement Learning for multi-agent territory division: The Hide-and-Seek game", "paper_id": "WOS:000340321800012"}