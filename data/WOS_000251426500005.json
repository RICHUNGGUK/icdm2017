{"auto_keywords": [{"score": 0.04559556617638774, "phrase": "artificial_neural_networks"}, {"score": 0.004815315858763141, "phrase": "distributed"}, {"score": 0.004616859702338596, "phrase": "large-scale_dataset"}, {"score": 0.004406260054779782, "phrase": "real_problems"}, {"score": 0.004205226397573506, "phrase": "open_question"}, {"score": 0.004127394241838696, "phrase": "computing_efficiency"}, {"score": 0.0040890178236849825, "phrase": "learning_stability"}, {"score": 0.004032118497047498, "phrase": "traditional_neural_networks"}, {"score": 0.003976007772514762, "phrase": "large_amount"}, {"score": 0.003939033374807842, "phrase": "running_time"}, {"score": 0.00379452936798156, "phrase": "large-scale_learning_dataset"}, {"score": 0.003570889780957037, "phrase": "neural_network"}, {"score": 0.003472149546470424, "phrase": "large-scale_classification"}, {"score": 0.0034398445347109396, "phrase": "protein_secondary_structure"}, {"score": 0.0031770714776989282, "phrase": "large-scale_dataset_classification"}, {"score": 0.0026974065918918275, "phrase": "experimental_evaluation"}, {"score": 0.0026722904884984348, "phrase": "distributed-learning_strategy"}, {"score": 0.0025741370320813968, "phrase": "time_complexity"}, {"score": 0.002514625755809045, "phrase": "classification_accuracy"}, {"score": 0.002410942838102255, "phrase": "novel_distributed-learning_strategy"}, {"score": 0.002344201345813952, "phrase": "parallel_computing_efficiency"}, {"score": 0.0022686625017583387, "phrase": "previous_algorithms"}, {"score": 0.0022058510091498666, "phrase": "protein_secondary_structure_prediction"}, {"score": 0.0021049977753042253, "phrase": "practical_applications"}], "paper_keywords": ["distributed learning strategy", " artificial neural network", " classification", " multiagent", " protein secondary structure prediction"], "paper_abstract": "Learning with very large-scale datasets is always necessary when handling real problems using artificial neural networks. However, it is still an open question how to balance computing efficiency and learning stability, when traditional neural networks spend a large amount of running time and memory to solve a problem with large-scale learning dataset. In this paper, we report the. rst evaluation of neural network distributed-learning strategies in large-scale classification over protein secondary structure. Our accomplishments include: (1) an architecture analysis on distributed-learning, (2) the development of scalable distributed system for large-scale dataset classification, (3) the description of a novel distributed-learning strategy based on chips, (4) a theoretical analysis of distributed-learning strategies for structure-distributed and data-distributed, (5) an investigation and experimental evaluation of distributed-learning strategy based-on chips with respect to time complexity and their effect on the classification accuracy of artificial neural networks. It is demonstrated that the novel distributed-learning strategy is better-balanced in parallel computing efficiency and stability as compared with the previous algorithms. The application of the protein secondary structure prediction demonstrates that this method is feasible and effective in practical applications.", "paper_title": "Distributed learning strategy based on chips for classification with large-scale dataset", "paper_id": "WOS:000251426500005"}