{"auto_keywords": [{"score": 0.043211748297316055, "phrase": "sample_entropy"}, {"score": 0.00481495049065317, "phrase": "entropy_estimators"}, {"score": 0.004527371799277529, "phrase": "continuous_random_variable"}, {"score": 0.004256895619944631, "phrase": "ebrahimi_et_al"}, {"score": 0.00400563394225143, "phrase": "lett"}, {"score": 0.003574830654211291, "phrase": "vasicek's_estimator"}, {"score": 0.0033957904376127187, "phrase": "j._r._stat"}, {"score": 0.0033616301506402936, "phrase": "soc"}, {"score": 0.003326848757253309, "phrase": "ser"}, {"score": 0.00283668130051541, "phrase": "scand"}, {"score": 0.0028076592725295646, "phrase": "j._statist"}, {"score": 0.002559447122366886, "phrase": "correa"}, {"score": 0.002443752244104908, "phrase": "comm"}, {"score": 0.002193447487477984, "phrase": "proposed_estimator"}, {"score": 0.0021709933368474223, "phrase": "smaller_mean-squared_error"}, {"score": 0.0021049977753042253, "phrase": "real_example"}], "paper_keywords": ["information theory", " entropy estimator", " exponential", " normal", " uniform"], "paper_abstract": "The paper introduces an estimator of the entropy of a continuous random variable. The estimator is obtained by modifying the estimator proposed by Ebrahimi et al. [Two measures of sample entropy, Statist. Probab. Lett. 20 (1994), pp. 225234]. The consistency of the estimator is proved and comparisons are made with Vasicek's estimator [A test for normality based on sample entropy, J. R. Stat. Soc. Ser. B 38 (1976), pp. 5459], van Es estimator [Estimating functionals related to a density by class of statistics based on spacings, Scand. J. Statist. 19 (1992), pp. 6172], Ebrahimi et al. estimator and Correa estimator [A new estimator of entropy, Comm. Statist. Theory Methods 24 (1995), pp. 24392449]. The results indicate that the proposed estimator has smaller mean-squared error than above estimators. A real example is presented and analysed.", "paper_title": "On the entropy estimators", "paper_id": "WOS:000317276900012"}