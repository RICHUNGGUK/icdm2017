{"auto_keywords": [{"score": 0.03589903250220555, "phrase": "labeled_samples"}, {"score": 0.013824636092943342, "phrase": "malicious_target_class"}, {"score": 0.010147678273401009, "phrase": "accurate_classifier"}, {"score": 0.00481495049065317, "phrase": "internet_worms"}, {"score": 0.0047323590206695305, "phrase": "user's_email_inbox"}, {"score": 0.00469159356595623, "phrase": "spam_bear_certain_similarities"}, {"score": 0.0037786899855446, "phrase": "innocuous_samples"}, {"score": 0.0035873530542209686, "phrase": "learning_techniques"}, {"score": 0.0032191849045822415, "phrase": "helpful_teacher"}, {"score": 0.0027781870000242004, "phrase": "great_extent"}, {"score": 0.0026835045963349647, "phrase": "practical_attacks"}, {"score": 0.0021604924928015283, "phrase": "highly_effective_instances"}], "paper_keywords": ["automatic signature generation", " machine learning", " worm", " spam"], "paper_abstract": "Defending a server against Internet worms and defending a user's email inbox against spam bear certain similarities. In both cases, a stream of samples arrives, and a classifier must automatically determine whether each sample falls into a malicious target class (e.g., worm network traffic, or spam email). A learner typically generates a classifier automatically by analyzing two labeled training pools: one of innocuous samples, and one of samples that fall in the malicious target class. Learning techniques have previously found success in settings where the content of the labeled samples used in training is either random, or even constructed by a helpful teacher, who aims to speed learning of an accurate classifier. In the case of learning classifiers for worms and spam, however, an adversary controls the content of the labeled samples to a great extent. In this paper, we describe practical attacks against learning, in which an adversary constructs labeled samples that, when used to train a learner, prevent or severely delay generation of an accurate classifier. We show that even a delusive adversary, whose samples are all correctly labeled, can obstruct learning. We simulate and implement highly effective instances of these attacks against the Polygraph [15] automatic polymorphic worm signature generation algorithms.", "paper_title": "Paragraph: Thwarting signature learning by training maliciously", "paper_id": "WOS:000241449400005"}