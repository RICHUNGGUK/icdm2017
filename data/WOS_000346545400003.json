{"auto_keywords": [{"score": 0.0489041172623351, "phrase": "regular_expressions"}, {"score": 0.006528496900880068, "phrase": "invalid_strings"}, {"score": 0.00481495049065317, "phrase": "valid_and_invalid_test_data"}, {"score": 0.004726052718910578, "phrase": "web_searches"}, {"score": 0.004624401316503576, "phrase": "automatic_input_data_generation"}, {"score": 0.0044968982227349625, "phrase": "program_coverage"}, {"score": 0.004359328949187473, "phrase": "path_constraints"}, {"score": 0.003934401172799425, "phrase": "missing_implementation"}, {"score": 0.003743487506598038, "phrase": "string_data_types"}, {"score": 0.0036742989715836745, "phrase": "novel_approach"}, {"score": 0.003640183972131238, "phrase": "string_test_data"}, {"score": 0.003606384575869449, "phrase": "validation_routines"}, {"score": 0.0034959622587016222, "phrase": "program_identifiers"}, {"score": 0.0034634973175137486, "phrase": "web_search_queries"}, {"score": 0.003367894141916481, "phrase": "string_type"}, {"score": 0.0033159224502739247, "phrase": "email_address"}, {"score": 0.003145125312514813, "phrase": "test_cases"}, {"score": 0.0029007179823144357, "phrase": "test_inputs"}, {"score": 0.0028382152567141784, "phrase": "validation_routine"}, {"score": 0.002759823989317719, "phrase": "empirical_study"}, {"score": 0.002633941112839883, "phrase": "dynamic_symbolic_execution_and_search-based_testing_approaches"}, {"score": 0.002429161048032688, "phrase": "valid_strings"}, {"score": 0.0023183255207530193, "phrase": "dynamic_symbolic_execution"}, {"score": 0.0023039337947957077, "phrase": "search-based_testing"}, {"score": 0.0021446993452098597, "phrase": "implementation_errors"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Test data generation", " Web searches", " Regular expressions"], "paper_abstract": "Classic approaches to automatic input data generation are usually driven by the goal of obtaining program coverage and the need to solve or find solutions to path constraints to achieve this. As inputs are generated with respect to the structure of the code, they can be ineffective, difficult for humans to read, and unsuitable for testing missing implementation. Furthermore, these approaches have known limitations when handling constraints that involve operations with string data types. This paper presents a novel approach for generating string test data for string validation routines, by harnessing the Internet. The technique uses program identifiers to construct web search queries for regular expressions that validate the format of a string type (such as an email address). It then performs further web searches for strings that match the regular expressions, producing examples of test cases that are both valid and realistic. Following this, our technique mutates the regular expressions to drive the search for invalid strings, and the production of test inputs that should be rejected by the validation routine. The paper presents the results of an empirical study evaluating our approach. The study was conducted on 24 string input validation routines collected from 10 open source projects. While dynamic symbolic execution and search-based testing approaches were only able to generate a very low number of values successfully, our approach generated values with an accuracy of 34% on average for the case of valid strings, and 99% on average for the case of invalid strings. Furthermore, whereas dynamic symbolic execution and search-based testing approaches were only capable of detecting faults in 8 routines, our approach detected faults in 17 out of the 19 validation routines known to contain implementation errors. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Automatic generation of valid and invalid test data for string validation routines using web searches and regular expressions", "paper_id": "WOS:000346545400003"}