{"auto_keywords": [{"score": 0.04385710888015479, "phrase": "reinforcement_learning"}, {"score": 0.043236008352619086, "phrase": "rgd_algorithm"}, {"score": 0.00481495049065317, "phrase": "value-function_approximation"}, {"score": 0.004605258779434384, "phrase": "restricted_gradient-descent"}, {"score": 0.004448471107649881, "phrase": "training_method"}, {"score": 0.004404658750724105, "phrase": "local_radial-basis_function_networks"}, {"score": 0.00395017798595193, "phrase": "relevant_features"}, {"score": 0.0038919354605673104, "phrase": "state_space"}, {"score": 0.003815607281455725, "phrase": "linear_model"}, {"score": 0.0037039021744127783, "phrase": "value_function"}, {"score": 0.0035249212996297332, "phrase": "gradient-descent_algorithm"}, {"score": 0.003472927015404464, "phrase": "hidden_units"}, {"score": 0.0033379819706003885, "phrase": "conservative_modifications"}, {"score": 0.0032724821705972357, "phrase": "learning_process"}, {"score": 0.0028203950416405563, "phrase": "changing_policy"}, {"score": 0.0027650234530228923, "phrase": "different_requirements"}, {"score": 0.002724207384182295, "phrase": "approximator_structure"}, {"score": 0.0026973310437053573, "phrase": "computational_experiments"}, {"score": 0.002579622880371082, "phrase": "better_value-function_approximations"}, {"score": 0.0025415366388578465, "phrase": "standard_gradient-descent_method"}, {"score": 0.0023476849595393872, "phrase": "acrobot_tasks"}, {"score": 0.0022901865731199413, "phrase": "sarsa"}, {"score": 0.0022675820982741347, "phrase": "competitive_results"}, {"score": 0.00216858686400722, "phrase": "evolutionary_and_recent_reinforcement-learning_algorithms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["reinforcement learning", " neuro-dynamic programming", " value-function approximation", " radial-basis-function networks"], "paper_abstract": "This work presents the restricted gradient-descent (RGD) algorithm, a training method for local radial-basis function networks specifically developed to be used in the context of reinforcement learning. The RGD algorithm can be seen as a way to extract relevant features from the state space to feed a linear model computing an approximation of the value function. Its basic idea is to restrict the way the standard gradient-descent algorithm changes the hidden units of the approximator, which results in conservative modifications that make the learning process less prone to divergence. The algorithm is also able to configure the topology of the network, an important characteristic in the context of reinforcement learning, where the changing policy may result in different requirements on the approximator structure. Computational experiments are presented showing that the RGD algorithm consistently generates better value-function approximations than the standard gradient-descent method, and that the latter is more susceptible to divergence. In the pole-balancing and Acrobot tasks, RGD combined with SARSA presents competitive results with other methods found in the literature, including evolutionary and recent reinforcement-learning algorithms. (c) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Restricted gradient-descent algorithm for value-function approximation in reinforcement learning", "paper_id": "WOS:000253329500004"}