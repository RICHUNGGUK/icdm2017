{"auto_keywords": [{"score": 0.03014720888291599, "phrase": "ml"}, {"score": 0.00481495049065317, "phrase": "noisy_machine_learning_problems"}, {"score": 0.004681226806682058, "phrase": "model-assisted_parameter_tuning"}, {"score": 0.004551199992546761, "phrase": "supervised_machine_learning"}, {"score": 0.004466519142879166, "phrase": "tuned_models"}, {"score": 0.003934540154108188, "phrase": "high_computation_times"}, {"score": 0.003861288537079326, "phrase": "real_burden"}, {"score": 0.003807242767474929, "phrase": "tuning_algorithms"}, {"score": 0.003684048704119722, "phrase": "reduced_number"}, {"score": 0.0034171734618737436, "phrase": "model_accuracies"}, {"score": 0.003229741076887148, "phrase": "novel_approach"}, {"score": 0.0030525578912808647, "phrase": "ml_models"}, {"score": 0.0029260602766930065, "phrase": "optimization_task"}, {"score": 0.002765490759189057, "phrase": "efficient_global_optimization"}, {"score": 0.0026384167183586015, "phrase": "noisy_experiments"}, {"score": 0.002564985233317994, "phrase": "ego_algorithms"}, {"score": 0.0024818877663889813, "phrase": "surrogate_models"}, {"score": 0.0023346234464901978, "phrase": "ego_techniques"}, {"score": 0.002301896758256575, "phrase": "traditional_approaches"}, {"score": 0.0022696277881969896, "phrase": "latin_hypercube_sampling"}, {"score": 0.0021857667493036786, "phrase": "ego_variants"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Machine learning", " Multi-criteria optimization", " Efficient Global Optimization", " Kriging", " Hypervolume indicator"], "paper_abstract": "Recent research revealed that model-assisted parameter tuning can improve the quality of supervised machine learning (ML) models. The tuned models were especially found to generalize better and to be more robust compared to other optimization approaches. However, the advantages of the tuning often came along with high computation times, meaning a real burden for employing tuning algorithms. While the training with a reduced number of patterns can be a solution to this, it is often connected with decreasing model accuracies and increasing instabilities and noise. Hence, we propose a novel approach defined by a two criteria optimization task, where both the runtime and the quality of ML models are optimized. Because the budgets for this optimization task are usually very restricted in ML, the surrogate-assisted Efficient Global Optimization (EGO) algorithm is adapted. In order to cope with noisy experiments, we apply two hypervolume indicator based EGO algorithms with smoothing and reinterpolation of the surrogate models. The techniques do not need replicates. We find that these EGO techniques can outperform traditional approaches such as latin hypercube sampling (LHS), as well as EGO variants with replicates. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Efficient multi-criteria optimization on noisy machine learning problems", "paper_id": "WOS:000350648100029"}