{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "dynamic_text_collections"}, {"score": 0.049860201304014265, "phrase": "latent_dirichlet_allocation_model"}, {"score": 0.03323025071867147, "phrase": "different_approaches"}, {"score": 0.029728311431631026, "phrase": "lda_model"}, {"score": 0.0047275981622989845, "phrase": "information_retrieval"}, {"score": 0.004673805754655082, "phrase": "automatic_text_summarization"}, {"score": 0.004631210802518759, "phrase": "static_underlying_collection"}, {"score": 0.004568041790417785, "phrase": "temporal_dimension"}, {"score": 0.004485148665944499, "phrase": "real_world_settings"}, {"score": 0.004444265372139406, "phrase": "individual_documents"}, {"score": 0.00434367302929861, "phrase": "prime_example"}, {"score": 0.004111404998055623, "phrase": "different_times"}, {"score": 0.004018316270624956, "phrase": "web_documents"}, {"score": 0.003972562128994128, "phrase": "dynamic_nature"}, {"score": 0.0038121021668201154, "phrase": "new_challenge"}, {"score": 0.003717238919538653, "phrase": "standard_text_summarization"}, {"score": 0.0037002454723820103, "phrase": "retrieval_techniques"}, {"score": 0.0036164322430169093, "phrase": "major_points"}, {"score": 0.0035588910818525987, "phrase": "entire_document"}, {"score": 0.003534510708818631, "phrase": "condensed_form"}, {"score": 0.0035022622417217627, "phrase": "new_task"}, {"score": 0.003195474819156168, "phrase": "specific_period"}, {"score": 0.003108773553351178, "phrase": "extractive_summarization_techniques"}, {"score": 0.0030803974549773673, "phrase": "individual_terms"}, {"score": 0.0029626516852718572, "phrase": "final_summary"}, {"score": 0.002862490912619878, "phrase": "hidden_topic_structures"}, {"score": 0.0027784204938480985, "phrase": "separate_topics"}, {"score": 0.0027593719700696453, "phrase": "changed_terms"}, {"score": 0.002605723511906145, "phrase": "wikipedia"}, {"score": 0.002546533249550624, "phrase": "proposed_system"}, {"score": 0.0025059724686541263, "phrase": "temporal_interval"}, {"score": 0.0024887871547718213, "phrase": "reference_summary"}, {"score": 0.0024717194013167423, "phrase": "article's_content"}, {"score": 0.0024046080682465732, "phrase": "significant_event"}, {"score": 0.002302046794342723, "phrase": "manual_summaries"}, {"score": 0.0022915080516157866, "phrase": "rouge_metrics"}, {"score": 0.0021837160156693205, "phrase": "rouge_scores"}, {"score": 0.002168735848715569, "phrase": "lda-based_approach"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Changes summarization", " Temporal term weighting", " Sentence ranking", " Latent Dirichlet Allocation", " Wikipedia"], "paper_abstract": "In the area of Information Retrieval, the task of automatic text summarization usually assumes a static underlying collection of documents, disregarding the temporal dimension of each document. However, in real world settings, collections and individual documents rarely stay unchanged over time. The World Wide Web is a prime example of a collection where information changes both frequently and significantly over time, with documents being added, modified or just deleted at different times. In this context, previous work addressing the summarization of web documents has simply discarded the dynamic nature of the web, considering only the latest published version of each individual document. This paper proposes and addresses a new challenge - the automatic summarization of changes in dynamic text collections. In standard text summarization, retrieval techniques present a summary to the user by capturing the major points expressed in the most recent version of an entire document in a condensed form. In this new task, the goal is to obtain a summary that describes the most significant changes made to a document during a given period. In other words, the idea is to have a summary of the revisions made to a document over a specific period of time. This paper proposes different approaches to generate summaries using extractive summarization techniques. First, individual terms are scored and then this information is used to rank and select sentences to produce the final summary. A system based on Latent Dirichlet Allocation model (LDA) is used to find the hidden topic structures of changes. The purpose of using the LDA model is to identify separate topics where the changed terms from each topic are likely to carry at least one significant change. The different approaches are then compared with the previous work in this area. A collection of articles from Wikipedia, including their revision history, is used to evaluate the proposed system. For each article, a temporal interval and a reference summary from the article's content are selected manually. The articles and intervals in which a significant event occurred are carefully selected. The summaries produced by each of the approaches are evaluated comparatively to the manual summaries using ROUGE metrics. It is observed that the approach using the LDA model outperforms all the other approaches. Statistical tests reveal that the differences in ROUGE scores for the LDA-based approach is statistically significant at 99% over baseline. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Summarization of changes in dynamic text collections using Latent Dirichlet Allocation model", "paper_id": "WOS:000362619300005"}