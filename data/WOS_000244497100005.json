{"auto_keywords": [{"score": 0.0449508648666577, "phrase": "minave"}, {"score": 0.04015076015860894, "phrase": "average_distribution"}, {"score": 0.00481495049065317, "phrase": "finite_memoryless_monotone_sources"}, {"score": 0.004623224002010226, "phrase": "finite_monotone_sources"}, {"score": 0.004581660953729745, "phrase": "n_symbols"}, {"score": 0.00447937340641839, "phrase": "selection_criterion"}, {"score": 0.004359625030298944, "phrase": "average_redundancy"}, {"score": 0.004129607015893271, "phrase": "average_probability_distribution_p-n"}, {"score": 0.004055678969604977, "phrase": "huffman"}, {"score": 0.003983016985197904, "phrase": "minimum_average_redundancy"}, {"score": 0.003638823482920949, "phrase": "average_entropy"}, {"score": 0.0035898158129721003, "phrase": "monotone_distributions"}, {"score": 0.0027867539356701302, "phrase": "simple_fixed-length_code"}, {"score": 0.002724428781940696, "phrase": "optimal_code"}, {"score": 0.002687702958515561, "phrase": "efficient_near-optimal_encoding_technique"}, {"score": 0.002511456696048684, "phrase": "minimax"}, {"score": 0.0024111277643627154, "phrase": "associated_codes"}, {"score": 0.00233594452829104, "phrase": "average_performance"}, {"score": 0.0023044434462692483, "phrase": "minimax_code"}, {"score": 0.0022427071130840647, "phrase": "informational_divergence"}, {"score": 0.0021925226977381244, "phrase": "minimax_distribution"}, {"score": 0.0021055166127310725, "phrase": "n."}], "paper_keywords": ["average redundancy", " finite monotone sources", " fixed-length code", " Huffman code", " Minave code", " minimax code", " minimum average criterion"], "paper_abstract": "The problem of selecting a code for finite monotone sources with N symbols is considered. The selection criterion is based on minimizing the average redundancy (called Minave criterion) instead of its maximum (i.e., Minimax criterion). The average probability distribution P-N, whose associated Huffman code has the minimum average redundancy, is derived. The entropy of the average distribution (i.e., H(P-N)) and the average entropy of the monotone distributions (i.e., H(P-N)) are studied. It is shown that both log N - H(P-N) and log N - H(P-N) are asymptotically equal to a constant (similar or equal to 0.61). Therefore, there is only a negligible penalty (at most 1.61 bits/symbol) in using a simple fixed-length code with respect to the optimal code. An efficient near-optimal encoding technique is also proposed. The consequences of the two approaches, i.e., Minave and Minimax, are compared in terms of their associated distributions and associated codes. In order to evaluate the average performance of the Minimax code, we prove that the informational divergence of the average distribution and Minimax distribution asymptotically grows as - 2.275 + log log N.", "paper_title": "The minimum average code for finite memoryless monotone sources", "paper_id": "WOS:000244497100005"}