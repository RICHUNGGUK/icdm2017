{"auto_keywords": [{"score": 0.04301774886307637, "phrase": "knn"}, {"score": 0.04114345524645095, "phrase": "knn's_accuracy"}, {"score": 0.008208091067339537, "phrase": "nb"}, {"score": 0.007994052516980885, "phrase": "knndw"}, {"score": 0.00481495049065317, "phrase": "dynamic_k-nearest-neighbor"}, {"score": 0.004597245530280526, "phrase": "k-nearest-neighbor"}, {"score": 0.004331675965350876, "phrase": "classification_problems"}, {"score": 0.003794954883650141, "phrase": "simple_vote"}, {"score": 0.0032807941194429235, "phrase": "irrelevant_attributes"}, {"score": 0.003091036449708673, "phrase": "improved_algorithm"}, {"score": 0.003050376875258722, "phrase": "dynamic_k"}, {"score": 0.0028930017387859804, "phrase": "attribute_weighted"}, {"score": 0.0023249757987087055, "phrase": "experimental_results"}, {"score": 0.002279217896185919, "phrase": "dknaw"}, {"score": 0.0021049977753042253, "phrase": "lwnb."}], "paper_keywords": [""], "paper_abstract": "K-Nearest-Neighbor (KNN) has been widely used in classification problems. However, there exist three main problems confronting KNN according to our observation: 1) KNN's accuracy is degraded by a simple vote; 2) KNN's accuracy is typically sensitive to the value of K; 3) KNN's accuracy may be dominated by some irrelevant attributes. In this paper, we presented an improved algorithm called Dynamic K-Nearest-Neighbor Naive Bayes with Attribute Weighted (DKNAW). We experimentally tested its accuracy, using the whole 36 UCI data sets selected by Weka[1], and compared it to NB, KNN, KNNDW, and LWNB[2]. The experimental results show that DKNAW significantly outperforms NB, KNN, and KNNDW and slightly outperforms LWNB.", "paper_title": "Dynamic K-Nearest-Neighbor naive Bayes with attribute weighted", "paper_id": "WOS:000241106000041"}