{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "mobile_robots"}, {"score": 0.029070079914963482, "phrase": "state_representation"}, {"score": 0.00468991337957911, "phrase": "main_problems"}, {"score": 0.004068510313497539, "phrase": "learning_approach"}, {"score": 0.003962779958955344, "phrase": "reinforcement-based_strategy"}, {"score": 0.0039006600544599537, "phrase": "dynamic_sensor-state_mapping"}, {"score": 0.0035477930970463432, "phrase": "different_environment"}, {"score": 0.0034738037330064885, "phrase": "different_task"}, {"score": 0.0033480035183418642, "phrase": "state_space"}, {"score": 0.0031594291695784286, "phrase": "learning_algorithm"}, {"score": 0.0030130446986008277, "phrase": "robot_failure"}, {"score": 0.00291923164591297, "phrase": "control_policy"}, {"score": 0.002858313353461727, "phrase": "desired_behaviour"}, {"score": 0.002613239644974825, "phrase": "empty_state_space"}, {"score": 0.0025722202862084186, "phrase": "new_states"}, {"score": 0.0025052769325080255, "phrase": "new_situations"}, {"score": 0.0024017638230939514, "phrase": "dynamic_creation"}, {"score": 0.002242577809454225, "phrase": "ad_hoc_representation"}, {"score": 0.00218419478369074, "phrase": "exhaustive_study"}], "paper_keywords": ["Reinforcement learning", " State representation", " Fuzzy ART", " Robot learning"], "paper_abstract": "One of the main problems of robots is the lack of adaptability and the need for adjustment every time the robot changes its working place. To solve this, we propose a learning approach for mobile robots using a reinforcement-based strategy and a dynamic sensor-state mapping. This strategy, practically parameterless, minimises the adjustments needed when the robot operates in a different environment or performs a different task. Our system will simultaneously learn the state space and the action to execute on each state. The learning algorithm will attempt to maximise the time before a robot failure in order to obtain a control policy suited to the desired behaviour, thus providing a more interpretable learning process. The state representation will be created dynamically, starting with an empty state space and adding new states as the robot finds new situations that has not seen before. A dynamic creation of the state representation will avoid the classic, error-prone and cyclic process of designing and testing an ad hoc representation. We performed an exhaustive study of our approach, comparing it with other classic strategies. Unexpectedly, learning both perception and action does not increase the learning time. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Simultaneous learning of perception and action in mobile robots", "paper_id": "WOS:000285908200011"}