{"auto_keywords": [{"score": 0.02522607067724401, "phrase": "zebra"}, {"score": 0.00481495049065317, "phrase": "data-centric_contention_management"}, {"score": 0.004771435352180973, "phrase": "hardware_transactional_memory"}, {"score": 0.0047283116134926645, "phrase": "transactional_contention_management_policies"}, {"score": 0.0046221918332842995, "phrase": "relative_performance"}, {"score": 0.004559661374976129, "phrase": "workload_characteristics"}, {"score": 0.004437115547184535, "phrase": "fixed-policy_transactional_memory"}, {"score": 0.004337502738649814, "phrase": "general_purpose_computing_systems"}, {"score": 0.004088807948906792, "phrase": "particular_concern"}, {"score": 0.004051828608343507, "phrase": "hardware_tm"}, {"score": 0.003942877273973902, "phrase": "traditional_designs"}, {"score": 0.0038194503801108324, "phrase": "adaptive_htms"}, {"score": 0.003716724243777879, "phrase": "major_challenges"}, {"score": 0.0036497740708342093, "phrase": "design_and_verification_costs"}, {"score": 0.003503497128638099, "phrase": "zebra_htm_design"}, {"score": 0.0034092396400930446, "phrase": "simple_yet_high-performance_approach"}, {"score": 0.0032725712584012953, "phrase": "prior_work"}, {"score": 0.0031556799865964974, "phrase": "transactional_code_blocks"}, {"score": 0.0029076695661972114, "phrase": "transactional_code"}, {"score": 0.002855251306715901, "phrase": "code_block"}, {"score": 0.002778385486855157, "phrase": "neat_match"}, {"score": 0.0026913130726432645, "phrase": "cache_coherence_protocol"}, {"score": 0.002423931101340771, "phrase": "best_performing_policy"}, {"score": 0.0022951216874671494, "phrase": "inherent_benefits"}, {"score": 0.0022743274702883456, "phrase": "traditional_eager_htms-parallel_commits"}, {"score": 0.0021731423856364003, "phrase": "deadlock_avoidance"}, {"score": 0.0021049977753042253, "phrase": "low-complexity_design"}], "paper_keywords": ["Multicore architectures", " transactional memory", " parallel programming", " cache coherence protocols"], "paper_abstract": "Transactional contention management policies show considerable variation in relative performance with changing workload characteristics. Consequently, incorporation of fixed-policy Transactional Memory (TM) in general purpose computing systems is suboptimal by design and renders such systems susceptible to pathologies. Of particular concern are Hardware TM (HTM) systems where traditional designs have hardwired policies in silicon. Adaptive HTMs hold promise, but pose major challenges in terms of design and verification costs. In this paper, we present the ZEBRA HTM design, which lays down a simple yet high-performance approach to implement adaptive contention management in hardware. Prior work in this area has associated contention with transactional code blocks. However, we discover that by associating contention with data (cache blocks) accessed by transactional code rather than the code block itself, we achieve a neat match in granularity with that of the cache coherence protocol. This leads to a design that is very simple and yet able to track closely or exceed the performance of the best performing policy for a given workload. ZEBRA, therefore, brings together the inherent benefits of traditional eager HTMs-parallel commits-and lazy HTMs-good optimistic concurrency without deadlock avoidance mechanisms-, combining them into a low-complexity design.", "paper_title": "ZEBRA: Data-Centric Contention Management in Hardware Transactional Memory", "paper_id": "WOS:000334673200025"}