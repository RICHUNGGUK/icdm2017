{"auto_keywords": [{"score": 0.03843856552198327, "phrase": "shannon"}, {"score": 0.02431949862568039, "phrase": "wiener"}, {"score": 0.011154341659888753, "phrase": "davis"}, {"score": 0.005042291534205297, "phrase": "shaw"}, {"score": 0.004661446705304497, "phrase": "formal_definitions"}, {"score": 0.004611369021812453, "phrase": "physical_entropy"}, {"score": 0.004512814529823301, "phrase": "statistical_mechanics"}, {"score": 0.004321952227344232, "phrase": "information_theory"}, {"score": 0.004229556887896042, "phrase": "possible_subjectivity"}, {"score": 0.004116823963855969, "phrase": "missing_information"}, {"score": 0.0035005841368145525, "phrase": "information_sciences"}, {"score": 0.0034442607586033657, "phrase": "particular_consideration"}, {"score": 0.0032629585038738856, "phrase": "literary_theory"}, {"score": 0.002944284514509205, "phrase": "original_sources"}, {"score": 0.002881255933884771, "phrase": "direct_quotation"}, {"score": 0.0021049977753042253, "phrase": "interesting_complexity"}], "paper_keywords": ["information theory", " information entropy", " complexity", " randomness", " physics"], "paper_abstract": "A review is presented of the relation between information and entropy, focusing on two main issues: the similarity of the formal definitions of physical entropy, according to statistical mechanics, and of information, according to information theory; and the possible subjectivity of entropy considered as missing information. The paper updates the 1983 analysis of Shaw and Davis. The difference in the interpretations of information given respectively by Shannon and by Wiener, significant for the information sciences, receives particular consideration. Analysis of a range of material, from literary theory to thermodynamics, is used to draw out the issues. Emphasis is placed on recourse to the original sources, and on direct quotation, to attempt to overcome some of the misunderstandings and oversimplifications that have occurred with these topics. Although it is strongly related to entropy, information is neither identical with it, nor its opposite. Information is related to order and pattern, but also to disorder and randomness. The relations between information and the interesting complexity, which embodies both patterns and randomness, are worthy of attention.", "paper_title": "A few exciting words: Information and entropy revisited", "paper_id": "WOS:000361184500001"}