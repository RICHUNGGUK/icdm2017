{"auto_keywords": [{"score": 0.03564250833883705, "phrase": "objectpatchnet"}, {"score": 0.00481495049065317, "phrase": "semantic_image"}, {"score": 0.004688989566904871, "phrase": "ever_increasing_internet_image_collection"}, {"score": 0.004606843145343611, "phrase": "real_world"}, {"score": 0.00436890075128947, "phrase": "multiple_metadata"}, {"score": 0.004311352359122141, "phrase": "textual_descriptions"}, {"score": 0.0042734067085669885, "phrase": "user_comments"}, {"score": 0.004106725110985659, "phrase": "knowledge_source"}, {"score": 0.004070573090297285, "phrase": "large-scale_image_applications"}, {"score": 0.003877329308854808, "phrase": "loosely_annotated_image_data"}, {"score": 0.003742554019950264, "phrase": "scalable_data-driven_solution"}, {"score": 0.003660700128133065, "phrase": "web-scale_image_data"}, {"score": 0.0035806300393050926, "phrase": "large-scale_loosely_annotated_images"}, {"score": 0.003163439005885435, "phrase": "discriminative_image_patches"}, {"score": 0.003121719509312159, "phrase": "object_category_labels"}, {"score": 0.0030264946886983833, "phrase": "co-occurrence_relationship"}, {"score": 0.00289546142998427, "phrase": "objectpatchnet_models"}, {"score": 0.002832081745025006, "phrase": "image_patches"}, {"score": 0.0026501241093379786, "phrase": "scalable_image_annotation_task"}, {"score": 0.002535344521369057, "phrase": "visual_vocabulary"}, {"score": 0.002512990657100351, "phrase": "semantic_labels"}, {"score": 0.0024148064819862337, "phrase": "inverted_file_indexing"}, {"score": 0.0023935128386292966, "phrase": "efficient_semantic_image_retrieval"}, {"score": 0.002320449502337321, "phrase": "large-scale_image_annotation"}, {"score": 0.0022999859758286423, "phrase": "large-scale_image_retrieval_applications"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Visual vocabulary", " Large-scale image retrieval", " Image annotation"], "paper_abstract": "The ever increasing Internet image collection densely samples the real world objects, scenes, etc. and is commonly accompanied with multiple metadata such as textual descriptions and user comments. Such image data has potential to serve as a knowledge source for large-scale image applications. Facilitated by such publically available and ever-increasing loosely annotated image data on the Internet, we propose a scalable data-driven solution for annotating and retrieving Web-scale image data. We extrapolate from large-scale loosely annotated images a compact and informative representation, namely ObjectPatchNet. Each vertex in ObjectPatchNet, which is called as an ObjectPatchNode, is defined as a collection of discriminative image patches annotated with object category labels. The edge linking two ObjectPatchNodes models the co-occurrence relationship among different objects in the same image. Therefore, ObjectPatchNet models not only probabilistically labeled image patches, but also the contextual relationship between objects. It is well suited to scalable image annotation task. Besides, we further take ObjectPatchNet as a visual vocabulary with semantic labels, and hence are able to easily develop inverted file indexing for efficient semantic image retrieval. ObjectPatchNet is tested on both large-scale image annotation and large-scale image retrieval applications. Experimental results manifest that ObjectPatchNet is both discriminative and efficient in these applications. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "ObjectPatchNet: Towards scalable and semantic image annotation and retrieval", "paper_id": "WOS:000328591500003"}