{"auto_keywords": [{"score": 0.03697556690300151, "phrase": "bellman"}, {"score": 0.03517083259120442, "phrase": "np"}, {"score": 0.00481495049065317, "phrase": "robust_approximate_bilinear"}, {"score": 0.004721162131188487, "phrase": "value_function_approximation"}, {"score": 0.00465964893087681, "phrase": "value_function_approximation_methods"}, {"score": 0.004363846362238565, "phrase": "prevailing_techniques"}, {"score": 0.004223032986655664, "phrase": "priori_error_bounds"}, {"score": 0.004086744751579196, "phrase": "new_approximate_bilinear_programming_formulation"}, {"score": 0.003877737556673229, "phrase": "global_optimization"}, {"score": 0.003752550907120244, "phrase": "strong_a_priori_guarantees"}, {"score": 0.0036793800647811365, "phrase": "robust_and_expected_policy_loss"}, {"score": 0.003400640983719424, "phrase": "bilinear_program"}, {"score": 0.0032054891061528896, "phrase": "worst-case_complexity"}, {"score": 0.0031019373572233706, "phrase": "bellman-residual_minimization"}, {"score": 0.0027741761880779535, "phrase": "simple_approximate_algorithm"}, {"score": 0.0027200305495826797, "phrase": "bilinear_programs"}, {"score": 0.002547043222086858, "phrase": "convergent_generalization"}, {"score": 0.0025137856138000014, "phrase": "approximate_policy_iteration"}, {"score": 0.0023694065215394593, "phrase": "bilinear_programming_algorithms"}, {"score": 0.0023384629381463054, "phrase": "incomplete_samples"}, {"score": 0.002218668420806254, "phrase": "proposed_approach"}, {"score": 0.0021049977753042253, "phrase": "simple_benchmark_problems"}], "paper_keywords": ["value function approximation", " approximate dynamic programming", " Markov decision processes"], "paper_abstract": "Value function approximation methods have been successfully used in many applications, but the prevailing techniques often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this worst-case complexity is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze the formulation as well as a simple approximate algorithm for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems.", "paper_title": "Robust Approximate Bilinear Programming for Value Function Approximation", "paper_id": "WOS:000298103200010"}