{"auto_keywords": [{"score": 0.025403408196310283, "phrase": "berkeley"}, {"score": 0.007527244288213804, "phrase": "stanford"}, {"score": 0.00481495049065317, "phrase": "probabilistic_joint_image_segmentation"}, {"score": 0.004746558955503364, "phrase": "figure-ground_composition"}, {"score": 0.004662427600500598, "phrase": "layered_statistical_model"}, {"score": 0.00462919213721188, "phrase": "image_segmentation"}, {"score": 0.0043095378396174125, "phrase": "consistent_image_segmentations"}, {"score": 0.0041283957965834875, "phrase": "maximal_cliques"}, {"score": 0.0040262522882015175, "phrase": "non-overlapping_figure-ground_segment_hypotheses"}, {"score": 0.0038431888954930083, "phrase": "spatially_neighboring_segments"}, {"score": 0.0037480753312398754, "phrase": "boundary_interface_statistics"}, {"score": 0.0037213333316218522, "phrase": "real_scenes"}, {"score": 0.0036553070575015344, "phrase": "segmentation_layer"}, {"score": 0.0035776145006375173, "phrase": "joint_image_segmentation"}, {"score": 0.003552084379143929, "phrase": "labeling_model"}, {"score": 0.003526735797659508, "phrase": "jsl"}, {"score": 0.003378385493203297, "phrase": "joint_probability_distribution"}, {"score": 0.003100085797251317, "phrase": "joint_distribution"}, {"score": 0.0030450484042676023, "phrase": "first_sampling_tilings"}, {"score": 0.0029169007179202164, "phrase": "particular_tiling"}, {"score": 0.0027841357057119317, "phrase": "maximum_likelihood"}, {"score": 0.0027543636132088332, "phrase": "novel_estimation_procedure"}, {"score": 0.0027054473441439422, "phrase": "incremental_saddle-point_approximation"}, {"score": 0.0025546627686955656, "phrase": "incorrect_configurations"}, {"score": 0.00249135867469492, "phrase": "candidate_models"}, {"score": 0.0024296194239886676, "phrase": "art_results"}, {"score": 0.00235247790591897, "phrase": "pascal_voc"}, {"score": 0.002253410447951689, "phrase": "segmentation_task"}, {"score": 0.0021354096283081317, "phrase": "test_set"}, {"score": 0.0021049977753042253, "phrase": "semantic_labeling"}], "paper_keywords": ["Image segmentation", " Image labeling", " Semantic segmentation", " Statistical models", " Learning and categorization"], "paper_abstract": "We propose a layered statistical model for image segmentation and labeling obtained by combining independently extracted, possibly overlapping sets of figure-ground (FG) segmentations. The process of constructing consistent image segmentations, called tilings, is cast as optimization over sets of maximal cliques sampled from a graph connecting all non-overlapping figure-ground segment hypotheses. Potential functions over cliques combine unary, Gestalt-based figure qualities, and pairwise compatibilities among spatially neighboring segments, constrained by T-junctions and the boundary interface statistics of real scenes. Building on the segmentation layer, we further derive a joint image segmentation and labeling model (JSL) which, given a bag of FGs, constructs a joint probability distribution over both the compatible image interpretations (tilings) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, followed by sampling labelings conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on maximum likelihood with a novel estimation procedure we refer to as incremental saddle-point approximation. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that are rated as probable by candidate models during learning. State of the art results are reported on the Berkeley, Stanford and Pascal VOC datasets, where an improvement of 28 % was achieved for the segmentation task only (tiling), and an accuracy of 47.8 % was obtained on the test set of VOC12 for semantic labeling (JSL).", "paper_title": "Probabilistic Joint Image Segmentation and Labeling by Figure-Ground Composition", "paper_id": "WOS:000331640500003"}