{"auto_keywords": [{"score": 0.04764245501005146, "phrase": "unlabeled_examples"}, {"score": 0.006392333992180113, "phrase": "ims"}, {"score": 0.006094167917157446, "phrase": "mpgr"}, {"score": 0.00481495049065317, "phrase": "gaussian_process"}, {"score": 0.004610058444728656, "phrase": "large_number"}, {"score": 0.004530552828603269, "phrase": "real-world_application"}, {"score": 0.004010960493946656, "phrase": "massive_unlabeled_instances"}, {"score": 0.003958937342925219, "phrase": "minimal_cost"}, {"score": 0.0035662378173267647, "phrase": "higher_classifier_accuracy"}, {"score": 0.0034441627435203804, "phrase": "work_efficiency"}, {"score": 0.0032974089700312423, "phrase": "traditional_active_learning_algorithms"}, {"score": 0.0032404677650217407, "phrase": "single_classifier"}, {"score": 0.003199199295556997, "phrase": "gaussian"}, {"score": 0.003048782877550259, "phrase": "classification_error_rates"}, {"score": 0.0030223381739019894, "phrase": "computing_time"}, {"score": 0.002831158147480665, "phrase": "manifold-preserving_graph_reduction"}, {"score": 0.002663620639539737, "phrase": "structural_spatial_connectivity"}, {"score": 0.002640507929183363, "phrase": "spatial_diversity"}, {"score": 0.0025389513338018414, "phrase": "active_learner"}, {"score": 0.0025059724686541263, "phrase": "informative_and_representative_candidates"}, {"score": 0.0024306740161601625, "phrase": "whole_unlabeled_data"}, {"score": 0.0023270035715132866, "phrase": "state-of-the-art_active_learning_method"}, {"score": 0.002306805449430156, "phrase": "quire"}, {"score": 0.0022277448907881306, "phrase": "multiple_data_sets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Active learning", " Gaussian process", " Margin sampling", " Support vector machine", " Manifold-preserving graph reduction"], "paper_abstract": "There are a large number of unlabeled examples in real-world application, and if the labels of these unlabeled examples are given manually, then the cost will be very high. The problem about how to label these massive unlabeled instances with the minimal cost is paid more and more attention. Active learning efficiently solves this bottleneck by selecting the most informative examples from the unlabeled examples and establishing a classifier with a higher classifier accuracy to label unlabeled examples, which greatly improves work efficiency. In this paper, we compare two kinds of traditional active learning algorithms relying on a single classifier, namely Gaussian process and margin sampling active learning, in two aspects of classification error rates and computing time. Moreover, we compare their improved versions (GPMAL and IMS) which apply the manifold-preserving graph reduction (MPGR) algorithm. MPGR constructs a subset which well exploits the structural spatial connectivity and spatial diversity among examples. By using MPGR, an active learner selects the informative and representative candidates from the subset instead of the whole unlabeled data set. In addition, a comparison with a state-of-the-art active learning method, QUIRE, is provided. Experimental results on multiple data sets show that both GPMAL and IMS have their own advantages. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Gaussian process versus margin sampling active learning", "paper_id": "WOS:000358808500014"}