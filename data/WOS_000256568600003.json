{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "feature_subset_selection"}, {"score": 0.004730458558634266, "phrase": "common_challenge"}, {"score": 0.00438105507792004, "phrase": "existing_feature_selection_algorithms"}, {"score": 0.00420368356873615, "phrase": "numerical_or_categorical_attributes"}, {"score": 0.003986103192038901, "phrase": "mixed_format"}, {"score": 0.003939296259436904, "phrase": "real-world_applications"}, {"score": 0.003757480190200645, "phrase": "pawlak's_rough_set_model"}, {"score": 0.0037133478610481994, "phrase": "delta_neighborhood_rough_set_model"}, {"score": 0.0036697319652557363, "phrase": "k-nearest-neighbor_rough_set_model"}, {"score": 0.0034388111429203222, "phrase": "delta_neighborhood_relations"}, {"score": 0.0033984088755183287, "phrase": "k-nearest-neighbor_relations"}, {"score": 0.003299460925313939, "phrase": "categorical_features"}, {"score": 0.0032223742018434856, "phrase": "equivalence_relations"}, {"score": 0.00314708280192844, "phrase": "induced_information_granules"}, {"score": 0.0030017207229983385, "phrase": "lower_and_upper_approximations"}, {"score": 0.002914288956035066, "phrase": "lower_approximations"}, {"score": 0.0026986685585382347, "phrase": "proposed_models"}, {"score": 0.0025587867729949037, "phrase": "mixed_features"}, {"score": 0.002498961351132696, "phrase": "greedy_attribute_reduction_algorithm"}, {"score": 0.002426137929553724, "phrase": "proposed_algorithm"}, {"score": 0.0023003498123420237, "phrase": "selected_features"}, {"score": 0.002273292707863821, "phrase": "classification_performance"}, {"score": 0.0022070310530794097, "phrase": "proposed_technique"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["feature selection", " numerical feature", " categorical feature", " delta neighborhood", " k-nearest-neighbor", " rough sets"], "paper_abstract": "Feature subset selection presents a common challenge for the applications where data with tens or hundreds of features are available. Existing feature selection algorithms are mainly designed for dealing with numerical or categorical attributes. However, data usually comes with a mixed format in real-world applications. In this paper, we generalize Pawlak's rough set model into delta neighborhood rough set model and k-nearest-neighbor rough set model, where the objects with numerical attributes are granulated with delta neighborhood relations or k-nearest-neighbor relations, while objects with categorical features are granulated with equivalence relations. Then the induced information granules are used to approximate the decision with lower and upper approximations. We compute the lower approximations of decision to measure the significance of attributes. Based on the proposed models, we give the definition of significance of mixed features and construct a greedy attribute reduction algorithm. We compare the proposed algorithm with others in terms of the number of selected features and classification performance. Experiments show the proposed technique is effective. (C) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Mixed feature selection based on granulation and approximation", "paper_id": "WOS:000256568600003"}