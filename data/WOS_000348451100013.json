{"auto_keywords": [{"score": 0.04669299523622419, "phrase": "feedforward_neural_networks"}, {"score": 0.00481495049065317, "phrase": "gradient_training_method"}, {"score": 0.004202741104973709, "phrase": "batch_gradient_method"}, {"score": 0.004072851768196397, "phrase": "smoothing_regularization"}, {"score": 0.003444732262403694, "phrase": "sparse_weights"}, {"score": 0.0028527866590859967, "phrase": "mild_conditions"}, {"score": 0.0027645030021576926, "phrase": "decreasing_monotonicity"}, {"score": 0.002678944072162297, "phrase": "error_functions"}, {"score": 0.0025960262223338293, "phrase": "training_process"}, {"score": 0.0023133055945761235, "phrase": "theoretical_analysis"}, {"score": 0.002195162304124588, "phrase": "better_sparsity"}], "paper_keywords": ["Feedforward neural networks", " Gradient method", " l(0) Regularization", " Sparsity", " Convergence"], "paper_abstract": "This paper considers the batch gradient method with the smoothing regularization (BGSL0) for training and pruning feedforward neural networks. We show why BGSL0 can produce sparse weights, which are crucial for pruning networks. We prove both the weak convergence and strong convergence of BGSL0 under mild conditions. The decreasing monotonicity of the error functions during the training process is also obtained. Two examples are given to substantiate the theoretical analysis and to show the better sparsity of BGSL0 than three typical regularization methods.", "paper_title": "Batch gradient training method with smoothing regularization for l(0) feedforward neural networks", "paper_id": "WOS:000348451100013"}