{"auto_keywords": [{"score": 0.048682370016489784, "phrase": "probability_laws"}, {"score": 0.012974838097374878, "phrase": "training_data"}, {"score": 0.010533593624771222, "phrase": "pair_q"}, {"score": 0.008732983367260205, "phrase": "classification_algorithm"}, {"score": 0.00481495049065317, "phrase": "finite_memory"}, {"score": 0.004649408360050023, "phrase": "positive_transitions"}, {"score": 0.004587257596576765, "phrase": "doubly_infinite_sequences"}, {"score": 0.004550364809125859, "phrase": "finite_alphabet_a._a_device"}, {"score": 0.004429508899912265, "phrase": "training_sequence"}, {"score": 0.004358534837000905, "phrase": "classifier's_task"}, {"score": 0.004300255514135844, "phrase": "second_probability_law_p"}, {"score": 0.004130032407463255, "phrase": "appropriate_criterion"}, {"score": 0.003987943985691915, "phrase": "infinite_amount"}, {"score": 0.003913462458156738, "phrase": "simple_matter"}, {"score": 0.003738297182416934, "phrase": "n_letters"}, {"score": 0.003580578449368225, "phrase": "minimum_length_sequence"}, {"score": 0.0032759168467374053, "phrase": "positive_number_n-delta"}, {"score": 0.0030377722496604427, "phrase": "universal_classifier"}, {"score": 0.0024949684851017194, "phrase": "classification_error"}, {"score": 0.0023385630742192667, "phrase": "proposed_algorithm"}, {"score": 0.0023197131534778417, "phrase": "largest_empirical_conditional_divergence"}, {"score": 0.0022579680942469236, "phrase": "tested_n-sequence"}, {"score": 0.002239766348597362, "phrase": "computational_complexity"}, {"score": 0.0021393542556570706, "phrase": "second_simplified_context_classification_algorithm"}], "paper_keywords": ["context-tree algorithms", " universal classification", " universal prediction"], "paper_abstract": "We consider the class of strong-mixing probability laws with positive transitions that are defined on doubly infinite sequences in a finite alphabet A. A device called the classifier (or discriminator) observes a training sequence whose probability law Q is unknown. The classifier's task is to consider a second probability law P and decide whether P = Q, or P and Q are sufficiently different according to some appropriate criterion Delta(Q P) > Delta. If the classifier has available an infinite amount of training data, this is a simple matter. However, here we study the case where the amount of training data is limited to N letters. We define a function N-Delta (Q\\P), which quantities the minimum length sequence needed to distinguish Q and P and the class M(N-Delta) of all probability laws pairs (Q\\P) that satisfy N-Delta (Q\\P) <= N-Delta for some given positive number N-Delta. It is shown that every pair Q, P of probability laws that are sufficiently different according to the A criterion is contained in We demonstrate that for any universal classifier there exists some Q for which the classification probability lambda(Q) = 1 for some N-sequence emerging from Q, for some P : (Q, P) is an element of M(N-Delta),Delta(Q, P) > Delta, if N < N-Delta. Conversely, we introduce a classification algorithm that is essentially optimal in the sense that for every (Q, P) G M(N-Delta), the probability of classification error A(Q) is uniformly vanishing with N for every P : (Q, P) is an element of M(N-Delta) if [GRAPHICS] The proposed algorithm finds the largest empirical conditional divergence for a set of contexts which appear in the tested N-sequence. The computational complexity of the classification algorithm is O(N 2 (log N)3). Also, we introduce a second simplified context classification algorithm with a computational complexity of only O(N (log N)(4)) that is efficient in the sense that for every pair (Q, P) G M(N-Delta) the pairwise probability of classification error A (Q, P) for the pair Q, P vanishes with N if [GRAPHICS] Conversely, lambda(Q, P) = 1 at least for some (Q, P) is an element of M (N-Delta), if N < N-Delta.", "paper_title": "Classification with finite memory revisited", "paper_id": "WOS:000251801500001"}