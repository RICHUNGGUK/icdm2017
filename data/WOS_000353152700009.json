{"auto_keywords": [{"score": 0.049324540543845156, "phrase": "multiagent_system"}, {"score": 0.00481495049065317, "phrase": "model_learning_and_knowledge"}, {"score": 0.004701556483912018, "phrase": "dyna-q_learning"}, {"score": 0.0045726187330434025, "phrase": "agents'_experiences"}, {"score": 0.004412000844879264, "phrase": "environmental_modeling"}, {"score": 0.004240116703465252, "phrase": "unvisited_states"}, {"score": 0.004206548500039536, "phrase": "unseen_situations"}, {"score": 0.0041074234667687875, "phrase": "learning_process"}, {"score": 0.003978866233891427, "phrase": "effective_and_accurate_model"}, {"score": 0.003931697356115479, "phrase": "limited_time"}, {"score": 0.003885085477351673, "phrase": "important_issue"}, {"score": 0.003823791479971016, "phrase": "complex_environments"}, {"score": 0.0037336520904376687, "phrase": "model-based_reinforcement_learning_method"}, {"score": 0.0036167509502276294, "phrase": "efficient_modeling"}, {"score": 0.0035455471410184404, "phrase": "proposed_algorithm"}, {"score": 0.003503497128638099, "phrase": "dyna-q_architecture"}, {"score": 0.0033937776228549557, "phrase": "tree_structure"}, {"score": 0.003287482882222731, "phrase": "real_experiences"}, {"score": 0.0032227404824789626, "phrase": "virtual_experiences"}, {"score": 0.0031718628581265266, "phrase": "elapsed_time"}, {"score": 0.002988101395789399, "phrase": "knowledge_sharing"}, {"score": 0.0028715299874549245, "phrase": "knowledge_sharing_methods"}, {"score": 0.0028487656293168795, "phrase": "multiagent_systems"}, {"score": 0.0027704982687041147, "phrase": "global_model"}, {"score": 0.0027485326424697095, "phrase": "scattered_local_models"}, {"score": 0.0027159094186967247, "phrase": "individual_agents"}, {"score": 0.002578946584040053, "phrase": "valid_simulated_experiences"}, {"score": 0.002558495760339272, "phrase": "indirect_learning"}, {"score": 0.00252812242767189, "phrase": "early_stage"}, {"score": 0.0024586427394152196, "phrase": "sharing_process"}, {"score": 0.002429451930463481, "phrase": "proposed_method"}, {"score": 0.002372103459168231, "phrase": "partial_branches"}, {"score": 0.0023346234464901978, "phrase": "required_and_useful_experiences"}, {"score": 0.0023069016803337365, "phrase": "experienced_peers"}, {"score": 0.0022524395393089544, "phrase": "whole_trees"}, {"score": 0.002225691463551672, "phrase": "simulation_results"}, {"score": 0.002190519728921395, "phrase": "proposed_sharing_method"}, {"score": 0.002138799283553239, "phrase": "sample_efficiency"}, {"score": 0.0021218313648947926, "phrase": "learning_acceleration"}, {"score": 0.0021049977753042253, "phrase": "multiagent_cooperation_applications"}], "paper_keywords": ["Decision tree", " Dyna-Q", " model sharing", " multiagent system"], "paper_abstract": "In a multiagent system, if agents' experiences could be accessible and assessed between peers for environmental modeling, they can alleviate the burden of exploration for unvisited states or unseen situations so as to accelerate the learning process. Since how to build up an effective and accurate model within a limited time is an important issue, especially for complex environments, this paper introduces a model-based reinforcement learning method based on a tree structure to achieve efficient modeling and less memory consumption. The proposed algorithm tailored a Dyna-Q architecture to multiagent systems by means of a tree structure for modeling. The tree-model built from real experiences is used to generate virtual experiences such that the elapsed time in learning could be reduced. As well, this model is suitable for knowledge sharing. This paper is inspired by the concept of knowledge sharing methods in multiagent systems where an agent could construct a global model from scattered local models held by individual agents. Consequently, it can increase modeling accuracy so as to provide valid simulated experiences for indirect learning at the early stage of learning. To simplify the sharing process, the proposed method applies resampling techniques to grafting partial branches of trees containing required and useful experiences disseminated from experienced peers, instead of merging the whole trees. The simulation results demonstrate that the proposed sharing method can achieve the objectives of sample efficiency and learning acceleration in multiagent cooperation applications.", "paper_title": "Model Learning and Knowledge Sharing for a Multiagent System With Dyna-Q Learning", "paper_id": "WOS:000353152700009"}