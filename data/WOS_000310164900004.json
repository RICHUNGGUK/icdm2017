{"auto_keywords": [{"score": 0.04819193630515393, "phrase": "virtual_environments"}, {"score": 0.03644451033122438, "phrase": "audio-visual_rendering_system"}, {"score": 0.029591656589440806, "phrase": "virtual_world"}, {"score": 0.004750873844061641, "phrase": "audio-visual_egocentric_distance"}, {"score": 0.004154995396099332, "phrase": "audio-visual_rendering"}, {"score": 0.004081385289516914, "phrase": "tracked_passive_visual_stereoscopy"}, {"score": 0.004045068727683462, "phrase": "acoustic_wave_field_synthesis"}, {"score": 0.0037658881407605445, "phrase": "experimental_results"}, {"score": 0.0036498637283470386, "phrase": "virtual_environment"}, {"score": 0.003569176320452192, "phrase": "rendered_distances"}, {"score": 0.003397866151093552, "phrase": "farther_distances"}, {"score": 0.003293142761521425, "phrase": "virtual_object"}, {"score": 0.0032492519841546682, "phrase": "modality-independent_distance"}, {"score": 0.0031916366459252992, "phrase": "audio_modality"}, {"score": 0.0030248510288162643, "phrase": "wfs"}, {"score": 0.002957919048133256, "phrase": "perceptually_meaningful_sound_fields"}, {"score": 0.002931569771634119, "phrase": "dynamic_audio-visual_cues"}, {"score": 0.0026806139086152365, "phrase": "better_visual_distance_perception"}, {"score": 0.0024620842899061614, "phrase": "visual_distance_underestimation"}, {"score": 0.0023754752864092437, "phrase": "observed_perceptual_distance_compression"}, {"score": 0.0022816681646340518, "phrase": "conflicting_distance_cues"}, {"score": 0.002181766829493483, "phrase": "real_world"}, {"score": 0.002162316557411786, "phrase": "virtual_objects"}, {"score": 0.0021049977753042253, "phrase": "physical_audio-visual_rendering_system"}], "paper_keywords": ["Experimentation", " Human Factors", " Virtual environments", " large-screen immersive displays", " wave field synthesis", " spatialized audio", " distance estimation", " spatial perception"], "paper_abstract": "We present a study on audio, visual, and audio-visual egocentric distance perception by moving subjects in virtual environments. Audio-visual rendering is provided using tracked passive visual stereoscopy and acoustic wave field synthesis (WFS). Distances are estimated using indirect blind-walking (triangulation) under each rendering condition. Experimental results show that distances perceived in the virtual environment are systematically overestimated for rendered distances closer than the position of the audio-visual rendering system and underestimated for farther distances. Interestingly, subjects perceived each virtual object at a modality-independent distance when using the audio modality, the visual modality, or the combination of both. WFS was able to synthesise perceptually meaningful sound fields. Dynamic audio-visual cues were used by subjects when estimating the distances in the virtual world. Moving may have provided subjects with a better visual distance perception of close distances than if they were static. No correlation between the feeling of presence and the visual distance underestimation has been found. To explain the observed perceptual distance compression, it is proposed that, due to conflicting distance cues, the audio-visual rendering system physically anchors the virtual world to the real world. Virtual objects are thus attracted by the physical audio-visual rendering system.", "paper_title": "Audio, Visual, and Audio-Visual Egocentric Distance Perception by Moving Subjects in Virtual Environments", "paper_id": "WOS:000310164900004"}