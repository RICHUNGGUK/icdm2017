{"auto_keywords": [{"score": 0.048609175050806475, "phrase": "low-rank_matrix_approximation"}, {"score": 0.00481495049065317, "phrase": "gradient_descent"}, {"score": 0.004119762981892093, "phrase": "global_convergence"}, {"score": 0.003993216345115755, "phrase": "gradient_search"}, {"score": 0.00331125524068286, "phrase": "large-scale_problems"}, {"score": 0.003015143226706584, "phrase": "dictionary_learning"}, {"score": 0.0029224275921964724, "phrase": "sparse_signal_representations"}, {"score": 0.0028325548627901004, "phrase": "matrix_completion"}, {"score": 0.002171836484200008, "phrase": "grassmann_manifold"}, {"score": 0.0021049977753042253, "phrase": "fubiny-study_distance"}], "paper_keywords": ["Dimensionality reduction", " low-rank matrix", " Grassmann manifold", " optimization", " gradient descent"], "paper_abstract": "This paper provides a proof of global convergence of gradient search for low-rank matrix approximation. Such approximations have recently been of interest for large-scale problems, as well as for dictionary learning for sparse signal representations and matrix completion. The proof is based on the interpretation of the problem as an optimization on the Grassmann manifold and Fubiny-Study distance on this space.", "paper_title": "Convergence of Gradient Descent for Low-Rank Matrix Approximation", "paper_id": "WOS:000357939400023"}