{"auto_keywords": [{"score": 0.048125764396188805, "phrase": "human_activities"}, {"score": 0.035025543591135695, "phrase": "human_motion"}, {"score": 0.00481495049065317, "phrase": "self-training_approach"}, {"score": 0.004708220751321872, "phrase": "complex_human_activity_patterns"}, {"score": 0.004545240514015278, "phrase": "big_challenges"}, {"score": 0.0045162169185355, "phrase": "computer_vision_research"}, {"score": 0.004458722048888919, "phrase": "potential_fields"}, {"score": 0.00433200902751133, "phrase": "human_computer_interaction"}, {"score": 0.004304341132415427, "phrase": "medical_research"}, {"score": 0.004181997052907647, "phrase": "unintrusive_observation"}, {"score": 0.004089239912167375, "phrase": "precise_recognition"}, {"score": 0.004063116219782945, "phrase": "human_fullbody_motions"}, {"score": 0.004024242000713225, "phrase": "presented_system"}, {"score": 0.0038973170902076707, "phrase": "large_spectrum"}, {"score": 0.0038353580177342118, "phrase": "wide_variety"}, {"score": 0.0031845067058084583, "phrase": "human_dynamics"}, {"score": 0.0030642636185693054, "phrase": "observed_activities"}, {"score": 0.002958021635132416, "phrase": "combined_result"}, {"score": 0.0028923335491735564, "phrase": "anthropometric_human_model"}, {"score": 0.0028190406955898822, "phrase": "probabilistic_tracking_framework"}, {"score": 0.0027830917623582903, "phrase": "detailed_biomechanical_representation"}, {"score": 0.00276528909544761, "phrase": "human_shape"}, {"score": 0.0026693851900137953, "phrase": "sophisticated_hierarchical_sampling_strategy"}, {"score": 0.002601727192945944, "phrase": "probabilistic_framework"}, {"score": 0.0025767987652419054, "phrase": "state-of-the-art_bayesian_methods"}, {"score": 0.002511481545937323, "phrase": "complex_manipulation_activities"}, {"score": 0.002495411946660959, "phrase": "everyday_environments"}, {"score": 0.0024556852466581527, "phrase": "learned_human_appearance_models"}, {"score": 0.0024399717841513354, "phrase": "implicit_environment_models"}, {"score": 0.0023857603808363527, "phrase": "locally_consistent_representation"}, {"score": 0.0022882500116799777, "phrase": "task-specific_motion_models"}, {"score": 0.00220177373403577, "phrase": "extensive_experimental_evaluation"}, {"score": 0.0021876814398268775, "phrase": "today's_benchmarks"}, {"score": 0.0021528432044086044, "phrase": "athletic_exercises"}, {"score": 0.0021390634151482566, "phrase": "ergonomic_case_studies"}, {"score": 0.002125371638869951, "phrase": "everyday_manipulation_tasks"}, {"score": 0.0021049977753042253, "phrase": "kitchen_environment"}], "paper_keywords": ["Markerless human motion capture", " Probabilistic state estimation", " Self-trained models of human motion", " Activity recognition"], "paper_abstract": "Automatically observing and understanding human activities is one of the big challenges in computer vision research. Among the potential fields of application are areas such as robotics, human computer interaction or medical research. In this article we present our work on unintrusive observation and interpretation of human activities for the precise recognition of human fullbody motions. The presented system requires no more than three cameras and is capable of tracking a large spectrum of motions in a wide variety of scenarios. This includes scenarios where the subject is partially occluded, where it manipulates objects as part of its activities, or where it interacts with the environment or other humans. Our system is self-training, . it is capable of learning models of human motion over time. These are used both to improve the prediction of human dynamics and to provide the basis for the recognition and interpretation of observed activities. The accuracy and robustness obtained by our system is the combined result of several contributions. By taking an anthropometric human model and optimizing it towards use in a probabilistic tracking framework we obtain a detailed biomechanical representation of human shape, posture and motion. Furthermore, we introduce a sophisticated hierarchical sampling strategy for tracking that is embedded in a probabilistic framework and outperforms state-of-the-art Bayesian methods. We then show how to track complex manipulation activities in everyday environments using a combination of learned human appearance models and implicit environment models. Finally, we discuss a locally consistent representation of human motion that we use as a basis for learning environment- and task-specific motion models. All methods presented in this article have been subject to extensive experimental evaluation on today's benchmarks and several challenging sequences ranging from athletic exercises to ergonomic case studies to everyday manipulation tasks in a kitchen environment.", "paper_title": "A Self-Training Approach for Visual Tracking and Recognition of Complex Human Activity Patterns", "paper_id": "WOS:000304143700003"}