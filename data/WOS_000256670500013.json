{"auto_keywords": [{"score": 0.03543704829341049, "phrase": "original_data"}, {"score": 0.027728414774718074, "phrase": "nonnegativity_constraints"}, {"score": 0.025308577446368247, "phrase": "basis_images"}, {"score": 0.00481495049065317, "phrase": "polynomial_feature_space"}, {"score": 0.004552648949390462, "phrase": "latent_variables"}, {"score": 0.004447600571127578, "phrase": "data_sets"}, {"score": 0.004344965512058521, "phrase": "principal_component_analysis"}, {"score": 0.004305046810475616, "phrase": "pca"}, {"score": 0.0042446888127531945, "phrase": "independent_component_analysis"}, {"score": 0.004146716743362567, "phrase": "factor_analysis"}, {"score": 0.0038661089070527424, "phrase": "recently_investigated_approach"}, {"score": 0.003672427620845944, "phrase": "lower_dimensional_space"}, {"score": 0.0036213041338584756, "phrase": "so-called_nonnegative_matrix_factorization"}, {"score": 0.003472149546470424, "phrase": "decomposition_factors"}, {"score": 0.0032674478600540477, "phrase": "nmf_objective_function"}, {"score": 0.0031919592786945126, "phrase": "euclidean_space"}, {"score": 0.002800247197692108, "phrase": "nmf_algorithm"}, {"score": 0.0027483472371835865, "phrase": "objective_function"}, {"score": 0.0027100527874262446, "phrase": "hilbert_space"}, {"score": 0.002538263784314365, "phrase": "kernel_functions"}, {"score": 0.0021049977753042253, "phrase": "proposed_approach"}], "paper_keywords": ["feature extraction", " image representation", " kernel theory", " nonnegative matrix factorization (NMF)", " pattern recognition"], "paper_abstract": "Plenty of methods have been proposed in order to discover latent variables (features) in data sets. Such approaches include the principal component analysis (PCA), independent component analysis (ICA), factor analysis (FA), etc., to mention only a few. A recently investigated approach to decompose a data set with a given dimensionality into a lower dimensional space is the so-called nonnegative matrix factorization (NMF). Its only requirement is that both decomposition factors are nonnegative. To approximate the original data, the minimization of the NMF objective function is performed in the Euclidean space, where the difference between the original data and the factors can be minimized by employing L-2-norm. In this paper, we propose a generalization of the NMF algorithm by translating the objective function into a Hilbert space (also called feature space) under nonnegativity constraints. With the help of kernel functions, we developed an approach that allows high-order dependencies between the basis images while keeping the nonnegativity constraints on both basis images and coefficients. Two practical applications, namely, facial expression and face recognition, show the potential of the proposed approach.", "paper_title": "Nonnegative matrix factorization in polynomial feature space", "paper_id": "WOS:000256670500013"}