{"auto_keywords": [{"score": 0.04876985276069079, "phrase": "rsm"}, {"score": 0.010612379340043753, "phrase": "low-rank_decomposition"}, {"score": 0.0072901333357979735, "phrase": "known_entries"}, {"score": 0.004788327412856401, "phrase": "large-scale_matrices"}, {"score": 0.004709333998298903, "phrase": "random_submatrix_method"}, {"score": 0.004309434349688514, "phrase": "matrix_y"}, {"score": 0.004145287175409325, "phrase": "known_entry_percentage"}, {"score": 0.0038675032017853163, "phrase": "floating-point_operations"}, {"score": 0.0034997585767546735, "phrase": "small_memory_requirement"}, {"score": 0.0033945024593626675, "phrase": "real_values"}, {"score": 0.00314933081335483, "phrase": "statistical_size"}, {"score": 0.0028895324165046166, "phrase": "random_noises"}, {"score": 0.0028181909702080584, "phrase": "smaller_singular_value"}, {"score": 0.0027333769432036905, "phrase": "r_largest_singular_values"}, {"score": 0.002592832591110617, "phrase": "l_-_r"}, {"score": 0.002542890176213921, "phrase": "minor_singular_values"}, {"score": 0.0024526718637463574, "phrase": "null_vectors"}, {"score": 0.002385452024255879, "phrase": "l_columns"}, {"score": 0.0023656467839555458, "phrase": "ground_truth"}, {"score": 0.0021824553326294702, "phrase": "experimental_results"}, {"score": 0.0021703562417144558, "phrase": "random_synthetic_matrices"}, {"score": 0.0021049977753042253, "phrase": "real_data_sets"}], "paper_keywords": ["Low-rank matrix decomposition", " random submatrix", " complexity", " memory-space", " precision"], "paper_abstract": "A random submatrix method (RSM) is proposed to calculate the low-rank decomposition (U) over cap (mxr)(V) over cap (T)(nxr) (r < m, n) of the matrix Y is an element of R-mxn (assuming m > n generally) with known entry percentage 0 < rho <= 1. RSM is very fast as only O(mr(2) rho(r)) or O(n(3) rho(3r)) floating-point operations (flops) are required, compared favorably with O(mnr + r(2)(m + n)) flops required by the state-of-the-art algorithms. Meanwhile, RSM has the advantage of a small memory requirement as only max (n(2), mr + nr) real values need to be saved. With the assumption that known entries are uniformly distributed in Y, submatrices formed by known entries are randomly selected from Y with statistical size k x n rho(k) or m rho(l) x l, where k or l takes r + 1 usually. We propose and prove a theorem, under random noises the probability that the subspace associated with a smaller singular value will turn into the space associated to anyone of the r largest singular values is smaller. Based on the theorem, the n rho(k) - k null vectors or the l - r right singular vectors associated with the minor singular values are calculated for each submatrix. The vectors ought to be the null vectors of the submatrix formed by the chosen n rho(k) or l columns of the ground truth of <(V)over cap>(T). If enough submatrices are randomly chosen, (V) over cap and (U) over cap can be estimated accordingly. The experimental results on random synthetic matrices with sizes such as 13 1072 x 1024 and on real data sets such as dinosaur indicate that RSM is 4.30 similar to 197.95 times faster than the state-of-the-art algorithms. It, meanwhile, has considerable high precision achieving or approximating to the best.", "paper_title": "A Random Algorithm for Low-Rank Decomposition of Large-Scale Matrices With Missing Entries", "paper_id": "WOS:000360814100002"}