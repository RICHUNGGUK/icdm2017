{"auto_keywords": [{"score": 0.049561472167574526, "phrase": "user_feedback"}, {"score": 0.007923406136409608, "phrase": "development_effort"}, {"score": 0.006734056580443167, "phrase": "'good_enough'_initial_integration"}, {"score": 0.005303230780242786, "phrase": "integration_quality"}, {"score": 0.00481495049065317, "phrase": "knowledge_rules"}, {"score": 0.0047113960612140335, "phrase": "probabilistic_data_integration"}, {"score": 0.004635187610736025, "phrase": "data_integration_efforts"}, {"score": 0.004366106816225457, "phrase": "entity_resolution"}, {"score": 0.004295459433506106, "phrase": "advanced_similarity_measurement_techniques"}, {"score": 0.004180234323211199, "phrase": "semantic_duplicates"}, {"score": 0.003811010743703935, "phrase": "semantic_problems"}, {"score": 0.0037493114715625784, "phrase": "often-used_rule"}, {"score": 0.003272375502853897, "phrase": "human_effort"}, {"score": 0.003236941658990869, "phrase": "data_integration_time"}, {"score": 0.0029830992938695033, "phrase": "remaining_semantic_uncertainty"}, {"score": 0.0029029754274913803, "phrase": "probabilistic_database"}, {"score": 0.002855934865505357, "phrase": "remaining_cases"}, {"score": 0.0027341763737751467, "phrase": "query_time"}, {"score": 0.0026898636962522505, "phrase": "main_contribution"}, {"score": 0.0026033756286033285, "phrase": "experimental_investigation"}, {"score": 0.0025059724686541263, "phrase": "rule_definition"}, {"score": 0.0024788167298469455, "phrase": "threshold_tuning"}, {"score": 0.0021396960731214203, "phrase": "rough_safe_thresholds"}], "paper_keywords": ["Data integration", " Entity resolution", " Uncertain databases", " Data quality", " User feedback"], "paper_abstract": "In data integration efforts, portal development in particular, much development time is devoted to entity resolution. Often advanced similarity measurement techniques are used to remove semantic duplicates or solve other semantic conflicts. It proves impossible, however, to automatically get rid of all semantic problems. An often-used rule of thumb states that about 90% of the development effort is devoted to semi-automatically resolving the remaining 10% hard cases. In an attempt to significantly decrease human effort at data integration time, we have proposed an approach that strives for a 'good enough' initial integration which stores any remaining semantic uncertainty and conflicts in a probabilistic database. The remaining cases are to be resolved with user feedback during query time. The main contribution of this paper is an experimental investigation of the effects and sensitivity of rule definition, threshold tuning, and user feedback on the integration quality. We claim that our approach indeed reduces development effort-and not merely shifts the effort-by showing that setting rough safe thresholds and defining only a few rules suffices to produce a 'good enough' initial integration that can be meaningfully used, and that user feedback is effective in gradually improving the integration quality.", "paper_title": "Qualitative effects of knowledge rules and user feedback in probabilistic data integration", "paper_id": "WOS:000270651600010"}