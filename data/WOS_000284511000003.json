{"auto_keywords": [{"score": 0.0347939044065656, "phrase": "relative_judgments"}, {"score": 0.02803887144942503, "phrase": "relative_methods"}, {"score": 0.00481495049065317, "phrase": "relative_relevance"}, {"score": 0.004731928080086235, "phrase": "traditional_evaluation"}, {"score": 0.004690952711665194, "phrase": "information_retrieval_systems"}, {"score": 0.004413846562278824, "phrase": "graded_scale"}, {"score": 0.004244375827831527, "phrase": "absolute_judgments"}, {"score": 0.0041170581247889654, "phrase": "new_challenges"}, {"score": 0.004063664475797303, "phrase": "traditional_evaluation_methodology"}, {"score": 0.003993544228876303, "phrase": "absolute_relevance_judgments"}, {"score": 0.003724911315685105, "phrase": "relevance_grade"}, {"score": 0.0033262524445013303, "phrase": "pairwise_preference_judgments"}, {"score": 0.0031568884810273226, "phrase": "formal_way"}, {"score": 0.0030888830649190282, "phrase": "new_strategy"}, {"score": 0.003062091606343783, "phrase": "select-the"}, {"score": 0.002918826667294298, "phrase": "user_studies"}, {"score": 0.002831158147480665, "phrase": "pairwise_preference_judgment_method"}, {"score": 0.0027943943921568456, "phrase": "absolute_judgment_method"}, {"score": 0.002583597507349819, "phrase": "absolute_method"}, {"score": 0.0024095777415804346, "phrase": "pairwise_method"}, {"score": 0.0022277448907881306, "phrase": "labeling_time"}, {"score": 0.0021797112385590913, "phrase": "discordant_pairs"}, {"score": 0.0021607883988873492, "phrase": "experts'_judgments"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Relative relevance judgment", " Human judgment", " Preference judgment"], "paper_abstract": "In the traditional evaluation of information retrieval systems, assessors are asked to determine the relevance of a document on a graded scale, independent of any other documents. Such judgments are absolute judgments. Learning to rank brings some new challenges to this traditional evaluation methodology, especially regarding absolute relevance judgments. Recently preferences judgments have been investigated as an alternative. Instead of assigning a relevance grade to a document, an assessor looks at a pair of pages and judges which one is better. In this paper, we generalize pairwise preference judgments to relative judgments. We formulate the problem of relative judgments in a formal way and then propose a new strategy called Select-the-Best-Ones to solve the problem. Through user studies, we compare our proposed method with a pairwise preference judgment method and an absolute judgment method. The results indicate that users can distinguish by about one more relevance degree when using relative methods than when using the absolute method. Consequently, the relative methods generate 15-30% more document pairs for learning to rank. Compared to the pairwise method, our proposed method increases the agreement among assessors from 95% to 99%, while halving the labeling time and the number of discordant pairs to experts' judgments. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Select-the-Best-Ones: A new way to judge relative relevance", "paper_id": "WOS:000284511000003"}