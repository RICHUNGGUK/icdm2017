{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "classic_index"}, {"score": 0.004766683200410046, "phrase": "finite-horizon_bandits"}, {"score": 0.004624751696958501, "phrase": "efficient_exact_computation"}, {"score": 0.0044870272550540415, "phrase": "gittins_index"}, {"score": 0.004419703921314433, "phrase": "finite-horizon_discrete-state_bandit"}, {"score": 0.004288059466938456, "phrase": "initial_state"}, {"score": 0.004245051445971095, "phrase": "average_productivity"}, {"score": 0.004139401327448807, "phrase": "maximum_ratio"}, {"score": 0.0040978781692373005, "phrase": "expected_total_discounted_reward"}, {"score": 0.004036369952624852, "phrase": "expected_total_discounted_time"}, {"score": 0.0038379074449947067, "phrase": "successive_plays"}, {"score": 0.0036861605703168397, "phrase": "optimal_policies"}, {"score": 0.00363080961099728, "phrase": "finite-horizon_one-armed_bandit_problem"}, {"score": 0.0035048567587934254, "phrase": "suboptimal_heuristic_index_rule"}, {"score": 0.003452218803245602, "phrase": "intractable_finite-horizon_multiarmed_bandit_problem"}, {"score": 0.0032989853590079153, "phrase": "gittins_index_rule"}, {"score": 0.003216804708106142, "phrase": "infinite-horizon_case"}, {"score": 0.0029374939071038146, "phrase": "scant_attention"}, {"score": 0.0028498718992429825, "phrase": "recursive_adaptive-greedy_algorithm"}, {"score": 0.0025761937402886954, "phrase": "project_states"}, {"score": 0.0025503112711036994, "phrase": "time_horizon_length"}, {"score": 0.002486734224234698, "phrase": "special_case"}, {"score": 0.0024247382451294255, "phrase": "limited_transitions"}, {"score": 0.0022252591643049744, "phrase": "time_horizon"}, {"score": 0.0021917966722180132, "phrase": "proposed_algorithm"}, {"score": 0.002137137933768807, "phrase": "computational_study"}, {"score": 0.0021049977753042253, "phrase": "conventional_calibration_method"}], "paper_keywords": ["dynamic programming, Markov", " bandits, finite-horizon", " index policies", " analysis of algorithms", " computational complexity"], "paper_abstract": "This paper considers the efficient exact computation of the counterpart of the Gittins index for a finite-horizon discrete-state bandit, which measures for each initial state the average productivity, given by the maximum ratio of expected total discounted reward earned to expected total discounted time expended that can be achieved through a number of successive plays stopping by the given horizon. Besides characterizing optimal policies for the finite-horizon one-armed bandit problem, such an index provides a suboptimal heuristic index rule for the intractable finite-horizon multiarmed bandit problem, which represents the natural extension of the Gittins index rule (optimal in the infinite-horizon case). Although such a finite-horizon index was introduced in classic work in the 1950s, investigation of its efficient exact computation has received scant attention. This paper introduces a recursive adaptive-greedy algorithm using only arithmetic operations that computes the index in (pseudo-)polynomial time in the problem parameters (number of project states and time horizon length). In the special case of a project with limited transitions per state, the complexity is either reduced or depends only on the length of the time horizon. The proposed algorithm is benchmarked in a computational study against the conventional calibration method.", "paper_title": "Computing a Classic Index for Finite-Horizon Bandits", "paper_id": "WOS:000290248600007"}