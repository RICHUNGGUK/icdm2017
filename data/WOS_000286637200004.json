{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "denoising_autoencoders"}, {"score": 0.011840153430451642, "phrase": "ordinary_autoencoders"}, {"score": 0.004605434064238641, "phrase": "deep_network"}, {"score": 0.004518442937844318, "phrase": "local_denoising_criterion"}, {"score": 0.004377077848734575, "phrase": "original_strategy"}, {"score": 0.004294381488413906, "phrase": "deep_networks"}, {"score": 0.003928572641266562, "phrase": "corrupted_versions"}, {"score": 0.003805591825381666, "phrase": "resulting_algorithm"}, {"score": 0.0037336520904376687, "phrase": "straightforward_variation"}, {"score": 0.0033937776228549557, "phrase": "classification_problems"}, {"score": 0.0033295966773779174, "phrase": "significantly_lower_classification_error"}, {"score": 0.0032253056091816465, "phrase": "performance_gap"}, {"score": 0.0031845067058084583, "phrase": "deep_belief_networks"}, {"score": 0.0029691377785333872, "phrase": "higher_level_representations"}, {"score": 0.0028944757265206332, "phrase": "purely_unsupervised_fashion"}, {"score": 0.002768293851511292, "phrase": "subsequent_svm_classifiers"}, {"score": 0.002733260122876447, "phrase": "qualitative_experiments"}, {"score": 0.0025160740422447837, "phrase": "gabor-like_edge_detectors"}, {"score": 0.002421726275597666, "phrase": "larger_stroke_detectors"}, {"score": 0.002391067964886602, "phrase": "digit_images"}, {"score": 0.0022292394795878643, "phrase": "denoising_criterion"}, {"score": 0.0021870332034407817, "phrase": "tractable_unsupervised_objective"}, {"score": 0.0021049977753042253, "phrase": "useful_higher_level_representations"}], "paper_keywords": ["deep learning", " unsupervised feature learning", " deep belief networks", " autoencoders", " denoising"], "paper_abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.", "paper_title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "paper_id": "WOS:000286637200004"}