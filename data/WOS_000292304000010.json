{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "latent_tree_graphical_models"}, {"score": 0.00465634671242413, "phrase": "latent_tree_graphical_model"}, {"score": 0.004258372598568333, "phrase": "redundant_hidden_nodes"}, {"score": 0.0041642891310893, "phrase": "observed_nodes"}, {"score": 0.004012061349250865, "phrase": "leaf_nodes"}, {"score": 0.00389427992231721, "phrase": "discrete_and_gaussian_random_variables"}, {"score": 0.0035479614996805383, "phrase": "latent_tree"}, {"score": 0.0034954725617970294, "phrase": "sibling_groups"}, {"score": 0.0034695189961509625, "phrase": "so-called_information_distances"}, {"score": 0.003405472159599445, "phrase": "main_contributions"}, {"score": 0.0031845067058084613, "phrase": "pre-processing_procedure"}, {"score": 0.003102488033561956, "phrase": "observed_variables"}, {"score": 0.0028581775908203683, "phrase": "true_latent_tree"}, {"score": 0.002815864555488112, "phrase": "subsequent_recursive_grouping"}, {"score": 0.002784540221128404, "phrase": "equivalent_procedures"}, {"score": 0.0025556529150386168, "phrase": "regularized_versions"}, {"score": 0.002508433454626386, "phrase": "latent_tree_approximations"}, {"score": 0.0024897902536499005, "phrase": "arbitrary_distributions"}, {"score": 0.002443784721651744, "phrase": "proposed_algorithms"}, {"score": 0.002398627220761343, "phrase": "extensive_numerical_experiments"}, {"score": 0.002354302193867031, "phrase": "hidden_markov_models"}, {"score": 0.0023368018449680295, "phrase": "star_graphs"}, {"score": 0.002234491924392106, "phrase": "real-world_data_sets"}, {"score": 0.0022013916728990564, "phrase": "dependency_structure"}, {"score": 0.0021850254717637172, "phrase": "monthly_stock_returns"}], "paper_keywords": ["graphical models", " Markov random fields", " hidden variables", " latent tree models", " structure learning"], "paper_abstract": "We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efficient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our algorithms can be applied to both discrete and Gaussian random variables and our learned models are such that all the observed and latent variables have the same domain (state space). Our first algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures such as neighbor-joining) on much smaller subsets of variables. This results in more accurate and efficient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world data sets by modeling the dependency structure of monthly stock returns in the S&P index and of the words in the 20 newsgroups data set.", "paper_title": "Learning Latent Tree Graphical Models", "paper_id": "WOS:000292304000010"}