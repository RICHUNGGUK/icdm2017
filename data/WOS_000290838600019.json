{"auto_keywords": [{"score": 0.04377588693056621, "phrase": "kmse"}, {"score": 0.010265348107704058, "phrase": "ikmse"}, {"score": 0.00481495049065317, "phrase": "feature_extraction"}, {"score": 0.003948613642680196, "phrase": "computational_burden"}, {"score": 0.0036653985479652854, "phrase": "training_patterns"}, {"score": 0.0030554300179552415, "phrase": "better_property"}, {"score": 0.0029559346879594254, "phrase": "computational_efficiency"}, {"score": 0.0028596699752849682, "phrase": "generalization_performance"}, {"score": 0.0027211030647461324, "phrase": "benchmark_data_sets"}, {"score": 0.00240328454980006, "phrase": "proposed_ikmse."}, {"score": 0.0022122603123901114, "phrase": "classification_fields"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Kernel minimum squared error", " Pattern classification", " Feature extraction", " Iterative strategy"], "paper_abstract": "In this paper, a fast method of selecting features for kernel minimum squared error (KMSE) is proposed to mitigate the computational burden in the case where the size of the training patterns is large. Compared with other existent algorithms of selecting features for KMSE, this iterative KMSE, viz. IKMSE, shows better property of enhancing the computational efficiency without sacrificing the generalization performance. Experimental reports on the benchmark data sets, nonlinear autoregressive model and real problem address the efficacy and feasibility of the proposed IKMSE. In addition, IKMSE can be easily extended to classification fields. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "A fast method of feature extraction for kernel MSE", "paper_id": "WOS:000290838600019"}