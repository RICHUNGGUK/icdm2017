{"auto_keywords": [{"score": 0.04906009284763672, "phrase": "incomplete_relevance_assessments"}, {"score": 0.024345314344360106, "phrase": "rank-biased_precision"}, {"score": 0.00481495049065317, "phrase": "information_retrieval_metrics"}, {"score": 0.004513957235071457, "phrase": "available_manpower"}, {"score": 0.004475266375001531, "phrase": "relevance_assessments"}, {"score": 0.0041773934798743405, "phrase": "ir_systems"}, {"score": 0.004141575435071002, "phrase": "incomplete_relevance_data"}, {"score": 0.003949974260471287, "phrase": "relevance_assessors"}, {"score": 0.0037028514035425037, "phrase": "ir_metrics"}, {"score": 0.003592859432554987, "phrase": "graded-relevance_test_collections"}, {"score": 0.003426605523830445, "phrase": "japanese"}, {"score": 0.003397244900770386, "phrase": "chinese"}, {"score": 0.003324743991757282, "phrase": "crosslingual_task"}, {"score": 0.0032820385246266773, "phrase": "previous_work"}, {"score": 0.0031982609080278643, "phrase": "original_relevance_data"}, {"score": 0.003157174936641705, "phrase": "ir_evaluation_environments"}, {"score": 0.003130077012135034, "phrase": "extremely_incomplete_relevance_data"}, {"score": 0.002985130390111069, "phrase": "discriminative_power"}, {"score": 0.0028715299874549245, "phrase": "system_pairs"}, {"score": 0.0028346293790895024, "phrase": "statistically_significant_difference"}, {"score": 0.0027741761880779535, "phrase": "type_i_error"}, {"score": 0.002726740692380339, "phrase": "kendall's_rank_correlation"}, {"score": 0.002668582249630492, "phrase": "overall_resemblance"}, {"score": 0.0024481970338770726, "phrase": "ap"}, {"score": 0.0024062084434999433, "phrase": "sakai"}, {"score": 0.0023346283130490186, "phrase": "buckley"}, {"score": 0.0023145714732722114, "phrase": "voorhees"}, {"score": 0.0022554356416795707, "phrase": "moffat"}, {"score": 0.002236055625099805, "phrase": "zobel"}], "paper_keywords": ["evaluation metrics", " relevance assessments", " test collections", " incompleteness"], "paper_abstract": "Modern information retrieval (IR) test collections have grown in size, but the available manpower for relevance assessments has more or less remained constant. Hence, how to reliably evaluate and compare IR systems using incomplete relevance data, where many documents exist that were never examined by the relevance assessors, is receiving a lot of attention. This article compares the robustness of IR metrics to incomplete relevance assessments, using four different sets of graded-relevance test collections with submitted runs-the TREC 2003 and 2004 robust track data and the NTCIR-6 Japanese and Chinese IR data from the crosslingual task. Following previous work, we artificially reduce the original relevance data to simulate IR evaluation environments with extremely incomplete relevance data. We then investigate the effect of this reduction on discriminative power, which we define as the proportion of system pairs with a statistically significant difference for a given probability of Type I Error, and on Kendall's rank correlation, which reflects the overall resemblance of two system rankings according to two different metrics or two different relevance data sets. According to these experiments, Q', nDCG' and AP' proposed by Sakai are superior to bpref proposed by Buckley and Voorhees and to Rank-Biased Precision proposed by Moffat and Zobel. We also point out some weaknesses of bpref and Rank-Biased Precision by examining their formal definitions.", "paper_title": "On information retrieval metrics designed for evaluation with incomplete relevance assessments", "paper_id": "WOS:000256963900004"}