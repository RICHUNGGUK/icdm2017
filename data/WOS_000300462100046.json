{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "filtering_problem"}, {"score": 0.04116861455251995, "phrase": "iterative_optimization_algorithms"}, {"score": 0.02793829552192061, "phrase": "particle_filters"}, {"score": 0.004664352260912174, "phrase": "stochastic_global_optimization"}, {"score": 0.004240116703465252, "phrase": "exactly_the_objective_function"}, {"score": 0.003978866233891427, "phrase": "exactly_the_functions"}, {"score": 0.0037336520904376687, "phrase": "noisy_measurements"}, {"score": 0.0036497740708342093, "phrase": "statistical_estimates"}, {"score": 0.003600351201298387, "phrase": "monte_carlo_sampling"}, {"score": 0.0034876091188625535, "phrase": "stochastic_maps"}, {"score": 0.0034403745992576808, "phrase": "global_optimization_amounts"}, {"score": 0.00330246222643051, "phrase": "stochastic_map"}, {"score": 0.0031990183472675377, "phrase": "best_properties"}, {"score": 0.0030988045934942587, "phrase": "filtering_techniques"}, {"score": 0.0028422946420821075, "phrase": "filtering_reformulation"}, {"score": 0.002816556934231128, "phrase": "global_optimization"}, {"score": 0.002753224894944965, "phrase": "special_case"}, {"score": 0.0027282915297520624, "phrase": "sequential_importance_sampling_methods"}, {"score": 0.0026548344763838213, "phrase": "increasing_popularity"}, {"score": 0.002401972657236928, "phrase": "stochastic_global_optimization_algorithm"}, {"score": 0.0023372808856170386, "phrase": "optimal_solution"}, {"score": 0.0022951216874671516, "phrase": "naive_global_optimization"}, {"score": 0.0022537212268979507, "phrase": "parametric_exponential_density_estimation"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Particle filtering", " Stochastic global optimization", " Exponential densities", " Parametric density estimation"], "paper_abstract": "We present a reformulation of stochastic global optimization as a filtering problem. The motivation behind this reformulation comes from the fact that for many optimization problems we cannot evaluate exactly the objective function to be optimized. Similarly, we may not be able to evaluate exactly the functions involved in iterative optimization algorithms. For example, we may only have access to noisy measurements of the functions or statistical estimates provided through Monte Carlo sampling. This makes iterative optimization algorithms behave like stochastic maps. Naive global optimization amounts to evolving a collection of realizations of this stochastic map and picking the realization with the best properties. This motivates the use of filtering techniques to allow focusing on realizations that are more promising than others. In particular, we present a filtering reformulation of global optimization in terms of a special case of sequential importance sampling methods called particle filters. The increasing popularity of particle filters is based on the simplicity of their implementation and their flexibility. We utilize the flexibility of particle filters to construct a stochastic global optimization algorithm which can converge to the optimal solution appreciably faster than naive global optimization. Several examples of parametric exponential density estimation are provided to demonstrate the efficiency of the approach. (C) 2011 Elsevier Inc. All rights reserved.", "paper_title": "Stochastic global optimization as a filtering problem", "paper_id": "WOS:000300462100046"}