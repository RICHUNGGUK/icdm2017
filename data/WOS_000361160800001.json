{"auto_keywords": [{"score": 0.027064812374860078, "phrase": "cuda"}, {"score": 0.00481495049065317, "phrase": "self-organizing_map"}, {"score": 0.0047468516620285525, "phrase": "data_and_network_partitioned_methods"}, {"score": 0.004701985831087475, "phrase": "high-dimensional_data"}, {"score": 0.004337276328199398, "phrase": "constant_challenge"}, {"score": 0.0040969285074699, "phrase": "underlying_complexities"}, {"score": 0.00398177607914381, "phrase": "dimensionality_reduction"}, {"score": 0.00370780481244907, "phrase": "data_analyst"}, {"score": 0.0036207206487324506, "phrase": "remaining_independent_and_dependent_variables"}, {"score": 0.0035864603175592854, "phrase": "contextual_self-organizing_maps"}, {"score": 0.0031244995579170465, "phrase": "realistic_industry_settings"}, {"score": 0.0030949200330108156, "phrase": "batch_self-organizing_maps"}, {"score": 0.003051073364818384, "phrase": "data-independent_method"}, {"score": 0.0029935730127943496, "phrase": "training_process"}, {"score": 0.002787406053363446, "phrase": "processing_data"}, {"score": 0.0026705669656375197, "phrase": "batch_self-organizing_map"}, {"score": 0.0025103699913760057, "phrase": "graphical_processing_unit"}, {"score": 0.0024747842618794255, "phrase": "significant_training_time_reductions"}, {"score": 0.0024281181691350085, "phrase": "training_times"}, {"score": 0.0023485549678832628, "phrase": "map_sizes"}, {"score": 0.0022608057001451414, "phrase": "reduced_training_times"}, {"score": 0.0022181655497912796, "phrase": "contextual_self-organizing_map"}, {"score": 0.0021659920594094407, "phrase": "engineering_data_visualization"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["GPU", " Self-organizing map", " Parallel computing", " Neural network", " Data visualization", " High-dimensional data"], "paper_abstract": "High-dimensional data is pervasive in many fields such as engineering, geospatial, and medical. It is a constant challenge to build tools that help people in these fields understand the underlying complexities of their data. Many techniques perform dimensionality reduction or other \"compression\" to show views of data in either two or three dimensions, leaving the data analyst to infer relationships with remaining independent and dependent variables. Contextual self-organizing maps offer a way to represent and interact with all dimensions of a data set simultaneously. However, computational times needed to generate these representations limit their feasibility to realistic industry settings. Batch self-organizing maps provide a data-independent method that allows the training process to be parallelized and therefore sped up, saving time and money involved in processing data prior to analysis. This research parallelizes the batch self-organizing map by combining network partitioning and data partitioning methods with CUDA on the graphical processing unit to achieve significant training time reductions. Reductions in training times of up to twenty-five times were found while using map sizes where other implementations have shown weakness. The reduced training times open up the contextual self-organizing map as viable option for engineering data visualization. (C) 2015 Elsevier Ltd. All rights reserved.", "paper_title": "Extending parallelization of the self-organizing map by combining data and network partitioned methods", "paper_id": "WOS:000361160800001"}