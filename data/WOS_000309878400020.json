{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "novel_regression_algorithm"}, {"score": 0.00470862827326028, "phrase": "flexible_support_vector_regression"}, {"score": 0.004306205029641728, "phrase": "insensitive_zone"}, {"score": 0.004211069233133584, "phrase": "classic_support_vector_regression"}, {"score": 0.0034824716857663114, "phrase": "generalized_parametric"}, {"score": 0.0034437574674180365, "phrase": "insensitive_loss_function"}, {"score": 0.0029447150254434842, "phrase": "optimization_criterion"}, {"score": 0.002815864555488112, "phrase": "unknown_regressor_and_its_up-_and_down-bound_functions"}, {"score": 0.002574788013427524, "phrase": "single_quadratic_programming_problem"}, {"score": 0.0021049977753042253, "phrase": "proposed_method"}], "paper_keywords": ["Support vector machine", " Function approximation", " epsilon-Insensitive margin", " Parametric insensitive loss function"], "paper_abstract": "In this paper, a novel regression algorithm coined flexible support vector regression is proposed. We first model the insensitive zone in classic support vector regression, respectively, by its up- and down-bound functions and then give a kind of generalized parametric insensitive loss function (GPILF). Subsequently, based on GPILF, we propose an optimization criterion such that the unknown regressor and its up- and down-bound functions can be found simultaneously by solving a single quadratic programming problem. Experimental results on both several publicly available benchmark data sets and time series prediction show the feasibility and effectiveness of the proposed method.", "paper_title": "A flexible support vector machine for regression", "paper_id": "WOS:000309878400020"}