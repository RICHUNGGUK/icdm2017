{"auto_keywords": [{"score": 0.045744145789340156, "phrase": "training_data"}, {"score": 0.0048208015580140234, "phrase": "domain"}, {"score": 0.0046597835420572825, "phrase": "domain_adaptation_problem"}, {"score": 0.004625983640628956, "phrase": "machine_learning"}, {"score": 0.004509594355983191, "phrase": "test_data"}, {"score": 0.004332559296185432, "phrase": "common_approach"}, {"score": 0.004208169035079665, "phrase": "standard_learner"}, {"score": 0.004162445117081609, "phrase": "learning_task"}, {"score": 0.004117215958803593, "phrase": "available_training_sample"}, {"score": 0.003955523458325766, "phrase": "test_distribution"}, {"score": 0.003800156761779008, "phrase": "not-perfectly-representative_training_sample"}, {"score": 0.003664196785572724, "phrase": "large_sizes"}, {"score": 0.0035719191251314918, "phrase": "learned_classifier_preforms"}, {"score": 0.0034692911858848893, "phrase": "target_generated_samples"}, {"score": 0.0031787077057214086, "phrase": "positive_answer"}, {"score": 0.003064913196639932, "phrase": "nearest_neighbor_algorithm"}, {"score": 0.00289123004877327, "phrase": "target_data_distributions"}, {"score": 0.002838993542880416, "phrase": "covariate_shift"}, {"score": 0.002535492891830764, "phrase": "slightly_different_learning_model"}, {"score": 0.002435766348668389, "phrase": "learned_classifier"}, {"score": 0.0023229408675543147, "phrase": "training_sample"}, {"score": 0.0022976536579329514, "phrase": "proper_learning"}, {"score": 0.002264364091368269, "phrase": "output_classifier"}, {"score": 0.002215329893200788, "phrase": "predefined_class"}, {"score": 0.0021049977753042253, "phrase": "target_distribution"}], "paper_keywords": ["Machine learning", " Domain adaptation", " Sample complexity"], "paper_abstract": "The Domain Adaptation problem in machine learning occurs when the distribution generating the test data differs from the one that generates the training data. A common approach to this issue is to train a standard learner for the learning task with the available training sample (generated by a distribution that is different from the test distribution). One can view such learning as learning from a not-perfectly-representative training sample. The question we focus on is under which circumstances large sizes of such training samples can guarantee that the learned classifier preforms just as well as one learned from target generated samples. In other words, are there circumstances in which quantity can compensate for quality (of the training data)? We give a positive answer, showing that this is possible when using a Nearest Neighbor algorithm. We show this under some assumptions about the relationship between the training and the target data distributions (the assumptions of covariate shift as well as a bound on the ratio of certain probability weights between the source (training) and target (test) distribution). We further show that in a slightly different learning model, when one imposes restrictions on the nature of the learned classifier, these assumptions are not always sufficient to allow such a replacement of the training sample: For proper learning, where the output classifier has to come from a predefined class, we prove that any learner needs access to data generated from the target distribution.", "paper_title": "Domain adaptation-can quantity compensate for quality?", "paper_id": "WOS:000334678300001"}