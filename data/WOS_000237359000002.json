{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "supervised_learning_framework"}, {"score": 0.004598933498016774, "phrase": "error_analysis"}, {"score": 0.0036553070575015344, "phrase": "true_gradient"}, {"score": 0.003088384809415812, "phrase": "variable_selection"}, {"score": 0.002860546817196148, "phrase": "variable_covariance"}, {"score": 0.002690406375639144, "phrase": "simulated_data"}], "paper_keywords": ["tikhnonov regularization", " variable selection", " reproducing kernel Hilbert space", " generalization bounds"], "paper_abstract": "We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efficient implementation with respect to both memory and time.", "paper_title": "Learning coordinate covariances via gradients", "paper_id": "WOS:000237359000002"}