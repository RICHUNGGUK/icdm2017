{"auto_keywords": [{"score": 0.03955963658357214, "phrase": "slaw_scheduler"}, {"score": 0.015720977068901274, "phrase": "adaptive"}, {"score": 0.015249457909746734, "phrase": "slaw"}, {"score": 0.011724608348952253, "phrase": "adaptive_approach"}, {"score": 0.009469001944417887, "phrase": "worst_case"}, {"score": 0.004734987730139094, "phrase": "multi-core_systems"}, {"score": 0.00445293569103488, "phrase": "adaptive_task_scheduling_algorithm"}, {"score": 0.004403480614319231, "phrase": "locality-aware_scheduling_framework"}, {"score": 0.004258372598568333, "phrase": "fixed_scheduling_policies"}, {"score": 0.004106541104485704, "phrase": "different_cases"}, {"score": 0.004072275820209647, "phrase": "clear_winner"}, {"score": 0.003971179114354392, "phrase": "successful_execution"}, {"score": 0.0039380388343479384, "phrase": "serial_version"}, {"score": 0.0039051740322436123, "phrase": "parallel_program"}, {"score": 0.003808209866576388, "phrase": "dynamic_task"}, {"score": 0.003797585222494326, "phrase": "parallel_languages"}, {"score": 0.003541357671562449, "phrase": "scheduling_policy"}, {"score": 0.00351179169020594, "phrase": "per-task_basis"}, {"score": 0.003405472159599442, "phrase": "stack_usage"}, {"score": 0.0033770368103682564, "phrase": "heap_space"}, {"score": 0.003311604528903634, "phrase": "experimental_results"}, {"score": 0.003247435909361318, "phrase": "slaw's_adaptive_scheduler"}, {"score": 0.003202361402737197, "phrase": "help-first_scheduler"}, {"score": 0.00315791054946254, "phrase": "work-first_scheduler"}, {"score": 0.0030197596397721566, "phrase": "fixed_policy"}, {"score": 0.002969520982118725, "phrase": "help-first_policy"}, {"score": 0.0028635111041882956, "phrase": "fixed_help-first_policy"}, {"score": 0.0028316582436089064, "phrase": "work-first_policy"}, {"score": 0.0027305564928619464, "phrase": "fixed_work-first_policy"}, {"score": 0.00269263690712392, "phrase": "large_irregular_recursive_parallel_computations"}, {"score": 0.0026404301857006576, "phrase": "bounded_stack_usage"}, {"score": 0.0025892330566358503, "phrase": "data_sizes"}, {"score": 0.0025037596405075866, "phrase": "single_fixed_policy"}, {"score": 0.002455205987751813, "phrase": "programming_models"}, {"score": 0.002441506836344489, "phrase": "locality_hints"}, {"score": 0.0022702051991699682, "phrase": "locality_awareness"}, {"score": 0.0022449372414691787, "phrase": "improved_performance"}, {"score": 0.0022324086872003985, "phrase": "increasing_temporal_data_reuse"}, {"score": 0.002146640685357449, "phrase": "locality-aware_scheduling"}, {"score": 0.0021049977753042253, "phrase": "locality-oblivious_scheduling"}], "paper_keywords": ["Algorithms", " Languages", " Performance", " Work-stealing", " work-first", " help-first"], "paper_abstract": "This poster introduces SLAW, a Scalable Locality-aware Adaptive Work-stealing scheduler. The SLAW features an adaptive task scheduling algorithm combined with a locality-aware scheduling framework. Past work has demonstrated the pros and cons of using fixed scheduling policies, such as work-first and help-first, in different cases without a clear winner. Prior work also assumes the availability and successful execution of a serial version of the parallel program. This assumption can limit the expressiveness of dynamic task parallel languages. The SLAW scheduler supports both work-first and help-first policies simultaneously. It does so by using an adaptive approach that selects a scheduling policy on a per-task basis at runtime. The SLAW scheduler also establishes bounds on the stack usage and the heap space needed to store tasks. The experimental results for the benchmarks studied show that SLAW's adaptive scheduler achieves 0.98x - 9.2x speedup over the help-first scheduler and 0.97x - 4.5x speedup over the work-first scheduler for 64-thread executions, thereby establishing the robustness of using an adaptive approach instead of a fixed policy. In contrast, the help-first policy is 9.2x slower than work-first in the worst case for a fixed help-first policy, and the work-first policy is 3.7x slower than help-first in the worst case for a fixed work-first policy. Further, for large irregular recursive parallel computations, the adaptive scheduler runs with bounded stack usage and achieves performance (and supports data sizes) that cannot be delivered by the use of any single fixed policy. The SLAW scheduler is designed for programming models where locality hints are provided to the runtime by the programmer or compiler, and achieves locality-awareness by grouping workers into places [1, 6]. Locality awareness can lead to improved performance by increasing temporal data reuse within a worker and among workers in the same place. Our experimental results show that locality-aware scheduling can achieve up to 2.6x speedup over locality-oblivious scheduling, for the benchmarks studied.", "paper_title": "SLAW: A Scalable Locality-aware Adaptive Work-stealing Scheduler for Multi-core Systems", "paper_id": "WOS:000280548100038"}