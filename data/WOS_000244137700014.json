{"auto_keywords": [{"score": 0.04598328037555732, "phrase": "kappa"}, {"score": 0.01466088822829542, "phrase": "fuzzy_sets"}, {"score": 0.01367316865379908, "phrase": "fuzzy_case"}, {"score": 0.011139179338913154, "phrase": "fuzzy_classification"}, {"score": 0.00481495049065317, "phrase": "agreement_measure"}, {"score": 0.004769958800452922, "phrase": "fuzzy_classifications"}, {"score": 0.0045726187330434025, "phrase": "assessment_method"}, {"score": 0.004241648852523704, "phrase": "cohen's_kappa_statistic"}, {"score": 0.004123781267600542, "phrase": "cohen's_kappa_coefficient"}, {"score": 0.003916098498242888, "phrase": "crisp_a-cut_subsets"}, {"score": 0.0038431888954930083, "phrase": "proposed_fuzzy_kappa"}, {"score": 0.0037363522297378777, "phrase": "overall_agreement"}, {"score": 0.003581620064675766, "phrase": "efficient_agreement_measure"}, {"score": 0.003369322866499541, "phrase": "fuzzy_segmentation"}, {"score": 0.003306559295382181, "phrase": "membership_function"}, {"score": 0.003110515172439764, "phrase": "expectation_agreement"}, {"score": 0.0029260602766930065, "phrase": "observed_agreement"}, {"score": 0.0025053520095236694, "phrase": "brain_tissues"}, {"score": 0.0024818877663889813, "phrase": "mri_images"}, {"score": 0.0023566987138026285, "phrase": "kappa_coefficient"}, {"score": 0.002248366278757655, "phrase": "fuzzy_kappa"}, {"score": 0.002196077907454079, "phrase": "cohen's_kappa"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["kappa statistic", " classification", " fuzzy", " agreement", " similarity", " assessment", " evaluation", " brain tissue", " MRI"], "paper_abstract": "In this paper, we propose an assessment method of agreement between fuzzy sets, called fuzzy Kappa which is deduced from the concept of Cohen's Kappa statistic. In fuzzy case, the Cohen's Kappa coefficient can be calculated generally by transforming the fuzzy sets into some crisp a-cut subsets. While the proposed fuzzy Kappa allows to directly evaluate an overall agreement between two fuzzy sets. Hence, it is an efficient agreement measure between a given fuzzy \"ground truth\" or reference and a result of fuzzy classification or fuzzy segmentation. Based on membership function, we define its agreement function and its probability distribution to formulate the deduction of the expectation agreement. So the fuzzy Kappa is calculated from the proportion of the observed agreement and the agreement expected by chance. All the definitions and deductions are detailed in this paper. Both Cohen's Kappa and the fuzzy Kappa are then used to evaluate the agreement between a fuzzy classification of brain tissues on MRI images and its \"ground truth\". A comparison of the two types of Kappa coefficient is carried out and shows the advantage of the fuzzy Kappa and some limitations of Cohen's Kappa in the fuzzy case. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Fuzzy kappa for the agreement measure of fuzzy classifications", "paper_id": "WOS:000244137700014"}