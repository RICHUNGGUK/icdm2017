{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "gaussian_mixture_model"}, {"score": 0.04547624598453918, "phrase": "supervised_information"}, {"score": 0.04375046794702629, "phrase": "pairwise_constraints"}, {"score": 0.04188369592933691, "phrase": "objective_function"}, {"score": 0.03951288245948403, "phrase": "cluster_algorithm"}, {"score": 0.0389405879729311, "phrase": "distance_metric"}, {"score": 0.004766683200410046, "phrase": "distance_metric_learning"}, {"score": 0.004721794321726492, "phrase": "semi"}, {"score": 0.0044870272550540415, "phrase": "unsupervised_clustering"}, {"score": 0.004397487039229447, "phrase": "small_amount"}, {"score": 0.0038964032225585117, "phrase": "distance_measure"}, {"score": 0.003540392285585644, "phrase": "probability_distribution"}, {"score": 0.003366233863411861, "phrase": "totally_opposite_result"}, {"score": 0.0031366648074858555, "phrase": "key_problem"}, {"score": 0.0029973957125353306, "phrase": "semi-supervised_hybrid_clustering_algorithm"}, {"score": 0.002750934994181092, "phrase": "probability_distribution_information"}, {"score": 0.002563219920877287, "phrase": "labeled_data"}, {"score": 0.0024993218789131437, "phrase": "gaussian_distribution_parameter"}, {"score": 0.002437012847684331, "phrase": "weight_matrix"}, {"score": 0.0023405257495466352, "phrase": "kullback-leibler_divergence"}, {"score": 0.0023053339328456234, "phrase": "\"distance\"_measurement"}, {"score": 0.0021807544600219216, "phrase": "real_world_data_sets"}, {"score": 0.002158836285985314, "phrase": "chinese_word_sense_induction"}], "paper_keywords": ["Semi-supervised clustering", " Gaussian mixture model", " Distance metric learning", " Expectation maximization"], "paper_abstract": "Semi-supervised clustering aim to aid and bias the unsupervised clustering by employing a small amount of supervised information. The supervised information is generally given as pairwise constraints, which was used to either modify the objective function or to learn the distance measure. Many previous work have shown that the cluster algorithm based on distance metric is significantly better than the cluster algorithm based on probability distribution in the some data set, there are a totally opposite result in another data set, so how to balance the two methods become a key problem. In this paper, we proposed a semi-supervised hybrid clustering algorithm that provides a principled framework integrating distance metric into Gaussian mixture model, which consider not only the intrinsic geometry information but also the probability distribution information of the data. In comparison to only using the pairwise constraints, the labeled data was used to initialize Gaussian distribution parameter and to construct the weight matrix of regularizer, and then we adopt Kullback-Leibler Divergence as the \"distance\" measurement to regularize the objective function. Experiments on several UCI data sets and the real world data sets of Chinese Word Sense Induction demonstrate the effectiveness of our semi-supervised cluster algorithm.", "paper_title": "Semi-supervised hybrid clustering by integrating Gaussian mixture model and distance metric learning", "paper_id": "WOS:000358199000006"}