{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "bayes_risk"}, {"score": 0.004248294537944227, "phrase": "predefined_function_class"}, {"score": 0.0034332736827447654, "phrase": "function_class"}, {"score": 0.003028727936200845, "phrase": "necessary_and_sufficient_conditions"}, {"score": 0.0026055251533586804, "phrase": "kernel_hilbert_spaces"}, {"score": 0.002509284173541882, "phrase": "support_vector_machines"}, {"score": 0.0022133711375320266, "phrase": "universal_consistency"}, {"score": 0.0021049977753042253, "phrase": "non-compact_input_domains"}], "paper_keywords": [""], "paper_abstract": "Many learning algorithms approximately minimize a risk functional over a predefined function class. In order to establish consistency for such algorithms it is therefore necessary to know whether this function class approximates the Bayes risk. In this work we present necessary and sufficient conditions for the latter. We then apply these results to reproducing kernel Hilbert spaces used in support vector machines (SVMs). Finally, we briefly discuss universal consistency of SVMs for non-compact input domains.", "paper_title": "Function classes that approximate the Bayes risk", "paper_id": "WOS:000239587900009"}