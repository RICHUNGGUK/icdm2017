{"auto_keywords": [{"score": 0.04675404263653011, "phrase": "alternating_optimization"}, {"score": 0.00481495049065317, "phrase": "sample-wise_alternating_optimization"}, {"score": 0.004682288586254123, "phrase": "support_vector_machines"}, {"score": 0.004608121749000898, "phrase": "multiple_kernel_learning"}, {"score": 0.0044277810648955624, "phrase": "standard_svm_solvers"}, {"score": 0.004375061788023814, "phrase": "local_combination"}, {"score": 0.004340263267055143, "phrase": "base_kernels"}, {"score": 0.004288581530198804, "phrase": "sample-specific_kernel_weights"}, {"score": 0.004039243956427751, "phrase": "mkl"}, {"score": 0.0039911102918360995, "phrase": "svm-tied_overall_complexity"}, {"score": 0.0037740295668681014, "phrase": "lmkl"}, {"score": 0.003729065012001506, "phrase": "sample-specific_character"}, {"score": 0.0036553070575015344, "phrase": "kernel_weights"}, {"score": 0.003456427643307904, "phrase": "new_primal-dual_equivalence"}, {"score": 0.002957471572421436, "phrase": "associated_sample-wise_alternating_optimization_method"}, {"score": 0.0028758391869611374, "phrase": "localized_kernel_weights"}, {"score": 0.0026760974046034854, "phrase": "closed-form_solutions"}, {"score": 0.0026022112631368223, "phrase": "test_time"}, {"score": 0.002571171956724731, "phrase": "learnt_kernel_weights"}, {"score": 0.0025405019456703325, "phrase": "training_data"}, {"score": 0.0024802523752944536, "phrase": "nearest-neighbor_rule"}, {"score": 0.0023925401023682717, "phrase": "test_part"}, {"score": 0.0023451553255837317, "phrase": "neighborhood_information"}, {"score": 0.0022895278869126848, "phrase": "empirical_loss"}, {"score": 0.0022531762659857507, "phrase": "sample-wise_objectives"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithm"}], "paper_keywords": ["Multiple kernel learning", " local learning", " support vector machine"], "paper_abstract": "Our objective is to train support vector machines (SVM)-based localized multiple kernel learning (LMKL), using the alternating optimization between the standard SVM solvers with the local combination of base kernels and the sample-specific kernel weights. The advantage of alternating optimization developed from the state-of-the-art MKL is the SVM-tied overall complexity and the simultaneous optimization on both the kernel weights and the classifier. Unfortunately, in LMKL, the sample-specific character makes the updating of kernel weights a difficult quadratic nonconvex problem. In this paper, starting from a new primal-dual equivalence, the canonical objective on which state-of-the-art methods are based is first decomposed into an ensemble of objectives corresponding to each sample, namely, sample-wise objectives. Then, the associated sample-wise alternating optimization method is conducted, in which the localized kernel weights can be independently obtained by solving their exclusive sample-wise objectives, either linear programming (for l(1)-norm) or with closed-form solutions (for l(p)-norm). At test time, the learnt kernel weights for the training data are deployed based on the nearest-neighbor rule. Hence, to guarantee their generality among the test part, we introduce the neighborhood information and incorporate it into the empirical loss when deriving the sample-wise objectives. Extensive experiments on four benchmark machine learning datasets and two real-world computer vision datasets demonstrate the effectiveness and efficiency of the proposed algorithm.", "paper_title": "Localized Multiple Kernel Learning via Sample-wise Alternating Optimization", "paper_id": "WOS:000328948900011"}