{"auto_keywords": [{"score": 0.049352273134126406, "phrase": "neural_simulation"}, {"score": 0.00481495049065317, "phrase": "script_languages"}, {"score": 0.004634254474150777, "phrase": "neural_network_simulators"}, {"score": 0.0041001740903338834, "phrase": "general_programming_language"}, {"score": 0.003976734139930852, "phrase": "python"}, {"score": 0.003797978200741566, "phrase": "ongoing_efforts"}, {"score": 0.003711723285313348, "phrase": "standardized_languages"}, {"score": 0.0030648097361278856, "phrase": "model_types"}, {"score": 0.0027955219242123013, "phrase": "complementary_goal"}, {"score": 0.0026698611202305694, "phrase": "cognitive_effort"}, {"score": 0.0022211005680689666, "phrase": "language_entropy"}], "paper_keywords": ["Simulation", " neural networks", " languages", " syntax", " Python", " NeuroML"], "paper_abstract": "In neural network simulators, models are specified according to a language, either specific or based on a general programming language (e. g. Python). There are also ongoing efforts to develop standardized languages, for example NeuroML. When designing these languages, efforts are often focused on expressivity, that is, on maximizing the number of model types than can be described and simulated. I argue that a complementary goal should be to minimize the cognitive effort required on the part of the user to use the language. I try to formalize this notion with the concept of \"language entropy\", and I propose a few practical guidelines to minimize the entropy of languages for neural simulation.", "paper_title": "On the design of script languages for neural simulation", "paper_id": "WOS:000311837300003"}