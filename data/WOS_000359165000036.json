{"auto_keywords": [{"score": 0.047383019958100334, "phrase": "mddm"}, {"score": 0.007740489563401459, "phrase": "shared_subspace"}, {"score": 0.007659689518883971, "phrase": "multi-label_dimensionality_reduction"}, {"score": 0.004592214823540993, "phrase": "dependence_maximization"}, {"score": 0.00433386573276633, "phrase": "high-dimensional_multi-label_data"}, {"score": 0.004133292222875435, "phrase": "lower-dimensional_feature_space"}, {"score": 0.00392125815948488, "phrase": "associated_class_labels"}, {"score": 0.0037005154613244363, "phrase": "dense_matrices"}, {"score": 0.0035106041824568618, "phrase": "high-dimensional_data"}, {"score": 0.0032437952431945724, "phrase": "multiple_labels"}, {"score": 0.0031262984171100856, "phrase": "multilabel_learning"}, {"score": 0.00291923164591297, "phrase": "novel_frarhework"}, {"score": 0.002697245874076643, "phrase": "linear_time_complexity"}, {"score": 0.0024272350606639147, "phrase": "least-squares_problem"}, {"score": 0.0022904032758108775, "phrase": "multiple_label_interactions"}, {"score": 0.002266364679369248, "phrase": "extensive_experiments"}, {"score": 0.0022307779508260205, "phrase": "benchmark_data_collections"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multi-label dimensionality reduction", " Dependence maximization", " Shared subspace", " Least squares"], "paper_abstract": "Multi-label Dimensionality reduction via Dependence Maximization (MDDM) has been proposed recently to cope with high-dimensional multi-label data. MDDM projects the original data onto a lower-dimensional feature space in which the dependence between the feature and the associated class labels is maximized. However, the computation of MDDM involves dense matrices eigen-decomposition that is computationally expensive for the high-dimensional data. In addition, MDDM cannot be guaranteed to capture the correlation between multiple labels, which are highly beneficial to multilabel learning. To efficiently solve MDDM, in this paper we propose a novel frarhework that does not require any eigen-decomposition of a matrix. Specifically, our algorithm has linear time complexity in the dimensionality of the data set. Further, we show that MDDM can be reformulated as a least-squares problem enabling us to integrate the shared subspace that can effectively uncover multiple label interactions. Extensive experiments conducted on benchmark data collections verify the effectiveness of our proposed model. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Learning shared subspace for multi-label dimensionality reduction via dependence maximization", "paper_id": "WOS:000359165000036"}