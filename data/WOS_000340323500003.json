{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "distributed_estimations"}, {"score": 0.004174064076143015, "phrase": "incremental_subgradient"}, {"score": 0.0036181717017041387, "phrase": "asynchronously_processing"}, {"score": 0.0032668821950788533, "phrase": "separable_non-differentiable_convex_optimization_problems"}, {"score": 0.0029196116767956273, "phrase": "distributed_estimators"}, {"score": 0.002860546817196148, "phrase": "equivalent_separable_convex_optimization_problems"}, {"score": 0.0025046245057922557, "phrase": "analytical_and_simulation_results"}, {"score": 0.002331641523873951, "phrase": "general_nonlinear_estimation_problems"}, {"score": 0.0021705796895687864, "phrase": "comparable_performances"}, {"score": 0.0021049977753042253, "phrase": "centralized_estimators"}], "paper_keywords": ["distributed signal processing", " signal processing in-network", " distributed estimation", " incremental subgradient", " optimization methods"], "paper_abstract": "A unified framework on distributed estimations in-network using incremental subgradient (IS) methods is introduced. The IS methods meet the requirement of the asynchronously processing, which are efficient solvers developed recently focusing on separable non-differentiable convex optimization problems. The main contribution of this paper is to formalize distributed estimators as equivalent separable convex optimization problems, where general skills and several specific cases on signal estimations are presented. Analytical and simulation results show that the IS framework can solve general nonlinear estimation problems in-network, and achieve comparable performances as the centralized estimators.", "paper_title": "The incremental subgradient methods on distributed estimations in-network", "paper_id": "WOS:000340323500003"}