{"auto_keywords": [{"score": 0.050078046858951175, "phrase": "reinforcement_learning_principles"}, {"score": 0.04270517427091104, "phrase": "basal_ganglia"}, {"score": 0.041533923108545034, "phrase": "schultz"}, {"score": 0.04048656726069317, "phrase": "dopamine_neurons"}, {"score": 0.028113035471299848, "phrase": "kakade"}, {"score": 0.004745866089203346, "phrase": "reward-based_learning_models"}, {"score": 0.00457750063555172, "phrase": "cambridge"}, {"score": 0.004533541125727748, "phrase": "mit_press"}, {"score": 0.004404341000823282, "phrase": "phasic_increases"}, {"score": 0.004330674019537557, "phrase": "dopamine-releasing_neurons"}, {"score": 0.0037026188983126807, "phrase": "reward-prediction_error"}, {"score": 0.0036231300436703772, "phrase": "phasic_activity"}, {"score": 0.003562482836976063, "phrase": "dopaminergic_spiking"}, {"score": 0.003511305304824043, "phrase": "salient_but_unpredicted_nonreward_stimuli"}, {"score": 0.0033621290753070546, "phrase": "horvitz"}, {"score": 0.003266201217790818, "phrase": "salient_non-reward_events"}, {"score": 0.0032506196830828634, "phrase": "neuroscience"}, {"score": 0.0031806677046452674, "phrase": "redgrave"}, {"score": 0.0031425314507114127, "phrase": "gurney"}, {"score": 0.0030089724182367694, "phrase": "novel_actions"}, {"score": 0.002634945965449888, "phrase": "dayan"}, {"score": 0.002510753146803752, "phrase": "neural_networks"}, {"score": 0.00239818378771419, "phrase": "unexpected_stimuli"}, {"score": 0.0022359993798189513, "phrase": "reward-prediction_learning_mechanisms"}, {"score": 0.0022252263986820542, "phrase": "reinforcement_learning"}], "paper_keywords": ["Novelty response", " Reinforcement learning", " Dopamine", " Orienting", " Reward-prediction error"], "paper_abstract": "Recent attempts to map reward-based learning models, like Reinforcement Learning [Sutton, R. S.. & Barto, A. G. (1998). Reinforcement Learning: An introduction. Cambridge, MA: MIT Press], to the brain are based on the observation that phasic increases and decreases in the spiking of dopamine-releasing neurons signal differences between predicted and received reward [Gillies, A., & Arbuthnott, G. (2000). Computational models of the basal ganglia. Movement Disorders, 15(5), 762-770; Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of Neurophysiology, 80(1), 1-27]. However, this reward-prediction error is only one of several signals communicated by that phasic activity; another involves an increase in dopaminergic spiking, reflecting the appearance of salient but unpredicted nonreward stimuli [Doya, K. (2002). Metalearning and neuromodulation. Neural Networks, 15(4-6),495-506; Horvitz, J. C. (2000). Mesolimbocortical and nigrostriatal dopamine responses to salient non-reward events. Neuroscience, 96(4), 651-656; Redgrave, P., & Gurney, K. (2006). The short-latency dopamine signal: A role in discovering novel actions? Nature Reviews Neuroscience, 7(12), 967-9751, especially when an organism subsequently orients towards the stimulus [Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of Neurophysiology, 80(1), 1-27]. To explain these findings, Kakade and Dayan [Kakade, S., & Dayan, P. (2002). Dopamine: Generalization and bonuses. Neural Networks, 15(4-6), 549-559.] and others have posited that novel, unexpected stimuli are intrinsically rewarding. The simulation reported in this article demonstrates that this assumption is not necessary because the effect it is intended to capture emerges from the reward-prediction learning mechanisms of Reinforcement Learning. Thus, Reinforcement Learning principles can be used to understand not just reward-related activity of the dopaminergic neurons of the basal ganglia, but also some of their apparently non-rewardrelated activity. (C) 2008 Elsevier Ltd. All rights reserved.", "paper_title": "The emergence of saliency and novelty responses from Reinforcement Learning principles", "paper_id": "WOS:000261926100010"}