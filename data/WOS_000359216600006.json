{"auto_keywords": [{"score": 0.04894388481876895, "phrase": "fused_lasso"}, {"score": 0.008783928016167605, "phrase": "flams"}, {"score": 0.007784789287395678, "phrase": "computational_cost"}, {"score": 0.007705375442944024, "phrase": "existing_solvers"}, {"score": 0.007049952075465761, "phrase": "adjacent_features"}, {"score": 0.005800478433584963, "phrase": "new_dual_formulation"}, {"score": 0.00481495049065317, "phrase": "lasso_screening_rules"}, {"score": 0.0046526806928044485, "phrase": "popular_regression_technique"}, {"score": 0.004374192072307276, "phrase": "smooth_feature_structure"}, {"score": 0.0041548413176193235, "phrase": "feature_dimension"}, {"score": 0.004000978459274141, "phrase": "novel_screening_rules"}, {"score": 0.0035972293900260414, "phrase": "substantial_savings"}, {"score": 0.0035481809241908044, "phrase": "memory_usage"}, {"score": 0.0034402448248944434, "phrase": "proposed_approach"}, {"score": 0.003404998261799824, "phrase": "first_attempt"}, {"score": 0.0033701115914053052, "phrase": "screening_methods"}, {"score": 0.003335581163845351, "phrase": "fused_lasso_problem"}, {"score": 0.0033127570056253252, "phrase": "general_data_matrix"}, {"score": 0.0029578641378775633, "phrase": "standard_lasso"}, {"score": 0.0028678338651577056, "phrase": "novel_framework"}, {"score": 0.002838435157502933, "phrase": "effective_and_efficient_screening_rules"}, {"score": 0.0026958888229781618, "phrase": "appealing_features"}, {"score": 0.0025604827895852646, "phrase": "detected_adjacent_features"}, {"score": 0.0021195184368629017, "phrase": "proposed_flams_rules"}], "paper_keywords": ["Fused lasso", " screening", " l(1)-regularization"], "paper_abstract": "Fused Lasso is a popular regression technique that encodes the smoothness of the data. It has been applied successfully to many applications with a smooth feature structure. However, the computational cost of the existing solvers for fused Lasso is prohibitive when the feature dimension is extremely large. In this paper, we propose novel screening rules that are able to quickly identity the adjacent features with the same coefficients. As a result, the number of variables to be estimated can be significantly reduced, leading to substantial savings in computational cost and memory usage. To the best of our knowledge, the proposed approach is the first attempt to develop screening methods for the fused Lasso problem with general data matrix. Our major contributions are: 1) we derive a new dual formulation of fused Lasso that comes with several desirable properties; 2) we show that the new dual formulation of fused Lasso is equivalent to that of the standard Lasso by two affine transformations; 3) we propose a novel framework for developing effective and efficient screening rules for fused Lasso via the monotonicity of the subdifferentials (FLAMS). Some appealing features of FLAMS are: 1) our methods are safe in the sense that the detected adjacent features are guaranteed to have the same coefficients; 2) the dataset needs to be scanned only once to run the screening, whose computational cost is negligible compared to that of solving the fused Lasso; (3) FLAMS is independent of the solvers and can be integrated with any existing solvers. We have evaluated the proposed FLAMS rules on both synthetic and real datasets. The experiments indicate that FLAMS is very effective in identifying the adjacent features with the same coefficients. The speedup gained by FLAMS can be orders of magnitude.", "paper_title": "Fused Lasso Screening Rules via the Monotonicity of Subdifferentials", "paper_id": "WOS:000359216600006"}