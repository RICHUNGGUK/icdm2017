{"auto_keywords": [{"score": 0.049378038061471535, "phrase": "non-convex_regularization"}, {"score": 0.03613346133936676, "phrase": "dpcd"}, {"score": 0.00481495049065317, "phrase": "descent_algorithm"}, {"score": 0.004406994912939125, "phrase": "machine_learning"}, {"score": 0.003945117054416623, "phrase": "convex_counterparts"}, {"score": 0.0032800186243040663, "phrase": "optimization_issues"}, {"score": 0.0032081215771180664, "phrase": "general_family"}, {"score": 0.003161065111494053, "phrase": "non-convex_regularized_problems"}, {"score": 0.002914288956035066, "phrase": "computational_complexity"}, {"score": 0.0028293966058473476, "phrase": "approximately_stationary_solution"}, {"score": 0.0027673496368158545, "phrase": "desired_precision"}, {"score": 0.0026472970609754095, "phrase": "data_size"}, {"score": 0.0025137856138000014, "phrase": "benchmark_datasets"}, {"score": 0.0023869914500743083, "phrase": "fast_convergence_rate"}, {"score": 0.0022665781920241245, "phrase": "training_models"}, {"score": 0.002233301265743633, "phrase": "significant_loss"}, {"score": 0.0022005118194652704, "phrase": "prediction_accuracy"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Non-convex regularization", " Non-convex optimization", " Coordinate descent", " Sparsity regularization"], "paper_abstract": "Non-convex regularization has attracted much attention in the fields of machine learning, since it is unbiased and improves the performance on many applications compared with the convex counterparts. The optimization is important but difficult for non-convex regularization. In this paper, we propose the Damping Proximal Coordinate Descent (DPCD) algorithms that address the optimization issues of a general family of non-convex regularized problems. DPCD is guaranteed to be globally convergent. The computational complexity of obtaining an approximately stationary solution with a desired precision is only linear to the data size. Our experiments on many machine learning benchmark datasets also show that DPCD has a fast convergence rate and it reduces the time of training models without significant loss of prediction accuracy. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Damping proximal coordinate descent algorithm for non-convex regularization", "paper_id": "WOS:000349572600016"}