{"auto_keywords": [{"score": 0.0490670488207421, "phrase": "adaptive_item_sequencing"}, {"score": 0.036419388385157375, "phrase": "irt-based_calibration"}, {"score": 0.013225186843559104, "phrase": "irt"}, {"score": 0.008960567723832257, "phrase": "learner_feedback"}, {"score": 0.007914160796728773, "phrase": "reliable_estimates"}, {"score": 0.00481495049065317, "phrase": "dynamic_electronic_learning_environments"}, {"score": 0.0044047527104545525, "phrase": "ability_level"}, {"score": 0.0042354707960660706, "phrase": "known_difficulty_level"}, {"score": 0.004190438004831167, "phrase": "difficulty_level"}, {"score": 0.0040726680518382965, "phrase": "item_response_theory"}, {"score": 0.003916098498242888, "phrase": "large_sample_size"}, {"score": 0.0038332390893263844, "phrase": "irt_models"}, {"score": 0.003607834976869384, "phrase": "relatively_simple_and_fast_alternative_estimation_methods"}, {"score": 0.0033356305227059072, "phrase": "real_data"}, {"score": 0.0033119237381809617, "phrase": "six_alternative_estimation_methods"}, {"score": 0.002790723338582723, "phrase": "strongest_relation"}, {"score": 0.0027708785175203556, "phrase": "irt-based_difficulty_estimates"}, {"score": 0.0027025247818325687, "phrase": "elo"}, {"score": 0.0025800088089364737, "phrase": "one-to-many_comparison"}, {"score": 0.0025343769150179764, "phrase": "stable_estimates"}, {"score": 0.0024984517279573906, "phrase": "small_sample_size"}, {"score": 0.0024281181691350085, "phrase": "elo_rating_system"}, {"score": 0.002368198542691635, "phrase": "sample_size"}, {"score": 0.002326304140277375, "phrase": "alternative_estimation_methods"}, {"score": 0.002150580119047236, "phrase": "bayesian_estimation_method"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Distance education and telelearning", " Intelligent tutoring systems", " Cooperative/collaborative learning", " Evaluation methodologies"], "paper_abstract": "The evolution from static to dynamic electronic learning environments has stimulated the research on adaptive item sequencing. A prerequisite for adaptive item sequencing, in which the difficulty of the item is constantly matched to the ability level of the learner, is to have items with a known difficulty level. The difficulty level can be estimated by means of the item response theory (IRT). However, the requirement of a large sample size for calibrating items based on IRT models is not easily met in many practical learning situations. The aim of this paper is to search for relatively simple and fast alternative estimation methods and to review the accuracy of these methods as compared to IRT-based calibration in one single setting, and this for various sample sizes. Using real data, six alternative estimation methods are compared next to IRT-based calibration: proportion correct, learner feedback, expert rating, one-to-many comparison (learner), one-to-many comparison (expert) and the Elo rating system. Results indicate that proportion correct has the strongest relation with IRT-based difficulty estimates, followed by learner feedback, the Elo rating system, expert rating and finally one-to-many comparison. Learner feedback and one-to-many comparison (learner) provide stable estimates even with a small sample size. IRT, proportion correct and the Elo rating system provide reliable estimates, especially with a sample size of 200-250 learners. The alternative estimation methods can be utilized for adaptive item sequencing when IRT-based calibration does not yet provide reliable estimates or can be used as a prior in a Bayesian estimation method. (C) 2011 Elsevier Ltd. All rights reserved.", "paper_title": "Item difficulty estimation: An auspicious collaboration between data and judgment", "paper_id": "WOS:000301275600018"}