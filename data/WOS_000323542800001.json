{"auto_keywords": [{"score": 0.049226991195811115, "phrase": "multi-core_clusters"}, {"score": 0.033933526052188745, "phrase": "d-lin"}, {"score": 0.00481495049065317, "phrase": "three-dimensional_stacking"}, {"score": 0.00477190623242136, "phrase": "tightly_coupled_data_memories"}, {"score": 0.004686963264680882, "phrase": "low-latency_interconnects"}, {"score": 0.004645057834614705, "phrase": "shared_tightly_coupled_data_memories"}, {"score": 0.004401342842183475, "phrase": "embedded_systems"}, {"score": 0.004284302478991387, "phrase": "convenient_shared_memory_abstraction"}, {"score": 0.004226949670141375, "phrase": "cache_coherence_overheads"}, {"score": 0.003916098498242888, "phrase": "processing_elements"}, {"score": 0.0038290829316346654, "phrase": "memory_banks"}, {"score": 0.003660788124282824, "phrase": "new_opportunities"}, {"score": 0.003611751858608048, "phrase": "design_modularity"}, {"score": 0.003563370084662192, "phrase": "latency_and_manufacturing_cost"}, {"score": 0.0033161524383516824, "phrase": "lin"}, {"score": 0.003271849746895131, "phrase": "distributed"}, {"score": 0.003141830113006653, "phrase": "synthesisable_rtl"}, {"score": 0.003071965703296204, "phrase": "modular_stacking"}, {"score": 0.0029901699209266435, "phrase": "multi-core_cluster"}, {"score": 0.00295008998986066, "phrase": "limited_number"}, {"score": 0.0027700246703406925, "phrase": "cu-cu"}, {"score": 0.0025545134727682716, "phrase": "architectural_simulation_results"}, {"score": 0.0024202176034746337, "phrase": "c-lin"}, {"score": 0.0023451553255837317, "phrase": "traditional_network-on-chips_and_simple_time-division_multiplexing_buses"}, {"score": 0.0022118619998034742, "phrase": "comparable_speed"}], "paper_keywords": [""], "paper_abstract": "Shared tightly coupled data memories are key architectural elements for building multi-core clusters in programmable accelerators and embedded systems, as they provide a convenient shared memory abstraction while avoiding cache coherence overheads. The performance of these memories largely depends on the architecture of the interconnect used between processing elements (PEs) and memory banks. The advent of three-dimensional (3D) technology has provided new opportunities to increase design modularity and reduce latency and manufacturing cost. In this study, the authors propose two 3D network architectures: C-logarithmic interconnect (LIN) and Distributed logarithmic interconnect (D-LIN) (designed in synthesisable RTL), which allow modular stacking of multiple L1 memory dies over a multi-core cluster with a limited number of PEs. The authors have used two through-silicon-via technologies: the state-of-the-art micro-bumps and the promising and dense Cu-Cu direct bonding. The overhead of electrostatic discharge protection circuits has been considered, as well. Architectural simulation results demonstrate that, in processor-to-L1-memory context, C-LIN and D-LIN perform significantly better than traditional network-on-chips and simple time-division multiplexing buses. Furthermore, post-layout results show that the proposed 3D architectures achieve comparable speed against their 2D counterparts, whereas enabling modularity: from 256 kB to 2 MB L1 memory configurations with a single mask set.", "paper_title": "A case for three-dimensional stacking of tightly coupled data memories over multi-core clusters using low-latency interconnects", "paper_id": "WOS:000323542800001"}