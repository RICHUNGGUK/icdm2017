{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "human-authored_summaries"}, {"score": 0.03827782610127831, "phrase": "information_content"}, {"score": 0.0047527118741586055, "phrase": "human-generated_summaries"}, {"score": 0.004395742407834023, "phrase": "task_restrictions"}, {"score": 0.0035923719610602245, "phrase": "information_coverage"}, {"score": 0.003257960296952439, "phrase": "question_answer-based_cross-comprehension_test"}, {"score": 0.0030524448705134283, "phrase": "exact_words"}, {"score": 0.0028973794653974327, "phrase": "summary_authors"}, {"score": 0.002559859451119449, "phrase": "candidate_summary"}, {"score": 0.002477753633057247, "phrase": "qualitative_analysis"}, {"score": 0.0023826870912207303, "phrase": "cross-comprehension_test"}, {"score": 0.0021049977753042253, "phrase": "summary_author"}], "paper_keywords": [""], "paper_abstract": "Human-generated summaries are a blend of content and style, bound by the task restrictions, but are 'subject to subjectiveness' of the individuals summarising the documents. We study the impact of various facets that cause subjectivity such as brevity, information content and information coverage on human-authored summaries. The scale of subjectivity is quantitatively measured among various summaries using a question answer-based cross-comprehension test. The test evaluates summaries for meaning rather than exact words based on questions, framed by the summary authors, derived from the summary. The number of questions that cannot be answered after reading the candidate summary reflects its subjectivity. The qualitative analysis of the outcome of the cross-comprehension test shows the relationship between the length of a summary, information content and nature of questions framed by the summary author.", "paper_title": "On the subjectivity of human-authored summaries", "paper_id": "WOS:000278490800002"}