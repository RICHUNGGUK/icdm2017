{"auto_keywords": [{"score": 0.04799155246762869, "phrase": "update_estimation_value"}, {"score": 0.04670675420350503, "phrase": "present_state"}, {"score": 0.04304590109574224, "phrase": "reinforcement_learning"}, {"score": 0.035540924832287046, "phrase": "eligibility_trace"}, {"score": 0.00481495049065317, "phrase": "cooperation_online_reinforcement_learning_approach"}, {"score": 0.004422325007817267, "phrase": "state_transition"}, {"score": 0.004386637056030611, "phrase": "next_state"}, {"score": 0.0043161190928529755, "phrase": "online_reinforcement_learning"}, {"score": 0.004281284532043714, "phrase": "polynomial_search_time"}, {"score": 0.003775803846431323, "phrase": "estimation_value"}, {"score": 0.0036257863613963245, "phrase": "perfect_online_reinforcement_learning"}, {"score": 0.003453584878128719, "phrase": "online_ant_reinforcement_learning_method"}, {"score": 0.003210446135114967, "phrase": "basic_mechanisms"}, {"score": 0.0031459887611258765, "phrase": "delayed_reward"}, {"score": 0.0029244418026475832, "phrase": "learning_changes"}, {"score": 0.0028890603352364273, "phrase": "reinforcing_event"}, {"score": 0.0026103363762901423, "phrase": "online_ant_reinforcement_learning_algorithms"}, {"score": 0.0023970487263732737, "phrase": "eligibility_traces"}, {"score": 0.0023680327896318915, "phrase": "replacing_traces"}, {"score": 0.002246292543825748, "phrase": "significant_improvement"}, {"score": 0.0021221627745801478, "phrase": "optimal_solution"}, {"score": 0.0021049977753042253, "phrase": "ant_colony_system"}], "paper_keywords": [""], "paper_abstract": "Online reinforcement learning achieves learning after update estimation value for (state, action) pairs selecting in present state before do state transition by next state. Therefore, online reinforcement learning needs polynomial search time to find most optimal value-function. But, a lots of reinforcement learning that are proposed for online reinforcement learning update estimation value for (state, action) pairs that agents select in present state, and because estimation value for unselected (state, action) pairs is evaluated in other episodes, perfect online reinforcement learning is not. Therefore, in this paper, we propose online ant reinforcement learning method using Ant-Q and eligibility trace to solve this problem. The eligibility trace is one of the basic mechanisms in reinforcement learning to handle delayed reward. The traces are said to indicate the degree to which each state is eligible for undergoing learning changes should a reinforcing event occur. Formally, there are two kinds of eligibility traces (accumulating trace or replacing traces). In this paper, we propose online ant reinforcement learning algorithms using an eligibility traces which is called replace-trace methods. This method is a hybrid of Ant-Q and eligibility traces. Although replacing traces are only slightly different from accumulating traces, it can produce a significant improvement in optimization. We could know through an experiment that proposed reinforcement learning method converges faster to optimal solution than Ant Colony System and Ant-Q.", "paper_title": "A cooperation online reinforcement learning approach in Ant-Q", "paper_id": "WOS:000241790100054"}