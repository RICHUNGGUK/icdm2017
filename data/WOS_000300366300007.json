{"auto_keywords": [{"score": 0.04780597585803098, "phrase": "last_hidden_layer"}, {"score": 0.04670675420350503, "phrase": "least_squares_method"}, {"score": 0.00481495049065317, "phrase": "multilayer_perceptron"}, {"score": 0.004263972370406983, "phrase": "fast_and_uncomplicated_method"}, {"score": 0.004161553532799662, "phrase": "multilayer_perceptrons"}, {"score": 0.004028796143006953, "phrase": "considerable_single-step_reduction"}, {"score": 0.0036257863613963245, "phrase": "squared_errors"}, {"score": 0.003210446135114967, "phrase": "neuron_activation_functions"}, {"score": 0.00303319936111073, "phrase": "single_application"}, {"score": 0.002796782643988021, "phrase": "neuron_weights"}, {"score": 0.0027295084882654917, "phrase": "hidden_layer"}, {"score": 0.0026638482306228575, "phrase": "essential_strong_points"}, {"score": 0.0022281257694320433, "phrase": "learning_process"}], "paper_keywords": ["Multilayer perceptron", " Activation functions", " Least squares method"], "paper_abstract": "This article presents a fast and uncomplicated method to modify multilayer perceptrons allowing for a considerable single-step reduction of the cost function which in this case is the mean of squared errors. The method consists in, but is not limited to the change of neuron activation functions in the last hidden layer and in the single application of the least squares method. No changes are made to neuron weights in any hidden layer. Some essential strong points of the method lie in the fact that it can be used to improve operation of networks trained earlier and the learning process need not be started from the very beginning.", "paper_title": "A method to improve the performance of multilayer perceptron by utilizing various activation functions in the last hidden layer and the least squares method", "paper_id": "WOS:000300366300007"}