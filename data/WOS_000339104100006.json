{"auto_keywords": [{"score": 0.0455466088830923, "phrase": "gating_network"}, {"score": 0.04387075461973788, "phrase": "problem_space"}, {"score": 0.03741065522412441, "phrase": "first_group"}, {"score": 0.034684690143566524, "phrase": "second_group"}, {"score": 0.004672444473700524, "phrase": "machine_learning"}, {"score": 0.004561468495520688, "phrase": "divide-and-conquer_principle"}, {"score": 0.004321273059270048, "phrase": "earlier_works"}, {"score": 0.004269630481719451, "phrase": "different_strategies"}, {"score": 0.00377446822050123, "phrase": "partitioning_strategies"}, {"score": 0.003522192233486405, "phrase": "conventional_me_and_the_extensions_of_this_method_stochastically_partition"}, {"score": 0.003407549468745519, "phrase": "special_employed_error_function"}, {"score": 0.003198912570466607, "phrase": "clustering_method"}, {"score": 0.0031701655096648784, "phrase": "experts'_training_process"}, {"score": 0.0030030115662983956, "phrase": "implicit_problem_space"}, {"score": 0.0029670762930358394, "phrase": "tacit_competitive_process"}, {"score": 0.002861822702206142, "phrase": "implicitly_localised_experts"}, {"score": 0.0027602925235474317, "phrase": "explicitly_localised_experts"}, {"score": 0.002694609420649617, "phrase": "pre-specified_clusters"}, {"score": 0.0025448001425538076, "phrase": "mele"}, {"score": 0.0024250944519924383, "phrase": "complementary_features"}, {"score": 0.0023040642263522505, "phrase": "negative_correlation_learning_methods"}, {"score": 0.0022764737473071296, "phrase": "investigated_methods"}, {"score": 0.002262802370005939, "phrase": "complementary_strengths"}, {"score": 0.0021824773921574636, "phrase": "integrated_approaches"}, {"score": 0.0021049977753042253, "phrase": "future_research_directions"}], "paper_keywords": ["Classifier combining", " Mixture of experts", " Mixture of implicitly localised experts", " Mixture of explicitly localised expert"], "paper_abstract": "Mixture of experts (ME) is one of the most popular and interesting combining methods, which has great potential to improve performance in machine learning. ME is established based on the divide-and-conquer principle in which the problem space is divided between a few neural network experts, supervised by a gating network. In earlier works on ME, different strategies were developed to divide the problem space between the experts. To survey and analyse these methods more clearly, we present a categorisation of the ME literature based on this difference. Various ME implementations were classified into two groups, according to the partitioning strategies used and both how and when the gating network is involved in the partitioning and combining procedures. In the first group, The conventional ME and the extensions of this method stochastically partition the problem space into a number of subspaces using a special employed error function, and experts become specialised in each subspace. In the second group, the problem space is explicitly partitioned by the clustering method before the experts' training process starts, and each expert is then assigned to one of these sub-spaces. Based on the implicit problem space partitioning using a tacit competitive process between the experts, we call the first group the mixture of implicitly localised experts (MILE), and the second group is called mixture of explicitly localised experts (MELE), as it uses pre-specified clusters. The properties of both groups are investigated in comparison with each other. Investigation of MILE versus MELE, discussing the advantages and disadvantages of each group, showed that the two approaches have complementary features. Moreover, the features of the ME method are compared with other popular combining methods, including boosting and negative correlation learning methods. As the investigated methods have complementary strengths and limitations, previous researches that attempted to combine their features in integrated approaches are reviewed and, moreover, some suggestions are proposed for future research directions.", "paper_title": "Mixture of experts: a literature survey", "paper_id": "WOS:000339104100006"}