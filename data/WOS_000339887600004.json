{"auto_keywords": [{"score": 0.04522579410150115, "phrase": "scientific_workflows"}, {"score": 0.015719716506582538, "phrase": "data-intensive_scientific_workflows"}, {"score": 0.014514542286714822, "phrase": "transferred_data"}, {"score": 0.01343880116109546, "phrase": "public_clouds"}, {"score": 0.004691805714620268, "phrase": "cloud_computing"}, {"score": 0.004651458073288542, "phrase": "scientific_community"}, {"score": 0.0046114557978424245, "phrase": "efficient_allocation"}, {"score": 0.004403823349864079, "phrase": "transferred_data_overhead"}, {"score": 0.004266501496765747, "phrase": "intermediate_data_products"}, {"score": 0.0039028150615369513, "phrase": "np"}, {"score": 0.003813106398912961, "phrase": "evolutionary_approach"}, {"score": 0.0037911950902695534, "phrase": "task_allocation"}, {"score": 0.003589193972443107, "phrase": "allocation_chromosome"}, {"score": 0.003417565253539315, "phrase": "execution_order"}, {"score": 0.003378385493203297, "phrase": "scientific_workflow_representation"}, {"score": 0.0033300397189425304, "phrase": "multi-objective_optimization"}, {"score": 0.003282383509542342, "phrase": "cloud_cost_model"}, {"score": 0.0032541167047457926, "phrase": "tailored_evolution_operators"}, {"score": 0.0031891008393769515, "phrase": "possible_solutions"}, {"score": 0.003152531962718477, "phrase": "crossover_and_mutation_operators"}, {"score": 0.003010389418340144, "phrase": "total_workflow_runtime"}, {"score": 0.0029844579234112466, "phrase": "crossover_operators"}, {"score": 0.002924813916236766, "phrase": "data_overhead"}, {"score": 0.0028912667826416494, "phrase": "mutation_operators_swamp"}, {"score": 0.002825319183890361, "phrase": "pre-defined_rules"}, {"score": 0.002776844761182965, "phrase": "proposed_approach"}, {"score": 0.0027608716347791266, "phrase": "current_state-of-the_art_approaches"}, {"score": 0.002659256549464528, "phrase": "existing_heuristics"}, {"score": 0.00264395802739923, "phrase": "small_workflows"}, {"score": 0.0026060946618700416, "phrase": "larger_synthetic_workflows"}, {"score": 0.00245997322259401, "phrase": "popular_scientific_workflow_managers"}, {"score": 0.00243877141503813, "phrase": "real_workflows"}, {"score": 0.0023762535034673017, "phrase": "public_cloud"}, {"score": 0.0023086626639649724, "phrase": "existing_schedulers"}, {"score": 0.0021981315377404245, "phrase": "improved_data_locality"}, {"score": 0.0021854800233595444, "phrase": "greater_impact"}, {"score": 0.0021233040503324005, "phrase": "data_provenance"}, {"score": 0.0021049977753042253, "phrase": "data_persistence"}], "paper_keywords": ["Data-intensive workflows", " Cloud computing", " Scheduling", " Allocation", " Evolutionary computation"], "paper_abstract": "An important challenge for the adoption of cloud computing in the scientific community remains the efficient allocation and execution of data-intensive scientific workflows to reduce execution time and the size of transferred data. The transferred data overhead is becoming significant with emerging scientific workflows that have input/output files and intermediate data products ranging in the hundreds of gigabytes. The allocation of scientific workflows on public clouds can be described through a variety of perspectives and parameters, and has been proved to be NP-complete. This paper proposes an evolutionary approach for task allocation on public clouds considering data transfer and execution time. In our framework, a solution is represented using an allocation chromosome that encodes the allocation of tasks to nodes, and an ordering chromosome that defines the execution order according to the scientific workflow representation. We propose a multi-objective optimization that relies on a cloud cost model and employs tailored evolution operators. Starting from a population of possible solutions, we employ crossover and mutation operators on both chromosomes aiming at optimizing the data transferred between nodes as well as the total workflow runtime. The crossover operators combine parts of solutions to reduce data overhead, whereas the mutation operators swamp between parts of the same chromosome according to pre-defined rules. Our experimental study compares between the proposed approach and current state-of-the art approaches using synthetic and real-life workflows. Our algorithm performs similarly to existing heuristics for small workflows and shows up to 80 % improvements for larger synthetic workflows. To further validate our approach we compare between the allocation and scheduling obtained by our approach with that obtained by popular scientific workflow managers, when real workflows with hundreds of tasks are executed on a public cloud. The results show a 10 % improvement in runtime over existing schedulers, caused by a 80 % reduction in transferred data and optimized allocation and ordering of tasks. This improved data locality has greater impact as it can be employed to improve and study data provenance and facilitate data persistence for scientific workflows.", "paper_title": "Science in the Cloud: Allocation and Execution of Data-Intensive Scientific Workflows", "paper_id": "WOS:000339887600004"}