{"auto_keywords": [{"score": 0.04974492695968205, "phrase": "generative_embeddings"}, {"score": 0.014667499720165926, "phrase": "generative_models"}, {"score": 0.00704290572063537, "phrase": "generative_embedding"}, {"score": 0.006848819141163261, "phrase": "generative_model"}, {"score": 0.00481495049065317, "phrase": "information_theoretic_kernels"}, {"score": 0.004714869813251479, "phrase": "classical_approaches"}, {"score": 0.004633052743252902, "phrase": "structured_objects"}, {"score": 0.00444241196374042, "phrase": "standard_bayesian_framework"}, {"score": 0.0042446888127531945, "phrase": "discriminative_learning"}, {"score": 0.003651039356522665, "phrase": "object_space"}, {"score": 0.0036128527616772848, "phrase": "fixed_dimensional_space"}, {"score": 0.0035625556447293804, "phrase": "discriminative_classifier_learning"}, {"score": 0.0029172107428045964, "phrase": "embedded_data"}, {"score": 0.0025801648787263662, "phrase": "different_approach"}, {"score": 0.0025175684590726796, "phrase": "discriminative_learning_step"}, {"score": 0.002447882520797866, "phrase": "probabilistic_nature"}, {"score": 0.002371783323468861, "phrase": "probability_measures"}, {"score": 0.002289993730291136, "phrase": "recent_family"}, {"score": 0.0022739766507258105, "phrase": "non-extensive_information_theoretic_kernels"}, {"score": 0.0022344212744309327, "phrase": "different_generative_embeddings"}, {"score": 0.0021878600131802453, "phrase": "different_medical_applications"}, {"score": 0.002149799391146313, "phrase": "state-of-the-art_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Hybrid generative-discriminative schemes", " Generative embeddings", " Probabilistic latent semantic analysis", " Information theoretic kernels"], "paper_abstract": "Classical approaches to learn classifiers for structured objects (e.g., images, sequences) use generative models in a standard Bayesian framework. To exploit the state-of-the-art performance of discriminative learning, while also taking advantage of generative models of the data, generative embeddings have been recently proposed as a way of building hybrid discriminative/generative approaches. A generative embedding is a mapping, induced by a generative model (usually learned from data), from the object space into a fixed dimensional space, adequate for discriminative classifier learning. Generative embeddings have been shown to often outperform the classifiers obtained directly from the generative models upon which they are built. Using a generative embedding for classification involves two main steps: (i) defining and learning a generative model and using it to build the embedding: (ii) discriminatively learning a (maybe kernel) classifier with the embedded data. The literature on generative embeddings is essentially focused on step (i), usually taking some standard off-the-shelf tool for step (ii). Here, we adopt a different approach, by focusing also on the discriminative learning step. In particular, we exploit the probabilistic nature of generative embeddings, by using kernels defined on probability measures; in particular we investigate the use of a recent family of non-extensive information theoretic kernels on the top of different generative embeddings. We show, in different medical applications that the approach yields state-of-the-art performance. (C) 2012 Elsevier B.V. All rights reserved.", "paper_title": "Combining information theoretic kernels with generative embeddings for classification", "paper_id": "WOS:000312171100018"}