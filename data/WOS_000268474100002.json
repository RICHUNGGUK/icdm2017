{"auto_keywords": [{"score": 0.03522192095984631, "phrase": "flash"}, {"score": 0.027007767302745306, "phrase": "lu"}, {"score": 0.00481495049065317, "phrase": "matrix_algorithms-by-blocks_for_thread-level_parallelism"}, {"score": 0.004695900553353526, "phrase": "thread-level_parallelism"}, {"score": 0.004637478829869904, "phrase": "primary_means"}, {"score": 0.004598933498016774, "phrase": "continued_performance_improvement"}, {"score": 0.004541712637353326, "phrase": "programmability_issue"}, {"score": 0.0043560464825349275, "phrase": "architectural_advances"}, {"score": 0.004248294537944227, "phrase": "legacy_libraries"}, {"score": 0.004160531596936783, "phrase": "linear_algebra"}, {"score": 0.0040916230716372265, "phrase": "viable_solution"}, {"score": 0.003990385737207714, "phrase": "early_design_decisions"}, {"score": 0.0037480753312398754, "phrase": "promising_solution"}, {"score": 0.0037158211531505953, "phrase": "supermatrix"}, {"score": 0.003701401653850141, "phrase": "problem_domain"}, {"score": 0.0036553070575015344, "phrase": "first_abstraction"}, {"score": 0.0034620831765465425, "phrase": "contiguous_blocks"}, {"score": 0.003334309053314909, "phrase": "operand_descriptions"}, {"score": 0.003265365666305185, "phrase": "particular_operation"}, {"score": 0.003197843245296877, "phrase": "library_implementor"}, {"score": 0.0031579997262619758, "phrase": "runtime_system"}, {"score": 0.0030414140732789186, "phrase": "data_dependencies"}, {"score": 0.002509284173541882, "phrase": "incremental_pivoting"}, {"score": 0.0023964590500856887, "phrase": "qr_factorization"}, {"score": 0.002337066031028783, "phrase": "out-of-core_computation"}, {"score": 0.00227914162528171, "phrase": "anecdotal_evidence"}, {"score": 0.0022041312401435346, "phrase": "core_functionality"}, {"score": 0.002149494537191303, "phrase": "high_productivity"}, {"score": 0.0021315842986705485, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "high_performance"}], "paper_keywords": ["Algorithms", " Performance", " Linear algebra", " libraries", " high-performance", " multithreaded architectures"], "paper_abstract": "With the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. We argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. We propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. The first abstraction, FLASH, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. Operand descriptions are registered for a particular operation a priori by the library implementor. A runtime system, SuperMatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. But not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. We show how our recently proposed LU factorization with incremental pivoting and a closely related algorithm-by-blocks for the QR factorization, both originally designed for out-of-core computation, overcome this difficulty. Anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable.", "paper_title": "Programming Matrix Algorithms-by-Blocks for Thread-Level Parallelism", "paper_id": "WOS:000268474100002"}