{"auto_keywords": [{"score": 0.029469450411198327, "phrase": "theorem"}, {"score": 0.0255366205104467, "phrase": "entropy"}, {"score": 0.00481495049065317, "phrase": "gelfand-yaglom-perez"}, {"score": 0.004741395005218184, "phrase": "generalized_relative_entropy_functionals"}, {"score": 0.004668957916795845, "phrase": "measure-theoretic_definition"}, {"score": 0.004668824567474769, "phrase": "kullback-leibler"}, {"score": 0.004367581653370531, "phrase": "basic_role"}, {"score": 0.0041490029730869345, "phrase": "mutual_information"}, {"score": 0.004106612344123407, "phrase": "conditional_forms"}, {"score": 0.003724810609448597, "phrase": "measure-theoretic_kl-entropy"}, {"score": 0.0036678472439508484, "phrase": "measure-theoretic_definitions"}, {"score": 0.0035565113234273926, "phrase": "ergodic_theorems"}, {"score": 0.0035201528214608914, "phrase": "information_theory"}, {"score": 0.0034841647157189985, "phrase": "non-discrete_cases"}, {"score": 0.0034308688638878286, "phrase": "fundamental_theorem"}, {"score": 0.0032256881625734777, "phrase": "m.s._pinsker"}, {"score": 0.0031116963435584982, "phrase": "random_variables"}, {"score": 0.0030171910207671205, "phrase": "holden-day"}, {"score": 0.002986329509791943, "phrase": "san_francisco"}, {"score": 0.0028221322455573624, "phrase": "amiel_feinstein"}, {"score": 0.002559444836360178, "phrase": "measurable_partitions"}, {"score": 0.0025307171959824733, "phrase": "renyi"}, {"score": 0.0023622133161746977, "phrase": "tsallis"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["measure space", " Kullback-Leibler", " Renyi"], "paper_abstract": "The measure-theoretic definition of Kullback-Leibler relative-entropy (or simply KL-entropy) plays a basic role in defining various classical information measures on general spaces. Entropy, mutual information and conditional forms of entropy can be expressed in terms of KL-entropy and hence properties of their measure-theoretic analogs will follow from those of measure-theoretic KL-entropy. These measure-theoretic definitions are key to extending the ergodic theorems of information theory to non-discrete cases. A fundamental theorem in this respect is the Gelfand-Yaglom-Perez (GYP) Theorem [M.S. Pinsker, Information and Information Stability of Random Variables and Process, 1960, Holden-Day, San Francisco, CA (English ed., 1964, translated and edited by Amiel Feinstein), Theorem. 2.4.2] which states that measure-theoretic relative-entropy equals the suprenmum of relative-entropies over all measurable partitions. This paper states and proves the GYP-theorem for Renyi relative-entropy of order greater than one. Consequently, the result can be easily extended to Tsallis relative-entropy. (C) 2007 Elsevier Inc. All rights reserved.", "paper_title": "Gelfand-Yaglom-Perez theorem for generalized relative entropy functionals", "paper_id": "WOS:000250899400015"}