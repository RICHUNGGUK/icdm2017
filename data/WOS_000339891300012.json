{"auto_keywords": [{"score": 0.037416159671967784, "phrase": "eeg"}, {"score": 0.015557614148972563, "phrase": "video_content"}, {"score": 0.014395562743566147, "phrase": "arousal_spaces"}, {"score": 0.00481495049065317, "phrase": "users'_eeg"}, {"score": 0.004569630374630379, "phrase": "novel_hybrid_approaches"}, {"score": 0.0043367547691560175, "phrase": "users'_electroencephalogram"}, {"score": 0.004030449969901066, "phrase": "video_clips"}, {"score": 0.003988487841970088, "phrase": "five_frequency_features"}, {"score": 0.0038249462699142733, "phrase": "eeg_signals"}, {"score": 0.0037456980766939836, "phrase": "statistical_analyses"}, {"score": 0.0035920758080335655, "phrase": "emotional_tags"}, {"score": 0.0031845067058084613, "phrase": "independent_feature-level_fusion"}, {"score": 0.0031513240597392843, "phrase": "decision-level_fusion"}, {"score": 0.003118486096035156, "phrase": "dependent_feature-level_fusion"}, {"score": 0.002867771524071872, "phrase": "psychophysiological_experiment"}, {"score": 0.0027790254309734428, "phrase": "emotion-induced_video_clips"}, {"score": 0.002750056254309311, "phrase": "users'_eeg_responses"}, {"score": 0.002693018258362651, "phrase": "selected_video_clips"}, {"score": 0.002651015392318126, "phrase": "emotional_video_tags"}, {"score": 0.0026096659285441384, "phrase": "participants'_self-report"}, {"score": 0.0025156683465194967, "phrase": "experimental_results"}, {"score": 0.0024634797027967203, "phrase": "proposed_fusion_methods"}, {"score": 0.0024250482323322606, "phrase": "conventional_emotional_tagging_methods"}, {"score": 0.002172266188075892, "phrase": "semantic_gap"}, {"score": 0.0021383680500215267, "phrase": "low-level_video_features"}, {"score": 0.0021049977753042253, "phrase": "users'_high-level_emotional_tags"}], "paper_keywords": ["Emotional tagging", " Videos", " Independent feature-level fusion", " Decision-level fusion", " Dependent feature-level fusion"], "paper_abstract": "In this paper, we propose novel hybrid approaches to annotate videos in valence and arousal spaces by using users' electroencephalogram (EEG) signals and video content. Firstly, several audio and visual features are extracted from video clips and five frequency features are extracted from each channel of the EEG signals. Secondly, statistical analyses are conducted to explore the relationships among emotional tags, EEG and video features. Thirdly, three Bayesian Networks are constructed to annotate videos by combining the video and EEG features at independent feature-level fusion, decision-level fusion and dependent feature-level fusion. In order to evaluate the effectiveness of our approaches, we designed and conducted the psychophysiological experiment to collect data, including emotion-induced video clips, users' EEG responses while watching the selected video clips, and emotional video tags collected through participants' self-report after watching each clip. The experimental results show that the proposed fusion methods outperform the conventional emotional tagging methods that use either video or EEG features alone in both valence and arousal spaces. Moreover, we can narrow down the semantic gap between the low-level video features and the users' high-level emotional tags with the help of EEG features.", "paper_title": "Hybrid video emotional tagging using users' EEG and video content", "paper_id": "WOS:000339891300012"}