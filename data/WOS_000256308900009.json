{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "supervised_learning"}, {"score": 0.004529093447865156, "phrase": "large_class"}, {"score": 0.004437613070853689, "phrase": "regularization_methods"}, {"score": 0.004216880725292, "phrase": "spectral_regularization"}, {"score": 0.003966388754366831, "phrase": "ill-posed_inverse_problems"}, {"score": 0.003769006950727621, "phrase": "regularized_learning_algorithms"}, {"score": 0.003509006038574667, "phrase": "consistent_kernel_methods"}, {"score": 0.0029196116767956273, "phrase": "numerical_stabilization"}, {"score": 0.002831462722290752, "phrase": "matrix_inversion_problem"}, {"score": 0.002530359917257626, "phrase": "common_derivation"}, {"score": 0.002479150189700726, "phrase": "different_computational_and_theoretical_properties"}, {"score": 0.0021049977753042253, "phrase": "real-world_problems"}], "paper_keywords": [""], "paper_abstract": "We discuss how a large class of regularization methods, collectively known as spectral regularization and originally designed for solving ill-posed inverse problems, gives rise to regularized learning algorithms. All of these algorithms are consistent kernel methods that can be easily implemented. The intuition behind their derivation is that the same principle allowing for the numerical stabilization of a matrix inversion problem is crucial to avoid overfitting. The various methods have a common derivation but different computational and theoretical properties. We describe examples of such algorithms, analyze their classification performance on several data sets and discuss their applicability to real-world problems.", "paper_title": "Spectral algorithms for supervised learning", "paper_id": "WOS:000256308900009"}