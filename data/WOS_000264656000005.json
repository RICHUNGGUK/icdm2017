{"auto_keywords": [{"score": 0.01225699895429025, "phrase": "desired_properties"}, {"score": 0.00481495049065317, "phrase": "temporally_extended_preferences"}, {"score": 0.004476878957065829, "phrase": "preferred_plan"}, {"score": 0.004223521213460728, "phrase": "planner's_input"}, {"score": 0.003941141575848601, "phrase": "rich_class"}, {"score": 0.003870009704443373, "phrase": "so-called_temporally_extended_preferences"}, {"score": 0.003786337629480068, "phrase": "simple_preferences"}, {"score": 0.003704467870496114, "phrase": "final_state"}, {"score": 0.0035459819741057836, "phrase": "entire_sequence"}, {"score": 0.003272767888056937, "phrase": "planning_problem"}, {"score": 0.003213660268958121, "phrase": "equivalent_planning_problem"}, {"score": 0.0030873424635143045, "phrase": "inputed_planning_domain"}, {"score": 0.0030537595549694134, "phrase": "new_set"}, {"score": 0.0028807066376546502, "phrase": "new_heuristics"}, {"score": 0.0028493650087584774, "phrase": "specialized_search_algorithm"}, {"score": 0.0027775505385261553, "phrase": "preferred_plans"}, {"score": 0.002737327060030586, "phrase": "fairly_general_conditions"}, {"score": 0.00250789768689397, "phrase": "admissible_heuristics"}, {"score": 0.0023657047082774286, "phrase": "restricted_plan_length"}, {"score": 0.002280948283235427, "phrase": "hplan-p_planning_system"}, {"score": 0.0021752783872387173, "phrase": "distinguished_performance"}, {"score": 0.0021515951441422082, "phrase": "qualitative_preferences_track"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Planning with preferences", " Temporally extended preferences", " PDDL3"], "paper_abstract": "Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred plan-i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPLAN-P planning system and used it to compete in the 5th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track. (C) 2008 Elsevier B.V. All rights reserved.", "paper_title": "A heuristic search approach to planning with temporally extended preferences", "paper_id": "WOS:000264656000005"}