{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "data_regression"}, {"score": 0.004602773636967002, "phrase": "novel_feature-selection_algorithm"}, {"score": 0.004334280585565239, "phrase": "irrelevant_features"}, {"score": 0.004237666411136617, "phrase": "proposed_method"}, {"score": 0.004112175423546628, "phrase": "well-established_machine-learning_technique"}, {"score": 0.0039308449157320815, "phrase": "underlying_data_distribution"}, {"score": 0.0038431888954930083, "phrase": "key_idea"}, {"score": 0.0036188435232325337, "phrase": "arbitrarily_complex_nonlinear_problem"}, {"score": 0.003485304124598714, "phrase": "locally_linear_ones"}, {"score": 0.0034332736827447654, "phrase": "local_information"}, {"score": 0.0032817813775647756, "phrase": "feature_relevance"}, {"score": 0.0032085525502105836, "phrase": "least_squares_loss_framework"}, {"score": 0.0026583500966092044, "phrase": "gradient_descent"}, {"score": 0.0025989960809479104, "phrase": "simple_update_rule"}, {"score": 0.002502995662011305, "phrase": "synthetic_and_real-world_data_sets"}, {"score": 0.0023390219342052623, "phrase": "feature-selection_problem"}, {"score": 0.002218933616777076, "phrase": "crown_copyright"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feature selection", " Local information", " Irrelevant feature", " Least squares loss", " Gradient descent", " Data regression"], "paper_abstract": "This paper presents a novel feature-selection algorithm for data regression with a lot of irrelevant features. The proposed method is based on well-established machine-learning technique without any assumption about the underlying data distribution. The key idea in this method is to decompose an arbitrarily complex nonlinear problem into a set of locally linear ones through local information, and to learn globally feature relevance within the least squares loss framework. In contrast to other feature-selection algorithms for data regression, the learning of this method is efficient since the solution can be readily found through gradient descent with a simple update rule. Experiments on some synthetic and real-world data sets demonstrate the viability of our formulation of the feature-selection problem and the effectiveness of our algorithm. Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.", "paper_title": "A local information-based feature-selection algorithm for data regression", "paper_id": "WOS:000318837400010"}