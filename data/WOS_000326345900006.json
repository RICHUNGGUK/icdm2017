{"auto_keywords": [{"score": 0.02633945252566188, "phrase": "balcan"}, {"score": 0.00481495049065317, "phrase": "iterative_self-labeling"}, {"score": 0.004741167616693272, "phrase": "linear_structured"}, {"score": 0.004668510065740584, "phrase": "strong_assumption"}, {"score": 0.004614745158543967, "phrase": "generalization_guarantees"}, {"score": 0.00456159659731398, "phrase": "standard_pac_framework"}, {"score": 0.004271729935895193, "phrase": "possibly_outdated_data"}, {"score": 0.004109865377936045, "phrase": "biased_collections"}, {"score": 0.00398478333092707, "phrase": "real-world_applications"}, {"score": 0.00393886140366453, "phrase": "different_source"}, {"score": 0.003760385494931853, "phrase": "new_research_area"}, {"score": 0.0037170399498025215, "phrase": "domain_adaptation"}, {"score": 0.003534889186471058, "phrase": "theoretical_results"}, {"score": 0.0034672240102960644, "phrase": "generalization_bounds"}, {"score": 0.0033877275374263314, "phrase": "self-labeling_da"}, {"score": 0.0032972747743006603, "phrase": "semi-labeled_target_data"}, {"score": 0.003111472132927442, "phrase": "target_domain"}, {"score": 0.002913483929932102, "phrase": "necessary_theoretical_conditions"}, {"score": 0.0028798723050615788, "phrase": "self-labeling_da_algorithm"}, {"score": 0.0028356575042385156, "phrase": "actual_domain_adaptation"}, {"score": 0.0027706013216537042, "phrase": "theoretical_recommendations"}, {"score": 0.0027175263159330523, "phrase": "new_iterative_da_algorithm"}, {"score": 0.002624533657819515, "phrase": "structured_data"}, {"score": 0.0025445415633818472, "phrase": "new_theory"}, {"score": 0.0023550410480905727, "phrase": "valid_kernel"}, {"score": 0.0022744242010653997, "phrase": "sparse_models"}, {"score": 0.0021965609224818853, "phrase": "structured_image_classification_task"}, {"score": 0.0021628142746829187, "phrase": "self-labeling_domain_adaptation"}, {"score": 0.0021378442381693847, "phrase": "new_original_way"}, {"score": 0.0021049977753042253, "phrase": "scaling_and_rotation_problems"}], "paper_keywords": ["Domain adaptation", " edit distance", " sparse learning"], "paper_abstract": "A strong assumption to derive generalization guarantees in the standard PAC framework is that training (or source) data and test (or target) data are drawn according to the same distribution. Because of the presence of possibly outdated data in the training set, or the use of biased collections, this assumption is often violated in real-world applications leading to different source and target distributions. To go around this problem, a new research area known as Domain Adaptation (DA) has recently been introduced giving rise to many adaptation algorithms and theoretical results in the form of generalization bounds. This paper deals with self-labeling DA whose goal is to iteratively incorporate semi-labeled target data in the learning set to progressively adapt the classifier from the source to the target domain. The contribution of this work is three-fold: First, we provide the minimum and necessary theoretical conditions for a self-labeling DA algorithm to perform an actual domain adaptation. Second, following these theoretical recommendations, we design a new iterative DA algorithm, called GESIDA, able to deal with structured data. This algorithm makes use of the new theory of learning with (epsilon, gamma, tau)-good similarity functions introduced by Balcan et al., which does not require the use of a valid kernel to learn well and allows us to induce sparse models. Finally, we apply our algorithm on a structured image classification task and show that self-labeling domain adaptation is a new original way to deal with scaling and rotation problems.", "paper_title": "ITERATIVE SELF-LABELING DOMAIN ADAPTATION FOR LINEAR STRUCTURED IMAGE CLASSIFICATION", "paper_id": "WOS:000326345900006"}