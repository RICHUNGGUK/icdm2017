{"auto_keywords": [{"score": 0.04350337824761055, "phrase": "increasing_dimensionality"}, {"score": 0.023078881942759068, "phrase": "high-dimensional_data_space"}, {"score": 0.019910029594324603, "phrase": "distance_function"}, {"score": 0.009319249460667225, "phrase": "distance_distribution"}, {"score": 0.006023864420749897, "phrase": "theoretical_results"}, {"score": 0.005150731928552142, "phrase": "sdp"}, {"score": 0.00481495049065317, "phrase": "distance_functions"}, {"score": 0.004554917803750564, "phrase": "nearest_neighbor_search"}, {"score": 0.0042668414931947615, "phrase": "pearson_variation"}, {"score": 0.0039969114480547865, "phrase": "high-dimensional_space"}, {"score": 0.003932129823796712, "phrase": "commonly_used_l-p_metric"}, {"score": 0.0038181472934630347, "phrase": "necessary_condition"}, {"score": 0.0036832960694422765, "phrase": "function_design"}, {"score": 0.00285396838599055, "phrase": "sufficient_conditions"}, {"score": 0.002789346122034762, "phrase": "sufficient_and_necessary_conditions"}, {"score": 0.002681941877978841, "phrase": "effective_and_valid_indices"}, {"score": 0.002570237158808816, "phrase": "theoretical_analysis"}, {"score": 0.0025368092524479615, "phrase": "unstable_phenomena"}, {"score": 0.0023995115472744376, "phrase": "meaningful_distance_function"}, {"score": 0.0021049977753042253, "phrase": "distance-based_applications"}], "paper_keywords": ["Curse of dimensionality", " classification", " data clustering", " nearest neighbor search"], "paper_abstract": "Effective distance functions in high-dimensional data space, such as those used for clustering, nearest neighbor search, and indexing, are very important in solutions for many data mining problems. Recent research has shown that if the Pearson variation of the distance distribution converges to zero with increasing dimensionality, the distance function will become unstable (or meaningless) in high-dimensional space, even with the commonly used L-p metric in the euclidean space. However, the necessary condition for unstability of a distance function, which is required for function design, remains unknown. In this paper, we shall prove that the following conditions are equivalent to unstability: 1) the Pearson variation of the distance distribution for any given query converges to 0 with increasing dimensionality, 2) the ratio of the distances of any two points to the query converges in probability to 1 with increasing dimensionality, and 3) the second moment coefficient converges to 1 with increasing dimensionality. With these results, we have the necessary and the sufficient conditions for unstability, whose negatives imply the sufficient and necessary conditions for stability. Based on these theoretical results, we employ some effective and valid indices for testing the stability of a distance function. In addition, this theoretical analysis inspires us that unstable phenomena are rooted in variation of the distance distribution. To demonstrate the theoretical results, we design a meaningful distance function, called the Shrinkage-Divergence Proximity (SDP), based on a given distance function. It is shown empirically that the SDP significantly outperforms other measures in terms of stability in high-dimensional data space, and is thus more suitable for distance-based applications.", "paper_title": "On the Design and Applicability of Distance Functions in High-Dimensional Data Space", "paper_id": "WOS:000263394400005"}