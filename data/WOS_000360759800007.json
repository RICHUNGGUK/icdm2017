{"auto_keywords": [{"score": 0.0495133738546852, "phrase": "action_recognition"}, {"score": 0.045984768987991385, "phrase": "local_spatiotemporal_features"}, {"score": 0.036570551756537416, "phrase": "quantization_error"}, {"score": 0.00481495049065317, "phrase": "spatiotemporal_interest_points"}, {"score": 0.00430067200637091, "phrase": "promising_results"}, {"score": 0.004249825947483216, "phrase": "human_action_recognition"}, {"score": 0.004125314447335291, "phrase": "structural_information"}, {"score": 0.003980687255880282, "phrase": "important_information"}, {"score": 0.003933609372601274, "phrase": "motion_structures"}, {"score": 0.0038411109150557504, "phrase": "recent_methods"}, {"score": 0.003706410380455668, "phrase": "quantized_spatiotemporal_features"}, {"score": 0.0033898953358387075, "phrase": "unreliable_representation"}, {"score": 0.0031561961485589633, "phrase": "coding_method"}, {"score": 0.003009390313137981, "phrase": "reconstruction_ability"}, {"score": 0.0029737652463012318, "phrase": "visual_words"}, {"score": 0.002903771635043968, "phrase": "probabilistic_interpretation"}, {"score": 0.0027358910467781155, "phrase": "new_type"}, {"score": 0.0025471636425006155, "phrase": "contextual_structural_information"}, {"score": 0.0025169965861573185, "phrase": "interest_points"}, {"score": 0.002428617044669043, "phrase": "multi-layered_contexts"}, {"score": 0.0022881443200668886, "phrase": "proposed_method"}, {"score": 0.0022077827041801193, "phrase": "experiment_results"}, {"score": 0.00213023743193503, "phrase": "better_performance"}, {"score": 0.0021049977753042253, "phrase": "previous_methods"}], "paper_keywords": ["Action recognition", " Contextual features", " Cumulative probability histogram", " Sparse coding"], "paper_abstract": "Although traditional bag-of-words model, together with local spatiotemporal features, has shown promising results for human action recognition, it ignores all structural information of features, which carries important information of motion structures in videos. Recent methods usually characterize the relationship of quantized spatiotemporal features to overcome this drawback. However, the propagation of quantization error leads to an unreliable representation. To alleviate the propagation of quantization error, we present a coding method, which considers not only the spatial similarity but also the reconstruction ability of visual words after giving a probabilistic interpretation of coding coefficients. Based on our coding method, a new type of feature called cumulative probability histogram is proposed to robustly characterize contextual structural information around interest points, which are extracted from multi-layered contexts and assumed to be complementary to local spatiotemporal features. The proposed method is verified on four benchmark datasets. Experiment results show that our method can achieve better performance than previous methods in action recognition.", "paper_title": "Augmenting bag-of-words: a robust contextual representation of spatiotemporal interest points for action recognition", "paper_id": "WOS:000360759800007"}