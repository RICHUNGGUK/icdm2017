{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "dissimilarity_data"}, {"score": 0.0047057084749407485, "phrase": "new_flexible_mahalanobis-like_metric"}, {"score": 0.004426308135681733, "phrase": "feature-based_representation"}, {"score": 0.004007083686875716, "phrase": "proximity_data"}, {"score": 0.0034118234596993836, "phrase": "particular_point"}, {"score": 0.003308862957060602, "phrase": "discriminant_analysis"}, {"score": 0.003088384809415812, "phrase": "gaussian_classifier"}, {"score": 0.0029951555610569225, "phrase": "decision_rules"}, {"score": 0.0027741761880779535, "phrase": "quadratic_classifier"}, {"score": 0.0024918548761791435, "phrase": "numerical_experiments"}, {"score": 0.002453934333239467, "phrase": "artificial_and_real_data"}, {"score": 0.0023435920614377306, "phrase": "support_vector_machines"}, {"score": 0.002290291401956985, "phrase": "knn_classifier"}], "paper_keywords": ["classification", " dissimilarity data", " Mahalanobis distance", " Gaussian classifier"], "paper_abstract": "Statistical pattern recognition traditionally relies on feature-based representation. For many applications, such vector representation is not available and we only possess proximity data (distance, dissimilarity, similarity, ranks, etc.). In this paper, we consider a particular point of view on discriminant analysis from dissimilarity data. Our approach is inspired by the Gaussian classifier and we defined decision rules to mimic the behavior of a linear or a quadratic classifier. The number of parameters is limited (two per class). Numerical experiments on artificial and real data show interesting behavior compared to Support Vector Machines and to kNN classifier: (a) lower or equivalent error rate, (b) equivalent CPU time, (c) more robustness with sparse dissimilarity data.", "paper_title": "Classification of dissimilarity data with a new flexible Mahalanobis-like metric", "paper_id": "WOS:000258579900010"}