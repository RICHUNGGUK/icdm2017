{"auto_keywords": [{"score": 0.03500259129099465, "phrase": "regression_problem"}, {"score": 0.027542576002845362, "phrase": "discrete_algorithms"}, {"score": 0.027396443083125638, "phrase": "acm"}, {"score": 0.02724960803874563, "phrase": "new_york"}, {"score": 0.027106442117478768, "phrase": "siam"}, {"score": 0.02695975608428217, "phrase": "philadelphia"}, {"score": 0.004685012337701536, "phrase": "matrix_a"}, {"score": 0.004583579470330216, "phrase": "vector_b"}, {"score": 0.004472078324288593, "phrase": "number_p"}, {"score": 0.0038470175104007524, "phrase": "efficient_two-stage_sampling-based_approximation_algorithm"}, {"score": 0.0032020057039570484, "phrase": "second_stage"}, {"score": 0.003132579620368124, "phrase": "first_stage"}, {"score": 0.0029331771326095504, "phrase": "new_sample"}, {"score": 0.0027691111795848183, "phrase": "existing_algorithms"}, {"score": 0.0027539714188091866, "phrase": "special_cases"}, {"score": 0.0026290681752397303, "phrase": "proceedings"}, {"score": 0.0024679299547216956, "phrase": "p._drineas"}, {"score": 0.0024210112323786374, "phrase": "s._muthukrishnan"}, {"score": 0.0021049977753042253, "phrase": "independent_interest"}], "paper_keywords": ["randomized algorithms", " sampling algorithms", " l(p) regression"], "paper_abstract": "The l(p) regression problem takes as input a matrix A is an element of R-nxd, a vector b is an element of R-n, and a number p is an element of [1,infinity), and it returns as output a number Z and a vector x(OPT) is an element of R-d such that Z = min(x subset of R)(d) parallel to Ax-b parallel to(p) = parallel to Ax(opt) -b parallel to p. In this paper, we construct coresets and obtain an efficient two-stage sampling-based approximation algorithm for the very overconstrained (n >> d) version of this classical problem, for all p is an element of left perpendicular1,infinity). The first stage of our algorithm nonuniformly samples (r) over cap1 = O(36(p)d(max){p/2+ 1, p}+1) rows of A and the corresponding elements of b, and then it solves the l(p) regression problem on the sample; we prove this is an 8-approximation. The second stage of our algorithm uses the output of the first stage to resample (r) over cap1/is an element of(2) constraints, and then it solves the l(p) regression problem on the new sample; we prove this is a (1 + is an element of)-approximation. Our algorithm unifies, improves upon, and extends the existing algorithms for special cases of l(p) regression, namely, p = 1, 2 [K. L. Clarkson, in Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms, ACM, New York, SIAM, Philadelphia, 2005, pp. 257-266; P. Drineas, M. W. Mahoney, and S. Muthukrishnan, in Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms, ACM, New York, SIAM, Philadelphia, 2006, pp. 1127-1136]. In the course of proving our result, we develop two concepts-well-conditioned bases and subspace-preserving sampling-that are of independent interest.", "paper_title": "SAMPLING ALGORITHMS AND CORESETS FOR l(p) REGRESSION", "paper_id": "WOS:000208048500012"}