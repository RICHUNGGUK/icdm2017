{"auto_keywords": [{"score": 0.02900730847963855, "phrase": "lu"}, {"score": 0.005464096824505604, "phrase": "amd"}, {"score": 0.005212547945841709, "phrase": "qr"}, {"score": 0.005191405097206416, "phrase": "lapack"}, {"score": 0.00481495049065317, "phrase": "lapack_panel_operations"}, {"score": 0.004762893197005439, "phrase": "parallel_cache_assignment"}, {"score": 0.0046801659414917, "phrase": "lq"}, {"score": 0.00456020621016909, "phrase": "block_algorithms"}, {"score": 0.004366106816225457, "phrase": "unblocked_algorithm"}, {"score": 0.0042489946866108895, "phrase": "remainder_matrix"}, {"score": 0.004046020290832207, "phrase": "excellent_scaling"}, {"score": 0.003980531478122155, "phrase": "panel_processing"}, {"score": 0.0037654203767945008, "phrase": "ql"}, {"score": 0.0037493114715625784, "phrase": "bus_speed"}, {"score": 0.003699283174275036, "phrase": "rq"}, {"score": 0.003531474887552889, "phrase": "amdahl's_law"}, {"score": 0.003362660648932247, "phrase": "panel_computation"}, {"score": 0.003290237137651949, "phrase": "dominant_cost"}, {"score": 0.003236941658990869, "phrase": "lapack_routines"}, {"score": 0.0031500213134140953, "phrase": "novel_parallel_cache_assignment_approach"}, {"score": 0.0029029754274913803, "phrase": "general_approach"}, {"score": 0.0023990982005125763, "phrase": "twenty_implementations"}, {"score": 0.0021396960731214203, "phrase": "significant_speedup"}, {"score": 0.0021049977753042253, "phrase": "existing_state"}], "paper_keywords": ["Performance", " Experimentation", " Design", " Algorithms", " ATLAS", " LAPACK", " QR", " QL", " LU", " factorization", " parallel", " multicore"], "paper_abstract": "In LAPACK many matrix operations are cast as block algorithms which iteratively process a panel using an unblocked algorithm and then update a remainder matrix using the high performance Level 3 BLAS. The Level 3 BLAS have excellent scaling, but panel processing tends to be bus bound, and thus scales with bus speed rather than the number of processors (p). Amdahl's law therefore ensures that as p grows, the panel computation will become the dominant cost of these LAPACK routines. Our contribution is a novel parallel cache assignment approach to the panel factorization which we show scales well with p. We apply this general approach to the QR, QL, RQ, LQ and LU panel factorizations. We show results for two commodity platforms: an 8-core Intel platform and a 32-core AMD platform. For both platforms and all twenty implementations (five factorizations each of which is available in 4 types), we present results that demonstrate that our approach yields significant speedup over the existing state of the art.", "paper_title": "Scaling LAPACK Panel Operations Using Parallel Cache Assignment", "paper_id": "WOS:000322130900001"}