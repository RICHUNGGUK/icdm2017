{"auto_keywords": [{"score": 0.03496401466222773, "phrase": "training_sentences"}, {"score": 0.00481495049065317, "phrase": "artificial_neural_network_training"}, {"score": 0.004767261858417823, "phrase": "speech_recognition"}, {"score": 0.004513255056769647, "phrase": "complete_ann_training_procedure"}, {"score": 0.004446349752828045, "phrase": "block_mode"}, {"score": 0.004424268136268051, "phrase": "back-propagation_learning_algorithm"}, {"score": 0.004251500979153001, "phrase": "observation_feature_vectors"}, {"score": 0.004188459638572375, "phrase": "speech_recognition_system"}, {"score": 0.004105850229681625, "phrase": "high_performance_simd_architecture"}, {"score": 0.0040249389271199525, "phrase": "cuda"}, {"score": 0.0037913267098534887, "phrase": "training_procedure"}, {"score": 0.00367967106192446, "phrase": "multi-thread_capabilities"}, {"score": 0.003643185395780667, "phrase": "multi-core_processors"}, {"score": 0.0034660938095219846, "phrase": "training_large_scale_sequential_patterns"}, {"score": 0.0031372369299648203, "phrase": "back-propagation_steps"}, {"score": 0.003044785536429499, "phrase": "huge_amount"}, {"score": 0.002984665318717504, "phrase": "host_memory"}, {"score": 0.0029403531729828574, "phrase": "cpu_card"}, {"score": 0.0028253682745674608, "phrase": "acoustic_models"}, {"score": 0.002797329638474873, "phrase": "large_vocabulary_speech_recognition_tasks"}, {"score": 0.0027420820738468577, "phrase": "six_times_reduction"}, {"score": 0.0026480044868438875, "phrase": "real-world_large_size_networks"}, {"score": 0.0025827837420330816, "phrase": "already_optimized_implementation"}, {"score": 0.002544422946205545, "phrase": "intel_mkl_libraries"}, {"score": 0.0023609780669713288, "phrase": "training_time"}, {"score": 0.0023028105026714533, "phrase": "huge_set"}, {"score": 0.002234925017093293, "phrase": "italian"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Artificial Neural Network", " Block Back-propagation", " Focused Attention Back-Propagation", " GPU", " CUDA", " Fast Training"], "paper_abstract": "In this paper we describe the implementation of a complete ANN training procedure using the block mode back-propagation learning algorithm for sequential patterns - such as the observation feature vectors of a speech recognition system - exploiting the high performance SIMD architecture of CPU using CUDA and its C-like language interface. We also compare the speed-up obtained implementing the training procedure only taking advantage of the multi-thread capabilities of multi-core processors. In our implementation we take into account all the peculiar aspects of training large scale sequential patterns, in particular, the re-segmentation of the training sentences, the block size for the feed-forward and for the back-propagation steps, and the transfer of huge amount of data from host memory to the CPU card. Our approach has been tested by training acoustic models for large vocabulary speech recognition tasks, showing a six times reduction of the time required to train real-world large size networks with respect to an already optimized implementation using the Intel MKL libraries. Thanks to these optimizations and to the support of the CPU, the training time for language having a huge set of training sentences (about one million for Italian) can be reduced from approximately a month to 5 days. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Parallel implementation of Artificial Neural Network training for speech recognition", "paper_id": "WOS:000279834800010"}