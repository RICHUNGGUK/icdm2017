{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "hierarchical_classifiers"}, {"score": 0.04167736417839304, "phrase": "hierarchical_classifier"}, {"score": 0.030479952228764876, "phrase": "loss_function"}, {"score": 0.004761010795871555, "phrase": "hierarchical_classification"}, {"score": 0.00419016653753787, "phrase": "internal_nodes"}, {"score": 0.004035630066743877, "phrase": "straightforward_approach"}, {"score": 0.003886770770835211, "phrase": "baseline_method"}, {"score": 0.0036052633308076933, "phrase": "top-down_evaluation_procedure"}, {"score": 0.003511613254403125, "phrase": "naive_approach"}, {"score": 0.003459191419478298, "phrase": "binary_classifiers"}, {"score": 0.003172551230409948, "phrase": "evaluation_procedure"}, {"score": 0.0030554300179552415, "phrase": "new_decomposition_method"}, {"score": 0.0030097975788892896, "phrase": "node_classifier"}, {"score": 0.0027602925235474317, "phrase": "bottom-up_learning_strategy"}, {"score": 0.0025124343366192954, "phrase": "experimental_results"}, {"score": 0.0024748913336907923, "phrase": "proposed_approach"}, {"score": 0.0024287485834314027, "phrase": "state-of-the-art_hierarchical_algorithms"}, {"score": 0.0023745087648471613, "phrase": "naive_baseline_method"}, {"score": 0.002252602622785975, "phrase": "parallel_implementations"}, {"score": 0.0021775526153883355, "phrase": "available_well-known_techniques"}, {"score": 0.0021530945641993152, "phrase": "binary_classification_svms"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Hierarchical classification", " Multi-label learning", " Structured output classification", " Cost-sensitive learning", " Support vector machines"], "paper_abstract": "In hierarchical classification, classes are arranged in a hierarchy represented by a tree or a forest, and each example is labeled with a set of classes located on paths from roots to leaves or internal nodes. In other words, both multiple and partial paths are allowed. A straightforward approach to learn a hierarchical classifier, usually used as a baseline method, consists in learning one binary classifier for each node of the hierarchy: the hierarchical classifier is then obtained using a top-down evaluation procedure. The main drawback of this naive approach is that these binary classifiers are constructed independently, when it is clear that there are dependencies between them that are motivated by the hierarchy and the evaluation procedure employed. In this paper, we present a new decomposition method in which each node classifier is built taking into account other classifiers, its descendants, and the loss function used to measure the goodness of hierarchical classifiers. Following a bottom-up learning strategy, the idea is to optimize the loss function at every subtree assuming that all classifiers are known except the one at the root. Experimental results show that the proposed approach has accuracies comparable to state-of-the-art hierarchical algorithms and is better than the naive baseline method described above. Moreover, the benefits of our proposal include the possibility of parallel implementations, as well as the use of all available well-known techniques to tune binary classification SVMs. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "A semi-dependent decomposition approach to learn hierarchical classifiers", "paper_id": "WOS:000280983100009"}