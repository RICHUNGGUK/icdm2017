{"auto_keywords": [{"score": 0.03644451033122438, "phrase": "mdm"}, {"score": 0.01050391592757389, "phrase": "minimal_distance_maximization"}, {"score": 0.00481495049065317, "phrase": "metric_learning"}, {"score": 0.004456044928556639, "phrase": "principal_component_analysis"}, {"score": 0.004410409277739647, "phrase": "pca"}, {"score": 0.004342430078306461, "phrase": "linear_discriminant_analysis"}, {"score": 0.003997891221012094, "phrase": "systematic_analysis"}, {"score": 0.003936389070602614, "phrase": "multi-class_ldr_problem"}, {"score": 0.003757480190200645, "phrase": "new_algorithm"}, {"score": 0.0035497797026638033, "phrase": "non-robustness_issue"}, {"score": 0.003336225354214798, "phrase": "-class_distance"}, {"score": 0.0032848684681050745, "phrase": "output_space"}, {"score": 0.003168079471049896, "phrase": "semi-definite_program"}, {"score": 0.003135520118897947, "phrase": "sdp"}, {"score": 0.0030239842475439814, "phrase": "close_connection"}, {"score": 0.0029928611380840757, "phrase": "\"weighted\"_ldr_methods"}, {"score": 0.0028420694298724966, "phrase": "lda"}, {"score": 0.002769395841379498, "phrase": "special_case"}, {"score": 0.0026570998719084153, "phrase": "overlapping_centroids"}, {"score": 0.0025625729433234623, "phrase": "homoscedastic_gaussian_assumption"}, {"score": 0.002445950676329976, "phrase": "non-parametric_way"}, {"score": 0.0023834640793589414, "phrase": "gradient-based_convex_approximation_algorithm"}, {"score": 0.002286781334850213, "phrase": "original_sdp."}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Linear dimensionality reduction (LDR)", " Metric learning", " Convex optimization", " Minimal distance maximization"], "paper_abstract": "Classic linear dimensionality reduction (LDR) methods, such as principal component analysis (PCA) and linear discriminant analysis (LDA), are known not to be robust against outliers. Following a systematic analysis of the multi-class LDR problem in a unified framework, we propose a new algorithm, called minimal distance maximization (MDM), to address the non-robustness issue. The principle behind MDM is to maximize the minimal between-class distance in the output space. MDM is formulated as a semi-definite program (SDP), and its dual problem reveals a close connection to \"weighted\" LDR methods. A soft version of MDM, in which LDA is subsumed as a special case, is also developed to deal with overlapping centroids. Finally, we drop the homoscedastic Gaussian assumption made in MDM by extending it in a non-parametric way, along with a gradient-based convex approximation algorithm to significantly reduce the complexity of the original SDP. The effectiveness of our proposed methods are validated on two UCI datasets and two face datasets. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Distance metric learning by minimal distance maximization", "paper_id": "WOS:000285233300013"}