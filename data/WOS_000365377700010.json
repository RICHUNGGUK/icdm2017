{"auto_keywords": [{"score": 0.04430256680288919, "phrase": "time_complexity"}, {"score": 0.03332367449741115, "phrase": "ldmdba"}, {"score": 0.02322730111336099, "phrase": "multiple_distances"}, {"score": 0.008516372636528206, "phrase": "tree_structures"}, {"score": 0.00803666586636214, "phrase": "different_datasets"}, {"score": 0.004319322238010043, "phrase": "necessary_training"}, {"score": 0.004187284711626886, "phrase": "basic_algorithm"}, {"score": 0.004043541331944951, "phrase": "large_scale_datasets"}, {"score": 0.0036695284042874395, "phrase": "different_complexity"}, {"score": 0.003488874535154583, "phrase": "\"location_difference"}, {"score": 0.003329994649432498, "phrase": "different_data_points"}, {"score": 0.003253279945540651, "phrase": "location_difference"}, {"score": 0.0032031180198375283, "phrase": "nearest_neighbors"}, {"score": 0.002940697344380201, "phrase": "search_tree"}, {"score": 0.002895341091441627, "phrase": "ldmdba_the_only_knn_method"}, {"score": 0.002817639725689939, "phrase": "high_dimensional_data"}, {"score": 0.0026788122684116224, "phrase": "existing_methods"}, {"score": 0.0025766822940260963, "phrase": "data_point"}, {"score": 0.0024118962715000197, "phrase": "query_point"}, {"score": 0.0023746766301040974, "phrase": "different_dimensions"}, {"score": 0.0022841153001549193, "phrase": "real_systems"}, {"score": 0.0022664208122072657, "phrase": "large_scale_databases"}, {"score": 0.002154693685426657, "phrase": "public_and_artificial_datasets"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Location difference of multiple distances", " kappa-nearest neighbors", " Tree structure"], "paper_abstract": "kappa-nearest neighbors (kNN) classifiers are commonly used in various applications due to their relative simplicity and the absence of necessary training. However, the time complexity of the basic algorithm is quadratic, which makes them inappropriate for large scale datasets. At the same time, the performance of most improved algorithms based on tree structures decreases rapidly with increase in dimensionality of dataset, and tree structures have different complexity in different datasets. In this paper, we introduce the concept of \"location difference of multiple distances, and use it to measure the difference between different data points. In this way, location difference of multiple distances based nearest neighbors searching algorithm (LDMDBA) is proposed. LDMDBA has a time complexity of O(logdnlogn) and does not rely on a search tree. This makes LDMDBA the only kNN method that can be efficiently applied to high dimensional data and has very good stability on different datasets. In addition, most of the existing methods have a time complexity of O(n) to predict a data point outside the dataset By contrast, LDMDBA has a time complexity of O(logdlogn) to predict a query point in datasets of different dimensions, and, therefore, can be applied in real systems and large scale databases. The effectiveness and efficiency of LDMDBA are demonstrated in experiments involving public and artificial datasets. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Location difference of multiple distances based kappa-nearest neighbors algorithm", "paper_id": "WOS:000365377700010"}