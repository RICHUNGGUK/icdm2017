{"auto_keywords": [{"score": 0.048799520930917564, "phrase": "broad_range"}, {"score": 0.02705657569724023, "phrase": "small_footprint"}, {"score": 0.00481495049065317, "phrase": "large-scale_neural_networks"}, {"score": 0.004782455247247007, "phrase": "machine-learning_tasks"}, {"score": 0.004499647408293102, "phrase": "embedded_systems"}, {"score": 0.0044692705329574, "phrase": "data_centers"}, {"score": 0.004349791059765796, "phrase": "small_set"}, {"score": 0.004320421465896323, "phrase": "machine-learning_algorithms"}, {"score": 0.0038764861784138117, "phrase": "heterogeneous_multicores"}, {"score": 0.0037346256869699975, "phrase": "machine-learning_accelerator"}, {"score": 0.003684332524184009, "phrase": "rare_combination"}, {"score": 0.0035979378467361762, "phrase": "small_number"}, {"score": 0.00357362651129581, "phrase": "target_algorithms"}, {"score": 0.003362060602812933, "phrase": "computational_part"}, {"score": 0.0030265495849271617, "phrase": "large-scale_cnns"}, {"score": 0.0029655761366079877, "phrase": "special_emphasis"}, {"score": 0.002895986736522535, "phrase": "accelerator_design"}, {"score": 0.0027152006721460347, "phrase": "high_throughput"}, {"score": 0.0026068650963042444, "phrase": "synaptic_weight_multiplications"}, {"score": 0.0025804616931593897, "phrase": "outputs_additions"}, {"score": 0.0023545181737729065, "phrase": "total_energy"}, {"score": 0.0023148964315569866, "phrase": "accelerator_characteristics"}, {"score": 0.0021776870246727233, "phrase": "state-of-the-art_machine-learning_algorithms"}, {"score": 0.0021049977753042253, "phrase": "broad_set"}], "paper_keywords": ["Architecture", " Processor", " Hardware"], "paper_abstract": "Machine-learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve toward heterogeneous multicores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have been focusing on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance, and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm(2) and 485mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.", "paper_title": "A Small-Footprint Accelerator for Large-Scale Neural Networks", "paper_id": "WOS:000356389000003"}