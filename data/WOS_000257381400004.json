{"auto_keywords": [{"score": 0.04641677642078125, "phrase": "different_modalities"}, {"score": 0.015719716506582538, "phrase": "enhanced_multi-modality_ontology"}, {"score": 0.004732997108321232, "phrase": "enhanced_multi-modality_ontology-based_approach"}, {"score": 0.004563427013973105, "phrase": "multimedia_retrieval"}, {"score": 0.00449537439517875, "phrase": "earliest_attempts"}, {"score": 0.00438105507792004, "phrase": "complex_domain"}, {"score": 0.004169972101509753, "phrase": "proper_structure"}, {"score": 0.004003260887655198, "phrase": "matching_degree"}, {"score": 0.003935068327680387, "phrase": "independent_ranking_mechanism"}, {"score": 0.003802135020791801, "phrase": "large_domains"}, {"score": 0.003777712700261133, "phrase": "first_question"}, {"score": 0.003689493736458129, "phrase": "wang_et_al"}, {"score": 0.0036344261627261213, "phrase": "image_retrieval"}, {"score": 0.003603327452846714, "phrase": "asia-pacific_workshop"}, {"score": 0.0035878776692526613, "phrase": "visual_information_processing"}, {"score": 0.0034890505116128606, "phrase": "ontology_help"}, {"score": 0.0034369643346301216, "phrase": "text_ontology"}, {"score": 0.003422225406860504, "phrase": "multi-modality_ontology"}, {"score": 0.00340023503721827, "phrase": "acm_press"}, {"score": 0.0033856530826561412, "phrase": "new_york"}, {"score": 0.0032223742018434856, "phrase": "main_focus"}, {"score": 0.0031606404966938568, "phrase": "new_ranking_mechanism"}, {"score": 0.00314708280192844, "phrase": "spearman's_ranking_correlation"}, {"score": 0.002956888045965422, "phrase": "second_question"}, {"score": 0.002937879003671346, "phrase": "semantic_matchmaking_result"}, {"score": 0.0028446460307503343, "phrase": "third_question"}, {"score": 0.0027961342030212353, "phrase": "scalability_issue"}, {"score": 0.002778155724601111, "phrase": "concept_similarity"}, {"score": 0.002742543864807522, "phrase": "different_ontologies"}, {"score": 0.0026214496600537986, "phrase": "right_step"}, {"score": 0.0025711986702188754, "phrase": "animal_kingdom"}, {"score": 0.0024842242921857705, "phrase": "wide_range"}, {"score": 0.0024105326885334962, "phrase": "data_sets"}, {"score": 0.0023796219690393632, "phrase": "ground_truth"}, {"score": 0.002354165323328269, "phrase": "image_content"}, {"score": 0.0023440589015485077, "phrase": "text_information"}, {"score": 0.0022991130024277765, "phrase": "retrieval_results"}, {"score": 0.002255026965525861, "phrase": "close_animal_species"}, {"score": 0.0022405198609939812, "phrase": "similar_appearances"}, {"score": 0.0022022878843247257, "phrase": "canine_family_graph"}, {"score": 0.002164706878371129, "phrase": "semantic_relationships"}, {"score": 0.0021507795710951384, "phrase": "unequivocal_evidence"}, {"score": 0.002132348817167717, "phrase": "good_accuracy"}, {"score": 0.0021186292534026655, "phrase": "comparable_result"}, {"score": 0.0021049977753042253, "phrase": "google_re-ranking_result"}], "paper_keywords": ["ontology", " web image retrieval", " Spearman's rank correlation coefficient"], "paper_abstract": "In this paper we present an enhanced multi-modality ontology-based approach for web image retrieval step by step. Several ontology-based approaches have been made in the field of multimedia retrieval. Our multi-modality approach is one of the earliest attempts to integrate information from different modalities and apply the model in a complex domain. In order to develop the model, we need to answer the following questions: (1) how to find the proper structure and construct an ontology which can integrate information from different modalities; (2) how to quantify the matching degree (concept similarity) and provide an independent ranking mechanism; (3) how to ensure the scalability of this approach when applied to large domains. The first question has been answered by our multi-modality ontology which has been discussed in Wang et al. (Does ontology help in image retrieval? In: Asia-Pacific workshop on visual information processing, 2006) and its extension (Wang et al., Does ontology help in image retrieval?-a comparison between keyword, text ontology and multi-modality ontology approaches, ACM Press, New York, NY, USA, pp 109-112, 2006). More details about this work is given later. The main focus of this paper is that we propose a new ranking mechanism using Spearman's ranking correlation to measure the similarity of concepts in the ontology. We take the priorities of information from different modalities into consideration. This algorithm gives the answer of the second question. The semantic matchmaking result is quantized and the degree of similarity between concepts is calculated. For the third question, importing of ontology will resolve the scalability issue but computing concept similarity and identify relationships when integrating different ontologies will be beyond the scope of this paper. To convince readers that our multi-modality ontology and concept similarity ranking is the right step forward, we decided to work on the animal kingdom. We believe this domain is challenging as demonstrated by images depict animals in a wide range of aspects, pose, configurations and appearances. We experimented with a data sets of 4,000 web images. Based on ground truth, we analyze the image content and text information, build up the enhanced multi-modality ontology and compare the retrieval results. Results show that we can even classify close animal species which share similar appearances and we can infer their hidden relationships from the canine family graph. By assigning a ranking to the semantic relationships we show unequivocal evidence that our improved model achieves good accuracy and performs comparable result with the Google re-ranking result in our previous work.", "paper_title": "Image retrieval++ - web image retrieval with an enhanced multi-modality ontology", "paper_id": "WOS:000257381400004"}