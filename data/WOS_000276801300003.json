{"auto_keywords": [{"score": 0.049631680892871025, "phrase": "transfer_learning"}, {"score": 0.04317179544664411, "phrase": "domain_adaptation"}, {"score": 0.00481495049065317, "phrase": "world_wide_knowledge"}, {"score": 0.00470325458481566, "phrase": "major_problem"}, {"score": 0.0046593017130868, "phrase": "classification_learning"}, {"score": 0.004342430078306461, "phrase": "new_data_instances"}, {"score": 0.0039716833701918365, "phrase": "target_domain_data"}, {"score": 0.00375395061615019, "phrase": "different_distributions"}, {"score": 0.00351491577968631, "phrase": "source_and_target_domains"}, {"score": 0.0033377940531761985, "phrase": "novel_transfer_learning_approach"}, {"score": 0.0031995142224983094, "phrase": "useful_knowledge"}, {"score": 0.003154701855951328, "phrase": "worldwide_knowledge_base"}, {"score": 0.0029260602766930065, "phrase": "classification_performance"}, {"score": 0.002650857539177183, "phrase": "auxiliary_source_data"}, {"score": 0.0025290375256888883, "phrase": "cross-domain_text_classification_problems"}, {"score": 0.0024014759148123736, "phrase": "major_contribution"}, {"score": 0.002301896758256575, "phrase": "large_amount"}, {"score": 0.002280333649167796, "phrase": "worldwide_knowledge"}, {"score": 0.0021652891317252994, "phrase": "target_domain"}], "paper_keywords": ["Data mining", " transfer learning", " cross-domain", " text classification", " Wikipedia"], "paper_abstract": "A major problem of classification learning is the lack of ground-truth labeled data. It is usually expensive to label new data instances for training a model. To solve this problem, domain adaptation in transfer learning has been proposed to classify target domain data by using some other source domain data, even when the data may have different distributions. However, domain adaptation may not work well when the differences between the source and target domains are large. In this paper, we design a novel transfer learning approach, called BIG (Bridging Information Gap), to effectively extract useful knowledge in a worldwide knowledge base, which is then used to link the source and target domains for improving the classification performance. BIG works when the source and target domains share the same feature space but different underlying data distributions. Using the auxiliary source data, we can extract a \"bridge\" that allows cross-domain text classification problems to be solved using standard semisupervised learning algorithms. A major contribution of our work is that with BIG, a large amount of worldwide knowledge can be easily adapted and used for learning in the target domain. We conduct experiments on several real-world cross-domain text classification tasks and demonstrate that our proposed approach can outperform several existing domain adaptation approaches significantly.", "paper_title": "Bridging Domains Using World Wide Knowledge for Transfer Learning", "paper_id": "WOS:000276801300003"}