{"auto_keywords": [{"score": 0.04630542274767929, "phrase": "svm"}, {"score": 0.04474664550820245, "phrase": "stray_examples"}, {"score": 0.012366063260564759, "phrase": "filtering_knn_emphasizer_stage"}, {"score": 0.01043492251916793, "phrase": "emphasized_weights"}, {"score": 0.00481495049065317, "phrase": "loss_regularized_svm"}, {"score": 0.004663074596384574, "phrase": "new_model"}, {"score": 0.004580769304731227, "phrase": "non-parametric_k-nearest-neighbor"}, {"score": 0.004467965784278827, "phrase": "underlying_support_vector_machine"}, {"score": 0.004342430078306461, "phrase": "meaningful_training_examples"}, {"score": 0.004205395707787349, "phrase": "different_class_labels"}, {"score": 0.004087207102190335, "phrase": "heavier_penalty"}, {"score": 0.004043744293248725, "phrase": "stray_example"}, {"score": 0.003986509098918949, "phrase": "stricter_loss_function"}, {"score": 0.0038744481564444173, "phrase": "shelter_stray_examples"}, {"score": 0.0037387746981660717, "phrase": "classical_classification_stage"}, {"score": 0.003569451476205552, "phrase": "training_examples"}, {"score": 0.0035189057079817285, "phrase": "arbitrary_weights"}, {"score": 0.003432160899769594, "phrase": "underlying_svm"}, {"score": 0.0034077704063304208, "phrase": "parameterized_real-valued_class_labels"}, {"score": 0.003150612723641925, "phrase": "heavier_penalties"}, {"score": 0.003083899825125453, "phrase": "quadratic_programming"}, {"score": 0.0029971349193857093, "phrase": "resultant_decision_function"}, {"score": 0.002965229277927781, "phrase": "higher_training_accuracy"}, {"score": 0.0029336622847544857, "phrase": "novel_idea"}, {"score": 0.002912803994461996, "phrase": "real-valued_class_labels"}, {"score": 0.0028409576481842457, "phrase": "effective_way"}, {"score": 0.0027316100642958544, "phrase": "additional_information"}, {"score": 0.0026737455737636867, "phrase": "knn_preprocessor"}, {"score": 0.0026452734009203764, "phrase": "filtering_stage"}, {"score": 0.0025434384320561403, "phrase": "classification_stage"}, {"score": 0.002445514186654732, "phrase": "knn_method"}, {"score": 0.0023766673143502384, "phrase": "regular_examples"}, {"score": 0.0023180144105895257, "phrase": "input_space"}, {"score": 0.0022527488455744674, "phrase": "simulated_application"}, {"score": 0.0022050057829960694, "phrase": "corresponding_properties"}], "paper_keywords": ["k-nearest-neighbor preprocessor", " Stray training examples", " Support vector machines", " Classification", " Pattern recognition"], "paper_abstract": "This paper presents a new model developed by merging a non-parametric k-nearest-neighbor (kNN) preprocessor into an underlying support vector machine (SVM) to provide shelters for meaningful training examples, especially for stray examples scattered around their counterpart examples with different class labels. Motivated by the method of adding heavier penalty to the stray example to attain a stricter loss function for optimization, the model acts to shelter stray examples. The model consists of a filtering kNN emphasizer stage and a classical classification stage. First, the filtering kNN emphasizer stage was employed to collect information from the training examples and to produce arbitrary weights for stray examples. Then, an underlying SVM with parameterized real-valued class labels was employed to carry those weights, representing various emphasized levels of the examples, in the classification. The emphasized weights given as heavier penalties changed the regularization in the quadratic programming of the SVM, and brought the resultant decision function into a higher training accuracy. The novel idea of real-valued class labels for conveying the emphasized weights provides an effective way to pursue the solution of the classification inspired by the additional information. The adoption of the kNN preprocessor as a filtering stage is effective since it is independent of SVM in the classification stage. Due to its property of estimating density locally, the kNN method has the advantage of distinguishing stray examples from regular examples by merely considering their circumstances in the input space. In this paper, detailed experimental results and a simulated application are given to address the corresponding properties. The results show that the model is promising in terms of its original expectations.", "paper_title": "Stray Example Sheltering by Loss Regularized SVM and kNN Preprocessor", "paper_id": "WOS:000263384200002"}