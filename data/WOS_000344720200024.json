{"auto_keywords": [{"score": 0.03134569565764058, "phrase": "touch_saliency"}, {"score": 0.015370721066323683, "phrase": "visual_attention_study"}, {"score": 0.008592890452166317, "phrase": "visual_saliency"}, {"score": 0.00481495049065317, "phrase": "alternative_ground_truth"}, {"score": 0.004749824776508639, "phrase": "eye_fixation_map"}, {"score": 0.004417012437860011, "phrase": "recorded_data"}, {"score": 0.004377077848734575, "phrase": "users'_daily_browsing_behavior"}, {"score": 0.00429828390000229, "phrase": "smart_phone_devices"}, {"score": 0.004259418153299952, "phrase": "touch_screens"}, {"score": 0.004201774694043468, "phrase": "touch_saliency_data"}, {"score": 0.0040334640350288, "phrase": "limited_screen_size"}, {"score": 0.003996983218174335, "phrase": "smart_phone_users"}, {"score": 0.0033630629374009607, "phrase": "touch_screen_fixation_maps"}, {"score": 0.003257727050827128, "phrase": "comprehensive_comparisons"}, {"score": 0.0028813415821988156, "phrase": "eye_fixations"}, {"score": 0.002765776659294382, "phrase": "data_collection"}, {"score": 0.002536763634539386, "phrase": "unified_saliency_prediction_model"}, {"score": 0.002502374576203785, "phrase": "visual_and_touch_saliency_detection"}, {"score": 0.0024460897937643032, "phrase": "middle-level_object_category_features"}, {"score": 0.0024129269611844794, "phrase": "pre-segmented_image_superpixels"}, {"score": 0.0023586493263143553, "phrase": "recently_proposed_multitask_sparsity_pursuit"}, {"score": 0.0022951216874671494, "phrase": "saliency_prediction"}, {"score": 0.0022743274702883456, "phrase": "extensive_evaluations"}, {"score": 0.002233301265743633, "phrase": "proposed_middle-level_category_features"}, {"score": 0.0021830553825770097, "phrase": "saliency_prediction_performance"}, {"score": 0.0021049977753042253, "phrase": "ground_truth"}], "paper_keywords": ["Fixations", " middle-level object category features", " touch saliency", " visual saliency"], "paper_abstract": "In this work, we propose an alternative ground truth to the eye fixation map in visual attention study, called touch saliency. As it can be directly collected from the recorded data of users' daily browsing behavior on widely used smart phone devices with touch screens, the touch saliency data is easy to obtain. Due to the limited screen size, smart phone users usually move and zoom in the images, and fix the region of interest on the screen when browsing images. Our studies are two-fold. First, we collect and study the characteristics of these touch screen fixation maps (named touch saliency) by comprehensive comparisons with their counterpart, the eye-fixation maps (namely, visual saliency). The comparisons show that the touch saliency is highly correlated with the eye fixations for the same stimuli, which indicates its utility in data collection for visual attention study. Based on the consistency between both touch saliency and visual saliency, our second task is to propose a unified saliency prediction model for both visual and touch saliency detection. This model utilizes middle-level object category features extracted from pre-segmented image superpixels as input to the recently proposed multitask sparsity pursuit (MTSP) framework for saliency prediction. Extensive evaluations show that the proposed middle-level category features can considerably improve the saliency prediction performance when taking both touch saliency and visual saliency as ground truth.", "paper_title": "Touch Saliency: Characteristics and Prediction", "paper_id": "WOS:000344720200024"}