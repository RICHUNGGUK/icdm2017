{"auto_keywords": [{"score": 0.03644371376157578, "phrase": "objective_function"}, {"score": 0.028340701344151276, "phrase": "auc"}, {"score": 0.00481495049065317, "phrase": "novel_maximum-margin"}, {"score": 0.004769050269566434, "phrase": "supervised_neural_networks"}, {"score": 0.004622860971591553, "phrase": "backpropagation_approach"}, {"score": 0.004589763483242731, "phrase": "third_one"}, {"score": 0.004556901872357751, "phrase": "information_theory"}, {"score": 0.004535124351828787, "phrase": "multilayer_perceptron"}, {"score": 0.004449045430179066, "phrase": "backpropagation_methods"}, {"score": 0.004375778545813459, "phrase": "mm"}, {"score": 0.004322967486451877, "phrase": "first_one"}, {"score": 0.004271490815763164, "phrase": "gradient_descent"}, {"score": 0.00425107145514943, "phrase": "adaptive_learning_rate_algorithm"}, {"score": 0.004230750549613182, "phrase": "gdx"}, {"score": 0.004190394726263161, "phrase": "maximum-margin_gdx"}, {"score": 0.004081385289516914, "phrase": "mlp_output-layer_hyperplane"}, {"score": 0.004003883922210733, "phrase": "mlp_layers"}, {"score": 0.003975200317233532, "phrase": "single_process"}, {"score": 0.003909065355459264, "phrase": "mm-based_objective_function"}, {"score": 0.003853251284913932, "phrase": "hidden_layers"}, {"score": 0.003789137457228946, "phrase": "hidden-layer_space"}, {"score": 0.003752979284571915, "phrase": "higher_margin"}, {"score": 0.0037260864222258506, "phrase": "output-layer_hyperplane"}, {"score": 0.003611751858608048, "phrase": "support_vector_machine"}, {"score": 0.003551641986820724, "phrase": "proposed_mm-based_objective_function"}, {"score": 0.003321008590273796, "phrase": "support_vectors"}, {"score": 0.003234542008677341, "phrase": "constrained_optimization_problem"}, {"score": 0.0032036590920314725, "phrase": "svm_training"}, {"score": 0.0030756523984820624, "phrase": "usual_svm_training_methods"}, {"score": 0.00296694687539797, "phrase": "training-data-set_size"}, {"score": 0.002903571161455954, "phrase": "interclass_interference"}, {"score": 0.0028279422436488116, "phrase": "fisher_discriminant_analysis"}, {"score": 0.002780840796557151, "phrase": "mlp_hidden_output"}, {"score": 0.002741091312632307, "phrase": "desirable_statistical_distribution"}, {"score": 0.0027149069559111207, "phrase": "training_methods"}, {"score": 0.0026954325211617195, "phrase": "maximum_area"}, {"score": 0.0026825270249932055, "phrase": "roc_curve"}, {"score": 0.002612640527241776, "phrase": "third_approach"}, {"score": 0.0025938977569302177, "phrase": "robust_training_framework"}, {"score": 0.002544570101090828, "phrase": "proposed_training_method"}, {"score": 0.0025263144229614944, "phrase": "main_idea"}, {"score": 0.00249617815217504, "phrase": "neural_model"}, {"score": 0.0024194922606441674, "phrase": "mici"}, {"score": 0.00240790403555452, "phrase": "mmgdx"}, {"score": 0.0023906264998772428, "phrase": "levenberg-marquard"}, {"score": 0.0023451553255837317, "phrase": "resulting_neural_network"}, {"score": 0.00232832698253493, "phrase": "assembled_neural_network"}, {"score": 0.002300547046275396, "phrase": "benchmark_data_sets"}, {"score": 0.0022895278869126848, "phrase": "real-world_problems"}, {"score": 0.0021049977753042253, "phrase": "balanced_error_rate"}], "paper_keywords": ["Information theory", " maximal-margin (MM) principle", " multilayer perceptron (MLP)", " pattern recognition", " supervised learning"], "paper_abstract": "This paper proposes three novel training methods, two of them based on the backpropagation approach and a third one based on information theory for multilayer perceptron (MLP) binary classifiers. Both backpropagation methods are based on the maximal-margin (MM) principle. The first one, based on the gradient descent with adaptive learning rate algorithm (GDX) and named maximum-margin GDX (MMGDX), directly increases the margin of the MLP output-layer hyperplane. The proposed method jointly optimizes both MLP layers in a single process, backpropagating the gradient of an MM-based objective function, through the output and hidden layers, in order to create a hidden-layer space that enables a higher margin for the output-layer hyperplane, avoiding the testing of many arbitrary kernels, as occurs in case of support vector machine (SVM) training. The proposed MM-based objective function aims to stretch out the margin to its limit. An objective function based on-norm is also proposed in order to take into account the idea of support vectors, however, overcoming the complexity involved in solving a constrained optimization problem, usually in SVM training. In fact, all the training methods proposed in this paper have time and space complexities O(N) while usual SVM training methods have time complexity O(N(3)) and space complexity O(N(2)), where is the training-data-set size. The second approach, named minimization of interclass interference (MICI), has an objective function inspired on the Fisher discriminant analysis. Such algorithm aims to create an MLP hidden output where the patterns have a desirable statistical distribution. In both training methods, the maximum area under ROC curve (AUC) is applied as stop criterion. The third approach offers a robust training framework able to take the best of each proposed training method. The main idea is to compose a neural model by using neurons extracted from three other neural networks, each one previously trained by MICI, MMGDX, and Levenberg-Marquard (LM), respectively. The resulting neural network was named assembled neural network (ASNN). Benchmark data sets of real-world problems have been used in experiments that enable a comparison with other state-of-the-art classifiers. The results provide evidence of the effectiveness of our methods regarding accuracy, AUC, and balanced error rate.", "paper_title": "Novel Maximum-Margin Training Algorithms for Supervised Neural Networks", "paper_id": "WOS:000278537600010"}