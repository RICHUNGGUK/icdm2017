{"auto_keywords": [{"score": 0.039533171561611936, "phrase": "eligibility_traces"}, {"score": 0.00481495049065317, "phrase": "experimental_analysis"}, {"score": 0.004621279527317074, "phrase": "different_eligibility"}, {"score": 0.004527371799277529, "phrase": "temporal_difference_learning"}, {"score": 0.0043452175611323335, "phrase": "reinforcement_learning_problems"}, {"score": 0.004278807087784626, "phrase": "temporal_difference_technique"}, {"score": 0.00421340730888561, "phrase": "state_value"}, {"score": 0.0041703614123283165, "phrase": "state-action_value"}, {"score": 0.0040438338011467845, "phrase": "dynamic_programming"}, {"score": 0.003802135020791801, "phrase": "monte_carlo_approach"}, {"score": 0.0034308688638878286, "phrase": "learning_process"}, {"score": 0.003292685392413911, "phrase": "underlying_mechanism"}, {"score": 0.0031277316566463978, "phrase": "off-policy_learning_algorithms"}, {"score": 0.0030171910207671205, "phrase": "performance_metrics"}, {"score": 0.002910545717767559, "phrase": "learning_problem"}, {"score": 0.0028659990472580154, "phrase": "simulation_environment"}, {"score": 0.002793260314529583, "phrase": "different_learning_algorithms"}, {"score": 0.002468938912850425, "phrase": "different_parameter_values"}, {"score": 0.002393907896783117, "phrase": "comparative_study"}, {"score": 0.0021049977753042253, "phrase": "different_approaches"}], "paper_keywords": ["Temporal difference learning", " eligibility traces", " agent", " decision-making"], "paper_abstract": "Temporal difference learning and eligibility traces are two mechanisms for solving reinforcement learning problems. The temporal difference technique bootstraps the state value or state-action value at every step as with dynamic programming, and learns by sampling episodes from experience as in the Monte Carlo approach. Eligibility traces is a mechanism that offers a means for recording the degree of which state is eligible for undergoing learning process. This paper aims to investigate the underlying mechanism of eligibility traces strategies using on-policy and off-policy learning algorithms. In doing so, the performance metrics can be obtained by defining the learning problem in a simulation environment, in conjunction with different learning algorithms. However, measuring learning performance and analysing sensibility are very expensive because such performance metrics can only be obtained by running an experiment with different parameter values. This paper proposes a comparative study for analysing the mechanism of eligibility traces. The objective of this paper is to compare and investigate the influences on performance caused by those different approaches.", "paper_title": "Experimental analysis on Sarsa(lambda) and Q(lambda) with different eligibility traces strategies", "paper_id": "WOS:000264443700007"}