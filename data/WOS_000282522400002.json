{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "individual_classification_decisions"}, {"score": 0.004546970518808189, "phrase": "modern_tools"}, {"score": 0.00447316895126645, "phrase": "machine_learning"}, {"score": 0.004293840838533456, "phrase": "black_box"}, {"score": 0.003956379470617348, "phrase": "unseen_data"}, {"score": 0.0030692309090066166, "phrase": "particular_label"}, {"score": 0.002994701080666466, "phrase": "single_instance"}, {"score": 0.0028046593475268174, "phrase": "particular_instance"}, {"score": 0.002541910967437926, "phrase": "decision_trees"}, {"score": 0.0021049977753042253, "phrase": "classification_method"}], "paper_keywords": ["explaining", " nonlinear", " black box model", " kernel methods", " Ames mutagenicity"], "paper_abstract": "After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.", "paper_title": "How to Explain Individual Classification Decisions", "paper_id": "WOS:000282522400002"}