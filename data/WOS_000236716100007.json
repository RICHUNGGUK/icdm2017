{"auto_keywords": [{"score": 0.032604320386754426, "phrase": "mcmc"}, {"score": 0.00481495049065317, "phrase": "hyperdynamic_sampling"}, {"score": 0.004767546015485477, "phrase": "sequential_random_sampling"}, {"score": 0.004697308822154191, "phrase": "markov_chain_monte-carlo"}, {"score": 0.004582528208024817, "phrase": "popular_strategy"}, {"score": 0.004492717291734985, "phrase": "multi-modal_distributions"}, {"score": 0.004448471107649881, "phrase": "high-dimensional_parameter_spaces"}, {"score": 0.0037222908218253054, "phrase": "good_minima"}, {"score": 0.0033214855341753544, "phrase": "long_periods"}, {"score": 0.0032887360307856635, "phrase": "unrepresentative_local_minima"}, {"score": 0.00320826350799124, "phrase": "biased_or_highly_variable_estimates"}, {"score": 0.0031297538950335233, "phrase": "general_strategy"}, {"score": 0.003023047748772801, "phrase": "voter's_'hyperdynamic_sampling"}, {"score": 0.0029199689963238726, "phrase": "local_gradient"}, {"score": 0.0028484940955893134, "phrase": "input_distribution"}, {"score": 0.0027650234530228897, "phrase": "adaptive_importance_sampler"}, {"score": 0.0026973310437053573, "phrase": "negative_curvature_regions"}, {"score": 0.002467038647952607, "phrase": "adjacent_cost_basins"}, {"score": 0.002394719113232821, "phrase": "inter-basin_transition_rates"}, {"score": 0.0023476849595393872, "phrase": "correct_relative_transition_probabilities"}, {"score": 0.002278856202878812, "phrase": "difficult_problem"}, {"score": 0.0021902063413831545, "phrase": "monocular_images"}, {"score": 0.002168586864007222, "phrase": "significantly_enhanced_minimum_exploration"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["hyperdynamics", " Markov chain Monte-Carlo", " importance sampling", " global-optimization", " human tracking"], "paper_abstract": "Sequential random sampling ('Markov Chain Monte-Carlo') is a popular strategy for many vision problems involving multi-modal distributions over high-dimensional parameter spaces. It applies both to importance sampling (where one wants to sample points according to their 'importance' for some calculation, but otherwise fairly) and to global-optimization (where one wants to find good minima, or at least good starting points for local minimization, regardless of fairness). Unfortunately, most sequential samplers are very prone to becoming trapped for long periods in unrepresentative local minima, which leads to biased or highly variable estimates. We present a general strategy for reducing MCMC trapping that generalizes Voter's 'hyperdynamic sampling' from computational chemistry. The local gradient and curvature of the input distribution are used to construct an adaptive importance sampler that focuses samples on negative curvature regions that are likely to contain low cost transition states' (codimension-1 saddle points representing 'mountain passes' connecting adjacent cost basins). This substantially accelerates inter-basin transition rates while still preserving correct relative transition probabilities. Experimental tests on the difficult problem of 3D articulated human pose estimation from monocular images show significantly enhanced minimum exploration. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Fast mixing hyperdynamic sampling", "paper_id": "WOS:000236716100007"}