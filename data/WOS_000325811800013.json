{"auto_keywords": [{"score": 0.042885425509035194, "phrase": "visual_continuity"}, {"score": 0.03434145117173228, "phrase": "sts."}, {"score": 0.00481495049065317, "phrase": "motion-tolerant_contextual_visual_saliency"}, {"score": 0.0047135086583390585, "phrase": "state_of_the_art_video_resizing_methods"}, {"score": 0.004497762477097628, "phrase": "significant_motion"}, {"score": 0.004255431326383001, "phrase": "consecutive_video_frames"}, {"score": 0.0037928847801943404, "phrase": "novel_approach"}, {"score": 0.0037446875860947916, "phrase": "visual_dynamics"}, {"score": 0.003697100573612401, "phrase": "spatiotemporal_slices"}, {"score": 0.0035883942463601688, "phrase": "rich_visual_patterns"}, {"score": 0.003542786326740931, "phrase": "large_temporal_scale"}, {"score": 0.0034828730516958807, "phrase": "patch-based_visual_patterns"}, {"score": 0.0033517342967390065, "phrase": "automatically_specified_spatiotemporal_extent"}, {"score": 0.0032950412841674026, "phrase": "contextual_information"}, {"score": 0.003104038398386081, "phrase": "video_frame"}, {"score": 0.0030645676308053444, "phrase": "eventually_an_importance_map"}, {"score": 0.0029617414861784525, "phrase": "video_clip"}, {"score": 0.0027311076665349657, "phrase": "multi-cue_approach"}, {"score": 0.0026620655619396263, "phrase": "mesh-based_non-homogeneous_warping_operation"}, {"score": 0.002550842302473891, "phrase": "performance_evaluation"}, {"score": 0.0024969812322112174, "phrase": "novel_measure"}, {"score": 0.002465210678474529, "phrase": "patch-based_kullback-leibler_divergence"}, {"score": 0.0022926465161639633, "phrase": "proposed_video_resizing_approach"}, {"score": 0.0022251433045184454, "phrase": "sts-based_approach"}, {"score": 0.0021968238865548812, "phrase": "retargeted_videos"}, {"score": 0.002123051344001408, "phrase": "continuous_dynamics"}, {"score": 0.0021049977753042253, "phrase": "visual_perception"}], "paper_keywords": ["Video signal processing", " image motion analysis", " image quality"], "paper_abstract": "State of the art video resizing methods usually produce perceivable visual discontinuities, especially in videos containing significant motion. To resolve the problem, contextual information about the focus of interest in consecutive video frames should be considered in order to preserve the visual continuity. In this paper, to detect the focus of interest with motion-tolerance, we propose a novel approach for modelling visual dynamics based on spatiotemporal slices (STS), which provide rich visual patterns along a large temporal scale. First, patch-based visual patterns are computed to generate a codebook of the automatically specified spatiotemporal extent determined by the contextual information in the STS. The codebook is then used to compute its associated response in each video frame, and eventually an importance map covering the focus of interest in a video clip can be obtained. To preserve the visual continuity of the content, particularly an important area, a multi-cue approach is used to guide a mesh-based non-homogeneous warping operation constrained by the trajectories in the STS. For the performance evaluation, we present a novel measure that utilizes patch-based Kullback-Leibler divergence (KL-divergence) to gauge the deformation of the focus of interest under the proposed video resizing approach. Experimental results show that the STS-based approach can generate retargeted videos effectively, while maintaining their isotropic manipulation and the continuous dynamics of visual perception.", "paper_title": "Preserving Motion-Tolerant Contextual Visual Saliency for Video Resizing", "paper_id": "WOS:000325811800013"}