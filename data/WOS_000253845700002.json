{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "feature_selection"}, {"score": 0.046093248534382916, "phrase": "supervision_information"}, {"score": 0.015561929573070624, "phrase": "pairwise_constraints"}, {"score": 0.004646088032529303, "phrase": "important_preprocessing_step"}, {"score": 0.00457553517865013, "phrase": "high-dimensional_data"}, {"score": 0.004460309246062286, "phrase": "feature_selection_methods"}, {"score": 0.0043258453436258405, "phrase": "unsupervised_ones"}, {"score": 0.004152818376630929, "phrase": "nearly_all_existing_supervised_feature_selection_methods"}, {"score": 0.0036928221419965253, "phrase": "i.e._pairwise_constraints"}, {"score": 0.003545024951402821, "phrase": "data_samples"}, {"score": 0.0029951555610569225, "phrase": "class_labels"}, {"score": 0.0028459677232555176, "phrase": "feature_selection_research"}, {"score": 0.0027041907335656782, "phrase": "constraint_score"}, {"score": 0.002622528664187849, "phrase": "well-known_fisher_score_and_laplacian_score_algorithms"}, {"score": 0.002453934333239467, "phrase": "experimental_results"}, {"score": 0.0023079225317714815, "phrase": "even_higher_performance"}, {"score": 0.002284444272376043, "phrase": "fisher_score"}, {"score": 0.002261204313587616, "phrase": "full_class_labels"}, {"score": 0.0022267859286766553, "phrase": "whole_training_data"}, {"score": 0.0021705796895687864, "phrase": "laplacian_score"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["feature selection", " pairwise constraints", " filter method", " constraint score", " fisher score", " Laplacian Score"], "paper_abstract": "Feature selection is an important preprocessing step in mining high-dimensional data. Generally, supervised feature selection methods with supervision information are superior to unsupervised ones without supervision information. In the literature, nearly all existing supervised feature selection methods use class labels as supervision information. In this paper, we propose to use another form of supervision information for feature selection, i.e. pairwise constraints, which specifies whether a pair of data samples belong to the same class (must-link constraints) or different classes (cannot-link constraints). Pairwise constraints arise naturally in many tasks and are more practical and inexpensive than class labels. This topic has not yet been addressed in feature selection research. We call our pairwise constraints guided feature selection algorithm as Constraint Score and compare it with the well-known Fisher Score and Laplacian Score algorithms. Experiments are carried out on several high-dimensional UCI and face data sets. Experimental results show that, with very few pairwise constraints, Constraint Score achieves similar or even higher performance than Fisher Score with full class labels on the whole training data, and significantly outperforms Laplacian Score. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Constraint Score: A new filter method for feature selection with pairwise constraints", "paper_id": "WOS:000253845700002"}