{"auto_keywords": [{"score": 0.04266671539648594, "phrase": "ga_line"}, {"score": 0.037677358159743844, "phrase": "state_features"}, {"score": 0.00481495049065317, "phrase": "multiple-load_carrier_scheduling_problem"}, {"score": 0.00459039896577789, "phrase": "multiple-load_carrier"}, {"score": 0.004418287966422041, "phrase": "line-side_buffers"}, {"score": 0.004355415885815204, "phrase": "general_assembly"}, {"score": 0.004314506969762921, "phrase": "ga"}, {"score": 0.003920780681351006, "phrase": "material_handling_distance"}, {"score": 0.0038465304237380125, "phrase": "scheduling_criteria"}, {"score": 0.0037556843869894566, "phrase": "scheduling_problem"}, {"score": 0.003702206069379459, "phrase": "reinforcement_learning"}, {"score": 0.003667064895478173, "phrase": "rl"}, {"score": 0.00349577373145384, "phrase": "reward_function"}, {"score": 0.003161740194257218, "phrase": "look-ahead_horizon"}, {"score": 0.002942811795757911, "phrase": "traditional_material"}, {"score": 0.0029147869733574844, "phrase": "request_generating_policy"}, {"score": 0.002845883666777154, "phrase": "look-ahead_based_request"}, {"score": 0.002778604641920052, "phrase": "material_handling_requests"}, {"score": 0.002687070469080394, "phrase": "current_buffer_information"}, {"score": 0.00263612191603326, "phrase": "future_part_requirement_information"}, {"score": 0.002549269480908849, "phrase": "heuristic_dispatching_algorithm"}, {"score": 0.0024534993283766332, "phrase": "future_requests"}, {"score": 0.002406968743305622, "phrase": "existing_ones"}, {"score": 0.0022725931637071852, "phrase": "simulation_experiments"}, {"score": 0.0022294857939412073, "phrase": "proposed_approach"}, {"score": 0.002187194307878705, "phrase": "numerical_results"}, {"score": 0.0021049977753042253, "phrase": "rl_approach"}], "paper_keywords": ["Materials handling", " Multi-criteria decision making", " Reinforcement learning", " Multiple-load carrier scheduling", " Look-ahead scheduling"], "paper_abstract": "This paper studies the problem of scheduling a multiple-load carrier which is used to deliver parts to line-side buffers of a general assembly (GA) line. In order to maximize the reward of the GA line, both the throughput of the GA line and the material handling distance are considered as scheduling criteria. After formulating the scheduling problem as a reinforcement learning (RL) problem by defining state features, actions and the reward function, we develop a Q() RL algorithm based scheduling approach. To improve performance, forecasted information such as quantities of parts required in a look-ahead horizon is used when we define state features and actions in formulation. Other than applying traditional material handling request generating policy, we use a look-ahead based request generating policy with which material handling requests are generated based not only on current buffer information but also on future part requirement information. Moreover, by utilizing a heuristic dispatching algorithm, the approach is able to handle future requests as well as existing ones. To evaluate the performance of the approach, we conduct simulation experiments to compare the proposed approach with other approaches. Numerical results demonstrate that the policies obtained by the RL approach outperform other approaches.", "paper_title": "A reinforcement learning based approach for a multiple-load carrier scheduling problem", "paper_id": "WOS:000365013800013"}