{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "bayesian_learning_of_noisy_markov_decision_processes"}, {"score": 0.0046390482426405324, "phrase": "inverse_reinforcement_learning_problem"}, {"score": 0.0038510053729959074, "phrase": "statistical_model"}, {"score": 0.0035744997458371335, "phrase": "markov_decision_process"}, {"score": 0.0034695189961509625, "phrase": "bayesian_approach"}, {"score": 0.0028795712616923462, "phrase": "unified_framework"}, {"score": 0.002815864555488112, "phrase": "new_markov_chain"}, {"score": 0.002652768049567487, "phrase": "posterior_distribution"}, {"score": 0.0025366596616434793, "phrase": "parameter_expansion_step"}, {"score": 0.0023719332915829268, "phrase": "good_convergence_properties"}, {"score": 0.002319431279824863, "phrase": "mcmc_sampler"}, {"score": 0.0021049977753042253, "phrase": "human_controller"}], "paper_keywords": ["Data augmentation", " parameter expansion", " Markov Chain Monte Carlo", " Markov decision process", " Bayesian inference"], "paper_abstract": "We consider the inverse reinforcement learning problem, that is, the problem of learning from, and then predicting or mimicking a controller based on state/action data. We propose a statistical model for such data, derived from the structure of a Markov decision process. Adopting a Bayesian approach to inference, we show how latent variables of the model can be estimated, and how predictions about actions can be made, in a unified framework. A new Markov chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior distribution. This step includes a parameter expansion step, which is shown to be essential for good convergence properties of the MCMC sampler. As an illustration, the method is applied to learning a human controller.", "paper_title": "Bayesian Learning of Noisy Markov Decision Processes", "paper_id": "WOS:000314179000004"}