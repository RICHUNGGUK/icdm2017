{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "learning_latent_variable_models"}, {"score": 0.004641351004344458, "phrase": "computationally_and_statistically_efficient_parameter_estimation_method"}, {"score": 0.004473982381245469, "phrase": "latent_variable_models"}, {"score": 0.004392565073466714, "phrase": "gaussian_mixture_models"}, {"score": 0.004339108074589307, "phrase": "hidden_markov_models"}, {"score": 0.004260134699483774, "phrase": "latent_dirichlet_allocation"}, {"score": 0.0037921662574137535, "phrase": "parameter_estimation"}, {"score": 0.003438058620195309, "phrase": "symmetric_tensor"}, {"score": 0.0031747618267940155, "phrase": "natural_generalization"}, {"score": 0.003116913644889395, "phrase": "singular_value_decomposition"}, {"score": 0.003022825806274343, "phrase": "tensor_decompositions"}, {"score": 0.002825681384008326, "phrase": "specially_structured_tensors"}, {"score": 0.0026252106881752067, "phrase": "power_iterations"}, {"score": 0.00259320616168125, "phrase": "maximization_approaches"}, {"score": 0.002424012799998538, "phrase": "detailed_analysis"}, {"score": 0.0023798115577749225, "phrase": "robust_tensor_power_method"}, {"score": 0.00226583335721762, "phrase": "wedin's_perturbation_theorem"}, {"score": 0.002224510044064184, "phrase": "singular_vectors"}, {"score": 0.0021309898622781124, "phrase": "robust_and_computationally_tractable_estimation_approach"}], "paper_keywords": ["latent variable models", " tensor decompositions", " mixture models", " topic models", " method of moments", " power method"], "paper_abstract": "This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models-including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation-which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.", "paper_title": "Tensor Decompositions for Learning Latent Variable Models", "paper_id": "WOS:000344638600005"}