{"auto_keywords": [{"score": 0.047409238560376495, "phrase": "concept_drift"}, {"score": 0.03820278862285535, "phrase": "imbalanced_data"}, {"score": 0.00481495049065317, "phrase": "incremental_learning_of_concept_drift"}, {"score": 0.004769182299692333, "phrase": "streaming_imbalanced_data"}, {"score": 0.004678940805063634, "phrase": "nonstationary_environments"}, {"score": 0.0035462807506718578, "phrase": "logical_combination"}, {"score": 0.0033969001271637964, "phrase": "well-established_smote"}, {"score": 0.0031921327477582523, "phrase": "smote"}, {"score": 0.0031018195951638882, "phrase": "strategic_use"}, {"score": 0.003072285184441679, "phrase": "minority_class_data"}, {"score": 0.002752139271586703, "phrase": "primary_novelty"}, {"score": 0.002648768009464535, "phrase": "voting_weights"}, {"score": 0.002611010097359509, "phrase": "ensemble_members"}, {"score": 0.002549269480908849, "phrase": "classifier's_time"}, {"score": 0.002524982927936405, "phrase": "imbalance-adjusted_accuracy"}, {"score": 0.002500927169609655, "phrase": "current_and_past_environments"}, {"score": 0.002305468528385778, "phrase": "challenging_problem"}, {"score": 0.002176747110119268, "phrase": "experimental_data"}, {"score": 0.0021049977753042253, "phrase": "future_research"}], "paper_keywords": ["Incremental learning", " concept drift", " class imbalance", " multiple classifier systems"], "paper_abstract": "Learning in nonstationary environments, also known as learning concept drift, is concerned with learning from data whose statistical characteristics change over time. Concept drift is further complicated if the data set is class imbalanced. While these two issues have been independently addressed, their joint treatment has been mostly underexplored. We describe two ensemble-based approaches for learning concept drift from imbalanced data. Our first approach is a logical combination of our previously introduced Learn(++).NSE algorithm for concept drift, with the well-established SMOTE for learning from imbalanced data. Our second approach makes two major modifications to Learn(++).NSE-SMOTE integration by replacing SMOTE with a subensemble that makes strategic use of minority class data; and replacing Learn(++).NSE and its class-independent error weighting mechanism with a penalty constraint that forces the algorithm to balance accuracy on all classes. The primary novelty of this approach is in determining the voting weights for combining ensemble members, based on each classifier's time and imbalance-adjusted accuracy on current and past environments. Favorable results in comparison to other approaches indicate that both approaches are able to address this challenging problem, each with its own specific areas of strength. We also release all experimental data as a resource and benchmark for future research.", "paper_title": "Incremental Learning of Concept Drift from Streaming Imbalanced Data", "paper_id": "WOS:000323503100009"}