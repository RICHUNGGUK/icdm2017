{"auto_keywords": [{"score": 0.041455326800874014, "phrase": "information_preservation"}, {"score": 0.021560745207293917, "phrase": "sanitizing_algorithm"}, {"score": 0.010612387000973441, "phrase": "statistical_privacy"}, {"score": 0.004778594263962987, "phrase": "data_processing_applications"}, {"score": 0.004184825434585617, "phrase": "statistical_models"}, {"score": 0.0038498211283780484, "phrase": "data_owner"}, {"score": 0.0036093693971289754, "phrase": "data_consumers"}, {"score": 0.0035685175471526823, "phrase": "sanitized_output"}, {"score": 0.003422664155485697, "phrase": "williams"}, {"score": 0.003396752445888339, "phrase": "mcsherry"}, {"score": 0.00330770035909988, "phrase": "information-preserving_properties"}, {"score": 0.0029296116638296748, "phrase": "bayesian"}, {"score": 0.002841701708055512, "phrase": "utility_measures"}, {"score": 0.0027048247303526583, "phrase": "bayesian_concepts"}, {"score": 0.0026741832921475667, "phrase": "subjective_probabilities"}, {"score": 0.002613935123129054, "phrase": "bayesian_views"}, {"score": 0.0025941547726803594, "phrase": "privacy_research"}, {"score": 0.0023591417077156555, "phrase": "average_error"}, {"score": 0.00233240715947344, "phrase": "bayesian_decision_maker"}, {"score": 0.0022284577808744316, "phrase": "sanitized_outputs"}, {"score": 0.002121056250381627, "phrase": "unattributed_histogram_problem"}], "paper_keywords": ["Security", " Measurement", " Privacy", " utility", " differential privacy", " decision theory", " information measures", " minimax"], "paper_abstract": "In statistical privacy, utility refers to two concepts: information preservation, how much statistical information is retained by a sanitizing algorithm, and usability, how (and with how much difficulty) one extracts this information to build statistical models, answer queries, and so forth. Some scenarios incentivize a separation between information preservation and usability, so that the data owner first chooses a sanitizing algorithm to maximize a measure of information preservation, and, afterward, the data consumers process the sanitized output according to their various individual needs [Ghosh et al. 2009; Williams and McSherry 2010]. We analyze the information-preserving properties of utility measures with a combination of two new and three existing utility axioms and study how violations of an axiom can be fixed. We show that the average (over possible outputs of the sanitizer) error of Bayesian decision makers forms the unique class of utility measures that satisfy all of the axioms. The axioms are agnostic to Bayesian concepts such as subjective probabilities and hence strengthen support for Bayesian views in privacy research. In particular, this result connects information preservation to aspects of usability-if the information preservation of a sanitizing algorithm should be measured as the average error of a Bayesian decision maker, shouldn't Bayesian decision theory be a good choice when it comes to using the sanitized outputs for various purposes? We put this idea to the test in the unattributed histogram problem where our decision-theoretic postprocessing algorithm empirically outperforms previously proposed approaches.", "paper_title": "Information Measures in Statistical Privacy and Data Processing Applications", "paper_id": "WOS:000355675000003"}