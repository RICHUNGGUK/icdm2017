{"auto_keywords": [{"score": 0.048163992431376076, "phrase": "deep_networks"}, {"score": 0.03414229704911109, "phrase": "deep_network"}, {"score": 0.026897811453592893, "phrase": "learning_problem"}, {"score": 0.00481495049065317, "phrase": "kernel_analysis_of_deep_networks"}, {"score": 0.004483120978156604, "phrase": "common_knowledge"}, {"score": 0.004347972379250724, "phrase": "efficient_and_well_generalizing_representation"}, {"score": 0.0036553070575015344, "phrase": "emerging_representation"}, {"score": 0.003438058620195309, "phrase": "layer-wise_evolution"}, {"score": 0.0030414140732789186, "phrase": "deeper_and_deeper_kernels"}, {"score": 0.0026091594819333654, "phrase": "increasingly_complex_kernels"}, {"score": 0.00235560370536971, "phrase": "increasingly_better_representations"}, {"score": 0.002126635713519053, "phrase": "deep_network_controls"}], "paper_keywords": ["deep networks", " kernel principal component analysis", " representations"], "paper_abstract": "When training deep networks it is common knowledge that an efficient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer.", "paper_title": "Kernel Analysis of Deep Networks", "paper_id": "WOS:000298102900002"}