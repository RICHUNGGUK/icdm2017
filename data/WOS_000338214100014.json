{"auto_keywords": [{"score": 0.04917331388552706, "phrase": "true_labels"}, {"score": 0.0072179536189210585, "phrase": "proposed_method"}, {"score": 0.00481495049065317, "phrase": "learning_problems"}, {"score": 0.004678940805063634, "phrase": "training_instances"}, {"score": 0.004397230847865867, "phrase": "alternative_approach"}, {"score": 0.004073590964805314, "phrase": "crowdsourcing_services"}, {"score": 0.00401560412621627, "phrase": "amazon's_mechanical_turk"}, {"score": 0.003958439439112026, "phrase": "collected_labels"}, {"score": 0.003702206069379459, "phrase": "negative_effects"}, {"score": 0.003580355398511932, "phrase": "malicious_labelers"}, {"score": 0.0031921317979928406, "phrase": "main_source"}, {"score": 0.0031018195951638882, "phrase": "labeler_accuracies"}, {"score": 0.002942811795757911, "phrase": "labeler_opinions"}, {"score": 0.0029147869733574844, "phrase": "useful_sources"}, {"score": 0.00283229915062791, "phrase": "accuracy_estimation_problem"}, {"score": 0.00276534038425712, "phrase": "estimation_problem"}, {"score": 0.0027259252868643926, "phrase": "optimization_problem"}, {"score": 0.002598543802992702, "phrase": "analytical_probabilities"}, {"score": 0.002537097217899195, "phrase": "estimated_accuracies"}, {"score": 0.0024301229420875155, "phrase": "provided_labels"}, {"score": 0.002372649454733249, "phrase": "efficient_semi-exhaustive_search_method"}, {"score": 0.0022617389804167943, "phrase": "simulated_data"}, {"score": 0.002166349705365602, "phrase": "promising_idea"}, {"score": 0.0021354538918029265, "phrase": "emerging_new_area"}, {"score": 0.0021049977753042253, "phrase": "source_code"}], "paper_keywords": ["Artificial intelligence", " Supervised learning", " Noisy labels", " Human labelers", " Crowdsourcing", " Agreement/disagreement"], "paper_abstract": "In many supervised learning problems, determining the true labels of training instances is expensive, laborious, and even practically impossible. As an alternative approach, it is much easier to collect multiple subjective (possibly noisy) labels from human labelers, especially with the crowdsourcing services such as Amazon's Mechanical Turk. The collected labels are then aggregated to estimate the true labels. In order to reduce the negative effects of novices, spammers, and malicious labelers, it necessitates taking into account the accuracies of the labelers. However, in the absence of true labels, we miss the main source of information to estimate the labeler accuracies. This paper demonstrates that the agreements or disagreements among labeler opinions are useful sources of information and facilitate the accuracy estimation problem. We represent this estimation problem as an optimization problem which its goal is to minimize the differences between the analytical probabilities of disagreements based on estimated accuracies and the probabilities of disagreements according to the provided labels. We present an efficient semi-exhaustive search method to solve this optimization problem. Our experiments on the simulated data and three real datasets show that the proposed method is a promising idea in this emerging new area. The source code of the proposed method is available for downloading at http://ceit.aut.ac.ir/similar to amirkhani.", "paper_title": "Agreement/disagreement based crowd labeling", "paper_id": "WOS:000338214100014"}