{"auto_keywords": [{"score": 0.045651853488492745, "phrase": "source-language_syntactic_structures"}, {"score": 0.03939778205217001, "phrase": "itg"}, {"score": 0.03396924523576886, "phrase": "probabilistic_models"}, {"score": 0.03331106309940366, "phrase": "projected_partial_structures"}, {"score": 0.00481495049065317, "phrase": "target-language_parser"}, {"score": 0.004776297980616768, "phrase": "cross-language_syntactic_projection_for_statistical_machine_translation"}, {"score": 0.00464343008299973, "phrase": "widely_different_word_orders"}, {"score": 0.004514241562580097, "phrase": "major_challenge"}, {"score": 0.004442032546224076, "phrase": "word_reordering_methods"}, {"score": 0.004232232993872917, "phrase": "word_reordering"}, {"score": 0.004164516065972361, "phrase": "high-quality_syntactic_parsers"}, {"score": 0.003999907278478564, "phrase": "preordering_method"}, {"score": 0.003951800212882157, "phrase": "target-language_syntactic_parser"}, {"score": 0.0038573081851680656, "phrase": "source-language_syntactic_parser"}, {"score": 0.0036898803847106023, "phrase": "syntactic_constituent_structures"}, {"score": 0.0036602262729063775, "phrase": "source-language_training_sentences"}, {"score": 0.0035726811019151984, "phrase": "target-language_training_sentences"}, {"score": 0.003487222504700868, "phrase": "constituent_structures"}, {"score": 0.0034452601929727752, "phrase": "target-language_sentences"}, {"score": 0.003403801095792972, "phrase": "corresponding_source-language_sentences"}, {"score": 0.0033223686422055834, "phrase": "parallel_sentences"}, {"score": 0.0032956583072431423, "phrase": "highly_synchronized_parallel_structures"}, {"score": 0.003114584448820957, "phrase": "pitman-yor_process"}, {"score": 0.003003452468800793, "phrase": "full_binary_syntactic_structures"}, {"score": 0.0029434299787644445, "phrase": "corresponding_target-language_syntactic_structures"}, {"score": 0.002748159131396737, "phrase": "produced_binary_syntactic_structures"}, {"score": 0.002682370021834353, "phrase": "proposed_method"}, {"score": 0.002597088221737922, "phrase": "highly_synchronized_parallel_syntactic_structures"}, {"score": 0.0025658094607383646, "phrase": "cross-language_syntactic_projection"}, {"score": 0.002504374718975704, "phrase": "preordering_model"}, {"score": 0.0024742098091226203, "phrase": "input_sentences"}, {"score": 0.0024052270356036743, "phrase": "japanese-english"}, {"score": 0.0023859363881167995, "phrase": "chinese"}, {"score": 0.002300681752315682, "phrase": "existing_methods"}, {"score": 0.002272964710550345, "phrase": "string-to-tree_syntax-based_smt"}, {"score": 0.0021049977753042253, "phrase": "source-language_dependency_parser"}], "paper_keywords": ["Theory", " Algorithms", " Design", " Experimentation", " Preordering", " syntactic projection", " constituent structure", " inversion transduction grammar"], "paper_abstract": "When translating between languages with widely different word orders, word reordering can present a major challenge. Although some word reordering methods do not employ source-language syntactic structures, such structures are inherently useful for word reordering. However, high-quality syntactic parsers are not available for many languages. We propose a preordering method using a target-language syntactic parser to process source-language syntactic structures without a source-language syntactic parser. To train our preordering model based on ITG, we produced syntactic constituent structures for source-language training sentences by (1) parsing target-language training sentences, (2) projecting constituent structures of the target-language sentences to the corresponding source-language sentences, (3) selecting parallel sentences with highly synchronized parallel structures, (4) producing probabilistic models for parsing using the projected partial structures and the Pitman-Yor process, and (5) parsing to produce full binary syntactic structures maximally synchronized with the corresponding target-language syntactic structures, using the constraints of the projected partial structures and the probabilistic models. Our ITG-based preordering model is trained using the produced binary syntactic structures and word alignments. The proposed method facilitates the learning of ITG by producing highly synchronized parallel syntactic structures based on cross-language syntactic projection and sentence selection. The preordering model jointly parses input sentences and identifies their reordered structures. Experiments with Japanese-English and Chinese-English patent translation indicate that our method outperforms existing methods, including string-to-tree syntax-based SMT, a preordering method that does not require a parser, and a preordering method that uses a source-language dependency parser.", "paper_title": "Preordering using a Target-Language Parser via Cross-Language Syntactic Projection for Statistical Machine Translation", "paper_id": "WOS:000370686400004"}