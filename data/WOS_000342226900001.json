{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "multiple_images"}, {"score": 0.01221610092704981, "phrase": "yarn_surface_evaluation"}, {"score": 0.004703075081891693, "phrase": "saliency_models"}, {"score": 0.0045324756541157574, "phrase": "computer_vision"}, {"score": 0.004502126088469323, "phrase": "image_understanding"}, {"score": 0.004427134352966977, "phrase": "existing_models"}, {"score": 0.00432423070113186, "phrase": "individual_image"}, {"score": 0.004056769841308405, "phrase": "remaining_items"}, {"score": 0.003922685601503529, "phrase": "absolute_saliency"}, {"score": 0.0037242297962852616, "phrase": "relative_saliency"}, {"score": 0.0036553070575015344, "phrase": "visual_inspection_tasks"}, {"score": 0.003522579870903474, "phrase": "yarn_image"}, {"score": 0.003406091279580424, "phrase": "graded_standard_images"}, {"score": 0.003152531962718477, "phrase": "visual_attention_model"}, {"score": 0.0030482430937467013, "phrase": "relative_saliency_model"}, {"score": 0.0029473940028924748, "phrase": "bottom-up_and_top-down_mechanisms"}, {"score": 0.0027187231747900814, "phrase": "structural_feature_extraction_strategy"}, {"score": 0.0023603078194781965, "phrase": "saliency_values"}, {"score": 0.002297585503415527, "phrase": "multiimage_contents"}, {"score": 0.0022744957687536307, "phrase": "single_image_content"}, {"score": 0.0022290085428279846, "phrase": "proposed_relative_saliency_model"}, {"score": 0.0021479597698614355, "phrase": "eye_tracking_technique"}, {"score": 0.0021049977753042253, "phrase": "proposed_concept"}], "paper_keywords": ["Comparison of multiple images", " relative saliency map", " visual attention", " yarn surface evaluation"], "paper_abstract": "Saliency models have been developed and widely demonstrated to benefit applications in computer vision and image understanding. In most of existing models, saliency is evaluated within an individual image. That is, saliency value of an item (object/region/pixel) represents the conspicuity of it as compared with the remaining items in the same image. We call this saliency as absolute saliency, which is uncomparable among images. However, saliency should be determined in the context of multiple images for some visual inspection tasks. For example, in yarn surface evaluation, saliency of a yarn image should be measured with regard to a set of graded standard images. We call this saliency the relative saliency, which is comparable among images. In this paper, a study of visual attention model for comparison of multiple images is explored, and a relative saliency model of multiple images is proposed based on a combination of bottom-up and top-down mechanisms, to enable relative saliency evaluation for the cases where other image contents are involved. To fully characterize the differences among multiple images, a structural feature extraction strategy is proposed, where two levels of feature (high-level, low-level) and three types of feature (global, local-local, local-global) are extracted. Mapping functions between features and saliency values are constructed and their outputs reflect relative saliency for multiimage contents instead of single image content. The performance of the proposed relative saliency model is well demonstrated in a yarn surface evaluation. Furthermore, the eye tracking technique is employed to verify the proposed concept of relative saliency for multiple images.", "paper_title": "Relative Saliency Model over Multiple Images with an Application to Yarn Surface Evaluation", "paper_id": "WOS:000342226900001"}