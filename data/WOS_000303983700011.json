{"auto_keywords": [{"score": 0.04074141016787776, "phrase": "var"}, {"score": 0.00481495049065317, "phrase": "highly_parallel_architectures"}, {"score": 0.004675768396728975, "phrase": "modern_financial_markets"}, {"score": 0.004562847483469705, "phrase": "changing_market_conditions"}, {"score": 0.004452641437293878, "phrase": "financial_risk_management_tools"}, {"score": 0.00428179779071721, "phrase": "value-at"}, {"score": 0.004037694641919865, "phrase": "key_decisions"}, {"score": 0.003770373393393271, "phrase": "monte_carlo_method"}, {"score": 0.0036792371574197826, "phrase": "computationally_intensive_method"}, {"score": 0.0035902958977673313, "phrase": "overnight_batch_job"}, {"score": 0.0034863899423238707, "phrase": "highly_parallel_computing_platforms"}, {"score": 0.0034355662264234864, "phrase": "multicore_cpus"}, {"score": 0.0034020946201397057, "phrase": "manycore_graphics_processing_units"}, {"score": 0.003287482882222731, "phrase": "computation_capability"}, {"score": 0.003192312493850036, "phrase": "desktop_computer"}, {"score": 0.003099888654615187, "phrase": "large_portfolios"}, {"score": 0.0030397596289795143, "phrase": "risk_factors"}, {"score": 0.0023677483882740317, "phrase": "optimization_perspectives"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["D", " 1", " 3 concurrent programming [parallel programming]", " G", " 3 probability and statistics [probabilistic algorithms (including Monte Carlo)]", " J", " 4 social and behavioral sciences [economics]"], "paper_abstract": "Values of portfolios in modern financial markets may change precipitously with changing market conditions. The utility of financial risk management tools is dependent on whether they can estimate Value-at-Risk (VaR) of portfolios on-demand when key decisions need to be made. However, VaR estimation of portfolios uses the Monte Carlo method, which is a computationally intensive method often run as an overnight batch job. With the proliferation of highly parallel computing platforms such as multicore CPUs and manycore graphics processing units (GPUs), teraFLOPS of computation capability is now available on a desktop computer, enabling the VaR of large portfolios with thousands of risk factors to be computed within only a fraction of a second. Achieving such performance in practice requires the assimilation of expertise in the following three areas: (i) application domain; (ii) statistical analytics; and (iii) parallel computing. This paper demonstrates that these areas of expertise inform optimization perspectives that, when combined, lead to 127 x speedup on our CPU-based implementation and 538 x speedup on our GPU-based implementation. Copyright (C) 2011 John Wiley & Sons, Ltd.", "paper_title": "Accelerating Value-at-Risk estimation on highly parallel architectures", "paper_id": "WOS:000303983700011"}