{"auto_keywords": [{"score": 0.04583834963802332, "phrase": "query_model"}, {"score": 0.004583205112737672, "phrase": "novel_interactive_framework"}, {"score": 0.003808845109556467, "phrase": "first_approach"}, {"score": 0.0036030386046805598, "phrase": "resulting_gestures"}, {"score": 0.0031649018957020337, "phrase": "user's_hands"}, {"score": 0.0029753513692467315, "phrase": "proposed_framework"}, {"score": 0.0027457584014804574, "phrase": "unobtrusive_interface"}, {"score": 0.002678734566245688, "phrase": "gesture_recognition"}, {"score": 0.002323826502178581, "phrase": "force_feedback_device"}, {"score": 0.0021049977753042253, "phrase": "usability_and_efficiency_criteria"}], "paper_keywords": ["3D content-based search", " Multimodal interfaces", " Sketch"], "paper_abstract": "This paper presents a novel interactive framework for 3D content-based search and retrieval using as query model an object that is dynamically sketched by the user. In particular, two approaches are presented for generating the query model. The first approach uses 2D sketching and symbolic representation of the resulting gestures. The second utilizes non-linear least squares minimization to model the 3D point cloud that is generated by the 3D tracking of the user's hands, using superquadrics. In the context of the proposed framework, three interfaces were integrated to the sketch-based 3D search system including (a) an unobtrusive interface that utilizes pointing gesture recognition to allow the user manipulate objects in 3D, (b) a haptic-VR interface composed by 3D data gloves and a force feedback device, and (c) a simple air-mouse. These interfaces were tested and comparative results were extracted according to usability and efficiency criteria.", "paper_title": "3D content-based search using sketches", "paper_id": "WOS:000261069900008"}