{"auto_keywords": [{"score": 0.04949240338189186, "phrase": "classifier_ensemble_reduction"}, {"score": 0.00481495049065317, "phrase": "feature_selection"}, {"score": 0.004697655699119423, "phrase": "classifier_ensembles"}, {"score": 0.004555028829645471, "phrase": "main_research_directions"}, {"score": 0.0044991925146184025, "phrase": "machine_learning"}, {"score": 0.004444037594955185, "phrase": "data_mining"}, {"score": 0.00430907756010007, "phrase": "multiple_classifiers"}, {"score": 0.004230068512904886, "phrase": "better_predictive_performance"}, {"score": 0.004076352180903171, "phrase": "single_model"}, {"score": 0.003625347557965931, "phrase": "ensemble_systems"}, {"score": 0.0035808664384880213, "phrase": "redundant_members"}, {"score": 0.0033873366086549735, "phrase": "group_diversity"}, {"score": 0.0033251716926666437, "phrase": "better_results"}, {"score": 0.003284361433827464, "phrase": "smaller_ensembles"}, {"score": 0.003204232566468624, "phrase": "memory_and_storage_requirements"}, {"score": 0.003145417398038261, "phrase": "system's_run-time_overhead"}, {"score": 0.0029027394930867902, "phrase": "feature_selection_problems"}, {"score": 0.002779894844207706, "phrase": "ensemble_predictions"}, {"score": 0.0027457584014804574, "phrase": "training_samples"}, {"score": 0.002565345470805401, "phrase": "global_heuristic_harmony_search"}, {"score": 0.002471973283582411, "phrase": "reduced_subset"}, {"score": 0.0023382335380460304, "phrase": "feature_subset_evaluation"}, {"score": 0.002295277579105177, "phrase": "resulting_technique"}, {"score": 0.002225426912273257, "phrase": "high_dimensional_and_large_sized_benchmark_datasets"}], "paper_keywords": ["Classifier ensemble reduction", " feature selection", " harmony search"], "paper_abstract": "Classifier ensembles constitute one of the main research directions in machine learning and data mining. The use of multiple classifiers generally allows better predictive performance than that achievable with a single model. Several approaches exist in the literature that provide means to construct and aggregate such ensembles. However, these ensemble systems contain redundant members that, if removed, may further increase group diversity and produce better results. Smaller ensembles also relax the memory and storage requirements, reducing system's run-time overhead while improving overall efficiency. This paper extends the ideas developed for feature selection problems to support classifier ensemble reduction, by transforming ensemble predictions into training samples, and treating classifiers as features. Also, the global heuristic harmony search is used to select a reduced subset of such artificial features, while attempting to maximize the feature subset evaluation. The resulting technique is systematically evaluated using high dimensional and large sized benchmark datasets, showing a superior classification performance against both original, unreduced ensembles, and randomly formed subsets.", "paper_title": "Feature Selection Inspired Classifier Ensemble Reduction", "paper_id": "WOS:000342226900002"}