{"auto_keywords": [{"score": 0.039144489494399866, "phrase": "control_algorithm"}, {"score": 0.03753385825085361, "phrase": "rl_method"}, {"score": 0.00481495049065317, "phrase": "rls-based_natural_actor-critic_learning_algorithm"}, {"score": 0.004375613460102577, "phrase": "small_decline"}, {"score": 0.004010960493946656, "phrase": "actuated_robots"}, {"score": 0.003907586297783078, "phrase": "flat_terrain"}, {"score": 0.0038568987435766014, "phrase": "passive_dynamic_walking"}, {"score": 0.003644714340300824, "phrase": "reinforcement_learning"}, {"score": 0.0035199624440843892, "phrase": "goal-directed_learning"}, {"score": 0.0034454866886529096, "phrase": "situations"}, {"score": 0.0033553473725594003, "phrase": "exemplary_supervision"}, {"score": 0.0033262524445013303, "phrase": "complete_models"}, {"score": 0.003062091606343783, "phrase": "evaluative_feedback"}, {"score": 0.002880927789104712, "phrase": "actuated_passive_dynamic_walker"}, {"score": 0.002843519597398796, "phrase": "control_objective"}, {"score": 0.0027581067089091434, "phrase": "level_ground"}, {"score": 0.002629026692001082, "phrase": "rl_algorithm"}, {"score": 0.002583597507349819, "phrase": "actor-critic_architecture"}, {"score": 0.0025500402081330394, "phrase": "natural_gradient_method"}, {"score": 0.002473420909291381, "phrase": "recursive_least-squares"}, {"score": 0.0023782753249846794, "phrase": "learning_process"}, {"score": 0.002218054404179068, "phrase": "computer_simulations"}, {"score": 0.0021797112385590913, "phrase": "eigenvalue_analysis"}, {"score": 0.0021607883988873492, "phrase": "stable_locomotion"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Passive dynamic walker", " Reinforcement learning (RL)", " Actor-critic architecture", " Natural gradient", " Recursive least-squares (RLS)"], "paper_abstract": "A passive dynamic walker belongs to a class of bipedal walking robots that are able to walk stably down a small decline without using any actuators. The purpose of this research is to design a controller in order to build actuated robots capable of walking on a flat terrain based on passive dynamic walking. To achieve this objective, a control algorithm was used based on reinforcement learning (RL). The RL method is a goal-directed learning of a mapping from Situations to actions without relying on exemplary supervision or complete models of the environment. The goal of the RL method is to maximize a reward, which is an evaluative feedback from the environment. In the process of constructing the reward of the actuated passive dynamic walker, the control objective, which is stable walking on level ground. is directly included. In this study, an RL algorithm based on the actor-critic architecture and the natural gradient method is applied. Also. the recursive least-squares (RLS) method was employed for the learning process in order to improve the efficiency of the method. The control algorithm was verified With computer simulations based on the eigenvalue analysis for stable locomotion. (C) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Passive dynamic walker controller design employing an RLS-based natural actor-critic learning algorithm", "paper_id": "WOS:000260748700006"}