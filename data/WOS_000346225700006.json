{"auto_keywords": [{"score": 0.0500785296201053, "phrase": "head_gaze"}, {"score": 0.04546222322123732, "phrase": "tightly_synchronized_head_gaze-speech"}, {"score": 0.010121008168456633, "phrase": "loosely_synchronized_head_gaze-speech"}, {"score": 0.0047310525657970615, "phrase": "real-time_synthetic_speech"}, {"score": 0.004487989502479992, "phrase": "socially_acceptable_interactions"}, {"score": 0.004448705308279165, "phrase": "loosely_synchronized_head_gaze-speech_acts"}, {"score": 0.004276120412040056, "phrase": "significant_human_effort"}, {"score": 0.00416478405812793, "phrase": "synchronization_events"}, {"score": 0.004074212307838049, "phrase": "interactive_dialog"}, {"score": 0.0037973594873521596, "phrase": "autonomous_synchronization"}, {"score": 0.0036179751816682454, "phrase": "sentence_structure"}, {"score": 0.003586278948128509, "phrase": "time_delays"}, {"score": 0.00346223706087128, "phrase": "simulated_disaster_site"}, {"score": 0.0034168317227203206, "phrase": "rescue_robot"}, {"score": 0.003284145698220425, "phrase": "victim_management_scenario"}, {"score": 0.003170520716404193, "phrase": "pre-_and_postinteraction_questionnaires"}, {"score": 0.0031151864044353245, "phrase": "social_acceptance_level"}, {"score": 0.0025665261770229757, "phrase": "appropriate_times"}, {"score": 0.0023192471808039746, "phrase": "fundamental_understanding"}, {"score": 0.0022687189700002254, "phrase": "social_head"}, {"score": 0.0022389308000757037, "phrase": "social_acceptance"}, {"score": 0.0022192891418394514, "phrase": "human-machine_interaction"}, {"score": 0.0021901485584955487, "phrase": "social_gaze"}, {"score": 0.0021236300589450143, "phrase": "practical_implementation"}, {"score": 0.0021049977753042253, "phrase": "social_robots"}], "paper_keywords": ["Autonomous generation", " human-robot interaction", " interactive conversation", " social head gaze", " user study and evaluation"], "paper_abstract": "This study demonstrates that robots can achieve socially acceptable interactions using loosely synchronized head gaze-speech acts. Prior approaches use tightly synchronized head gaze-speech, which requires significant human effort and time to manually annotate synchronization events in advance, restricts interactive dialog, or requires that the operator acts as a puppeteer. This paper describes how autonomous synchronization of head gaze can be achieved by exploiting affordances in the sentence structure and time delays. A 93-participant user study was conducted in a simulated disaster site. The rescue robot \"Survivor Buddy\" generated head gaze for a victim management scenario using a 911 dialog. The study used pre- and postinteraction questionnaires to compare the social acceptance level of loosely synchronized head gaze-speech against tightly synchronized head gaze-speech (manual annotation) and no head gaze-speech conditions. The results indicated that for attributes of Self-Assessment Manikin, i.e., Arousal, Robot Likeability, Human-Like Behavior, Understanding Robot Behavior, Gaze-Speech Synchronization, Looking at Objects at Appropriate Times, and Natural Movement, the loosely synchronized head gaze-speech is similar to tightly synchronized head gaze-speech and preferred to the no head gaze-speech case. This study contributes to a fundamental understanding of the role of social head gaze in social acceptance for human-machine interaction, how social gaze can be produced, and promotes practical implementation in social robots.", "paper_title": "Evaluation of Head Gaze Loosely Synchronized With Real-Time Synthetic Speech for Social Robots", "paper_id": "WOS:000346225700006"}