{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "facial_expressions"}, {"score": 0.014163868183285784, "phrase": "continuous_model"}, {"score": 0.014050675926286898, "phrase": "facial_expression"}, {"score": 0.013141446985582736, "phrase": "different_intensities"}, {"score": 0.01293201797909755, "phrase": "categorical_model"}, {"score": 0.004763585574804203, "phrase": "humans"}, {"score": 0.004240997872184065, "phrase": "feature_vector"}, {"score": 0.004206767038617782, "phrase": "face_space"}, {"score": 0.003921393143378986, "phrase": "c_classifiers"}, {"score": 0.00385832528624328, "phrase": "specific_emotion_category"}, {"score": 0.0037150687722055727, "phrase": "morphing_sequence"}, {"score": 0.0036553070575015344, "phrase": "surprise_face"}, {"score": 0.0034442607586033657, "phrase": "latter_finding"}, {"score": 0.003124790943886441, "phrase": "emotion_categories"}, {"score": 0.002928399649340438, "phrase": "revised_model"}, {"score": 0.002865710140187219, "phrase": "cognitive_science_and_neuroscience_literature"}, {"score": 0.0028195727965553367, "phrase": "c_distinct_continuous_spaces"}, {"score": 0.0027443174539994925, "phrase": "c_face_spaces"}, {"score": 0.002599763359352916, "phrase": "major_task"}, {"score": 0.0023905702405714146, "phrase": "resulting_model"}, {"score": 0.0022830706430588482, "phrase": "research_directions"}, {"score": 0.002270745003370529, "phrase": "machine_learning"}, {"score": 0.002258485755625332, "phrase": "computer_vision_researchers"}, {"score": 0.0021164256448611063, "phrase": "human_perception"}, {"score": 0.0021049977753042253, "phrase": "social_interactions"}], "paper_keywords": ["vision", " face perception", " emotions", " computational modeling", " categorical perception", " face detection"], "paper_abstract": "In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion-the continuous and the categorical model. The continuous model defines each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classifiers, each tuned to a specific emotion category. This model explains, among other findings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difficult time justifying this latter finding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justifies the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly configural. According to this model, the major task for the classification of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in studies of human perception, social interactions and disorders.", "paper_title": "A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives", "paper_id": "WOS:000305456600008"}