{"auto_keywords": [{"score": 0.04969652007027205, "phrase": "interactive_question"}, {"score": 0.03593848500567063, "phrase": "operating_conditions"}, {"score": 0.03015792196082935, "phrase": "hand-coded_policies"}, {"score": 0.027346758619168025, "phrase": "learned_policies"}, {"score": 0.00481495049065317, "phrase": "adaptive_dialogue_strategies"}, {"score": 0.004719359108879602, "phrase": "policy_learning"}, {"score": 0.004662913362513954, "phrase": "active_topic"}, {"score": 0.0046256566852302256, "phrase": "dialogue_systems_research"}, {"score": 0.004286171444202565, "phrase": "first_step"}, {"score": 0.004234884779773412, "phrase": "adaptive_interaction_policies"}, {"score": 0.0037847308929559163, "phrase": "competing_trade-offs"}, {"score": 0.003694652993546994, "phrase": "answer_list"}, {"score": 0.0034508665218929745, "phrase": "communication_channel"}, {"score": 0.0033417571465258, "phrase": "objective_function"}, {"score": 0.0032491108935498794, "phrase": "hand-coded_threshold-based_policy"}, {"score": 0.003171740347925348, "phrase": "reinforcement_learning_policy"}, {"score": 0.0029504765287722465, "phrase": "complex_trade-off_problem"}, {"score": 0.0028003107731426322, "phrase": "wide_range"}, {"score": 0.0027778947555117243, "phrase": "noise_conditions"}, {"score": 0.0027556576784848207, "phrase": "user_types"}, {"score": 0.002625914195439153, "phrase": "wide_spectrum"}, {"score": 0.00254282157860369, "phrase": "average_relative_increase"}, {"score": 0.0023369711258107244, "phrase": "hand-coded_ones"}, {"score": 0.002182573128803447, "phrase": "significant_effect"}, {"score": 0.002130545585846588, "phrase": "qualitative_descriptions"}, {"score": 0.0021049977753042253, "phrase": "learned_iqa_policies"}], "paper_keywords": [""], "paper_abstract": "Policy learning is an active topic in dialogue systems research, but it has not been explored in relation to interactive question answering (IQA). We take a first step in learning adaptive interaction policies for question answering : we address the question of how to acquire enough reliable query constraints, how many database results to present to the user and when to present them, given the competing trade-offs between the length of the answer list, the length of the interaction, the type of database and the noise in the communication channel. The operating conditions are reflected in an objective function which we use to derive a hand-coded threshold-based policy and rewards to train a reinforcement learning policy. The same objective function is used for evaluation. We show that we can learn strategies for this complex trade-off problem which perform significantly better than a variety of hand-coded policies, for a wide range of noise conditions, user types, types of DB and turn-penalties. Our policy learning framework thus covers a wide spectrum of operating conditions. The learned policies produce an average relative increase in reward of 86.78% over the hand-coded policies. In 93% of the cases the learned policies perform significantly better than the hand-coded ones (p < .001). Furthermore we show that the type of database has a significant effect on learning and we give qualitative descriptions of the learned IQA policies.", "paper_title": "Does this list contain what you were searching for? Learning adaptive dialogue strategies for interactive question answering", "paper_id": "WOS:000278274100004"}