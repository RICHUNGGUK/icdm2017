{"auto_keywords": [{"score": 0.02712650942783671, "phrase": "str."}, {"score": 0.00481495049065317, "phrase": "tensor_learning_for_regression"}, {"score": 0.004547438494330851, "phrase": "tensorial_representations"}, {"score": 0.004162826481635768, "phrase": "multiple_modes"}, {"score": 0.0040771431404398855, "phrase": "simultaneous_projections"}, {"score": 0.004014035333627087, "phrase": "input_tensor"}, {"score": 0.0034881076466982226, "phrase": "higher_rank_tensor_ridge_regression"}, {"score": 0.00331125524068286, "phrase": "higher_rank_support_tensor_regression"}, {"score": 0.0031597433845085092, "phrase": "frobenius_norm"}, {"score": 0.0030308784612749647, "phrase": "group-sparsity_norm"}, {"score": 0.0028921584725218642, "phrase": "low_rank_decomposition"}, {"score": 0.002847339938387273, "phrase": "tensorial_weight"}, {"score": 0.0027169974137406148, "phrase": "automatic_selection"}, {"score": 0.002633425544817301, "phrase": "learning_process"}, {"score": 0.002579140290642792, "phrase": "optimal-rank_trr"}, {"score": 0.002324007171619245, "phrase": "real_data"}, {"score": 0.002299921909935613, "phrase": "publicly_available_databases"}, {"score": 0.0021049977753042253, "phrase": "proposed_algorithms"}], "paper_keywords": ["Canonical decomposition (CANDECOMP)/parallel-factor (PARAFAC", " CP) decomposition", " Frobenius norm", " group-sparsity norm", " ridge regression (RR)", " support vector regression (SVR)", " tensors"], "paper_abstract": "In this paper, we exploit the advantages of tensorial representations and propose several tensor learning models for regression. The model is based on the canonical/parallel-factor decomposition of tensors of multiple modes and allows the simultaneous projections of an input tensor to more than one direction along each mode. Two empirical risk functions are studied, namely, the square loss and epsilon-insensitive loss functions. The former leads to higher rank tensor ridge regression (TRR), and the latter leads to higher rank support tensor regression (STR), both formulated using the Frobenius norm for regularization. We also use the group-sparsity norm for regularization, favoring in that way the low rank decomposition of the tensorial weight. In that way, we achieve the automatic selection of the rank during the learning process and obtain the optimal-rank TRR and STR. Experiments conducted for the problems of head-pose, human-age, and 3-D body-pose estimations using real data from publicly available databases, verified not only the superiority of tensors over their vector counterparts but also the efficiency of the proposed algorithms.", "paper_title": "Tensor Learning for Regression", "paper_id": "WOS:000300559700033"}