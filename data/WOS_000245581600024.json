{"auto_keywords": [{"score": 0.047510470298773616, "phrase": "pca"}, {"score": 0.040171416088267346, "phrase": "observed_data"}, {"score": 0.03516336140334411, "phrase": "rotational_ambiguity"}, {"score": 0.033482245998655624, "phrase": "exact_principal_directions"}, {"score": 0.004815115658569046, "phrase": "integrated"}, {"score": 0.004641351004344458, "phrase": "common_derivation"}, {"score": 0.004598933498016774, "phrase": "principal_component_analysis"}, {"score": 0.004332471656145115, "phrase": "centered_data"}, {"score": 0.0042928648367867835, "phrase": "linear_model"}, {"score": 0.004195418010131832, "phrase": "reconstruction_error"}, {"score": 0.004007083686875716, "phrase": "principal_subspace_analysis"}, {"score": 0.003916098498242888, "phrase": "principal_axes"}, {"score": 0.0035887342563437935, "phrase": "alternative_error_measure"}, {"score": 0.003555902292752528, "phrase": "integrated-squared_error"}, {"score": 0.00338060449446278, "phrase": "exact_principal_axes"}, {"score": 0.0030836562939502063, "phrase": "ise."}, {"score": 0.003027462227648001, "phrase": "simple_em_algorithm"}, {"score": 0.00283870598459543, "phrase": "s.t._roweis"}, {"score": 0.00274877942992125, "phrase": "spca"}, {"score": 0.002673957615079041, "phrase": "neural_information_processing_systems"}, {"score": 0.0026011706232569316, "phrase": "mit_press"}, {"score": 0.0025773838915195734, "phrase": "cambridge"}, {"score": 0.0023079225317714815, "phrase": "generalized_hebbian_algorithm"}, {"score": 0.0022867826967466537, "phrase": "gha"}, {"score": 0.0021940118003312397, "phrase": "ise_minimization"}, {"score": 0.002163930783583212, "phrase": "single-layer_linear_feedforward_neural_network"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["EM algorithm", " generalized Hebbian algorithm", " generative models", " probabilistic coupled models", " separable LS", " PCA"], "paper_abstract": "A common derivation of principal component analysis (PCA) is based on the minimization of the squared-error between centered data and linear model, corresponding to the reconstruction error. In fact, minimizing the squared-error leads to principal subspace analysis where scaled and rotated principal axes of a set of observed data, are estimated. In this paper, we introduce and investigate an alternative error measure, integrated-squared error (ISE), the minimization of which determines the exact principal axes (without rotational ambiguity) of a set of observed data. We show that exact principal directions emerge from the minimization of ISE. We present a simple EM algorithm, 'EM-ePCA', which is similar to EM-PCA [S.T. Roweis, EM algorithms for PCA and SPCA, in: Advances in Neural Information Processing Systems, vol. 10, MIT Press, Cambridge, 1998, pp. 626-632.], but finds exact principal directions without rotational ambiguity. In addition, we revisit the generalized Hebbian algorithm (GHA) and show that it emerges from the ISE minimization in a single-layer linear feedforward neural network. (c) 2006 Elsevier B.V. All rights reserved.", "paper_title": "Learning principal directions: Integrated-squared-error minimization", "paper_id": "WOS:000245581600024"}