{"auto_keywords": [{"score": 0.03988824892922573, "phrase": "discriminative_features"}, {"score": 0.038959775584256603, "phrase": "original_variables"}, {"score": 0.03679048193550959, "phrase": "optimal_discriminative_features"}, {"score": 0.00481495049065317, "phrase": "linear_discriminative_features"}, {"score": 0.004781498797706604, "phrase": "linear_discriminant_analysis"}, {"score": 0.004748768448825942, "phrase": "lda"}, {"score": 0.0046176782475198085, "phrase": "linear_features"}, {"score": 0.004537875262684779, "phrase": "real_applications"}, {"score": 0.004443921948822114, "phrase": "extracted_features"}, {"score": 0.004321656690200878, "phrase": "error_rate"}, {"score": 0.0031568304678554243, "phrase": "within-class_scattering_matrix"}, {"score": 0.0026051114364288126, "phrase": "relevant-discriminative_features"}, {"score": 0.0025510720021028107, "phrase": "simulated_experiment"}, {"score": 0.0024039564563674673, "phrase": "experimental_results"}, {"score": 0.002370589610989257, "phrase": "usps_handwritten_digitals"}, {"score": 0.0023540893189518425, "phrase": "pie"}, {"score": 0.0023052356887304226, "phrase": "maximum_margin_criterion"}, {"score": 0.002281194103324554, "phrase": "reasonable_compromise"}, {"score": 0.0021951623041245897, "phrase": "averaged_class_margin"}, {"score": 0.00217987162651941, "phrase": "euclidean_distance"}, {"score": 0.0021496083700137305, "phrase": "primary_space"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Linear Discriminant Analysis", " Dimensionality reduction", " Relevance"], "paper_abstract": "Linear Discriminant Analysis (LDA) has been widely used to extract linear features for classification. In real applications, the usefulness of the extracted features usually needs to be confirmed using an error rate of classification embedded in a classifier. Little attention has been paid to whether and how discriminative features themselves can be interpreted as indicators of usefulness. We refer to this as relevance, i.e., the capability of discriminative features to characterize the contribution of the original variables to classification. We approach the relevance by considering how it could be lost while extracting optimal discriminative features. Then, the discrepancy between the relevance and optimality of discriminative features is shown to originate from the \"angle\" between the space spanned by eigenvectors of the within-class scattering matrix, and the primary space in which the original variables reside. In particular, for a given dataset, the larger the \"angle\", the less evident is the relevance discovered from optimal discriminative features. Furthermore, the relevance and optimality are regarded as two constraint conditions, or a tradeoff, in order to extract relevant-discriminative features. At last, a simulated experiment is used to show how the relevance is lost when the \"angle\" is changed. Experimental results based on both USPS handwritten digitals and PIE face databases show that a maximum margin criterion is a reasonable compromise between the relevance and optimality, since it approximates the averaged class margin using Euclidean distance measured in the primary space. (C) 2010 Elsevier Inc. All rights reserved.", "paper_title": "On the relevance of linear discriminative features", "paper_id": "WOS:000280380500003"}