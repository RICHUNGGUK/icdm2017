{"auto_keywords": [{"score": 0.04888510471747535, "phrase": "multicore_cpus"}, {"score": 0.022181815008472255, "phrase": "sparse_matrix"}, {"score": 0.009393353883209237, "phrase": "sparse_matrices"}, {"score": 0.008742175255071502, "phrase": "dense_blocks"}, {"score": 0.008555693795949351, "phrase": "non-zero_elements"}, {"score": 0.00643886863426533, "phrase": "heterogeneous_computing_platform"}, {"score": 0.005884285039219963, "phrase": "average_performance_improvement"}, {"score": 0.00481495049065317, "phrase": "performance_optimization"}, {"score": 0.004320932773749168, "phrase": "wide_adaptability"}, {"score": 0.0042898529847155255, "phrase": "different_types"}, {"score": 0.0041677440361312604, "phrase": "existing_methods"}, {"score": 0.004078439401031372, "phrase": "particular_sparse_matrices"}, {"score": 0.0039055075176836263, "phrase": "probability_distribution"}, {"score": 0.0035554741718298085, "phrase": "partitioning_strategy"}, {"score": 0.0034918342672996066, "phrase": "probabilistic_modeling"}, {"score": 0.0032957163929772716, "phrase": "highest_mean_density"}, {"score": 0.0031331419276289306, "phrase": "computing_powers"}, {"score": 0.003110579040263398, "phrase": "heterogeneous_processors"}, {"score": 0.0030219357765636497, "phrase": "cpu-gpu_hybrid_parallel_computing_model"}, {"score": 0.002862462250995423, "phrase": "load_distribution"}, {"score": 0.0028111930581414804, "phrase": "spmv"}, {"score": 0.002423882939663573, "phrase": "unique_order"}, {"score": 0.0023890733439136586, "phrase": "probability_mass_function"}, {"score": 0.00213567715613494, "phrase": "partitioning_methods"}, {"score": 0.0021049977753042253, "phrase": "original_row_order"}], "paper_keywords": ["GPU", " matrix partition", " multicore CPU", " probability distribution", " sparse matrix-vector multiplication"], "paper_abstract": "This paper presents a sparse matrix partitioning strategy to improve the performance of SpMV on GPUs and multicore CPUs. This method has wide adaptability for different types of sparse matrices, and is different from existing methods which only adapt to some particular sparse matrices. In addition, our partitioning method can obtain dense blocks by analyzing the probability distribution of non-zero elements in a sparse matrix, and result in very low proportion of zero padded. We make the following significant contributions. (1) We present a partitioning strategy of sparse matrices based on probabilistic modeling of non-zero elements in a row. (2) We prove that our method has the highest mean density compared with other strategies according to certain given ratios of partition obtained from the computing powers of heterogeneous processors. (3) We develop a CPU-GPU hybrid parallel computing model for SpMV on GPUs and multicore CPUs in a heterogeneous computing platform. Our partitioning strategy has balanced load distribution and the performance of SpMV is significantly improved when a sparse matrix is partitioned into dense blocks using our method. The average performance improvement of our solution for SpMV is about 15.75 percent on multicore CPUs, compared to that of the other solutions. By considering the rows of a matrix in a unique order based on the probability mass function of the number of non-zeros in a row, the average performance improvement of our solution for SpMV is about 33.52 percent on GPUs and multicore CPUs of a heterogeneous computing platform, compared to that of the partitioning methods based on the original row order of a matrix.", "paper_title": "Performance Optimization Using Partitioned SpMV on GPUs and Multicore CPUs", "paper_id": "WOS:000359236500016"}