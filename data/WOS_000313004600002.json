{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "multi-agent_q-learning"}, {"score": 0.004674952036988355, "phrase": "multi-agent_learning"}, {"score": 0.0045122922461952805, "phrase": "great_deal"}, {"score": 0.004154332348800334, "phrase": "multi_agent_learning"}, {"score": 0.0040334640350288, "phrase": "replicator_equations"}, {"score": 0.003986103192038901, "phrase": "population_biology"}, {"score": 0.0037797419108291227, "phrase": "discrete_strategy_spaces"}, {"score": 0.0037133478610481994, "phrase": "small_number"}, {"score": 0.0036697319652557363, "phrase": "available_actions"}, {"score": 0.003338690782620756, "phrase": "continuous_spectra"}, {"score": 0.003165739628969694, "phrase": "replicator_framework"}, {"score": 0.0030554300179552415, "phrase": "adaptive_dynamics"}, {"score": 0.0030195184114140063, "phrase": "q-learning_agents"}, {"score": 0.002984027623988469, "phrase": "continuous_strategy_spaces"}, {"score": 0.002914288956035066, "phrase": "probability_vectors"}, {"score": 0.0028800314406625996, "phrase": "agents'_strategies"}, {"score": 0.0027961342030212353, "phrase": "probability_measures"}, {"score": 0.002763261749602258, "phrase": "continuous_variables"}, {"score": 0.002651213849567608, "phrase": "ordinary_differential_equations"}, {"score": 0.0026045914281028473, "phrase": "discrete_case"}, {"score": 0.002498961351132696, "phrase": "coupled_integral-differential_replicator_equations"}, {"score": 0.0023976048394496446, "phrase": "individual_agent_strategies"}, {"score": 0.0023003498123420237, "phrase": "functional_equations"}, {"score": 0.002259883463607508, "phrase": "steady_state"}, {"score": 0.0022201273900989416, "phrase": "replicator_dynamics"}], "paper_keywords": ["Multi-agent reinforcement learning", " Replicator dynamics", " Continuous strategies"], "paper_abstract": "The problem of multi-agent learning and adaptation has attracted a great deal of attention in recent years. It has been suggested that the dynamics of multi agent learning can be studied using replicator equations from population biology. Most existing studies so far have been limited to discrete strategy spaces with a small number of available actions. In many cases, however, the choices available to agents are better characterized by continuous spectra. This paper suggests a generalization of the replicator framework that allows to study the adaptive dynamics of Q-learning agents with continuous strategy spaces. Instead of probability vectors, agents' strategies are now characterized by probability measures over continuous variables. As a result, the ordinary differential equations for the discrete case are replaced by a system of coupled integral-differential replicator equations that describe the mutual evolution of individual agent strategies. We derive a set of functional equations describing the steady state of the replicator dynamics, examine their solutions for several two-player games, and confirm our analytical results using simulations.", "paper_title": "Continuous strategy replicator dynamics for multi-agent Q-learning", "paper_id": "WOS:000313004600002"}