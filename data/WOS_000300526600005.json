{"auto_keywords": [{"score": 0.03260519143539766, "phrase": "projected_gradient"}, {"score": 0.024920082969955962, "phrase": "proposed_multitask_learning_formulation"}, {"score": 0.00481495049065317, "phrase": "incoherent_sparse"}, {"score": 0.004662705558154439, "phrase": "multiple_tasks"}, {"score": 0.004494570360108983, "phrase": "incoherent_sparse_and_low-rank_patterns"}, {"score": 0.004273196830970426, "phrase": "linear_multitask_learning_formulation"}, {"score": 0.004176194589991187, "phrase": "sparse_and_low-rank_patterns"}, {"score": 0.004081385289516914, "phrase": "cardinality_regularization_term"}, {"score": 0.004025531820226182, "phrase": "low-rank_constraint"}, {"score": 0.0036553070575015344, "phrase": "semidefinite_programming"}, {"score": 0.0036218682585355895, "phrase": "small-size_problems"}, {"score": 0.0035233696341819437, "phrase": "general_projected_gradient_scheme"}, {"score": 0.0033651019362620866, "phrase": "optimization_formulation"}, {"score": 0.0033190180867511605, "phrase": "objective_function"}, {"score": 0.0032436052954996097, "phrase": "feasible_domain"}, {"score": 0.003027462227648001, "phrase": "global_convergence"}, {"score": 0.002985988377995789, "phrase": "projected_gradient_scheme"}, {"score": 0.00286493504971388, "phrase": "constrained_optimization_problem"}, {"score": 0.002786964045867666, "phrase": "optimal_solution"}, {"score": 0.002673957615079041, "phrase": "unconstrained_optimization_subproblem"}, {"score": 0.0026373136897864763, "phrase": "euclidean_projection_subproblem"}, {"score": 0.0023725232579531273, "phrase": "presented_projected_gradient_algorithms"}, {"score": 0.0023079225317714815, "phrase": "least_squares"}, {"score": 0.002276283303864666, "phrase": "experimental_results"}, {"score": 0.002224510044064184, "phrase": "real-world_data_sets"}, {"score": 0.0021049977753042253, "phrase": "proposed_projected_gradient_algorithms"}], "paper_keywords": ["Multitask learning", " low-rank and sparse patterns", " trace norm"], "paper_abstract": "We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multitask learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is nonconvex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We propose employing the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formulation, the objective function is nondifferentiable and the feasible domain is nontrivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of the projected gradient involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and a Euclidean projection subproblem. We also present two projected gradient algorithms and analyze their rates of convergence in detail. In addition, we illustrate the use of the presented projected gradient algorithms for the proposed multitask learning formulation using the least squares loss. Experimental results on a collection of real-world data sets demonstrate the effectiveness of the proposed multitask learning formulation and the efficiency of the proposed projected gradient algorithms.", "paper_title": "Learning Incoherent Sparse and Low-Rank Patterns from Multiple Tasks", "paper_id": "WOS:000300526600005"}