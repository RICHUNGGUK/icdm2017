{"auto_keywords": [{"score": 0.049770881888195125, "phrase": "baldwinian"}, {"score": 0.04607389008278537, "phrase": "lamarckian"}, {"score": 0.008745697414260222, "phrase": "first_problem"}, {"score": 0.008551094353618853, "phrase": "problem_landscape"}, {"score": 0.00809617830285141, "phrase": "baldwinian_learning"}, {"score": 0.00481495049065317, "phrase": "centroid-based_memetic_algorithm_-_adaptive_lamarckian"}, {"score": 0.004692227830462433, "phrase": "specific_learning_schemes"}, {"score": 0.004662036750452837, "phrase": "memetic_algorithms"}, {"score": 0.0045726187330434025, "phrase": "significant_impact"}, {"score": 0.004204459295724526, "phrase": "different_types"}, {"score": 0.004137119737646718, "phrase": "previous_studies"}, {"score": 0.004057728601458481, "phrase": "learning_schemes"}, {"score": 0.003967021436047945, "phrase": "single_optimisation_framework"}, {"score": 0.0036592457471262894, "phrase": "different_learning_scheme"}, {"score": 0.003635676878573833, "phrase": "noisy_design_optimisation"}, {"score": 0.003531474887552889, "phrase": "simple_probabilistic_approach"}, {"score": 0.0033971539563683174, "phrase": "centroid-based_approach"}, {"score": 0.0032051601987130207, "phrase": "effective_allocation"}, {"score": 0.003133451532433272, "phrase": "local_search_cost"}, {"score": 0.003033776234384367, "phrase": "optimisation_process"}, {"score": 0.002956316987429418, "phrase": "right_learning_scheme"}, {"score": 0.002880829737447174, "phrase": "right_time"}, {"score": 0.0028072645684856313, "phrase": "higher_search_performance"}, {"score": 0.0027622407190589326, "phrase": "empirical_study"}, {"score": 0.0026916956768363158, "phrase": "benchmark_problems"}, {"score": 0.0026314443645799913, "phrase": "simple_benchmark_problems"}, {"score": 0.0025892330566358503, "phrase": "static_and_gradient_information"}, {"score": 0.002427035025924384, "phrase": "slower_convergence"}, {"score": 0.0024035955457586103, "phrase": "second_problem"}, {"score": 0.002372693822379709, "phrase": "noisy_versions"}, {"score": 0.002260301305523054, "phrase": "random_noise_perturbation"}, {"score": 0.0022312377180023282, "phrase": "design_vector"}, {"score": 0.002181270821530302, "phrase": "favour_learning_processes"}, {"score": 0.0021672007484097575, "phrase": "re-sample_search"}, {"score": 0.0021049977753042253, "phrase": "cbma"}], "paper_keywords": ["memetic computing", " memetic algorithm", " non-linear optimisation", " Lamarckian process", " Baldwinian process"], "paper_abstract": "The application of specific learning schemes in memetic algorithms (MAs) can have significant impact on their performances. One main issue revolves around two different learning schemes, specifically, Lamarckian and Baldwinian. It has been shown that the two learning schemes are better suited for different types of problems and some previous studies have attempted to combine both learning schemes as a means to develop a single optimisation framework capable of solving more classes of problems. However, most of the past approaches are often implemented heuristically and have not investigated the effect of different learning scheme on noisy design optimisation. In this article, we introduce a simple probabilistic approach to address this issue. In particular, we investigate a centroid-based approach that combines the two learning schemes within an MA framework (centroid-based MS; CBMA) through the effective allocation of resources (in terms of local search cost) that are based on information obtained during the optimisation process itself. A scheme that applies the right learning scheme (Lamarckian or Baldwinian) at the right time (during search) would lead to higher search performance. We conducted an empirical study to test this hypothesis using two different types of benchmark problems. The first problem set consists of simple benchmark problems whereby the problem landscape is static and gradient information can be obtained accurately. These problems are known to favour Lamarckian learning while Baldwinian learning is known to exhibit slower convergence. The second problem set consists of noisy versions of the first problem set whereby the problem landscape is dynamic as a result of the random noise perturbation injected into the design vector. These problems are known to favour learning processes that re-sample search points such as Baldwinian learning. Our experiments show that CBMA manages to adaptively allocate resources productively according to problem in most of the cases.", "paper_title": "Centroid-based memetic algorithm - adaptive Lamarckian and Baldwinian learning", "paper_id": "WOS:000304361900002"}