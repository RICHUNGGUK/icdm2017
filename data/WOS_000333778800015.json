{"auto_keywords": [{"score": 0.04246301635594336, "phrase": "ordered_rule"}, {"score": 0.015483621666231859, "phrase": "associative_classifiers"}, {"score": 0.0085939698733547, "phrase": "feature_selection"}, {"score": 0.00481495049065317, "phrase": "small_number"}, {"score": 0.004616560363089716, "phrase": "accurate_model"}, {"score": 0.004563880494020726, "phrase": "individual_rule"}, {"score": 0.004460309246062286, "phrase": "existing_associative_classifiers"}, {"score": 0.004375789703278455, "phrase": "large_number"}, {"score": 0.003946195919158241, "phrase": "tree_model"}, {"score": 0.003669330508576073, "phrase": "non-leaf_node"}, {"score": 0.0035586268203884673, "phrase": "new_tree_model"}, {"score": 0.0031360786664165093, "phrase": "concise_rule_conditions"}, {"score": 0.003018196463874475, "phrase": "condition-based_classifier"}, {"score": 0.0028936226331006563, "phrase": "associative_classifier"}, {"score": 0.002784828661426853, "phrase": "rule_transformation_algorithm"}, {"score": 0.0027215202678634517, "phrase": "regular_binary_decision_trees"}, {"score": 0.002680114113899346, "phrase": "ordered_set"}, {"score": 0.0026393362602995254, "phrase": "simple_rule_conditions"}, {"score": 0.0025694584194998356, "phrase": "binary_representation"}, {"score": 0.002482320290902079, "phrase": "experimental_studies"}, {"score": 0.002435190484324197, "phrase": "competitive_accuracy_performance"}, {"score": 0.0023889533502525527, "phrase": "significantly_smaller_number"}, {"score": 0.0022815262950738814, "phrase": "well-known_associative_classifiers"}, {"score": 0.0022554348020494023, "phrase": "cba"}, {"score": 0.0021956951486095805, "phrase": "garc"}, {"score": 0.0021049977753042253, "phrase": "even_a_smaller_number"}], "paper_keywords": ["Association rule", " Decision tree", " Feature selection", " Rule-based classifier", " Rule pruning"], "paper_abstract": "Associative classifiers have been proposed to achieve an accurate model with each individual rule being interpretable. However, existing associative classifiers often consist of a large number of rules and, thus, can be difficult to interpret. We show that associative classifiers consisting of an ordered rule set can be represented as a tree model. From this view, it is clear that these classifiers are restricted in that at least one child node of a non-leaf node is never split. We propose a new tree model, i.e., condition-based tree (CBT), to relax the restriction. Furthermore, we also propose an algorithm to transform a CBT to an ordered rule set with concise rule conditions. This ordered rule set is referred to as a condition-based classifier (CBC). Thus, the interpretability of an associative classifier is maintained, but more expressive models are possible. The rule transformation algorithm can be also applied to regular binary decision trees to extract an ordered set of rules with simple rule conditions. Feature selection is applied to a binary representation of conditions to simplify/improve the models further. Experimental studies show that CBC has competitive accuracy performance, and has a significantly smaller number of rules (median of 10 rules per data set) than well-known associative classifiers such as CBA (median of 47) and GARC (median of 21). CBC with feature selection has even a smaller number of rules. (C) 2013 Elsevier B.V. All rights reserved.", "paper_title": "CBC: An associative classifier with a small number of rules", "paper_id": "WOS:000333778800015"}