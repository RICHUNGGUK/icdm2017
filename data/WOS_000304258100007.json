{"auto_keywords": [{"score": 0.04442850327214472, "phrase": "cost-based_performance"}, {"score": 0.04274518365190482, "phrase": "model-training_process"}, {"score": 0.014517673557390946, "phrase": "cost-sensitive_learning"}, {"score": 0.00481495049065317, "phrase": "decision_trees"}, {"score": 0.004772137871562584, "phrase": "cost-sensitive_classification"}, {"score": 0.004645959941491617, "phrase": "software_quality_prediction"}, {"score": 0.00458412183693466, "phrase": "empirical_study"}, {"score": 0.00419229472352669, "phrase": "software_quality_prediction_model"}, {"score": 0.0037157201349604222, "phrase": "defect_predictors"}, {"score": 0.0035851703423654432, "phrase": "cost-sensitive_learning_technique"}, {"score": 0.003505908076430666, "phrase": "six_different_cost-sensitive_learning_techniques"}, {"score": 0.003474694300991964, "phrase": "adacost"}, {"score": 0.0033079039414389833, "phrase": "random_undersampling"}, {"score": 0.003177392642813126, "phrase": "case_study"}, {"score": 0.0030113280682863234, "phrase": "unique_insight"}, {"score": 0.002944715025443487, "phrase": "defection_prediction_models"}, {"score": 0.0028032928749970026, "phrase": "misclassification_cost"}, {"score": 0.00269263690712392, "phrase": "practical_appeal"}, {"score": 0.0025863375961196005, "phrase": "software_quality_practitioner"}, {"score": 0.0025518429503461736, "phrase": "clear_process"}, {"score": 0.002473129577319634, "phrase": "model_training"}, {"score": 0.0024075916292945715, "phrase": "model_evaluation"}, {"score": 0.0023437863565921053, "phrase": "defect_prediction_model"}, {"score": 0.0023229000713081626, "phrase": "rus"}, {"score": 0.002271476019266231, "phrase": "best_cost-sensitive_technique"}, {"score": 0.0021526564019119466, "phrase": "john_wiley"}, {"score": 0.0021049977753042253, "phrase": "wires_data_mining_knowl"}], "paper_keywords": [""], "paper_abstract": "This empirical study investigates two commonly used decision tree classification algorithms in the context of cost-sensitive learning. A review of the literature shows that the cost-based performance of a software quality prediction model is usually determined after the model-training process has been completed. In contrast, we incorporate cost-sensitive learning during the model-training process. The C4.5 and Random Forest decision tree algorithms are used to build defect predictors either with, or without, any cost-sensitive learning technique. The paper investigates six different cost-sensitive learning techniques: AdaCost, Adc2, Csb2, MetaCost, Weighting, and Random Undersampling (RUS). The data come from case study include 15 software measurement datasets obtained from several high-assurance systems. In addition, to a unique insight into the cost-based performance of defection prediction models, this study is one of the first to use misclassification cost as a parameter during the model-training process. The practical appeal of this research is that it provides a software quality practitioner with a clear process for how to consider (during model training) and analyze (during model evaluation) the cost-based performance of a defect prediction model. RUS is ranked as the best cost-sensitive technique among those considered in this study. (C) 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 448-459 DOI:10.1002/widm.38", "paper_title": "The use of decision trees for cost-sensitive classification: an empirical study in software quality prediction", "paper_id": "WOS:000304258100007"}