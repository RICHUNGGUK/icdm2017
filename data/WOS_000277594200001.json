{"auto_keywords": [{"score": 0.048019249999775485, "phrase": "decision_making"}, {"score": 0.00481495049065317, "phrase": "reward-modulated_hebbian_learning"}, {"score": 0.004294962474996283, "phrase": "linear_neuron"}, {"score": 0.003985602342418578, "phrase": "synaptic_weight"}, {"score": 0.0036660436272785476, "phrase": "postsynaptic_neurons"}, {"score": 0.003539252646181146, "phrase": "particular_action"}, {"score": 0.0034319005131102495, "phrase": "candidate_actions"}, {"score": 0.0033868916699805224, "phrase": "winner-take-all_operation"}, {"score": 0.003101503914610488, "phrase": "global_reward_signal"}, {"score": 0.003020662943371515, "phrase": "hebb"}, {"score": 0.002890557197908352, "phrase": "pre-_and_postsynaptic_neurons"}, {"score": 0.0028276174348291923, "phrase": "weighted_sum"}, {"score": 0.002790511479398411, "phrase": "presynaptic_inputs"}, {"score": 0.002753891109273246, "phrase": "postsynaptic_neuron"}, {"score": 0.002705808393126695, "phrase": "perceptron_learning_rule"}, {"score": 0.002670296619878048, "phrase": "rescorla-wagner_rule"}, {"score": 0.0026352496813713292, "phrase": "simple_approach"}, {"score": 0.0026121403051734744, "phrase": "action-selection_learning"}, {"score": 0.002555247330102505, "phrase": "sensory_inputs"}, {"score": 0.002499590390751339, "phrase": "bayesian_decision_stage"}, {"score": 0.0024667783849055634, "phrase": "suitably_preprocessed_form"}, {"score": 0.002381365139274329, "phrase": "larger_timescale"}, {"score": 0.002339771435910601, "phrase": "salient_dependencies"}, {"score": 0.0023192471808039746, "phrase": "input_features"}, {"score": 0.0022687189700002254, "phrase": "fast_learning"}, {"score": 0.0022192891418394514, "phrase": "interesting_new_hypotheses"}, {"score": 0.0021998194157393353, "phrase": "neural_nodes"}, {"score": 0.0021805201234139475, "phrase": "computational_goals"}, {"score": 0.0021613897794439227, "phrase": "cortical_areas"}, {"score": 0.0021049977753042253, "phrase": "final_decision_stage"}], "paper_keywords": [""], "paper_abstract": "We introduce a framework for decision making in which the learning of decision making is reduced to its simplest and biologically most plausible form: Hebbian learning on a linear neuron. We cast our Bayesian-Hebb learning rule as reinforcement learning in which certain decisions are rewarded and prove that each synaptic weight will on average converge exponentially fast to the log-odd of receiving a reward when its pre- and postsynaptic neurons are active. In our simple architecture, a particular action is selected from the set of candidate actions by a winner-take-all operation. The global reward assigned to this action then modulates the update of each synapse. Apart from this global reward signal, our reward-modulated Bayesian Hebb rule is a pure Hebb update that depends only on the coactivation of the pre- and postsynaptic neurons, not on the weighted sum of all presynaptic inputs to the postsynaptic neuron as in the perceptron learning rule or the Rescorla-Wagner rule. This simple approach to action-selection learning requires that information about sensory inputs be presented to the Bayesian decision stage in a suitably preprocessed form resulting from other adaptive processes (acting on a larger timescale) that detect salient dependencies among input features. Hence our proposed framework for fast learning of decisions also provides interesting new hypotheses regarding neural nodes and computational goals of cortical areas that provide input to the final decision stage.", "paper_title": "Reward-Modulated Hebbian Learning of Decision Making", "paper_id": "WOS:000277594200001"}