{"auto_keywords": [{"score": 0.04826300170743257, "phrase": "shannon_code"}, {"score": 0.006494848226492878, "phrase": "average_redundancy"}, {"score": 0.004392565073466714, "phrase": "optimal_code"}, {"score": 0.0042670138420098916, "phrase": "huffman"}, {"score": 0.003987755561492638, "phrase": "worst_case"}, {"score": 0.0037267132967205136, "phrase": "worst_case_viewpoint"}, {"score": 0.0033830587647810132, "phrase": "average_point"}, {"score": 0.0032232653428491435, "phrase": "random_variable"}, {"score": 0.0027341763737751467, "phrase": "huffman_code"}, {"score": 0.002252701813819903, "phrase": "alphabet_size"}], "paper_keywords": ["Average redundancy", " Huffman code", " Shannon code"], "paper_abstract": "In order to determine how suboptimal the Shannon code is, one should compare its performance with that of the optimal code, i.e., the corresponding Huffman code, in some sense. It is well known that in the worst case the redundancy of both the Shannon and Huffman codes can be arbitrarily close to 1. Beyond this worst case viewpoint, very little is known. In this paper, we compare the performance of these codes from an average point of view. The redundancy is considered as a random variable on the set of all sources with symbols and its average is evaluated. It is shown that the average redundancy of the Shannon code is very close to 0.5 bits, whereas the average redundancy of the Huffman code is less than n(-1) (1 + ln n) + 0.086 bits. It is also proven that the variance of the redundancy of the Shannon code tends to zero as increases. Therefore, for sources with alphabet size, the redundancy of the Shannon code is approximately 0.5 bits with probability approaching 1 as n -> infinity.", "paper_title": "How Suboptimal Is the Shannon Code?", "paper_id": "WOS:000312896600027"}