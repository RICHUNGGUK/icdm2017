{"auto_keywords": [{"score": 0.040301904488830594, "phrase": "failure_proximity"}, {"score": 0.006953714574218716, "phrase": "six_failure_proximities"}, {"score": 0.00481495049065317, "phrase": "systematic_study_of_failure_proximity"}, {"score": 0.00478778190213169, "phrase": "software_end_users"}, {"score": 0.004747314816584038, "phrase": "best_testers"}, {"score": 0.0045888223900501695, "phrase": "in-house_testing"}, {"score": 0.004486098156258553, "phrase": "failure_reporting_components"}, {"score": 0.0044105602025248936, "phrase": "released_software"}, {"score": 0.004360906420260988, "phrase": "watson_system"}, {"score": 0.004287467395064377, "phrase": "mozilla_quality_feedback_agent"}, {"score": 0.004144263368848345, "phrase": "collected_failure_data"}, {"score": 0.004097595199141165, "phrase": "effective_failure_indexing_technique"}, {"score": 0.004028572475177205, "phrase": "optimal_case"}, {"score": 0.0037532257703888315, "phrase": "indexing_technique"}, {"score": 0.003627800727103479, "phrase": "first_systematic_study"}, {"score": 0.0034279721355412285, "phrase": "distance_function"}, {"score": 0.003370190834214415, "phrase": "extracted_signatures"}, {"score": 0.003239114833599948, "phrase": "different_instantiations"}, {"score": 0.0030005055373644096, "phrase": "simplest_approach"}, {"score": 0.002975099633631871, "phrase": "failure_points"}, {"score": 0.0029249294679606656, "phrase": "fault_localization_algorithms"}, {"score": 0.00290016162105678, "phrase": "failure_signatures"}, {"score": 0.0028674628275126824, "phrase": "technical_details"}, {"score": 0.0026940816246330494, "phrase": "systematic_view"}, {"score": 0.002656158309155626, "phrase": "fair_comparison"}, {"score": 0.0026113524778702624, "phrase": "first_set"}, {"score": 0.0025965853546625352, "phrase": "evaluation_metrics"}, {"score": 0.0025455532832484367, "phrase": "different_failure_proximities"}, {"score": 0.0024884548152669496, "phrase": "six_proximities"}, {"score": 0.0023646113279358646, "phrase": "proposed_metrics"}, {"score": 0.0023445770108033288, "phrase": "experimental_result"}, {"score": 0.0022405528759521856, "phrase": "systematic_study"}], "paper_keywords": ["Failure analysis", " failure indexing", " failure proximity", " fault localization", " debugging aids", " statistical debugging", " software maintenance"], "paper_abstract": "Software end users are the best testers, who keep revealing bugs in software that has undergone rigorous in-house testing. In order to leverage their testing efforts, failure reporting components have been widely deployed in released software: The Microsoft Dr.Watson System [1] and the Mozilla Quality Feedback Agent [2] are the two most typical examples. Many utilities of the collected failure data depend on an effective failure indexing technique, which, in the optimal case, would index all failures caused by the same bug together. Unfortunately, the problem of failure proximity, which underpins the effectiveness of an indexing technique, has not been systematically studied. This paper presents the first systematic study of failure proximity. A failure proximity consists of two components: a fingerprinting function that extracts signatures from failures and a distance function that calculates (from the extracted signatures) the likelihood of two failures being due to the same bug. By considering different instantiations of the two functions, we study an array of six failure proximities (two of them are new) in this paper. These proximities range from the simplest approach which checks failure points to the most sophisticated approach which utilizes fault localization algorithms to extract failure signatures. Besides presenting technical details of each proximity, we also study the properties of each proximity and trade-offs between proximities. Altogether these deliver a systematic view of failure proximity. For fair comparison, this study proposes the first set of evaluation metrics that objectively quantifies the effectiveness of different failure proximities. We carry out three case studies of the six proximities on three mid-sized programs (namely, flex, grep, and gzip) and evaluate their effectiveness using the proposed metrics. The experimental result clearly validates our identified properties and trade-offs. In summary, this study not only presents a systematic study of six failure proximities, the problem formulation, the proposed metrics, and the experimental result, but would also help guide further investigation in the future.", "paper_title": "A Systematic Study of Failure Proximity", "paper_id": "WOS:000261175200008"}