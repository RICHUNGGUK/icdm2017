{"auto_keywords": [{"score": 0.035874878891473845, "phrase": "posterior_probabilities"}, {"score": 0.00481495049065317, "phrase": "high-dimensional_linear_regression"}, {"score": 0.00475471531245055, "phrase": "ising"}, {"score": 0.004234532601573088, "phrase": "important_and_challenging_component"}, {"score": 0.004109454074072009, "phrase": "machine_learning"}, {"score": 0.004074397977869244, "phrase": "feature_selection"}, {"score": 0.00350696416714684, "phrase": "new_approach"}, {"score": 0.003374250301861101, "phrase": "feature_relevance"}, {"score": 0.0032050528424273994, "phrase": "regression_problem"}, {"score": 0.003031287195944274, "phrase": "marginal_posterior_probabilities"}, {"score": 0.0028916124463906983, "phrase": "ising_model"}, {"score": 0.0028669153591624696, "phrase": "weak_couplings"}, {"score": 0.0028181504086563967, "phrase": "mean_field_approximation"}, {"score": 0.0026882700311968025, "phrase": "feature_selection_path"}, {"score": 0.0025099367332615794, "phrase": "analytical_results"}, {"score": 0.002414861891567233, "phrase": "simple_regression_problems"}, {"score": 0.0022936610196951962, "phrase": "high-dimensional_regression"}, {"score": 0.002254624822004988, "phrase": "gene_expression"}, {"score": 0.0021049977753042253, "phrase": "bayesian_feature_selection"}], "paper_keywords": [""], "paper_abstract": "Motivation: Feature selection, identifying a subset of variables that are relevant for predicting a response, is an important and challenging component of many methods in statistics and machine learning. Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples, as is often the case for many genomic datasets. Results: Here, we introduce a new approach-the Bayesian Ising Approximation (BIA)-to rapidly calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the regime where the regression problem is strongly regularized by the prior, we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model with weak couplings. Using a mean field approximation, we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems. Finally, we demonstrate the applicability of the BIA to high-dimensional regression by analyzing a gene expression dataset with nearly 30 000 features. These results also highlight the impact of correlations between features on Bayesian feature selection.", "paper_title": "Bayesian feature selection for high-dimensional linear regression via the Ising approximation with applications to genomics", "paper_id": "WOS:000356625300008"}