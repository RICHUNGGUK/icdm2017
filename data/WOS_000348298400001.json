{"auto_keywords": [{"score": 0.026793043670423025, "phrase": "csiszar"}, {"score": 0.00481495049065317, "phrase": "symmetric_divergence_measures"}, {"score": 0.004271490815763164, "phrase": "tight_bounds"}, {"score": 0.0038348235692738783, "phrase": "total_variation_distance"}, {"score": 0.0031654684543975077, "phrase": "two-_or_three-element_probability_distributions"}, {"score": 0.0028758391869611374, "phrase": "lossless_source_coding"}, {"score": 0.0021821915946356168, "phrase": "yardi_et_al"}, {"score": 0.0021049977753042253, "phrase": "channel-code_detection"}], "paper_keywords": ["Bhattacharyya distance", " Chernoff information", " f -divergence", " Jeffreys' divergence", " lossless source coding", " total variation distance"], "paper_abstract": "Tight bounds for several symmetric divergence measures are derived in terms of the total variation distance. It is shown that each of these bounds is attained by a pair of two- or three-element probability distributions. An application of these bounds for lossless source coding is provided, refining and improving a certain bound by Csiszar. Another application of these bounds has been recently introduced by Yardi et al. for channel-code detection.", "paper_title": "Tight Bounds for Symmetric Divergence Measures and a Refined Bound for Lossless Source Coding", "paper_id": "WOS:000348298400001"}