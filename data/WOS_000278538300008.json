{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "neural_network_modeling_study"}, {"score": 0.004723322467153553, "phrase": "external_events"}, {"score": 0.004611224230543916, "phrase": "different_sensory_information"}, {"score": 0.004523455475492876, "phrase": "strong_evidence"}, {"score": 0.004480196733208298, "phrase": "visual-tactile_integrations"}, {"score": 0.004394910907549057, "phrase": "visual_and_tactile_information"}, {"score": 0.0042495320879288615, "phrase": "challenging_problem"}, {"score": 0.004148631410346188, "phrase": "neural_network_model"}, {"score": 0.004011366422179016, "phrase": "underlying_visual-tactile_interactions"}, {"score": 0.003750260412839128, "phrase": "feedforward_connections"}, {"score": 0.003696550794743764, "phrase": "downstream_bimodal_area"}, {"score": 0.003643607569483598, "phrase": "unimodal_areas"}, {"score": 0.0034558655164987134, "phrase": "bimodal_area"}, {"score": 0.003422781254306241, "phrase": "direct_reciprocal_connections"}, {"score": 0.003293570937654716, "phrase": "visual-tactile_interactions"}, {"score": 0.0032308006242867224, "phrase": "faint_tactile_stimuli"}, {"score": 0.003169222811082848, "phrase": "concomitant_visual_input"}, {"score": 0.0030790429005600898, "phrase": "visual_information"}, {"score": 0.0029914213495838998, "phrase": "poor_unisensory_information"}, {"score": 0.002837188425640173, "phrase": "conflict_situations"}, {"score": 0.002690886043654469, "phrase": "distinct_roles"}, {"score": 0.0026268961477241026, "phrase": "direct_synapses"}, {"score": 0.0025034338848015166, "phrase": "low-intensity_tactile_stimuli"}, {"score": 0.00247944491239733, "phrase": "cross-modal_stimulation"}, {"score": 0.0023743009016047424, "phrase": "visual_enhancement"}, {"score": 0.0023515464194592195, "phrase": "tactile_spatial_localization"}, {"score": 0.0022956075241520064, "phrase": "better_comprehension"}, {"score": 0.002198242191529977, "phrase": "neural_system"}, {"score": 0.002156301969685775, "phrase": "physiological_knowledge"}, {"score": 0.0021356322417874106, "phrase": "clinical_practice"}, {"score": 0.0021049977753042253, "phrase": "technological_applications"}], "paper_keywords": ["Inverse effectiveness", " multisensory conflict", " multisensory perception", " tactile processing", " visual-tactile enhancement"], "paper_abstract": "Perception of external events often depends on integrating different sensory information. Many studies show strong evidence for visual-tactile integrations. Understanding how visual and tactile information are merged together is still a challenging problem. Here, a neural network model was used to investigate the mechanisms underlying visual-tactile interactions. It includes two unimodal areas (visual and tactile, respectively), sending feedforward connections into a downstream bimodal area. The unimodal areas influence each other via two synaptic mechanisms: feedback synapses from the bimodal area and direct reciprocal connections. The network reproduces a variety of visual-tactile interactions: 1) detection of faint tactile stimuli is facilitated by concomitant visual input; 2) tactile spatial resolution is improved by visual information; 3) cross-modal advantages are maximum when poor unisensory information is available (inverse effectiveness); and 4) conflict situations are resolved based on the more reliable sensory cue. The model identifies distinct roles for the feedback and direct synapses: the first are fundamental to improve detection of low-intensity tactile stimuli in cross-modal stimulation, and the second are mostly implicated in visual enhancement of tactile spatial localization and resolution. A better comprehension of how vision and touch interact in the neural system may contribute to physiological knowledge, clinical practice, and technological applications.", "paper_title": "Integrating Information From Vision and Touch: A Neural Network Modeling Study", "paper_id": "WOS:000278538300008"}