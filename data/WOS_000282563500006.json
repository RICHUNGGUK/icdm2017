{"auto_keywords": [{"score": 0.04630684633148268, "phrase": "posed_expressions"}, {"score": 0.04071201963678792, "phrase": "emotion_intensity"}, {"score": 0.00481495049065317, "phrase": "spontaneous_speech"}, {"score": 0.004553510708976456, "phrase": "previous_studies"}, {"score": 0.004519736937243476, "phrase": "vocal_expression"}, {"score": 0.004306205029641728, "phrase": "large_corpus"}, {"score": 0.004274257668843717, "phrase": "authentic_affective_speech"}, {"score": 0.004226778595997366, "phrase": "real-life_voice_controlled_telephone_services"}, {"score": 0.0039822874870125095, "phrase": "perceived_irritation"}, {"score": 0.00383668723824441, "phrase": "selected_utterances"}, {"score": 0.003601235776892337, "phrase": "comprehensive_set"}, {"score": 0.0035744997458371335, "phrase": "acoustic_measures"}, {"score": 0.0034566143156030426, "phrase": "voice_source"}, {"score": 0.003418186564522323, "phrase": "temporal_characteristics"}, {"score": 0.0030451958332949735, "phrase": "listeners'_ratings"}, {"score": 0.002911961514073769, "phrase": "acoustic_correlates"}, {"score": 0.0027949428646076627, "phrase": "previous_findings"}, {"score": 0.002722930209707764, "phrase": "effect_sizes"}, {"score": 0.0026726283357752585, "phrase": "authentic_expressions"}, {"score": 0.0025940660106162404, "phrase": "lda_classifiers"}, {"score": 0.0025461386232786356, "phrase": "speaker_adaptation"}, {"score": 0.0024075916292945715, "phrase": "human_performance"}, {"score": 0.0023807981747706376, "phrase": "human_listeners"}, {"score": 0.002319431279824863, "phrase": "individual_utterances"}, {"score": 0.0021526564019119466, "phrase": "future_research"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Acoustic features", " Automatic speech classification", " Emotion recognition", " Human-computer interaction", " Spontaneous speech"], "paper_abstract": "The majority of previous studies on vocal expression have been conducted on posed expressions. In contrast, we utilized a large corpus of authentic affective speech recorded from real-life voice controlled telephone services. Listeners rated a selection of 200 utterances from this corpus with regard to level of perceived irritation, resignation, neutrality, and emotion intensity. The selected utterances came from 64 different speakers who each provided both neutral and affective stimuli. All utterances were further automatically analyzed regarding a comprehensive set of acoustic measures related to F0, intensity, formants, voice source, and temporal characteristics of speech. Results first showed that several significant acoustic differences were found between utterances classified as neutral and utterances classified as irritated or resigned using a within-persons design. Second, listeners' ratings on each scale were associated with several acoustic measures. In general the acoustic correlates of irritation, resignation, and emotion intensity were similar to previous findings obtained with posed expressions, though the effect sizes were smaller for the authentic expressions. Third, automatic classification (using LDA classifiers both with and without speaker adaptation) of irritation, resignation, and neutral performed at a level comparable to human performance, though human listeners and machines did not necessarily classify individual utterances similarly. Fourth, clearly perceived exemplars of irritation and resignation were rare in our corpus. These findings were discussed in relation to future research. (C) 2010 Elsevier Ltd. All rights reserved.", "paper_title": "Expression of affect in spontaneous speech: Acoustic correlates and automatic detection of irritation and resignation", "paper_id": "WOS:000282563500006"}