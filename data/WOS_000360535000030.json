{"auto_keywords": [{"score": 0.03455018268948868, "phrase": "scoped_synchronization"}, {"score": 0.00481495049065317, "phrase": "commodity_heterogeneous_systems"}, {"score": 0.0043863636475983845, "phrase": "global_communication"}, {"score": 0.004330941610682453, "phrase": "heterogeneous_system"}, {"score": 0.00422218060227958, "phrase": "homogeneous_cpu_systems"}, {"score": 0.00416882434439484, "phrase": "synchronization_mechanisms"}, {"score": 0.0037495207047193034, "phrase": "scoped_operations"}, {"score": 0.0036090884921108086, "phrase": "formal_and_approachable_model"}, {"score": 0.0033865859510157238, "phrase": "portability_and_performance_issues"}, {"score": 0.0032321559007422087, "phrase": "new_class"}, {"score": 0.003204841443605308, "phrase": "memory_consistency_models"}, {"score": 0.0031375577552605533, "phrase": "data-race-free_models"}, {"score": 0.0030588424160997328, "phrase": "java"}, {"score": 0.0030199761431186434, "phrase": "sequential_consistency"}, {"score": 0.002956567356402432, "phrase": "hrf"}, {"score": 0.0029067881142952664, "phrase": "new_models"}, {"score": 0.002833689453299307, "phrase": "\"sufficient\"_synchronization"}, {"score": 0.002750721367040336, "phrase": "\"sufficient\"_scope"}, {"score": 0.002548331088496964, "phrase": "highly_regular_parallelism"}, {"score": 0.002360796860462937, "phrase": "different_scopes"}, {"score": 0.002311192286067765, "phrase": "transitive_communication"}, {"score": 0.002215081186105616, "phrase": "forward-looking_programs"}, {"score": 0.0021049977753042253, "phrase": "task_runtime"}], "paper_keywords": ["memory consistency model", " heterogeneous systems", " data-race-free", " task runtime"], "paper_abstract": "Commodity heterogeneous systems (e.g., integrated CPUs and GPUs), now support a unified, shared memory address space for all components. Because the latency of global communication in a heterogeneous system can be prohibitively high, heterogeneous systems (unlike homogeneous CPU systems) provide synchronization mechanisms that only guarantee ordering among a subset of threads, which we call a scope. Unfortunately, the consequences and semantics of these scoped operations are not yet well understood. Without a formal and approachable model to reason about the behavior of these operations, we risk an array of portability and performance issues. In this paper, we embrace scoped synchronization with a new class of memory consistency models that add scoped synchronization to data-race-free models like those of C++ and Java. Called sequential consistency for heterogeneousrace- free (SC for HRF), the new models guarantee SC for programs with \"sufficient\" synchronization (no data races) of \"sufficient\" scope. We discuss two such models. The first, HRF-direct, works well for programs with highly regular parallelism. The second, HRF-indirect, builds on HRF-direct by allowing synchronization using different scopes in some cases involving transitive communication. We quantitatively show that HRF-indirect encourages forward-looking programs with irregular parallelism by showing up to a 10% performance increase in a task runtime for GPUs.", "paper_title": "Heterogeneous-race-free Memory Models", "paper_id": "WOS:000360535000030"}