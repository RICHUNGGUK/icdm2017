{"auto_keywords": [{"score": 0.03128898671324493, "phrase": "vgbp"}, {"score": 0.00481495049065317, "phrase": "real-time_motion_control"}, {"score": 0.004750178261675602, "phrase": "value-gradient_based_policy"}, {"score": 0.00441909542037375, "phrase": "optimal_control_policy"}, {"score": 0.004300951511110487, "phrase": "unknown_environment"}, {"score": 0.004243167053054154, "phrase": "rl"}, {"score": 0.004110993414942228, "phrase": "optimal_control_problems"}, {"score": 0.003965061486705487, "phrase": "main_causes"}, {"score": 0.00391167729353583, "phrase": "inefficient_use"}, {"score": 0.003671865026158296, "phrase": "prior_knowledge"}, {"score": 0.00357362651129581, "phrase": "control_task"}, {"score": 0.0034780071331554003, "phrase": "learning_speed"}, {"score": 0.003400274964240732, "phrase": "learning_rate_parameter"}, {"score": 0.00298238929037041, "phrase": "main_difference"}, {"score": 0.0028764052339926775, "phrase": "sarsa"}, {"score": 0.0027867539356701302, "phrase": "learning_agent"}, {"score": 0.0027491902732542013, "phrase": "direct_access"}, {"score": 0.002712131567923321, "phrase": "reward_function"}, {"score": 0.002568812627436125, "phrase": "process_model"}, {"score": 0.0024774888010414206, "phrase": "control_actions"}, {"score": 0.0024220634963019114, "phrase": "right-hand_side"}, {"score": 0.002389403849342894, "phrase": "bellman_equation"}, {"score": 0.00233594452829104, "phrase": "fast_learning_convergence"}, {"score": 0.0022631003204259224, "phrase": "underactuated_pendulum_swing-up_task"}, {"score": 0.0021925226977381244, "phrase": "experimental_results"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Reinforcement learning", " Process model", " Robotics", " Local linear regression", " Least squares temporal difference"], "paper_abstract": "Reinforcement learning (RL) is a framework that enables a controller to find an optimal control policy for a task in an unknown environment. Although RL has been successfully used to solve optimal control problems, learning is generally slow. The main causes are the inefficient use of information collected during interaction with the system and the inability to use prior knowledge on the system or the control task. In addition, the learning speed heavily depends on the learning rate parameter, which is difficult to tune. In this paper, we present a sample-efficient, learning-rate-free version of the Value-Gradient Based Policy (VGBP) algorithm. The main difference between VGBP and other frequently used algorithms, such as Sarsa, is that in VGBP the learning agent has a direct access to the reward function, rather than just the immediate reward values. Furthermore, the agent learns a process model. This enables the algorithm to select control actions by optimizing over the right-hand side of the Bellman equation. We demonstrate the fast learning convergence in simulations and experiments with the underactuated pendulum swing-up task. In addition, we present experimental results for a more complex 2-DOF robotic manipulator. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Learning rate free reinforcement learning for real-time motion control using a value-gradient based policy", "paper_id": "WOS:000347499900007"}