{"auto_keywords": [{"score": 0.04649562408446413, "phrase": "infiniband"}, {"score": 0.00481495049065317, "phrase": "high_performance_rdma_protocols"}, {"score": 0.004761850836420121, "phrase": "hpc._modern_network_communication_libraries"}, {"score": 0.004683292152590589, "phrase": "remote_directory_memory_access"}, {"score": 0.004555217356295278, "phrase": "os_bypass_protocols"}, {"score": 0.004191540451296395, "phrase": "significant_performance_advantages"}, {"score": 0.0039433581938239926, "phrase": "hidden_per_buffer_setup_costs"}, {"score": 0.003509487090932835, "phrase": "existing_techniques"}, {"score": 0.003283272764794988, "phrase": "pipeline_algorithm"}, {"score": 0.003123203316511694, "phrase": "memory_registration"}, {"score": 0.003088702479841264, "phrase": "rdma_operations"}, {"score": 0.0029544572689681934, "phrase": "large-memory_usage_pattern"}, {"score": 0.002718231974492536, "phrase": "registered_memory"}, {"score": 0.002445865715873089, "phrase": "memory_buffers"}, {"score": 0.0023525345110929326, "phrase": "superior_performance"}, {"score": 0.0023136311468961125, "phrase": "effective_bandwidth_benchmark"}, {"score": 0.0022377394845878268, "phrase": "open_mpi's_pml"}, {"score": 0.0021763973978248005, "phrase": "messaging_layer"}, {"score": 0.0021049977753042253, "phrase": "'pipeline'_protocol"}], "paper_keywords": [""], "paper_abstract": "Modern network communication libraries that leverage Remote Directory Memory Access (RDMA) and OS bypass protocols, such as Infiniband [2] and Myrinet [10] can offer significant performance advantages over conventional send/receive protocols. However, this performance often comes with hidden per buffer setup costs [4]. This paper describes a unique long-message MPI [9] library 'pipeline' protocol that addresses these constraints while avoiding some of the pitfalls of existing techniques. By using portable send/receive semantics to hide the cost of initializing the pipeline algorithm, and then effectively overlapping the cost of memory registration with RDMA operations, this protocol provides very good performance for any large-memory usage pattern. This approach avoids the use of non-portable memory hooks or keeping registered memory from being returned to the OS. Through this approach, bandwidth may be increased up to 67% when memory buffers are not effectively reused while providing superior performance in the effective bandwidth benchmark. Several user level protocols are explored using Open MPI's PML (Point to point messaging layer) and compared/contrasted to this 'pipeline' protocol.", "paper_title": "High performance RDMA protocols in HPC", "paper_id": "WOS:000241557200008"}