{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "laplacian"}, {"score": 0.04238370962483186, "phrase": "basis_functions"}, {"score": 0.028342423256945797, "phrase": "proposed_approach"}, {"score": 0.0228766605035782, "phrase": "proposed_framework"}, {"score": 0.004671803485255806, "phrase": "markov_decision_processes"}, {"score": 0.004567230180529598, "phrase": "novel_spectral_framework"}, {"score": 0.004517080125395335, "phrase": "markov"}, {"score": 0.004332199142914195, "phrase": "optimal_policies"}, {"score": 0.004283423748543866, "phrase": "major_components"}, {"score": 0.004002044115117644, "phrase": "symmetric_diffusion_operators"}, {"score": 0.003927203683176278, "phrase": "specific_instantiation"}, {"score": 0.003853757393357659, "phrase": "global_basis_functions"}, {"score": 0.0038247634489737142, "phrase": "proto-value_functions"}, {"score": 0.003614124997453808, "phrase": "undirected_graph"}, {"score": 0.0035734052458933547, "phrase": "state_transitions"}, {"score": 0.003313380246267812, "phrase": "sample_collection_phase"}, {"score": 0.002837811982066224, "phrase": "rpi_framework"}, {"score": 0.0028164399450837465, "phrase": "least-squares_policy_iteration"}, {"score": 0.0027532820796396713, "phrase": "parameter_estimation_method"}, {"score": 0.0026511422582578027, "phrase": "large_discrete_and_continuous_state_spaces"}, {"score": 0.0026113590844098374, "phrase": "nystrom"}, {"score": 0.0025819015229390663, "phrase": "out-of-sample_interpolation"}, {"score": 0.0025049752758014602, "phrase": "kronecker_sum_factorization"}, {"score": 0.002476721068743341, "phrase": "compact_eigenfunctions"}, {"score": 0.002458061764297103, "phrase": "product_spaces"}, {"score": 0.002430335416564108, "phrase": "factored_mdps"}, {"score": 0.0023490143960001675, "phrase": "illustrative_discrete_and_continuous_control_tasks"}, {"score": 0.002129020447793464, "phrase": "large_mdps"}], "paper_keywords": ["Markov decision processes", " reinforcement learning", " value function approximation", " manifold learning", " spectral graph theory"], "paper_abstract": "This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A specific instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a final parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A specific instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystrom extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are briefly summarized at the end.", "paper_title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes", "paper_id": "WOS:000252744800001"}