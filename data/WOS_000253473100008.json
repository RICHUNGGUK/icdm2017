{"auto_keywords": [{"score": 0.0047057084749407485, "phrase": "intra_prediction"}, {"score": 0.004662705558154439, "phrase": "inter_prediction"}, {"score": 0.00443308765153716, "phrase": "processing_cycles"}, {"score": 0.004392565073466714, "phrase": "computation_complexity"}, {"score": 0.004273196830970426, "phrase": "huge_number"}, {"score": 0.004234129588974058, "phrase": "memory_accesses"}, {"score": 0.00410017409033388, "phrase": "total_decoding_cycles"}, {"score": 0.0038981499110532307, "phrase": "vlsi_implementation"}, {"score": 0.0038447943243941685, "phrase": "novel_power-efficient_and_highly_self-adaptive_prediction_engine"}, {"score": 0.0036052633308076933, "phrase": "different_prediction_requirements"}, {"score": 0.003427540517249369, "phrase": "correlated_memory_accesses"}, {"score": 0.0032436052954996097, "phrase": "unnecessary_decoding_operations"}, {"score": 0.003213920601037463, "phrase": "energy_dissipation"}, {"score": 0.0031553611552558986, "phrase": "fixed_real-time_throughput"}, {"score": 0.003097865379563127, "phrase": "conventional_designs"}, {"score": 0.002985988377995789, "phrase": "higher_efficiency"}, {"score": 0.0029586543333524904, "phrase": "lower_power_consumption"}, {"score": 0.00286493504971388, "phrase": "redundant_operations"}, {"score": 0.002825681384008326, "phrase": "wide_employment"}, {"score": 0.0027614468443073028, "phrase": "parallel_processing"}, {"score": 0.002723607240517261, "phrase": "different_prediction_modes"}, {"score": 0.002472822097373027, "phrase": "proposed_prediction_engine"}, {"score": 0.00226583335721762, "phrase": "whole_chip_area"}], "paper_keywords": ["adaptive", " decoding", " H.264", " parallel processing", " pipeline", " prediction", " VLSI"], "paper_abstract": "Prediction, including intra prediction and inter prediction, is the most critical issue in H.264/AVC decoding in terms of processing cycles and computation complexity. These two predictions demand a huge number of memory accesses and account for up to 80% of the total decoding cycles. In this paper, we present the design and VLSI implementation of a novel power-efficient and highly self-adaptive prediction engine that utilizes a 4 x 4 block level pipeline. Based on the different prediction requirements, the prediction pipeline stages, as well as the correlated memory accesses and datapaths, are fully adjustable, which helps to reduce unnecessary decoding operations and energy dissipation while retaining the fixed real-time throughput. Compared with conventional designs, this paper has the advantage of higher efficiency and lower power consumption due to the elimination of all redundant operations and the wide employment of the pipeline and parallel processing. Under different prediction modes, our design is able to decode each macroblock within 500 cycles. A prototype H.264/AVC baseline decoder chip that utilizes the proposed prediction engine is fabricated with UMC 0.18-mu m CMOS IP6 M technology. The prediction engine contains 79 K gates and 2.8 kb single-port on-chip SRAM, and occupies half of the whole chip area. When running at 1.5 MHz for QCIF 30 Us real-time decoding, the prediction engine dissipates 268 mu W at a 1.8-V power supply.", "paper_title": "A power-efficient and self-adaptive prediction engine for H.264/AVC decoding", "paper_id": "WOS:000253473100008"}