{"auto_keywords": [{"score": 0.04393033017315467, "phrase": "feature_vector"}, {"score": 0.00481495049065317, "phrase": "pruned_dependency_patterns"}, {"score": 0.004685012337701536, "phrase": "novel_text_classification_approach"}, {"score": 0.004363277689798392, "phrase": "standard_bag-of-words_method"}, {"score": 0.0039322149318145845, "phrase": "dependency_type"}, {"score": 0.0035630518377071916, "phrase": "filtering_features"}, {"score": 0.003524227590204673, "phrase": "low_frequencies"}, {"score": 0.0034478392374444177, "phrase": "word_features"}, {"score": 0.0034102660898135155, "phrase": "dependency_features"}, {"score": 0.003373101013123386, "phrase": "parameter_tuning"}, {"score": 0.003299977511820374, "phrase": "eight_different_pruning_levels"}, {"score": 0.003228434077467506, "phrase": "optimal_levels"}, {"score": 0.0030730639262332698, "phrase": "different_characteristics"}, {"score": 0.002989991779830814, "phrase": "significant_improvement"}, {"score": 0.0029412271277166873, "phrase": "success_rates"}, {"score": 0.0024679299547216956, "phrase": "text_classification"}, {"score": 0.00236199235951202, "phrase": "dataset_perspective"}, {"score": 0.002260591903283384, "phrase": "similar_formality_levels"}, {"score": 0.0022359278646792153, "phrase": "similar_leading_dependencies"}, {"score": 0.0021994343316143125, "phrase": "close_behavior"}, {"score": 0.002175436093794315, "phrase": "varying_pruning_levels"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Text classification", " Lexical dependency", " Pruning analysis", " Reuters-21578"], "paper_abstract": "We propose a novel text classification approach based on two main concepts, lexical dependency and pruning. We extend the standard bag-of-words method by including dependency patterns in the feature vector. We perform experiments with 37 lexical dependencies and the effect of each dependency type is analyzed separately in order to identify the most discriminative dependencies. We analyze the effect of pruning (filtering features with low frequencies) for both word features and dependency features. Parameter tuning is performed with eight different pruning levels to determine the optimal levels. The experiments were repeated on three datasets with different characteristics. We observed a significant improvement on the success rates as well as a reduction on the dimensionality of the feature vector. We argue that, in contrast to the works in the literature, a much higher pruning level should be used in text classification. By analyzing the results from the dataset perspective, we also show that datasets in similar formality levels have similar leading dependencies and show close behavior with varying pruning levels. (C) 2010 Elsevier B.V. All rights reserved.", "paper_title": "Text classification with the support of pruned dependency patterns", "paper_id": "WOS:000281368200014"}