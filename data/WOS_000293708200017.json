{"auto_keywords": [{"score": 0.02998999927405984, "phrase": "combination_coefficient"}, {"score": 0.01509702596182973, "phrase": "image_retrieval"}, {"score": 0.014433610484275352, "phrase": "multiview_data"}, {"score": 0.00481495049065317, "phrase": "dimension_reduction"}, {"score": 0.004685575787741084, "phrase": "real-world_applications"}, {"score": 0.0045804107341714, "phrase": "document_classification"}, {"score": 0.004477595431209663, "phrase": "different_features"}, {"score": 0.003996983218174335, "phrase": "conventional_concatenating_strategy"}, {"score": 0.003907212558276185, "phrase": "different_views"}, {"score": 0.00385431710959668, "phrase": "long_vector"}, {"score": 0.0035194572615570977, "phrase": "concatenating_strategy"}, {"score": 0.0032429499567088113, "phrase": "multiview_stochastic_neighbor"}, {"score": 0.0030988045934942587, "phrase": "heterogeneous_features"}, {"score": 0.003056819704624705, "phrase": "unified_representation"}, {"score": 0.0030291453183733897, "phrase": "subsequent_processing"}, {"score": 0.00297454367746853, "phrase": "probabilistic_framework"}, {"score": 0.0029209233709517634, "phrase": "conventional_strategies"}, {"score": 0.0027159094186967247, "phrase": "data_embedding"}, {"score": 0.0026427848602443293, "phrase": "important_role"}, {"score": 0.0025951294037626174, "phrase": "complementary_information"}, {"score": 0.0023372808856170386, "phrase": "optimal_rate"}, {"score": 0.002316105585203396, "phrase": "smooth_problems"}, {"score": 0.0022743274702883456, "phrase": "synthetic_and_real_data"}, {"score": 0.0021731423856364003, "phrase": "data_visualization"}, {"score": 0.002133937537670314, "phrase": "object_categorization"}, {"score": 0.0021049977753042253, "phrase": "scene_recognition"}], "paper_keywords": ["Dimension reduction", " image retrieval", " multiview learning", " stochastic neighbor embedding"], "paper_abstract": "Dimension reduction has been widely used in real-world applications such as image retrieval and document classification. In many scenarios, different features (or multiview data) can be obtained, and how to duly utilize them is a challenge. It is not appropriate for the conventional concatenating strategy to arrange features of different views into a long vector. That is because each view has its specific statistical property and physical interpretation. Even worse, the performance of the concatenating strategy will deteriorate if some views are corrupted by noise. In this paper, we propose a multiview stochastic neighbor embedding (m-SNE) that systematically integrates heterogeneous features into a unified representation for subsequent processing based on a probabilistic framework. Compared with conventional strategies, our approach can automatically learn a combination coefficient for each view adapted to its contribution to the data embedding. This combination coefficient plays an important role in utilizing the complementary information in multiview data. Also, our algorithm for learning the combination coefficient converges at a rate of O(1/k(2)), which is the optimal rate for smooth problems. Experiments on synthetic and real data sets suggest the effectiveness and robustness of m-SNE for data visualization, image retrieval, object categorization, and scene recognition.", "paper_title": "m-SNE: Multiview Stochastic Neighbor Embedding", "paper_id": "WOS:000293708200017"}