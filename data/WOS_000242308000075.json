{"auto_keywords": [{"score": 0.04780597585803098, "phrase": "rl"}, {"score": 0.00481495049065317, "phrase": "systems_management_policies"}, {"score": 0.004737579882204798, "phrase": "hybrid_reinforcement_learning"}, {"score": 0.004661446705304497, "phrase": "reinforcement_learning"}, {"score": 0.004476399615305783, "phrase": "particular_promise"}, {"score": 0.00436890075128947, "phrase": "emerging_application_domain"}, {"score": 0.004298666679374568, "phrase": "performance_management"}, {"score": 0.004229556887896042, "phrase": "computing_systems"}, {"score": 0.00412796126884884, "phrase": "recent_work"}, {"score": 0.004061584718427419, "phrase": "online_rl"}, {"score": 0.003996271202811961, "phrase": "effective_server_allocation_policies"}, {"score": 0.003900257249255497, "phrase": "prototype_data_center"}, {"score": 0.003806541299696042, "phrase": "explicit_system_models"}, {"score": 0.003453584878128719, "phrase": "substantially_improved_and_more_practical_\"hybrid\"_approach"}, {"score": 0.003133252930886608, "phrase": "queuing-theoretic_policy"}, {"score": 0.002936331371903168, "phrase": "potentially_poor_performance"}, {"score": 0.0028890603352364273, "phrase": "live_online_training"}, {"score": 0.0027741761880779535, "phrase": "nonlinear_function_approximators"}, {"score": 0.002707444334463544, "phrase": "tabular_value_functions"}, {"score": 0.002416589459425828, "phrase": "exploratory_actions"}, {"score": 0.0022830706430588482, "phrase": "closed-loop_traffic"}, {"score": 0.0022101055924375725, "phrase": "large_switching_delays"}, {"score": 0.0021394674440767124, "phrase": "significant_performance_improvement"}, {"score": 0.0021049977753042253, "phrase": "state-of-art_queuing_model_policies"}], "paper_keywords": [""], "paper_abstract": "Reinforcement Learning (RL) holds particular promise in an emerging application domain of performance management of computing systems. In recent work, online RL yielded effective server allocation policies in a prototype Data Center, without explicit system models or built-in domain knowledge. This paper presents a substantially improved and more practical \"hybrid\" approach, in which RL trains offline on data collected while a queuing-theoretic policy controls the system. This approach avoids potentially poor performance in live online training. Additionally we use nonlinear function approximators instead of tabular value functions; this greatly improves scalability, and surprisingly, eliminated the need for exploratory actions. In experiments using both open-loop and closed-loop traffic as well as large switching delays, our results show significant performance improvement over state-of-art queuing model policies.", "paper_title": "Improvement of systems management policies using hybrid reinforcement learning", "paper_id": "WOS:000242308000075"}