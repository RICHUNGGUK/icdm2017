{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "facial_expressions"}, {"score": 0.009845041538549718, "phrase": "manual_annotations"}, {"score": 0.00695668858183444, "phrase": "first_encounters"}, {"score": 0.004926004771318866, "phrase": "communicative_functions"}, {"score": 0.004554488671294504, "phrase": "automatic_identification"}, {"score": 0.004290969772225093, "phrase": "danish_corpus"}, {"score": 0.004091130285858113, "phrase": "support_vector"}, {"score": 0.0038390240716280302, "phrase": "classification_experiments"}, {"score": 0.0037188360359702182, "phrase": "naturally-occurring_conversations"}, {"score": 0.0034619440946558186, "phrase": "co-occurring_speech_tokens"}, {"score": 0.003248483521309273, "phrase": "emotion_labels"}, {"score": 0.0028944757265206332, "phrase": "coarse-grained_descriptions"}, {"score": 0.002837451022172113, "phrase": "co-occurring_speech"}, {"score": 0.002737615043512257, "phrase": "emotion_identification"}, {"score": 0.0026518169452004465, "phrase": "emotion_label_list"}, {"score": 0.0026307897899760383, "phrase": "fine_grained_emotions"}, {"score": 0.0026099289298770023, "phrase": "affective_states"}, {"score": 0.00252812242767189, "phrase": "shape_features"}, {"score": 0.002439143581855858, "phrase": "classification_results"}, {"score": 0.002391067964886602, "phrase": "annotation_scheme"}, {"score": 0.0023346234464901978, "phrase": "dimensional_description"}, {"score": 0.0021731423856364003, "phrase": "test_emotional_behaviours"}, {"score": 0.0021559025985688255, "phrase": "emotional_cognitive_infocommunicative_systems"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multimodal corpus", " Multimodal communication", " Emotion", " Machine learning", " Feedback", " Turn management", " Annotation"], "paper_abstract": "This paper deals with the automatic identification of emotions from the manual annotations of the shape and functions of facial expressions in a Danish corpus of video recorded naturally occurring first encounters. More specifically, a support vector classified is trained on the corpus annotations to identify emotions in facial expressions. In the classification experiments, we test to what extent emotions expressed in naturally-occurring conversations can be identified automatically by a classifier trained on the manual annotations of the shape of facial expressions and co-occurring speech tokens. We also investigate the relation between emotions and the communicative functions of facial expressions. Both emotion labels and their values in a three dimensional space are identified. The three dimensions are Pleasure, Arousal and Dominance. The results of our experiments indicate that the classifiers perform well in identifying emotions from the coarse-grained descriptions of facial expressions and co-occurring speech. The communicative functions of facial expressions also contribute to emotion identification. The results are promising because the emotion label list comprises fine grained emotions and affective states in naturally occurring conversations, while the shape features of facial expressions are very coarse grained. The classification results also assess that the annotation scheme combining a discrete and a dimensional description, and the manual annotations produced according to it are reliable and can be used to model and test emotional behaviours in emotional cognitive infocommunicative systems. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Predicting emotions in facial expressions from the annotations in naturally occurring first encounters", "paper_id": "WOS:000345817600005"}