{"auto_keywords": [{"score": 0.04920822493417558, "phrase": "mkl"}, {"score": 0.015719652428206398, "phrase": "data-dependent_priors"}, {"score": 0.015227885379930799, "phrase": "classifier_ensemble"}, {"score": 0.012352999200674834, "phrase": "multiple_views"}, {"score": 0.0047709540557306284, "phrase": "multiple_kernel_learning"}, {"score": 0.004556901872357751, "phrase": "learning_problems"}, {"score": 0.003970439654470374, "phrase": "novel_probabilistic_interpretation"}, {"score": 0.0038802832668554457, "phrase": "maximum_entropy_discrimination"}, {"score": 0.0034751259234824913, "phrase": "novel_data-dependent"}, {"score": 0.0033651019362620866, "phrase": "kernel_predictors"}, {"score": 0.0032886455096627324, "phrase": "prediction_performance"}, {"score": 0.003097865379563127, "phrase": "proposed_probabilistic_framework"}, {"score": 0.002999749669403939, "phrase": "hierarchical_bayesian_model"}, {"score": 0.00286493504971388, "phrase": "classification_model"}, {"score": 0.0028127163745564777, "phrase": "resultant_problem"}, {"score": 0.0026494723416072316, "phrase": "missing_labels"}, {"score": 0.002472822097373027, "phrase": "existing_mkl_models"}, {"score": 0.002405496949144104, "phrase": "proposed_mkl_framework"}, {"score": 0.002286781334850213, "phrase": "extensive_experiments"}, {"score": 0.0021049977753042253, "phrase": "partial_correspondence"}], "paper_keywords": ["Data fusion", " dirty data", " missing views", " multiple kernel learning", " partial correspondence", " semisupervised learning"], "paper_abstract": "Multiple kernel learning (MKL) and classifier ensemble are two mainstream methods for solving learning problems in which some sets of features/views are more informative than others, or the features/views within a given set are inconsistent. In this paper, we first present a novel probabilistic interpretation of MKL such that maximum entropy discrimination with a noninformative prior over multiple views is equivalent to the formulation of MKL. Instead of using the noninformative prior, we introduce a novel data-dependent prior based on an ensemble of kernel predictors, which enhances the prediction performance of MKL by leveraging the merits of the classifier ensemble. With the proposed probabilistic framework of MKL, we propose a hierarchical Bayesian model to learn the proposed data-dependent prior and classification model simultaneously. The resultant problem is convex and other information (e.g., instances with either missing views or missing labels) can be seamlessly incorporated into the data-dependent priors. Furthermore, a variety of existing MKL models can be recovered under the proposed MKL framework and can be readily extended to incorporate these priors. Extensive experiments demonstrate the benefits of our proposed framework in supervised and semisupervised settings, as well as in tasks with partial correspondence among multiple views.", "paper_title": "Generalized Multiple Kernel Learning With Data-Dependent Priors", "paper_id": "WOS:000354957000002"}