{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "insertion_points"}, {"score": 0.043077802045340466, "phrase": "insertion_point"}, {"score": 0.004606148097788071, "phrase": "robotic_tasks"}, {"score": 0.004550783060775205, "phrase": "robot-assisted_laparoscopic_surgery"}, {"score": 0.0043358695338488445, "phrase": "surgical_instruments"}, {"score": 0.004064958518350867, "phrase": "abdominal_wall"}, {"score": 0.003920049436883885, "phrase": "surgical_robot"}, {"score": 0.0038263136898042285, "phrase": "visual_feedback"}, {"score": 0.003749908582313067, "phrase": "accurate_vision-based_positioning"}, {"score": 0.00363080961099728, "phrase": "motion_constraint"}, {"score": 0.0036016815323122275, "phrase": "mis"}, {"score": 0.0032559935867182035, "phrase": "real-time_image_segmentation_issues"}, {"score": 0.003152531962718477, "phrase": "region_seeds"}, {"score": 0.0030278017562213265, "phrase": "\"eye-to-hand\"_robot_vision_system"}, {"score": 0.0028155572612699976, "phrase": "velocity_screw"}, {"score": 0.002748159131396737, "phrase": "appropriate_frame"}, {"score": 0.002607598760956807, "phrase": "main_part"}, {"score": 0.0025761937402886954, "phrase": "larger_problem"}, {"score": 0.002534906456854208, "phrase": "overall_transformation"}, {"score": 0.002504374718975704, "phrase": "camera_and_the_robot_end-effector_frame"}, {"score": 0.0024247382451294255, "phrase": "novel_algorithm"}, {"score": 0.002395530243993313, "phrase": "pose_determination"}, {"score": 0.0023762535034673017, "phrase": "cylindrical-shaped_instruments"}, {"score": 0.0023381630439887184, "phrase": "proposed_method"}, {"score": 0.0021392979397427984, "phrase": "robot_kinematics"}, {"score": 0.0021049977753042253, "phrase": "external_measurement_device"}], "paper_keywords": [""], "paper_abstract": "In robot-assisted laparoscopic surgery, an endoscopic camera is used to control the motion of surgical instruments. With this minimally invasive surgical (MIS) technique, every instrument has to pass through an insertion point in the abdominal wall and is mounted on the end-effector of a surgical robot which can be controlled by visual feedback. To achieve an accurate vision-based positioning of laparoscopic instruments, we introduce the motion constraint in MIS which is based on the location of out-of-field of view insertion points. The knowledge of the (image of the) insertion point location is helpful for real-time image segmentation issues, particularly to initiate the search for region seeds corresponding to the instruments. Moreover, with this \"eye-to-hand\" robot vision system, visual servoing is a very convenient technique to automatically guide an instrument but it requires the velocity screw to be expressed in the appropriate frame. Then, the location of the insertion point is seen as the main part of the larger problem of determining the overall transformation between the camera and the robot end-effector frame. This is achieved thanks to a novel algorithm for the pose determination of cylindrical-shaped instruments. With the proposed method, the location of insertion points can be recovered, on-line, with no marker, without any knowledge of robot kinematics and without an external measurement device.", "paper_title": "The role of insertion points in the detection and positioning of instruments in laparoscopy for robotic tasks", "paper_id": "WOS:000241556300065"}