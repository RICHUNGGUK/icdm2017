{"auto_keywords": [{"score": 0.026112725660358566, "phrase": "global_loss_function"}, {"score": 0.00481495049065317, "phrase": "online_learning"}, {"score": 0.004733625412289433, "phrase": "multiple_tasks"}, {"score": 0.004497762477097628, "phrase": "online_round"}, {"score": 0.0039244530915794025, "phrase": "parallel_tasks"}, {"score": 0.003512702405367876, "phrase": "common_goal"}, {"score": 0.00317095235382607, "phrase": "single_global_loss_function"}, {"score": 0.0029871209444889716, "phrase": "multiple_predictions"}, {"score": 0.0027662954811372175, "phrase": "individual_prediction"}, {"score": 0.002561752783323717, "phrase": "loss_values"}, {"score": 0.002332133125576037, "phrase": "online_algorithms"}, {"score": 0.0022346638243276717, "phrase": "absolute_norm"}, {"score": 0.0021049977753042253, "phrase": "worst-case_relative_loss_bounds"}], "paper_keywords": [""], "paper_abstract": "We study the problem of online learning of multiple tasks in parallel. On each online round, the algorithm receives an instance and makes a prediction for each one of the parallel tasks. We consider the case where these tasks all contribute toward a common goal. We capture the relationship between the tasks by using a single global loss function to evaluate the quality of the multiple predictions made on each round. Specifically, each individual prediction is associated with its own individual loss, and then these loss values are combined using a global loss function. We present several families of online algorithms which can use any absolute norm as a global loss function. We prove worst-case relative loss bounds for all of our algorithms.", "paper_title": "Online multitask learning", "paper_id": "WOS:000239587900034"}