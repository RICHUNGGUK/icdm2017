{"auto_keywords": [{"score": 0.035761459630628156, "phrase": "amd_core_math_library"}, {"score": 0.004815112813718618, "phrase": "cache"}, {"score": 0.004331393784852601, "phrase": "contemporary_and_future_processor_architectures"}, {"score": 0.004244102948402086, "phrase": "recent_port"}, {"score": 0.004192571571622019, "phrase": "latest_results"}, {"score": 0.004158563941891469, "phrase": "cache-oblivious_algorithms"}, {"score": 0.004025258572860201, "phrase": "sgi's_ultraviolet"}, {"score": 0.00399260264462941, "phrase": "shared-memory_machine"}, {"score": 0.003928146867112212, "phrase": "amd"}, {"score": 0.003880371656522308, "phrase": "bulldozer"}, {"score": 0.003817655499320344, "phrase": "intel's_future"}, {"score": 0.003771282384646587, "phrase": "core_architecture"}, {"score": 0.003740679163860117, "phrase": "tifammy's_matrix_multiplication_and_lu_decomposition_routines"}, {"score": 0.0034761488481944657, "phrase": "vendors'_architecture-specific_and_optimized_libraries"}, {"score": 0.003310236836421397, "phrase": "vectorization_compiler_switches"}, {"score": 0.0032833625797814474, "phrase": "tifammy's_highly_optimized_vector_intrinsics_version"}, {"score": 0.0032040376301815544, "phrase": "architectural_properties"}, {"score": 0.0031139032290048788, "phrase": "heterogeneous_cores"}, {"score": 0.0029895074299598275, "phrase": "bare-metal_performance"}, {"score": 0.0029531640201737384, "phrase": "test_platforms'_ease"}, {"score": 0.00278930116312699, "phrase": "new_and_upcoming_silicon"}, {"score": 0.002688781336915723, "phrase": "code_change_abstraction_levels"}, {"score": 0.002529213370528279, "phrase": "memory_organization"}, {"score": 0.002508663916554954, "phrase": "tifammy"}, {"score": 0.0024781515864648242, "phrase": "equally_efficient_performance"}, {"score": 0.002379092507901856, "phrase": "architectural_parameters"}, {"score": 0.0021049977753042253, "phrase": "john_wiley"}], "paper_keywords": ["shared-memory platforms", " cache oblivious", " block recursive", " linear algebra", " performance", " parallelization"], "paper_abstract": "This article highlights the issue of upcoming wider single-instruction, multiple-data units as well as steadily increasing core counts on contemporary and future processor architectures. We present the recent port to and latest results of cache-oblivious algorithms and implementations of our TifaMMy code on four architectures: SGI's UltraViolet distributed shared-memory machine, Intel's latest x86 architecture code-named Sandy Bridge, AMD's new Bulldozer architecture, and Intel's future Many Integrated Core architecture. TifaMMy's matrix multiplication and LU decomposition routines have been adapted and tuned with regard to these architectures. Results are discussed and compared with vendors' architecture-specific and optimized libraries, Math Kernel Library and AMD Core Math Library, for both a standard C++ version with vectorization compiler switches and TifaMMy's highly optimized vector intrinsics version. We provide insights into architectural properties and comment on the feasibility of heterogeneous cores and accelerators, namely graphics processing units. Besides bare-metal performance, the test platforms' ease of use is analyzed in detail, and the portability of our approach to new and upcoming silicon is discussed with regard to required effort on code change abstraction levels.As a result, we demonstrate that because of its generic structure in terms of memory organization, TifaMMy executes with equally efficient performance on all four architectures as it automatically adapts itself to architectural parameters without losing performance against the Math Kernel Library and AMD Core Math Library, underlining its generic and cache-oblivious properties, as the porting effort was relatively low compared with that in other implementations.Copyright (c) 2012 John Wiley & Sons, Ltd.", "paper_title": "Cache-oblivious matrix algorithms in the age of multicores and many cores", "paper_id": "WOS:000355001700003"}