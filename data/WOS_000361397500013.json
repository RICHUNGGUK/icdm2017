{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "topological_q-learning"}, {"score": 0.0489041172623351, "phrase": "mobile_robot_navigation"}, {"score": 0.0047555023078686386, "phrase": "internally_guided_exploration"}, {"score": 0.004581505405526037, "phrase": "learning_convergence"}, {"score": 0.0045249263666466005, "phrase": "reinforcement_learning"}, {"score": 0.0041478818670853115, "phrase": "different_approaches"}, {"score": 0.0038978806337961565, "phrase": "robot's_environment"}, {"score": 0.0038259422140859557, "phrase": "rl"}, {"score": 0.0036857412768922827, "phrase": "great_importance"}, {"score": 0.0035951877427580006, "phrase": "high_number"}, {"score": 0.0034420211080526094, "phrase": "value_function"}, {"score": 0.0033159224502739247, "phrase": "optimal_or_a_nearly_optimal_policy"}, {"score": 0.002855934865505357, "phrase": "topological_ordering"}, {"score": 0.002803104254950008, "phrase": "observed_states"}, {"score": 0.0025691622090542304, "phrase": "incremental_topological_map"}, {"score": 0.0024904187737391807, "phrase": "instantaneous_topological_map_model"}, {"score": 0.0023990982005125763, "phrase": "value_function_updates"}, {"score": 0.00231111848128734, "phrase": "guided_exploration_strategy"}, {"score": 0.0021715812867126884, "phrase": "original_q-learning"}, {"score": 0.0021049977753042253, "phrase": "static_and_dynamic_environments"}], "paper_keywords": ["Reinforcement learning", " Q-learning", " Convergence acceleration", " Topological map", " Guided exploration"], "paper_abstract": "Improving the learning convergence of reinforcement learning (RL) in mobile robot navigation has been the interest of many recent works that have investigated different approaches to obtain knowledge from effectively and efficiently exploring the robot's environment. In RL, this knowledge is of great importance for reducing the high number of interactions required for updating the value function and to eventually find an optimal or a nearly optimal policy for the agent. In this paper, we propose a topological Q-learning (TQ-learning) algorithm that makes use of the topological ordering among the observed states of the environment in which the agent acts. This algorithm builds an incremental topological map of the environment using Instantaneous Topological Map model which we use for accelerating value function updates as well as providing a guided exploration strategy for the agent. We evaluate our algorithm against the original Q-learning and the Influence Zone algorithms in static and dynamic environments.", "paper_title": "Topological Q-learning with internally guided exploration for mobile robot navigation", "paper_id": "WOS:000361397500013"}