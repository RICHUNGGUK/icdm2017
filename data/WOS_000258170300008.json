{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "least_squares"}, {"score": 0.004681971826924847, "phrase": "vector_regression"}, {"score": 0.004489327204473076, "phrase": "simultaneous_learning"}, {"score": 0.003689628075766217, "phrase": "regularized_least_squares_approach_based_support_vector_machine"}, {"score": 0.003206916619690171, "phrase": "proposed_algorithm"}, {"score": 0.0029069972482610403, "phrase": "quadratic_programming_solver"}, {"score": 0.0024912073108138613, "phrase": "structured_system"}, {"score": 0.002422249273477746, "phrase": "linear_equations"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["support vector machines", " regularized least squares", " machine learning", " function approximation"], "paper_abstract": "in this paper, we propose a regularized least squares approach based support vector machine for simultaneously approximating a function and its derivatives. The proposed algorithm is simple and fast as no quadratic programming solver needs to be employed. Effectively, only the solution of a structured system of linear equations is needed. (c) 2008 Published by Elsevier Inc.", "paper_title": "Regularized least squares support vector regression for the simultaneous learning of a function and its derivatives", "paper_id": "WOS:000258170300008"}