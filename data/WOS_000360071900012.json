{"auto_keywords": [{"score": 0.04838011372739916, "phrase": "sparse_representations"}, {"score": 0.03553177938022497, "phrase": "linear_classifier"}, {"score": 0.00481495049065317, "phrase": "dictionary_learning_for_fast_classification"}, {"score": 0.004499647408293102, "phrase": "excellent_results"}, {"score": 0.00441909542037375, "phrase": "classification_tasks"}, {"score": 0.004320421465896323, "phrase": "high_cost"}, {"score": 0.004223941453664225, "phrase": "test_time"}, {"score": 0.004167086373747367, "phrase": "major_obstacle"}, {"score": 0.0040010534672646775, "phrase": "large-scale_problems"}, {"score": 0.00389404213807752, "phrase": "computational_power"}, {"score": 0.0037219886365287085, "phrase": "simple_yet_efficient_alternative"}, {"score": 0.003638823482920949, "phrase": "feature_extraction"}, {"score": 0.003557509959237602, "phrase": "classification_scheme"}, {"score": 0.0034937647771945803, "phrase": "soft-thresholding_nonlinear_mapping"}, {"score": 0.0033242742850902295, "phrase": "novel_supervised_dictionary_learning_algorithm"}, {"score": 0.00326469463991041, "phrase": "low_complexity_classification_architecture"}, {"score": 0.0031917146566068245, "phrase": "dictionary_learning_problem"}, {"score": 0.002889444701599236, "phrase": "iterative_dc_solver"}, {"score": 0.0026634937939104177, "phrase": "classification_problem"}, {"score": 0.002639502113350897, "phrase": "generic_learning_procedures"}, {"score": 0.002522738549524317, "phrase": "recent_sparse_coding_classifiers"}, {"score": 0.0024111277643627154, "phrase": "adopted_classification_scheme"}, {"score": 0.0023465400668244386, "phrase": "testing_stage"}, {"score": 0.0022733662000757318, "phrase": "proposed_scheme"}, {"score": 0.002212460584002629, "phrase": "adequately_trained_soft-thresholding_mapping"}], "paper_keywords": ["Dictionary learning", " Soft-thresholding", " Sparse coding", " Rectifier linear units", " Neural networks"], "paper_abstract": "Classifiers based on sparse representations have recently been shown to provide excellent results in many visual recognition and classification tasks. However, the high cost of computing sparse representations at test time is a major obstacle that limits the applicability of these methods in large-scale problems, or in scenarios where computational power is restricted. We consider in this paper a simple yet efficient alternative to sparse coding for feature extraction. We study a classification scheme that applies the soft-thresholding nonlinear mapping in a dictionary, followed by a linear classifier. A novel supervised dictionary learning algorithm tailored for this low complexity classification architecture is proposed. The dictionary learning problem, which jointly learns the dictionary and linear classifier, is cast as a difference of convex (DC) program and solved efficiently with an iterative DC solver. We conduct experiments on several datasets, and show that our learning algorithm that leverages the structure of the classification problem outperforms generic learning procedures. Our simple classifier based on soft-thresholding also competes with the recent sparse coding classifiers, when the dictionary is learned appropriately. The adopted classification scheme further requires less computational time at the testing stage, compared to other classifiers. The proposed scheme shows the potential of the adequately trained soft-thresholding mapping for classification and paves the way towards the development of very efficient classification methods for vision problems.", "paper_title": "Dictionary Learning for Fast Classification Based on Soft-thresholding", "paper_id": "WOS:000360071900012"}