{"auto_keywords": [{"score": 0.048339607507904316, "phrase": "ffnn"}, {"score": 0.010612384911619756, "phrase": "feedforward_neural_networks"}, {"score": 0.007712237832306394, "phrase": "training_data"}, {"score": 0.00471700544744796, "phrase": "heavy-tailed_noise"}, {"score": 0.004380540407926982, "phrase": "sequential_and_especially_real_time"}, {"score": 0.004326833704481825, "phrase": "neural_networks_models"}, {"score": 0.0041184843463633386, "phrase": "wide_range"}, {"score": 0.004084744133085742, "phrase": "engineering_problems"}, {"score": 0.00405127920820053, "phrase": "recent_research_results"}, {"score": 0.003793206550770237, "phrase": "new_approach"}, {"score": 0.0036104596826132965, "phrase": "real_time"}, {"score": 0.003352658916650986, "phrase": "new_learning_algorithm"}, {"score": 0.003284361433827464, "phrase": "conventional_extended_kalman_filter"}, {"score": 0.0032174507570710835, "phrase": "extended_kalman_filter"}, {"score": 0.0030876784650745973, "phrase": "probabilistic_generative_model"}, {"score": 0.0030497740719977835, "phrase": "measurement_noise_covariance"}, {"score": 0.0029509483926257645, "phrase": "noise_measurement_covariance"}, {"score": 0.0029027394930867902, "phrase": "stochastic_process"}, {"score": 0.002843581231499026, "phrase": "symmetric_positive-definite_matrices"}, {"score": 0.002762774046280398, "phrase": "inverse_wishart_distribution"}, {"score": 0.0027176309244222, "phrase": "iteration_ekf-or"}, {"score": 0.002684257009647205, "phrase": "noise_estimates"}, {"score": 0.0026622351303038885, "phrase": "current_best_estimate"}, {"score": 0.0026403934420630155, "phrase": "ffnn_parameters"}, {"score": 0.002618730478351454, "phrase": "bayesian_framework"}, {"score": 0.002523420125608051, "phrase": "analytical_intractability"}, {"score": 0.002492425259337031, "phrase": "bayes'_update_step"}, {"score": 0.0024416087858984644, "phrase": "structured_variational_approximation"}, {"score": 0.0024116163689723354, "phrase": "mathematical_expressions"}, {"score": 0.002333421318779658, "phrase": "first_principles"}, {"score": 0.002314271047073693, "phrase": "extensive_experimental_study"}, {"score": 0.0022577559526646904, "phrase": "developed_learning_algorithm"}, {"score": 0.0022300169122921906, "phrase": "low_prediction_error"}, {"score": 0.0022117133940361025, "phrase": "good_generalization"}, {"score": 0.0021755548389283856, "phrase": "outliers'_presence"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feedforward neural networks", " Sequential learning", " Robust extended Kalman filter", " Structured variational approximation", " Heavy-tailed noise", " Inverse Wishart distribution"], "paper_abstract": "Feedforward neural networks (FFNN) are among the most used neural networks for modeling of various nonlinear problems in engineering. In sequential and especially real time processing all neural networks models fail when faced with outliers. Outliers are found across a wide range of engineering problems. Recent research results in the field have shown that to avoid overfitting or divergence of the model, new approach is needed especially if FFNN is to run sequentially or in real time. To accommodate limitations of FFNN when training data contains a certain number of outliers, this paper presents new learning algorithm based on improvement of conventional extended Kalman filter (EKF). Extended Kalman filter robust to outliers (EKF-OR) is probabilistic generative model in which measurement noise covariance is not constant; the sequence of noise measurement covariance is modeled as stochastic process over the set of symmetric positive-definite matrices in which prior is modeled as inverse Wishart distribution. In each iteration EKF-OR simultaneously estimates noise estimates and current best estimate of FFNN parameters. Bayesian framework enables one to mathematically derive expressions, while analytical intractability of the Bayes' update step is solved by using structured variational approximation. All mathematical expressions in the paper are derived using the first principles. Extensive experimental study shows that FFNN trained with developed learning algorithm, achieves low prediction error and good generalization quality regardless of outliers' presence in training data. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Robust sequential learning of feedforward neural networks in the presence of heavy-tailed noise", "paper_id": "WOS:000349730800004"}