{"auto_keywords": [{"score": 0.04103590613212116, "phrase": "training_data"}, {"score": 0.011059043261116109, "phrase": "depth_gradients"}, {"score": 0.00481495049065317, "phrase": "single_image_depth"}, {"score": 0.004752873752686246, "phrase": "gradient_samples"}, {"score": 0.00469159356595623, "phrase": "scene_depth"}, {"score": 0.004631099806015235, "phrase": "single_monocular_image"}, {"score": 0.00457138247462329, "phrase": "highly_ill-posed_problem"}, {"score": 0.004396790930873782, "phrase": "new_gradient-domain_approach"}, {"score": 0.004084920064588169, "phrase": "target_depth_field"}, {"score": 0.003980191398041843, "phrase": "rgb-d_image_pairs"}, {"score": 0.003828091961373454, "phrase": "proposed_method"}, {"score": 0.0037786899855446, "phrase": "non-parametric_learning_process"}, {"score": 0.0037138072936226, "phrase": "analogous_depth_field"}, {"score": 0.003665874747419775, "phrase": "reliable_depth_gradients"}, {"score": 0.003634262647873338, "phrase": "visual_correspondence"}, {"score": 0.0035873530542209686, "phrase": "training_image_pairs"}, {"score": 0.0035410467987580484, "phrase": "existing_data-driven_approaches"}, {"score": 0.003480230347985338, "phrase": "depth_values"}, {"score": 0.0033471722682967046, "phrase": "reconstruction_cues"}, {"score": 0.0032331613500559764, "phrase": "poisson_reconstruction"}, {"score": 0.0030960762496815768, "phrase": "training_rgb-d_data"}, {"score": 0.0029264925857346497, "phrase": "reconstructed_depth_maps"}, {"score": 0.002888692115811987, "phrase": "desired_depth_distribution"}, {"score": 0.0028513785049061767, "phrase": "input_image"}, {"score": 0.0026835045963349647, "phrase": "indoor_scenes"}, {"score": 0.002525489173110693, "phrase": "scene_characteristics"}, {"score": 0.0024928553725344933, "phrase": "better_cues"}, {"score": 0.0024713335323674223, "phrase": "depth_recovery"}, {"score": 0.002387083859537412, "phrase": "great_variety"}, {"score": 0.002315719347244451, "phrase": "substantial_appearance"}, {"score": 0.0022957232752482196, "phrase": "geometric_variations"}, {"score": 0.002266051712795851, "phrase": "experimental_results"}, {"score": 0.0021418339161425994, "phrase": "depth_domain"}], "paper_keywords": ["Depth estimation", " 2D-to-3D conversion", " gradient transfer", " non-parametric sampling", " image analogy"], "paper_abstract": "Inferring scene depth from a single monocular image is a highly ill-posed problem in computer vision. This paper presents a new gradient-domain approach, called depth analogy, that makes use of analogy as a means for synthesizing a target depth field, when a collection of RGB-D image pairs is given as training data. Specifically, the proposed method employs a non-parametric learning process that creates an analogous depth field by sampling reliable depth gradients using visual correspondence established on training image pairs. Unlike existing data-driven approaches that directly select depth values from training data, our framework transfers depth gradients as reconstruction cues, which are then integrated by the Poisson reconstruction. The performance of most conventional approaches relies heavily on the training RGB-D data used in the process, and such a dependency severely degenerates the quality of reconstructed depth maps when the desired depth distribution of an input image is quite different from that of the training data, e.g., outdoor versus indoor scenes. Our key observation is that using depth gradients in the reconstruction is less sensitive to scene characteristics, providing better cues for depth recovery. Thus, our gradient-domain approach can support a great variety of training range datasets that involve substantial appearance and geometric variations. The experimental results demonstrate that our (depth) gradient-domain approach outperforms existing data-driven approaches directly working on depth domain, even when only uncorrelated training datasets are available.", "paper_title": "Depth Analogy: Data-Driven Approach for Single Image Depth Estimation Using Gradient Samples", "paper_id": "WOS:000365022400001"}