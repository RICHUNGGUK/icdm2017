{"auto_keywords": [{"score": 0.03369594687840479, "phrase": "defacto"}, {"score": 0.006663523773052561, "phrase": "web_pages"}, {"score": 0.00481495049065317, "phrase": "defacto-temporal"}, {"score": 0.004764859786310636, "phrase": "deep_fact_validation"}, {"score": 0.004682525167261325, "phrase": "main_tasks"}, {"score": 0.004601606675066839, "phrase": "knowledge_bases"}, {"score": 0.004306610946695577, "phrase": "provided_knowledge"}, {"score": 0.004144510906908392, "phrase": "human_curators"}, {"score": 0.004101366149219787, "phrase": "three-step_process"}, {"score": 0.004058668702112495, "phrase": "appropriate_keyword_queries"}, {"score": 0.00396075511616602, "phrase": "standard_search_engines"}, {"score": 0.003919515745050147, "phrase": "potentially_relevant_documents"}, {"score": 0.0038383157378810277, "phrase": "relevant_content"}, {"score": 0.0032689949867583633, "phrase": "trustworthy_sources"}, {"score": 0.0031348622158167195, "phrase": "effective_way"}, {"score": 0.0030378759123133644, "phrase": "relevant_excerpts"}, {"score": 0.002974886152979993, "phrase": "useful_additional_information"}, {"score": 0.002832928222306059, "phrase": "input_fact"}, {"score": 0.00256895975568914, "phrase": "temporal_scope"}, {"score": 0.0024039564563674673, "phrase": "automatic_evaluation"}, {"score": 0.0022105600003496225, "phrase": "generic_evaluation_framework"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Web of Data", " Fact validation", " NLP", " Provenance"], "paper_abstract": "One of the main tasks when creating and maintaining knowledge bases is to validate facts and provide sources for them in order to ensure correctness and traceability of the provided knowledge. So far, this task is often addressed by human curators in a three-step process: issuing appropriate keyword queries for the statement to check using standard search engines, retrieving potentially relevant documents and screening those documents for relevant content. The drawbacks of this process are manifold. Most importantly, it is very time-consuming as the experts have to carry out several search processes and must often read several documents. In this article, we present DeFacto (Deep Fact Validation)-an algorithm able to validate facts by finding trustworthy sources for them on the Web. DeFacto aims to provide an effective way of validating facts by supplying the user with relevant excerpts of web pages as well as useful additional information including a score for the confidence DeFacto has in the correctness of the input fact. To achieve this goal, DeFacto collects and combines evidence from web pages written in several languages. In addition, DeFacto provides support for facts with a temporal scope, i.e., it can estimate in which time frame a fact was valid. Given that the automatic evaluation of facts has not been paid much attention to so far, generic benchmarks for evaluating these frameworks were not previously available. We thus also present a generic evaluation framework for fact checking and make it publicly available. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "DeFacto-Temporal and multilingual Deep Fact Validation", "paper_id": "WOS:000373626400004"}