{"auto_keywords": [{"score": 0.03818047705679454, "phrase": "block_diagonality"}, {"score": 0.00481495049065317, "phrase": "bayesian_networks"}, {"score": 0.004781498797706604, "phrase": "conditional_random_fields"}, {"score": 0.004731754442072492, "phrase": "triple_jump_extrapolation_method"}, {"score": 0.004682525167261325, "phrase": "effective_approximation"}, {"score": 0.00464998919563789, "phrase": "aitken's_acceleration"}, {"score": 0.00450633982324153, "phrase": "data_mining"}, {"score": 0.004459856624675937, "phrase": "em"}, {"score": 0.004428452434981208, "phrase": "generalized_iterative_scaling"}, {"score": 0.0038119880553339784, "phrase": "jacobian"}, {"score": 0.0035920758080335655, "phrase": "first_hint"}, {"score": 0.0033969686698961713, "phrase": "em_mapping"}, {"score": 0.0033498651263935194, "phrase": "bayesian_network"}, {"score": 0.0031568304678554243, "phrase": "gis_mapping"}, {"score": 0.0030378759123133644, "phrase": "feature_dependencies"}, {"score": 0.002954180291607795, "phrase": "controlled_and_real-world_data_sets"}, {"score": 0.002651015392318126, "phrase": "large-scale_crf_models"}, {"score": 0.002586972610351776, "phrase": "componentwise_extrapolation"}, {"score": 0.002437818660282327, "phrase": "crf"}, {"score": 0.0022653055209664284, "phrase": "stochastic_gradient_descent"}, {"score": 0.0022260654608669593, "phrase": "careful_tuning"}, {"score": 0.002195194011707633, "phrase": "sgd"}, {"score": 0.002119764367112061, "phrase": "useful_foundation"}, {"score": 0.0021049977753042253, "phrase": "automatic_tuning"}], "paper_keywords": ["Bayesian networks", " Conditional random fields", " Expectation maximization (EM) algorithm", " Generalized iterative scaling", " Aitken's extrapolation"], "paper_abstract": "The triple jump extrapolation method is an effective approximation of Aitken's acceleration that can accelerate the convergence of many algorithms for data mining, including EM and generalized iterative scaling (GIS). It has two options-global and componentwise extrapolation. Empirical studies showed that neither can dominate the other and it is not known which one is better under what condition. In this paper, we investigate this problem and conclude that, when the Jacobian is (block) diagonal, componentwise extrapolation will be more effective. We derive two hints to determine the block diagonality. The first hint is that when we have a highly sparse data set, the Jacobian of the EM mapping for training a Bayesian network will be block diagonal. The second is that the block diagonality of the Jacobian of the GIS mapping for training CRF is negatively correlated with the strength of feature dependencies. We empirically verify these hints with controlled and real-world data sets and show that our hints can accurately predict which method will be superior. We also show that both global and componentwise extrapolation can provide substantial acceleration. In particular, when applied to train large-scale CRF models, the GIS variant accelerated by componentwise extrapolation not only outperforms its global extrapolation counterpart, as our hint predicts, but can also compete with limited-memory BFGS (L-BFGS), the de facto standard for CRF training, in terms of both computational efficiency and F-scores. Though none of the above methods are as fast as stochastic gradient descent (SGD), careful tuning is required for SGD and the results given in this paper provide a useful foundation for automatic tuning.", "paper_title": "Global and componentwise extrapolations for accelerating training of Bayesian networks and conditional random fields", "paper_id": "WOS:000266266600003"}