{"auto_keywords": [{"score": 0.0461275073104745, "phrase": "computational_inference"}, {"score": 0.00481495049065317, "phrase": "current_state"}, {"score": 0.004566954456805558, "phrase": "recent_decades"}, {"score": 0.004477270137801732, "phrase": "enormous_improvements"}, {"score": 0.004360412703519539, "phrase": "statistical_models"}, {"score": 0.004218602458288531, "phrase": "competitive_continual_enhancements"}, {"score": 0.0041357305373417455, "phrase": "wide_range"}, {"score": 0.004081385289516914, "phrase": "computational_tools"}, {"score": 0.00400119808662365, "phrase": "bayesian_inference"}, {"score": 0.003845500924733243, "phrase": "mcmc_techniques"}, {"score": 0.0036472537997966938, "phrase": "random_walk_proposals"}, {"score": 0.0035993041437026225, "phrase": "langevin_drift"}, {"score": 0.0035285548582081627, "phrase": "hamiltonian_monte_carlo"}, {"score": 0.003153041030690117, "phrase": "impressive_evolution"}, {"score": 0.003010250522930773, "phrase": "even_steeper_increase"}, {"score": 0.002671990515821213, "phrase": "ever_more_complex_datasets"}, {"score": 0.0025678829712650437, "phrase": "new_type"}, {"score": 0.00234043119560776, "phrase": "raw_data"}, {"score": 0.0022641658133181115, "phrase": "approximate_models"}, {"score": 0.0021049977753042253, "phrase": "next_computational_revolution"}], "paper_keywords": ["Bayesian analysis", " MCMC algorithms", " ABC techniques", " Optimisation"], "paper_abstract": "Recent decades have seen enormous improvements in computational inference for statistical models; there have been competitive continual enhancements in a wide range of computational tools. In Bayesian inference, first and foremost, MCMC techniques have continued to evolve, moving from random walk proposals to Langevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical and algorithmic innovations opening new opportunities to practitioners. However, this impressive evolution in capacity is confronted by an even steeper increase in the complexity of the datasets to be addressed. The difficulties of modelling and then handling ever more complex datasets most likely call for a new type of tool for computational inference that dramatically reduces the dimension and size of the raw data while capturing its essential aspects. Approximate models and algorithms may thus be at the core of the next computational revolution.", "paper_title": "Bayesian computation: a summary of the current state, and samples backwards and forwards", "paper_id": "WOS:000356828600012"}