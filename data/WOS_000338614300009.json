{"auto_keywords": [{"score": 0.039961575637278306, "phrase": "code_optimizations"}, {"score": 0.00481495049065317, "phrase": "genetic-based_multi-point_statistics_simulation_code"}, {"score": 0.004404994123834589, "phrase": "genetic_algorithms_concerns"}, {"score": 0.003757480190200645, "phrase": "parallelization_schemes"}, {"score": 0.0036864465771943933, "phrase": "genetic-based_mps_code"}, {"score": 0.003503497128638099, "phrase": "execution_time"}, {"score": 0.003245900020944865, "phrase": "array_accesses"}, {"score": 0.003026391655379905, "phrase": "accessed_data"}, {"score": 0.0029691377785333872, "phrase": "hybrid_parallelization_scheme"}, {"score": 0.002912963877144772, "phrase": "fine-grain_parallelization"}, {"score": 0.0028216858159564808, "phrase": "shared-memory_programming_model"}, {"score": 0.0027159094186967247, "phrase": "coarse-grain_distribution"}, {"score": 0.0025974916908139472, "phrase": "distributed-memory_programming_model"}, {"score": 0.0021870332034407817, "phrase": "distributed-shared_memory_supercomputing_facility"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Geostatistics", " Stochastic simulation", " Multi-point statistics", " Code optimization", " Parallel computing", " Genetic algorithms"], "paper_abstract": "One of the main difficulties using multi-point statistical (MPS) simulation based on annealing techniques or genetic algorithms concerns the excessive amount of time and memory that must be spent in order to achieve convergence. In this work we propose code optimizations and parallelization schemes over a genetic-based MPS code with the aim of speeding up the execution time. The code optimizations involve the reduction of cache misses in the array accesses, avoid branching instructions and increase the locality of the accessed data. The hybrid parallelization scheme involves a fine-grain parallelization of loops using a shared-memory programming model (OpenMP) and a coarse-grain distribution of load among several computational nodes using a distributed-memory programming model (MPI). Convergence, execution time and speed-up results are presented using 2D training images of sizes 100 x 100 x 1 and 1000 x 1000 x 1 on a distributed-shared memory supercomputing facility. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "Tuning and hybrid parallelization of a genetic-based multi-point statistics simulation code", "paper_id": "WOS:000338614300009"}