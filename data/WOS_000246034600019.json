{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "context-tree_prediction"}, {"score": 0.04691383034029242, "phrase": "individual_sequences"}, {"score": 0.026091749439275776, "phrase": "critical_growth_rate"}, {"score": 0.00472807509484242, "phrase": "evident_success"}, {"score": 0.004706601114516316, "phrase": "context-tree_based_methods"}, {"score": 0.004517651748055463, "phrase": "universal_prediction"}, {"score": 0.0043859027329523756, "phrase": "prediction_schemes"}, {"score": 0.004326392590041596, "phrase": "instant_t"}, {"score": 0.004238631509452084, "phrase": "data_sequence_x"}, {"score": 0.00313768320233279, "phrase": "gutman"}, {"score": 0.0030950416862326213, "phrase": "general_finite-state_predictors"}, {"score": 0.0030608161533582454, "phrase": "markov"}, {"score": 0.0029978124537790246, "phrase": "fixed_order"}, {"score": 0.0028641729745364952, "phrase": "feder_et_al"}, {"score": 0.0028381665940971036, "phrase": "asymptotic_regime"}, {"score": 0.0027116246112603875, "phrase": "memory_size"}, {"score": 0.00266259765968597, "phrase": "length_n"}, {"score": 0.0026444413422763314, "phrase": "data_sequence"}, {"score": 0.0023810183846557486, "phrase": "best_context-tree_predictor"}, {"score": 0.0022701871111262205, "phrase": "n."}, {"score": 0.002233665284240771, "phrase": "universal_context-tree_algorithm"}, {"score": 0.0022133711375320266, "phrase": "optimum_performance"}, {"score": 0.002188261979417045, "phrase": "growth_rate"}, {"score": 0.0021049977753042253, "phrase": "linear_case"}], "paper_keywords": ["context-tree algorithm", " finite-memory machine", " finite-state machine", " individual sequence", " predictability", " universal prediction"], "paper_abstract": "Motivated by the evident success of context-tree based methods in lossless data compression, we explore, in this correspondence, methods of the same spirit in universal prediction of individual sequences. By context-tree prediction, we refer to a family of prediction schemes, where at each time instant t, after having observed all outcomes of the data sequence x (1) . . . , x (t) (-) (1), but not yet x (t), the prediction is based on a \"context\" (or a state) that consists of the kappa most recent past outcomes X (t) (-) (kappa), . . ., X (t) (-) (1), where the choice of kappa may depend on the contents of a possibly longer, though limited, portion of the observed past X (t) (-) (kappa max) , . . . , x (t -) (1). This is different from the study reported in the paper by Feder, Merhav, and Gutman (1992), where general finite-state predictors as well as \"Markov\" (finite-memory) predictors of fixed order, were studied in the regime of individual sequences. Another important difference between this study and the work of Feder et al. is the asymptotic regime. While in their work, the resources of the predictor (i.e., the number of states or the memory size) were kept fixed regardless of the length N of the data sequence, here we investigate situations where the number of contexts, or states, is allowed to grow concurrently with N. We are primarily interested in the following fundamental question: What is the critical growth rate of the number of contexts, below which the performance of the best context-tree predictor is still universally achievable, but above which it is not? We show that this critical growth rate is linear in N. In particular, we propose a universal context-tree algorithm that essentially achieves optimum performance as long as the growth rate is sublinear, and show that, on the other hand, this is impossible in the linear case.", "paper_title": "On context-tree prediction of individual sequences", "paper_id": "WOS:000246034600019"}