{"auto_keywords": [{"score": 0.04625336728938103, "phrase": "semi-supervised_feature_selection"}, {"score": 0.027046570723241507, "phrase": "optimal_solution"}, {"score": 0.02608720480127399, "phrase": "level_method"}, {"score": 0.00481495049065317, "phrase": "feature_selection"}, {"score": 0.0047268728636719725, "phrase": "huge_amount"}, {"score": 0.004290112502811656, "phrase": "small_amount"}, {"score": 0.0042506731642425275, "phrase": "labeled_examples"}, {"score": 0.004134508282135594, "phrase": "unlabeled_examples"}, {"score": 0.004058826900076531, "phrase": "small_number"}, {"score": 0.0040215052006051235, "phrase": "labeled_samples"}, {"score": 0.0038935506745975835, "phrase": "relevant_features"}, {"score": 0.003839962121679795, "phrase": "critical_problem"}, {"score": 0.0035994372026264478, "phrase": "unlabeled_data"}, {"score": 0.0034368818187904744, "phrase": "novel_discriminative_semi-supervised_feature_selection_method"}, {"score": 0.003281643474564305, "phrase": "proposed_approach"}, {"score": 0.0031918759137610523, "phrase": "classification_margin"}, {"score": 0.0031625006830124512, "phrase": "different_classes"}, {"score": 0.0030476702431093687, "phrase": "probability_distribution"}, {"score": 0.002991823455621222, "phrase": "labeled_and_unlabeled_data"}, {"score": 0.0029234476524900794, "phrase": "previous_semi-supervised_feature_selection_algorithms"}, {"score": 0.0026775328564629577, "phrase": "proposed_feature_selection_method"}, {"score": 0.002640636493247601, "phrase": "convex-concave_optimization_problem"}, {"score": 0.0025922289692173997, "phrase": "saddle_point"}, {"score": 0.002418453525649304, "phrase": "fairly_recent_optimization_method"}, {"score": 0.0023198180380964305, "phrase": "theoretic_proof"}, {"score": 0.0022878396720586044, "phrase": "convergence_rate"}, {"score": 0.0021843879253500894, "phrase": "empirical_evaluation"}, {"score": 0.0021049977753042253, "phrase": "proposed_semi-supervised_feature_selection_method"}], "paper_keywords": ["Feature selection", " level method", " manifold regularization", " multiple kernel learning", " semi-supervised learning"], "paper_abstract": "Feature selection has attracted a huge amount of interest in both research and application communities of data mining. We consider the problem of semi-supervised feature selection, where we are given a small amount of labeled examples and a large amount of unlabeled examples. Since a small number of labeled samples are usually insufficient for identifying the relevant features, the critical problem arising from semi-supervised feature selection is how to take advantage of the information underneath the unlabeled data. To address this problem, we propose a novel discriminative semi-supervised feature selection method based on the idea of manifold regularization. The proposed approach selects features through maximizing the classification margin between different classes and simultaneously exploiting the geometry of the probability distribution that generates both labeled and unlabeled data. In comparison with previous semi-supervised feature selection algorithms, our proposed semi-supervised feature selection method is an embedded feature selection method and is able to find more discriminative features. We formulate the proposed feature selection method into a convex-concave optimization problem, where the saddle point corresponds to the optimal solution. To find the optimal solution, the level method, a fairly recent optimization method, is employed. We also present a theoretic proof of the convergence rate for the application of the level method to our problem. Empirical evaluation on several benchmark data sets demonstrates the effectiveness of the proposed semi-supervised feature selection method.", "paper_title": "Discriminative Semi-Supervised Feature Selection Via Manifold Regularization", "paper_id": "WOS:000281973300001"}