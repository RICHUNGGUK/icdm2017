{"auto_keywords": [{"score": 0.04783182929351233, "phrase": "head_gesture"}, {"score": 0.04631644833296801, "phrase": "eye_tracking"}, {"score": 0.044027087409931595, "phrase": "face_detection"}, {"score": 0.04362260194050128, "phrase": "eye_location"}, {"score": 0.04184592131486041, "phrase": "integrated_feature_space"}, {"score": 0.026743842901613694, "phrase": "directional_vector"}, {"score": 0.00481495049065317, "phrase": "feature_interpolation"}, {"score": 0.004523455475492876, "phrase": "proposed_system"}, {"score": 0.00429057323623293, "phrase": "eye_tracking_step"}, {"score": 0.0041287394089001405, "phrase": "eye_feature_interpolation"}, {"score": 0.004030695216899165, "phrase": "face_region"}, {"score": 0.003953932197201054, "phrase": "multiple_bayesian_classifiers"}, {"score": 0.0038230844545071303, "phrase": "candidate_windows"}, {"score": 0.0035914198812657897, "phrase": "detected_face_region"}, {"score": 0.00334144503360907, "phrase": "real-time_eye_tracking"}, {"score": 0.003108814986752103, "phrase": "feature_vector"}, {"score": 0.0029343925814926787, "phrase": "detected_eyes"}, {"score": 0.002864628676257225, "phrase": "normalized_vector"}, {"score": 0.00242047093248845, "phrase": "head_movement"}, {"score": 0.0023857603808363527, "phrase": "hmms_vector"}, {"score": 0.0023066880707738736, "phrase": "neutral_as_well_as_positive_and_negative_gesture"}, {"score": 0.0021049977753042253, "phrase": "notable_success"}], "paper_keywords": [""], "paper_abstract": "This paper addresses a technique of recognizing a head gesture. The proposed system is composed of eye tracking and head motion decision. The eye tracking step is divided into face detection, eye location and eye feature interpolation. Face detection obtains the face region using integrated feature space. Multiple Bayesian classifiers are employed for selection of face candidate windows on integrated feature space. Eye location extracts the location of eyes from the detected face region. Eye location is performed at the region close to a pair of eyes for real-time eye tracking. If a pair of eyes is not located, the system can estimate feature vector using mean velocity measure(MVM). After eye tracking, the coordinates of the detected eyes are transformed into the normalized vector of the x-coordinate and the y-coordinate. Head gesture recognition using HMMs. Head gesture can be recognized by HMMs those are adapted by a directional vector. The directional vector represents the direction of head movement. The HMMs vector can also be used to determine neutral as well as positive and negative gesture. The experimental results are reported. These techniques are implemented on a lot of images and a notable success is notified.", "paper_title": "Head gesture recognition using feature interpolation", "paper_id": "WOS:000242122000070"}