{"auto_keywords": [{"score": 0.0378743853874972, "phrase": "labeling_noise"}, {"score": 0.01933456752940614, "phrase": "training_data"}, {"score": 0.006395913027164554, "phrase": "relevance_labels"}, {"score": 0.005046716813111295, "phrase": "graphical_model"}, {"score": 0.00481495049065317, "phrase": "noisy_data"}, {"score": 0.0047057084749407485, "phrase": "ranking_function"}, {"score": 0.004622450915803531, "phrase": "emerging_research_area"}, {"score": 0.004598933498016774, "phrase": "information_retrieval"}, {"score": 0.00457553517865013, "phrase": "machine_learning"}, {"score": 0.0042928648367867835, "phrase": "query_intent"}, {"score": 0.004238452771031322, "phrase": "domain_knowledge"}, {"score": 0.004195418010131832, "phrase": "vague_definition"}, {"score": 0.004174064076143015, "phrase": "relevance_levels"}, {"score": 0.00411064950774695, "phrase": "common_annotators"}, {"score": 0.004079302945139528, "phrase": "reliable_relevance_labels"}, {"score": 0.0035000584372426183, "phrase": "two-step_approach"}, {"score": 0.0034733518170738517, "phrase": "existing_algorithms"}, {"score": 0.0034468482725097723, "phrase": "noisy_training_data"}, {"score": 0.0034118234596993836, "phrase": "first_step"}, {"score": 0.003325805424442597, "phrase": "training_document"}, {"score": 0.0031042018721479385, "phrase": "generative_process"}, {"score": 0.0030805064329754747, "phrase": "training_query"}, {"score": 0.0030569913145731408, "phrase": "feature_vectors"}, {"score": 0.002889928791032578, "phrase": "maximum_likelihood_estimation"}, {"score": 0.002860546817196148, "phrase": "conditional_probability"}, {"score": 0.00283870598459543, "phrase": "relevance_label"}, {"score": 0.0028170314398963704, "phrase": "feature_vector"}, {"score": 0.0025694584194998335, "phrase": "second_step"}, {"score": 0.0025433263646920364, "phrase": "existing_learning-to-rank_algorithms"}, {"score": 0.0024476704470769504, "phrase": "larger_weights"}, {"score": 0.0024289743270061157, "phrase": "training_documents"}, {"score": 0.002416589459425828, "phrase": "smaller_degrees"}, {"score": 0.002392008392892289, "phrase": "smaller_weights"}, {"score": 0.002367676766637248, "phrase": "larger_degrees"}, {"score": 0.0022961534630650347, "phrase": "mcrank"}, {"score": 0.002272794656458419, "phrase": "rankboost"}, {"score": 0.002255431275670053, "phrase": "ranknet"}, {"score": 0.002243929279945891, "phrase": "empirical_results"}, {"score": 0.0022324858095648338, "phrase": "benchmark_datasets"}, {"score": 0.0022097732604884767, "phrase": "proposed_approach"}, {"score": 0.0021872912745910127, "phrase": "noisy_documents"}, {"score": 0.0021761360137091713, "phrase": "clean_ones"}, {"score": 0.0021049977753042253, "phrase": "better_performances"}], "paper_keywords": ["Noisy data", " robust learning"], "paper_abstract": "Learning to rank, which learns the ranking function from training data, has become an emerging research area in information retrieval and machine learning. Most existing work on learning to rank assumes that the training data is clean, which is not always true, however. The ambiguity of query intent, the lack of domain knowledge, and the vague definition of relevance levels all make it difficult for common annotators to give reliable relevance labels to some documents. As a result, the relevance labels in the training data of learning to rank usually contain noise. If we ignore this fact, the performance of learning-to-rank algorithms will be damaged. In this article, we propose considering the labeling noise in the process of learning to rank and using a two-step approach to extend existing algorithms to handle noisy training data. In the first step, we estimate the degree of labeling noise for a training document. To this end, we assume that the majority of the relevance labels in the training data are reliable and we use a graphical model to describe the generative process of a training query, the feature vectors of its associated documents, and the relevance labels of these documents. The parameters in the graphical model are learned by means of maximum likelihood estimation. Then the conditional probability of the relevance label given the feature vector of a document is computed. If the probability is large, we regard the degree of labeling noise for this document as small; otherwise, we regard the degree as large. In the second step, we extend existing learning-to-rank algorithms by incorporating the estimated degree of labeling noise into their loss functions. Specifically, we give larger weights to those training documents with smaller degrees of labeling noise and smaller weights to those with larger degrees of labeling noise. As examples, we demonstrate the extensions for McRank, RankSVM, RankBoost, and RankNet. Empirical results on benchmark datasets show that the proposed approach can effectively distinguish noisy documents from clean ones, and the extended learning-to-rank algorithms can achieve better performances than baselines.", "paper_title": "Learning to Rank from Noisy Data", "paper_id": "WOS:000363900100001"}