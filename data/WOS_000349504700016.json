{"auto_keywords": [{"score": 0.039246520904196544, "phrase": "k-nearest_neighbors"}, {"score": 0.036379424137657886, "phrase": "specific_challenges"}, {"score": 0.02893863895447082, "phrase": "conditional_mutual_information_maximization"}, {"score": 0.00481495049065317, "phrase": "classifier-specific_filter_measure_performance"}, {"score": 0.0047051051659828275, "phrase": "feature_selection"}, {"score": 0.004640398886726948, "phrase": "important_part"}, {"score": 0.004597754207658615, "phrase": "classifier_design"}, {"score": 0.004410629943506433, "phrase": "feature_subsets"}, {"score": 0.004077616905584829, "phrase": "filter-based_feature_subset_evaluation_measures"}, {"score": 0.0038935506745975835, "phrase": "specific_classifiers"}, {"score": 0.0037006243674469657, "phrase": "support_vector_machine_classifiers"}, {"score": 0.0031625006830124512, "phrase": "classifier_accuracy"}, {"score": 0.0030196181347744372, "phrase": "common_filter_measures"}, {"score": 0.0029234476524900794, "phrase": "best_filter_measure"}, {"score": 0.0027784560880013886, "phrase": "subset-based_relief"}, {"score": 0.0027528751037412128, "phrase": "correlation_feature_selection"}, {"score": 0.0026899455459275575, "phrase": "fisher's_interclass_separability_criterion"}, {"score": 0.0026163210719740847, "phrase": "support_vector_machines"}, {"score": 0.0025683581446365165, "phrase": "large_number"}, {"score": 0.0025212722607043546, "phrase": "feature_selection_measures"}, {"score": 0.0024409345168575833, "phrase": "single_measure"}, {"score": 0.0023198180380964305, "phrase": "single_classifier"}, {"score": 0.0022772782536385717, "phrase": "overall_performance"}, {"score": 0.0022458849724980904, "phrase": "feature_selection_method"}, {"score": 0.002164264600379492, "phrase": "subsequent_classifier"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feature selection", " Classification", " Filter measures"], "paper_abstract": "Feature selection is an important part of classifier design. There are many possible methods for searching and evaluating feature subsets, but little consensus on which methods are best. This paper examines a number of filter-based feature subset evaluation measures with the goal of assessing their performance with respect to specific classifiers. This work tests 16 common filter measures for use with K-nearest neighbors and support vector machine classifiers. The measures are tested on 20 real and 20 artificial data sets, which are designed to probe for specific challenges. The strengths and weaknesses of each measure are discussed with respect to the specific challenges and correlation with classifier accuracy. The results highlight several challenging problems with a number of common filter measures. The results indicate that the best filter measure is classifier-specific. K-nearest neighbors classifiers work well with subset-based RELIEF, correlation feature selection or conditional mutual information maximization, whereas Fisher's interclass separability criterion and conditional mutual information maximization work better for support vector machines. Despite the large number and variety of feature selection measures proposed in the literature, no single measure is guaranteed to outperform the others, even within a single classifier, and the overall performance of a feature selection method cannot be characterized independently of the subsequent classifier. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "An evaluation of classifier-specific filter measure performance for feature selection", "paper_id": "WOS:000349504700016"}