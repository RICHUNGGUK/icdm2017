{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "thread_migration_prediction"}, {"score": 0.004769702911065878, "phrase": "distributed_shared_caches"}, {"score": 0.004571269461883144, "phrase": "mainstream_parallel_architecture"}, {"score": 0.004464602028530629, "phrase": "scalability_reasons"}, {"score": 0.00438105507792004, "phrase": "high_core_counts"}, {"score": 0.0043194180167921165, "phrase": "tiled_cmps"}, {"score": 0.004278807087784626, "phrase": "physically_distributed_shared_caches"}, {"score": 0.003930000775297499, "phrase": "physical_distances"}, {"score": 0.0038382392088338784, "phrase": "home_cores"}, {"score": 0.0036958398682001015, "phrase": "data_locality"}, {"score": 0.003459191419478298, "phrase": "data_replication"}, {"score": 0.003015950477134494, "phrase": "shared_data_locality"}, {"score": 0.002987557906013826, "phrase": "nuca_designs"}, {"score": 0.002931569771634119, "phrase": "multiple_round-trip_remote_cache_accesses"}, {"score": 0.002822712709335093, "phrase": "high_migration_costs"}, {"score": 0.0027050595279115015, "phrase": "thread_migrations"}, {"score": 0.0025678829712650437, "phrase": "line_prediction_scheme"}, {"score": 0.0024842242921857705, "phrase": "remote_access"}, {"score": 0.002437645786158106, "phrase": "traditional_nuca_designs"}, {"score": 0.0023694065215394593, "phrase": "thread_migration"}, {"score": 0.002336004938424957, "phrase": "instruction_level"}, {"score": 0.0022706045233150795, "phrase": "parallel_benchmarks"}, {"score": 0.002145233716114996, "phrase": "shared-nuca_design"}, {"score": 0.0021049977753042253, "phrase": "remote_accesses"}], "paper_keywords": ["Parallel Architecture", " Distributed Caches", " Cache Coherence", " Data Locality"], "paper_abstract": "Chip-multiprocessors (CMPs) have become the mainstream parallel architecture in recent years; for scalability reasons, designs with high core counts tend towards tiled CMPs with physically distributed shared caches. This naturally leads to a Non-Uniform Cache Access (NUCA) design, where on-chip access latencies depend on the physical distances between requesting cores and home cores where the data is cached. Improving data locality is thus key to performance, and several studies have addressed this problem using data replication and data migration. In this paper, we consider another mechanism, hardware-level thread migration. This approach, we argue, can better exploit shared data locality for NUCA designs by effectively replacing multiple round-trip remote cache accesses with a smaller number of migrations. High migration costs, however, make it crucial to use thread migrations judiciously; we therefore propose a novel, on-line prediction scheme which decides whether to perform a remote access (as in traditional NUCA designs) or to perform a thread migration at the instruction level. For a set of parallel benchmarks, our thread migration predictor improves the performance by 24% on average over the shared-NUCA design that only uses remote accesses.", "paper_title": "Thread Migration Prediction for Distributed Shared Caches", "paper_id": "WOS:000344987900014"}