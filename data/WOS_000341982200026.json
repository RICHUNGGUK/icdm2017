{"auto_keywords": [{"score": 0.05007719498317069, "phrase": "minimum_variance_estimation"}, {"score": 0.03910717026765933, "phrase": "estimator_variance"}, {"score": 0.038107811724402994, "phrase": "cramer-rao"}, {"score": 0.033814640117899336, "phrase": "rkhs"}, {"score": 0.004662427600500598, "phrase": "sufficient_statistics"}, {"score": 0.004490552331642995, "phrase": "mathematical_theory"}, {"score": 0.00441883624011048, "phrase": "kernel_hilbert_spaces"}, {"score": 0.004301834610818624, "phrase": "powerful_tools"}, {"score": 0.004011867299107537, "phrase": "classical_rkhs-based_analysis"}, {"score": 0.00396902345997532, "phrase": "mve"}, {"score": 0.0038226070564362697, "phrase": "geometric_formulation"}, {"score": 0.003781772207256374, "phrase": "five_known_lower_bounds"}, {"score": 0.003489064521613279, "phrase": "bhattacharyya"}, {"score": 0.0034148805966330653, "phrase": "hammersley-chapman-robbins"}, {"score": 0.003306559295382181, "phrase": "orthogonal_projections"}, {"score": 0.0028293966058473476, "phrase": "lower_semicontinuous_function"}, {"score": 0.0027841357057119317, "phrase": "parameter_vector"}, {"score": 0.0026242699269026204, "phrase": "mve_problem"}, {"score": 0.0024868972942701582, "phrase": "sufficient_statistic"}, {"score": 0.0024209250169088575, "phrase": "mve_problems"}, {"score": 0.0023694065215394593, "phrase": "exponential_family"}, {"score": 0.0022941723937662927, "phrase": "novel_closed-form_lower_bounds"}, {"score": 0.0021049977753042253, "phrase": "minimum_achievable_variance"}], "paper_keywords": ["Minimum variance estimation", " exponential family", " reproducing kernel Hilbert space", " RKHS", " Cramer-Rao bound", " Barankin bound", " Hammersley-Chapman-Robbins bound", " Bhattacharyya bound", " locally minimum variance unbiased estimator"], "paper_abstract": "The mathematical theory of reproducing kernel Hilbert spaces (RKHSs) provides powerful tools for minimum variance estimation (MVE) problems. Here, we extend the classical RKHS-based analysis of MVE in several directions. We develop a geometric formulation of five known lower bounds on the estimator variance (Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections onto a subspace of the RKHS associated with a given MVE problem. We show that, under mild conditions, the Barankin bound (the tightest possible lower bound on the estimator variance) is a lower semicontinuous function of the parameter vector. We also show that the RKHS associated with an MVE problem remains unchanged if the observation is replaced by a sufficient statistic. Finally, for MVE problems conforming to an exponential family of distributions, we derive novel closed-form lower bounds on the estimator variance and show that a reduction of the parameter set leaves the minimum achievable variance unchanged.", "paper_title": "The RKHS Approach to Minimum Variance Estimation Revisited: Variance Bounds, Sufficient Statistics, and Exponential Families", "paper_id": "WOS:000341982200026"}