{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "stochastic_meta_descent"}, {"score": 0.004633429997452688, "phrase": "efficient_and_scalable_online_learning_algorithm"}, {"score": 0.0037743802306714545, "phrase": "output_links"}, {"score": 0.0036553070575015344, "phrase": "reduced_storage"}, {"score": 0.003608732492631878, "phrase": "computational_complexity"}, {"score": 0.0034503458458332082, "phrase": "smd"}, {"score": 0.003256808693019209, "phrase": "stochastic_gradient-descent_problems"}, {"score": 0.0030938932434437178, "phrase": "curvature_information"}, {"score": 0.002958021635132416, "phrase": "learning_process"}, {"score": 0.0028463059289874637, "phrase": "clustered_version"}, {"score": 0.0026693851900137953, "phrase": "dramatic_reduction"}, {"score": 0.002635339648135844, "phrase": "resource_requirements"}, {"score": 0.002535779689260244, "phrase": "simulation_results"}, {"score": 0.0024556852466581527, "phrase": "regular_rtrl"}, {"score": 0.0024243586247372087, "phrase": "almost_an_order"}, {"score": 0.002259054486157494, "phrase": "parallel_hardware_realization"}, {"score": 0.0021876814398268775, "phrase": "localized_property"}, {"score": 0.0021049977753042253, "phrase": "learning_framework"}], "paper_keywords": ["constrained optimization", " real-time recurrent learning (RTRL)", " recurrent neural networks (RNNs)"], "paper_abstract": "This brief presents an efficient and scalable online learning algorithm for recurrent neural networks (RNNs). The approach is based on the real-time recurrent learning (RTRL) algorithm, whereby the sensitivity set of each neuron is reduced to weights associated with either its input or output links. This yields a reduced storage and computational complexity of O(N-2). Stochastic meta descent (SMD), an adaptive step size scheme for stochastic gradient-descent problems, is employed as means of incorporating curvature information in order to substantially accelerate the learning process. We also introduce a clustered version of our algorithm to further improve its scalability attributes. Despite the dramatic reduction in resource requirements, it is shown through simulation results that the approach outperforms regular RTRL by almost an order of magnitude. Moreover, the scheme lends itself to parallel hardware realization by virtue of the localized property that is inherent to the learning framework.", "paper_title": "A fast and scalable recurrent neural network based on stochastic meta descent", "paper_id": "WOS:000259499900013"}