{"auto_keywords": [{"score": 0.029638871966991165, "phrase": "proposed_bandwidth_manager"}, {"score": 0.00481495049065317, "phrase": "live_migration"}, {"score": 0.004783730450730986, "phrase": "virtual_machines"}, {"score": 0.004676038405185284, "phrase": "dynamic_balanced_use"}, {"score": 0.004600596158672458, "phrase": "virtualized_datacenters"}, {"score": 0.004511663118269957, "phrase": "reduced_energy_consumption"}, {"score": 0.004438861041839598, "phrase": "bandwidth_consumption"}, {"score": 0.004381462398812729, "phrase": "current_state-of-the-art_live_vm_migration_techniques"}, {"score": 0.003960972732123083, "phrase": "field_trials"}, {"score": 0.0038843558517333327, "phrase": "intra-datacenter_live_migration"}, {"score": 0.003859193987752753, "phrase": "vms"}, {"score": 0.003747705541406206, "phrase": "migration-induced_communication_energy"}, {"score": 0.003663250792230243, "phrase": "hard_constraints"}, {"score": 0.0036276389022283427, "phrase": "total_migration_time"}, {"score": 0.0035458802311188497, "phrase": "migrating_applications"}, {"score": 0.0035228596881051763, "phrase": "overall_available_bandwidth"}, {"score": 0.0033548872248104814, "phrase": "geometric_programming"}, {"score": 0.0032685906228558706, "phrase": "suitably_developed_adaptive_version"}, {"score": 0.003236802914194731, "phrase": "so-called_primal-dual_gradient-based_iterations"}, {"score": 0.003082427650896418, "phrase": "resulting_bandwidth_manager"}, {"score": 0.0030524448705134283, "phrase": "intra-datacenter_wired_test-bed"}, {"score": 0.0029449743792479433, "phrase": "extensive_field_trials"}, {"score": 0.0029163246959273605, "phrase": "carried_out_field_trials"}, {"score": 0.002832032388853002, "phrase": "energy_savings"}, {"score": 0.002679387429189666, "phrase": "xen"}, {"score": 0.0026619765051779515, "phrase": "kvm"}, {"score": 0.0026446799936747797, "phrase": "vmware"}, {"score": 0.002576603426735759, "phrase": "strict_qos_constraints"}, {"score": 0.0024616504443079512, "phrase": "abrupt_changes"}, {"score": 0.00242184774931214, "phrase": "dirty_rates"}, {"score": 0.002398274973378986, "phrase": "running_applications"}, {"score": 0.002181882210287102, "phrase": "distributed_and_scalable_way"}, {"score": 0.00212569405594774, "phrase": "cpu_computing_power"}], "paper_keywords": ["Intra-datacenter live virtual machine migration", " Quality of service", " Adaptive and distributed bandwidth management", " Energy-saving", " Software prototype"], "paper_abstract": "Live virtual machine (VM) migration aims at enabling the dynamic balanced use of the networking/computing physical resources of virtualized datacenters, so to lead to reduced energy consumption. However, the bandwidth consumption and latency of current state-of-the-art live VM migration techniques still reduce the experienced benefits to much less than their potential. Motivated by this consideration, in this paper, we analytically characterize, prototype in software and test through field trials the optimal bandwidth manager for intra-datacenter live migration of VMs. The goal is the minimization of the migration-induced communication energy under service level agreement (SLA)-induced hard constraints on the total migration time, downtime, slowdown of the migrating applications and overall available bandwidth. For this purpose, after recognizing that the resulting (nonconvex) optimization problem is an instance of Geometric Programming, we solve it by resorting to suitably developed adaptive version of the so-called primal-dual gradient-based iterations and, then, we analytically characterize its feasibility conditions. Hence, we prototype the resulting bandwidth manager atop an intra-datacenter wired test-bed, and, then, test and compare its energy performance through extensive field trials. The carried out field trials point out that: (i) the energy savings attained by the proposed bandwidth manager over the state-of-the-art ones currently utilized by Xen, KVM and VMware hypervisors are over 40% and approach 66% under strict QoS constraints; (ii) the proposed bandwidth manager is capable to quickly adapt to the abrupt changes possibly experienced by the dirty rates of the running applications and/or the round trip times of the utilized (possibly, congested) TCP/IP connections; and, (iii) its actual implementation may be carried out in a distributed and scalable way, and it consumes less than 1.5% of the CPU computing power per migrated VM. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Minimum-energy bandwidth management for QoS live migration of virtual machines", "paper_id": "WOS:000367123100001"}