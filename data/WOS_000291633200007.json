{"auto_keywords": [{"score": 0.031369790019902345, "phrase": "herding"}, {"score": 0.00481495049065317, "phrase": "quickly_generating_representative"}, {"score": 0.004701015188740582, "phrase": "rbm-derived_process"}, {"score": 0.003999089046152065, "phrase": "model_parameters"}, {"score": 0.0036993855514735746, "phrase": "sampling_process"}, {"score": 0.0035687137873965684, "phrase": "learning_algorithm"}, {"score": 0.0032229262928681304, "phrase": "markov_chain"}, {"score": 0.003071965703296204, "phrase": "fpcd"}, {"score": 0.0028586409810976367, "phrase": "pure_sampling_algorithm"}, {"score": 0.0023592715993204796, "phrase": "log_likelihood"}, {"score": 0.0023171750718791713, "phrase": "training_data"}, {"score": 0.002262209824105154, "phrase": "empirical_evidence"}, {"score": 0.0022218413146898887, "phrase": "new_algorithm"}, {"score": 0.0021049977753042253, "phrase": "gibbs_sampling"}], "paper_keywords": [""], "paper_abstract": "Two recently proposed learning algorithms, herding and fast persistent contrastive divergence (FPCD), share the following interesting characteristic: they exploit changes in the model parameters while sampling in order to escape modes and mix better during the sampling process that is part of the learning algorithm. We justify such approaches as ways to escape modes while keeping approximately the same asymptotic distribution of the Markov chain. In that spirit, we extend FPCD using an idea borrowed from Herding in order to obtain a pure sampling algorithm, which we call the rates-FPCD sampler. Interestingly, this sampler can improve the model as we collect more samples, since it optimizes a lower bound on the log likelihood of the training data. We provide empirical evidence that this new algorithm displays substantially better and more robust mixing than Gibbs sampling.", "paper_title": "Quickly Generating Representative Samples from an RBM-Derived Process", "paper_id": "WOS:000291633200007"}