{"auto_keywords": [{"score": 0.03279713920666235, "phrase": "ir."}, {"score": 0.00481495049065317, "phrase": "inverted_files"}, {"score": 0.004550300345169981, "phrase": "static_pruning"}, {"score": 0.0044524122425287005, "phrase": "lossy_data_compression"}, {"score": 0.0041170581247889654, "phrase": "retrieval_performance"}, {"score": 0.003958937342925219, "phrase": "pruning_criteria"}, {"score": 0.0038906171474520756, "phrase": "term_weighting_functions"}, {"score": 0.0036926236934657864, "phrase": "document's_contents"}, {"score": 0.0036288825833675127, "phrase": "document-term_occurrences"}, {"score": 0.0035507457285983268, "phrase": "low_weight"}, {"score": 0.0034143003168921114, "phrase": "main_assumption"}, {"score": 0.0032830808525570903, "phrase": "document_content"}, {"score": 0.00321236577162671, "phrase": "novel_pruning_technique"}, {"score": 0.003129509028432954, "phrase": "probabilistic_model"}, {"score": 0.0029830992938695007, "phrase": "decision_criterion"}, {"score": 0.0029443683459283955, "phrase": "posting_list_entries"}, {"score": 0.0028559348655053545, "phrase": "proposed_approach"}, {"score": 0.002640507929183363, "phrase": "aforementioned_criterion"}, {"score": 0.0025389513338018414, "phrase": "five_trec_collections"}, {"score": 0.0024519545383399773, "phrase": "almost_every_situation"}, {"score": 0.002357632753637638, "phrase": "index_pruning"}, {"score": 0.0023270035715132866, "phrase": "main_contribution"}, {"score": 0.002257070536352418, "phrase": "pruning_technique"}, {"score": 0.0021797112385590913, "phrase": "probabilistic_retrieval_models"}, {"score": 0.0021049977753042253, "phrase": "final_model"}], "paper_keywords": ["Algorithms", " Experimentation", " Performance", " Pruning", " inverted files", " probabilistic models", " compression", " efficiency"], "paper_abstract": "Information retrieval (IR) systems typically compress their indexes in order to increase their efficiency. Static pruning is a form of lossy data compression: it removes from the index, data that is estimated to be the least important to retrieval performance, according to some criterion. Generally, pruning criteria are derived from term weighting functions, which assign weights to terms according to their contribution to a document's contents. Usually, document-term occurrences that are assigned a low weight are ruled out from the index. The main assumption is that those entries contribute little to the document content. We present a novel pruning technique that is based on a probabilistic model of IR. We employ the Probability Ranking Principle as a decision criterion over which posting list entries are to be pruned. The proposed approach requires the estimation of three probabilities, combining them in such a way that we gather all the necessary information to apply the aforementioned criterion. We evaluate our proposed pruning technique on five TREC collections and various retrieval tasks, and show that in almost every situation it outperforms the state of the art in index pruning. The main contribution of this work is proposing a pruning technique that stems directly from the same source as probabilistic retrieval models, and hence is independent of the final model used for retrieval.", "paper_title": "Probabilistic Static Pruning of Inverted Files", "paper_id": "WOS:000274028700001"}