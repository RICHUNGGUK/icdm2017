{"auto_keywords": [{"score": 0.03171939562914439, "phrase": "lu"}, {"score": 0.0055254472601095955, "phrase": "qr"}, {"score": 0.005201734237684065, "phrase": "lapack"}, {"score": 0.00481495049065317, "phrase": "lapack_panel_operations"}, {"score": 0.004766683200410046, "phrase": "parallel_cache_assignment"}, {"score": 0.004578382275810468, "phrase": "block_algorithms"}, {"score": 0.004397487039229447, "phrase": "unblocked_algorithm"}, {"score": 0.004288059466938456, "phrase": "remainder_matrix"}, {"score": 0.0040978781692373005, "phrase": "excellent_weak_scaling"}, {"score": 0.004036369952624852, "phrase": "panel_processing"}, {"score": 0.0038186039088445524, "phrase": "bus_speed"}, {"score": 0.0036125439380579626, "phrase": "amdahl's_law"}, {"score": 0.003452218803245602, "phrase": "panel_computation"}, {"score": 0.0033832583769880576, "phrase": "dominant_cost"}, {"score": 0.003332440548684892, "phrase": "lapack_routines"}, {"score": 0.0032494292163722065, "phrase": "novel_parallel_cache_assignment_approach"}, {"score": 0.003073988343893906, "phrase": "general_approach"}, {"score": 0.0028498718992429825, "phrase": "superlinear_panel_factorization_speedups"}, {"score": 0.002709589081384597, "phrase": "complicated_reformulations"}, {"score": 0.0026688629249775925, "phrase": "computational_approach"}, {"score": 0.0026420518944303716, "phrase": "new_kernels"}, {"score": 0.0025761937402886954, "phrase": "new_mathematics"}, {"score": 0.0024993218789131437, "phrase": "high-order_flop_count"}, {"score": 0.0023523750536537102, "phrase": "straight-forward_alternative"}, {"score": 0.0021807544600219216, "phrase": "critical_stumbling_block"}, {"score": 0.002158836285985314, "phrase": "dense_linear_algebra"}, {"score": 0.0021049977753042253, "phrase": "massive_parallelism"}], "paper_keywords": ["Performance", " Experimentation", " Design", " Algorithms", " ATLAS", " LAPACK", " QR", " LU", " factorization", " parallel", " multicore", " multi-core", " GPU"], "paper_abstract": "In LAPACK many matrix operations are cast as block algorithms which iteratively process a panel using an unblocked algorithm and then update a remainder matrix using the high performance Level 3 BLAS. The Level 3 BLAS have excellent weak scaling, but panel processing tends to be bus bound, and thus scales with bus speed rather than the number of processors (p). Amdahl's law therefore ensures that as p grows, the panel computation will become the dominant cost of these LAPACK routines. Our contribution is a novel parallel cache assignment approach which we show scales well with p. We apply this general approach to the QR and LU panel factorizations on two commodity 8-core platforms with very different cache structures, and demonstrate superlinear panel factorization speedups on both machines. Other approaches to this problem demand complicated reformulations of the computational approach, new kernels to be tuned, new mathematics, an inflation of the high-order flop count, and do not perform as well. By demonstrating a straight-forward alternative that avoids all of these contortions and scales with p, we address a critical stumbling block for dense linear algebra in the age of massive parallelism.", "paper_title": "Scaling LAPACK Panel Operations Using Parallel Cache Assignment", "paper_id": "WOS:000280548100021"}