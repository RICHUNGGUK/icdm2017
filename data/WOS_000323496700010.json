{"auto_keywords": [{"score": 0.045417031794566444, "phrase": "realistic_scenarios"}, {"score": 0.015014189695959043, "phrase": "human_action_recognition"}, {"score": 0.00481495049065317, "phrase": "realistic_human_action_recognition_with_multimodal_feature_selection"}, {"score": 0.00477114275418626, "phrase": "fusion"}, {"score": 0.004704495156558224, "phrase": "promising_results"}, {"score": 0.004554082077300675, "phrase": "well-controlled_conditions"}, {"score": 0.004388035083357706, "phrase": "human_actions"}, {"score": 0.00428733103049275, "phrase": "increased_difficulties"}, {"score": 0.004228016636379321, "phrase": "dynamic_backgrounds"}, {"score": 0.003888905257972882, "phrase": "realistic_human_action_videos"}, {"score": 0.003764469758319617, "phrase": "first_time"}, {"score": 0.003644001325962353, "phrase": "audio_signals"}, {"score": 0.0031845067058084583, "phrase": "diverse_audio_cues"}, {"score": 0.0030257422786589723, "phrase": "effective_features"}, {"score": 0.0029838294169759663, "phrase": "large_number"}, {"score": 0.002956209594959227, "phrase": "audio_features"}, {"score": 0.0029152570900178956, "phrase": "generalized_multiple_kernel_learning_algorithm"}, {"score": 0.002874870266177579, "phrase": "widely_used_space-time_interest_point_descriptors"}, {"score": 0.0027570267191549774, "phrase": "support_vector_machine"}, {"score": 0.0025832180231667853, "phrase": "final_stage"}, {"score": 0.002488851299416018, "phrase": "recognition_results"}, {"score": 0.0024543571925896073, "phrase": "audio_and_visual_modalities"}, {"score": 0.002331909406363425, "phrase": "proposed_approach"}, {"score": 0.002278284823857861, "phrase": "better_recognition_performance_improvement"}, {"score": 0.0022258906408344973, "phrase": "scene_context"}, {"score": 0.0021246816854197732, "phrase": "realistic_action_recognition"}], "paper_keywords": ["Fuzzy integral", " multimodal fusion", " multiple kernel learning (MKL)", " realistic human action recognition"], "paper_abstract": "Although promising results have been achieved for human action recognition under well-controlled conditions, it is very challenging to recognize human actions in realistic scenarios due to increased difficulties such as dynamic backgrounds. In this paper, we propose to take multimodal (i.e., audiovisual) characteristics of realistic human action videos into account in human action recognition for the first time, since, in realistic scenarios, audio signals accompanying an action generally provide a cue to the nature of the action, such as phone ringing to answering the phone. In order to cope with diverse audio cues of an action in realistic scenarios, we propose to identify effective features from a large number of audio features with the generalized multiple kernel learning algorithm. The widely used space-time interest point descriptors are utilized as visual features, and a support vector machine is employed for both audio- and video-based classifications. At the final stage, fuzzy integral is utilized to fuse recognition results of both audio and visual modalities. Experimental results on the challenging Hollywood-2 Human Action data set demonstrate that the proposed approach is able to achieve better recognition performance improvement than that of integrating scene context. It is also discovered how audio context influences realistic action recognition from our comprehensive experiments.", "paper_title": "Realistic Human Action Recognition With Multimodal Feature Selection and Fusion", "paper_id": "WOS:000323496700010"}