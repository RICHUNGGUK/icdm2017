{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "algorithmically_constructed_classification_systems"}, {"score": 0.04786476323484584, "phrase": "citation_practices"}, {"score": 0.0468746567066957, "phrase": "citation_impact_indicators"}, {"score": 0.02914314450208736, "phrase": "classification_system"}, {"score": 0.0046597835420572825, "phrase": "normalizing_citation_impact_indicators"}, {"score": 0.0045591142048535165, "phrase": "scientific_fields"}, {"score": 0.004380143676997257, "phrase": "field_classification_system"}, {"score": 0.004285489630701221, "phrase": "web_of_science_journal_subject_categories"}, {"score": 0.0040873354289955605, "phrase": "subject_categories"}, {"score": 0.004042919099733537, "phrase": "quite_broad_scope"}, {"score": 0.0036909954181293405, "phrase": "classification_systems"}, {"score": 0.0036375920152538783, "phrase": "large-scale_clustering"}, {"score": 0.003419084516677794, "phrase": "different_granularity_level"}, {"score": 0.0032371745435372168, "phrase": "granularity_levels"}, {"score": 0.0031212935348800467, "phrase": "wos_classification_system"}, {"score": 0.0030315735229977958, "phrase": "key_characteristics"}, {"score": 0.0028702214190712036, "phrase": "optimal_choice"}, {"score": 0.002717433692385074, "phrase": "citation_impact"}, {"score": 0.0026201089205321704, "phrase": "cwts_leiden_ranking"}, {"score": 0.0023229408675543147, "phrase": "individual_universities"}, {"score": 0.002280948283235427, "phrase": "substantial_differences"}, {"score": 0.0021515951441422082, "phrase": "appropriately_chosen_algorithmically_constructed_classification_system"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Field normalization", " Classification systems", " Clustering methodology", " Citation impact indicators", " University rankings"], "paper_abstract": "We study the problem of normalizing citation impact indicators for differences in citation practices across scientific fields. Normalization of citation impact indicators is usually done based on a field classification system. In practice, the Web of Science journal subject categories are often used for this purpose. However, many of these subject categories have a quite broad scope and are not sufficiently homogeneous in terms of citation practices. As an alternative, we propose to work with algorithmically constructed classification systems. We construct these classification systems by performing a large-scale clustering of publications based on their citation relations. In our analysis, 12 classification systems are constructed, each at a different granularity level. The number of fields in these systems ranges from 390 to 73,205 in granularity levels 1-12. This contrasts with the 236 subject categories in the WoS classification system. Based on an investigation of some key characteristics of the 12 classification systems, we argue that working with a few thousand fields may be an optimal choice. We then study the effect of the choice of a classification system on the citation impact of the 500 universities included in the 2013 edition of the CWTS Leiden Ranking. We consider both the MNCS and the PPtop 10% indicator. Globally, for all the universities taken together citation impact indicators generally turn out to be relatively insensitive to the choice of a classification system. Nevertheless, for individual universities, we sometimes observe substantial differences between indicators normalized based on the journal subject categories and indicators normalized based on an appropriately chosen algorithmically constructed classification system. (C) 2014 Elsevier Ltd. All rights reserved.", "paper_title": "Field-normalized citation impact indicators using algorithmically constructed classification systems of science", "paper_id": "WOS:000349770500012"}