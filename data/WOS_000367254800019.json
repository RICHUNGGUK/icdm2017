{"auto_keywords": [{"score": 0.005553405629313146, "phrase": "hsync"}, {"score": 0.00498079708717249, "phrase": "future_performance"}, {"score": 0.004814957820695754, "phrase": "sync"}, {"score": 0.0047842129415024475, "phrase": "async"}, {"score": 0.004693167092003573, "phrase": "distributed_graph-parallel_computation"}, {"score": 0.0046632033374901715, "phrase": "large-scale_graph-structured_computation"}, {"score": 0.004618614357974438, "phrase": "iterative_and_convergence-oriented_computing_nature"}, {"score": 0.004473027089024405, "phrase": "convergence_condition"}, {"score": 0.004452678943618141, "phrase": "powerswitch"}, {"score": 0.00429057323623293, "phrase": "graph-structured_programs"}, {"score": 0.003798654583736764, "phrase": "deep_understanding"}, {"score": 0.0037743802306714545, "phrase": "underlying_graph_engines"}, {"score": 0.003714368305523811, "phrase": "suboptimal_performance"}, {"score": 0.003643607569483598, "phrase": "first_comprehensive_characterization"}, {"score": 0.0035173498506324476, "phrase": "typical_graph-parallel_applications"}, {"score": 0.003362944428837724, "phrase": "different_graph_algorithms"}, {"score": 0.003194736764837261, "phrase": "single_mode"}, {"score": 0.002958021635132416, "phrase": "graph-parallel_program"}, {"score": 0.0029109517286657486, "phrase": "optimal_performance"}, {"score": 0.002864628676257225, "phrase": "execution_statistics"}, {"score": 0.002703869368554536, "phrase": "mode_switch"}, {"score": 0.002635339648135844, "phrase": "online_sampling"}, {"score": 0.002408845758886625, "phrase": "powergraph"}, {"score": 0.002280916026414715, "phrase": "adaptive_execution"}, {"score": 0.0022663183381878044, "phrase": "graph_algorithms"}, {"score": 0.0021049977753042253, "phrase": "timely_switch"}], "paper_keywords": ["Distributed Graph-parallel Computation", " Computation Modes"], "paper_abstract": "Large-scale graph-structured computation usually exhibits iterative and convergence-oriented computing nature, where input data is computed iteratively until a convergence condition is reached. Such features have led to the development of two different computation modes for graph-structured programs, namely synchronous (Sync) and asynchronous (Async) modes. Unfortunately, there is currently no in-depth study on their execution properties and thus programmers have to manually choose a mode, either requiring a deep understanding of underlying graph engines, or suffering from suboptimal performance. This paper makes the first comprehensive characterization on the performance of the two modes on a set of typical graph-parallel applications. Our study shows that the performance of the two modes varies significantly with different graph algorithms, partitioning methods, execution stages, input graphs and cluster scales, and no single mode consistently outperforms the other. To this end, this paper proposes Hsync, a hybrid graph computation mode that adaptively switches a graph-parallel program between the two modes for optimal performance. Hsync constantly collects execution statistics on-the-fly and leverages a set of heuristics to predict future performance and determine when a mode switch could be profitable. We have built online sampling and offline profiling approaches combined with a set of heuristics to accurately predicting future performance in the two modes. A prototype called PowerSwitch has been built based on PowerGraph, a state-of-the-art distributed graph-parallel system, to support adaptive execution of graph algorithms. On a 48-node EC2-like cluster, PowerSwitch consistently outperforms the best of both modes, with a speedup ranging from 9% to 73% due to timely switch between two modes.", "paper_title": "SYNC or ASYNC: Time to Fuse for Distributed Graph-Parallel Computation", "paper_id": "WOS:000367254800019"}