{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dynamic_algorithm_portfolios"}, {"score": 0.004753194996637899, "phrase": "algorithm_selection"}, {"score": 0.004513957235071457, "phrase": "runtime_distribution"}, {"score": 0.0043705604923728195, "phrase": "preliminary_training_phase"}, {"score": 0.00409723267214392, "phrase": "model-based_algorithm_selection"}, {"score": 0.0035543706594585076, "phrase": "bandit_problems"}, {"score": 0.003441352119167824, "phrase": "fully_dynamic_and_online_algorithm_selection_technique"}, {"score": 0.0032468685934998335, "phrase": "candidate_algorithms"}, {"score": 0.0029658905122290536, "phrase": "redundant_set"}, {"score": 0.002927780883930371, "phrase": "time_allocators"}, {"score": 0.0028715299874549245, "phrase": "partially_trained_model"}, {"score": 0.00281635677605879, "phrase": "machine_time_shares"}, {"score": 0.002709161672473817, "phrase": "bandit_problem_solver"}, {"score": 0.0026570998719084153, "phrase": "model-based_shares"}, {"score": 0.0026060359255636444, "phrase": "uniform_share"}, {"score": 0.00247460024299964, "phrase": "best_time_allocators"}, {"score": 0.0022749744354271816, "phrase": "sat_solvers"}, {"score": 0.0022312377180023282, "phrase": "mixed_sat-unsat_benchmark"}], "paper_keywords": ["algorithm selection", " algorithm portfolios", " online learning", " life-long learning", " bandit problem", " expert advice", " survival analysis", " satisfiability", " constraint programming"], "paper_abstract": "Algorithm selection can be performed using a model of runtime distribution, learned during a preliminary training phase. There is a trade-off between the performance of model-based algorithm selection, and the cost of learning the model. In this paper, we treat this trade-off in the context of bandit problems. We propose a fully dynamic and online algorithm selection technique, with no separate training phase: all candidate algorithms are run in parallel, while a model incrementally learns their runtime distributions. A redundant set of time allocators uses the partially trained model to propose machine time shares for the algorithms. A bandit problem solver mixes the model-based shares with a uniform share, gradually increasing the impact of the best time allocators as the model improves. We present experiments with a set of SAT solvers on a mixed SAT-UNSAT benchmark; and with a set of solvers for the Auction Winner Determination problem.", "paper_title": "Learning dynamic algorithm portfolios", "paper_id": "WOS:000244292100005"}