{"auto_keywords": [{"score": 0.049324540543845184, "phrase": "online_gradient_algorithm"}, {"score": 0.015719716506582538, "phrase": "pi-sigma_network"}, {"score": 0.010941742795972686, "phrase": "update_increment"}, {"score": 0.004590820632326585, "phrase": "small_weight_update"}, {"score": 0.0043080553835735825, "phrase": "feedforward_neural_networks"}, {"score": 0.004240116703465252, "phrase": "product_units"}, {"score": 0.004140203942496321, "phrase": "output_layer"}, {"score": 0.003947358102140903, "phrase": "simplest_and_most_often_used_training_method"}, {"score": 0.0034482025128821548, "phrase": "pi-sigma_networks"}, {"score": 0.002694375425481759, "phrase": "adaptive_penalty_term"}, {"score": 0.0026307897899760383, "phrase": "error_function"}, {"score": 0.0021049977753042253, "phrase": "numerical_experiments"}], "paper_keywords": [""], "paper_abstract": "A pi-sigma network is a class of feedforward neural networks with product units in the output layer. An online gradient algorithm is the simplest and most often used training method for feedforward neural networks. But there arises a problem when the online gradient algorithm is used for pi-sigma networks in that the update increment of the weights may become very small, especially early in training, resulting in a very slow convergence. To overcome this difficulty, we introduce an adaptive penalty term into the error function, so as to increase the magnitude of the update increment of the weights when it is too small. This strategy brings about faster convergence as shown by the numerical experiments carried out in this letter.", "paper_title": "Training pi-sigma network by online gradient algorithm with penalty for small weight update", "paper_id": "WOS:000250751900010"}