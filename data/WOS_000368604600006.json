{"auto_keywords": [{"score": 0.04094912027989201, "phrase": "communication_cost"}, {"score": 0.038115196221187, "phrase": "hpc_data_centers"}, {"score": 0.03443316687236447, "phrase": "system_performance"}, {"score": 0.00481495049065317, "phrase": "hpc_job_allocation"}, {"score": 0.004731492618963217, "phrase": "communication_and_cooling_costs"}, {"score": 0.004608997772926119, "phrase": "critical_aspects"}, {"score": 0.004568871736067739, "phrase": "high_performance_computing"}, {"score": 0.0044896599221451216, "phrase": "data_centers"}, {"score": 0.004450568197899267, "phrase": "highly_parallel_hpc_applications"}, {"score": 0.004392565073466714, "phrase": "multiple_nodes"}, {"score": 0.004316396683332752, "phrase": "long_durations"}, {"score": 0.004042295865725559, "phrase": "parallel_applications"}, {"score": 0.003802135020791801, "phrase": "significant_impact"}, {"score": 0.003769006950727621, "phrase": "data_center_performance"}, {"score": 0.0037361664435836845, "phrase": "energy_consumption"}, {"score": 0.0036553070575015344, "phrase": "first_-order_constraint"}, {"score": 0.003498782065781398, "phrase": "computing_clusters"}, {"score": 0.003408091901859624, "phrase": "cooling_infrastructure"}, {"score": 0.0032054891061528896, "phrase": "cooling_energy_cost"}, {"score": 0.0031636612155559267, "phrase": "server_nodes"}, {"score": 0.0030281251520610604, "phrase": "cooling_energy_consumption"}, {"score": 0.0029755458462182565, "phrase": "open_question"}, {"score": 0.002898378776839434, "phrase": "job_allocation_methodology"}, {"score": 0.0027863537694678094, "phrase": "cooling_energy"}, {"score": 0.0025863988422430797, "phrase": "structural_simulation_toolkit"}, {"score": 0.0024864020775253767, "phrase": "large-scale_data_centers"}, {"score": 0.0023902621773166963, "phrase": "real-world_workloads"}, {"score": 0.002287783663138828, "phrase": "performance-aware_job_allocation_algorithms"}, {"score": 0.0022382002476510573, "phrase": "comparable_running_times"}, {"score": 0.0021993066642488237, "phrase": "cooling_power"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["High-performance computing", " Data center", " Job allocation", " Joint optimization", " Cooling energy", " Communication cost"], "paper_abstract": "Performance and energy are critical aspects in high performance computing (HPC) data centers. Highly parallel HPC applications that require multiple nodes usually run for long durations in the range of minutes, hours or days. As the threads of parallel applications communicate with each other intensively, the communication cost of these applications has a significant impact on data center performance. Energy consumption has also become a first -order constraint of HPC data centers. Nearly half of the energy in the computing clusters today is consumed by the cooling infrastructure. Existing job allocation policies either target improving the system performance or reducing the cooling energy cost of the server nodes. How to optimize the system performance while minimizing the cooling energy consumption is still an open question. This paper proposes a job allocation methodology aimed at jointly reducing the communication cost and the cooling energy of HPC data centers. In order to evaluate and validate our optimization algorithm, we implement our joint job allocation methodology in the structural simulation toolkit (SST) - a simulation framework for large-scale data centers. We evaluate our joint optimization algorithm using traces extracted from real-world workloads. Experimental results show that, in comparison to performance-aware job allocation algorithms, our algorithm achieves comparable running times and reduces the cooling power by up to 42.21% across all the jobs. (C) 2014 Elsevier Inc. All rights reserved.", "paper_title": "Simulation and optimization of HPC job allocation for jointly reducing communication and cooling costs", "paper_id": "WOS:000368604600006"}