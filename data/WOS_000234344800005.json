{"auto_keywords": [{"score": 0.04791786215360241, "phrase": "threshold_functions"}, {"score": 0.04524114703359911, "phrase": "hardware_implementation"}, {"score": 0.00481495049065317, "phrase": "hardware_realizable_multilayer_perceptrons"}, {"score": 0.0047051051659828275, "phrase": "multilayer_perceptrons"}, {"score": 0.004661930953187312, "phrase": "mlp"}, {"score": 0.004290112502811656, "phrase": "neural_networks"}, {"score": 0.004058826900076531, "phrase": "internal_representations"}, {"score": 0.0038935506745975835, "phrase": "stationary_tasks"}, {"score": 0.003752275718257509, "phrase": "appropriate_weights"}, {"score": 0.0036665842836420223, "phrase": "threshold_activation_functions"}, {"score": 0.0036328561748082138, "phrase": "software_simulation"}, {"score": 0.003517223847512266, "phrase": "weight_values"}, {"score": 0.0034368818187904744, "phrase": "efficient_training"}, {"score": 0.003312121699003206, "phrase": "considerable_ongoing_research"}, {"score": 0.0029780219173522115, "phrase": "error_function"}, {"score": 0.002909960617042717, "phrase": "gradient_descent"}, {"score": 0.0027784560880013886, "phrase": "activation_functions"}, {"score": 0.0026651772919894534, "phrase": "evolution-motivated_approach"}, {"score": 0.0024636239664210433, "phrase": "proposed_evolutionary_strategy"}, {"score": 0.002418453525649304, "phrase": "gradient_related_information"}, {"score": 0.0023198180380964305, "phrase": "threshold_activations"}, {"score": 0.0021945194903838132, "phrase": "\"on-chip\"_training"}, {"score": 0.0021049977753042253, "phrase": "integer_weights"}], "paper_keywords": ["feedforward neural networks", " backpropagation algorithm", " neural networks with threshold activations", " integer weight neural networks", " integer programming", " steepest descent", " unconstrained optimization", " differential evolution"], "paper_abstract": "The use of multilayer perceptrons (MLP) with threshold functions (binary step function activations) greatly reduces the complexity of the hardware implementation of neural networks, provides tolerance to noise and improves the interpretation of the internal representations. In certain case, such as in learning stationary tasks, it may be sufficient to find appropriate weights for an MLP with threshold activation functions by software simulation and, then, transfer the weight values to the hardware implementation. Efficient training of these networks is a subject of considerable ongoing research. Methods available in the literature mainly focus on two-state (threshold) nodes and try to train the networks by approximating the gradient of the error function and modifying appropriately the gradient descent, or by progressively altering the shape of the activation functions. In this paper, we propose an evolution-motivated approach, which is eminently suitable for networks with threshold functions and compare its performance with four other methods. The proposed evolutionary strategy does not need gradient related information, it is applicable to a situation where threshold activations are used from the beginning of the training, as in \"on-chip\" training, and is able to train networks with integer weights.", "paper_title": "Evolutionary training of hardware realizable multilayer perceptrons", "paper_id": "WOS:000234344800005"}