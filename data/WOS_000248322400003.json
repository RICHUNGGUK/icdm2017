{"auto_keywords": [{"score": 0.044222651518052894, "phrase": "source-driven_method"}, {"score": 0.04331239719139538, "phrase": "individual_approaches"}, {"score": 0.0412575500402469, "phrase": "spectrum_envelope"}, {"score": 0.00481495049065317, "phrase": "monaural_speech_segregation"}, {"score": 0.004661116930794176, "phrase": "model-driven_techniques"}, {"score": 0.004528488165249491, "phrase": "prevalent_methods"}, {"score": 0.004399616575575586, "phrase": "new_single_channel_speech_segregation_technique"}, {"score": 0.004274396662662802, "phrase": "model-driven_method"}, {"score": 0.004019964326034457, "phrase": "harmonic_modelling"}, {"score": 0.003891430578662587, "phrase": "main_components"}, {"score": 0.003849502288990788, "phrase": "analysis_and_synthesis_stages"}, {"score": 0.0035683401689358993, "phrase": "new_model-driven_technique"}, {"score": 0.003416963042033614, "phrase": "quantized_envelopes"}, {"score": 0.0030219357765636497, "phrase": "previous_model-driven_techniques"}, {"score": 0.0028936710552719806, "phrase": "unvoiced_regions"}, {"score": 0.00283158908387463, "phrase": "crosstalk_effect"}, {"score": 0.0026532001044943096, "phrase": "source-driven_techniques"}, {"score": 0.002624577999455744, "phrase": "subjective_and_objective_experiments"}, {"score": 0.0025405463228033486, "phrase": "speaker-dependent_case"}, {"score": 0.002522240133079871, "phrase": "model-based_separation"}, {"score": 0.0024950272594582673, "phrase": "best_quality"}, {"score": 0.0024503217593801587, "phrase": "independent_scenario"}, {"score": 0.0024326641072463157, "phrase": "integrated_model"}, {"score": 0.00232935232323113, "phrase": "human_auditory_system"}, {"score": 0.0022958971655679832, "phrase": "grouping_cues"}, {"score": 0.002230418233881411, "phrase": "priori_knowledge"}, {"score": 0.002151183757599893, "phrase": "speech_signals"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["speech processing", " monaural speech segregation", " CASA", " speech coding", " harmonic modelling", " vector quantization", " envelope extraction", " multi-pitch tracking", " MIXMAX estimator"], "paper_abstract": "In this paper by exploiting the prevalent methods in speech coding and synthesis, a new single channel speech segregation technique is presented. The technique integrates a model-driven method with a source-driven method to take advantage of both individual approaches and reduce their pitfalls significantly. We apply harmonic modelling in which the pitch and spectrum envelope are the main components for the analysis and synthesis stages. Pitch values of two speakers are obtained by using a source-driven method. The spectrum envelope, is obtained by using a new model-driven technique consisting of four components: a trained codebook of the vector quantized envelopes (VQ-based separation), a mixture-maximum approximation (MIXMAX), minimum mean square error estimator (MMSE), and a harmonic synthesizer. In contrast with previous model-driven techniques, this approach is speaker independent and can separate out the unvoiced regions as well as suppress the crosstalk effect which both are the drawbacks of source-driven or equivalently computational auditory scene analysis (CASA) models. We compare our fused model with both model- and source-driven techniques by conducting subjective and objective experiments. The results show that although for the speaker-dependent case, model-based separation delivers the best quality, for a speaker independent scenario the integrated model outperforms the individual approaches. This result supports the idea that the human auditory system takes on both grouping cues (e.g., pitch tracking) and a priori knowledge (e.g., trained quantized envelopes) to segregate speech signals. (C) 2007 Elsevier B.V. All rights reserved.", "paper_title": "Monaural speech segregation based on fusion of source-driven with model-driven techniques", "paper_id": "WOS:000248322400003"}