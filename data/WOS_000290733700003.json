{"auto_keywords": [{"score": 0.04678178460605593, "phrase": "mobile_users"}, {"score": 0.014109111870241295, "phrase": "semantic_importance"}, {"score": 0.013976999452563307, "phrase": "user_preferences"}, {"score": 0.012021855198848304, "phrase": "mobile_user_preferences"}, {"score": 0.01157687557080965, "phrase": "mobile_devices"}, {"score": 0.00481495049065317, "phrase": "user-centric_mobile_display_devices"}, {"score": 0.0046797114400479135, "phrase": "semantic_image_adaptation_scheme"}, {"score": 0.004635477242873163, "phrase": "heterogeneous_mobile_display_devices"}, {"score": 0.0042354707960660706, "phrase": "limited_mobile_display_constraints"}, {"score": 0.004175533280178052, "phrase": "main_contributions"}, {"score": 0.004116440453049531, "phrase": "proposed_scheme"}, {"score": 0.003962899903266524, "phrase": "query_information"}, {"score": 0.003925414526537783, "phrase": "low_level_image"}, {"score": 0.0038515000499988673, "phrase": "semantically_important_image_contents"}, {"score": 0.0037432199549135826, "phrase": "user_feedback"}, {"score": 0.003419943984098415, "phrase": "semantic_gap"}, {"score": 0.003307988966112615, "phrase": "bayesian_fusion_approach"}, {"score": 0.0032456625605347417, "phrase": "low_level_features"}, {"score": 0.0032149396890830575, "phrase": "high_level_semantics"}, {"score": 0.0029935730127943496, "phrase": "adaptation_process"}, {"score": 0.0027741761880779535, "phrase": "perceptually_optimized_adaptation"}, {"score": 0.002696101728474703, "phrase": "best_image_content"}, {"score": 0.002632716358833918, "phrase": "mobile_display_capacities"}, {"score": 0.0026077803487003, "phrase": "extensive_experiments"}, {"score": 0.0024513404432665153, "phrase": "kodak's_consumer_image_database"}, {"score": 0.0023485549678832628, "phrase": "proposed_semantic_adaptation_scheme"}, {"score": 0.0022500695985527668, "phrase": "perceptually_relevant_adaptation"}, {"score": 0.0021352776064254195, "phrase": "user_intentions"}, {"score": 0.0021049977753042253, "phrase": "mobile_environment_constraints"}], "paper_keywords": ["Image adaptation", " mobile display devices", " perceptual experience", " semantics", " user preference"], "paper_abstract": "This paper proposes a semantic image adaptation scheme for heterogeneous mobile display devices. This scheme aims to provide mobile users with the most desired image content by integrating the content semantic importance with user preferences under limited mobile display constraints. The main contributions of the proposed scheme are: 1) seamless integration of mobile user supplied query information with low level image features to identify semantically important image contents; 2) integration of semantic importance and user feedback to dynamically update mobile user preferences; and 3) perceptually optimized adaptation for image display on mobile devices. In order to bridge the semantic gap for adaptation, we design a Bayesian fusion approach to properly integrate low level features with high level semantics. To accommodate the variation of user preferences, the system involves mobile users in the adaptation process with only a few simple feedbacks so as to present to the users most interesting content on mobile devices. Eventually, perceptually optimized adaptation is performed to present the best image content for mobile users according to mobile display capacities. Extensive experiments have been carried out based on several common events [ 1] defined in Kodak's consumer image database. These experiments show that by utilizing the proposed semantic adaptation scheme with integration of the semantics and mobile user preferences, perceptually relevant adaptation can be effectively carried out to tailor the image towards user intentions under the mobile environment constraints.", "paper_title": "Event-Based Semantic Image Adaptation for User-Centric Mobile Display Devices", "paper_id": "WOS:000290733700003"}