{"auto_keywords": [{"score": 0.028379663660535164, "phrase": "openmp"}, {"score": 0.00481495049065317, "phrase": "efficient_parallel_cfd-dem_simulations"}, {"score": 0.0047741248360741485, "phrase": "openmp."}, {"score": 0.004693476992528682, "phrase": "parallelization_strategies"}, {"score": 0.004516962628112147, "phrase": "dense_particulate_systems"}, {"score": 0.004459605573798577, "phrase": "computational_fluid_dynamics"}, {"score": 0.004328576620172575, "phrase": "field_equations"}, {"score": 0.004291886450102411, "phrase": "cfd"}, {"score": 0.004201381218977942, "phrase": "spatial_domain_decomposition_techniques"}, {"score": 0.004148014765950199, "phrase": "n-body_particulate_phase"}, {"score": 0.003776750773624222, "phrase": "efficient_parallelization"}, {"score": 0.0036191240628152205, "phrase": "openmp_thread_based_parallelization"}, {"score": 0.003557924423642353, "phrase": "mpi_processes"}, {"score": 0.0034532961266947734, "phrase": "dense_fluid-particulate_systems"}, {"score": 0.0031981188104809994, "phrase": "fluidized_bed_calculations"}, {"score": 0.003117307618682836, "phrase": "uniform_particle_loading"}, {"score": 0.003064673404472922, "phrase": "mpi"}, {"score": 0.002924074850256874, "phrase": "processing_cores"}, {"score": 0.0028139168128067343, "phrase": "communication_overhead"}, {"score": 0.0027781249331615813, "phrase": "ghost_particle_lists"}, {"score": 0.002754516260621121, "phrase": "processor_boundaries"}, {"score": 0.0025291726171859756, "phrase": "mpi."}, {"score": 0.0025076619172878945, "phrase": "rotary_kiln_heat_transfer_calculations"}, {"score": 0.002444254652283002, "phrase": "spatially_non-uniform_particle_distributions"}, {"score": 0.002362192695471035, "phrase": "parallelization_mode"}, {"score": 0.0023123057299992587, "phrase": "load_imbalances"}, {"score": 0.0022731540640627307, "phrase": "increased_overheads"}, {"score": 0.0022442249871042026, "phrase": "non-local_data"}], "paper_keywords": ["Computational fluid dynamics (CFD)", " Discrete element method (DEM)", " OpenMP", " MPI", " Euler-Lagrange", " Multiphase flows"], "paper_abstract": "The paper describes parallelization strategies for the Discrete Element Method (DEM) used for simulating dense particulate systems coupled to Computational Fluid Dynamics (CFD). While the field equations of CFD are best parallelized by spatial domain decomposition techniques, the N-body particulate phase is best parallelized over the number of particles. When the two are coupled together, both modes are needed for efficient parallelization. It is shown that under these requirements, OpenMP thread based parallelization has advantages over MPI processes. Two representative examples, fairly typical of dense fluid-particulate systems are investigated, including the validation of the DEM-CFD and thermal-DEM implementation with experiments. Fluidized bed calculations are performed on beds with uniform particle loading, parallelized with MPI and OpenMP. It is shown that as the number of processing cores and the number of particles increase, the communication overhead of building ghost particle lists at processor boundaries dominates time to solution, and OpenMP which does not require this step is about twice as fast as MPI. In rotary kiln heat transfer calculations, which are characterized by spatially non-uniform particle distributions, the low overhead of switching the parallelization mode in OpenMP eliminates the load imbalances, but introduces increased overheads in fetching non-local data. In spite of this, it is shown that OpenMP is between 50-90% faster than MPI. (C) 2013 Elsevier Inc. All rights reserved.", "paper_title": "Efficient parallel CFD-DEM simulations using OpenMP", "paper_id": "WOS:000326596600028"}