{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "semantic_space_models"}, {"score": 0.004556901872357751, "phrase": "vector-based_semantic_space_models"}, {"score": 0.00443308765153716, "phrase": "word_co-occurrence_counts"}, {"score": 0.004081385289516914, "phrase": "lexical_meaning"}, {"score": 0.0036553070575015344, "phrase": "novel_flamework"}, {"score": 0.0035072146967082083, "phrase": "semantic_spaces"}, {"score": 0.0033651019362620866, "phrase": "syntactic_relations"}, {"score": 0.0026986685585382347, "phrase": "linguistic_knowledge"}, {"score": 0.0025537472239953807, "phrase": "construction_process"}, {"score": 0.002163930783583212, "phrase": "cognitive_science"}, {"score": 0.0021049977753042253, "phrase": "natural_language_processing"}], "paper_keywords": [""], "paper_abstract": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel flamework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art.", "paper_title": "Dependency-based construction of semantic space models", "paper_id": "WOS:000247471600002"}