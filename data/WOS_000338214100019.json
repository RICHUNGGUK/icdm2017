{"auto_keywords": [{"score": 0.03972797957320609, "phrase": "local_information"}, {"score": 0.004814956211743368, "phrase": "k-nearest"}, {"score": 0.004786002454145624, "phrase": "neighbor-based_weighted_twin_support_vector_regression"}, {"score": 0.00472877316607668, "phrase": "twin"}, {"score": 0.004588158931776543, "phrase": "tsvr"}, {"score": 0.004451846834142146, "phrase": "down-bound_functions"}, {"score": 0.004293584349522444, "phrase": "smaller-sized_quadratic_programming_problems"}, {"score": 0.004116012194714212, "phrase": "single_large_one"}, {"score": 0.004017846065981189, "phrase": "classical_svr"}, {"score": 0.003626000977962191, "phrase": "tsvr."}, {"score": 0.0035609154384221567, "phrase": "full_use"}, {"score": 0.0033725614574134396, "phrase": "prediction_accuracy"}, {"score": 0.003312009953245191, "phrase": "k-nearest_neighbor-based_weighted_tsvr"}, {"score": 0.00291743479583421, "phrase": "major_weight"}, {"score": 0.002830617996609902, "phrase": "training_sample"}, {"score": 0.0026970382454914437, "phrase": "minor_weight"}, {"score": 0.0025083651612064144, "phrase": "computational_speed"}, {"score": 0.002478225374613311, "phrase": "successive_overrelaxation_approach"}, {"score": 0.002361236012575149, "phrase": "experimental_results"}, {"score": 0.0023328599525181707, "phrase": "eight_benchmark_datasets"}, {"score": 0.0022909325048608054, "phrase": "real_dataset"}, {"score": 0.002209319719984651, "phrase": "lower_prediction_error"}, {"score": 0.002156529381840657, "phrase": "lower_running_time"}], "paper_keywords": ["TSVR", " K-nearest neighbor", " Weights", " Successive overrelaxation"], "paper_abstract": "Twin support vector regression (TSVR) finds I mu-insensitive up- and down-bound functions by resolving a pair of smaller-sized quadratic programming problems (QPPs) rather than a single large one as in a classical SVR, which makes its computational speed greatly improved. However the local information among samples are not exploited in TSVR. To make full use of the knowledge of samples and improve the prediction accuracy, a K-nearest neighbor-based weighted TSVR (KNNWTSVR) is proposed in this paper, where the local information among samples are utilized. Specifically, a major weight is given to the training sample if it has more K-nearest neighbors. Otherwise a minor weight is given to it. Moreover, to further enhance the computational speed, successive overrelaxation approach is employed to resolve the QPPs. Experimental results on eight benchmark datasets and a real dataset demonstrate our weighted TSVR not only yields lower prediction error but also costs lower running time in comparison with other algorithms.", "paper_title": "K-nearest neighbor-based weighted twin support vector regression", "paper_id": "WOS:000338214100019"}