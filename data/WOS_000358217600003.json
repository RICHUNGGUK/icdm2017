{"auto_keywords": [{"score": 0.04218262960910763, "phrase": "integrated_architecture"}, {"score": 0.03709788182167048, "phrase": "decoupled_architecture"}, {"score": 0.00481495049065317, "phrase": "complex_language_models"}, {"score": 0.0047709540557306284, "phrase": "asr_and_lu_systems"}, {"score": 0.004598933498016774, "phrase": "different_methods"}, {"score": 0.004515252649226697, "phrase": "complex_language_model"}, {"score": 0.004234129588974058, "phrase": "automatic_speech_recognition"}, {"score": 0.003757480190200645, "phrase": "different_stochastic_finite-state_automata"}, {"score": 0.003689053436223376, "phrase": "specific_language_model"}, {"score": 0.0034751259234824913, "phrase": "two-pass_decoder"}, {"score": 0.00338060449446278, "phrase": "complex_lm"}, {"score": 0.0032886455096627324, "phrase": "n-best_list"}, {"score": 0.0032436052954996097, "phrase": "formal_definition"}, {"score": 0.0030554300179552415, "phrase": "theoretical_comparison"}, {"score": 0.0029722900280316216, "phrase": "different_experiments"}, {"score": 0.00286493504971388, "phrase": "proposed_approaches"}, {"score": 0.0027614468443073028, "phrase": "hierarchical_lms"}, {"score": 0.002723607240517261, "phrase": "baseline_word-based_lm"}, {"score": 0.002613163083407071, "phrase": "better_asr_system_performance"}, {"score": 0.002461472107682778, "phrase": "two-pass_strategy"}, {"score": 0.0023944552329463035, "phrase": "different_models"}, {"score": 0.002361632539339945, "phrase": "standard_decoder"}, {"score": 0.002255431275670053, "phrase": "complex_lms"}, {"score": 0.0021539955106204337, "phrase": "language_understanding"}, {"score": 0.0021049977753042253, "phrase": "proposed_architectures"}], "paper_keywords": ["Finite-state models", " Integration architectures", " Hierarchical models", " word graph"], "paper_abstract": "Throughout this work, we explore different methods to integrate a complex Language Model (a hierarchical Language Model based on classes of phrases) into an automatic speech recognition (ASR) system. First of all, an integrated architecture is considered, where the integration is carried out via the composition of the different Stochastic Finite-State Automata associated with the specific Language Model (LM). On the other hand, a decoupled architecture with a two-pass decoder is employed, where the complex LM is used to reorder the N-best list. The formal definition of both methods is provided in this work, thus enabling the theoretical comparison between them. Additionally, different experiments were carried out to compare empirically the proposed approaches. The results show that although the hierarchical LMs outperform a baseline word-based LM in both cases, the integrated architecture can provide better ASR system performance. However, the decoupled architecture could be more versatile due to the two-pass strategy, allowing the integration of different models using a standard decoder. Additionally, the use of this kind of complex LMs can also be extended to other NLP applications, such as language understanding, by employing the proposed architectures.", "paper_title": "Integration of complex language models in ASR and LU systems", "paper_id": "WOS:000358217600003"}