{"auto_keywords": [{"score": 0.03562071693205257, "phrase": "k_nearest_neighbors"}, {"score": 0.004539005582217309, "phrase": "experimental_results"}, {"score": 0.0043150485532080065, "phrase": "theoretical_properties"}, {"score": 0.004207230913063179, "phrase": "recently_proposed_nonparametric_estimator"}, {"score": 0.0038996120252338556, "phrase": "regression_function"}, {"score": 0.0032387402145603412, "phrase": "sample_d-n"}, {"score": 0.00266693888510356, "phrase": "corresponding_response_variables"}, {"score": 0.0022145052062614514, "phrase": "optimal_rate"}, {"score": 0.0021049977753042253, "phrase": "unknown_distribution"}], "paper_keywords": ["nonparametric estimation", " nearest neighbor methods", " mathematical statistics"], "paper_abstract": "Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function m(x) = E[Y vertical bar X = x] as follows: first identify the k nearest neighbors of x in the sample D-n, then keep only those for which x is itself one of the k nearest neighbors, and finally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data-splitting.", "paper_title": "On the Mutual Nearest Neighbors Estimate in Regression", "paper_id": "WOS:000324799600007"}