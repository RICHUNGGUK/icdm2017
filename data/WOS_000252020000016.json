{"auto_keywords": [{"score": 0.045693275459193064, "phrase": "conditional_probabilities"}, {"score": 0.013315579994925282, "phrase": "adaboost"}, {"score": 0.00481495049065317, "phrase": "shrinkage_estimators"}, {"score": 0.004701015188740582, "phrase": "class_probability"}, {"score": 0.004271490815763164, "phrase": "output_labels"}, {"score": 0.0041703614123283165, "phrase": "multiclass_classification_problems"}, {"score": 0.003975200317233532, "phrase": "highly_accurate_classifiers"}, {"score": 0.0034841647157189985, "phrase": "conditional_probability"}, {"score": 0.0031654684543975077, "phrase": "training_samples"}, {"score": 0.0030171910207671205, "phrase": "loss_functions"}, {"score": 0.002841545331522694, "phrase": "shrinkage_estimator"}, {"score": 0.00240213106042981, "phrase": "uniform_distribution"}, {"score": 0.0023451553255837317, "phrase": "numerical_experiments"}, {"score": 0.0021561514733516676, "phrase": "proposed_loss_functions"}, {"score": 0.0021049977753042253, "phrase": "significantly_better_results"}], "paper_keywords": ["multiclass classification", " loss functions", " conditional probability", " shrinkage estimator", " learning process"], "paper_abstract": "Our purpose is to estimate conditional probabilities of output labels in multiclass classification problems. Adaboost provides highly accurate classifiers and has potential to estimate conditional probabilities. However, the conditional probability estimated by Adaboost tends to overfit to training samples. We propose loss functions for boosting that provide shrinkage estimator. The effect of regularization is realized by shrinkage of probabilities toward the uniform distribution. Numerical experiments indicate that boosting algorithms based on proposed loss functions show significantly better results than existing boosting algorithms for estimation of conditional probabilities.", "paper_title": "Multiclass boosting algorithms for shrinkage estimators of class probability", "paper_id": "WOS:000252020000016"}