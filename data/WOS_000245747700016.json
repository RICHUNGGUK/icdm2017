{"auto_keywords": [{"score": 0.04900761970662832, "phrase": "nonlinear_systems"}, {"score": 0.004816293322450462, "phrase": "markov"}, {"score": 0.004550029253278477, "phrase": "reinforcement_learning"}, {"score": 0.004324031677173549, "phrase": "robust_controllers"}, {"score": 0.004203343568960231, "phrase": "bounded_external_disturbances"}, {"score": 0.004156012879026324, "phrase": "parametric_uncertainties"}, {"score": 0.00399449659126893, "phrase": "markov_games"}, {"score": 0.003882970272082245, "phrase": "differential_game"}, {"score": 0.0037959868107895053, "phrase": "'disturbing'_agent"}, {"score": 0.0037109446302945903, "phrase": "worst_possible_disturbance"}, {"score": 0.003648411055689428, "phrase": "'control'_agent"}, {"score": 0.0035666632046674153, "phrase": "best_control_input"}, {"score": 0.0033893426032054366, "phrase": "min-max_solution"}, {"score": 0.003332210257373994, "phrase": "value_function"}, {"score": 0.003239114833599948, "phrase": "online_procedure"}, {"score": 0.0031845067058084613, "phrase": "optimal_value_function"}, {"score": 0.0030955250132409964, "phrase": "robust_control_policy"}, {"score": 0.0030606302942851027, "phrase": "proposed_game-theoretic_paradigm"}, {"score": 0.002958281631770322, "phrase": "control_task"}, {"score": 0.0029083942467546305, "phrase": "highly_nonlinear_two-link_robot_system"}, {"score": 0.002795228413298423, "phrase": "proposed_markov_game_controller"}, {"score": 0.0027480831196871093, "phrase": "standard_rl-based_robust_controller"}, {"score": 0.0025097154697345096, "phrase": "robot_control_task"}, {"score": 0.0024673738215365104, "phrase": "proposed_controller"}, {"score": 0.0024395426929046415, "phrase": "superior_robustness"}, {"score": 0.0023848164282879885, "phrase": "payload_mass_and_external_disturbances"}, {"score": 0.0022405528759521856, "phrase": "neural_networks"}, {"score": 0.002190280748985751, "phrase": "markov_game_framework"}, {"score": 0.002141134176057931, "phrase": "continuous_state-action_spaces"}], "paper_keywords": ["reinforcement learning", " Markov decision process", " matrix games", " Markov games", " Markov game controller", " neural networks", " inverted pendulum", " two link robot"], "paper_abstract": "This paper proposes a reinforcement learning ( RL)-based game-theoretic formulation for designing robust controllers for nonlinear systems affected by bounded external disturbances and parametric uncertainties. Based on the theory of Markov games, we consider a differential game in which a 'disturbing' agent tries to make worst possible disturbance while a 'control' agent tries to make best control input. The problem is formulated as finding a min-max solution of a value function. We propose an online procedure for learning optimal value function and for calculating a robust control policy. Proposed game-theoretic paradigm has been tested on the control task of a highly nonlinear two-link robot system. We compare the performance of proposed Markov game controller with a standard RL-based robust controller, and an H-infinity theory-based robust game controller. For the robot control task, the proposed controller achieved superior robustness to changes in payload mass and external disturbances, over other control schemes. Results also validate the effectiveness of neural networks in extending the Markov game framework to problems with continuous state-action spaces. (c) 2006 Elsevier B. V. All rights reserved.", "paper_title": "A robust Markov game controller for nonlinear systems", "paper_id": "WOS:000245747700016"}