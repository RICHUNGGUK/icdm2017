{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "unbiased_criterion"}, {"score": 0.004610058444728656, "phrase": "local_correspondences"}, {"score": 0.00456571874088313, "phrase": "different_images"}, {"score": 0.004413846562278824, "phrase": "stable_local_invariant_descriptors"}, {"score": 0.004350308086118336, "phrase": "scale-invariant_keypoints"}, {"score": 0.0041450181887917135, "phrase": "new_keypoint_detectors"}, {"score": 0.0038364365168131586, "phrase": "preceding_ones"}, {"score": 0.0036908381347418805, "phrase": "fair_comparison"}, {"score": 0.0034826993555190765, "phrase": "repeatability_criterion"}, {"score": 0.003350480663660326, "phrase": "image_perturbations"}, {"score": 0.002968695624366393, "phrase": "classic_repeatability_criterion"}, {"score": 0.002925899335792914, "phrase": "favoring_algorithms"}, {"score": 0.002897710777957041, "phrase": "redundant_overlapped_detections"}, {"score": 0.0027209713296448296, "phrase": "repeatability_rate"}, {"score": 0.002530359917257626, "phrase": "popular_benchmark"}, {"score": 0.0024578987081792405, "phrase": "j._comput"}, {"score": 0.0021670777282518424, "phrase": "feature_detectors"}, {"score": 0.0021049977753042253, "phrase": "amended_comparator"}], "paper_keywords": ["feature detectors", " performance evaluation", " repeatability criteria", " image matching", " descriptors", " scale invariant feature transform (SIFT)"], "paper_abstract": "Most computer vision applications rely on algorithms finding local correspondences between different images. These algorithms detect and compare stable local invariant descriptors centered at scale-invariant keypoints. Because of the importance of the problem, new keypoint detectors and descriptors are constantly being proposed, each one claiming to perform better than the preceding ones. This raises the question of a fair comparison between very diverse methods. This evaluation has been based mainly on a repeatability criterion of the keypoints under a series of image perturbations (blur, illumination, noise, rotations, homotheties, homographies, etc). In this paper, we argue that the classic repeatability criterion is biased favoring algorithms producing redundant overlapped detections. To overcome this bias, we propose a variant of the repeatability rate taking into account the descriptors overlap. We apply this variant to revisit the popular benchmark by [Mikolajczyk et al. Int. J. Comput. Vis., 65 (2005), pp. 43-72], comparing several classic and recently introduced feature detectors. Experimental evidence shows that the hierarchy of these feature detectors is severely disrupted by the amended comparator.", "paper_title": "Is Repeatability an Unbiased Criterion for Ranking Feature Detectors?", "paper_id": "WOS:000367019300013"}