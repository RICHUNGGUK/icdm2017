{"auto_keywords": [{"score": 0.025617816326778516, "phrase": "consensus_quality"}, {"score": 0.00481495049065317, "phrase": "crowd_labeling"}, {"score": 0.004777214679969785, "phrase": "machine_learning_applications"}, {"score": 0.004684157900507474, "phrase": "vast_amounts"}, {"score": 0.0045748683993128425, "phrase": "reliable_labels"}, {"score": 0.004415675459182871, "phrase": "unlabeled_data"}, {"score": 0.004363846362238565, "phrase": "common_solution"}, {"score": 0.0036553070575015344, "phrase": "consensus_label"}, {"score": 0.0035699363258657212, "phrase": "consensus_extraction"}, {"score": 0.003541923233726252, "phrase": "noisy_labels"}, {"score": 0.003459191419478298, "phrase": "main_focus"}, {"score": 0.003432044343307207, "phrase": "binary_label_data"}, {"score": 0.003312486229143855, "phrase": "crowd_consensus_estimation"}, {"score": 0.003286486670483032, "phrase": "continuous_labels"}, {"score": 0.0031970796597571367, "phrase": "ordinal_or_binary_labels"}, {"score": 0.0030254743224136273, "phrase": "gold_standard"}, {"score": 0.0028743610212291727, "phrase": "feature_vectors"}, {"score": 0.002763261749602258, "phrase": "training_phase"}, {"score": 0.0027093285574752457, "phrase": "better_consensus"}, {"score": 0.00266693888510356, "phrase": "different_annotator_behaviors"}, {"score": 0.0025436978068459565, "phrase": "new_metric"}, {"score": 0.0025137856138000014, "phrase": "annotator_quality"}, {"score": 0.0024357240356443653, "phrase": "good_annotators"}, {"score": 0.0023787690061742566, "phrase": "crowd_labeling_costs"}, {"score": 0.0023139986108451946, "phrase": "proposed_models"}, {"score": 0.002286781334850213, "phrase": "commonly_used_methods"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Label noise", " Crowdsourcing", " Crowd labeling", " Annotator behavior", " Annotator quality", " Consensus"], "paper_abstract": "Machine learning applications can benefit greatly from vast amounts of data, provided that reliable labels are available. Mobilizing crowds to annotate the unlabeled data is a common solution. Although the labels provided by the crowd are subjective and noisy, the wisdom of crowds can be captured by a variety of techniques. Finding the mean or finding the median of a samples annotations are widely used approaches for finding the consensus label of that sample. Improving consensus extraction from noisy labels is a very popular topic, the main focus being binary label data. In this paper, we focus on crowd consensus estimation of continuous labels, which is also adaptable to ordinal or binary labels. Our approach is designed to work on situations where there is no gold standard; it is only dependent on the annotations and not on the feature vectors of the instances, and does not require a training phase. For achieving a better consensus, we investigate different annotator behaviors and incorporate them into four novel Bayesian models. Moreover, we introduce a new metric to examine annotator quality, which can be used for finding good annotators to enhance consensus quality and reduce crowd labeling costs. The results show that the proposed models outperform the commonly used methods. With the use of our annotator scoring mechanism, we are able to sustain consensus quality with much fewer annotations. (C) 2015 Elsevier B.V. All rights reserved.", "paper_title": "Modeling annotator behaviors for crowd labeling", "paper_id": "WOS:000354139100013"}