{"auto_keywords": [{"score": 0.05007852962010534, "phrase": "blas"}, {"score": 0.006659518184838901, "phrase": "solid-state_drives"}, {"score": 0.004811995097290128, "phrase": "gc_overhead"}, {"score": 0.004779085953614184, "phrase": "block-level_adaptive_striping"}, {"score": 0.004722886112237108, "phrase": "existing_device-level_schemes"}, {"score": 0.00453537012358554, "phrase": "garbage_collection"}, {"score": 0.004146015717402716, "phrase": "data_placement_scheme"}, {"score": 0.0040997541467722625, "phrase": "ssd"}, {"score": 0.003890540531028228, "phrase": "data_access_pattern"}, {"score": 0.003861534816807933, "phrase": "existing_fixed_and_device-level_data_placement_schemes"}, {"score": 0.003691969158027291, "phrase": "degraded_performance"}, {"score": 0.0035965032173588753, "phrase": "adaptive_block-level_data_placement_scheme"}, {"score": 0.0033121909731471787, "phrase": "different_data_placement_policies"}, {"score": 0.0032629585038738856, "phrase": "different_access_patterns"}, {"score": 0.003214455466823939, "phrase": "read-intensive_blocks"}, {"score": 0.0030963190575539574, "phrase": "read_parallelism"}, {"score": 0.0030049342542235466, "phrase": "remaining_blocks"}, {"score": 0.0027985291920293127, "phrase": "placement_policy"}, {"score": 0.00276725708403607, "phrase": "logical_block"}, {"score": 0.002685558057339722, "phrase": "access_pattern_changes"}, {"score": 0.002606264765409507, "phrase": "parallelism-aware_write_buffer_management_approach"}, {"score": 0.002510424048850241, "phrase": "write_parallelism"}, {"score": 0.0024916819803100635, "phrase": "performance_results"}, {"score": 0.002436289099444074, "phrase": "significant_improvement"}, {"score": 0.0024090550572099688, "phrase": "ssd_response_time"}, {"score": 0.002311769781106355, "phrase": "device-level_page_striping"}, {"score": 0.002294507393862231, "phrase": "device-level_block_striping"}, {"score": 0.0021609589091422608, "phrase": "low_gc_overhead"}, {"score": 0.0021049977753042253, "phrase": "workload_changes"}], "paper_keywords": ["Design", " Algorithms", " Performance", " SSD", " NAND flash memory", " data placement", " write buffer"], "paper_abstract": "Increasing the degree of parallelism and reducing the overhead of garbage collection (GC overhead) are the two keys to enhancing the performance of solid-state drives (SSDs). SSDs employmultichannel architectures, and a data placement scheme in an SSD determines how the data are striped to the channels. Without considering the data access pattern, existing fixed and device-level data placement schemes may have either high GC overhead or poor I/ O parallelism, resulting in degraded performance. In this article, an adaptive block-level data placement scheme called BLAS is proposed to maximize the I/ O parallelism while simultaneously minimizing the GC overhead. In contrast to existing device-level schemes, BLAS allows different data placement policies for blocks with different access patterns. Pages in read-intensive blocks are scattered over various channels to maximize the degree of read parallelism, while pages in each of the remaining blocks are attempted to be gathered in the same physical block to minimize the GC overhead. Moreover, BLAS allows the placement policy for a logical block to be changed dynamically according to the access pattern changes of that block. Finally, a parallelism-aware write buffer management approach is adopted in BLAS to maximize the degree of write parallelism. Performance results show that BLAS yields a significant improvement in the SSD response time when compared to existing device-level schemes. In particular, BLAS outperforms device-level page striping and device-level block striping by factors of up to 8.75 and 7.41, respectively. Moreover, BLAS achieves low GC overhead and is effective in adapting to workload changes.", "paper_title": "BLAS: Block-Level Adaptive Striping for Solid-State Drives", "paper_id": "WOS:000333554400012"}