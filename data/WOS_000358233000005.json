{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "super_fast_event_recognition"}, {"score": 0.04969652007027205, "phrase": "internet_videos"}, {"score": 0.0269212605519727, "phrase": "visual_frames"}, {"score": 0.004681653551412616, "phrase": "high-level_events"}, {"score": 0.00444377150105556, "phrase": "state-of-the-art_recognition_performance"}, {"score": 0.004201033616196156, "phrase": "temporal_motion_trajectories"}, {"score": 0.004068302313449334, "phrase": "large-scale_datasets"}, {"score": 0.003923965929636472, "phrase": "comprehensive_study"}, {"score": 0.0038925903520043623, "phrase": "efficient_methods"}, {"score": 0.0037999551521987426, "phrase": "technical_options"}, {"score": 0.0036212219870826725, "phrase": "multimodal_baseline"}, {"score": 0.0035635256137029592, "phrase": "good_performance"}, {"score": 0.0035350218469335573, "phrase": "popular_benchmarks"}, {"score": 0.0033822643082593285, "phrase": "computational_cost"}, {"score": 0.0033283624928018177, "phrase": "recognition_accuracy"}, {"score": 0.003236086384959101, "phrase": "alternative_features"}, {"score": 0.003171740347925348, "phrase": "fusion_strategies"}, {"score": 0.002915125480162171, "phrase": "event_recognition"}, {"score": 0.0028342734482926677, "phrase": "minimum_number"}, {"score": 0.0028115863316685937, "phrase": "visual_and_audio_frames"}, {"score": 0.0027556576784848207, "phrase": "comparable_accuracy"}, {"score": 0.002625914195439153, "phrase": "similar_results"}, {"score": 0.0022358683283304533, "phrase": "baseline_approach"}, {"score": 0.0022179605282337395, "phrase": "even_higher_recognition_accuracies"}, {"score": 0.0021049977753042253, "phrase": "regular_desktop_computer"}], "paper_keywords": ["Consumer videos", " efficiency", " event recognition", " Internet videos", " real time"], "paper_abstract": "Techniques for recognizing high-level events in consumer videos on the Internet have many applications. Systems that produced state-of-the-art recognition performance usually contain modules requiring extensive computation, such as the extraction of the temporal motion trajectories, which cannot be deployed on large-scale datasets. In this paper, we provide a comprehensive study on efficient methods in this area and identify technical options for super fast event recognition in Internet videos. We start from analyzing a multimodal baseline that has produced good performance on popular benchmarks, by systematically evaluating each component in terms of both computational cost and contribution to recognition accuracy. After that, we identify alternative features, classifiers, and fusion strategies that can all be efficiently computed. In addition, we also provide a study on the following interesting question: for event recognition in Internet videos, what is the minimum number of visual and audio frames needed to obtain a comparable accuracy to that of using all the frames? Results on two rigorously designed datasets indicate that similar results can be maintained by using only a small portion of the visual frames. We also find that, different from the visual frames, the soundtracks contain little redundant information and thus sampling is always harmful. Integrating all the findings, our suggested recognition system is 2,350-fold faster than a baseline approach with even higher recognition accuracies. It recognizes 20 classes on a 120-second video sequence in just 1.78 seconds, using a regular desktop computer.", "paper_title": "Super Fast Event Recognition in Internet Videos", "paper_id": "WOS:000358233000005"}