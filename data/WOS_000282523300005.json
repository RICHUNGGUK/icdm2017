{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "dimension_reduction"}, {"score": 0.004707188147442449, "phrase": "statistical_dependence"}, {"score": 0.004575854901681043, "phrase": "modeling_framework"}, {"score": 0.004299621153829837, "phrase": "euclidean_spaces"}, {"score": 0.00417961150804265, "phrase": "manifold_setting"}, {"score": 0.004109212945115815, "phrase": "central_quantity"}, {"score": 0.0037959868107895053, "phrase": "classification_function"}, {"score": 0.0036691380448651443, "phrase": "gradient_estimates"}, {"score": 0.0034086028336466688, "phrase": "first_quantity"}, {"score": 0.003313380246267812, "phrase": "supervised_dimension_reduction"}, {"score": 0.003148612096891635, "phrase": "graphical_model_encoding_dependencies"}, {"score": 0.0030433302702291116, "phrase": "response_variable"}, {"score": 0.0029920129598858545, "phrase": "second_quantity"}, {"score": 0.0029083942467546305, "phrase": "nonlinear_projections"}, {"score": 0.002552781867125423, "phrase": "gradient_outer_product"}, {"score": 0.0025239898026543964, "phrase": "standard_statistical_quantities"}, {"score": 0.0024257447861185813, "phrase": "simple_and_precise_comparison"}, {"score": 0.0023579143175099324, "phrase": "supervised_dimensionality_reduction_methods"}, {"score": 0.002215274527909304, "phrase": "informative_directions"}, {"score": 0.002129020447793464, "phrase": "graphical_model"}, {"score": 0.0021049977753042253, "phrase": "variable_dependencies"}], "paper_keywords": ["gradient estimates", " manifold learning", " graphical models", " inverse regression", " dimension reduction", " gradient diffusion maps"], "paper_abstract": "The problems of dimension reduction and inference of statistical dependence are addressed by the modeling framework of learning gradients. The models we propose hold for Euclidean spaces as well as the manifold setting. The central quantity in this approach is an estimate of the gradient of the regression or classification function. Two quadratic forms are constructed from gradient estimates: the gradient outer product and gradient based diffusion maps. The first quantity can be used for supervised dimension reduction on manifolds as well as inference of a graphical model encoding dependencies that are predictive of a response variable. The second quantity can be used for nonlinear projections that incorporate both the geometric structure of the manifold as well as variation of the response variable on the manifold. We relate the gradient outer product to standard statistical quantities such as covariances and provide a simple and precise comparison of a variety of supervised dimensionality reduction methods. We provide rates of convergence for both inference of informative directions as well as inference of a graphical model of variable dependencies.", "paper_title": "Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence", "paper_id": "WOS:000282523300005"}