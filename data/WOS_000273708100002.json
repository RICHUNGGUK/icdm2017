{"auto_keywords": [{"score": 0.046800023565389516, "phrase": "system's_long-term_performance"}, {"score": 0.00481495049065317, "phrase": "online_reinforcement_learning_for_dynamic_multimedia_systems"}, {"score": 0.0047026237903774895, "phrase": "systematic_cross-layer_framework"}, {"score": 0.004539005582217309, "phrase": "autonomous_and_foresighted_decisions"}, {"score": 0.004420022113586985, "phrase": "application's_real-time_delay_constraints"}, {"score": 0.004329630571352367, "phrase": "cross-layer_optimization_offline"}, {"score": 0.0042410797042450816, "phrase": "multimedia_system's_probabilistic_dynamics"}, {"score": 0.004093453936415085, "phrase": "layered_markov_decision_process"}, {"score": 0.0037133478610481994, "phrase": "multimedia_system_layers"}, {"score": 0.003658907979536578, "phrase": "repeated_interactions"}, {"score": 0.0033684179374769605, "phrase": "layer's_learning_performance"}, {"score": 0.003260690513595527, "phrase": "learning_processes"}, {"score": 0.0031193028729331667, "phrase": "learning_model"}, {"score": 0.0029928611380840757, "phrase": "multimedia_system's_limited_memory"}, {"score": 0.002966438503447793, "phrase": "multimedia_application's_real-time_delay_constraints"}, {"score": 0.0028546020769742177, "phrase": "different_design_constraints"}, {"score": 0.0028293966058473476, "phrase": "first_algorithm"}, {"score": 0.0027714435547214183, "phrase": "centralized_manner"}, {"score": 0.002706659627293686, "phrase": "decentralized_manner"}, {"score": 0.0025361866386341796, "phrase": "proposed_reinforcement"}, {"score": 0.0025286975935774245, "phrase": "learning_algorithms"}, {"score": 0.00246957392654378, "phrase": "complementary_accelerated_learning_algorithm"}, {"score": 0.002447759818767671, "phrase": "partial_knowledge"}, {"score": 0.002426137929553724, "phrase": "system's_dynamics"}, {"score": 0.0023764249306830196, "phrase": "system's_performance"}, {"score": 0.0023208532782596444, "phrase": "decentralized_learning"}, {"score": 0.0023003498123420237, "phrase": "equally_as_well_as_centralized_learning"}, {"score": 0.0022005118194652704, "phrase": "existing_application-independent_reinforcement_learning_algorithms"}, {"score": 0.002142696637011647, "phrase": "multimedia_systems"}], "paper_keywords": ["Cross-layer multimedia system optimization", " layered Markov decision process", " reinforcement learning", " resource-constrained multimedia system"], "paper_abstract": "In our previous work, we proposed a systematic cross-layer framework for dynamic multimedia systems, which allows each layer to make autonomous and foresighted decisions that maximize the system's long-term performance, while meeting the application's real-time delay constraints. The proposed solution solved the cross-layer optimization offline, under the assumption that the multimedia system's probabilistic dynamics were known a priori, by modeling the system as a layered Markov decision process. In practice, however, these dynamics are unknown a priori and, therefore, must be learned online. In this paper, we address this problem by allowing the multimedia system layers to learn, through repeated interactions with each other, to autonomously optimize the system's long-term performance at run-time. The two key challenges in this layered learning setting are: (i) each layer's learning performance is directly impacted by not only its own dynamics, but also by the learning processes of the other layers with which it interacts; and (ii) selecting a learning model that appropriately balances time-complexity (i. e., learning speed) with the multimedia system's limited memory and the multimedia application's real-time delay constraints. We propose two reinforcement learning algorithms for optimizing the system under different design constraints: the first algorithm solves the cross-layer optimization in a centralized manner and the second solves it in a decentralized manner. We analyze both algorithms in terms of their required computation, memory, and interlayer communication overheads. After noting that the proposed reinforcement learning algorithms learn too slowly, we introduce a complementary accelerated learning algorithm that exploits partial knowledge about the system's dynamics in order to dramatically improve the system's performance. In our experiments, we demonstrate that decentralized learning can perform equally as well as centralized learning, while enabling the layers to act autonomously. Additionally, we show that existing application-independent reinforcement learning algorithms, and existing myopic learning algorithms deployed in multimedia systems, perform significantly worse than our proposed application-aware and foresighted learning methods.", "paper_title": "Online Reinforcement Learning for Dynamic Multimedia Systems", "paper_id": "WOS:000273708100002"}