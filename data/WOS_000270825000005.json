{"auto_keywords": [{"score": 0.04431277486761943, "phrase": "varying_gaussian_kernels"}, {"score": 0.03760388876523762, "phrase": "regularization_error"}, {"score": 0.03698933881555943, "phrase": "sample_error"}, {"score": 0.0357890904355328, "phrase": "gaussian_kernels"}, {"score": 0.0048151414141376836, "phrase": "gaussians"}, {"score": 0.004733625412289433, "phrase": "convex_loss"}, {"score": 0.004536244367689415, "phrase": "binary_classification_algorithms"}, {"score": 0.004421770937919324, "phrase": "tikhonov_regularization_schemes"}, {"score": 0.004310173770196077, "phrase": "general_convex_loss_functions"}, {"score": 0.00402610372759447, "phrase": "fast_convergence_rates"}, {"score": 0.0039244530915794025, "phrase": "excess_misclassification_error"}, {"score": 0.0036657110980445416, "phrase": "learning_rates"}, {"score": 0.0034532961266947734, "phrase": "special_structures"}, {"score": 0.00317095235382607, "phrase": "nice_approximation_scheme"}, {"score": 0.0030908254851076005, "phrase": "fourier_analysis_technique"}, {"score": 0.0029871209444889716, "phrase": "regularizing_functions"}, {"score": 0.002936577024096783, "phrase": "polynomial_decays"}, {"score": 0.002790004829699627, "phrase": "sobolev_smoothness_condition"}, {"score": 0.002583713580625699, "phrase": "projection_operator"}, {"score": 0.002433843373780471, "phrase": "covering_numbers"}, {"score": 0.002372298201966367, "phrase": "kernel_hilbert_spaces"}, {"score": 0.0021968238865548812, "phrase": "general_loss_function"}], "paper_keywords": ["reproducing kernel Hilbert space", " binary classification", " general convex loss", " varying Gaussian kernels", " covering number", " approximation"], "paper_abstract": "This paper considers binary classification algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassification error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis.", "paper_title": "Classification with Gaussians and Convex Loss", "paper_id": "WOS:000270825000005"}