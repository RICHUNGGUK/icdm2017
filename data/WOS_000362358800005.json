{"auto_keywords": [{"score": 0.04856216099390579, "phrase": "perceptual_image_quality_scores"}, {"score": 0.03405024562525929, "phrase": "preference_label"}, {"score": 0.03014720888291599, "phrase": "proposed_biqa_method"}, {"score": 0.00481495049065317, "phrase": "rank_for_blind_image_quality_assessment"}, {"score": 0.004780633011300165, "phrase": "blind_image_quality_assessment"}, {"score": 0.004746560365560286, "phrase": "biqa"}, {"score": 0.004596192494271827, "phrase": "reference_images"}, {"score": 0.004563427013973105, "phrase": "state-of-the-art_biqa_methods"}, {"score": 0.004450568197899267, "phrase": "large_number"}, {"score": 0.0043560464825349275, "phrase": "robust_model"}, {"score": 0.004294145092032327, "phrase": "subjective_quality_scores"}, {"score": 0.004055176557424983, "phrase": "large-scale_database"}, {"score": 0.00398325070540449, "phrase": "existing_databases"}, {"score": 0.003775008774288817, "phrase": "subjective_experiments"}, {"score": 0.0037213333316218522, "phrase": "human_quality_evaluations"}, {"score": 0.0035648266669315943, "phrase": "preference_image_pairs"}, {"score": 0.003259522768246318, "phrase": "robust_biqa_model"}, {"score": 0.0031788080316930687, "phrase": "relative_quality"}, {"score": 0.0030124946762144717, "phrase": "image_content"}, {"score": 0.0029909851863915283, "phrase": "distortion_type"}, {"score": 0.0029590077170443666, "phrase": "subject_identity"}, {"score": 0.002482443883144136, "phrase": "multiple_kernel_learning_algorithm"}, {"score": 0.002455889980202819, "phrase": "group_lasso"}, {"score": 0.0023950275815082297, "phrase": "simple_but_effective_strategy"}, {"score": 0.0021975548867457623, "phrase": "state-of-the-art_biqa_algorithms"}, {"score": 0.0021049977753042253, "phrase": "new_distortion_categories"}], "paper_keywords": ["Image quality assessment (IQA)", " learning preferences", " learning to rank", " multiple kernel learning (MKL)", " universal blind IQA (BIQA)"], "paper_abstract": "Blind image quality assessment (BIQA) aims to predict perceptual image quality scores without access to reference images. State-of-the-art BIQA methods typically require subjects to score a large number of images to train a robust model. However, subjective quality scores are imprecise, biased, and inconsistent, and it is challenging to obtain a large-scale database, or to extend existing databases, because of the inconvenience of collecting images, training the subjects, conducting subjective experiments, and realigning human quality evaluations. To combat these limitations, this paper explores and exploits preference image pairs (PIPs) such as the quality of image I-a is better than that of image I-b for training a robust BIQA model. The preference label, representing the relative quality of two images, is generally precise and consistent, and is not sensitive to image content, distortion type, or subject identity; such PIPs can be generated at a very low cost. The proposed BIQA method is one of learning to rank. We first formulate the problem of learning the mapping from the image features to the preference label as one of classification. In particular, we investigate the utilization of a multiple kernel learning algorithm based on group lasso to provide a solution. A simple but effective strategy to estimate perceptual image quality scores is then presented. Experiments show that the proposed BIQA method is highly effective and achieves a performance comparable with that of state-of-the-art BIQA algorithms. Moreover, the proposed method can be easily extended to new distortion categories.", "paper_title": "Learning to Rank for Blind Image Quality Assessment", "paper_id": "WOS:000362358800005"}