{"auto_keywords": [{"score": 0.041552918470593295, "phrase": "pca"}, {"score": 0.022250241655736087, "phrase": "k-means"}, {"score": 0.009922757824381753, "phrase": "gmm"}, {"score": 0.007672761970968143, "phrase": "good_candidate_direction"}, {"score": 0.005859786615484019, "phrase": "var-part"}, {"score": 0.00481495049065317, "phrase": "deterministic_methods"}, {"score": 0.004701556483912018, "phrase": "gaussian_mixture_clustering"}, {"score": 0.0045737512337094745, "phrase": "gaussian"}, {"score": 0.0043945048085683005, "phrase": "initial_guess"}, {"score": 0.004290969772225093, "phrase": "clustering_algorithms"}, {"score": 0.004223299391912042, "phrase": "random_starts"}, {"score": 0.004123781267600542, "phrase": "deterministic_method"}, {"score": 0.0037634608487353183, "phrase": "sum-squared-error_criterion"}, {"score": 0.0036601552758254757, "phrase": "largest_eigenvalue"}, {"score": 0.0035455471410184404, "phrase": "largest_sum-squared-error"}, {"score": 0.003313741359811683, "phrase": "cluster's_largest_eigenvector"}, {"score": 0.0031845067058084583, "phrase": "gmm_clustering"}, {"score": 0.0030602966787192745, "phrase": "covariance_matrices"}, {"score": 0.0029292375702719468, "phrase": "largest_eigenvector"}, {"score": 0.002882980094838904, "phrase": "largest_determinant"}, {"score": 0.0024586427394152196, "phrase": "diagonal_covariance_matrix"}, {"score": 0.002391067964886602, "phrase": "similar_performance"}, {"score": 0.002372103459168231, "phrase": "pca_partitioning"}, {"score": 0.002164505365535068, "phrase": "optimum_values"}, {"score": 0.0021049977753042253, "phrase": "faster_convergence_rates"}], "paper_keywords": ["K-means", " Gaussian mixture", " initialization", " PCA", " clustering"], "paper_abstract": "The performance of K-means and Gaussian mixture model (GMM) clustering depends on the initial guess of partitions. Typically, clustering algorithms are initialized by random starts. In our search for a deterministic method, we found two promising approaches: principal component analysis (PCA) partitioning and Var-Part (Variance Partitioning). K-means clustering tries to minimize the sum-squared-error criterion. The largest eigenvector with the largest eigenvalue is the component which contributes to the largest sum-squared-error. Hence, a good candidate direction to project a cluster for splitting is the direction of the cluster's largest eigenvector, the basis for PCA partitioning. Similarly, GMM clustering maximizes the likelihood; minimizing the determinant of the covariance matrices of each cluster helps to increase the likelihood. The largest eigenvector contributes to the largest determinant and is thus a good candidate direction for splitting. However, PCA is computationally expensive. We, thus, introduce Var-Part, which is computationally less complex (with complexity equal to one K-means iteration) and approximates PCA partitioning assuming diagonal covariance matrix. Experiments reveal that Var-Part has similar performance with PCA partitioning, sometimes better, and leads K-means (and GMM) to yield sum-squared-error (and maximum-likelihood) values close to the optimum values obtained by several random-start runs and often at faster convergence rates.", "paper_title": "In search of deterministic methods for initializing K-means and Gaussian mixture clustering", "paper_id": "WOS:000249280000002"}