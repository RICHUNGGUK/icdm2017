{"auto_keywords": [{"score": 0.02864039936811681, "phrase": "high_probability"}, {"score": 0.00481495049065317, "phrase": "multi-armed_bandit"}, {"score": 0.004758457292840376, "phrase": "reinforcement_learning_problems"}, {"score": 0.004647442352794694, "phrase": "statistical_confidence_intervals"}, {"score": 0.004355267258397068, "phrase": "bandit_problem"}, {"score": 0.004228577663320847, "phrase": "n_arms"}, {"score": 0.003735349093835257, "phrase": "epsilon-optimal_arm"}, {"score": 0.0035210572541244664, "phrase": "lower_bound"}, {"score": 0.003479693611501194, "phrase": "mannor"}, {"score": 0.0034388121658278465, "phrase": "tsitsiklis"}, {"score": 0.0032223742018434856, "phrase": "action_elimination_procedures"}, {"score": 0.0031845067058084613, "phrase": "reinforcement_learning_algorithms"}, {"score": 0.002966438503447793, "phrase": "confidence_interval"}, {"score": 0.002914288956035066, "phrase": "value_function"}, {"score": 0.0024405312482831646, "phrase": "elimination_method"}, {"score": 0.0023003498123420237, "phrase": "learned_policy"}, {"score": 0.0021682027434117095, "phrase": "considerable_speedup"}, {"score": 0.0021049977753042253, "phrase": "epsilon-greedy_q-learning"}], "paper_keywords": [""], "paper_abstract": "We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/epsilon(2)) log(1/delta)) times to find an epsilon-optimal arm with probability of at least 1-delta. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over epsilon-greedy Q-learning.", "paper_title": "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems", "paper_id": "WOS:000245388400008"}