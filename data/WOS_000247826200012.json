{"auto_keywords": [{"score": 0.04672841457010714, "phrase": "approximation_spaces"}, {"score": 0.014776455064572442, "phrase": "behaviour_patterns"}, {"score": 0.014163415439647324, "phrase": "reinforcement_learning"}, {"score": 0.012884781790890896, "phrase": "expected_value"}, {"score": 0.00481495049065317, "phrase": "off-policy_monte_carlo"}, {"score": 0.00462846651646366, "phrase": "monte_carlo"}, {"score": 0.004598310988662782, "phrase": "mc"}, {"score": 0.0044491728131708505, "phrase": "rough_set_theory"}, {"score": 0.004405440436639404, "phrase": "zdzislaw_pawlak"}, {"score": 0.004262733542795058, "phrase": "action_selections"}, {"score": 0.004165586713723931, "phrase": "reward_signal"}, {"score": 0.003912874578852994, "phrase": "cumulative_future_discounted_rewards"}, {"score": 0.0038743931383378558, "phrase": "agent_actions"}, {"score": 0.0036997388927095935, "phrase": "weighted_sampling"}, {"score": 0.0036392821787269727, "phrase": "mc_methods"}, {"score": 0.003264224201388006, "phrase": "approximation_space"}, {"score": 0.003168797616694096, "phrase": "agent_behaviours"}, {"score": 0.003025854378927907, "phrase": "accepted_agent_behaviours"}, {"score": 0.0029763765837545, "phrase": "behaviour_evaluation_norm"}, {"score": 0.00289888453546698, "phrase": "adaptive_action_control_strategy"}, {"score": 0.0027680831622262027, "phrase": "adaptive_learning"}, {"score": 0.0027408292745619233, "phrase": "oliver_selfridge"}, {"score": 0.0026871217010784143, "phrase": "approximate_spaces"}, {"score": 0.002565850749422255, "phrase": "monocular_vision_system"}, {"score": 0.002482585701725093, "phrase": "reinforcement_learning_methods"}, {"score": 0.002433926157079375, "phrase": "vision_system"}, {"score": 0.002394104138423188, "phrase": "moving_object"}, {"score": 0.002263508433567761, "phrase": "camera_field"}, {"score": 0.002161313898853851, "phrase": "rt_form"}, {"score": 0.0021470954708827125, "phrase": "off-policy_mc_learning"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["approximation space", " behaviour pattern", " expectation", " off-policy learning", " Monte Carlo method", " rough sets", " run-and-twiddle"], "paper_abstract": "This paper introduces an approach to off-policy Monte Carlo (MC) learning guided by behaviour patterns gleaned from approximation spaces and rough set theory introduced by Zdzislaw Pawlak in 1981. During reinforcement learning, an agent makes action selections in an effort to maximize a reward signal obtained from the environment. The problem considered in this paper is how to estimate the expected value of cumulative future discounted rewards in evaluating agent actions during reinforcement learning. The solution to this problem results from a form of weighted sampling using a combination of MC methods and approximation spaces to estimate the expected value of returns on actions. This is made possible by considering behaviour patterns of an agent in the context of approximation spaces. The framework provided by an approximation space makes it possible to measure the degree that agent behaviours are a part of (\"covered by\") a set of accepted agent behaviours that serve as a behaviour evaluation norm. Furthermore, this article introduces an adaptive action control strategy called run-and-twiddle (RT) (a form of adaptive learning introduced by Oliver Selfridge in 1984), where approximate spaces are constructed on a \"need by need\" basis. Finally, a monocular vision system has been selected to facilitate the evaluation of the reinforcement learning methods. The goal of the vision system is to track a moving object, and rewards are based on the proximity of the object to the centre of the camera field of view. The contribution of this article is the introduction of a RT form of off-policy MC learning. (c) 2006 Elsevier Ltd. All rights reserved.", "paper_title": "Approximation spaces in off-policy Monte Carlo learning", "paper_id": "WOS:000247826200012"}