{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "novel_algorithms"}, {"score": 0.004712463245074863, "phrase": "particular_class"}, {"score": 0.00467207889925365, "phrase": "mixed-norm_regularization"}, {"score": 0.004379977556546879, "phrase": "norm_regularization"}, {"score": 0.004286708921931472, "phrase": "rkhs_norms"}, {"score": 0.0040708542958968605, "phrase": "non-sparse_combinations"}, {"score": 0.0036553070575015344, "phrase": "variable_sparsity_kernel_learning"}, {"score": 0.003546722347907148, "phrase": "previous_attempts"}, {"score": 0.003501175642882274, "phrase": "non-convex_formulation"}, {"score": 0.0034118234596993836, "phrase": "convex_formulation"}, {"score": 0.0033680032592969633, "phrase": "efficient_mirror-descent"}, {"score": 0.0032398798149196432, "phrase": "proposed_md_based_algorithm"}, {"score": 0.0031166151196841308, "phrase": "computational_complexity"}, {"score": 0.0029214769181390653, "phrase": "training_data_points"}, {"score": 0.00253400005142168, "phrase": "detailed_proof"}, {"score": 0.002427035025924384, "phrase": "experimental_results"}, {"score": 0.0023855210798264205, "phrase": "vskl_formulations"}, {"score": 0.0023245746777044766, "phrase": "multi-modal_learning_tasks"}, {"score": 0.002226430168622392, "phrase": "md_based_algorithm"}, {"score": 0.0021049977753042253, "phrase": "computational_efficiency"}], "paper_keywords": ["multiple kernel learning", " mirror descent", " mixed-norm", " object categorization", " scalability"], "paper_abstract": "This paper(1) presents novel algorithms and applications for a particular class of mixed-norm regularization based Multiple Kernel Learning (MKL) formulations. The formulations assume that the given kernels are grouped and employ l(1) norm regularization for promoting sparsity within RKHS norms of each group and l(s), s >= 2 norm regularization for promoting non-sparse combinations across groups. Various sparsity levels in combining the kernels can be achieved by varying the grouping of kernels-hence we name the formulations as Variable Sparsity Kernel Learning (VSKL) formulations. While previous attempts have a non-convex formulation, here we present a convex formulation which admits efficient Mirror-Descent (MD) based solving techniques. The proposed MD based algorithm optimizes over product of simplices and has a computational complexity of O (m(2)n(tot) log n(max)/epsilon(2)) where m is no. training data points, n(max), n(tot) are the maximum no. kernels in any group, total no. kernels respectively and epsilon is the error in approximating the objective. A detailed proof of convergence of the algorithm is also presented. Experimental results show that the VSKL formulations are well-suited for multi-modal learning tasks like object categorization. Results also show that the MD based algorithm outperforms state-of-the-art MKL solvers in terms of computational efficiency.", "paper_title": "Variable Sparsity Kernel Learning", "paper_id": "WOS:000288896800007"}