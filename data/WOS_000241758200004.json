{"auto_keywords": [{"score": 0.04224272831819151, "phrase": "ensemble_members"}, {"score": 0.03727254882129724, "phrase": "static_approach"}, {"score": 0.03352763753987084, "phrase": "variable_approach"}, {"score": 0.00481495049065317, "phrase": "algorithmic_extension"}, {"score": 0.004653489198769671, "phrase": "stacking"}, {"score": 0.00417160036938041, "phrase": "training_accuracy"}, {"score": 0.0037140731069193896, "phrase": "standard_stacking_algorithm"}, {"score": 0.0030460403366412126, "phrase": "appropriate_level"}, {"score": 0.002447101794397243, "phrase": "smaller_and_less_complex_ensembles"}, {"score": 0.0023810183846557486, "phrase": "latter_respect"}], "paper_keywords": ["machine learning", " ensemble learning", " stacking", " pruning"], "paper_abstract": "In this paper we investigate an algorithmic extension to the technique of Stacking for regression that prunes the ensemble set before application based on a consideration of the training accuracy and diversity of the ensemble members. We evaluate two variants of this approach in comparison to the standard Stacking algorithm, one of which is a static approach that prunes back the ensemble to the same constant size, the other of which is a variable approach prunes the ensemble to an appropriate level based on measures of accuracy and diversity of the ensemble members. We show that on average both techniques are robust in performance to their non-pruned counterpart, while having the advantage of producing smaller and less complex ensembles. In the latter respect, the static approach proved more effective, but we show that the variable approach lends itself better for further optimization.", "paper_title": "Pruning extensions to stacking", "paper_id": "WOS:000241758200004"}