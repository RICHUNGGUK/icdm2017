{"auto_keywords": [{"score": 0.025400041920749714, "phrase": "proposed_eas"}, {"score": 0.00481495049065317, "phrase": "dynamically_fast_and_statistically_robust_evolutionary_algorithms"}, {"score": 0.00470862827326028, "phrase": "general_optimization_problems"}, {"score": 0.00458412183693466, "phrase": "solution_representation_schemes"}, {"score": 0.004154995396099332, "phrase": "different_formulations"}, {"score": 0.004045068727683462, "phrase": "similar_and_generalized_strategies"}, {"score": 0.0038855864345030563, "phrase": "existing_eas"}, {"score": 0.0037157201349604222, "phrase": "new_tradeoff_function-based_mutation"}, {"score": 0.00358555943458669, "phrase": "cauchy"}, {"score": 0.0035558970189749576, "phrase": "gaussian"}, {"score": 0.0035216195857272403, "phrase": "random_as_well_as_chaotic_mutations"}, {"score": 0.0034283921404628975, "phrase": "generalized_learning_rule"}, {"score": 0.0031210475158175406, "phrase": "learning_rule"}, {"score": 0.0030794433253250476, "phrase": "theoretical_study"}, {"score": 0.002905454531687249, "phrase": "search_strategy"}, {"score": 0.0028539179130824786, "phrase": "new_tradeoff-based_mutations"}, {"score": 0.0027047136845332917, "phrase": "orthogonal_arrays"}, {"score": 0.0026686444508167875, "phrase": "ea_experimentation"}, {"score": 0.0026095920086044145, "phrase": "noise-based_tuning"}, {"score": 0.002574788013427524, "phrase": "robust_parameter_tuning"}, {"score": 0.0023333173799963795, "phrase": "different_problems"}, {"score": 0.0023125191783901367, "phrase": "varying_complexities"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["evolutionary algorithms", " numerical optimization", " combinatorial optimization", " mutation step size", " chaos", " noise", " design of experiments", " parameter tuning"], "paper_abstract": "This paper characterizes general optimization problems into four categories based oil the solution representation schemes, as they have been the key to the design of various evolutionary algorithms (EAs). Four EAs have been designed for different formulations with the aim of utilizing similar and generalized strategies for all of them. Several modifications to the existing EAs have been proposed and studied. First, a new tradeoff function-based mutation has been proposed that takes advantages of Cauchy, Gaussian, random as well as chaotic mutations. In addition, a generalized learning rule has also been proposed to ensure more thorough and explorative search. A theoretical analysis has been performed to establish the convergence of the learning rule. A theoretical study has also been performed in order to investigate the various aspects of the search strategy employed by the new tradeoff-based mutations. A more logical parameter tuning has been done by introducing the concept of orthogonal arrays in the EA experimentation. The use of noise-based tuning ensures the robust parameter tuning that enables the EAs to perform remarkably well in the further experimentations. The performance of the proposed EAs has been analyzed for different problems of varying complexities. The results prove the supremacy of the proposed EAs over other well-established strategies given in the literature. (c) 2007 Elsevier Ltd. All rights reserved.", "paper_title": "Improved and generalized learning strategies for dynamically fast and statistically robust evolutionary algorithms", "paper_id": "WOS:000257010700003"}