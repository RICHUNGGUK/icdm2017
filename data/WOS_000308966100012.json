{"auto_keywords": [{"score": 0.04681065843992322, "phrase": "regular_and_context-free_languages"}, {"score": 0.00481495049065317, "phrase": "symbolic_representation"}, {"score": 0.004677032188041644, "phrase": "simple_recurrent_error_backpropagation_networks"}, {"score": 0.004543046350523362, "phrase": "temporal_sequence_data"}, {"score": 0.004358240019060018, "phrase": "relatively_large_and_opaque_weight_matrices"}, {"score": 0.004268663239979243, "phrase": "substantial_research"}, {"score": 0.004180919839686364, "phrase": "symbolic_human-readable_interpretations"}, {"score": 0.004077995480957875, "phrase": "feedforward_networks"}, {"score": 0.0039446780026489905, "phrase": "rule_extraction"}, {"score": 0.0038796562792487, "phrase": "recurrent_networks"}, {"score": 0.003799878496075633, "phrase": "dynamical_systems"}, {"score": 0.003690927840780375, "phrase": "finite-state_machine"}, {"score": 0.003540661500026125, "phrase": "network's_hidden_layer_activation_space"}, {"score": 0.00345344412693677, "phrase": "finite_number"}, {"score": 0.0033266163944946185, "phrase": "better_techniques"}, {"score": 0.0032581736321024373, "phrase": "activation_space"}, {"score": 0.0031125001790084936, "phrase": "network_training_process"}, {"score": 0.0030611534228319717, "phrase": "better_representation"}, {"score": 0.0030357976028714557, "phrase": "hidden_layer_activation_space"}, {"score": 0.00284034646798833, "phrase": "powerful_general_technique"}, {"score": 0.0027934768366077397, "phrase": "error_backpropagation_training_process"}, {"score": 0.0027246145918972025, "phrase": "activation_space_representation"}, {"score": 0.002496659907186784, "phrase": "computational_experiments"}, {"score": 0.0024656866249273125, "phrase": "modified_learning_method"}, {"score": 0.002404885278896803, "phrase": "substantially_fewer_states"}, {"score": 0.0023651843133643768, "phrase": "unmodified_backpropagation_learning"}, {"score": 0.002316476356455289, "phrase": "neural_networks'_accuracy"}, {"score": 0.0021853532419450887, "phrase": "learned_pattern_encodings"}, {"score": 0.002158233639156017, "phrase": "hidden_layer"}, {"score": 0.0021314498639048085, "phrase": "effective_way"}, {"score": 0.0021049977753042253, "phrase": "contemporary_fsm_extraction_methods"}], "paper_keywords": ["Finite-state machines", " hidden layer representation", " penalty function", " recurrent neural networks"], "paper_abstract": "Simple recurrent error backpropagation networks have been widely used to learn temporal sequence data, including regular and context-free languages. However, the production of relatively large and opaque weight matrices during learning has inspired substantial research on how to extract symbolic human-readable interpretations from trained networks. Unlike feedforward networks, where research has focused mainly on rule extraction, most past work with recurrent networks has viewed them as dynamical systems that can be approximated symbolically by finite-state machine (FSMs). With this approach, the network's hidden layer activation space is typically divided into a finite number of regions. Past research has mainly focused on better techniques for dividing up this activation space. In contrast, very little work has tried to influence the network training process to produce a better representation in hidden layer activation space, and that which has been done has had only limited success. Here we propose a powerful general technique to bias the error backpropagation training process so that it learns an activation space representation from which it is easier to extract FSMs. Using four publicly available data sets that are based on regular and context-free languages, we show via computational experiments that the modified learning method helps to extract FSMs with substantially fewer states and less variance than unmodified backpropagation learning, without decreasing the neural networks' accuracy. We conclude that modifying error backpropagation so that it more effectively separates learned pattern encodings in the hidden layer is an effective way to improve contemporary FSM extraction methods.", "paper_title": "Symbolic Representation of Recurrent Neural Network Dynamics", "paper_id": "WOS:000308966100012"}