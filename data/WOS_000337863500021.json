{"auto_keywords": [{"score": 0.043276982624438305, "phrase": "image_features"}, {"score": 0.042392974267918604, "phrase": "visual_content"}, {"score": 0.024085532277688674, "phrase": "lsi"}, {"score": 0.00481495049065317, "phrase": "multi-modal_incompleteness_ontology"}, {"score": 0.004743164872278112, "phrase": "information_fusion"}, {"score": 0.0047228510372287515, "phrase": "image_retrieval"}, {"score": 0.004692542523226182, "phrase": "significant_effort"}, {"score": 0.004505033991632825, "phrase": "automatic_domain_specific_knowledge-base"}, {"score": 0.0044473848205867856, "phrase": "metadata_extraction"}, {"score": 0.00442833210405781, "phrase": "visual_information"}, {"score": 0.004399905285945236, "phrase": "associated_textual_information"}, {"score": 0.004315710310147, "phrase": "visual_and_textual_information"}, {"score": 0.004278807087784626, "phrase": "complete_domain-specific_kb"}, {"score": 0.00416102793456816, "phrase": "natural_language"}, {"score": 0.003943562715818296, "phrase": "kb"}, {"score": 0.0038185038836798463, "phrase": "easy_task"}, {"score": 0.0036422425344358037, "phrase": "integrated_ontology_model"}, {"score": 0.0035571758390689967, "phrase": "image_processing_algorithm_errors"}, {"score": 0.0034740889984192815, "phrase": "complete_kb"}, {"score": 0.003451766520526601, "phrase": "specific_domain"}, {"score": 0.003306559295382181, "phrase": "first_index"}, {"score": 0.003292377743364292, "phrase": "low-level_features"}, {"score": 0.0032501957991199044, "phrase": "novel_technique"}, {"score": 0.003167441161387607, "phrase": "visual_word"}, {"score": 0.0031403257100142876, "phrase": "ontology_model"}, {"score": 0.003066945490483524, "phrase": "concept_features"}, {"score": 0.0030081804762010092, "phrase": "second_index"}, {"score": 0.0029824243255516343, "phrase": "textual_description"}, {"score": 0.0028081842633816607, "phrase": "single_indexing_model"}, {"score": 0.0027602925235474317, "phrase": "image_retrieval_efficiency"}, {"score": 0.0027307746993761035, "phrase": "rich_index"}, {"score": 0.002684199575281511, "phrase": "desired_images"}, {"score": 0.0026214496600537986, "phrase": "similar_words"}, {"score": 0.0023239757294367175, "phrase": "multi-modal_metadata"}, {"score": 0.0023090260882414485, "phrase": "proposed_kb"}, {"score": 0.0022745156593932496, "phrase": "additional_experiment"}, {"score": 0.0022405198609939812, "phrase": "incomplete_kb"}, {"score": 0.0021881193183028854, "phrase": "relevant_images"}, {"score": 0.002164847407801454, "phrase": "ontology"}, {"score": 0.002132348817167717, "phrase": "enhanced_retrieval_performance"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Multi-Modal Ontology", " Knowledge base", " Incomplete Ontology", " Visual and textual information fusion"], "paper_abstract": "A significant effort by researchers has advanced the ability of computers to understand, index and annotate images. This entails automatic domain specific knowledge-base (KB) construction and metadata extraction from visual information and any associated textual information. However, it is challenging to fuse visual and textual information and build a complete domain-specific KB for image annotation due to several factors such as: the ambiguity of natural language to describe image features; the semantic gap when using image features to represent visual content and the incompleteness of the metadata in the KB. Typically the KB is based upon a domain specific Ontology. However, it is not an easy task to extract the data needed from annotations and images, and then to automatically process these and transform them into an integrated Ontology model, because of the ambiguity of terms and because of image processing algorithm errors. As such, it is difficult to construct a complete KB covering a specific domain of knowledge. This paper presents a Multi-Modal Incompleteness Ontology-based (MMIO) system for image retrieval based upon fusing two derived indices. The first index exploits low-level features extracted from images. A novel technique is proposed to represent the semantics of visual content, by restructuring visual word vectors into an Ontology model by computing the distance between the visual word features and concept features, the so called concept range. The second index relies on a textual description which is processed to extract and recognise the concepts, properties, or instances that are defined in an Ontology. The two indexes are fused into a single indexing model, which is used to enhance the image retrieval efficiency. Nonetheless, this rich index may not be sufficient to find the desired images. Therefore, a Latent Semantic Indexing (LSI) algorithm is exploited to search for similar words to those used in a query. As a result, it is possible to retrieve images with a query using (similar) words that do not appear in the caption. The efficiency of the KB is validated experimentally with respect to three criteria, correctness, multimodality, and robustness. The results show that the multi-modal metadata in the proposed KB could be utilised efficiently. An additional experiment demonstrates that LSI can handle an incomplete KB effectively. Using LSI, the system can still retrieve relevant images when information in the Ontology is missing, leading to an enhanced retrieval performance. (C) 2014 Elsevier B.V. All rights reserved.", "paper_title": "A Multi-Modal Incompleteness Ontology model (MMIO) to enhance information fusion for image retrieval", "paper_id": "WOS:000337863500021"}