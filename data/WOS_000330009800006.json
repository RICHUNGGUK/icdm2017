{"auto_keywords": [{"score": 0.0482792484677782, "phrase": "feedforward_neural_networks"}, {"score": 0.0432428177101078, "phrase": "error_function"}, {"score": 0.00481495049065317, "phrase": "gradient_method"}, {"score": 0.004363277689798392, "phrase": "novel_method"}, {"score": 0.003175792987190496, "phrase": "key_point"}, {"score": 0.0026942333853202556, "phrase": "gradient_value"}, {"score": 0.002564505790176378, "phrase": "namely_the_final_weights"}, {"score": 0.00221153232497724, "phrase": "supporting_numerical_examples"}, {"score": 0.0021049977753042253, "phrase": "elsevier_ltd."}], "paper_keywords": ["Feedforward neural networks", " Batch gradient method", " Smoothing L-1/2 regularization", " Convergence"], "paper_abstract": "The aim of this paper is to develop a novel method to prune feedforward neural networks by introducing an L-1/2 regularization term into the error function. This procedure forces weights to become smaller during the training and can eventually removed after the training. The usual L-1/2 regularization term involves absolute values and is not differentiable at the origin, which typically causes oscillation of the gradient of the error function during the training. A key point of this paper is to modify the usual L-1/2 regularization term by smoothing it at the origin. This approach offers the following three advantages: First, it removes the oscillation of the gradient value. Secondly, it gives better pruning, namely the final weights to be removed are smaller than those produced through the usual L-1/2 regularization. Thirdly, it makes it possible to prove the convergence of the training. Supporting numerical examples are also provided. (C) 2013 Elsevier Ltd. All rights reserved.", "paper_title": "Batch gradient method with smoothing L-1/2 regularization for training of feedforward neural networks", "paper_id": "WOS:000330009800006"}