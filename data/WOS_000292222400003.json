{"auto_keywords": [{"score": 0.049651030276363126, "phrase": "imprecise_transition_probabilities"}, {"score": 0.0363811913049306, "phrase": "mdp-ips"}, {"score": 0.0258678242431025, "phrase": "magnitude_speedup"}, {"score": 0.00481495049065317, "phrase": "factored_mdps"}, {"score": 0.004686963264680882, "phrase": "real-world_decision-theoretic_planning_problems"}, {"score": 0.004603660056842852, "phrase": "mdp"}, {"score": 0.00438161736774803, "phrase": "completely_accurate_estimate"}, {"score": 0.004342430078306461, "phrase": "transition_probabilities"}, {"score": 0.0042459818200777846, "phrase": "natural_uncertainty"}, {"score": 0.0041703614123283165, "phrase": "transition_specification"}, {"score": 0.003376216135716632, "phrase": "external_calls"}, {"score": 0.0033459905006404207, "phrase": "optimization_routines"}, {"score": 0.003058179790439378, "phrase": "factored_mdp-ip"}, {"score": 0.0030171910207671205, "phrase": "efficient_dynamic_programming_methods"}, {"score": 0.002910545717767559, "phrase": "key_computational_bottleneck"}, {"score": 0.0028458096625791625, "phrase": "factored_mdp-ips"}, {"score": 0.0027575842698911173, "phrase": "nonlinear_constrained_optimization_problems"}, {"score": 0.0026720866990543744, "phrase": "approximation_techniques"}, {"score": 0.002612640527241776, "phrase": "computational_overhead"}, {"score": 0.00257760787870709, "phrase": "nonlinear_solver"}, {"score": 0.0025202582748972122, "phrase": "approximately_optimal_solutions"}, {"score": 0.002387758958649797, "phrase": "traditional_\"flat\"_dynamic_programming_approaches"}, {"score": 0.002262209824105154, "phrase": "factored_mdp_approximate_value_iteration_techniques"}, {"score": 0.0022019274077124795, "phrase": "lowest_error"}, {"score": 0.0021723899772112423, "phrase": "approximation_algorithm"}, {"score": 0.0021049977753042253, "phrase": "elsevier_b.v."}], "paper_keywords": ["Probabilistic planning", " Markov Decision Process", " Robust planning"], "paper_abstract": "When modeling real-world decision-theoretic planning problems in the Markov Decision Process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilities. For example, natural uncertainty arises in the transition specification due to elicitation of MOP transition models from an expert or estimation from data, or non-stationary transition distributions arising from insufficient state knowledge. In the interest of obtaining the most robust policy under transition uncertainty, the Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced to model such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs, they often require external calls to optimization routines and thus can be extremely time-consuming in practice. To address this deficiency, we introduce the factored MDP-IP and propose efficient dynamic programming methods to exploit its structure. Noting that the key computational bottleneck in the solution of factored MDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target approximation techniques to drastically reduce the computational overhead of the nonlinear solver while producing bounded, approximately optimal solutions. Our results show up to two orders of magnitude speedup in comparison to traditional \"flat\" dynamic programming approaches and up to an order of magnitude speedup over the extension of factored MDP approximate value iteration techniques to MDP-IPs while producing the lowest error of any approximation algorithm evaluated. (C) 2011 Elsevier B.V. All rights reserved.", "paper_title": "Efficient solutions to factored MDPs with imprecise transition probabilities", "paper_id": "WOS:000292222400003"}