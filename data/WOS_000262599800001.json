{"auto_keywords": [{"score": 0.04975833397985591, "phrase": "boundary_methods"}, {"score": 0.04901899987844769, "phrase": "linear_discriminant_analysis"}, {"score": 0.048809682481968285, "phrase": "quadratic_discriminant_analysis"}, {"score": 0.04839358903954864, "phrase": "support_vector_machines"}, {"score": 0.04787820591481736, "phrase": "data_structure"}, {"score": 0.0454761423311978, "phrase": "mahalanobis_distance"}, {"score": 0.04479753776688935, "phrase": "qda"}, {"score": 0.043375536632117836, "phrase": "lvq"}, {"score": 0.00481495049065317, "phrase": "five_common_classifiers"}, {"score": 0.004729585435671683, "phrase": "centroids"}, {"score": 0.0046663612183942054, "phrase": "learning_vector_quantization"}, {"score": 0.0045732587698679005, "phrase": "five_methods"}, {"score": 0.004502126088469323, "phrase": "euclidean_distance"}, {"score": 0.004461981764178059, "phrase": "edc"}, {"score": 0.00441274143103218, "phrase": "lda"}, {"score": 0.004333927564687078, "phrase": "pooled_variance_covariance_matrix"}, {"score": 0.004209539728590977, "phrase": "individual_class_variance_covariance_matrix_-_non-bayesian_form"}, {"score": 0.004061317076006302, "phrase": "soft_boundaries"}, {"score": 0.004043158538239064, "phrase": "radial_basis_functions"}, {"score": 0.003622680201367453, "phrase": "five_variables"}, {"score": 0.003487222504700868, "phrase": "significant_degree"}, {"score": 0.0033946556965877873, "phrase": "elongated_hyperellipsoids"}, {"score": 0.0033794677514415354, "phrase": "small_overlap"}, {"score": 0.0032971365838445005, "phrase": "complete_overlap"}, {"score": 0.003216804708106142, "phrase": "crescent_shape"}, {"score": 0.003131393466870228, "phrase": "multinormal_populations"}, {"score": 0.0030895394261534776, "phrase": "potential_discriminators"}, {"score": 0.0030619470282229307, "phrase": "large_proportion"}, {"score": 0.002907992115184249, "phrase": "test_set"}, {"score": 0.002856272073830495, "phrase": "different_test"}, {"score": 0.002430185991522955, "phrase": "complex_boundaries"}, {"score": 0.0022719445763866937, "phrase": "multinormal_data"}, {"score": 0.0022365262201954643, "phrase": "non-discriminative_variables"}, {"score": 0.0021479597698614355, "phrase": "model_building"}, {"score": 0.002128758578353008, "phrase": "optimal_type"}], "paper_keywords": ["Classification", " Boundary methods", " Euclidean distance", " Mahalanobis distance", " Linear Discriminant Analysis", " Quadratic Discriminant Analysis", " Learning Vector Quantization", " Support Vector Machines"], "paper_abstract": "Five methods for discrimination are described, namely Euclidean Distance to centroids (EDC), Linear Discriminant Analysis (LDA) (based on the Mahalanobis distance and pooled variance covariance matrix), Quadratic Discriminant Analysis (QDA) (based on the Mahalanobis distance and individual class variance covariance matrix - non-Bayesian form), Learning Vector Quantization (LVQ) and Support Vector Machines (SVMs) (using soft boundaries and Radial Basis Functions), and illustrated graphically as boundary methods. The performance of each method was determined using four synthetic datasets each consisting of 200 samples half belonging to one of two classes, and a further two synthetic datasets containing 400 samples, again equally split between the two classes. In datasets 1 to 3. five variables were distributed multinormally, in dataset 1 the classes are distributed roughly circularly but with a significant degree of overlap, in dataset 2. the distribution is in elongated hyperellipsoids with small overlap, and in dataset 3 there is a region of complete overlap between classes. In dataset 4 two variables are distributed in a crescent shape. In datasets 5 and 6, 100 variables were generated from multinormal populations, some of which were potential discriminators, however a large proportion of the variables were designed to be uninformative. The methods were optimised using a training set and their performance evaluated using a test set: this was repeated 100 times for different test and training set splits. The average % correctly classified was computed for each class and model, as well as the model stability for each sample (the proportion of times the sample is classified into the same group over all 100 iterations). The conclusions are that the performance of the classifiers depends very much on the distribution of data. Approaches such as LVQ and SVMs that try to determine complex boundaries perform best when the data is not normally distributed such as in dataset 4, but can be prone to overfitting otherwise. QDA tends to perform best on multinormal data although it can be influenced by non-discriminative variables which show a difference in variance. It is recommended to look at the data structure prior to model building to determine the optimal type of model. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "Comparison of performance of five common classifiers represented as boundary methods: Euclidean Distance to Centroids, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Learning Vector Quantization and Support Vector Machines, as dependent on data structure", "paper_id": "WOS:000262599800001"}