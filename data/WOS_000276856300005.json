{"auto_keywords": [{"score": 0.045130573715649755, "phrase": "ahpios"}, {"score": 0.029007308479638578, "phrase": "two-tiered_cooperative_cache"}, {"score": 0.004635187610736025, "phrase": "novel_design"}, {"score": 0.003958937342925219, "phrase": "mpi_implementation"}, {"score": 0.0037697665240643066, "phrase": "high_performance"}, {"score": 0.0036886073959526396, "phrase": "mpi_applications"}, {"score": 0.0035701213515857227, "phrase": "distributed_partitions"}, {"score": 0.003512308265830794, "phrase": "convenient_way"}, {"score": 0.003362660648932247, "phrase": "storage_management_system"}, {"score": 0.003236941658990869, "phrase": "tight_integration"}, {"score": 0.0030989899982042236, "phrase": "ahpios_partitions"}, {"score": 0.002646267286675984, "phrase": "spatial_locality"}, {"score": 0.0026175952455594277, "phrase": "data-intensive_parallel_applications"}, {"score": 0.0025611773887867255, "phrase": "access_latency"}, {"score": 0.0024519545383399773, "phrase": "asynchronous_data_staging_strategy"}, {"score": 0.0023219374444738723, "phrase": "storage_resources"}, {"score": 0.002210814150677161, "phrase": "integrated_ahpios"}, {"score": 0.00217496508306238, "phrase": "substantial_performance_benefit"}, {"score": 0.0021396960731214203, "phrase": "traditional_mpi-io_solutions"}, {"score": 0.0021049977753042253, "phrase": "pvfs_or_lustre_parallel_file_systems"}], "paper_keywords": ["parallel I/O", " parallel systems", " distributed file systems", " parallelism and concurrency"], "paper_abstract": "In this paper we present the novel design, implementation, and evaluation of an ad-hoc parallel I/O system (AHPIOS). AHPIOS is the first scalable parallel I/O system completely implemented in the Message Passing Interface (MPI). The MPI implementation brings the advantages of portability, scalability and high performance. AHPIOS allows MPI applications to dynamically manage and scale distributed partitions in a convenient way. The configuration of both the MPI-IO and the storage management system is unified and allows for a tight integration of the optimizations of these layers. AHPIOS partitions are elastic: they conveniently scale up and down with the number of resources. We develop two collective I/O strategies, which leverage a two-tiered cooperative cache in order to exploit the spatial locality of data-intensive parallel applications. The file access latency is hidden from the applications through an asynchronous data staging strategy. The two-tiered cooperative cache scales with both the number of processors and storage resources. Our experimental section demonstrates that, with various optimizations, integrated AHPIOS offers a substantial performance benefit over the traditional MPI-IO solutions on both PVFS or Lustre parallel file systems.", "paper_title": "A SCALABLE MESSAGE PASSING INTERFACE IMPLEMENTATION OF AN AD-HOC PARALLEL I/O SYSTEM", "paper_id": "WOS:000276856300005"}