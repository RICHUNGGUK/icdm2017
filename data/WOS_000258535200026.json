{"auto_keywords": [{"score": 0.033397426537435056, "phrase": "experimental_design"}, {"score": 0.00481495049065317, "phrase": "experimental_computer_science"}, {"score": 0.004720606036240263, "phrase": "relevant_workloads"}, {"score": 0.0046741260449047976, "phrase": "appropriate_experimental_design"}, {"score": 0.004605258779434384, "phrase": "rigorous_analysis"}, {"score": 0.004212759658511567, "phrase": "managed_languages"}, {"score": 0.004150890523309346, "phrase": "java"}, {"score": 0.004049207745667061, "phrase": "ruby"}, {"score": 0.00395017798595193, "phrase": "imminent_rise"}, {"score": 0.003911253966141701, "phrase": "commodity_multicore_architectures"}, {"score": 0.003834548373258131, "phrase": "new_methodological_challenges"}, {"score": 0.003192406091820197, "phrase": "benchmark_suite_design"}, {"score": 0.003068327486877065, "phrase": "java_applications"}, {"score": 0.0029637093924781825, "phrase": "new_criteria"}, {"score": 0.0028911669203979156, "phrase": "diverse_applications"}, {"score": 0.0028484940955893134, "phrase": "benchmark_suite"}, {"score": 0.0026973310437053573, "phrase": "java_runtime_system"}, {"score": 0.0021049977753042253, "phrase": "sound_methodological_foundations"}], "paper_keywords": [""], "paper_abstract": "Evaluation methodology underpins all innovation in experimental computer science. It requires relevant workloads, appropriate experimental design, and rigorous analysis. Unfortunately, methodology is not keeping pace with the changes in our field. The rise of managed languages such as Java, C #, and Ruby in the past decade and the imminent rise of commodity multicore architectures for the next decade pose new methodological challenges that are not yet widely understood. This paper explores the consequences of our collective inattention to methodology on innovation, makes recommendations for addressing this problem in one domain, and provides guidelines for other domains. We describe benchmark suite design, experimental design, and analysis for evaluating Java applications. For example, we introduce new criteria for measuring and selecting diverse applications for a benchmark suite. We show that the complexity and nondeterminism of the Java runtime system make experimental design a first-order consideration, and we recommend mechanisms for addressing complexity and nondeterminism. Drawing on these results, we suggest how to adapt methodology more broadly. To continue to deliver innovations, our field needs to significantly increase participation in and funding for developing sound methodological foundations.", "paper_title": "Wake up and smell the coffee: Evaluation methodology for the 21st century", "paper_id": "WOS:000258535200026"}