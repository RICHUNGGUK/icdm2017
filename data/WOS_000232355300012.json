{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "relevance_feedback_information"}, {"score": 0.004466519142879166, "phrase": "user_search_formulations"}, {"score": 0.004283913685201902, "phrase": "new_high_accuracy_retrieval"}, {"score": 0.00390792986752692, "phrase": "first_method"}, {"score": 0.0033905067962602515, "phrase": "second_method"}, {"score": 0.0031186510842824626, "phrase": "noun_phrases"}, {"score": 0.0030160945545795494, "phrase": "initial_document"}, {"score": 0.002728166176898541, "phrase": "user_feedback"}, {"score": 0.002660574167091781, "phrase": "trec_results"}, {"score": 0.00246765655376112, "phrase": "effective_means"}, {"score": 0.002426717832834137, "phrase": "interactive_query_expansion"}, {"score": 0.0023665766694993535, "phrase": "significant_performance_improvements"}, {"score": 0.002158505888557746, "phrase": "detailed_analysis"}, {"score": 0.0021049977753042253, "phrase": "evaluation_results"}], "paper_keywords": ["information retrieval", " query expansion", " natural language processing", " interactive retrieval", " relevance feedback"], "paper_abstract": "The paper presents two approaches to interactively refining user search formulations and their evaluation in the new High Accuracy Retrieval from Documents (HARD) track of TREC-12. The first method consists of asking the user to select a number of sentences that represent documents. The second method consists of showing to the user a list of noun phrases extracted from the initial document set. Both methods then expand the query based on the user feedback. The TREC results show that one of the methods is an effective means of interactive query expansion and yields significant performance improvements. The paper presents a comparison of the methods and detailed analysis of the evaluation results. (c) 2004 Elsevier Ltd. All rights reserved.", "paper_title": "Elicitation and use of relevance feedback information", "paper_id": "WOS:000232355300012"}