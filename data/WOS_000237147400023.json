{"auto_keywords": [{"score": 0.024973734692230173, "phrase": "mmse"}, {"score": 0.008347927423915458, "phrase": "epi"}, {"score": 0.00481495049065317, "phrase": "entropy-power_inequality"}, {"score": 0.0038810583785936505, "phrase": "simple_proof"}, {"score": 0.003611751858608048, "phrase": "shannon's_entropy-power_inequality"}, {"score": 0.002708399936381754, "phrase": "mutual_information"}, {"score": 0.0025202582748972122, "phrase": "minimum_mean-square_error"}, {"score": 0.0021049977753042253, "phrase": "gaussian_channels"}], "paper_keywords": ["differential entropy", " entropy-power inequality (EPI)", " minimum mean-square error (MMSE)"], "paper_abstract": "This correspondence gives a simple proof of Shannon's entropy-power inequality (EPI) using the relationship between mutual information and minimum mean-square error (MMSE) in Gaussian channels.", "paper_title": "A simple proof of the entropy-power inequality", "paper_id": "WOS:000237147400023"}