{"auto_keywords": [{"score": 0.04646369500712274, "phrase": "end_hosts"}, {"score": 0.00481495049065317, "phrase": "large_scale_traceroute_datasets"}, {"score": 0.004713162564945179, "phrase": "parallel_computation"}, {"score": 0.004613516457919661, "phrase": "routing_overlay_network"}, {"score": 0.004483909657713567, "phrase": "shorter_detour_paths"}, {"score": 0.004373480870285314, "phrase": "default_path"}, {"score": 0.004311598785718415, "phrase": "detour_paths"}, {"score": 0.004205395707787349, "phrase": "triangle_inequality_violation"}, {"score": 0.0040726680518382965, "phrase": "internet_delay_space"}, {"score": 0.003958194802333944, "phrase": "intermediate_hop"}, {"score": 0.00388828234270598, "phrase": "direct_latency"}, {"score": 0.0036990033106718183, "phrase": "interdomain_routing_policies"}, {"score": 0.0036727235025069828, "phrase": "autonomous_systems"}, {"score": 0.003569451476205552, "phrase": "internet_exchange_points"}, {"score": 0.0035063811070537233, "phrase": "identifying_detours"}, {"score": 0.0034690732089830045, "phrase": "global_overlay_network"}, {"score": 0.0034444213065548688, "phrase": "large_amounts"}, {"score": 0.003419943984098415, "phrase": "computational_capabilities"}, {"score": 0.003371508176308189, "phrase": "sheer_number"}, {"score": 0.0033475472608077823, "phrase": "possible_paths"}, {"score": 0.003195885276330599, "phrase": "parallel_programming_paradigms"}, {"score": 0.003150612723641925, "phrase": "massively_parallel_capabilities"}, {"score": 0.003105979498529766, "phrase": "large_network_measurement_datasets"}, {"score": 0.003051073364818384, "phrase": "network_research_community"}, {"score": 0.0030293828200315797, "phrase": "caida."}, {"score": 0.0029971349193857093, "phrase": "internet_routes"}, {"score": 0.0029441472095672397, "phrase": "potential_tivs"}, {"score": 0.002881793492919646, "phrase": "large_scale_analysis"}, {"score": 0.0027807832748641195, "phrase": "efficient_parallel_solution"}, {"score": 0.002589245004668921, "phrase": "gpgpu"}, {"score": 0.0025073851194697397, "phrase": "desktop_environments"}, {"score": 0.0024895500851796603, "phrase": "readily_available_software"}, {"score": 0.002445514186654732, "phrase": "parallel_solutions"}, {"score": 0.0024281181691350085, "phrase": "high_improvements"}, {"score": 0.002342972434255588, "phrase": "serial_methodologies"}, {"score": 0.0022527488455744656, "phrase": "parallel_programming"}, {"score": 0.00223672097810462, "phrase": "readily_available_hardware"}, {"score": 0.002212892265364764, "phrase": "large_amount"}, {"score": 0.002127667143545464, "phrase": "networking_research_community"}, {"score": 0.0021049977753042253, "phrase": "future_scalable_internet_routing_overlays"}], "paper_keywords": ["Internet topology", " Internet exchange points", " GPGPU", " CUDA", " Data analysis", " Parallel computation"], "paper_abstract": "The creation of a routing overlay network on the Internet requires the identification of shorter detour paths between end hosts in comparison to the default path available. These detour paths are typically the edges forming a Triangle Inequality Violation (TIV), an artifact of the Internet delay space where the sum of latencies across an intermediate hop is lesser than the direct latency between the pair of end hosts. These violations are caused mainly due to interdomain routing policies between Autonomous Systems (ASes) and AS peering through Internet eXchange Points (IXPs). Identifying detours for a global overlay network requires large amounts of computational capabilities due to the sheer number of possible paths linking source and destination ASes. In this work, we use parallel programming paradigms to exploit the massively parallel capabilities of analyzing the large network measurement datasets made available to the network research community by CAIDA. We study Internet routes traversing IXPs and measure potential TIVs created by these paths. Large scale analysis of the dataset is carried out by implementing an efficient parallel solution on the CPU and then the general purpose graphics processor unit (GPGPU) as well. Both multicore CPU and GPGPU implementations can be carried out with ease on desktop environments with readily available software. We find both parallel solutions yield high improvements in speedup (2-35x) in comparison to the serial methodologies thereby opening up the possibility of harnessing the power of parallel programming with readily available hardware. The large amount of data analyzed and studied helps draw various inferences for the networking research community in building future scalable Internet routing overlays with greater routing efficiencies.", "paper_title": "Analysis of large scale traceroute datasets in Internet routing overlays by parallel computation", "paper_id": "WOS:000310428700017"}