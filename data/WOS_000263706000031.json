{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "knowledge-based_authentication"}, {"score": 0.004089725258435817, "phrase": "wrapper_method"}, {"score": 0.003926105480202798, "phrase": "learning_machine"}, {"score": 0.003807684498435389, "phrase": "generative_probabilistic_model"}, {"score": 0.0034733518170738517, "phrase": "kullback-leibler_divergence"}, {"score": 0.0033685408148574876, "phrase": "true_empirical_distribution"}, {"score": 0.0032336801561165113, "phrase": "legitimate_knowledge"}, {"score": 0.0031360786664165093, "phrase": "approximating_distribution"}, {"score": 0.0030414140732789186, "phrase": "attacking_strategy"}, {"score": 0.002690406375639144, "phrase": "optimization_problem"}, {"score": 0.0024289743270061157, "phrase": "maximum_entropy"}, {"score": 0.002284444272376043, "phrase": "proposed_adaptive_methods"}, {"score": 0.0021705796895687864, "phrase": "commonly_used_random_selection_method"}], "paper_keywords": ["Feature selection", " Maximum entropy", " Probabilistic model", " Metrics", " Security", " Knowledge-based authentication"], "paper_abstract": "Feature selection is critical to knowledge-based authentication. In this paper, we adopt a wrapper method in which the learning machine is a generative probabilistic model, and the objective is to maximize the Kullback-Leibler divergence between the true empirical distribution defined by the legitimate knowledge and the approximating distribution representing an attacking strategy, both in the same feature space, The closed-form solutions to this optimization problem lead to three adaptive algorithms, unified under the principle of maximum entropy. Our experimental results show that the proposed adaptive methods are superior to the commonly used random selection method. (c) 2008 Elsevier B.V. All rights reserved.", "paper_title": "A maximum entropy approach to feature selection in knowledge-based authentication", "paper_id": "WOS:000263706000031"}