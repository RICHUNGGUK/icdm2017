{"auto_keywords": [{"score": 0.03773755728379266, "phrase": "concurrent_execution"}, {"score": 0.015583657937346096, "phrase": "elastic_kernels"}, {"score": 0.010665878854363847, "phrase": "two-program_workloads"}, {"score": 0.00481495049065317, "phrase": "gpgpu_concurrency"}, {"score": 0.004710306419574744, "phrase": "new_generation"}, {"score": 0.004527619017250708, "phrase": "gpgpu_programs"}, {"score": 0.004487989502479992, "phrase": "gpu_programming_models"}, {"score": 0.004429233231659658, "phrase": "cuda"}, {"score": 0.004128317208406523, "phrase": "cuda_programs"}, {"score": 0.003985602342418578, "phrase": "available_resources"}, {"score": 0.003633927813203731, "phrase": "current_gpus"}, {"score": 0.0033572130837348623, "phrase": "gpu_kernels"}, {"score": 0.0033277936960179892, "phrase": "multiprogram_workloads"}, {"score": 0.003298631258267805, "phrase": "current_nvidia_fermi_gpus"}, {"score": 0.00308788133472785, "phrase": "serialized_execution"}, {"score": 0.0029548937818509656, "phrase": "resource_allocation"}, {"score": 0.002890557197908352, "phrase": "major_serialization_bottleneck"}, {"score": 0.002802825862847951, "phrase": "cuda_kernels"}, {"score": 0.0027417911324611917, "phrase": "fine-grained_control"}, {"score": 0.0025328377393370642, "phrase": "current_cuda_policy"}, {"score": 0.0024667783849055634, "phrase": "real_hardware"}, {"score": 0.0024451427672857458, "phrase": "multiprogrammed_workloads"}, {"score": 0.0023090525096074264, "phrase": "system_throughput"}, {"score": 0.0022192891418394535, "phrase": "average_normalized_turnaround_time"}, {"score": 0.0021049977753042253, "phrase": "current_cuda_concurrency_implementation"}], "paper_keywords": ["GPGPU", " CUDA", " Concurrent Kernels"], "paper_abstract": "Each new generation of GPUs vastly increases the resources available to GPGPU programs. GPU programming models (like CUDA) were designed to scale to use these resources. However, we find that CUDA programs actually do not scale to utilize all available resources, with over 30% of resources going unused on average for programs of the Parboil2 suite that we used in our work. Current GPUs therefore allow concurrent execution of kernels to improve utilization. In this work, we study concurrent execution of GPU kernels using multiprogram workloads on current NVIDIA Fermi GPUs. On two-program workloads from the Parboil2 benchmark suite we find concurrent execution is often no better than serialized execution. We identify that the lack of control over resource allocation to kernels is a major serialization bottleneck. We propose transformations that convert CUDA kernels into elastic kernels which permit fine-grained control over their resource usage. We then propose several elastic-kernel aware concurrency policies that offer significantly better performance and concurrency compared to the current CUDA policy. We evaluate our proposals on real hardware using multiprogrammed workloads constructed from benchmarks in the Parboil 2 suite. On average, our proposals increase system throughput (STP) by 1.21x and improve the average normalized turnaround time (ANTT) by 3.73x for two-program workloads when compared to the current CUDA concurrency implementation.", "paper_title": "Improving GPGPU Concurrency with Elastic Kernels", "paper_id": "WOS:000321213100033"}