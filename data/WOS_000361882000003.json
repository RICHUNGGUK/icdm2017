{"auto_keywords": [{"score": 0.040068893161497504, "phrase": "relevance_judgements"}, {"score": 0.00481495049065317, "phrase": "information_retrieval_systems"}, {"score": 0.0047409388179452255, "phrase": "dominant_approach"}, {"score": 0.004596295252559861, "phrase": "information_retrieval"}, {"score": 0.004410248125081398, "phrase": "reusable_test_collections"}, {"score": 0.004320055598195391, "phrase": "cranfield_paradigm"}, {"score": 0.004123781267600542, "phrase": "new_ir_evaluation_methodology"}, {"score": 0.004060350364088702, "phrase": "pooled_test-collections"}, {"score": 0.0039772848945723435, "phrase": "continuous_use"}, {"score": 0.0038758293658651237, "phrase": "professional_editors"}, {"score": 0.0036996627006622975, "phrase": "static_collection"}, {"score": 0.0036427316134070007, "phrase": "finite_set"}, {"score": 0.003459191419478298, "phrase": "ir_evaluation_paradigm"}, {"score": 0.0034236045013591437, "phrase": "retrieval_approaches"}, {"score": 0.003267925215811654, "phrase": "new_retrieval_technique"}, {"score": 0.003071274938566466, "phrase": "overall_set"}, {"score": 0.0027837615811247963, "phrase": "relevant_documents"}, {"score": 0.002670884656785558, "phrase": "new_pooling_strategies"}, {"score": 0.0024971144603069006, "phrase": "standard_ir_metrics"}, {"score": 0.002471400498582011, "phrase": "ir_system_ranking"}, {"score": 0.0023834640793589414, "phrase": "continuous_evaluation_context"}, {"score": 0.0023467391904246834, "phrase": "continuous_and_non-continuous_evaluation_results"}, {"score": 0.002126907587540404, "phrase": "continuous_evaluation_campaign"}], "paper_keywords": ["Information retrieval evaluation", " Crowdsourcing", " Continuous evaluation", " Poolingtechniques"], "paper_abstract": "The dominant approach to evaluate the effectiveness of information retrieval (IR) systems is by means of reusable test collections built following the Cranfield paradigm. In this paper, we propose a new IR evaluation methodology based on pooled test-collections and on the continuous use of either crowdsourcing or professional editors to obtain relevance judgements. Instead of building a static collection for a finite set of systems known a priori, we propose an IR evaluation paradigm where retrieval approaches are evaluated iteratively on the same collection. Each new retrieval technique takes care of obtaining its missing relevance judgements and hence contributes to augmenting the overall set of relevance judgements of the collection. We also propose two metrics: Fairness Score, and opportunistic number of relevant documents, which we then use to define new pooling strategies. The goal of this work is to study the behavior of standard IR metrics, IR system ranking, and of several pooling techniques in a continuous evaluation context by comparing continuous and non-continuous evaluation results on classic test collections. We both use standard and crowdsourced relevance judgements, and we actually run a continuous evaluation campaign over several existing IR systems.", "paper_title": "Pooling-based continuous evaluation of information retrieval systems", "paper_id": "WOS:000361882000003"}