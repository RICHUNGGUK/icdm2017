{"auto_keywords": [{"score": 0.049643973872019624, "phrase": "gnss"}, {"score": 0.00481495049065317, "phrase": "visually_impaired_combining_object_localization"}, {"score": 0.004706305193222972, "phrase": "spatial_audio"}, {"score": 0.004642292792558686, "phrase": "complex_routes"}, {"score": 0.0042178318724732005, "phrase": "artificial_vision"}, {"score": 0.004066498558415824, "phrase": "personal_autonomy"}, {"score": 0.0040111533608228195, "phrase": "virtual_augmented_reality_system"}, {"score": 0.0039027036493682887, "phrase": "adapted_geographic_information_system"}, {"score": 0.0037454814403672697, "phrase": "route_selection"}, {"score": 0.0035781807623238905, "phrase": "important_geolocated_objects"}, {"score": 0.0034813964296826973, "phrase": "real-time_embedded_vision_algorithms"}, {"score": 0.003265591844292821, "phrase": "sensorimotor_actions"}, {"score": 0.003021392767766593, "phrase": "spatialized_semantic_audio_rendering"}, {"score": 0.0029396242707327986, "phrase": "head-centered_reference_frame"}, {"score": 0.0028600623410146796, "phrase": "overall_project_design"}, {"score": 0.0027954035431845344, "phrase": "navig_system"}, {"score": 0.0026949679939994226, "phrase": "new_type"}, {"score": 0.0026461080202599694, "phrase": "localization_device"}, {"score": 0.002551022760887132, "phrase": "bio-inspired_vision_system"}, {"score": 0.002244277924422011, "phrase": "recognized_object"}, {"score": 0.0021537217025910356, "phrase": "guidance_directives"}, {"score": 0.0021243539405631866, "phrase": "participative_design"}, {"score": 0.0021049977753042253, "phrase": "potential_users"}], "paper_keywords": ["Assisted navigation", " Guidance", " Spatial audio", " Visually impaired assistive device", " Need analysis"], "paper_abstract": "Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.", "paper_title": "NAVIG: augmented reality guidance system for the visually impaired Combining object localization, GNSS, and spatial audio", "paper_id": "WOS:000310233000001"}