{"auto_keywords": [{"score": 0.03951537737195524, "phrase": "active_imitation"}, {"score": 0.00481495049065317, "phrase": "iid_learning"}, {"score": 0.004738078275822989, "phrase": "standard_passive_imitation_learning"}, {"score": 0.004371660146981191, "phrase": "target_policy"}, {"score": 0.004278807087784626, "phrase": "full_execution_trajectories"}, {"score": 0.004055176557424983, "phrase": "substantial_expert_effort"}, {"score": 0.003470358620464986, "phrase": "desired_action"}, {"score": 0.0034332736827447654, "phrase": "individual_states"}, {"score": 0.0032189132350239013, "phrase": "learner's_interactions"}, {"score": 0.003167441161387607, "phrase": "environment_simulator"}, {"score": 0.0030834712590249863, "phrase": "new_approach"}, {"score": 0.002969628818856786, "phrase": "active_i.i.d"}, {"score": 0.002652639568848117, "phrase": "non-stationary_and_stationary_policies"}, {"score": 0.0025822818714499795, "phrase": "first_time"}, {"score": 0.0025409639091476363, "phrase": "label_complexity"}, {"score": 0.002447101794397243, "phrase": "active_imitation_learning"}, {"score": 0.0023566987138026285, "phrase": "passive_learning"}, {"score": 0.0022696277881969896, "phrase": "practical_algorithm"}, {"score": 0.0021049977753042253, "phrase": "five_test_domains"}], "paper_keywords": ["imitation learning", " active learning", " active imitation learning", " reductions"], "paper_abstract": "In standard passive imitation learning, the goal is to learn a policy that performs as well as a target policy by passively observing full execution trajectories of it. Unfortunately, generating such trajectories can require substantial expert effort and be impractical in some cases. In this paper, we consider active imitation learning with the goal of reducing this effort by querying the expert about the desired action at individual states, which are selected based on answers to past queries and the learner's interactions with an environment simulator. We introduce a new approach based on reducing active imitation learning to active i.i.d. learning, which can leverage progress in the i.i.d. setting. Our first contribution is to analyze reductions for both non-stationary and stationary policies, showing for the first time that the label complexity (number of queries) of active imitation learning can be less than that of passive learning. Our second contribution is to introduce a practical algorithm inspired by the reductions, which is shown to be highly effective in five test domains compared to a number of alternatives.", "paper_title": "Active Imitation Learning: Formal and Practical Reductions to IID Learning", "paper_id": "WOS:000354999700004"}