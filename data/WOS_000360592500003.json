{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "visual_saliency"}, {"score": 0.04836165556138413, "phrase": "hamming_distance"}, {"score": 0.00474946741204285, "phrase": "local_sparse_representation"}, {"score": 0.004496280772084575, "phrase": "important_domain"}, {"score": 0.004414907432703781, "phrase": "computer_vision"}, {"score": 0.004335000368745273, "phrase": "significant_role"}, {"score": 0.00429558848734449, "phrase": "attention_mechanisms"}, {"score": 0.004256533389293113, "phrase": "neural_processing"}, {"score": 0.0042178318724732005, "phrase": "visual_information"}, {"score": 0.004103817210338686, "phrase": "new_approach"}, {"score": 0.0040111533608228195, "phrase": "image_representations"}, {"score": 0.003974673997871831, "phrase": "salient_locations"}, {"score": 0.0038849148160362257, "phrase": "saliency_maps"}, {"score": 0.003832031470970347, "phrase": "developed_method"}, {"score": 0.0037454814403672697, "phrase": "efficient_comparison_scheme"}, {"score": 0.0036944890904337833, "phrase": "local_sparse_representations"}, {"score": 0.003611034322786669, "phrase": "overlapping_image_patches"}, {"score": 0.003561865840737996, "phrase": "sparse_coding_stage"}, {"score": 0.0034813964296826973, "phrase": "overcomplete_dictionary"}, {"score": 0.003418327319110734, "phrase": "soft-competitive_bio-inspired_algorithm"}, {"score": 0.0032805541677236325, "phrase": "resulting_local_sparse_codes"}, {"score": 0.0030771609361092164, "phrase": "calculated_distances"}, {"score": 0.002993887707344862, "phrase": "saliency_strength"}, {"score": 0.002953097497434808, "phrase": "individual_patch"}, {"score": 0.0028731719610706214, "phrase": "saliency_values"}, {"score": 0.0027573101010593863, "phrase": "final_map"}, {"score": 0.0026220103561723066, "phrase": "competitive_performance"}, {"score": 0.0025862736445290088, "phrase": "proposed_approach"}, {"score": 0.0023386327847292805, "phrase": "visual_conditions"}, {"score": 0.002254571365634257, "phrase": "ideal_solution"}, {"score": 0.0022238315004700607, "phrase": "hardware_implementation"}, {"score": 0.002193509835371438, "phrase": "frontend_saliency_modeling_module"}, {"score": 0.002163600705898299, "phrase": "computer_vision_system"}, {"score": 0.0021049977753042253, "phrase": "elsevier_inc."}], "paper_keywords": ["Visual saliency", " Attention modeling", " Sparse representation", " Hamming distance"], "paper_abstract": "Modeling of visual saliency is an important domain of research in computer vision, given the significant role of attention mechanisms during neural processing of visual information. This work presents a new approach for the construction of image representations of salient locations, generally known as saliency maps. The developed method is based on an efficient comparison scheme for the local sparse representations deriving from non-overlapping image patches. The sparse coding stage is implemented via an overcomplete dictionary trained with a soft-competitive bio-inspired algorithm and the use of natural images. The resulting local sparse codes are pairwise compared using the Hamming distance as a gauge of their co-activation. The calculated distances are used to quantify the saliency strength for each individual patch, and then, the saliency values are non-linearly filtered to form the final map. The evaluation results obtained on four image databases, demonstrate the competitive performance of the proposed approach compared to several state-of-the-art saliency modeling algorithms. More importantly, the proposed scheme is simple, efficient, and robust under a variety of visual conditions. Thus, it appears as an ideal solution for a hardware implementation of a frontend saliency modeling module in a computer vision system. (C) 2015 Elsevier Inc. All rights reserved.", "paper_title": "Efficient modeling of visual saliency based on local sparse representation and the use of hamming distance", "paper_id": "WOS:000360592500003"}