{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "dai-liao_conjugate_gradient_methods"}, {"score": 0.004550029253278477, "phrase": "eigenvalue_study"}, {"score": 0.004398094237005956, "phrase": "descent_class"}, {"score": 0.004062937864702115, "phrase": "interesting_feature"}, {"score": 0.003927203683176278, "phrase": "proposed_class"}, {"score": 0.0037109446302945903, "phrase": "efficient_nonlinear_conjugate_gradient_methods"}, {"score": 0.0035869322486559623, "phrase": "hager"}, {"score": 0.003506845396435114, "phrase": "zhang"}, {"score": 0.0033893612554587327, "phrase": "dai"}, {"score": 0.003313386653428454, "phrase": "kou"}, {"score": 0.0032026066791743866, "phrase": "special_cases"}, {"score": 0.002859345731792013, "phrase": "suggested_class"}, {"score": 0.0027017308406240563, "phrase": "uniformly_convex_objective_functions"}, {"score": 0.0023313149689465386, "phrase": "proposed_methods"}, {"score": 0.0021778895748464024, "phrase": "performance_profile"}, {"score": 0.002105008120522126, "phrase": "dolan"}], "paper_keywords": ["unconstrained optimization", " large-scale optimization", " conjugate gradient algorithm", " descent condition", " global convergence"], "paper_abstract": "Based on an eigenvalue study, a descent class of Dai-Liao conjugate gradient methods is proposed. An interesting feature of the proposed class is its inclusion of the efficient nonlinear conjugate gradient methods proposed by Hager and Zhang, and Dai and Kou, as special cases. It is shown that the methods of the suggested class are globally convergent for uniformly convex objective functions. Numerical results are reported, they demonstrate the efficiency of the proposed methods in the sense of the performance profile introduced by Dolan and More.", "paper_title": "A descent family of Dai-Liao conjugate gradient methods", "paper_id": "WOS:000328027000009"}