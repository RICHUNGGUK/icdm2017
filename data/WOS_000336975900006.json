{"auto_keywords": [{"score": 0.04971075098097412, "phrase": "mapreduce"}, {"score": 0.015719485609185666, "phrase": "massive_probabilistic_data"}, {"score": 0.008352749593119086, "phrase": "set_similarity"}, {"score": 0.0067848918657710785, "phrase": "hadoop_join"}, {"score": 0.004439941010250576, "phrase": "effective_approach"}, {"score": 0.0042552618754756934, "phrase": "popular_paradigm"}, {"score": 0.004190019394361629, "phrase": "large_volume_data"}, {"score": 0.0038485929962942776, "phrase": "map_side_pruning"}, {"score": 0.0037749456215035856, "phrase": "reduce_side_pruning"}, {"score": 0.003702702337768558, "phrase": "side_pruning"}, {"score": 0.0036178264212372497, "phrase": "existence_probability"}, {"score": 0.003548579306515579, "phrase": "probabilistic_sets"}, {"score": 0.0034941337957136013, "phrase": "map_task_side"}, {"score": 0.003310047704590791, "phrase": "reduce_side"}, {"score": 0.0032718761046588835, "phrase": "probability_sum"}, {"score": 0.0030994630902300133, "phrase": "candidate_pairs"}, {"score": 0.003075583447856341, "phrase": "reduce_task_side"}, {"score": 0.003005039427642664, "phrase": "comparison_cost"}, {"score": 0.002891032998395276, "phrase": "hybrid_solution"}, {"score": 0.0028356575042385156, "phrase": "reduce-side_pruning_methods"}, {"score": 0.0027175263159330523, "phrase": "comprehensive_experiments"}, {"score": 0.0026347074232350503, "phrase": "speedup_ratio"}, {"score": 0.0025942468561537682, "phrase": "naive_method"}, {"score": 0.00257424957319189, "phrase": "block_nested"}, {"score": 0.0025249264547573943, "phrase": "experiment_results"}, {"score": 0.0024385083712241988, "phrase": "block_nested_loop_join"}, {"score": 0.002401053532270901, "phrase": "good_scalability"}, {"score": 0.00230098581403254, "phrase": "first_work"}, {"score": 0.0022222152430170254, "phrase": "massive_probabilistic_data_problem"}, {"score": 0.0021049977753042253, "phrase": "new_way"}], "paper_keywords": ["Set similarity join", " MapReduce", " Probabilistic data"], "paper_abstract": "In this paper, we focus on set similarity join on massive probabilistic data using MapReduce, there is no effective approach that can process this problem efficiently. MapReduce is a popular paradigm that can process large volume data more efficiently, in this paper, we proposed two approaches using MapReduce to deal with this task: Hadoop Join by Map Side Pruning and Hadoop Join by Reduce Side Pruning. Hadoop Join by Map Side Pruning uses the sum of the existence probability to filter out the probabilistic sets directly at the Map task side which have no any chance to be similar with any other probabilistic set. Hadoop Join by Reduce Side Pruning uses probability sum based pruning principle and probability upper bound based pruning principle to reduce the candidate pairs at Reduce task side, it can save the comparison cost. Based on the above approaches, we proposed a hybrid solution that employs both Map-side and Reduce-side pruning methods. Finally we implemented the above approaches on Hadoop-0.20.2 and performed comprehensive experiments to their performance, we also test the speedup ratio compared with the naive method: Block Nested Loop Join. The experiment results show that our approaches have much better performance than that of Block Nested Loop Join and also have good scalability. To the best of our knowledge, this is the first work to try to deal with set similarity join on massive probabilistic data problem using MapReduce paradigm, and the approaches proposed in this paper provide a new way to process the massive probabilistic data.", "paper_title": "Set similarity join on massive probabilistic data using MapReduce", "paper_id": "WOS:000336975900006"}