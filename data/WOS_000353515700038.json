{"auto_keywords": [{"score": 0.05007852962010532, "phrase": "discrete_distributions"}, {"score": 0.004754324449031752, "phrase": "general_methodology"}, {"score": 0.004670722261414736, "phrase": "essentially_minimax_estimators"}, {"score": 0.0046353421974267475, "phrase": "wide_class"}, {"score": 0.004588583394515502, "phrase": "finite_dimensional_parameters"}, {"score": 0.004451105494353582, "phrase": "support_size"}, {"score": 0.0042096265108205804, "phrase": "respective_regions"}, {"score": 0.004083455907659561, "phrase": "nonsmooth_regime"}, {"score": 0.004032002950217453, "phrase": "unbiased_estimator"}, {"score": 0.004001441644488586, "phrase": "best_polynomial_approximation"}, {"score": 0.003971111060168551, "phrase": "functional_whereas"}, {"score": 0.0039310261964323545, "phrase": "smooth_regime"}, {"score": 0.0038814864204867855, "phrase": "bias-corrected_version"}, {"score": 0.0038520615699124123, "phrase": "maximum_likelihood"}, {"score": 0.0036614948691735105, "phrase": "resulting_schemes"}, {"score": 0.0032415934630668484, "phrase": "optimal_sample_complexity"}, {"score": 0.0031926001871260524, "phrase": "entropy_estimation"}, {"score": 0.0031363735212808857, "phrase": "sample_complexity"}, {"score": 0.00292860449584936, "phrase": "mle."}, {"score": 0.002769483748975581, "phrase": "infinite_support_size"}, {"score": 0.0027207868759773165, "phrase": "mle"}, {"score": 0.002632332114727894, "phrase": "minimax_rate-optimal_estimators"}, {"score": 0.002508318655705876, "phrase": "n_samples"}, {"score": 0.002414544068560716, "phrase": "practical_advantages"}, {"score": 0.0023599666007519788, "phrase": "mutual_information"}, {"score": 0.002283298119790413, "phrase": "running_time"}, {"score": 0.002214734873559146, "phrase": "minimax_rate-optimal_mutual_information_estimator"}, {"score": 0.002148226022654619, "phrase": "chow-liu_algorithm"}, {"score": 0.002131912585982578, "phrase": "graphical_models"}, {"score": 0.0021157227694266513, "phrase": "wide_use"}, {"score": 0.0021049977753042253, "phrase": "information_measure_estimation"}], "paper_keywords": ["Mean squared error", " entropy estimation", " nonsmooth functional estimation", " maximum likelihood estimator", " approximation theory", " minimax lower bound", " polynomial approximation", " minimax-optimality", " high dimensional statistics", " Renyi entropy", " Chow-Liu algorithm"], "paper_abstract": "We propose a general methodology for the construction and analysis of essentially minimax estimators for a wide class of functionals of finite dimensional parameters, and elaborate on the case of discrete distributions, where the support size S is unknown and may be comparable with or even much larger than the number of observations n. We treat the respective regions where the functional is nonsmooth and smooth separately. In the nonsmooth regime, we apply an unbiased estimator for the best polynomial approximation of the functional whereas, in the smooth regime, we apply a bias-corrected version of the maximum likelihood estimator (MLE). We illustrate the merit of this approach by thoroughly analyzing the performance of the resulting schemes for estimating two important information measures: 1) the entropy H(P) = Sigma(S)(i=1)-p(i) ln p(i) and 2) F-alpha(P) = Sigma(S)(i=1) p(i)(alpha), alpha > 0. We obtain the minimax L-2 rates for estimating these functionals. In particular, we demonstrate that our estimator achieves the optimal sample complexity n similar to S/ln S for entropy estimation. We also demonstrate that the sample complexity for estimating F-alpha(P), 0 < alpha < 1, is n similar to S-1/alpha/ln S, which can be achieved by our estimator but not the MLE. For 1 < alpha < 3/2, we show the minimax L-2 rate for estimating F-alpha(P) is (n ln n)(-2(alpha-1)) for infinite support size, while the maximum L-2 rate for the MLE is n(-2(alpha-1)). For all the above cases, the behavior of the minimax rate-optimal estimators with n samples is essentially that of the MLE (plug-in rule) with n ln n samples, which we term \"effective sample size enlargement.\" We highlight the practical advantages of our schemes for the estimation of entropy and mutual information. We compare our performance with various existing approaches, and demonstrate that our approach reduces running time and boosts the accuracy. Moreover, we show that the minimax rate-optimal mutual information estimator yielded by our framework leads to significant performance boosts over the Chow-Liu algorithm in learning graphical models. The wide use of information measure estimation suggests that the insights and estimators obtained in this paper could be broadly applicable.", "paper_title": "Minimax Estimation of Functionals of Discrete Distributions", "paper_id": "WOS:000353515700038"}