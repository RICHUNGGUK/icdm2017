{"auto_keywords": [{"score": 0.04448671687401457, "phrase": "design_matrix"}, {"score": 0.015719716506582538, "phrase": "lossy_compression"}, {"score": 0.014464336175272765, "phrase": "sparse_regression_code"}, {"score": 0.011945748912351065, "phrase": "optimal_distortion-rate_function"}, {"score": 0.004767546015485477, "phrase": "sparse_linear_regression"}, {"score": 0.004720606036240263, "phrase": "computationally_efficient_encoding"}, {"score": 0.004674475148121863, "phrase": "decoding"}, {"score": 0.004582528208024817, "phrase": "computationally_efficient_encoders"}, {"score": 0.004150661689086371, "phrase": "linear_combinations"}, {"score": 0.003989487818395848, "phrase": "proposed_encoding_algorithm_sequentially"}, {"score": 0.0037967593942595233, "phrase": "source_sequence"}, {"score": 0.0034386894664467003, "phrase": "squared-error_distortion_criterion"}, {"score": 0.003192406091820197, "phrase": "tradeoff_distortion_performance"}, {"score": 0.0031609252620168446, "phrase": "encoding_complexity"}, {"score": 0.002993231973906559, "phrase": "block_length"}, {"score": 0.0028911669203979156, "phrase": "computational_resource"}, {"score": 0.002657511557782469, "phrase": "fixed_distortion-level"}, {"score": 0.002618278365749875, "phrase": "gaussian_distortion-rate_function"}, {"score": 0.0025541693207036167, "phrase": "excess_distortion"}, {"score": 0.0023710856920150574, "phrase": "ergodic_source"}, {"score": 0.002336071138145897, "phrase": "proposed_encoder"}, {"score": 0.0022675820982741347, "phrase": "i.i.d_gaussian_source"}, {"score": 0.002147180332779021, "phrase": "good_empirical_performance"}, {"score": 0.0021049977753042253, "phrase": "low_and_moderate_rates"}], "paper_keywords": ["Lossy compression", " computationally efficient encoding", " squared error distortion", " Gaussian rate-distortion", " sparse regression", " compressed sensing"], "paper_abstract": "We propose computationally efficient encoders and decoders for lossy compression using a sparse regression code. The codebook is defined by a design matrix and codewords are structured linear combinations of columns of this matrix. The proposed encoding algorithm sequentially chooses columns of the design matrix to successively approximate the source sequence. It is shown to achieve the optimal distortion-rate function for independent identically distributed (i.i.d) Gaussian sources under the squared-error distortion criterion. For a given rate, the parameters of the design matrix can be varied to tradeoff distortion performance with encoding complexity. An example of such a tradeoff as a function of the block length n is the following. With computational resource (space or time) per source sample of O((n/log n)(2)), for a fixed distortion-level above the Gaussian distortion-rate function, the probability of excess distortion decays exponentially in n. The sparse regression code is robust in the following sense: for any ergodic source, the proposed encoder achieves the optimal distortion-rate function of an i.i.d Gaussian source with the same variance. Simulations show that the encoder has good empirical performance, especially at low and moderate rates.", "paper_title": "Lossy Compression via Sparse Linear Regression: Computationally Efficient Encoding and Decoding", "paper_id": "WOS:000341980300015"}