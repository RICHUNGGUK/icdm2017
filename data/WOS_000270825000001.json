{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "artificial_variables"}, {"score": 0.004554082077300675, "phrase": "predictive_models"}, {"score": 0.004228016636379321, "phrase": "redundant_subset"}, {"score": 0.003888905257972882, "phrase": "modern_data_sets"}, {"score": 0.003446353878654566, "phrase": "interactive_effects"}, {"score": 0.0033515736929172644, "phrase": "complex_models"}, {"score": 0.0029699874949715367, "phrase": "feature_selection_methods"}, {"score": 0.002731500545019039, "phrase": "tree-based_ensembles"}, {"score": 0.0026317312518257803, "phrase": "compact_subset"}, {"score": 0.0025832180231667853, "phrase": "non-redundant_features"}, {"score": 0.0025355968118803956, "phrase": "parallel_and_serial_ensembles"}, {"score": 0.002375714262344562, "phrase": "mixed_method"}, {"score": 0.0021848422732893926, "phrase": "secondary_effect"}, {"score": 0.0021049977753042253, "phrase": "actual_examples"}], "paper_keywords": ["trees", " resampling", " importance", " masking", " residuals"], "paper_abstract": "Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for filters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach.", "paper_title": "Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination", "paper_id": "WOS:000270825000001"}