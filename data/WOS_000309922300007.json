{"auto_keywords": [{"score": 0.03970543330650032, "phrase": "candidate_text_regions"}, {"score": 0.03727104775004972, "phrase": "clutter_background"}, {"score": 0.00481495049065317, "phrase": "using_visual"}, {"score": 0.004578382275810468, "phrase": "nature_scene"}, {"score": 0.0044644734709113985, "phrase": "better_understanding"}, {"score": 0.004419703921314433, "phrase": "semantic_meaning"}, {"score": 0.004245051445971095, "phrase": "important_role"}, {"score": 0.004139401327448807, "phrase": "image_retrieval"}, {"score": 0.0040978781692373005, "phrase": "image_categorization"}, {"score": 0.004056769841308405, "phrase": "social_media_processing"}, {"score": 0.003975781274672803, "phrase": "traditional_approach"}, {"score": 0.0038964032225585117, "phrase": "low_level_image"}, {"score": 0.003487222504700868, "phrase": "adopted_low_level_image_features"}, {"score": 0.003332440548684892, "phrase": "text_region"}, {"score": 0.0032006150973842846, "phrase": "recent_popular_research"}, {"score": 0.0031684791285366315, "phrase": "attention_model"}, {"score": 0.0031366648074858555, "phrase": "salience_detection"}, {"score": 0.002952356518921907, "phrase": "text_detection"}, {"score": 0.002922705896608178, "phrase": "nature_scene_image"}, {"score": 0.0028933521924166287, "phrase": "saliency_map"}, {"score": 0.0027370835853738626, "phrase": "adjusted_saliency_map"}, {"score": 0.0026420518944303716, "phrase": "common_low_level_features"}, {"score": 0.0025503112711036994, "phrase": "efficient_low_level_text_feature"}, {"score": 0.002524688179363081, "phrase": "histogram_of_edge-direction"}, {"score": 0.002499322250987408, "phrase": "hoe"}, {"score": 0.0023405257495466352, "phrase": "edge_direction_information"}, {"score": 0.0022252591643049744, "phrase": "encouraging_experimental_results"}, {"score": 0.002158836285985314, "phrase": "nature_scene_images"}], "paper_keywords": ["Text location", " visual attention", " histogram of edge direction", " connected component analysis", " edge map"], "paper_abstract": "Locating text region from an image of nature scene is significantly helpful for better understanding the semantic meaning of the image, which plays an important role in many applications such as image retrieval, image categorization, social media processing, etc. Traditional approach relies on the low level image features to progressively locate the candidate text regions. However, these approaches often suffer for the cases of the clutter background since the adopted low level image features are fairly simple which may not reliably distinguish text region from the clutter background. Motivated by the recent popular research on attention model, salience detection is revisited in this paper. Based on the case of text detection on nature scene image, saliency map is further analyzed and is adjusted accordingly. Using the adjusted saliency map, the candidate text regions detected by the common low level features are further verified. Moreover, efficient low level text feature, Histogram of Edge-direction (HOE), is adopted in this paper, which statistically describes the edge direction information of the region of interest on the image. Encouraging experimental results have been obtained on the nature scene images with the text of various languages.", "paper_title": "TEXT LOCATION IN SCENE IMAGES USING VISUAL ATTENTION MODEL", "paper_id": "WOS:000309922300007"}