{"auto_keywords": [{"score": 0.02816365002013277, "phrase": "repeated_coalition_formation"}, {"score": 0.00481495049065317, "phrase": "optimal_repeated_coalition_formation"}, {"score": 0.004734853555630616, "phrase": "coalition_formation"}, {"score": 0.004675652357253327, "phrase": "central_problem"}, {"score": 0.004636594774238505, "phrase": "multiagent_systems_research"}, {"score": 0.004540365779658683, "phrase": "common_knowledge"}, {"score": 0.004502433187097914, "phrase": "agent_types"}, {"score": 0.00403709306806495, "phrase": "repeated_interaction"}, {"score": 0.00387221309328005, "phrase": "bayesian"}, {"score": 0.0031512402798625056, "phrase": "explicit_tradeoffs"}, {"score": 0.0030857431184193765, "phrase": "\"new\"_coalitions"}, {"score": 0.0029837589912490026, "phrase": "new_potential_partners"}, {"score": 0.0027317585060414253, "phrase": "optimal_exploration_policies"}, {"score": 0.00267495669519805, "phrase": "decision_making"}, {"score": 0.002630364569659529, "phrase": "type_uncertainty"}, {"score": 0.0026083473154448326, "phrase": "bayesian_reinforcement_learning_techniques"}, {"score": 0.002500987934534128, "phrase": "optimal_bayesian_solution"}, {"score": 0.0024489727828322693, "phrase": "type-learning_problem"}, {"score": 0.0024182831453515782, "phrase": "tractable_means"}, {"score": 0.0023879771786116228, "phrase": "good_sequential_performance"}, {"score": 0.0022514836269627186, "phrase": "particular_exhibits"}, {"score": 0.0022420373745572837, "phrase": "consistently_good_performance"}, {"score": 0.002122775261369663, "phrase": "knowledge_transfer"}, {"score": 0.0021049977753042253, "phrase": "different_dynamic_tasks"}], "paper_keywords": ["Coalition formation", " Multiagent learning", " Bayesian reinforcement learning"], "paper_abstract": "Coalition formation is a central problem in multiagent systems research, but most models assume common knowledge of agent types. In practice, however, agents are often unsure of the types or capabilities of their potential partners, but gain information about these capabilities through repeated interaction. In this paper, we propose a novel Bayesian, model-based reinforcement learning framework for this problem, assuming that coalitions are formed (and tasks undertaken) repeatedly. Our model allows agents to refine their beliefs about the types of others as they interact within a coalition. The model also allows agents to make explicit tradeoffs between exploration (forming \"new\" coalitions to learn more about the types of new potential partners) and exploitation (relying on partners about which more is known), using value of information to define optimal exploration policies. Our framework effectively integrates decision making during repeated coalition formation under type uncertainty with Bayesian reinforcement learning techniques. Specifically, we present several learning algorithms to approximate the optimal Bayesian solution to the repeated coalition formation and type-learning problem, providing tractable means to ensure good sequential performance. We evaluate our algorithms in a variety of settings, showing that one method in particular exhibits consistently good performance in practice. We also demonstrate the ability of our model to facilitate knowledge transfer across different dynamic tasks.", "paper_title": "Sequentially optimal repeated coalition formation under uncertainty", "paper_id": "WOS:000299514900004"}