{"auto_keywords": [{"score": 0.0324488978001811, "phrase": "genetic_algorithm"}, {"score": 0.00481495049065317, "phrase": "parameter_control_of_genetic_algorithms"}, {"score": 0.004664352260912174, "phrase": "bayesian_networks_-_a_case_study"}, {"score": 0.004400329132129292, "phrase": "evolutionary_algorithms"}, {"score": 0.0043080553835735825, "phrase": "important_issue"}, {"score": 0.004262643556463569, "phrase": "evolutionary_computation"}, {"score": 0.004064117897000019, "phrase": "parameter_tuning"}, {"score": 0.004021267064832957, "phrase": "parameter_control"}, {"score": 0.00385431710959668, "phrase": "self-adaptive_parameter_control"}, {"score": 0.0037139103792217143, "phrase": "bayesian_network_learning"}, {"score": 0.003559675269411366, "phrase": "bayesian_network"}, {"score": 0.003522124318486352, "phrase": "genetic_algorithm_parameters"}, {"score": 0.0032184697893776052, "phrase": "best_individuals"}, {"score": 0.0028944757265206332, "phrase": "respective_parameter_configuration"}, {"score": 0.002818692837561076, "phrase": "time-consuming_tasks"}, {"score": 0.0027303606813078255, "phrase": "small-sized_population"}, {"score": 0.0025892330566358503, "phrase": "promising_individuals"}, {"score": 0.002468450551616404, "phrase": "optimal_search_problem"}, {"score": 0.0024423826953133844, "phrase": "simultaneous_row"}, {"score": 0.002219790619936024, "phrase": "sharp_reduction"}, {"score": 0.002196342933346204, "phrase": "computational_time"}], "paper_keywords": [""], "paper_abstract": "Parameter setting for evolutionary algorithms is still an important issue in evolutionary computation. There are two main approaches to parameter setting: parameter tuning and parameter control. In this paper, we introduce self-adaptive parameter control of a genetic algorithm based on Bayesian network learning and simulation. The nodes of this Bayesian network are genetic algorithm parameters to be controlled. Its structure captures probabilistic conditional (in)dependence relationships between the parameters. They are learned from the best individuals, i.e., the best configurations of the genetic algorithm. Individuals are evaluated by running the genetic algorithm for the respective parameter configuration. Since all these runs are time-consuming tasks, each genetic algorithm uses a small-sized population and is stopped before convergence. In this way promising individuals should not be lost. Experiments with an optimal search problem for simultaneous row and column orderings yield the same optima as state-of-the-art methods but with a sharp reduction in computational time. Moreover, our approach can cope with as yet unsolved high-dimensional problems.", "paper_title": "Parameter Control of Genetic Algorithms by Learning and Simulation of Bayesian Networks - A Case Study for the Optimal Ordering of Tables", "paper_id": "WOS:000321566800013"}