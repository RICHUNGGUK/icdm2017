{"auto_keywords": [{"score": 0.04962656673317324, "phrase": "gentle_adaboost"}, {"score": 0.007740489563401459, "phrase": "generalization_error"}, {"score": 0.006843681458172365, "phrase": "small_margins"}, {"score": 0.005476524760732595, "phrase": "weak_hypotheses"}, {"score": 0.004701985831087475, "phrase": "margin_distribution"}, {"score": 0.004548253531664032, "phrase": "object_detection"}, {"score": 0.0045052562941109734, "phrase": "pattern_recognition"}, {"score": 0.004175533280178052, "phrase": "larger_weights"}, {"score": 0.003944112858235306, "phrase": "small-margin_instances"}, {"score": 0.0035022161192936234, "phrase": "whole_data_distribution"}, {"score": 0.003307988966112615, "phrase": "late_training_phase"}, {"score": 0.0031845067058084583, "phrase": "classifier_distortion"}, {"score": 0.002854505078275351, "phrase": "late-selected_ones"}, {"score": 0.0027218799769229596, "phrase": "new_variant"}, {"score": 0.0023936956017015696, "phrase": "weight_increase"}, {"score": 0.0023485549678832628, "phrase": "minimal_margins"}, {"score": 0.0022287497627555895, "phrase": "\"classifier_distortion"}, {"score": 0.0021352776064254195, "phrase": "far_lower_generalization_errors"}, {"score": 0.0021049977753042253, "phrase": "similar_training_speed"}], "paper_keywords": ["Gentle AdaBoost", " generalization error", " machine learning", " modest AdaBoost", " training error"], "paper_abstract": "Gentle AdaBoost is widely used in object detection and pattern recognition due to its efficiency and stability. To focus on instances with small margins, Gentle AdaBoost assigns larger weights to these instances during the training. However, misclassification of small-margin instances can still occur, which will cause the weights of these instances to become larger and larger. Eventually, several large-weight instances might dominate the whole data distribution, encouraging Gentle AdaBoost to choose weak hypotheses that fit only these instances in the late training phase. This phenomenon, known as \"classifier distortion\", degrades the generalization error and can easily lead to overfitting since the deviation of all selected weak hypotheses is increased by the late-selected ones. To solve this problem, we propose a new variant which we call \"Penalized AdaBoost\". In each iteration, our approach not only penalizes the misclassification of instances with small margins but also restrains the weight increase for instances with minimal margins. Our method performs better than Gentle AdaBoost because it avoids the \"classifier distortion\" effectively. Experiments show that our method achieves far lower generalization errors and a similar training speed compared with Gentle AdaBoost.", "paper_title": "Penalized AdaBoost: Improving the Generalization Error of Gentle AdaBoost through a Margin Distribution", "paper_id": "WOS:000364422600002"}