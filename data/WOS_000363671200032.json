{"auto_keywords": [{"score": 0.046448599314105624, "phrase": "source_frame"}, {"score": 0.00481495049065317, "phrase": "interpolation_for_video_cutout"}, {"score": 0.004731492618963217, "phrase": "jumpcut"}, {"score": 0.004529093447865156, "phrase": "interactive_video_cutout"}, {"score": 0.004373398434399966, "phrase": "foreground_mask"}, {"score": 0.0038862297330905836, "phrase": "background_and_foreground_regions"}, {"score": 0.0038355531523269217, "phrase": "different_motions"}, {"score": 0.0033636292024305406, "phrase": "coherent_labeling"}, {"score": 0.0032764307492734145, "phrase": "target_frame"}, {"score": 0.0031360786664165093, "phrase": "novel_edge"}, {"score": 0.00308163068747474, "phrase": "silhouette_edges"}, {"score": 0.0028857130140890787, "phrase": "modified_level_set_method"}, {"score": 0.0027985846558073457, "phrase": "clean_mask"}, {"score": 0.002737960968372391, "phrase": "pixel_labels"}, {"score": 0.0026321198895843173, "phrase": "resulting_mask_transfer_method"}, {"score": 0.002530359917257626, "phrase": "foreground_masks"}, {"score": 0.0024325244760735566, "phrase": "proposed_method"}, {"score": 0.002287783663138828, "phrase": "wide_variety"}, {"score": 0.0022678201262721323, "phrase": "video_sequences"}, {"score": 0.0021993066642488237, "phrase": "required_amount"}, {"score": 0.0021801135036476136, "phrase": "user_effort"}, {"score": 0.0021049977753042253, "phrase": "effective_interactive_video_object_cutout_tool"}], "paper_keywords": ["video segmentation", " foreground extraction", " object cutout"], "paper_abstract": "We introduce JumpCut, a new mask transfer and interpolation method for interactive video cutout. Given a source frame for which a foreground mask is already available, we compute an estimate of the foreground mask at another, typically non-successive, target frame. Observing that the background and foreground regions typically exhibit different motions, we leverage these differences by computing two separate nearest-neighbor fields (split-NNF) from the target to the source frame. These NNFs are then used to jointly predict a coherent labeling of the pixels in the target frame. The same split-NNF is also used to aid a novel edge classifier in detecting silhouette edges (S-edges) that separate the foreground from the background. A modified level set method is then applied to produce a clean mask, based on the pixel labels and the S-edges computed by the previous two steps. The resulting mask transfer method may also be used for coherently interpolating the foreground masks between two distant source frames. Our results demonstrate that the proposed method is significantly more accurate than the existing state-of-the-art on a wide variety of video sequences. Thus, it reduces the required amount of user effort, and provides a basis for an effective interactive video object cutout tool.", "paper_title": "JumpCut: Non-Successive Mask Transfer and Interpolation for Video Cutout", "paper_id": "WOS:000363671200032"}