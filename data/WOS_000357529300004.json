{"auto_keywords": [{"score": 0.00481495049065317, "phrase": "sharp_bounds"}, {"score": 0.004676575291629554, "phrase": "estimation_error"}, {"score": 0.004542158659382197, "phrase": "empirical_risk_minimization_procedure"}, {"score": 0.004326624385772596, "phrase": "convex_class"}, {"score": 0.004081385289516914, "phrase": "squared_loss"}, {"score": 0.0038876302202924644, "phrase": "class_members"}, {"score": 0.0035616743422949766, "phrase": "rapidly_decaying_tails"}, {"score": 0.0033271045779740683, "phrase": "concentration-based_argument"}, {"score": 0.003048001809453513, "phrase": "\"small-ball\"_assumption"}, {"score": 0.0028195727965553367, "phrase": "heavy-tailed_functions"}, {"score": 0.0027383842769094354, "phrase": "heavy-tailed_targets"}, {"score": 0.002659527314613792, "phrase": "resulting_estimates"}, {"score": 0.0025331002386232014, "phrase": "\"noise_level"}, {"score": 0.0021049977753042253, "phrase": "known_bounds"}], "paper_keywords": ["Theory", " Empirical risk minimization", " small-ball method", " empirical processes"], "paper_abstract": "We obtain sharp bounds on the estimation error of the Empirical Risk Minimization procedure, performed in a convex class and with respect to the squared loss, without assuming that class members and the target are bounded functions or have rapidly decaying tails. Rather than resorting to a concentration-based argument, the method used here relies on a \"small-ball\" assumption and thus holds for classes consisting of heavy-tailed functions and for heavy-tailed targets. The resulting estimates scale correctly with the \"noise level\" of the problem, and when applied to the classical, bounded scenario, always improve the known bounds.", "paper_title": "Learning without Concentration", "paper_id": "WOS:000357529300004"}