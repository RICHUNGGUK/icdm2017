Related Papers: 

Doc id:  WOS:000328802900013
Doc Title:  Three-dimensional solid texture analysis in biomedical imaging: Review and opportunities
Doc Abstract:  Three-dimensional computerized characterization of biomedical solid textures is key to large-scale and high-throughput screening of imaging data. Such data increasingly become available in the clinical and research environments with an ever increasing spatial resolution. In this text we exhaustively analyze the state-of-the-art in 3-D biomedical texture analysis to identify the specific needs of the application domains and extract promising trends in image processing algorithms. The geometrical properties of biomedical textures are studied both in their natural space and on digitized lattices. It is found that most of the tissue types have strong multi-scale directional properties, that are well captured by imaging protocols with high resolutions and spherical spatial transfer functions. The information modeled by the various image processing techniques is analyzed and visualized by displaying their 3-D texture primitives. We demonstrate that non-convolutional approaches are expected to provide best results when the size of structures are inferior to five voxels. For larger structures, it is shown that only multi-scale directional convolutional approaches that are non-separable allow for an unbiased modeling of 3-D biomedical textures. With the increase of high-resolution isotropic imaging protocols in clinical routine and research, these models are expected to best leverage the wealth of 3-D biomedical texture analysis in the future. Future research directions and opportunities are proposed to efficiently model personalized image-based phenotypes of normal biomedical tissue and its alterations. The integration of the clinical and genomic context is expected to better explain the intra class variation of healthy biomedical textures. Using texture synthesis, this provides the exciting opportunity to simulate and visualize texture atlases of normal ageing process and disease progression for enhanced treatment planning and clinical care management. (C) 2013 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000260713900019
Doc Title:  A data envelopment analysis method with assurance region for weight generation in the analytic hierarchy process
Doc Abstract:  Data envelopment analysis (DEA) has been combined with the analytic hierarchy process (AHP) to form a DEAHP for weight derivation and aggregation in the AHR This paper proposes a DEA model with assurance region (AR) for priority derivation in the AHP, which is referred to as the DEA/AR model. This new DEA model can overcome the shortcomings of the DEAHP Such as illogical local weights, over insensitivity to some comparisons, information loss, and overestimation of some local weights, and provide better priority estimate and better decision conclusions than the DEAHP. Numerical examples including a real application of the AHP to the recruitment of a research fellow for a research project are provided to show the advantages of the DEA/AR model and its potential applications in multiple criteria decision analysis (MCDA). (C) 2008 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000284659800010
Doc Title:  From a cellular automaton model of tumor-immune interactions to its macroscopic dynamical equation: A drift-diffusion data analysis approach
Doc Abstract:  Several models of tumor growth have been developed from various perspectives and for multiple scales. Due to the complexity of interactions, how the macroscopic dynamics formed by such interactions at the microscopic level is a difficult problem. In this paper, we focus on reconstructing a model from the output of an experimental model. This is carried out by the data analysis approach. We simulate the growth process of tumor with immune competition by using cellular automata technique adapted from previous studies. We employ an analysis of data given by the simulation output to derive an evolution equation of macroscopic dynamics of tumor growth. In a numerical example we show that the dynamics of tumor at stationary state can be described by an Ornstein-Uhlenbeck process. We show further how the result can be linked to the stochastic Gompertz model. (C) 2010 Elsevier Ltd. All rights reserved.

------------------------

Doc id:  WOS:000340597400014
Doc Title:  Comparative Blood Flow Visualization for Cerebral Aneurysm Treatment Assessment
Doc Abstract:  A pathological vessel dilation in the brain, termed cerebral aneurysm, bears a high risk of rupture, and is associated with a high mortality. In recent years, incidental findings of unruptured aneurysms have become more frequent, mainly due to advances in medical imaging. The pathological condition is often treated with a stent that diverts the blood flow from the aneurysm sac back to the original vessel. Prior to treatment, neuroradiologists need to decide on the optimal stent configuration and judge the long-term rupture risk, for which blood flow information is essential. Modern patient-specific simulations can model the hemodynamics for various stent configurations, providing important indicators to support the decision-making process. However, the necessary visual analysis of these data becomes tedious and time-consuming, because of the abundance of information. We introduce a comprehensive comparative visualization that integrates morphology with blood flow indicators to facilitate treatment assessment. To deal with the visual complexity, we propose a details-on-demand approach, combining established medical visualization techniques with innovative glyphs inspired by information visualization concepts. In an evaluation we have obtained informal feedback from domain experts, gauging the value of our visualization.

------------------------

Doc id:  WOS:000263495600007
Doc Title:  Supporting user-oriented analysis for multi-view domain-specific visual languages
Doc Abstract:  The integration Of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data. transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. (C) 2008 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000319952500014
Doc Title:  Model-based functional mixture discriminant analysis with hidden process regression for curve classification
Doc Abstract:  In this paper, we study the modeling and the classification of functional data presenting regime changes over time. We propose a new model-based functional mixture discriminant analysis approach based on a specific hidden process regression model that governs the regime changes over time. Our approach is particularly adapted to handle the problem of complex-shaped classes of curves, where each class is potentially composed of several sub-classes, and to deal with the regime changes within each homogeneous sub-class. The proposed model explicitly integrates the heterogeneity of each class of curves via a mixture model formulation, and the regime changes within each sub-class through a hidden logistic process. Each class of complex-shaped curves is modeled by a finite number of homogeneous clusters, each of them being decomposed into several regimes. The model parameters of each class are learned by maximizing the observed-data log-likelihood by using a dedicated expectation-maximization (EM) algorithm. Comparisons are performed with alternative curve classification approaches, including functional linear discriminant analysis and functional mixture discriminant analysis with polynomial regression mixtures and spline regression mixtures. Results obtained on simulated data and real data show that the proposed approach outperforms the alternative approaches in terms of discrimination, and significantly improves the curves approximation. (C) 2013 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000238899200003
Doc Title:  Analysis and modelling of a broadband fiber access network with high peer-to-peer traffic load
Doc Abstract:  This paper provides a statistical characterization of the traffic in a fiber network for voice and data transmission. This research operates with real data from more than 33,000 users. The most significant applications (ftp, http, p2p services,...) have been processed. Also, a model of the traffic generated is implemented, representing the usage that network subscribers make of the system. We demonstrate that this traffic is statistically self-similar, such behaviour having serious implications for the design, control and analysis of high-speed networks. The results in these processes have been validated using the real data provided by the fiber network operator Telecable Asturias, SAU. (c) 2005 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000320687700002
Doc Title:  Event sequence modeling of IT adoption in healthcare
Doc Abstract:  Information systems research is replete with examples of the importance of business processes defining IT adoption. Business processes are influenced by both organizational and operational concerns. We evaluate the comparative importance of operational and organizational influences for complementary IT systems. In the context of acute-care hospitals the analysis shows that an organizational approach to automating a process is related to different financial outcomes than an operational approach. Six complementary systems supporting a three-stage medication management process are studied: prescribing, dispensing, and administration. The analysis uses firm-level, panel data extracted from the HIMSS Analytics database spanning ten years of IT adoption for 140 hospitals. We have augmented the HIMSS dataset with matching demographic and financial details from the American Hospital Association and the Centers for Medicare and Medicaid Services. Using event sequence analysis we explore whether organizations are more likely to adopt organization boundary spanning systems and if the sequence of adoption follows the temporal ordering of the business process steps. The research also investigates if there is a relationship between the paths to IT adoption and financial performance. Comparison of the two measures suggests that the organizational model of adoption is observed more often in the data. Following the organizational model of adoption is associated with approximately $155 dollar increase in net income per patient day; whereas the operational model of adoption is associated with approximately $225 dollars decrease in net income per patient day. However, this effect diminishes with the adoption of each additional system thus demonstrating that the adoption path effects may only be relevant in the short-term. (C) 2012 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000294747600004
Doc Title:  Interactive analysis of discrete-event logistics systems with support of a data warehouse
Doc Abstract:  We propose an interactive approach for the flexible analysis of detailed state-transition data collected from discrete-event simulation models of logistics systems. To this end, we focus on multidimensional modeling in order to reveal the drivers of time-variant system performance. Online Analytical Processing functionality provides fast and flexible organization, aggregation and visualization of information. We illustrate the advantages of such an approach with data from simulation studies of the Upper Mississippi River waterway system. (C) 2011 Elsevier By. All rights reserved.

------------------------

Doc id:  WOS:000255423800017
Doc Title:  Forensic memory analysis: From stack and code to execution history
Doc Abstract:  Forensics memory analysis has recently gained great attention in cyber forensics community. However, most of the proposals have focused on the extraction of important kernel data structures such as executive objects from the memory. In this paper, we propose a formal approach to analyze the stack memory of process threads to discover a partial execution history of the process. Our approach uses a process logic to model the extracted properties from the stack and then verify these properties against models generated from the program assembly code. The main focus of the paper is on Windows thread stack analysis though the same idea is applicable to other operating systems. (C) 2007 DFRWS. Published by Elsevier Ltd. All rights reserved.

------------------------

Doc id:  WOS:000355635800004
Doc Title:  Numerical simulation of the complete rock blasting response by SPH-DAM-FEM approach
Doc Abstract:  Rock mass blasting is a complex process which involves the coupling of both discontinuous and continuous media. This paper aims to provide a numerical simulator capable of reproducing the complete blasting response in rock mass by incorporating large deformations, damage distribution and blasting vibration with a single numerical model. A Smooth Particle Hydrodynamics (SPH) algorithm and a modified blasting damage model were introduced into the explicit finite element code LS-DYNA with a subroutine interface in order to develop the coupled SPH-DAM-FEM numerical simulator. The coupled SPH technology with the modified Damage model is used to simulate the near zone in the blasting process and the FEM to capture the far field response of the rock. The complete dynamic response of a presplit blast and a bench blast was investigated with the developed coupled model in two case studies regarding a blasting excavation of high rock slopes in China. The discontinuous characteristic of the blast process, large deformations and damage were analyzed carefully. The reproduction of the damage contour and vibration was compared with site measurements. From the analysis conducted, it is shown that the results produced by the developed coupled model match quite well with field observations. This demonstrates the capability of the proposed coupled model. To complete the analysis, a sensitivity analysis of the main SPH parameters was studied parametrically to obtain a better understanding and expose more capabilities and applications of the proposed coupled approach. (C) 2015 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000301307700012
Doc Title:  Sensitivity analysis of CORSIM with respect to the process of freeway flow breakdown at bottleneck locations
Doc Abstract:  Various microscopic simulation models have been used for studying traffic operations along freeway segments. An important desirable function of these models is their ability to obtain capacity and replicate the breakdown process realistically. The objectives of this paper are to evaluate the capability of a microsimulation model. CORSIM, to replicate the process of breakdown and to perform a sensitivity analysis on driver behavior parameters. The research findings indicate that CORSIM has some strengths and some weaknesses with respect to the breakdown process. Sensitivity analysis shows the different effects of these parameters on the breakdown occurrence and provides recommendations on the application of these parameters to provide a more realistic representation of traffic operations. (C) 2011 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000255002900004
Doc Title:  Messages of oscillatory correlograms: A spike train model
Doc Abstract:  Oscillatory correlograms are widely used to study neuronal activity that shows a joint periodic rhythm. In most cases, the statistical analysis of cross-correlation histograms (CCH) features is based on the null model of independent processes, and the resulting conclusions about the underlying processes remain qualitative. Therefore, we propose a spike train model for synchronous oscillatory firing activity that directly links characteristics of the CCH to parameters of the underlying processes. The model focuses particularly on asymmetric central peaks, which differ in slope and width on the two sides. Asymmetric peaks can be associated with phase offsets in the (sub-) millisecond range. These spatiotemporal firing patterns can be highly consistent across units yet invisible in the underlying processes. The proposed model includes a single temporal parameter that accounts for this peak asymmetry. The model provides approaches for the analysis of oscillatory correlograms, taking into account dependencies and nonstationarities in the underlying processes. In particular, the auto- and the cross-correlogram can be investigated in a joint analysis because they depend on the same spike train parameters. Particular temporal interactions such as the degree to which different units synchronize in a common oscillatory rhythm can also be investigated. The analysis is demonstrated by application to a simulated data set.

------------------------

Doc id:  WOS:000296219800001
Doc Title:  Variable window adaptive Kernel Principal Component Analysis for nonlinear nonstationary process monitoring
Doc Abstract:  On-line control of nonlinear nonstationary processes using multivariate statistical methods has recently prompt a lot of interest due to its industrial practical importance. Indeed basic process control methods do not allow monitoring of such processes. For this purpose this study proposes a variable window real-time monitoring system based on a fast block adaptive Kernel Principal Component Analysis scheme. While previous adaptive KPCA models allow only handling of one observation at a time, in this study we propose a way to fast update or downdate the KPCA model when a block of data is provided and not only one observation. Using a variable window size procedure to determine the model size and adaptive chart parameters, this model is applied to monitor two simulated benchmark processes. A comparison of performances of the adopted control strategy with various Principal Component Analysis (PCA) control models shows that the derived strategy is robust and yields better detection abilities of disturbances. (C) 2011 Elsevier Ltd. All rights reserved.

------------------------

Doc id:  WOS:000291521300004
Doc Title:  Analysis on demand: Instantaneous soundness checking of industrial business process models
Doc Abstract:  We report on a case study on control-now analysis of business process models. We checked 735 industrial business process models from financial services, telecommunications, and other domains. We investigated these models for soundness (absence of deadlock and lack of synchronization) using three different approaches: the business process verification tool Woflan, the Petri net model checker LoLA, and a recently developed technique based on SESE decomposition. We evaluate the various techniques used by these approaches in terms of their ability of accelerating the check. Our results show that industrial business process models can be checked in a few milliseconds, which enables tight integration of modeling with control-flow analysis. We also briefly compare the diagnostic information delivered by the different approaches and report some first insights from industrial applications. (C) 2011 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000292785200013
Doc Title:  A fuzzy reasoning and fuzzy-analytical hierarchy process based approach to the process of railway risk information: A railway risk management system
Doc Abstract:  Risk management is becoming increasingly important for railway companies in order to safeguard their passengers and employees while improving safety and reducing maintenance costs. However, in many circumstances, the application of probabilistic risk analysis tools may not give satisfactory results because the risk data are incomplete or there is a high level of uncertainty involved in the risk data. This article presents the development of a risk management system for railway risk analysis using fuzzy reasoning approach and fuzzy analytical hierarchy decision making process. In the system, fuzzy reasoning approach (FRA) is employed to estimate the risk level of each hazardous event in terms of failure frequency, consequence severity and consequence probability. This allows imprecision or approximate information in the risk analysis process. Fuzzy analytical hierarchy process (fuzzy-AHP) technique is then incorporated into the risk model to use its advantage in determining the relative importance of the risk contributions so that the risk assessment can be progressed from hazardous event level to hazard group level and finally to railway system level. This risk assessment system can evaluate both qualitative and quantitative risk data and information associated with a railway system effectively and efficiently, which will provide railway risk analysts, managers and engineers with a method and tool to improve their safety management of railway systems and set safety standards. A case study on risk assessment of shunting at Hammersmith depot is used to illustrate the application of the proposed risk assessment system. (C) 2011 Elsevier Inc. All rights reserved.

------------------------

Doc id:  WOS:000311775200024
Doc Title:  A Bayesian latent variable model with classification and regression tree approach for behavior and credit scoring
Doc Abstract:  A Bayesian latent variable model with classification and regression tree approach is built to overcome three challenges encountered by a bank in credit-granting process. These three challenges include (1) the bank wants to predict the future performance of an applicant accurately; (2) given current information about cardholders' credit usage and repayment behavior, financial institutions would like to determine the optimal credit limit and APR for an applicant; and (3) the bank would like to improve its efficiency by automating the process of credit-granting decisions. Data from a leading bank in Taiwan is used to illustrate the combined approach. The data set consists of each credit card holder's credit usage and repayment data, demographic information, and credit report. Empirical study shows that the demographic variables used in most credit scoring models have little explanatory ability with regard to a cardholder's credit usage and repayment behavior. A cardholder's credit history provides the most important information in credit scoring. The continuous latent customer quality from the Bayesian latent variable model allows considerable latitude for producing finer rules for credit granting decisions. Compared to the performance of discriminant analysis, logistic regression, neural network, multivariate adaptive regression splines (MARS) and support vector machine (SVM), the proposed model has a 92.9% accuracy rate in predicting customer types, is less impacted by prior probabilities, and has a significantly low Type I errors in comparison with the other five approaches. (C) 2012 Elsevier B.V. All rights reserved.

------------------------

Doc id:  WOS:000301219300017
Doc Title:  On-line principal component analysis with application to process modeling
Doc Abstract:  Principal component analysis (PCA) has been widely applied in process monitoring and modeling. The time-varying property of industrial processes requires the adaptive ability of the PCA. This paper introduces a novel PCA algorithm, named on-line PCA (OLPCA). It updates the PCA model according to the process status. The approximate linear dependence (ALD) condition is used to check each new sample. A recursive algorithm is proposed to reconstruct the PCA model with selected samples. Three types of experiments, a synthetic data, a benchmark problem, and a ball mill load experimental data, are used to illustrate our modeling method. The results show that the proposed OLPCA is computationally faster, and the modeling accuracy is higher than conventional moving window PCA (MWPCA) and recursive PCA (RPCA) for time-varying process modeling. (c) 2011 Elsevier By. All rights reserved.

------------------------

Doc id:  WOS:000306771100010
Doc Title:  Fractal projection pursuit classification model applied to geochemical survey data
Doc Abstract:  A new hybrid exploratory data analysis method, fractal projection pursuit classification (FPPC) model, is developed on the basis of the projection pursuit (PP) and fractal models. In this model, objective classification results are obtained by applying the projection index on the basis of the number-size fractal model. The real-coded acceleration genetic algorithm (RAGA) is used to optimize the projection index to establish the optimum projection direction in the model. Stream sedimentary geochemical data, Gejiu Mining District, Yunnan Province, China, were chosen in a case study to demonstrate the processing data analysis using FPPC. The results show that the anomalies are associated with known mineral deposits in the eastern part of the Gejiu District, and correlated with faults and granite in the western part of the study area. It is demonstrated that FPPC can be a powerful tool for multi-factor classification analysis and provide an effective approach to identify anomalies for mineral exploration. (C) 2011 Elsevier Ltd. All rights reserved.

------------------------

